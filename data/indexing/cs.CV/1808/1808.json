[{"id": "1808.00022", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou and Ronald Poppe", "title": "Analyzing Human-Human Interactions: A Survey", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2019.102799", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many videos depict people, and it is their interactions that inform us of\ntheir activities, relation to one another and the cultural and social setting.\nWith advances in human action recognition, researchers have begun to address\nthe automated recognition of these human-human interactions from video. The\nmain challenges stem from dealing with the considerable variation in recording\nsetting, the appearance of the people depicted and the coordinated performance\nof their interaction. This survey provides a summary of these challenges and\ndatasets to address these, followed by an in-depth discussion of relevant\nvision-based recognition and detection methods. We focus on recent, promising\nwork based on deep learning and convolutional neural networks (CNNs). Finally,\nwe outline directions to overcome the limitations of the current\nstate-of-the-art to analyze and, eventually, understand social human actions.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 18:37:41 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 09:24:43 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""]]}, {"id": "1808.00035", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Sobhan Soleymani, Hadi Kazemi, Seyed Mehdi Iranmanesh,\n  Jeremy Dawson, Nasser M. Nasrabadi", "title": "ID Preserving Generative Adversarial Network for Partial Latent\n  Fingerprint Reconstruction", "comments": "Accepted in BTAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing recognition tasks using latent fingerprint samples is often\nchallenging for automated identification systems due to poor quality,\ndistortion, and partially missing information from the input samples. We\npropose a direct latent fingerprint reconstruction model based on conditional\ngenerative adversarial networks (cGANs). Two modifications are applied to the\ncGAN to adapt it for the task of latent fingerprint reconstruction. First, the\nmodel is forced to generate three additional maps to the ridge map to ensure\nthat the orientation and frequency information is considered in the generation\nprocess, and prevent the model from filling large missing areas and generating\nerroneous minutiae. Second, a perceptual ID preservation approach is developed\nto force the generator to preserve the ID information during the reconstruction\nprocess. Using a synthetically generated database of latent fingerprints, the\ndeep network learns to predict missing information from the input latent\nsamples. We evaluate the proposed method in combination with two different\nfingerprint matching algorithms on several publicly available latent\nfingerprint datasets. We achieved the rank-10 accuracy of 88.02\\% on the\nIIIT-Delhi latent fingerprint database for the task of latent-to-latent\nmatching and rank-50 accuracy of 70.89\\% on the IIIT-Delhi MOLF database for\nthe task of latent-to-sensor matching. Experimental results of matching\nreconstructed samples in both latent-to-sensor and latent-to-latent frameworks\nindicate that the proposed method significantly increases the matching accuracy\nof the fingerprint recognition systems for the latent samples.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:20:49 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Kazemi", "Hadi", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1808.00043", "submitter": "Muhammad Waleed Gondal", "authors": "Muhammad Waleed Gondal, Bernhard Sch\\\"olkopf, Michael Hirsch", "title": "The Unreasonable Effectiveness of Texture Transfer for Single Image\n  Super-resolution", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While implicit generative models such as GANs have shown impressive results\nin high quality image reconstruction and manipulation using a combination of\nvarious losses, we consider a simpler approach leading to surprisingly strong\nresults. We show that texture loss alone allows the generation of perceptually\nhigh quality images. We provide a better understanding of texture constraining\nmechanism and develop a novel semantically guided texture constraining method\nfor further improvement. Using a recently developed perceptual metric employing\n\"deep features\" and termed LPIPS, the method obtains state-of-the-art results.\nMoreover, we show that a texture representation of those deep features better\ncapture the perceptual quality of an image than the original deep features.\nUsing texture information, off-the-shelf deep classification networks (without\ntraining) perform as well as the best performing (tuned and calibrated) LPIPS\nmetrics. The code is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:43:24 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Gondal", "Muhammad Waleed", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1808.00046", "submitter": "Mandar Gogate", "authors": "Ahsan Adeel, Mandar Gogate, Amir Hussain, William M. Whitmer", "title": "Lip-Reading Driven Deep Learning Approach for Speech Enhancement", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel lip-reading driven deep learning framework for\nspeech enhancement. The proposed approach leverages the complementary strengths\nof both deep learning and analytical acoustic modelling (filtering based\napproach) as compared to recently published, comparatively simpler benchmark\napproaches that rely only on deep learning. The proposed audio-visual (AV)\nspeech enhancement framework operates at two levels. In the first level, a\nnovel deep learning-based lip-reading regression model is employed. In the\nsecond level, lip-reading approximated clean-audio features are exploited,\nusing an enhanced, visually-derived Wiener filter (EVWF), for the clean audio\npower spectrum estimation. Specifically, a stacked long-short-term memory\n(LSTM) based lip-reading regression model is designed for clean audio features\nestimation using only temporal visual features considering different number of\nprior visual frames. For clean speech spectrum estimation, a new\nfilterbank-domain EVWF is formulated, which exploits estimated speech features.\nThe proposed EVWF is compared with conventional Spectral Subtraction and\nLog-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM\ndriven AV mapping. The potential of the proposed speech enhancement framework\nis evaluated under different dynamic real-world commercially-motivated\nscenarios (e.g. cafe, public transport, pedestrian area) at different SNR\nlevels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora.\nFor objective testing, perceptual evaluation of speech quality is used to\nevaluate the quality of restored speech. For subjective testing, the standard\nmean-opinion-score method is used with inferential statistics. Comparative\nsimulation results demonstrate significant lip-reading and speech enhancement\nimprovement in terms of both speech quality and speech intelligibility.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:50:13 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Adeel", "Ahsan", ""], ["Gogate", "Mandar", ""], ["Hussain", "Amir", ""], ["Whitmer", "William M.", ""]]}, {"id": "1808.00057", "submitter": "Cong Gao", "authors": "Cong Gao, Xingtong Liu, Michael Peven, Mathias Unberath and Austin\n  Reiter", "title": "Learning to See Forces: Surgical Force Prediction with RGB-Point Cloud\n  Temporal Convolutional Networks", "comments": "MICCAI 2018 workshop, CARE(Computer Assisted and Robotic Endoscopy)", "journal-ref": null, "doi": "10.1007/978-3-030-01201-4_14", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic surgery has been proven to offer clear advantages during surgical\nprocedures, however, one of the major limitations is obtaining haptic feedback.\nSince it is often challenging to devise a hardware solution with accurate force\nfeedback, we propose the use of \"visual cues\" to infer forces from tissue\ndeformation. Endoscopic video is a passive sensor that is freely available, in\nthe sense that any minimally-invasive procedure already utilizes it. To this\nend, we employ deep learning to infer forces from video as an attractive\nlow-cost and accurate alternative to typically complex and expensive hardware\nsolutions. First, we demonstrate our approach in a phantom setting using the da\nVinci Surgical System affixed with an OptoForce sensor. Second, we then\nvalidate our method on an ex vivo liver organ. Our method results in a mean\nabsolute error of 0.814 N in the ex vivo study, suggesting that it may be a\npromising alternative to hardware based surgical force feedback in endoscopic\nprocedures.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 20:04:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Gao", "Cong", ""], ["Liu", "Xingtong", ""], ["Peven", "Michael", ""], ["Unberath", "Mathias", ""], ["Reiter", "Austin", ""]]}, {"id": "1808.00059", "submitter": "Seyed Mehdi Iranmanesh", "authors": "Seyed Mehdi Iranmanesh, Hadi Kazemi, Sobhan Soleymani, Ali Dabouei,\n  Nasser M. Nasrabadi", "title": "Deep Sketch-Photo Face Recognition Assisted by Facial Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep coupled framework to address the problem of\nmatching sketch image against a gallery of mugshots. Face sketches have the\nessential in- formation about the spatial topology and geometric details of\nfaces while missing some important facial attributes such as ethnicity, hair,\neye, and skin color. We propose a cou- pled deep neural network architecture\nwhich utilizes facial attributes in order to improve the sketch-photo\nrecognition performance. The proposed Attribute-Assisted Deep Con- volutional\nNeural Network (AADCNN) method exploits the facial attributes and leverages the\nloss functions from the facial attributes identification and face verification\ntasks in order to learn rich discriminative features in a common em- bedding\nsubspace. The facial attribute identification task increases the inter-personal\nvariations by pushing apart the embedded features extracted from individuals\nwith differ- ent facial attributes, while the verification task reduces the\nintra-personal variations by pulling together all the fea- tures that are\nrelated to one person. The learned discrim- inative features can be well\ngeneralized to new identities not seen in the training data. The proposed\narchitecture is able to make full use of the sketch and complementary fa- cial\nattribute information to train a deep model compared to the conventional\nsketch-photo recognition methods. Exten- sive experiments are performed on\ncomposite (E-PRIP) and semi-forensic (IIIT-D semi-forensic) datasets. The\nresults show the superiority of our method compared to the state- of-the-art\nmodels in sketch-photo recognition algorithms\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 20:10:01 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Iranmanesh", "Seyed Mehdi", ""], ["Kazemi", "Hadi", ""], ["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1808.00060", "submitter": "Mandar Gogate", "authors": "Mandar Gogate, Ahsan Adeel, Ricard Marxer, Jon Barker, Amir Hussain", "title": "DNN driven Speaker Independent Audio-Visual Mask Estimation for Speech\n  Separation", "comments": "Accepted for Interspeech 2018, 5 pages, 4 figures", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2516", "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human auditory cortex excels at selectively suppressing background noise to\nfocus on a target speaker. The process of selective attention in the brain is\nknown to contextually exploit the available audio and visual cues to better\nfocus on target speaker while filtering out other noises. In this study, we\npropose a novel deep neural network (DNN) based audiovisual (AV) mask\nestimation model. The proposed AV mask estimation model contextually integrates\nthe temporal dynamics of both audio and noise-immune visual features for\nimproved mask estimation and speech separation. For optimal AV features\nextraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is\nexploited to leverages the complementary strengths of a stacked long short term\nmemory (LSTM) and convolution LSTM network. The comparative simulation results\nin terms of speech quality and intelligibility demonstrate significant\nperformance improvement of our proposed AV mask estimation model as compared to\naudio-only and visual-only mask estimation approaches for both speaker\ndependent and independent scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 20:12:15 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Gogate", "Mandar", ""], ["Adeel", "Ahsan", ""], ["Marxer", "Ricard", ""], ["Barker", "Jon", ""], ["Hussain", "Amir", ""]]}, {"id": "1808.00118", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov, Miao Song, Jashanjot Singh, Joey Paquet, Mourad\n  Debbabi, Sudhir Mudur", "title": "Toward Multimodal Interaction in Scalable Visual Digital Evidence\n  Visualization Using Computer Vision Techniques and ISS", "comments": "reformatted; ICPRAI 2018 conference proceedings, pp. 151-157,\n  CENPARMI, Concordia University, Montreal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization requirements in Forensic Lucid have to do with different levels\nof case knowledge abstraction, representation, aggregation, as well as the\noperational aspects as the final long-term goal of this proposal. It\nencompasses anything from the finer detailed representation of hierarchical\ncontexts to Forensic Lucid programs, to the documented evidence and its\nmanagement, its linkage to programs, to evaluation, and to the management of\nGIPSY software networks. This includes an ability to arbitrarily switch between\nthose views combined with usable multimodal interaction. The purpose is to\ndetermine how the findings can be applied to Forensic Lucid and investigation\ncase management. It is also natural to want a convenient and usable evidence\nvisualization, its semantic linkage and the reasoning machinery for it. Thus,\nwe propose a scalable management, visualization, and evaluation of digital\nevidence using the modified interactive 3D documentary system - Illimitable\nSpace System - (ISS) to represent, semantically link, and provide a usable\ninterface to digital investigators that is navigable via different multimodal\ninteraction techniques using Computer Vision techniques including gestures, as\nwell as eye-gaze and audio.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 00:28:32 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Mokhov", "Serguei A.", ""], ["Song", "Miao", ""], ["Singh", "Jashanjot", ""], ["Paquet", "Joey", ""], ["Debbabi", "Mourad", ""], ["Mudur", "Sudhir", ""]]}, {"id": "1808.00130", "submitter": "Duo Lu", "authors": "Duo Lu, Dijiang Huang", "title": "FMCode: A 3D In-the-Air Finger Motion Based User Login Framework for\n  Gesture Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications using gesture-based human-computer interface require a new user\nlogin method with gestures because it does not have a traditional input method\nto type a password. However, due to various challenges, existing gesture-based\nauthentication systems are generally considered too weak to be useful in\npractice. In this paper, we propose a unified user login framework using 3D\nin-air-handwriting, called FMCode. We define new types of features critical to\ndistinguish legitimate users from attackers and utilize Support Vector Machine\n(SVM) for user authentication. The features and data-driven models are\nspecially designed to accommodate minor behavior variations that existing\ngesture authentication methods neglect. In addition, we use deep neural network\napproaches to efficiently identify the user based on his or her\nin-air-handwriting, which avoids expansive account database search methods\nemployed by existing work. On a dataset collected by us with over 100 users,\nour prototype system achieves 0.1% and 0.5% best Equal Error Rate (EER) for\nuser authentication, as well as 96.7% and 94.3% accuracy for user\nidentification, using two types of gesture input devices. Compared to existing\nbehavioral biometric systems using gesture and in-air-handwriting, our\nframework achieves the state-of-the-art performance. In addition, our\nexperimental results show that FMCode is capable to defend against client-side\nspoofing attacks, and it performs persistently in the long run. These results\nand discoveries pave the way to practical usage of gesture-based user login\nover the gesture interface.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 01:24:22 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lu", "Duo", ""], ["Huang", "Dijiang", ""]]}, {"id": "1808.00136", "submitter": "Rafael Felix", "authors": "Rafael Felix, B. G. Vijay Kumar, Ian Reid, Gustavo Carneiro", "title": "Multi-modal Cycle-consistent Generalized Zero-Shot Learning", "comments": "Accepted at ECCV 2018, 15th European Conference on Computer Vision,\n  September 8 to 14, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In generalized zero shot learning (GZSL), the set of classes are split into\nseen and unseen classes, where training relies on the semantic features of the\nseen and unseen classes and the visual representations of only the seen\nclasses, while testing uses the visual representations of the seen and unseen\nclasses. Current methods address GZSL by learning a transformation from the\nvisual to the semantic space, exploring the assumption that the distribution of\nclasses in the semantic and visual spaces is relatively similar. Such methods\ntend to transform unseen testing visual representations into one of the seen\nclasses' semantic features instead of the semantic features of the correct\nunseen class, resulting in low accuracy GZSL classification. Recently,\ngenerative adversarial networks (GAN) have been explored to synthesize visual\nrepresentations of the unseen classes from their semantic features - the\nsynthesized representations of the seen and unseen classes are then used to\ntrain the GZSL classifier. This approach has been shown to boost GZSL\nclassification accuracy, however, there is no guarantee that synthetic visual\nrepresentations can generate back their semantic feature in a multi-modal\ncycle-consistent manner. This constraint can result in synthetic visual\nrepresentations that do not represent well their semantic features. In this\npaper, we propose the use of such constraint based on a new regularization for\nthe GAN training that forces the generated visual features to reconstruct their\noriginal semantic features. Once our model is trained with this multi-modal\ncycle-consistent semantic compatibility, we can then synthesize more\nrepresentative visual representations for the seen and, more importantly, for\nthe unseen classes. Our proposed approach shows the best GZSL classification\nresults in the field in several publicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 01:47:55 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 03:55:52 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Felix", "Rafael", ""], ["Kumar", "B. G. Vijay", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1808.00141", "submitter": "Cristian Rodriguez", "authors": "Cristian Rodriguez, Basura Fernando and Hongdong Li", "title": "Action Anticipation By Predicting Future Dynamic Images", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action-anticipation methods predict what is the future action by\nobserving only a few portion of an action in progress. This is critical for\napplications where computers have to react to human actions as early as\npossible such as autonomous driving, human-robotic interaction, assistive\nrobotics among others. In this paper, we present a method for human action\nanticipation by predicting the most plausible future human motion. We represent\nhuman motion using Dynamic Images and make use of tailored loss functions to\nencourage a generative model to produce accurate future motion prediction. Our\nmethod outperforms the currently best performing action-anticipation methods by\n4% on JHMDB-21, 5.2% on UT-Interaction and 5.1% on UCF 101-24 benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 02:10:50 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Rodriguez", "Cristian", ""], ["Fernando", "Basura", ""], ["Li", "Hongdong", ""]]}, {"id": "1808.00150", "submitter": "Xinjing Cheng", "authors": "Xinjing Cheng, Peng Wang, and Ruigang Yang", "title": "Depth Estimation via Affinity Learned with Convolutional Spatial\n  Propagation Network", "comments": "14 pages, 8 figures, ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from a single image is a fundamental problem in computer\nvision. In this paper, we propose a simple yet effective convolutional spatial\npropagation network (CSPN) to learn the affinity matrix for depth prediction.\nSpecifically, we adopt an efficient linear propagation model, where the\npropagation is performed with a manner of recurrent convolutional operation,\nand the affinity among neighboring pixels is learned through a deep\nconvolutional neural network (CNN). We apply the designed CSPN to two depth\nestimation tasks given a single image: (1) To refine the depth output from\nstate-of-the-art (SOTA) existing methods; and (2) to convert sparse depth\nsamples to a dense depth map by embedding the depth samples within the\npropagation procedure. The second task is inspired by the availability of\nLIDARs that provides sparse but accurate depth measurements. We experimented\nthe proposed CSPN over two popular benchmarks for depth estimation, i.e. NYU v2\nand KITTI, where we show that our proposed approach improves in not only\nquality (e.g., 30% more reduction in depth error), but also speed (e.g., 2 to 5\ntimes faster) than prior SOTA methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 03:23:06 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Cheng", "Xinjing", ""], ["Wang", "Peng", ""], ["Yang", "Ruigang", ""]]}, {"id": "1808.00157", "submitter": "Ke Gong", "authors": "Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang, Liang Lin", "title": "Instance-level Human Parsing via Part Grouping Network", "comments": "Accepted by ECCV 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-level human parsing towards real-world human analysis scenarios is\nstill under-explored due to the absence of sufficient data resources and\ntechnical difficulty in parsing multiple instances in a single pass. Several\nrelated works all follow the \"parsing-by-detection\" pipeline that heavily\nrelies on separately trained detection models to localize instances and then\nperforms human parsing for each instance sequentially. Nonetheless, two\ndiscrepant optimization targets of detection and parsing lead to suboptimal\nrepresentation learning and error accumulation for final results. In this work,\nwe make the first attempt to explore a detection-free Part Grouping Network\n(PGN) for efficiently parsing multiple people in an image in a single pass. Our\nPGN reformulates instance-level human parsing as two twinned sub-tasks that can\nbe jointly learned and mutually refined via a unified network: 1) semantic part\nsegmentation for assigning each pixel as a human part (e.g., face, arms); 2)\ninstance-aware edge detection to group semantic parts into distinct person\ninstances. Thus the shared intermediate representation would be endowed with\ncapabilities in both characterizing fine-grained parts and inferring instance\nbelongings of each part. Finally, a simple instance partition process is\nemployed to get final results during inference. We conducted experiments on\nPASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art\nmethods. Furthermore, we show its superiority on a newly collected multi-person\nparsing dataset (CIHP) including 38,280 diverse images, which is the largest\ndataset so far and can facilitate more advanced human analysis. The CIHP\nbenchmark and our source code are available at http://sysu-hcp.net/lip/.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 03:51:59 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Gong", "Ke", ""], ["Liang", "Xiaodan", ""], ["Li", "Yicheng", ""], ["Chen", "Yimin", ""], ["Yang", "Ming", ""], ["Lin", "Liang", ""]]}, {"id": "1808.00171", "submitter": "Xu Yang", "authors": "Xu Yang, Hanwang Zhang, Jianfei Cai", "title": "Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fact that it is prohibitively expensive to completely annotate\nvisual relationships, i.e., the (obj1, rel, obj2) triplets, relationship models\nare inevitably biased to object classes of limited pairwise patterns, leading\nto poor generalization to rare or unseen object combinations. Therefore, we are\ninterested in learning object-agnostic visual features for more generalizable\nrelationship models. By \"agnostic\", we mean that the feature is less likely\nbiased to the classes of paired objects. To alleviate the bias, we propose a\nnovel \\texttt{Shuffle-Then-Assemble} pre-training strategy. First, we discard\nall the triplet relationship annotations in an image, leaving two unpaired\nobject domains without obj1-obj2 alignment. Then, our feature learning is to\nrecover possible obj1-obj2 pairs. In particular, we design a cycle of residual\ntransformations between the two domains, to capture shared but not\nobject-specific visual patterns. Extensive experiments on two visual\nrelationship benchmarks show that by using our pre-trained features, naive\nrelationship models can be consistently improved and even outperform other\nstate-of-the-art relationship models. Code has been made available at:\n\\url{https://github.com/yangxuntu/vrd}.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 05:20:28 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Yang", "Xu", ""], ["Zhang", "Hanwang", ""], ["Cai", "Jianfei", ""]]}, {"id": "1808.00178", "submitter": "Sebastian Bodenstedt", "authors": "Sebastian Bodenstedt and Antonia Ohnemus and Darko Katic and\n  Anna-Laura Wekerle and Martin Wagner and Hannes Kenngott and Beat\n  M\\\"uller-Stich and R\\\"udiger Dillmann and Stefanie Speidel", "title": "Real-time image-based instrument classification for laparoscopic surgery", "comments": "Workshop paper accepted and presented at Modeling and Monitoring of\n  Computer Assisted Interventions (M2CAI) (2015)", "journal-ref": "Modeling and Monitoring of Computer Assisted Interventions (M2CAI)\n  (2015)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During laparoscopic surgery, context-aware assistance systems aim to\nalleviate some of the difficulties the surgeon faces. To ensure that the right\ninformation is provided at the right time, the current phase of the\nintervention has to be known. Real-time locating and classification the\nsurgical tools currently in use are key components of both an activity-based\nphase recognition and assistance generation.\n  In this paper, we present an image-based approach that detects and classifies\ntools during laparoscopic interventions in real-time. First, potential\ninstrument bounding boxes are detected using a pixel-wise random forest\nsegmentation. Each of these bounding boxes is then classified using a cascade\nof random forest. For this, multiple features, such as histograms over hue and\nsaturation, gradients and SURF feature, are extracted from each detected\nbounding box.\n  We evaluated our approach on five different videos from two different types\nof procedures. We distinguished between the four most common classes of\ninstruments (LigaSure, atraumatic grasper, aspirator, clip applier) and\nbackground. Our method succesfully located up to 86% of all instruments\nrespectively. On manually provided bounding boxes, we achieve a instrument type\nrecognition rate of up to 58% and on automatically detected bounding boxes up\nto 49%.\n  To our knowledge, this is the first approach that allows an image-based\nclassification of surgical tools in a laparoscopic setting in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:08:16 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Bodenstedt", "Sebastian", ""], ["Ohnemus", "Antonia", ""], ["Katic", "Darko", ""], ["Wekerle", "Anna-Laura", ""], ["Wagner", "Martin", ""], ["Kenngott", "Hannes", ""], ["M\u00fcller-Stich", "Beat", ""], ["Dillmann", "R\u00fcdiger", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1808.00184", "submitter": "Hongxiang Gu", "authors": "Hongxiang Gu and Viswanathan Swaminathan", "title": "From Thumbnails to Summaries - A single Deep Neural Network to Rule Them\n  All", "comments": "6 pages, 2 figures, IEEE International Conference on Multimedia and\n  Expo (ICME) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summaries come in many forms, from traditional single-image thumbnails,\nanimated thumbnails, storyboards, to trailer-like video summaries. Content\ncreators use the summaries to display the most attractive portion of their\nvideos; the users use them to quickly evaluate if a video is worth watching.\nAll forms of summaries are essential to video viewers, content creators, and\nadvertisers. Often video content management systems have to generate multiple\nversions of summaries that vary in duration and presentational forms. We\npresent a framework ReconstSum that utilizes LSTM-based autoencoder\narchitecture to extract and select a sparse subset of video frames or keyshots\nthat optimally represent the input video in an unsupervised manner. The encoder\nselects a subset from the input video while the decoder seeks to reconstruct\nthe video from the selection. The goal is to minimize the difference between\nthe original input video and the reconstructed video. Our method is easily\nextendable to generate a variety of applications including static video\nthumbnails, animated thumbnails, storyboards and \"trailer-like\" highlights. We\nspecifically study and evaluate two most popular use cases: thumbnail\ngeneration and storyboard generation. We demonstrate that our methods generate\nbetter results than the state-of-the-art techniques in both use cases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:24:39 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Gu", "Hongxiang", ""], ["Swaminathan", "Viswanathan", ""]]}, {"id": "1808.00191", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh", "title": "Graph R-CNN for Scene Graph Generation", "comments": "16 pages, ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel scene graph generation model called Graph R-CNN, that is\nboth effective and efficient at detecting objects and their relations in\nimages. Our model contains a Relation Proposal Network (RePN) that efficiently\ndeals with the quadratic number of potential relations between objects in an\nimage. We also propose an attentional Graph Convolutional Network (aGCN) that\neffectively captures contextual information between objects and relations.\nFinally, we introduce a new evaluation metric that is more holistic and\nrealistic than existing metrics. We report state-of-the-art performance on\nscene graph generation as evaluated using both existing and our proposed\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:50:19 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Yang", "Jianwei", ""], ["Lu", "Jiasen", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1808.00193", "submitter": "Chen Yukang", "authors": "Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, Chang Huang,\n  Lisen Mu, Xinggang Wang", "title": "Reinforced Evolutionary Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an important yet challenging task in\nnetwork design due to its high computational consumption. To address this\nissue, we propose the Reinforced Evolutionary Neural Architecture Search (RE-\nNAS), which is an evolutionary method with the reinforced mutation for NAS. Our\nmethod integrates reinforced mutation into an evolution algorithm for neural\narchitecture exploration, in which a mutation controller is introduced to learn\nthe effects of slight modifications and make mutation actions. The reinforced\nmutation controller guides the model population to evolve efficiently.\nFurthermore, as child models can inherit parameters from their parents during\nevolution, our method requires very limited computational resources. In\nexperiments, we conduct the proposed search method on CIFAR-10 and obtain a\npowerful network architecture, RENASNet. This architecture achieves a\ncompetitive result on CIFAR-10. The explored network architecture is\ntransferable to ImageNet and achieves a new state-of-the-art accuracy, i.e.,\n75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test\nits performance on semantic segmentation with DeepLabv3 on the PASCAL VOC.\nRENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83%\nmIOU without being pre-trained on COCO.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 06:53:53 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 02:09:29 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 11:12:02 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chen", "Yukang", ""], ["Meng", "Gaofeng", ""], ["Zhang", "Qian", ""], ["Xiang", "Shiming", ""], ["Huang", "Chang", ""], ["Mu", "Lisen", ""], ["Wang", "Xinggang", ""]]}, {"id": "1808.00195", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Sayaka Shiota and Hitoshi Kiya", "title": "A Pseudo Multi-Exposure Fusion Method Using Single Image", "comments": "To appear in IEICE Trans. Fundamentals, vol.E101-A, no.11, November\n  2018", "journal-ref": null, "doi": "10.1587/transfun.E101.A.1806", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel pseudo multi-exposure image fusion method based\non a single image. Multi-exposure image fusion is used to produce images\nwithout saturation regions, by using photos with different exposures. However,\nit is difficult to take photos suited for the multi-exposure image fusion when\nwe take a photo of dynamic scenes or record a video. In addition, the\nmulti-exposure image fusion cannot be applied to existing images with a single\nexposure or videos. The proposed method enables us to produce pseudo\nmulti-exposure images from a single image. To produce multi-exposure images,\nthe proposed method utilizes the relationship between the exposure values and\npixel values, which is obtained by assuming that a digital camera has a linear\nresponse function. Moreover, it is shown that the use of a local contrast\nenhancement method allows us to produce pseudo multi-exposure images with\nhigher quality. Most of conventional multi-exposure image fusion methods are\nalso applicable to the proposed multi-exposure images. Experimental results\nshow the effectiveness of the proposed method by comparing the proposed one\nwith conventional ones.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:03:11 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1808.00208", "submitter": "Nakul Agarwal", "authors": "A.H. Abdul Hafez, Nakul Agarwal, C.V. Jawahar", "title": "Connecting Visual Experiences using Max-flow Network with Application to\n  Visual Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by the fact that multiple representations of the environment\nare required to stand for the changes in appearance with time and for changes\nthat appear in a cyclic manner. These changes are, for example, from day to\nnight time, and from day to day across seasons. In such situations, the robot\nvisits the same routes multiple times and collects different appearances of it.\nMultiple visual experiences usually find robotic vision applications like\nvisual localization, mapping, place recognition, and autonomous navigation. The\nnovelty in this paper is an algorithm that connects multiple visual experiences\nvia aligning multiple image sequences. This problem is solved by finding the\nmaximum flow in a directed graph flow-network, whose vertices represent the\nmatches between frames in the test and reference sequences. Edges of the graph\nrepresent the cost of these matches. The problem of finding the best match is\nreduced to finding the minimum-cut surface, which is solved as a maximum flow\nnetwork problem. Application to visual localization is considered in this paper\nto show the effectiveness of the proposed multiple image sequence alignment\nmethod, without loosing its generality. Experimental evaluations show that the\nprecision of sequence matching is improved by considering multiple visual\nsequences for the same route, and that the method performs favorably against\nstate-of-the-art single representation methods like SeqSLAM and ABLE-M.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:46:58 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Hafez", "A. H. Abdul", ""], ["Agarwal", "Nakul", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1808.00244", "submitter": "Mikhail Belyaev", "authors": "Egor Krivov, Valery Kostjuchenko, Alexandra Dalechina, Boris\n  Shirokikh, Gleb karchuk, Alexander Denisenko, Andrey Golanov, Mikhail Belyaev", "title": "Tumor Delineation For Brain Radiosurgery by a ConvNet and Non-Uniform\n  Patch Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are actively used for brain lesion segmentation. One of\nthe most popular models is DeepMedic, which was developed for segmentation of\nrelatively large lesions like glioma and ischemic stroke. In our work, we\nconsider segmentation of brain tumors appropriate to stereotactic radiosurgery\nwhich limits typical lesion sizes. These differences in target volumes lead to\na large number of false negatives (especially for small lesions) as well as to\nan increased number of false positives for DeepMedic. We propose a new\npatch-sampling procedure to increase network performance for small lesions. We\nused a 6-year dataset from a stereotactic radiosurgery center. To evaluate our\napproach, we conducted experiments with the three most frequent brain tumors:\nmetastasis, meningioma, schwannoma. In addition to cross-validation, we\nestimated quality on a hold-out test set which was collected several years\nlater than the train one. The experimental results show solid improvements in\nboth cases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 09:41:50 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Krivov", "Egor", ""], ["Kostjuchenko", "Valery", ""], ["Dalechina", "Alexandra", ""], ["Shirokikh", "Boris", ""], ["karchuk", "Gleb", ""], ["Denisenko", "Alexander", ""], ["Golanov", "Andrey", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1808.00255", "submitter": "Caner Sahin", "authors": "Caner Sahin and Tae-Kyun Kim", "title": "Category-level 6D Object Pose Recovery in Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-class variations, distribution shifts among source and target domains\nare the major challenges of category-level tasks. In this study, we address\ncategory-level full 6D object pose estimation in the context of depth modality,\nintroducing a novel part-based architecture that can tackle the above-mentioned\nchallenges. Our architecture particularly adapts the distribution shifts\narising from shape discrepancies, and naturally removes the variations of\ntexture, illumination, pose, etc., so we call it as \"Intrinsic Structure\nAdaptor (ISA)\". We engineer ISA based on the followings: i) \"Semantically\nSelected Centers (SSC)\" are proposed in order to define the \"6D pose\" at the\nlevel of categories. ii) 3D skeleton structures, which we derive as\nshape-invariant features, are used to represent the parts extracted from the\ninstances of given categories, and privileged one-class learning is employed\nbased on these parts. iii) Graph matching is performed during training in such\na way that the adaptation/generalization capability of the proposed\narchitecture is improved across unseen instances. Experiments validate the\npromising performance of the proposed architecture on both synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 10:36:21 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Sahin", "Caner", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1808.00257", "submitter": "Tom Runia", "authors": "Rijnder Wever, Tom F.H. Runia", "title": "Subitizing with Variational Autoencoders", "comments": null, "journal-ref": "European Conference on Computer Vision 2018 - Workshop on\n  Brain-Driven Computer Vision", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerosity, the number of objects in a set, is a basic property of a given\nvisual scene. Many animals develop the perceptual ability to subitize: the\nnear-instantaneous identification of the numerosity in small sets of visual\nitems. In computer vision, it has been shown that numerosity emerges as a\nstatistical property in neural networks during unsupervised learning from\nsimple synthetic images. In this work, we focus on more complex natural images\nusing unsupervised hierarchical neural networks. Specifically, we show that\nvariational autoencoders are able to spontaneously perform subitizing after\ntraining without supervision on a large amount images from the Salient Object\nSubitizing dataset. While our method is unable to outperform supervised\nconvolutional networks for subitizing, we observe that the networks learn to\nencode numerosity as basic visual property. Moreover, we find that the learned\nrepresentations are likely invariant to object area; an observation in\nalignment with studies on biological neural networks in cognitive neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 10:45:52 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Wever", "Rijnder", ""], ["Runia", "Tom F. H.", ""]]}, {"id": "1808.00262", "submitter": "Carola Figueroa Flores", "authors": "Carola Figueroa Flores, Abel Gonzalez-Garc\\'ia, Joost van de Weijer\n  and Bogdan Raducanu", "title": "Saliency for Fine-grained Object Recognition in Domains with Scarce\n  Training Data", "comments": "Published in Pattern Recognition journal", "journal-ref": "Pattern Recognition, 2019", "doi": "10.1016/j.patcog.2019.05.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the role of saliency to improve the classification\naccuracy of a Convolutional Neural Network (CNN) for the case when scarce\ntraining data is available. Our approach consists in adding a saliency branch\nto an existing CNN architecture which is used to modulate the standard\nbottom-up visual features from the original image input, acting as an\nattentional mechanism that guides the feature extraction process. The main aim\nof the proposed approach is to enable the effective training of a fine-grained\nrecognition model with limited training samples and to improve the performance\non the task, thereby alleviating the need to annotate large dataset. % The vast\nmajority of saliency methods are evaluated on their ability to generate\nsaliency maps, and not on their functionality in a complete vision pipeline.\nOur proposed pipeline allows to evaluate saliency methods for the high-level\ntask of object recognition. We perform extensive experiments on various\nfine-grained datasets (Flowers, Birds, Cars, and Dogs) under different\nconditions and show that saliency can considerably improve the network's\nperformance, especially for the case of scarce training data. Furthermore, our\nexperiments show that saliency methods that obtain improved saliency maps (as\nmeasured by traditional saliency benchmarks) also translate to saliency methods\nthat yield improved performance gains when applied in an object recognition\npipeline.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 10:55:14 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 16:17:21 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 16:56:10 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Flores", "Carola Figueroa", ""], ["Gonzalez-Garc\u00eda", "Abel", ""], ["van de Weijer", "Joost", ""], ["Raducanu", "Bogdan", ""]]}, {"id": "1808.00265", "submitter": "Yundong Zhang", "authors": "Yundong Zhang, Juan Carlos Niebles, Alvaro Soto", "title": "Interpretable Visual Question Answering by Visual Grounding from\n  Attention Supervision Mining", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of VQA models that are interpretable is their ability to ground\ntheir answers to relevant regions in the image. Current approaches with this\ncapability rely on supervised learning and human annotated groundings to train\nattention mechanisms inside the VQA architecture. Unfortunately, obtaining\nhuman annotations specific for visual grounding is difficult and expensive. In\nthis work, we demonstrate that we can effectively train a VQA architecture with\ngrounding supervision that can be automatically obtained from available region\ndescriptions and object annotations. We also show that our model trained with\nthis mined supervision generates visual groundings that achieve a higher\ncorrelation with respect to manually-annotated groundings, meanwhile achieving\nstate-of-the-art VQA accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 11:06:08 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Zhang", "Yundong", ""], ["Niebles", "Juan Carlos", ""], ["Soto", "Alvaro", ""]]}, {"id": "1808.00273", "submitter": "Wenjia Bai", "authors": "Wenjia Bai, Hideaki Suzuki, Chen Qin, Giacomo Tarroni, Ozan Oktay,\n  Paul M. Matthews, Daniel Rueckert", "title": "Recurrent neural networks for aortic image sequence segmentation with\n  sparse annotations", "comments": "Accepted for publication by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of image sequences is an important task in medical image\nanalysis, which enables clinicians to assess the anatomy and function of moving\norgans. However, direct application of a segmentation algorithm to each time\nframe of a sequence may ignore the temporal continuity inherent in the\nsequence. In this work, we propose an image sequence segmentation algorithm by\ncombining a fully convolutional network with a recurrent neural network, which\nincorporates both spatial and temporal information into the segmentation task.\nA key challenge in training this network is that the available manual\nannotations are temporally sparse, which forbids end-to-end training. We\naddress this challenge by performing non-rigid label propagation on the\nannotations and introducing an exponentially weighted loss function for\ntraining. Experiments on aortic MR image sequences demonstrate that the\nproposed method significantly improves both accuracy and temporal smoothness of\nsegmentation, compared to a baseline method that utilises spatial information\nonly. It achieves an average Dice metric of 0.960 for the ascending aorta and\n0.953 for the descending aorta.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 11:20:54 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Bai", "Wenjia", ""], ["Suzuki", "Hideaki", ""], ["Qin", "Chen", ""], ["Tarroni", "Giacomo", ""], ["Oktay", "Ozan", ""], ["Matthews", "Paul M.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1808.00278", "submitter": "Zechun Liu", "authors": "Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting\n  Cheng", "title": "Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved\n  Representational Capability and Advanced Training Algorithm", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2018. Code\n  is available on: https://github.com/liuzechun/Bi-Real-net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we study the 1-bit convolutional neural networks (CNNs), of\nwhich both the weights and activations are binary. While being efficient, the\nclassification accuracy of the current 1-bit CNNs is much worse compared to\ntheir counterpart real-valued CNN models on the large-scale dataset, like\nImageNet. To minimize the performance gap between the 1-bit and real-valued CNN\nmodels, we propose a novel model, dubbed Bi-Real net, which connects the real\nactivations (after the 1-bit convolution and/or BatchNorm layer, before the\nsign function) to activations of the consecutive block, through an identity\nshortcut. Consequently, compared to the standard 1-bit CNN, the\nrepresentational capability of the Bi-Real net is significantly enhanced and\nthe additional cost on computation is negligible. Moreover, we develop a\nspecific training algorithm including three technical novelties for 1- bit\nCNNs. Firstly, we derive a tight approximation to the derivative of the\nnon-differentiable sign function with respect to activation. Secondly, we\npropose a magnitude-aware gradient with respect to the weight for updating the\nweight parameters. Thirdly, we pre-train the real-valued CNN model with a clip\nfunction, rather than the ReLU function, to better initialize the Bi-Real net.\nExperiments on ImageNet show that the Bi-Real net with the proposed training\nalgorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers,\nrespectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net\nachieves up to 10% higher top-1 accuracy with more memory saving and lower\ncomputational cost. Keywords: binary neural network, 1-bit CNNs,\n1-layer-per-block\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 11:40:59 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 09:17:12 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 12:23:08 GMT"}, {"version": "v4", "created": "Thu, 27 Sep 2018 08:21:57 GMT"}, {"version": "v5", "created": "Sat, 29 Sep 2018 03:34:59 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Zechun", ""], ["Wu", "Baoyuan", ""], ["Luo", "Wenhan", ""], ["Yang", "Xin", ""], ["Liu", "Wei", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "1808.00286", "submitter": "Francisco M. Castro", "authors": "Francisco M. Castro, Nicol\\'as Guil, Manuel J. Mar\\'in-Jim\\'enez,\n  Jes\\'us P\\'erez-Serrano, Manuel Ujald\\'on", "title": "Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs", "comments": "To appear in Concurrency and Computation: Practice and Experience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) applications are gaining momentum in the realm of\nArtificial Intelligence, particularly after GPUs have demonstrated remarkable\nskills for accelerating their challenging computational requirements. Within\nthis context, Convolutional Neural Network (CNN) models constitute a\nrepresentative example of success on a wide set of complex applications,\nparticularly on datasets where the target can be represented through a\nhierarchy of local features of increasing semantic complexity. In most of the\nreal scenarios, the roadmap to improve results relies on CNN settings involving\nbrute force computation, and researchers have lately proven Nvidia GPUs to be\none of the best hardware counterparts for acceleration. Our work complements\nthose findings with an energy study on critical parameters for the deployment\nof CNNs on flagship image and video applications: object recognition and people\nidentification by gait, respectively. We evaluate energy consumption on four\ndifferent networks based on the two most popular ones (ResNet/AlexNet): ResNet\n(167 layers), a 2D CNN (15 layers), a CaffeNet (25 layers) and a ResNetIm (94\nlayers) using batch sizes of 64, 128 and 256, and then correlate those with\nspeed-up and accuracy to determine optimal settings. Experimental results on a\nmulti-GPU server endowed with twin Maxwell and twin Pascal Titan X GPUs\ndemonstrate that energy correlates with performance and that Pascal may have up\nto 40% gains versus Maxwell. Larger batch sizes extend performance gains and\nenergy savings, but we have to keep an eye on accuracy, which sometimes shows a\npreference for small batches. We expect this work to provide a preliminary\nguidance for a wide set of CNN and DL applications in modern HPC times, where\nthe GFLOPS/w ratio constitutes the primary goal.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 12:08:02 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Castro", "Francisco M.", ""], ["Guil", "Nicol\u00e1s", ""], ["Mar\u00edn-Jim\u00e9nez", "Manuel J.", ""], ["P\u00e9rez-Serrano", "Jes\u00fas", ""], ["Ujald\u00f3n", "Manuel", ""]]}, {"id": "1808.00288", "submitter": "Jiong Wang", "authors": "Yingying Zhu, Jiong Wang, Lingxi Xie, Liang Zheng", "title": "Attention-based Pyramid Aggregation Network for Visual Place Recognition", "comments": "Accepted to ACM Multimedia 2018", "journal-ref": null, "doi": "10.1145/3240508.3240525", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is challenging in the urban environment and is\nusually viewed as a large scale image retrieval task. The intrinsic challenges\nin place recognition exist that the confusing objects such as cars and trees\nfrequently occur in the complex urban scene, and buildings with repetitive\nstructures may cause over-counting and the burstiness problem degrading the\nimage representations. To address these problems, we present an Attention-based\nPyramid Aggregation Network (APANet), which is trained in an end-to-end manner\nfor place recognition. One main component of APANet, the spatial pyramid\npooling, can effectively encode the multi-size buildings containing\ngeo-information. The other one, the attention block, is adopted as a region\nevaluator for suppressing the confusing regional features while highlighting\nthe discriminative ones. When testing, we further propose a simple yet\neffective PCA power whitening strategy, which significantly improves the widely\nused PCA whitening by reasonably limiting the impact of over-counting.\nExperimental evaluations demonstrate that the proposed APANet outperforms the\nstate-of-the-art methods on two place recognition benchmarks, and generalizes\nwell on standard image retrieval datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 12:10:40 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Zhu", "Yingying", ""], ["Wang", "Jiong", ""], ["Xie", "Lingxi", ""], ["Zheng", "Liang", ""]]}, {"id": "1808.00297", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh, Suman Saha, Fabio Cuzzolin", "title": "TraMNet - Transition Matrix Network for Efficient Action Tube Proposals", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art methods solve spatiotemporal action localisation by\nextending 2D anchors to 3D-cuboid proposals on stacks of frames, to generate\nsets of temporally connected bounding boxes called \\textit{action micro-tubes}.\nHowever, they fail to consider that the underlying anchor proposal hypotheses\nshould also move (transition) from frame to frame, as the actor or the camera\ndoes. Assuming we evaluate $n$ 2D anchors in each frame, then the number of\npossible transitions from each 2D anchor to the next, for a sequence of $f$\nconsecutive frames, is in the order of $O(n^f)$, expensive even for small\nvalues of $f$. To avoid this problem, we introduce a Transition-Matrix-based\nNetwork (TraMNet) which relies on computing transition probabilities between\nanchor proposals while maximising their overlap with ground truth bounding\nboxes across frames, and enforcing sparsity via a transition threshold. As the\nresulting transition matrix is sparse and stochastic, this reduces the proposal\nhypothesis search space from $O(n^f)$ to the cardinality of the thresholded\nmatrix. At training time, transitions are specific to cell locations of the\nfeature maps, so that a sparse (efficient) transition matrix is used to train\nthe network. At test time, a denser transition matrix can be obtained either by\ndecreasing the threshold or by adding to it all the relative transitions\noriginating from any cell location, allowing the network to handle transitions\nin the test data that might not have been present in the training data, and\nmaking detection translation-invariant. Finally, we show that our network can\nhandle sparse annotations such as those available in the DALY dataset. We\nreport extensive experiments on the DALY, UCF101-24 and Transformed-UCF101-24\ndatasets to support our claims.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 12:33:57 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Singh", "Gurkirt", ""], ["Saha", "Suman", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1808.00300", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Carl Doersch and Adam Santoro and Peter\n  Battaglia", "title": "Learning Visual Question Answering by Bootstrapping Hard Attention", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms in biological perception are thought to select subsets\nof perceptual information for more sophisticated processing which would be\nprohibitive to perform on all sensory inputs. In computer vision, however,\nthere has been relatively little exploration of hard attention, where some\ninformation is selectively ignored, in spite of the success of soft attention,\nwhere information is re-weighted and aggregated, but never filtered out. Here,\nwe introduce a new approach for hard attention and find it achieves very\ncompetitive performance on a recently-released visual question answering\ndatasets, equalling and in some cases surpassing similar soft attention\narchitectures while entirely ignoring some features. Even though the hard\nattention mechanism is thought to be non-differentiable, we found that the\nfeature magnitudes correlate with semantic relevance, and provide a useful\nsignal for our mechanism's attentional selection criterion. Because hard\nattention selects important features of the input information, it can also be\nmore efficient than analogous soft attention mechanisms. This is especially\nimportant for recent approaches that use non-local pairwise operations, whereby\ncomputational and memory costs are quadratic in the size of the set of\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 12:39:43 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Doersch", "Carl", ""], ["Santoro", "Adam", ""], ["Battaglia", "Peter", ""]]}, {"id": "1808.00313", "submitter": "Xinyu Huang", "authors": "Qichuan Geng and Xinyu Huang and Zhong Zhou and Ruigang Yang", "title": "A Network Structure to Explicitly Reduce Confusion Errors in Semantic\n  Segmentation", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confusing classes that are ubiquitous in real world often degrade performance\nfor many vision related applications like object detection, classification, and\nsegmentation. The confusion errors are not only caused by similar visual\npatterns but also amplified by various factors during the training of our\ndesigned models, such as reduced feature resolution in the encoding process or\nimbalanced data distributions. A large amount of deep learning based network\nstructures has been proposed in recent years to deal with these individual\nfactors and improve network performance. However, to our knowledge, no existing\nwork in semantic image segmentation is designed to tackle confusion errors\nexplicitly. In this paper, we present a novel and general network structure\nthat reduces confusion errors in more direct manner and apply the network for\nsemantic segmentation. There are two major contributions in our network\nstructure: 1) We ensemble subnets with heterogeneous output spaces based on the\ndiscriminative confusing groups. The training for each subnet can distinguish\nconfusing classes within the group without affecting unrelated classes outside\nthe group. 2) We propose an improved cross-entropy loss function that maximizes\nthe probability assigned to the correct class and penalizes the probabilities\nassigned to the confusing classes at the same time. Our network structure is a\ngeneral structure and can be easily adapted to any other networks to further\nreduce confusion errors. Without any changes in the feature encoder and\npost-processing steps, our experiments demonstrate consistent and significant\nimprovements on different baseline models on Cityscapes and PASCAL VOC datasets\n(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38).\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 13:37:59 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Geng", "Qichuan", ""], ["Huang", "Xinyu", ""], ["Zhou", "Zhong", ""], ["Yang", "Ruigang", ""]]}, {"id": "1808.00327", "submitter": "Xinge Zhu", "authors": "Xinge Zhu, Zhichao Yin, Jianping Shi, Hongsheng Li and Dahua Lin", "title": "Generative Adversarial Frontal View to Bird View Synthesis", "comments": "Accepted to 3DV 2018; Codes are available at\n  https://github.com/WERush/BridgeGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Environment perception is an important task with great practical value and\nbird view is an essential part for creating panoramas of surrounding\nenvironment. Due to the large gap and severe deformation between the frontal\nview and bird view, generating a bird view image from a single frontal view is\nchallenging. To tackle this problem, we propose the BridgeGAN, i.e., a novel\ngenerative model for bird view synthesis. First, an intermediate view, i.e.,\nhomography view, is introduced to bridge the large gap. Next, conditioned on\nthe three views (frontal view, homography view and bird view) in our task, a\nmulti-GAN based model is proposed to learn the challenging cross-view\ntranslation. Extensive experiments conducted on a synthetic dataset have\ndemonstrated that the images generated by our model are much better than those\ngenerated by existing methods, with more consistent global appearance and\nsharper details. Ablation studies and discussions show its reliability and\nrobustness in some challenging cases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 14:09:02 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 14:13:51 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 03:21:20 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhu", "Xinge", ""], ["Yin", "Zhichao", ""], ["Shi", "Jianping", ""], ["Li", "Hongsheng", ""], ["Lin", "Dahua", ""]]}, {"id": "1808.00361", "submitter": "Jonathan Connell", "authors": "Jonathan Connell, Benjamin Herta", "title": "Structured Differential Learning for Automatic Threshold Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": "IBM Research Report RC25144", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique that can automatically tune the parameters of a\nrule-based computer vision system comprised of thresholds, combinational logic,\nand time constants. This lets us retain the flexibility and perspicacity of a\nconventionally structured system while allowing us to perform approximate\ngradient descent using labeled data. While this is only a heuristic procedure,\nas far as we are aware there is no other efficient technique for tuning such\nsystems. We describe the components of the system and the associated supervised\nlearning mechanism. We also demonstrate the utility of the algorithm by\ncomparing its performance versus hand tuning for an automotive headlight\ncontroller. Despite having over 100 parameters, the method is able to\nprofitably adjust the system values given just the desired output for a number\nof videos.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:13:28 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Connell", "Jonathan", ""], ["Herta", "Benjamin", ""]]}, {"id": "1808.00362", "submitter": "Stephen Lombardi", "authors": "Stephen Lombardi, Jason Saragih, Tomas Simon, Yaser Sheikh", "title": "Deep Appearance Models for Face Rendering", "comments": "Accepted to SIGGRAPH 2018", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2018) 37, 4, Article 68", "doi": "10.1145/3197517.3201401", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep appearance model for rendering the human face. Inspired\nby Active Appearance Models, we develop a data-driven rendering pipeline that\nlearns a joint representation of facial geometry and appearance from a\nmultiview capture setup. Vertex positions and view-specific textures are\nmodeled using a deep variational autoencoder that captures complex nonlinear\neffects while producing a smooth and compact latent representation.\nView-specific texture enables the modeling of view-dependent effects such as\nspecularity. In addition, it can also correct for imperfect geometry stemming\nfrom biased or low resolution estimates. This is a significant departure from\nthe traditional graphics pipeline, which requires highly accurate geometry as\nwell as all elements of the shading model to achieve realism through\nphysically-inspired light transport. Acquiring such a high level of accuracy is\ndifficult in practice, especially for complex and intricate parts of the face,\nsuch as eyelashes and the oral cavity. These are handled naturally by our\napproach, which does not rely on precise estimates of geometry. Instead, the\nshading model accommodates deficiencies in geometry though the flexibility\nafforded by the neural network employed. At inference time, we condition the\ndecoding network on the viewpoint of the camera in order to generate the\nappropriate texture for rendering. The resulting system can be implemented\nsimply using existing rendering engines through dynamic textures with flat\nlighting. This representation, together with a novel unsupervised technique for\nmapping images to facial states, results in a system that is naturally suited\nto real-time interactive settings such as Virtual Reality (VR).\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:13:48 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lombardi", "Stephen", ""], ["Saragih", "Jason", ""], ["Simon", "Tomas", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1808.00391", "submitter": "Juan-Manuel Perez-Rua", "authors": "Juan-Manuel Perez-Rua, Moez Baccouche, Stephane Pateux", "title": "Efficient Progressive Neural Architecture Search", "comments": "Accepted for publication by the BMVA (BMVC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the difficult problem of finding an optimal neural\narchitecture design for a given image classification task. We propose a method\nthat aggregates two main results of the previous state-of-the-art in neural\narchitecture search. These are, appealing to the strong sampling efficiency of\na search scheme based on sequential model-based optimization (SMBO), and\nincreasing training efficiency by sharing weights among sampled architectures.\nSequential search has previously demonstrated its capabilities to find\nstate-of-the-art neural architectures for image classification. However, its\ncomputational cost remains high, even unreachable under modest computational\nsettings. Affording SMBO with weight-sharing alleviates this problem. On the\nother hand, progressive search with SMBO is inherently greedy, as it leverages\na learned surrogate function to predict the validation error of neural\narchitectures. This prediction is directly used to rank the sampled neural\narchitectures. We propose to attenuate the greediness of the original SMBO\nmethod by relaxing the role of the surrogate function so it predicts\narchitecture sampling probability instead. We demonstrate with experiments on\nthe CIFAR-10 dataset that our method, denominated Efficient progressive neural\narchitecture search (EPNAS), leads to increased search efficiency, while\nretaining competitiveness of found architectures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:56:08 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Perez-Rua", "Juan-Manuel", ""], ["Baccouche", "Moez", ""], ["Pateux", "Stephane", ""]]}, {"id": "1808.00435", "submitter": "Sheng Chen", "authors": "Sheng Chen, Jia Guo, Yang Liu, Xiang Gao, Zhen Han", "title": "Global Norm-Aware Pooling for Pose-Robust Face Recognition at Low False\n  Positive Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Global Norm-Aware Pooling (GNAP) block,\nwhich reweights local features in a convolutional neural network (CNN)\nadaptively according to their L2 norms and outputs a global feature vector with\na global average pooling layer. Our GNAP block is designed to give dynamic\nweights to local features in different spatial positions without losing spatial\nsymmetry. We use a GNAP block in a face feature embedding CNN to produce\ndiscriminative face feature vectors for pose-robust face recognition. The GNAP\nblock is of very cheap computational cost, but it is very powerful for\nfrontal-profile face recognition. Under the CFP frontal-profile protocol, the\nGNAP block can not only reduce EER dramatically but also boost TPR@FPR=0.1%\n(TPR i.e. True Positive Rate, FPR i.e. False Positive Rate) substantially. Our\nexperiments show that the GNAP block greatly promotes pose-robust face\nrecognition over the base model especially at low false positive rate.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:32:31 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Chen", "Sheng", ""], ["Guo", "Jia", ""], ["Liu", "Yang", ""], ["Gao", "Xiang", ""], ["Han", "Zhen", ""]]}, {"id": "1808.00447", "submitter": "Troy Chinen", "authors": "Troy Chinen, Johannes Ball\\'e, Chunhui Gu, Sung Jin Hwang, Sergey\n  Ioffe, Nick Johnston, Thomas Leung, David Minnen, Sean O'Malley, Charles\n  Rosenberg, George Toderici", "title": "Towards a Semantic Perceptual Image Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a full reference, perceptual image metric based on VGG-16, an\nartificial neural network trained on object classification. We fit the metric\nto a new database based on 140k unique images annotated with ground truth by\nhuman raters who received minimal instruction. The resulting metric shows\ncompetitive performance on TID 2013, a database widely used to assess image\nquality assessments methods. More interestingly, it shows strong responses to\nobjects potentially carrying semantic relevance such as faces and text, which\nwe demonstrate using a visualization technique and ablation experiments. In\neffect, the metric appears to model a higher influence of semantic context on\njudgments, which we observe particularly in untrained raters. As the vast\nmajority of users of image processing systems are unfamiliar with Image Quality\nAssessment (IQA) tasks, these findings may have significant impact on\nreal-world applications of perceptual metrics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:58:23 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Chinen", "Troy", ""], ["Ball\u00e9", "Johannes", ""], ["Gu", "Chunhui", ""], ["Hwang", "Sung Jin", ""], ["Ioffe", "Sergey", ""], ["Johnston", "Nick", ""], ["Leung", "Thomas", ""], ["Minnen", "David", ""], ["O'Malley", "Sean", ""], ["Rosenberg", "Charles", ""], ["Toderici", "George", ""]]}, {"id": "1808.00449", "submitter": "Wei-Sheng Lai", "authors": "Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer,\n  Ming-Hsuan Yang", "title": "Learning Blind Video Temporal Consistency", "comments": "This work is accepted in ECCV 2018. Project website:\n  http://vllab.ucmerced.edu/wlai24/video_consistency/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying image processing algorithms independently to each frame of a video\noften leads to undesired inconsistent results over time. Developing temporally\nconsistent video-based extensions, however, requires domain knowledge for\nindividual tasks and is unable to generalize to other applications. In this\npaper, we present an efficient end-to-end approach based on deep recurrent\nnetwork for enforcing temporal consistency in a video. Our method takes the\noriginal unprocessed and per-frame processed videos as inputs to produce a\ntemporally consistent video. Consequently, our approach is agnostic to specific\nimage processing algorithms applied on the original video. We train the\nproposed network by minimizing both short-term and long-term temporal losses as\nwell as the perceptual loss to strike a balance between temporal stability and\nperceptual similarity with the processed frames. At test time, our model does\nnot require computing optical flow and thus achieves real-time speed even for\nhigh-resolution videos. We show that our single model can handle multiple and\nunseen tasks, including but not limited to artistic style transfer,\nenhancement, colorization, image-to-image translation and intrinsic image\ndecomposition. Extensive objective evaluation and subject study demonstrate\nthat the proposed approach performs favorably against the state-of-the-art\nmethods on various types of videos.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:59:15 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lai", "Wei-Sheng", ""], ["Huang", "Jia-Bin", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""], ["Yumer", "Ersin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1808.00457", "submitter": "Yao Sun", "authors": "Yao Sun, Yang Deng, Yue Xu, Shuo Zhang, Mingwang Zhu, Kehong Yuan", "title": "A Multi-channel Network with Image Retrieval for Accurate Brain Tissue\n  Segmentation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is widely used in the pathological and\nfunctional studies of the brain, such as epilepsy, tumor diagnosis, etc.\nAutomated accurate brain tissue segmentation like cerebro-spinal fluid (CSF),\ngray matter (GM), white matter (WM) is the basis of these studies and many\nresearchers are seeking it to the best. Based on the truth that multi-channel\nsegmentation network with its own ground truth achieves up to average dice\nratio 0.98, we propose a novel method that we add a fourth channel with the\nground truth of the most similar image's obtained by CBIR from the database.\nThe results show that the method improves the segmentation performance, as\nmeasured by average dice ratio, by approximately 0.01 in the MRBrainS18\ndatabase. In addition, our method is concise and robust, which can be used to\nany network architecture that needs not be modified a lot.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 16:27:00 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 13:26:01 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Sun", "Yao", ""], ["Deng", "Yang", ""], ["Xu", "Yue", ""], ["Zhang", "Shuo", ""], ["Zhu", "Mingwang", ""], ["Yuan", "Kehong", ""]]}, {"id": "1808.00495", "submitter": "Hugues Thomas", "authors": "Hugues Thomas, Jean-Emmanuel Deschaud, Beatriz Marcotegui,\n  Fran\\c{c}ois Goulette, Yann Le Gall", "title": "Semantic Classification of 3D Point Clouds with Multiscale Spherical\n  Neighborhoods", "comments": "3DV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new definition of multiscale neighborhoods in 3D\npoint clouds. This definition, based on spherical neighborhoods and\nproportional subsampling, allows the computation of features with a consistent\ngeometrical meaning, which is not the case when using k-nearest neighbors. With\nan appropriate learning strategy, the proposed features can be used in a random\nforest to classify 3D points. In this semantic classification task, we show\nthat our multiscale features outperform state-of-the-art features using the\nsame experimental conditions. Furthermore, their classification power competes\nwith more elaborate classification approaches including Deep Learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 18:24:02 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Thomas", "Hugues", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Marcotegui", "Beatriz", ""], ["Goulette", "Fran\u00e7ois", ""], ["Gall", "Yann Le", ""]]}, {"id": "1808.00505", "submitter": "Martin Storath", "authors": "Martin Storath, Andreas Weinmann", "title": "Wavelet Sparse Regularization for Manifold-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the sparse regularization of manifold-valued data\nwith respect to an interpolatory wavelet/multiscale transform. We propose and\nstudy variational models for this task and provide results on their\nwell-posedness. We present algorithms for a numerical realization of these\nmodels in the manifold setup. Further, we provide experimental results to show\nthe potential of the proposed schemes for applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 18:47:36 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Storath", "Martin", ""], ["Weinmann", "Andreas", ""]]}, {"id": "1808.00558", "submitter": "David Schubert", "authors": "David Schubert, Nikolaus Demmel, Vladyslav Usenko, J\\\"org St\\\"uckler,\n  Daniel Cremers", "title": "Direct Sparse Odometry with Rolling Shutter", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01237-3_42", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neglecting the effects of rolling-shutter cameras for visual odometry (VO)\nseverely degrades accuracy and robustness. In this paper, we propose a novel\ndirect monocular VO method that incorporates a rolling-shutter model. Our\napproach extends direct sparse odometry which performs direct bundle adjustment\nof a set of recent keyframe poses and the depths of a sparse set of image\npoints. We estimate the velocity at each keyframe and impose a\nconstant-velocity prior for the optimization. In this way, we obtain a near\nreal-time, accurate direct VO method. Our approach achieves improved results on\nchallenging rolling-shutter sequences over state-of-the-art global-shutter VO.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 20:48:02 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Schubert", "David", ""], ["Demmel", "Nikolaus", ""], ["Usenko", "Vladyslav", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1808.00588", "submitter": "Zeba Khanam", "authors": "Jose Carlos Villarreal Guerra, Zeba Khanam, Shoaib Ehsan, Rustam\n  Stolkin, Klaus McDonald-Maier", "title": "Weather Classification: A new multi-class dataset, data augmentation\n  approach and comprehensive evaluations of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weather conditions often disrupt the proper functioning of transportation\nsystems. Present systems either deploy an array of sensors or use an in-vehicle\ncamera to predict weather conditions. These solutions have resulted in\nincremental cost and limited scope. To ensure smooth operation of all\ntransportation services in all-weather conditions, a reliable detection system\nis necessary to classify weather in wild. The challenges involved in solving\nthis problem is that weather conditions are diverse in nature and there is an\nabsence of discriminate features among various weather conditions. The existing\nworks to solve this problem have been scene specific and have targeted\nclassification of two categories of weather. In this paper, we have created a\nnew open source dataset consisting of images depicting three classes of weather\ni.e rain, snow and fog called RFS Dataset. A novel algorithm has also been\nproposed which has used super pixel delimiting masks as a form of data\naugmentation, leading to reasonable results with respect to ten Convolutional\nNeural Network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 22:33:23 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Guerra", "Jose Carlos Villarreal", ""], ["Khanam", "Zeba", ""], ["Ehsan", "Shoaib", ""], ["Stolkin", "Rustam", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1808.00601", "submitter": "Francesco Lomio", "authors": "Francesco Lomio, Ricardo Farinha, Mauri Laasonen, Heikki Huttunen", "title": "Classification of Building Information Model (BIM) Structures with Deep\n  Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": "10.1109/EUVIP.2018.8611701", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study an application of machine learning to the construction\nindustry and we use classical and modern machine learning methods to categorize\nimages of building designs into three classes: Apartment building, Industrial\nbuilding or Other. No real images are used, but only images extracted from\nBuilding Information Model (BIM) software, as these are used by the\nconstruction industry to store building designs. For this task, we compared\nfour different methods: the first is based on classical machine learning, where\nHistogram of Oriented Gradients (HOG) was used for feature extraction and a\nSupport Vector Machine (SVM) for classification; the other three methods are\nbased on deep learning, covering common pre-trained networks as well as ones\ndesigned from scratch. To validate the accuracy of the models, a database of\n240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and\nabove 89% for the neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 23:56:28 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Lomio", "Francesco", ""], ["Farinha", "Ricardo", ""], ["Laasonen", "Mauri", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1808.00605", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Jiangxin Dong, Yang Liu, Jiawei Zhang, Jimmy Ren, Jinhui\n  Tang, Yu-Wing Tai and Ming-Hsuan Yang", "title": "Physics-Based Generative Adversarial Models for Image Restoration and\n  Beyond", "comments": "IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to directly solve numerous image restoration problems\n(e.g., image deblurring, image dehazing, image deraining, etc.). These problems\nare highly ill-posed, and the common assumptions for existing methods are\nusually based on heuristic image priors. In this paper, we find that these\nproblems can be solved by generative models with adversarial learning. However,\nthe basic formulation of generative adversarial networks (GANs) does not\ngenerate realistic images, and some structures of the estimated images are\nusually not preserved well. Motivated by an interesting observation that the\nestimated results should be consistent with the observed inputs under the\nphysics models, we propose a physics model constrained learning algorithm so\nthat it can guide the estimation of the specific task in the conventional GAN\nframework. The proposed algorithm is trained in an end-to-end fashion and can\nbe applied to a variety of image restoration and related low-level vision\nproblems. Extensive experiments demonstrate that our method performs favorably\nagainst the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 00:12:33 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 15:40:03 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Pan", "Jinshan", ""], ["Dong", "Jiangxin", ""], ["Liu", "Yang", ""], ["Zhang", "Jiawei", ""], ["Ren", "Jimmy", ""], ["Tang", "Jinhui", ""], ["Tai", "Yu-Wing", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1808.00641", "submitter": "Swati Rallapalli", "authors": "ShreeRanjani SrirangamSridharan, Oytun Ulutan, Shehzad Noor Taus\n  Priyo, Swati Rallapalli, Mudhakar Srivatsa", "title": "Object Localization and Size Estimation from RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth sensing cameras (e.g., Kinect sensor, Tango phone) can acquire color\nand depth images that are registered to a common viewpoint. This opens the\npossibility of developing algorithms that exploit the advantages of both\nsensing modalities. Traditionally, cues from color images have been used for\nobject localization (e.g., YOLO). However, the addition of a depth image can be\nfurther used to segment images that might otherwise have identical color\ninformation. Further, the depth image can be used for object size\n(height/width) estimation (in real-world measurements units, such as meters) as\nopposed to image based segmentation that would only support drawing bounding\nboxes around objects of interest. In this paper, we first collect color camera\ninformation along with depth information using a custom Android application on\nTango Phab2 phone. Second, we perform timing and spatial alignment between the\ntwo data sources. Finally, we evaluate several ways of measuring the height of\nthe object of interest within the captured images under a variety of settings.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 02:35:02 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["SrirangamSridharan", "ShreeRanjani", ""], ["Ulutan", "Oytun", ""], ["Priyo", "Shehzad Noor Taus", ""], ["Rallapalli", "Swati", ""], ["Srivatsa", "Mudhakar", ""]]}, {"id": "1808.00661", "submitter": "Ke Gong", "authors": "Qixian Zhou, Xiaodan Liang, Ke Gong, Liang Lin", "title": "Adaptive Temporal Encoding Network for Video Instance-level Human\n  Parsing", "comments": "To appear in ACM MM 2018. Code link:\n  https://github.com/HCPLab-SYSU/ATEN. Dataset link: http://sysu-hcp.net/lip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond the existing single-person and multiple-person human parsing tasks in\nstatic images, this paper makes the first attempt to investigate a more\nrealistic video instance-level human parsing that simultaneously segments out\neach person instance and parses each instance into more fine-grained parts\n(e.g., head, leg, dress). We introduce a novel Adaptive Temporal Encoding\nNetwork (ATEN) that alternatively performs temporal encoding among key frames\nand flow-guided feature propagation from other consecutive frames between two\nkey frames. Specifically, ATEN first incorporates a Parsing-RCNN to produce the\ninstance-level parsing result for each key frame, which integrates both the\nglobal human parsing and instance-level human segmentation into a unified\nmodel. To balance between accuracy and efficiency, the flow-guided feature\npropagation is used to directly parse consecutive frames according to their\nidentified temporal consistency with key frames. On the other hand, ATEN\nleverages the convolution gated recurrent units (convGRU) to exploit temporal\nchanges over a series of key frames, which are further used to facilitate the\nframe-level instance-level parsing. By alternatively performing direct feature\npropagation between consistent frames and temporal encoding network among key\nframes, our ATEN achieves a good balance between frame-level accuracy and time\nefficiency, which is a common crucial problem in video object segmentation\nresearch. To demonstrate the superiority of our ATEN, extensive experiments are\nconducted on the most popular video segmentation benchmark (DAVIS) and a newly\ncollected Video Instance-level Parsing (VIP) dataset, which is the first video\ninstance-level human parsing dataset comprised of 404 sequences and over 20k\nframes with instance-level and pixel-wise annotations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 04:24:36 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 09:46:46 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Zhou", "Qixian", ""], ["Liang", "Xiaodan", ""], ["Gong", "Ke", ""], ["Lin", "Liang", ""]]}, {"id": "1808.00671", "submitter": "Wentao Yuan", "authors": "Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert", "title": "PCN: Point Completion Network", "comments": "3DV 2018 oral. Honorable mention for Best Paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape completion, the problem of estimating the complete geometry of objects\nfrom partial observations, lies at the core of many vision and robotics\napplications. In this work, we propose Point Completion Network (PCN), a novel\nlearning-based approach for shape completion. Unlike existing shape completion\nmethods, PCN directly operates on raw point clouds without any structural\nassumption (e.g. symmetry) or annotation (e.g. semantic class) about the\nunderlying shape. It features a decoder design that enables the generation of\nfine-grained completions while maintaining a small number of parameters. Our\nexperiments show that PCN produces dense, complete point clouds with realistic\nstructures in the missing regions on inputs with various levels of\nincompleteness and noise, including cars from LiDAR scans in the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 05:20:21 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 04:57:39 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 18:56:43 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Yuan", "Wentao", ""], ["Khot", "Tejas", ""], ["Held", "David", ""], ["Mertz", "Christoph", ""], ["Hebert", "Martial", ""]]}, {"id": "1808.00677", "submitter": "Yuting Gao", "authors": "Yuting Gao, Zheng Huang, Yuchen Dai, Cheng Xu, Kai Chen, Jie Tuo", "title": "Double Supervised Network with Attention Mechanism for Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Double Supervised Network with Attention Mechanism\n(DSAN), a novel end-to-end trainable framework for scene text recognition. It\nincorporates one text attention module during feature extraction which enforces\nthe model to focus on text regions and the whole framework is supervised by two\nbranches. One supervision branch comes from context-level modelling and another\ncomes from one extra supervision enhancement branch which aims at tackling\ninexplicit semantic information at character level. These two supervisions can\nbenefit each other and yield better performance. The proposed approach can\nrecognize text in arbitrary length and does not need any predefined lexicon.\nOur method outperforms the current state-of-the-art methods on three text\nrecognition benchmarks: IIIT5K, ICDAR2013 and SVT reaching accuracy 88.6%,\n92.3% and 84.1% respectively which suggests the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 06:01:52 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 10:36:15 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 13:05:11 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gao", "Yuting", ""], ["Huang", "Zheng", ""], ["Dai", "Yuchen", ""], ["Xu", "Cheng", ""], ["Chen", "Kai", ""], ["Tuo", "Jie", ""]]}, {"id": "1808.00692", "submitter": "Tong Qin", "authors": "Tong Qin and Shaojie Shen", "title": "Online Temporal Calibration for Monocular Visual-Inertial Systems", "comments": "IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate state estimation is a fundamental module for various intelligent\napplications, such as robot navigation, autonomous driving, virtual and\naugmented reality. Visual and inertial fusion is a popular technology for 6-DOF\nstate estimation in recent years. Time instants at which different sensors'\nmeasurements are recorded are of crucial importance to the system's robustness\nand accuracy. In practice, timestamps of each sensor typically suffer from\ntriggering and transmission delays, leading to temporal misalignment (time\noffsets) among different sensors. Such temporal offset dramatically influences\nthe performance of sensor fusion. To this end, we propose an online approach\nfor calibrating temporal offset between visual and inertial measurements. Our\napproach achieves temporal offset calibration by jointly optimizing time\noffset, camera and IMU states, as well as feature locations in a SLAM system.\nFurthermore, the approach is a general model, which can be easily employed in\nseveral feature-based optimization frameworks. Simulation and experimental\nresults demonstrate the high accuracy of our calibration approach even compared\nwith other state-of-art offline tools. The VIO comparison against other methods\nproves that the online temporal calibration significantly benefits\nvisual-inertial systems. The source code of temporal calibration is integrated\ninto our public project, VINS-Mono.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:16:25 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Qin", "Tong", ""], ["Shen", "Shaojie", ""]]}, {"id": "1808.00703", "submitter": "Hammad Haleem", "authors": "Hammad Haleem, Yong Wang, Abishek Puri, Sahil Wadhwa and Huamin Qu", "title": "Evaluating the Readability of Force Directed Graph Layouts: A Deep\n  Learning Approach", "comments": "This work has been accepted at IEEE CG&A", "journal-ref": null, "doi": "10.1109/MCG.2018.2881501", "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing graph layout algorithms are usually not able to optimize all the\naesthetic properties desired in a graph layout. To evaluate how well the\ndesired visual features are reflected in a graph layout, many readability\nmetrics have been proposed in the past decades. However, the calculation of\nthese readability metrics often requires access to the node and edge\ncoordinates and is usually computationally inefficient, especially for dense\ngraphs. Importantly, when the node and edge coordinates are not accessible, it\nbecomes impossible to evaluate the graph layouts quantitatively. In this paper,\nwe present a novel deep learning-based approach to evaluate the readability of\ngraph layouts by directly using graph images. A convolutional neural network\narchitecture is proposed and trained on a benchmark dataset of graph images,\nwhich is composed of synthetically-generated graphs and graphs created by\nsampling from real large networks. Multiple representative readability metrics\n(including edge crossing, node spread, and group overlap) are considered in the\nproposed approach. We quantitatively compare our approach to traditional\nmethods and qualitatively evaluate our approach using a case study and\nvisualizing convolutional layers. This work is a first step towards using deep\nlearning based methods to evaluate images from the visualization field\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:57:59 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 23:20:45 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Haleem", "Hammad", ""], ["Wang", "Yong", ""], ["Puri", "Abishek", ""], ["Wadhwa", "Sahil", ""], ["Qu", "Huamin", ""]]}, {"id": "1808.00736", "submitter": "Sindi Shkodrani", "authors": "Sindi Shkodrani, Michael Hofmann, Efstratios Gavves", "title": "Dynamic Adaptation on Non-Stationary Visual Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to learn models on a supervised source domain that\nperform well on an unsupervised target. Prior work has examined domain\nadaptation in the context of stationary domain shifts, i.e. static data sets.\nHowever, with large-scale or dynamic data sources, data from a defined domain\nis not usually available all at once. For instance, in a streaming data\nscenario, dataset statistics effectively become a function of time. We\nintroduce a framework for adaptation over non-stationary distribution shifts\napplicable to large-scale and streaming data scenarios. The model is adapted\nsequentially over incoming unsupervised streaming data batches. This enables\nimprovements over several batches without the need for any additionally\nannotated data. To demonstrate the effectiveness of our proposed framework, we\nmodify associative domain adaptation to work well on source and target data\nbatches with unequal class distributions. We apply our method to several\nadaptation benchmark datasets for classification and show improved classifier\naccuracy not only for the currently adapted batch, but also when applied on\nfuture stream batches. Furthermore, we show the applicability of our\nassociative learning modifications to semantic segmentation, where we achieve\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:49:32 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Shkodrani", "Sindi", ""], ["Hofmann", "Michael", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1808.00739", "submitter": "Minyoung Chung", "authors": "Minyoung Chung, Jingyu Lee, Minkyung Lee, Jeongjin Lee, and Yeong-Gil\n  Shin", "title": "Deeply Self-Supervised Contour Embedded Neural Network Applied to Liver\n  Segmentation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.cmpb.2020.105447", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Herein, a neural network-based liver segmentation algorithm is\nproposed, and its performance was evaluated using abdominal computed tomography\n(CT) images. Methods: A fully convolutional network was developed to overcome\nthe volumetric image segmentation problem. To guide a neural network to\naccurately delineate a target liver object, the network was deeply supervised\nby applying the adaptive self-supervision scheme to derive the essential\ncontour, which acted as a complement with the global shape. The discriminative\ncontour, shape, and deep features were internally merged for the segmentation\nresults. Results and Conclusion: 160 abdominal CT images were used for training\nand validation. The quantitative evaluation of the proposed network was\nperformed through an eight-fold cross-validation. The result showed that the\nmethod, which uses the contour feature, segmented the liver more accurately\nthan the state-of-the-art with a 2.13% improvement in the dice score.\nSignificance: In this study, a new framework was introduced to guide a neural\nnetwork and learn complementary contour features. The proposed neural network\ndemonstrates that the guided contour features can significantly improve the\nperformance of the segmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:53:11 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 23:48:09 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 02:13:46 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 06:28:03 GMT"}, {"version": "v5", "created": "Thu, 17 Oct 2019 10:30:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chung", "Minyoung", ""], ["Lee", "Jingyu", ""], ["Lee", "Minkyung", ""], ["Lee", "Jeongjin", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "1808.00758", "submitter": "Bo Yang", "authors": "Bo Yang, Sen Wang, Andrew Markham, Niki Trigoni", "title": "Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D\n  Reconstruction", "comments": "IJCV 2019. Code and data are available at\n  https://github.com/Yang7879/AttSets", "journal-ref": null, "doi": "10.1007/s11263-019-01217-w", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering an underlying 3D shape from a set of\nimages. Existing learning based approaches usually resort to recurrent neural\nnets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to\nfuse multiple deep features encoded from input images. However, GRU based\napproaches are unable to consistently estimate 3D shapes given different\npermutations of the same set of input images as the recurrent unit is\npermutation variant. It is also unlikely to refine the 3D shape given more\nimages due to the long-term memory loss of GRU. Commonly used pooling\napproaches are limited to capturing partial information, e.g., max/mean values,\nignoring other valuable features. In this paper, we present a new feed-forward\nneural module, named AttSets, together with a dedicated training algorithm,\nnamed FASet, to attentively aggregate an arbitrarily sized deep feature set for\nmulti-view 3D reconstruction. The AttSets module is permutation invariant,\ncomputationally efficient and flexible to implement, while the FASet algorithm\nenables the AttSets based network to be remarkably robust and generalize to an\narbitrary number of input images. We thoroughly evaluate FASet and the\nproperties of AttSets on multiple large public datasets. Extensive experiments\nshow that AttSets together with FASet algorithm significantly outperforms\nexisting aggregation approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 11:09:13 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 06:32:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yang", "Bo", ""], ["Wang", "Sen", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1808.00769", "submitter": "Maximilian Jaritz", "authors": "Maximilian Jaritz, Raoul de Charette, Emilie Wirbel, Xavier Perrotton,\n  Fawzi Nashashibi", "title": "Sparse and Dense Data with CNNs: Depth Completion and Semantic\n  Segmentation", "comments": "3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are designed for dense data, but vision data is\noften sparse (stereo depth, point clouds, pen stroke, etc.). We present a\nmethod to handle sparse depth data with optional dense RGB, and accomplish\ndepth completion and semantic segmentation changing only the last layer. Our\nproposal efficiently learns sparse features without the need of an additional\nvalidity mask. We show how to ensure network robustness to varying input\nsparsities. Our method even works with densities as low as 0.8% (8 layer\nlidar), and outperforms all published state-of-the-art on the Kitti depth\ncompletion benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 11:59:45 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:13:45 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Jaritz", "Maximilian", ""], ["de Charette", "Raoul", ""], ["Wirbel", "Emilie", ""], ["Perrotton", "Xavier", ""], ["Nashashibi", "Fawzi", ""]]}, {"id": "1808.00783", "submitter": "Peter M. Roth", "authors": "Mina Basirat and Peter M. Roth", "title": "The Quest for the Golden Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 12:44:09 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Basirat", "Mina", ""], ["Roth", "Peter M.", ""]]}, {"id": "1808.00793", "submitter": "Nicolas Toussaint", "authors": "Nicolas Toussaint, Bishesh Khanal, Matthew Sinclair, Alberto Gomez,\n  Emily Skelton, Jacqueline Matthew, and Julia A. Schnabel", "title": "Weakly Supervised Localisation for Fetal Ultrasound Images", "comments": "4th Workshop on Deep Learning for Medical Image Analysis, MICCAI\n  2018, Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of detecting and localising fetal anatomical\nregions in 2D ultrasound images, where only image-level labels are present at\ntraining, i.e. without any localisation or segmentation information. We examine\nthe use of convolutional neural network architectures coupled with soft\nproposal layers. The resulting network simultaneously performs anatomical\nregion detection (classification) and localisation tasks. We generate a\nproposal map describing the attention of the network for a particular class.\nThe network is trained on 85,500 2D fetal Ultrasound images and their\nassociated labels. Labels correspond to six anatomical regions: head, spine,\nthorax, abdomen, limbs, and placenta. Detection achieves an average accuracy of\n90\\% on individual regions, and show that the proposal maps correlate well with\nrelevant anatomical structures. This work presents itself as a powerful and\nessential step towards subsequent tasks such as fetal position and pose\nestimation, organ-specific segmentation, or image-guided navigation. Code and\nadditional material is available at https://ntoussaint.github.io/fetalnav\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 13:02:37 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Toussaint", "Nicolas", ""], ["Khanal", "Bishesh", ""], ["Sinclair", "Matthew", ""], ["Gomez", "Alberto", ""], ["Skelton", "Emily", ""], ["Matthew", "Jacqueline", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1808.00845", "submitter": "Jiaxin Cai", "authors": "Jiaxin Cai, Xin Tang", "title": "RGB Video Based Tennis Action Recognition Using a Deep Historical Long\n  Short-Term Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has attracted increasing attention from RGB input in\ncomputer vision partially due to potential applications on somatic simulation\nand statistics of sport such as virtual tennis game and tennis techniques and\ntactics analysis by video. Recently, deep learning based methods have achieved\npromising performance for action recognition. In this paper, we propose\nweighted Long Short-Term Memory adopted with convolutional neural network\nrepresentations for three dimensional tennis shots recognition. First, the\nlocal two-dimensional convolutional neural network spatial representations are\nextracted from each video frame individually using a pre-trained Inception\nnetwork. Then, a weighted Long Short-Term Memory decoder is introduced to take\nthe output state at time t and the historical embedding feature at time t-1 to\ngenerate feature vector using a score weighting scheme. Finally, we use the\nadopted CNN and weighted LSTM to map the original visual features into a vector\nspace to generate the spatial-temporal semantical description of visual\nsequences and classify the action video content. Experiments on the benchmark\ndemonstrate that our method using only simple raw RGB video can achieve better\nperformance than the state-of-the-art baselines for tennis shot recognition.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:58:51 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 14:28:06 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Cai", "Jiaxin", ""], ["Tang", "Xin", ""]]}, {"id": "1808.00856", "submitter": "Emanuel Aldea", "authors": "Nicola Pellican\\`o and Emanuel Aldea and Sylvie Le H\\'egarat-Mascle", "title": "Geometry-Based Multiple Camera Head Detection in Dense Crowds", "comments": "Proceedings of the 28th British Machine Vision Conference (BMVC) -\n  5th Activity Monitoring by Multiple Distributed Sensing Workshop, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of head detection in crowded environments.\nOur detection is based entirely on the geometric consistency across cameras\nwith overlapping fields of view, and no additional learning process is\nrequired. We propose a fully unsupervised method for inferring scene and camera\ngeometry, in contrast to existing algorithms which require specific calibration\nprocedures. Moreover, we avoid relying on the presence of body parts other than\nheads or on background subtraction, which have limited effectiveness under\nheavy clutter. We cast the head detection problem as a stereo MRF-based\noptimization of a dense pedestrian height map, and we introduce a constraint\nwhich aligns the height gradient according to the vertical vanishing point\ndirection. We validate the method in an outdoor setting with varying pedestrian\ndensity levels. With only three views, our approach is able to detect\nsimultaneously tens of heavily occluded pedestrians across a large, homogeneous\narea.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 15:23:47 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Pellican\u00f2", "Nicola", ""], ["Aldea", "Emanuel", ""], ["H\u00e9garat-Mascle", "Sylvie Le", ""]]}, {"id": "1808.00878", "submitter": "Hazrat Ali", "authors": "Hazrat Ali, Adnan Ali Awan, Sanaullah Khan, Omer Shafique, Atiq ur\n  Rahman, Shahid Khan", "title": "Supervised classification for object identification in urban areas using\n  satellite imagery", "comments": "2018 International Conference on Computing, Mathematics and\n  Engineering Technologies (iCoMET)", "journal-ref": "H. Ali et al., 2018 International Conference on Computing,\n  Mathematics and Engineering Technologies (iCoMET), Sukkur, 2018, pp. 1-4", "doi": "10.1109/ICOMET.2018.8346383", "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a useful method to achieve classification in satellite\nimagery. The approach is based on pixel level study employing various features\nsuch as correlation, homogeneity, energy and contrast. In this study gray-scale\nimages are used for training the classification model. For supervised\nclassification, two classification techniques are employed namely the Support\nVector Machine (SVM) and the Naive Bayes. With textural features used for\ngray-scale images, Naive Bayes performs better with an overall accuracy of 76%\ncompared to 68% achieved by SVM. The computational time is evaluated while\nperforming the experiment with two different window sizes i.e., 50x50 and\n70x70. The required computational time on a single image is found to be 27\nseconds for a window size of 70x70 and 45 seconds for a window size of 50x50.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:00:32 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Ali", "Hazrat", ""], ["Awan", "Adnan Ali", ""], ["Khan", "Sanaullah", ""], ["Shafique", "Omer", ""], ["Rahman", "Atiq ur", ""], ["Khan", "Shahid", ""]]}, {"id": "1808.00897", "submitter": "Changqian Yu", "authors": "Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang", "title": "BiSeNet: Bilateral Segmentation Network for Real-time Semantic\n  Segmentation", "comments": "Accepted to ECCV 2018. 17 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation requires both rich spatial information and sizeable\nreceptive field. However, modern approaches usually compromise spatial\nresolution to achieve real-time inference speed, which leads to poor\nperformance. In this paper, we address this dilemma with a novel Bilateral\nSegmentation Network (BiSeNet). We first design a Spatial Path with a small\nstride to preserve the spatial information and generate high-resolution\nfeatures. Meanwhile, a Context Path with a fast downsampling strategy is\nemployed to obtain sufficient receptive field. On top of the two paths, we\nintroduce a new Feature Fusion Module to combine features efficiently. The\nproposed architecture makes a right balance between the speed and segmentation\nperformance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a\n2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with\nspeed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster\nthan the existing methods with comparable performance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:34:01 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Yu", "Changqian", ""], ["Wang", "Jingbo", ""], ["Peng", "Chao", ""], ["Gao", "Changxin", ""], ["Yu", "Gang", ""], ["Sang", "Nong", ""]]}, {"id": "1808.00928", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet", "title": "Learning Actionable Representations from Visual Observations", "comments": "This work is accepted in IROS 2018. Project website:\n  https://sites.google.com/view/actionablerepresentations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore a new approach for robots to teach themselves about\nthe world simply by observing it. In particular we investigate the\neffectiveness of learning task-agnostic representations for continuous control\ntasks. We extend Time-Contrastive Networks (TCN) that learn from visual\nobservations by embedding multiple frames jointly in the embedding space as\nopposed to a single frame. We show that by doing so, we are now able to encode\nboth position and velocity attributes significantly more accurately. We test\nthe usefulness of this self-supervised approach in a reinforcement learning\nsetting. We show that the representations learned by agents observing\nthemselves take random actions, or other agents perform tasks successfully, can\nenable the learning of continuous control policies using algorithms like\nProximal Policy Optimization (PPO) using only the learned embeddings as input.\nWe also demonstrate significant improvements on the real-world Pouring dataset\nwith a relative error reduction of 39.4% for motion attributes and 11.1% for\nstatic attributes compared to the single-frame baseline. Video results are\navailable at https://sites.google.com/view/actionablerepresentations .\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:24:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 16:03:59 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2019 23:09:02 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Tompson", "Jonathan", ""], ["Lynch", "Corey", ""], ["Sermanet", "Pierre", ""]]}, {"id": "1808.00948", "submitter": "Hsin-Ying Lee", "authors": "Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, and\n  Ming-Hsuan Yang", "title": "Diverse Image-to-Image Translation via Disentangled Representations", "comments": "ECCV 2018 (Oral). Project page: http://vllab.ucmerced.edu/hylee/DRIT/\n  Code: https://github.com/HsinYingLee/DRIT/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation aims to learn the mapping between two visual\ndomains. There are two main challenges for many applications: 1) the lack of\naligned training pairs and 2) multiple possible outputs from a single input\nimage. In this work, we present an approach based on disentangled\nrepresentation for producing diverse outputs without paired training images. To\nachieve diversity, we propose to embed images onto two spaces: a\ndomain-invariant content space capturing shared information across domains and\na domain-specific attribute space. Our model takes the encoded content features\nextracted from a given input and the attribute vectors sampled from the\nattribute space to produce diverse outputs at test time. To handle unpaired\ntraining data, we introduce a novel cross-cycle consistency loss based on\ndisentangled representations. Qualitative results show that our model can\ngenerate diverse and realistic images on a wide range of tasks without paired\ntraining data. For quantitative comparisons, we measure realism with user study\nand diversity with a perceptual distance metric. We apply the proposed model to\ndomain adaptation and show competitive performance when compared to the\nstate-of-the-art on the MNIST-M and the LineMod datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 17:54:27 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Lee", "Hsin-Ying", ""], ["Tseng", "Hung-Yu", ""], ["Huang", "Jia-Bin", ""], ["Singh", "Maneesh Kumar", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1808.00956", "submitter": "Hiroyuki Kobayashi", "authors": "Osamu Watanabe, Hiroyuki Kobayashi and Hitoshi Kiya", "title": "Two-Layer Lossless HDR Coding using Histogram Packing Technique with\n  Backward Compatibility to JPEG", "comments": "To appear in IEICE Trans. Fundamentals, vol.E101-A, no.11, November\n  2018. arXiv admin note: substantial text overlap with arXiv:1806.10746", "journal-ref": null, "doi": "10.1587/transfun.E101.A.1823", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient two-layer coding method using the histogram packing technique\nwith the backward compatibility to the legacy JPEG is proposed in this paper.\nThe JPEG XT, which is the international standard to compress HDR images, adopts\ntwo-layer coding scheme for backward compatibility to the legacy JPEG. However,\nthis two-layer coding structure does not give better lossless performance than\nthe other existing methods for HDR image compression with single-layer\nstructure. Moreover, the lossless compression of the JPEG XT has a problem on\ndetermination of the coding parameters; The lossless performance is affected by\nthe input images and/or the parameter values. That is, finding appropriate\ncombination of the values is necessary to achieve good lossless performance. It\nis firstly pointed out that the histogram packing technique considering the\nhistogram sparseness of HDR images is able to improve the performance of\nlossless compression. Then, a novel two-layer coding with the histogram packing\ntechnique and an additional lossless encoder is proposed. The experimental\nresults demonstrate that not only the proposed method has a better lossless\ncompression performance than that of the JPEG XT, but also there is no need to\ndetermine image-dependent parameter values for good compression performance\nwithout losing the backward compatibility to the well known legacy JPEG\nstandard.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 03:02:48 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Watanabe", "Osamu", ""], ["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1808.00995", "submitter": "Connor Greenwell", "authors": "Connor Greenwell, Scott Workman, Nathan Jacobs", "title": "What Goes Where: Predicting Object Distributions from Above", "comments": "4 pages, 5 figures, IGARSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a cross-view learning approach, in which images\ncaptured from a ground-level view are used as weakly supervised annotations for\ninterpreting overhead imagery. The outcome is a convolutional neural network\nfor overhead imagery that is capable of predicting the type and count of\nobjects that are likely to be seen from a ground-level perspective. We\ndemonstrate our approach on a large dataset of geotagged ground-level and\noverhead imagery and find that our network captures semantically meaningful\nfeatures, despite being trained without manual annotations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 19:20:30 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Greenwell", "Connor", ""], ["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1808.01026", "submitter": "Sobhan Soleymani", "authors": "Sobhan Soleymani, Ali Dabouei, Seyed Mehdi Iranmanesh, Hadi Kazemi,\n  Jeremy Dawson, and Nasser M. Nasrabadi", "title": "Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device\n  Text-Independent Speaker Verification", "comments": "Accepted in 9th IEEE International Conference on Biometrics: Theory,\n  Applications, and Systems (BTAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel cross-device text-independent speaker verification\narchitecture is proposed. Majority of the state-of-the-art deep architectures\nthat are used for speaker verification tasks consider Mel-frequency cepstral\ncoefficients. In contrast, our proposed Siamese convolutional neural network\narchitecture uses Mel-frequency spectrogram coefficients to benefit from the\ndependency of the adjacent spectro-temporal features. Moreover, although\nspectro-temporal features have proved to be highly reliable in speaker\nverification models, they only represent some aspects of short-term acoustic\nlevel traits of the speaker's voice. However, the human voice consists of\nseveral linguistic levels such as acoustic, lexicon, prosody, and phonetics,\nthat can be utilized in speaker verification models. To compensate for these\ninherited shortcomings in spectro-temporal features, we propose to enhance the\nproposed Siamese convolutional neural network architecture by deploying a\nmultilayer perceptron network to incorporate the prosodic, jitter, and shimmer\nfeatures. The proposed end-to-end verification architecture performs feature\nextraction and verification simultaneously. This proposed architecture displays\nsignificant improvement over classical signal processing approaches and deep\nalgorithms for forensic cross-device speaker verification.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 19:21:59 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Kazemi", "Hadi", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1808.01047", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira Bermudez", "title": "A Data Dependent Multiscale Model for Hyperspectral Unmixing With\n  Spectral Variability", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2963959", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral variability in hyperspectral images can result from factors\nincluding environmental, illumination, atmospheric and temporal changes. Its\noccurrence may lead to the propagation of significant estimation errors in the\nunmixing process. To address this issue, extended linear mixing models have\nbeen proposed which lead to large scale nonsmooth ill-posed inverse problems.\nFurthermore, the regularization strategies used to obtain meaningful results\nhave introduced interdependencies among abundance solutions that further\nincrease the complexity of the resulting optimization problem. In this paper we\npresent a novel data dependent multiscale model for hyperspectral unmixing\naccounting for spectral variability. The new method incorporates spatial\ncontextual information to the abundances in extended linear mixing models by\nusing a multiscale transform based on superpixels. The proposed method results\nin a fast algorithm that solves the abundance estimation problem only once in\neach scale during each iteration. Simulation results using synthetic and real\nimages compare the performances, both in accuracy and execution time, of the\nproposed algorithm and other state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 23:28:54 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 07:04:44 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 13:00:25 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2020 14:51:36 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1808.01050", "submitter": "Haroon Idrees", "authors": "Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya\n  Al-Maadeed, Nasir Rajpoot, Mubarak Shah", "title": "Composition Loss for Counting, Density Map Estimation and Localization\n  in Dense Crowds", "comments": null, "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With multiple crowd gatherings of millions of people every year in events\nranging from pilgrimages to protests, concerts to marathons, and festivals to\nfunerals; visual crowd analysis is emerging as a new frontier in computer\nvision. In particular, counting in highly dense crowds is a challenging problem\nwith far-reaching applicability in crowd safety and management, as well as\ngauging political significance of protests and demonstrations. In this paper,\nwe propose a novel approach that simultaneously solves the problems of\ncounting, density map estimation and localization of people in a given dense\ncrowd image. Our formulation is based on an important observation that the\nthree problems are inherently related to each other making the loss function\nfor optimizing a deep CNN decomposable. Since localization requires\nhigh-quality images and annotations, we introduce UCF-QNRF dataset that\novercomes the shortcomings of previous datasets, and contains 1.25 million\nhumans manually marked with dot annotations. Finally, we present evaluation\nmeasures and comparison with recent deep CNN networks, including those\ndeveloped specifically for crowd counting. Our approach significantly\noutperforms state-of-the-art on the new dataset, which is the most challenging\ndataset with the largest number of crowd annotations in the most diverse set of\nscenes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 23:38:48 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Idrees", "Haroon", ""], ["Tayyab", "Muhmmad", ""], ["Athrey", "Kishan", ""], ["Zhang", "Dong", ""], ["Al-Maadeed", "Somaya", ""], ["Rajpoot", "Nasir", ""], ["Shah", "Mubarak", ""]]}, {"id": "1808.01058", "submitter": "Jyothi Swaroop Guntupalli", "authors": "Dileep George, Alexander Lavin, J. Swaroop Guntupalli, David Mely,\n  Nick Hay, Miguel Lazaro-Gredilla", "title": "Cortical Microcircuits from a Generative Vision Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding the information processing roles of cortical circuits is an\noutstanding problem in neuroscience and artificial intelligence. The\ntheoretical setting of Bayesian inference has been suggested as a framework for\nunderstanding cortical computation. Based on a recently published generative\nmodel for visual inference (George et al., 2017), we derive a family of\nanatomically instantiated and functional cortical circuit models. In contrast\nto simplistic models of Bayesian inference, the underlying generative model's\nrepresentational choices are validated with real-world tasks that required\nefficient inference and strong generalization. The cortical circuit model is\nderived by systematically comparing the computational requirements of this\nmodel with known anatomical constraints. The derived model suggests precise\nfunctional roles for the feedforward, feedback and lateral connections observed\nin different laminae and columns, and assigns a computational role for the path\nthrough the thalamus.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 01:20:08 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["George", "Dileep", ""], ["Lavin", "Alexander", ""], ["Guntupalli", "J. Swaroop", ""], ["Mely", "David", ""], ["Hay", "Nick", ""], ["Lazaro-Gredilla", "Miguel", ""]]}, {"id": "1808.01066", "submitter": "Fateme Bahri", "authors": "Fateme Bahri, Moein Shakeri and Nilanjan Ray", "title": "Online Illumination Invariant Moving Object Detection by Generative\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving object detection (MOD) is a significant problem in computer vision\nthat has many real world applications. Different categories of methods have\nbeen proposed to solve MOD. One of the challenges is to separate moving objects\nfrom illumination changes and shadows that are present in most real world\nvideos. State-of-the-art methods that can handle illumination changes and\nshadows work in a batch mode; thus, these methods are not suitable for long\nvideo sequences or real-time applications. In this paper, we propose an\nextension of a state-of-the-art batch MOD method (ILISD) to an\nonline/incremental MOD using unsupervised and generative neural networks, which\nuse illumination invariant image representations. For each image in a sequence,\nwe use a low-dimensional representation of a background image by a neural\nnetwork and then based on the illumination invariant representation, decompose\nthe foreground image into: illumination change and moving objects. Optimization\nis performed by stochastic gradient descent in an end-to-end and unsupervised\nfashion. Our algorithm can work in both batch and online modes. In the batch\nmode, like other batch methods, optimizer uses all the images. In online mode,\nimages can be incrementally fed into the optimizer. Based on our experimental\nevaluation on benchmark image sequences, both the online and the batch modes of\nour algorithm achieve state-of-the-art accuracy on most data sets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 02:11:32 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Bahri", "Fateme", ""], ["Shakeri", "Moein", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1808.01097", "submitter": "Sheng Guo", "authors": "Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong,\n  Matthew R. Scott and Dinglong Huang", "title": "CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images", "comments": "Accepted to ECCV 2018. 16 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet efficient approach capable of training deep neural\nnetworks on large-scale weakly-supervised web images, which are crawled raw\nfrom the Internet by using text queries, without any human annotation. We\ndevelop a principled learning strategy by leveraging curriculum learning, with\nthe goal of handling a massive amount of noisy labels and data imbalance\neffectively. We design a new learning curriculum by measuring the complexity of\ndata using its distribution density in a feature space, and rank the complexity\nin an unsupervised manner. This allows for an efficient implementation of\ncurriculum learning on large-scale web images, resulting in a high-performance\nCNN model, where the negative impact of noisy labels is reduced substantially.\nImportantly, we show by experiments that those images with highly noisy labels\ncan surprisingly improve the generalization capability of the model, by serving\nas a manner of regularization. Our approaches obtain state-of-the-art\nperformance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101.\nWith an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on\nthe WebVision challenge for 1000-category classification. This result was the\ntop performance by a wide margin, outperforming second place by a nearly 50%\nrelative error rate. Code and models are available at:\nhttps://github.com/MalongTech/CurriculumNet .\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 06:42:11 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 13:53:16 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 08:44:33 GMT"}, {"version": "v4", "created": "Thu, 18 Oct 2018 12:05:35 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Zhang", "Haozhi", ""], ["Zhuang", "Chenfan", ""], ["Dong", "Dengke", ""], ["Scott", "Matthew R.", ""], ["Huang", "Dinglong", ""]]}, {"id": "1808.01099", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Bolei Zhou, Rebecca Russell, Vincent Kee, Syler Wagner,\n  Mitchell Hebert, Antonio Torralba, David M.S. Johnson", "title": "Real-Time Object Pose Estimation with Pose Interpreter Networks", "comments": "To appear at 2018 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2018). Code available at\n  https://github.com/jimmyyhwu/pose-interpreter-networks", "journal-ref": null, "doi": "10.1109/IROS.2018.8593662", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce pose interpreter networks for 6-DoF object pose\nestimation. In contrast to other CNN-based approaches to pose estimation that\nrequire expensively annotated object pose data, our pose interpreter network is\ntrained entirely on synthetic pose data. We use object masks as an intermediate\nrepresentation to bridge real and synthetic. We show that when combined with a\nsegmentation model trained on RGB images, our synthetically trained pose\ninterpreter network is able to generalize to real data. Our end-to-end system\nfor object pose estimation runs in real-time (20 Hz) on live RGB data, without\nusing depth information or ICP refinement.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 07:12:03 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wu", "Jimmy", ""], ["Zhou", "Bolei", ""], ["Russell", "Rebecca", ""], ["Kee", "Vincent", ""], ["Wagner", "Syler", ""], ["Hebert", "Mitchell", ""], ["Torralba", "Antonio", ""], ["Johnson", "David M. S.", ""]]}, {"id": "1808.01101", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Gozde Bozdagi Akar", "title": "Exploiting Local Indexing and Deep Feature Confidence Scores for Fast\n  Image-to-Video Search", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cost-effective visual representation and fast query-by-example search are\ntwo challenging goals that should be maintained for web-scale visual retrieval\ntasks on moderate hardware. This paper introduces a fast and robust method that\nensures both of these goals by obtaining state-of-the-art performance for an\nimage-to-video search scenario. Hence, we present critical enhancements to\nwell-known indexing and visual representation techniques by promoting faster,\nbetter and moderate retrieval performance. We also boost the superiority of our\nmethod for some visual challenges by exploiting individual decisions of local\nand global descriptors at query time. For instance, local content descriptors\nrepresent copied/duplicated scenes with large geometric deformations such as\nscale, orientation and affine transformation. In contrast, the use of global\ncontent descriptors is more practical for near-duplicate and semantic searches.\nExperiments are conducted on a large-scale Stanford I2V dataset. The\nexperimental results show that our method is useful in terms of complexity and\nquery processing time for large-scale visual retrieval scenarios, even if local\nand global representations are used together. The proposed method is superior\nand achieves state-of-the-art performance based on the mean average precision\n(MAP) score of this dataset. Lastly, we report additional MAP scores after\nupdating the ground annotations unveiled by retrieval results of the proposed\nmethod, and it shows that the actual performance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 07:29:43 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 14:42:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "1808.01102", "submitter": "Tatiana Tommasi", "authors": "Fabio M. Carlucci, Paolo Russo, Tatiana Tommasi, Barbara Caputo", "title": "Hallucinating Agnostic Images to Generalize Across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generalize across visual domains is crucial for the robustness\nof artificial recognition systems. Although many training sources may be\navailable in real contexts, the access to even unlabeled target samples cannot\nbe taken for granted, which makes standard unsupervised domain adaptation\nmethods inapplicable in the wild. In this work we investigate how to exploit\nmultiple sources by hallucinating a deep visual domain composed of images,\npossibly unrealistic, able to maintain categorical knowledge while discarding\nspecific source styles. The produced agnostic images are the result of a deep\narchitecture that applies pixel adaptation on the original source data guided\nby two adversarial domain classifier branches at image and feature level. Our\napproach is conceived to learn only from source data, but it seamlessly extends\nto the use of unlabeled target samples. Remarkable results for both\nmulti-source domain adaptation and domain generalization support the power of\nhallucinating agnostic images in this framework.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 07:31:01 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 23:16:21 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Carlucci", "Fabio M.", ""], ["Russo", "Paolo", ""], ["Tommasi", "Tatiana", ""], ["Caputo", "Barbara", ""]]}, {"id": "1808.01104", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Gozde Bozdagi Akar", "title": "Improved Deep Spectral Convolution Network For Hyperspectral Unmixing\n  With Multinomial Mixture Kernel and Endmember Uncertainty", "comments": "Submitted to Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we propose a novel framework for hyperspectral unmixing by\nusing an improved deep spectral convolution network (DSCN++) combined with\nendmember uncertainty. DSCN++ is used to compute high-level representations\nwhich are further modeled with Multinomial Mixture Model to estimate abundance\nmaps. In the reconstruction step, a new trainable uncertainty term based on a\nnonlinear neural network model is introduced to provide robustness to endmember\nuncertainty. For the optimization of the coefficients of the multinomial model\nand the uncertainty term, Wasserstein Generative Adversarial Network (WGAN) is\nexploited to improve stability and to capture uncertainty. Experiments are\nperformed on both real and synthetic datasets. The results validate that the\nproposed method obtains state-of-the-art hyperspectral unmixing performance\nparticularly on the real datasets compared to the baseline techniques.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 07:40:25 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 12:30:09 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 10:45:27 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 18:38:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "1808.01106", "submitter": "Yang Du", "authors": "Yang Du and Chunfeng Yuan and Bing Li and Lili Zhao and Yangxi Li and\n  Weiming Hu", "title": "Interaction-aware Spatio-temporal Pyramid Attention Networks for Action\n  Classification", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local features at neighboring spatial positions in feature maps have high\ncorrelation since their receptive fields are often overlapped. Self-attention\nusually uses the weighted sum (or other functions) with internal elements of\neach local feature to obtain its weight score, which ignores interactions among\nlocal features. To address this, we propose an effective interaction-aware\nself-attention model inspired by PCA to learn attention maps. Furthermore,\nsince different layers in a deep network capture feature maps of different\nscales, we use these feature maps to construct a spatial pyramid and then\nutilize multi-scale information to obtain more accurate attention scores, which\nare used to weight the local features in all spatial positions of feature maps\nto calculate attention maps. Moreover, our spatial pyramid attention is\nunrestricted to the number of its input feature maps so it is easily extended\nto a spatio-temporal version. Finally, our model is embedded in general CNNs to\nform end-to-end attention networks for action classification. Experimental\nresults show that our method achieves the state-of-the-art results on the\nUCF101, HMDB51 and untrimmed Charades.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 07:44:25 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Du", "Yang", ""], ["Yuan", "Chunfeng", ""], ["Li", "Bing", ""], ["Zhao", "Lili", ""], ["Li", "Yangxi", ""], ["Hu", "Weiming", ""]]}, {"id": "1808.01111", "submitter": "Nikolaus Demmel", "authors": "Xiang Gao, Rui Wang, Nikolaus Demmel, Daniel Cremers", "title": "LDSO: Direct Sparse Odometry with Loop Closure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an extension of Direct Sparse Odometry (DSO) to a\nmonocular visual SLAM system with loop closure detection and pose-graph\noptimization (LDSO). As a direct technique, DSO can utilize any image pixel\nwith sufficient intensity gradient, which makes it robust even in featureless\nareas. LDSO retains this robustness, while at the same time ensuring\nrepeatability of some of these points by favoring corner features in the\ntracking frontend. This repeatability allows to reliably detect loop closure\ncandidates with a conventional feature-based bag-of-words (BoW) approach. Loop\nclosure candidates are verified geometrically and Sim(3) relative pose\nconstraints are estimated by jointly minimizing 2D and 3D geometric error\nterms. These constraints are fused with a co-visibility graph of relative poses\nextracted from DSO's sliding window optimization. Our evaluation on publicly\navailable datasets demonstrates that the modified point selection strategy\nretains the tracking accuracy and robustness, and the integrated pose-graph\noptimization significantly reduces the accumulated rotation-, translation- and\nscale-drift, resulting in an overall performance comparable to state-of-the-art\nfeature-based systems, even without global bundle adjustment.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 08:12:29 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Gao", "Xiang", ""], ["Wang", "Rui", ""], ["Demmel", "Nikolaus", ""], ["Cremers", "Daniel", ""]]}, {"id": "1808.01119", "submitter": "Ting-Yao Hu", "authors": "Ting-Yao Hu, Xiaojun Chang, and Alexander G. Hauptmann", "title": "Multi-shot Person Re-identification through Set Distance with Visual\n  Distributional Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to identify a specific person at distinct times\nand locations. It is challenging because of occlusion, illumination, and\nviewpoint change in camera views. Recently, multi-shot person re-id task\nreceives more attention since it is closer to real-world application. A key\npoint of a good algorithm for multi-shot person re-id is the temporal\naggregation of the person appearance features. While most of the current\napproaches apply pooling strategies and obtain a fixed-size vector\nrepresentation, these may lose the matching evidence between examples. In this\nwork, we propose the idea of visual distributional representation, which\ninterprets an image set as samples drawn from an unknown distribution in\nappearance feature space. Based on the supervision signals from a downstream\ntask of interest, the method reshapes the appearance feature space and further\nlearns the unknown distribution of each image set. In the context of multi-shot\nperson re-id, we apply this novel concept along with Wasserstein distance and\nlearn a distributional set distance function between two image sets. In this\nway, the proper alignment between two image sets can be discovered naturally in\na non-parametric manner. Our experiment results on two public datasets show the\nadvantages of our proposed method compared to other state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 08:39:23 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 18:09:04 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Hu", "Ting-Yao", ""], ["Chang", "Xiaojun", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1808.01121", "submitter": "Yang He", "authors": "Yang He, Bernt Schiele, Mario Fritz", "title": "Diverse Conditional Image Generation by Stochastic Regression with\n  Latent Drop-Out Codes", "comments": "This version withdrawn by arXiv administrators because the submitter\n  did not have the right to agree to our license at the time of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Deep Learning and probabilistic modeling have led to\nstrong improvements in generative models for images. On the one hand,\nGenerative Adversarial Networks (GANs) have contributed a highly effective\nadversarial learning procedure, but still suffer from stability issues. On the\nother hand, Conditional Variational Auto-Encoders (CVAE) models provide a sound\nway of conditional modeling but suffer from mode-mixing issues. Therefore,\nrecent work has turned back to simple and stable regression models that are\neffective at generation but give up on the sampling mechanism and the latent\ncode representation. We propose a novel and efficient stochastic regression\napproach with latent drop-out codes that combines the merits of both lines of\nresearch. In addition, a new training objective increases coverage of the\ntraining distribution leading to improvements over the state of the art in\nterms of accuracy as well as diversity.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 08:47:55 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["He", "Yang", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1808.01124", "submitter": "Minh-Tan Pham", "authors": "Minh-Tan Pham", "title": "Efficient texture retrieval using multiscale local extrema descriptors\n  and covariance embedding", "comments": "15 pages, to appear in ECCV Workshops 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient method for texture retrieval using\nmultiscale feature extraction and embedding based on the local extrema\nkeypoints. The idea is to first represent each texture image by its local\nmaximum and local minimum pixels. The image is then divided into regular\noverlapping blocks and each one is characterized by a feature vector\nconstructed from the radiometric, geometric and structural information of its\nlocal extrema. All feature vectors are finally embedded into a covariance\nmatrix which will be exploited for dissimilarity measurement within retrieval\ntask. Thanks to the method's simplicity, multiscale scheme can be easily\nimplemented to improve its scale-space representation capacity. We argue that\nour handcrafted features are easy to implement, fast to run but can provide\nvery competitive performance compared to handcrafted and CNN-based learned\ndescriptors from the literature. In particular, the proposed framework provides\nhighly competitive retrieval rate for several texture databases including\n94.95% for MIT Vistex, 79.87% for Stex, 76.15% for Outex TC-00013 and 89.74%\nfor USPtex.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:04:00 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Pham", "Minh-Tan", ""]]}, {"id": "1808.01134", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Aditya Ganeshan, Rahul M. V., Aditya Prakash, R.\n  Venkatesh Babu", "title": "iSPA-Net: Iterative Semantic Pose Alignment Network", "comments": "Accepted at ACMMM 2018. Code available at\n  https://github.com/val-iisc/iSPA-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and extracting 3D information of objects from monocular 2D\nimages is a fundamental problem in computer vision. In the task of 3D object\npose estimation, recent data driven deep neural network based approaches suffer\nfrom scarcity of real images with 3D keypoint and pose annotations. Drawing\ninspiration from human cognition, where the annotators use a 3D CAD model as\nstructural reference to acquire ground-truth viewpoints for real images; we\npropose an iterative Semantic Pose Alignment Network, called iSPA-Net. Our\napproach focuses on exploiting semantic 3D structural regularity to solve the\ntask of fine-grained pose estimation by predicting viewpoint difference between\na given pair of images. Such image comparison based approach also alleviates\nthe problem of data scarcity and hence enhances scalability of the proposed\napproach for novel object categories with minimal annotation. The fine-grained\nobject pose estimator is also aided by correspondence of learned spatial\ndescriptor of the input image pair. The proposed pose alignment framework\nenjoys the faculty to refine its initial pose estimation in consecutive\niterations by utilizing an online rendering setup along with effectiveness of a\nnon-uniform bin classification of pose-difference. This enables iSPA-Net to\nachieve state-of-the-art performance on various real image viewpoint estimation\ndatasets. Further, we demonstrate effectiveness of the approach for multiple\napplications. First, we show results for active object viewpoint localization\nto capture images from similar pose considering only a single image as pose\nreference. Second, we demonstrate the ability of the learned semantic\ncorrespondence to perform unsupervised part-segmentation transfer using only a\nsingle part-annotated 3D template model per object class. To encourage\nreproducible research, we have released the codes for our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:44:47 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Ganeshan", "Aditya", ""], ["V.", "Rahul M.", ""], ["Prakash", "Aditya", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1808.01150", "submitter": "Faizal Hafiz", "authors": "Faizal Hafiz, Akshya Swain, Nitish Patel, Chirag Naik", "title": "A Two-Dimensional (2-D) Learning Framework for Particle Swarm based\n  Feature Selection", "comments": null, "journal-ref": "Elsevier - Pattern Recognition, Volume 76, 2018, Pages 416-433", "doi": "10.1016/j.patcog.2017.11.027", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new generalized two dimensional learning approach for\nparticle swarm based feature selection. The core idea of the proposed approach\nis to include the information about the subset cardinality into the learning\nframework by extending the dimension of the velocity. The 2D-learning framework\nretains all the key features of the original PSO, despite the extra learning\ndimension. Most of the popular variants of PSO can easily be adapted into this\n2D learning framework for feature selection problems. The efficacy of the\nproposed learning approach has been evaluated considering several benchmark\ndata and two induction algorithms: Naive-Bayes and k-Nearest Neighbor. The\nresults of the comparative investigation including the time-complexity analysis\nwith GA, ACO and five other PSO variants illustrate that the proposed 2D\nlearning approach gives feature subset with relatively smaller cardinality and\nbetter classification performance with shorter run times.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 10:50:43 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Hafiz", "Faizal", ""], ["Swain", "Akshya", ""], ["Patel", "Nitish", ""], ["Naik", "Chirag", ""]]}, {"id": "1808.01153", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Phani Krishna Uppala, and R. Venkatesh Babu", "title": "Ask, Acquire, and Attack: Data-free UAP Generation using Class\n  Impressions", "comments": "Accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are susceptible to input specific noise, called\nadversarial perturbations. Moreover, there exist input-agnostic noise, called\nUniversal Adversarial Perturbations (UAP) that can affect inference of the\nmodels over most input samples. Given a model, there exist broadly two\napproaches to craft UAPs: (i) data-driven: that require data, and (ii)\ndata-free: that do not require data samples. Data-driven approaches require\nactual samples from the underlying data distribution and craft UAPs with high\nsuccess (fooling) rate. However, data-free approaches craft UAPs without\nutilizing any data samples and therefore result in lesser success rates. In\nthis paper, for data-free scenarios, we propose a novel approach that emulates\nthe effect of data samples with class impressions in order to craft UAPs using\ndata-driven objectives. Class impression for a given pair of category and model\nis a generic representation (in the input space) of the samples belonging to\nthat category. Further, we present a neural network based generative model that\nutilizes the acquired class impressions to learn crafting UAPs. Experimental\nevaluation demonstrates that the learned generative model, (i) readily crafts\nUAPs via simple feed-forwarding through neural network layers, and (ii)\nachieves state-of-the-art success rates for data-free scenario and closer to\nthat for data-driven setting without actually utilizing any data samples.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 11:02:26 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Uppala", "Phani Krishna", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1808.01200", "submitter": "Tanya Nair", "authors": "Tanya Nair, Doina Precup, Douglas L. Arnold, Tal Arbel", "title": "Exploring Uncertainty Measures in Deep Networks for Multiple Sclerosis\n  Lesion Detection and Segmentation", "comments": "Updated references in Introduction; Accepted to the 21st\n  International Conference on Medical Image Computing and Computer Assisted\n  Intervention (MICCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) networks have recently been shown to outperform other\nsegmentation methods on various public, medical-image challenge datasets\n[3,11,16], especially for large pathologies. However, in the context of\ndiseases such as Multiple Sclerosis (MS), monitoring all the focal lesions\nvisible on MRI sequences, even very small ones, is essential for disease\nstaging, prognosis, and evaluating treatment efficacy. Moreover, producing\ndeterministic outputs hinders DL adoption into clinical routines. Uncertainty\nestimates for the predictions would permit subsequent revision by clinicians.\nWe present the first exploration of multiple uncertainty estimates based on\nMonte Carlo (MC) dropout [4] in the context of deep networks for lesion\ndetection and segmentation in medical images. Specifically, we develop a 3D MS\nlesion segmentation CNN, augmented to provide four different voxel-based\nuncertainty measures based on MC dropout. We train the network on a\nproprietary, large-scale, multi-site, multi-scanner, clinical MS dataset, and\ncompute lesion-wise uncertainties by accumulating evidence from voxel-wise\nuncertainties within detected lesions. We analyze the performance of\nvoxel-based segmentation and lesion-level detection by choosing operating\npoints based on the uncertainty. Empirical evidence suggests that uncertainty\nmeasures consistently allow us to choose superior operating points compared\nonly using the network's sigmoid output as a probability.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 14:19:32 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 21:07:08 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Nair", "Tanya", ""], ["Precup", "Doina", ""], ["Arnold", "Douglas L.", ""], ["Arbel", "Tal", ""]]}, {"id": "1808.01244", "submitter": "Hei Law", "authors": "Hei Law, Jia Deng", "title": "CornerNet: Detecting Objects as Paired Keypoints", "comments": "Extended version with additional results. Test AP on MS COOO improved\n  from 42.1% to 42.2% after a bug fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CornerNet, a new approach to object detection where we detect an\nobject bounding box as a pair of keypoints, the top-left corner and the\nbottom-right corner, using a single convolution neural network. By detecting\nobjects as paired keypoints, we eliminate the need for designing a set of\nanchor boxes commonly used in prior single-stage detectors. In addition to our\nnovel formulation, we introduce corner pooling, a new type of pooling layer\nthat helps the network better localize corners. Experiments show that CornerNet\nachieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 16:05:55 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 18:58:40 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Law", "Hei", ""], ["Deng", "Jia", ""]]}, {"id": "1808.01265", "submitter": "Dengxin Dai", "authors": "Christos Sakaridis, Dengxin Dai, Simon Hecker, Luc Van Gool", "title": "Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy\n  Scene Understanding", "comments": "final version, ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of semantic scene understanding under dense\nfog. Although considerable progress has been made in semantic scene\nunderstanding, it is mainly related to clear-weather scenes. Extending\nrecognition methods to adverse weather conditions such as fog is crucial for\noutdoor applications. In this paper, we propose a novel method, named\nCurriculum Model Adaptation (CMAda), which gradually adapts a semantic\nsegmentation model from light synthetic fog to dense real fog in multiple\nsteps, using both synthetic and real foggy data. In addition, we present three\nother main stand-alone contributions: 1) a novel method to add synthetic fog to\nreal, clear-weather scenes using semantic input; 2) a new fog density\nestimator; 3) the Foggy Zurich dataset comprising $3808$ real foggy images,\nwith pixel-level semantic annotations for $16$ images with dense fog. Our\nexperiments show that 1) our fog simulation slightly outperforms a\nstate-of-the-art competing simulation with respect to the task of semantic\nfoggy scene understanding (SFSU); 2) CMAda improves the performance of\nstate-of-the-art models for SFSU significantly by leveraging unlabeled real\nfoggy data. The datasets and code are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 17:26:39 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Sakaridis", "Christos", ""], ["Dai", "Dengxin", ""], ["Hecker", "Simon", ""], ["Van Gool", "Luc", ""]]}, {"id": "1808.01316", "submitter": "Bihan Wen Dr", "authors": "Bihan Wen, Yanjun Li and Yoram Bresler", "title": "The Power of Complementary Regularizers: Image Recovery via Transform\n  Learning and Low-Rank Modeling", "comments": "13 pages, 7 figures, submitted to TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on adaptive sparse and on low-rank signal modeling have\ndemonstrated their usefulness in various image / video processing applications.\nPatch-based methods exploit local patch sparsity, whereas other works apply\nlow-rankness of grouped patches to exploit image non-local structures. However,\nusing either approach alone usually limits performance in image reconstruction\nor recovery applications. In this work, we propose a simultaneous sparsity and\nlow-rank model, dubbed STROLLR, to better represent natural images. In order to\nfully utilize both the local and non-local image properties, we develop an\nimage restoration framework using a transform learning scheme with joint\nlow-rank regularization. The approach owes some of its computational efficiency\nand good performance to the use of transform learning for adaptive sparse\nrepresentation rather than the popular synthesis dictionary learning\nalgorithms, which involve approximation of NP-hard sparse coding and expensive\nlearning steps. We demonstrate the proposed framework in various applications\nto image denoising, inpainting, and compressed sensing based magnetic resonance\nimaging. Results show promising performance compared to state-of-the-art\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 19:29:19 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wen", "Bihan", ""], ["Li", "Yanjun", ""], ["Bresler", "Yoram", ""]]}, {"id": "1808.01337", "submitter": "Vignesh Ganapathi-Subramanian", "authors": "Vignesh Ganapathi-Subramanian, Olga Diamanti, Soeren Pirk, Chengcheng\n  Tang, Matthias Niessner, Leonidas J. Guibas", "title": "Parsing Geometry Using Structure-Aware Shape Templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-life man-made objects often exhibit strong and easily-identifiable\nstructure, as a direct result of their design or their intended functionality.\nStructure typically appears in the form of individual parts and their\narrangement. Knowing about object structure can be an important cue for object\nrecognition and scene understanding - a key goal for various AR and robotics\napplications. However, commodity RGB-D sensors used in these scenarios only\nproduce raw, unorganized point clouds, without structural information about the\ncaptured scene. Moreover, the generated data is commonly partial and\nsusceptible to artifacts and noise, which makes inferring the structure of\nscanned objects challenging. In this paper, we organize large shape collections\ninto parameterized shape templates to capture the underlying structure of the\nobjects. The templates allow us to transfer the structural information onto new\nobjects and incomplete scans. We employ a deep neural network that matches the\npartial scan with one of the shape templates, then match and fit it to complete\nand detailed models from the collection. This allows us to faithfully label its\nparts and to guide the reconstruction of the scanned object. We showcase the\neffectiveness of our method by comparing it to other state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:14:58 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 02:40:46 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Ganapathi-Subramanian", "Vignesh", ""], ["Diamanti", "Olga", ""], ["Pirk", "Soeren", ""], ["Tang", "Chengcheng", ""], ["Niessner", "Matthias", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1808.01338", "submitter": "Thiemo Alldieck", "authors": "Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard\n  Pons-Moll", "title": "Detailed Human Avatars from Monocular Video", "comments": "International Conference on 3D Vision (3DV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for high detail-preserving human avatar creation\nfrom monocular video. A parameterized body model is refined and optimized to\nmaximally resemble subjects from a video showing them from all sides. Our\navatars feature a natural face, hairstyle, clothes with garment wrinkles, and\nhigh-resolution texture. Our paper contributes facial landmark and\nshading-based human body shape refinement, a semantic texture prior, and a\nnovel texture stitching strategy, resulting in the most sophisticated-looking\nhuman avatars obtained from a single video to date. Numerous results show the\nrobustness and versatility of our method. A user study illustrates its\nsuperiority over the state-of-the-art in terms of identity preservation, level\nof detail, realism, and overall user preference.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:15:29 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Alldieck", "Thiemo", ""], ["Magnor", "Marcus", ""], ["Xu", "Weipeng", ""], ["Theobalt", "Christian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1808.01340", "submitter": "Joao Carreira", "authors": "Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier and\n  Andrew Zisserman", "title": "A Short Note about Kinetics-600", "comments": "Companion to public release of kinetics-600 test set labels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an extension of the DeepMind Kinetics human action dataset from\n400 classes, each with at least 400 video clips, to 600 classes, each with at\nleast 600 video clips. In order to scale up the dataset we changed the data\ncollection process so it uses multiple queries per class, with some of them in\na language other than english -- portuguese. This paper details the changes\nbetween the two versions of the dataset and includes a comprehensive set of\nstatistics of the new version as well as baseline results using the I3D neural\nnetwork architecture. The paper is a companion to the release of the ground\ntruth labels for the public test set.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:17:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Carreira", "Joao", ""], ["Noland", "Eric", ""], ["Banki-Horvath", "Andras", ""], ["Hillier", "Chloe", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1808.01343", "submitter": "Rahul Sawhney", "authors": "Rahul Sawhney, Fuxin Li, Henrik I. Christensen, Charles L. Isbell", "title": "Purely Geometric Scene Association and Retrieval - A Case for Macro\n  Scale 3D Geometry", "comments": "Accepted in ICRA '18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problems of measuring geometric similarity between 3D scenes,\nrepresented through point clouds or range data frames, and associating them.\nOur approach leverages macro-scale 3D structural geometry - the relative\nconfiguration of arbitrary surfaces and relationships among structures that are\npotentially far apart. We express such discriminative information in a\nviewpoint-invariant feature space. These are subsequently encoded in a\nframe-level signature that can be utilized to measure geometric similarity.\nSuch a characterization is robust to noise, incomplete and partially\noverlapping data besides viewpoint changes. We show how it can be employed to\nselect a diverse set of data frames which have structurally similar content,\nand how to validate whether views with similar geometric content are from the\nsame scene. The problem is formulated as one of general purpose retrieval from\nan unannotated, spatio-temporally unordered database. Empirical analysis\nindicates that the presented approach thoroughly outperforms baselines on depth\n/ range data. Its depth-only performance is competitive with state-of-the-art\napproaches with RGB or RGB-D inputs, including ones based on deep learning.\nExperiments show retrieval performance to hold up well with much sparser\ndatabases, which is indicative of the approach's robustness. The approach\ngeneralized well - it did not require dataset specific training, and scaled up\nin our experiments. Finally, we also demonstrate how geometrically diverse\nselection of views can result in richer 3D reconstructions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:20:40 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Sawhney", "Rahul", ""], ["Li", "Fuxin", ""], ["Christensen", "Henrik I.", ""], ["Isbell", "Charles L.", ""]]}, {"id": "1808.01355", "submitter": "Arunava Chakravarty", "authors": "Arunava Chakravarty, Jayanthi Sivswamy", "title": "A Deep Learning based Joint Segmentation and Classification Framework\n  for Glaucoma Assesment in Retinal Color Fundus Images", "comments": "8 pages, submitted to the REFUGE glaucoma segmentation grand\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Computer Aided diagnostic tools can be used for the early detection\nof glaucoma to prevent irreversible vision loss. In this work, we present a\nMulti-task Convolutional Neural Network (CNN) that jointly segments the Optic\nDisc (OD), Optic Cup (OC) and predicts the presence of glaucoma in color fundus\nimages. The CNN utilizes a combination of image appearance features and\nstructural features obtained from the OD-OC segmentation to obtain a robust\nprediction. The use of fewer network parameters and the sharing of the CNN\nfeatures for multiple related tasks ensures the good generalizability of the\narchitecture, allowing it to be trained on small training sets. The\ncross-testing performance of the proposed method on an independent validation\nset acquired using a different camera and image resolution was found to be good\nwith an average dice score of 0.92 for OD, 0.84 for OC and AUC of 0.95 on the\ntask of glaucoma classification illustrating its potential as a mass screening\ntool for the early detection of glaucoma.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 09:12:37 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chakravarty", "Arunava", ""], ["Sivswamy", "Jayanthi", ""]]}, {"id": "1808.01356", "submitter": "Beatriz Blanco-Filgueira", "authors": "Beatriz Blanco-Filgueira, Daniel Garc\\'ia-Lesta, Mauro\n  Fern\\'andez-Sanjurjo, V\\'ictor M. Brea and Paula L\\'opez", "title": "Deep Learning-Based Multiple Object Visual Tracking on Embedded System\n  for IoT and Mobile Edge Computing Applications", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute and memory demands of state-of-the-art deep learning methods are\nstill a shortcoming that must be addressed to make them useful at IoT\nend-nodes. In particular, recent results depict a hopeful prospect for image\nprocessing using Convolutional Neural Netwoks, CNNs, but the gap between\nsoftware and hardware implementations is already considerable for IoT and\nmobile edge computing applications due to their high power consumption. This\nproposal performs low-power and real time deep learning-based multiple object\nvisual tracking implemented on an NVIDIA Jetson TX2 development kit. It\nincludes a camera and wireless connection capability and it is battery powered\nfor mobile and outdoor applications. A collection of representative sequences\ncaptured with the on-board camera, dETRUSC video dataset, is used to exemplify\nthe performance of the proposed algorithm and to facilitate benchmarking. The\nresults in terms of power consumption and frame rate demonstrate the\nfeasibility of deep learning algorithms on embedded platforms although more\neffort to joint algorithm and hardware design of CNNs is needed.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 10:33:09 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Blanco-Filgueira", "Beatriz", ""], ["Garc\u00eda-Lesta", "Daniel", ""], ["Fern\u00e1ndez-Sanjurjo", "Mauro", ""], ["Brea", "V\u00edctor M.", ""], ["L\u00f3pez", "Paula", ""]]}, {"id": "1808.01357", "submitter": "Mirco Planamente", "authors": "Mirco Planamente, Mohammad Reza Loghmani and Barbara Caputo", "title": "A recurrent multi-scale approach to RBG-D Object Recognition", "comments": "Master thesis extracted from the paper arXiv:1806.01673 submitted to\n  accv 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological development aims to produce generations of increasingly\nefficient robots able to perform complex tasks. This requires considerable\nefforts, from the scientific community, to find new algorithms that solve\ncomputer vision problems, such as object recognition. The diffusion of RGB-D\ncameras directed the study towards the research of new architectures able to\nexploit the RGB and Depth information. The project that is developed in this\nthesis concerns the realization of a new end-to-end architecture for the\nrecognition of RGB-D objects called RCFusion. Our method generates compact and\nhighly discriminative multi-modal features by combining complementary RGB and\ndepth information representing different levels of abstraction. We evaluate our\nmethod on standard object recognition datasets, RGB-D Object Dataset and\nJHUIT-50. The experiments performed show that our method outperforms the\nexisting approaches and establishes new state-of-the-art results for both\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 08:15:06 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 15:37:49 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 16:50:09 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Planamente", "Mirco", ""], ["Loghmani", "Mohammad Reza", ""], ["Caputo", "Barbara", ""]]}, {"id": "1808.01358", "submitter": "Hiroki Ohashi", "authors": "Hiroki Ohashi, Mohammad Al-Naser, Sheraz Ahmed, Katsuyuki Nakamura,\n  Takuto Sato, Andreas Dengel", "title": "Attributes' Importance for Zero-Shot Pose-Classification Based on\n  Wearable Sensors", "comments": "The paper was published at Sensors, an open access journal\n  (http://www.mdpi.com/1424-8220/18/8/2485). This article belongs to the\n  Special Issue Artificial Intelligence and Machine Learning in Sensors\n  Networks", "journal-ref": "Sensors 2018, 18, 2485", "doi": "10.3390/s18082485", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet effective method for improving the\nperformance of zero-shot learning (ZSL). ZSL classifies instances of unseen\nclasses, from which no training data is available, by utilizing the attributes\nof the classes. Conventional ZSL methods have equally dealt with all the\navailable attributes, but this sometimes causes misclassification. This is\nbecause an attribute that is effective for classifying instances of one class\nis not always effective for another class. In this case, a metric of\nclassifying the latter class can be undesirably influenced by the irrelevant\nattribute. This paper solves this problem by taking the importance of each\nattribute for each class into account when calculating the metric. In addition\nto the proposal of this new method, this paper also contributes by providing a\ndataset for pose classification based on wearable sensors, named HDPoseDS. It\ncontains 22 classes of poses performed by 10 subjects with 31 IMU sensors\nacross full body. To the best of our knowledge, it is the richest\nwearable-sensor dataset especially in terms of sensor density, and thus it is\nsuitable for studying zero-shot pose/action recognition. The presented method\nwas evaluated on HDPoseDS and outperformed relative improvement of 5.9% in\ncomparison to the best baseline method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:51:01 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ohashi", "Hiroki", ""], ["Al-Naser", "Mohammad", ""], ["Ahmed", "Sheraz", ""], ["Nakamura", "Katsuyuki", ""], ["Sato", "Takuto", ""], ["Dengel", "Andreas", ""]]}, {"id": "1808.01405", "submitter": "Pouya Bashivan", "authors": "Pouya Bashivan, Mark Tensen, James J DiCarlo", "title": "Teacher Guided Architecture Search", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the recent improvement in neural networks for computer vision has\nresulted from discovery of new networks architectures. Most prior work has used\nthe performance of candidate models following limited training to automatically\nguide the search in a feasible way. Could further gains in computational\nefficiency be achieved by guiding the search via measurements of a high\nperforming network with unknown detailed architecture (e.g. the primate visual\nsystem)? As one step toward this goal, we use representational similarity\nanalysis to evaluate the similarity of internal activations of candidate\nnetworks with those of a (fixed, high performing) teacher network. We show that\nadopting this evaluation metric could produce up to an order of magnitude in\nsearch efficiency over performance-guided methods. Our approach finds a\nconvolutional cell structure with similar performance as was previously found\nusing other methods but at a total computational cost that is two orders of\nmagnitude lower than Neural Architecture Search (NAS) and more than four times\nlower than progressive neural architecture search (PNAS). We further show that\nmeasurements from only ~300 neurons from primate visual system provides enough\nsignal to find a network with an Imagenet top-1 error that is significantly\nlower than that achieved by performance-guided architecture search alone. These\nresults suggest that representational matching can be used to accelerate\nnetwork architecture search in cases where one has access to some or all of the\ninternal representations of a teacher network of interest, such as the brain's\nsensory processing networks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 01:43:03 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 21:45:05 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 13:09:37 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Bashivan", "Pouya", ""], ["Tensen", "Mark", ""], ["DiCarlo", "James J", ""]]}, {"id": "1808.01415", "submitter": "Dongmian Zou", "authors": "Dongmian Zou, Radu Balan, Maneesh Singh", "title": "On Lipschitz Bounds of General Convolutional Neural Networks", "comments": "26 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many convolutional neural networks (CNNs) have a feed-forward structure. In\nthis paper, a linear program that estimates the Lipschitz bound of such CNNs is\nproposed. Several CNNs, including the scattering networks, the AlexNet and the\nGoogleNet, are studied numerically and compared to the theoretical bounds.\nNext, concentration inequalities of the output distribution to a stationary\nrandom input signal expressed in terms of the Lipschitz bound are established.\nThe Lipschitz bound is further used to establish a nonlinear discriminant\nanalysis designed to measure the separation between features of different\nclasses.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 03:21:47 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zou", "Dongmian", ""], ["Balan", "Radu", ""], ["Singh", "Maneesh", ""]]}, {"id": "1808.01423", "submitter": "Chris Tensmeyer", "authors": "Chris Tensmeyer, Curtis Wigington, Brian Davis, Seth Stewart, Tony\n  Martinez, William Barrett", "title": "Language Model Supervision for Handwriting Recognition Model Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training state-of-the-art offline handwriting recognition (HWR) models\nrequires large labeled datasets, but unfortunately such datasets are not\navailable in all languages and domains due to the high cost of manual\nlabeling.We address this problem by showing how high resource languages can be\nleveraged to help train models for low resource languages.We propose a transfer\nlearning methodology where we adapt HWR models trained on a source language to\na target language that uses the same writing script.This methodology only\nrequires labeled data in the source language, unlabeled data in the target\nlanguage, and a language model of the target language. The language model is\nused in a bootstrapping fashion to refine predictions in the target language\nfor use as ground truth in training the model.Using this approach we\ndemonstrate improved transferability among French, English, and Spanish\nlanguages using both historical and modern handwriting datasets. In the best\ncase, transferring with the proposed methodology results in character error\nrates nearly as good as full supervised training.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 04:27:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Tensmeyer", "Chris", ""], ["Wigington", "Curtis", ""], ["Davis", "Brian", ""], ["Stewart", "Seth", ""], ["Martinez", "Tony", ""], ["Barrett", "William", ""]]}, {"id": "1808.01424", "submitter": "Jing Dong", "authors": "Jing Dong, Byron Boots, Frank Dellaert, Ranveer Chandra, Sudipta N.\n  Sinha", "title": "Learning to Align Images using Weak Geometric Supervision", "comments": "Accepted in 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image alignment tasks require accurate pixel correspondences, which are\nusually recovered by matching local feature descriptors. Such descriptors are\noften derived using supervised learning on existing datasets with ground truth\ncorrespondences. However, the cost of creating such datasets is usually\nprohibitive. In this paper, we propose a new approach to align two images\nrelated by an unknown 2D homography where the local descriptor is learned from\nscratch from the images and the homography is estimated simultaneously. Our key\ninsight is that a siamese convolutional neural network can be trained jointly\nwhile iteratively updating the homography parameters by optimizing a single\nloss function. Our method is currently weakly supervised because the input\nimages need to be roughly aligned.\n  We have used this method to align images of different modalities such as RGB\nand near-infra-red (NIR) without using any prior labeled data. Images\nautomatically aligned by our method were then used to train descriptors that\ngeneralize to new images. We also evaluated our method on RGB images. On the\nHPatches benchmark, our method achieves comparable accuracy to deep local\ndescriptors that were trained offline in a supervised setting.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 04:28:52 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Dong", "Jing", ""], ["Boots", "Byron", ""], ["Dellaert", "Frank", ""], ["Chandra", "Ranveer", ""], ["Sinha", "Sudipta N.", ""]]}, {"id": "1808.01427", "submitter": "Elena Balashova", "authors": "Elena Balashova, Vivek Singh, Jiangping Wang, Brian Teixeira, Terrence\n  Chen, Thomas Funkhouser", "title": "Structure-Aware Shape Synthesis", "comments": "Accepted to 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new procedure to guide training of a data-driven shape\ngenerative model using a structure-aware loss function. Complex 3D shapes often\ncan be summarized using a coarsely defined structure which is consistent and\nrobust across variety of observations. However, existing synthesis techniques\ndo not account for structure during training, and thus often generate\nimplausible and structurally unrealistic shapes. During training, we enforce\nstructural constraints in order to enforce consistency and structure across the\nentire manifold. We propose a novel methodology for training 3D generative\nmodels that incorporates structural information into an end-to-end training\npipeline.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 05:15:49 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Balashova", "Elena", ""], ["Singh", "Vivek", ""], ["Wang", "Jiangping", ""], ["Teixeira", "Brian", ""], ["Chen", "Terrence", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1808.01452", "submitter": "Raghav Gurbaxani", "authors": "Raghav Gurbaxani, Shivank Mishra", "title": "Traits & Transferability of Adversarial Examples against Instance\n  Segmentation & Object Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent advancements in deploying neural networks for image\nclassification, it has been found that adversarial examples are able to fool\nthese models leading them to misclassify the images. Since these models are now\nbeing widely deployed, we provide an insight on the threat of these adversarial\nexamples by evaluating their characteristics and transferability to more\ncomplex models that utilize Image Classification as a subtask.\n  We demonstrate the ineffectiveness of adversarial examples when applied to\nInstance Segmentation & Object Detection models. We show that this\nineffectiveness arises from the inability of adversarial examples to withstand\ntransformations such as scaling or a change in lighting conditions. Moreover,\nwe show that there exists a small threshold below which the adversarial\nproperty is retained while applying these input transformations.\n  Additionally, these attacks demonstrate weak cross-network transferability\nacross neural network architectures, e.g. VGG16 and ResNet50, however, the\nattack may fool both the networks if passed sequentially through networks\nduring its formation.\n  The lack of scalability and transferability challenges the question of how\nadversarial images would be effective in the real world.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 08:57:58 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gurbaxani", "Raghav", ""], ["Mishra", "Shivank", ""]]}, {"id": "1808.01454", "submitter": "Chuanxia Zheng", "authors": "Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai", "title": "T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth\n  Estimation Tasks", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for single-image depth estimation use training datasets with\nreal image-depth pairs or stereo pairs, which are not easy to acquire. We\npropose a framework, trained on synthetic image-depth pairs and unpaired real\nimages, that comprises an image translation network for enhancing realism of\ninput images, followed by a depth prediction network. A key idea is having the\nfirst network act as a wide-spectrum input translator, taking in either\nsynthetic or real images, and ideally producing minimally modified realistic\nimages. This is done via a reconstruction loss when the training input is real,\nand GAN loss when synthetic, removing the need for heuristic\nself-regularization. The second network is trained on a task loss for synthetic\nimage-depth pairs, with extra GAN loss to unify real and synthetic feature\ndistributions. Importantly, the framework can be trained end-to-end, leading to\ngood results, even surpassing early deep-learning methods that use real paired\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 09:10:14 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zheng", "Chuanxia", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""]]}, {"id": "1808.01462", "submitter": "Djamila Aouada", "authors": "Eman Ahmed, Alexandre Saint, Abd El Rahman Shabayek, Kseniya\n  Cherenkova, Rig Das, Gleb Gusev, Djamila Aouada and Bjorn Ottersten", "title": "A survey on Deep Learning Advances on Different 3D Data Representations", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D data is a valuable asset the computer vision filed as it provides rich\ninformation about the full geometry of sensed objects and scenes. Recently,\nwith the availability of both large 3D datasets and computational power, it is\ntoday possible to consider applying deep learning to learn specific tasks on 3D\ndata such as segmentation, recognition and correspondence. Depending on the\nconsidered 3D data representation, different challenges may be foreseen in\nusing existent deep learning architectures. In this work, we provide a\ncomprehensive overview about various 3D data representations highlighting the\ndifference between Euclidean and non-Euclidean ones. We also discuss how Deep\nLearning methods are applied on each representation, analyzing the challenges\nto overcome.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 10:18:55 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 18:20:17 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ahmed", "Eman", ""], ["Saint", "Alexandre", ""], ["Shabayek", "Abd El Rahman", ""], ["Cherenkova", "Kseniya", ""], ["Das", "Rig", ""], ["Gusev", "Gleb", ""], ["Aouada", "Djamila", ""], ["Ottersten", "Bjorn", ""]]}, {"id": "1808.01477", "submitter": "Hacer Yalim Keles", "authors": "Long Ang Lim and Hacer Yalim Keles", "title": "Learning Multi-scale Features for Foreground Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/s10044-019-00845-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground segmentation algorithms aim segmenting moving objects from the\nbackground in a robust way under various challenging scenarios. Encoder-decoder\ntype deep neural networks that are used in this domain recently perform\nimpressive segmentation results. In this work, we propose a novel robust\nencoder-decoder structure neural network that can be trained end-to-end using\nonly a few training examples. The proposed method extends the Feature Pooling\nModule (FPM) of FgSegNet by introducing features fusions inside this module,\nwhich is capable of extracting multi-scale features within images; resulting in\na robust feature pooling against camera motion, which can alleviate the need of\nmulti-scale inputs to the network. Our method outperforms all existing\nstate-of-the-art methods in CDnet2014 dataset by an average overall F-Measure\nof 0.9847. We also evaluate the effectiveness of our method on SBI2015 and UCSD\nBackground Subtraction datasets. The source code of the proposed method is made\navailable at https://github.com/lim-anggun/FgSegNet_v2 .\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 12:55:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lim", "Long Ang", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "1808.01491", "submitter": "Guanbin Li", "authors": "Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong, Liang Lin", "title": "Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining", "comments": "Accepted to ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image rain streaks removal has recently witnessed substantial progress\ndue to the development of deep convolutional neural networks. However, existing\ndeep learning based methods either focus on the entrance and exit of the\nnetwork by decomposing the input image into high and low frequency information\nand employing residual learning to reduce the mapping range, or focus on the\nintroduction of cascaded learning scheme to decompose the task of rain streaks\nremoval into multi-stages. These methods treat the convolutional neural network\nas an encapsulated end-to-end mapping module without deepening into the\nrationality and superiority of neural network design. In this paper, we delve\ninto an effective end-to-end neural network structure for stronger feature\nexpression and spatial correlation learning. Specifically, we propose a\nnon-locally enhanced encoder-decoder network framework, which consists of a\npooling indices embedded encoder-decoder network to efficiently learn\nincreasingly abstract feature representation for more accurate rain streaks\nmodeling while perfectly preserving the image detail. The proposed\nencoder-decoder framework is composed of a series of non-locally enhanced dense\nblocks that are designed to not only fully exploit hierarchical features from\nall the convolutional layers but also well capture the long-distance\ndependencies and structural information. Extensive experiments on synthetic and\nreal datasets demonstrate that the proposed method can effectively remove\nrain-streaks on rainy image of various densities while well preserving the\nimage details, which achieves significant improvements over the recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 15:09:01 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Li", "Guanbin", ""], ["He", "Xiang", ""], ["Zhang", "Wei", ""], ["Chang", "Huiyou", ""], ["Dong", "Le", ""], ["Lin", "Liang", ""]]}, {"id": "1808.01517", "submitter": "Simon Koppers", "authors": "Simon Koppers and Dorit Merhof", "title": "DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DELIMIT is a framework extension for deep learning in diffusion imaging,\nwhich extends the basic framework PyTorch towards spherical signals. Based on\nseveral novel layers, deep learning can be applied to spherical diffusion\nimaging data in a very convenient way. First, two spherical harmonic\ninterpolation layers are added to the extension, which allow to transform the\nsignal from spherical surface space into the spherical harmonic space, and vice\nversa. In addition, a local spherical convolution layer is introduced that adds\nthe possibility to include gradient neighborhood information within the\nnetwork. Furthermore, these extensions can also be utilized for the\npreprocessing of diffusion signals.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 18:26:24 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Koppers", "Simon", ""], ["Merhof", "Dorit", ""]]}, {"id": "1808.01525", "submitter": "Denis Tome", "authors": "Denis Tome, Matteo Toso, Lourdes Agapito, Chris Russell", "title": "Rethinking Pose in 3D: Multi-stage Refinement and Recovery for\n  Markerless Motion Capture", "comments": "International Conference on 3DVision (3dv)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a CNN-based approach for multi-camera markerless motion capture of\nthe human body. Unlike existing methods that first perform pose estimation on\nindividual cameras and generate 3D models as post-processing, our approach\nmakes use of 3D reasoning throughout a multi-stage approach. This novelty\nallows us to use provisional 3D models of human pose to rethink where the\njoints should be located in the image and to recover from past mistakes. Our\nprincipled refinement of 3D human poses lets us make use of image cues, even\nfrom images where we previously misdetected joints, to refine our estimates as\npart of an end-to-end approach. Finally, we demonstrate how the high-quality\noutput of our multi-camera setup can be used as an additional training source\nto improve the accuracy of existing single camera models.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 19:53:30 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Tome", "Denis", ""], ["Toso", "Matteo", ""], ["Agapito", "Lourdes", ""], ["Russell", "Chris", ""]]}, {"id": "1808.01556", "submitter": "Rongtian Ye", "authors": "Rongtian Ye, Fangyu Liu, Liqiang Zhang", "title": "3D Depthwise Convolution: Reducing Model Parameters in 3D Vision Tasks", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard 3D convolution operations require much larger amounts of memory and\ncomputation cost than 2D convolution operations. The fact has hindered the\ndevelopment of deep neural nets in many 3D vision tasks. In this paper, we\ninvestigate the possibility of applying depthwise separable convolutions in 3D\nscenario and introduce the use of 3D depthwise convolution. A 3D depthwise\nconvolution splits a single standard 3D convolution into two separate steps,\nwhich would drastically reduce the number of parameters in 3D convolutions with\nmore than one order of magnitude. We experiment with 3D depthwise convolution\non popular CNN architectures and also compare it with a similar structure\ncalled pseudo-3D convolution. The results demonstrate that, with 3D depthwise\nconvolutions, 3D vision tasks like classification and reconstruction can be\ncarried out with more light-weighted neural networks while still delivering\ncomparable performances.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 03:50:54 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ye", "Rongtian", ""], ["Liu", "Fangyu", ""], ["Zhang", "Liqiang", ""]]}, {"id": "1808.01558", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Hengliang Zhu, Xin Tan, Yangyang Hao, Lizhuang Ma", "title": "Deep Multi-Center Learning for Face Alignment", "comments": "This paper has been accepted by Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2018.11.108", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmarks are highly correlated with each other since a certain\nlandmark can be estimated by its neighboring landmarks. Most of the existing\ndeep learning methods only use one fully-connected layer called shape\nprediction layer to estimate the locations of facial landmarks. In this paper,\nwe propose a novel deep learning framework named Multi-Center Learning with\nmultiple shape prediction layers for face alignment. In particular, each shape\nprediction layer emphasizes on the detection of a certain cluster of\nsemantically relevant landmarks respectively. Challenging landmarks are focused\nfirstly, and each cluster of landmarks is further optimized respectively.\nMoreover, to reduce the model complexity, we propose a model assembling method\nto integrate multiple shape prediction layers into one shape prediction layer.\nExtensive experiments demonstrate that our method is effective for handling\ncomplex occlusions and appearance variations with real-time performance. The\ncode for our method is available at\nhttps://github.com/ZhiwenShao/MCNet-Extension.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 04:01:53 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 06:30:36 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Shao", "Zhiwen", ""], ["Zhu", "Hengliang", ""], ["Tan", "Xin", ""], ["Hao", "Yangyang", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1808.01562", "submitter": "Han Shen", "authors": "Han Shen, Lichao Huang, Chang Huang, Wei Xu", "title": "Tracklet Association Tracker: An End-to-End Learning-based Association\n  Approach for Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional multiple object tracking methods divide the task into two parts:\naffinity learning and data association. The separation of the task requires to\ndefine a hand-crafted training goal in affinity learning stage and a\nhand-crafted cost function of data association stage, which prevents the\ntracking goals from learning directly from the feature. In this paper, we\npresent a new multiple object tracking (MOT) framework with data-driven\nassociation method, named as Tracklet Association Tracker (TAT). The framework\naims at gluing feature learning and data association into a unity by a bi-level\noptimization formulation so that the association results can be directly\nlearned from features. To boost the performance, we also adopt the popular\nhierarchical association and perform the necessary alignment and selection of\nraw detection responses. Our model trains over 20X faster than a similar\napproach, and achieves the state-of-the-art performance on both MOT2016 and\nMOT2017 benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 05:41:45 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Shen", "Han", ""], ["Huang", "Lichao", ""], ["Huang", "Chang", ""], ["Xu", "Wei", ""]]}, {"id": "1808.01571", "submitter": "Dapeng Chen", "authors": "Dapeng Chen, Hongsheng Li, Xihui Liu, Yantao Shen, Zejian Yuan,\n  Xiaogang Wang", "title": "Improving Deep Visual Representation for Person Re-identification by\n  Global and Local Image-language Association", "comments": "ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is an important task that requires learning\ndiscriminative visual features for distinguishing different person identities.\nDiverse auxiliary information has been utilized to improve the visual feature\nlearning. In this paper, we propose to exploit natural language description as\nadditional training supervisions for effective visual features. Compared with\nother auxiliary information, language can describe a specific person from more\ncompact and semantic visual aspects, thus is complementary to the pixel-level\nimage data. Our method not only learns better global visual feature with the\nsupervision of the overall description but also enforces semantic consistencies\nbetween local visual and linguistic features, which is achieved by building\nglobal and local image-language associations. The global image-language\nassociation is established according to the identity labels, while the local\nassociation is based upon the implicit correspondences between image regions\nand noun phrases. Extensive experiments demonstrate the effectiveness of\nemploying language as training supervisions with the two association schemes.\nOur method achieves state-of-the-art performance without utilizing any\nauxiliary information during testing and shows better performance than other\njoint embedding methods for the image-language association.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 07:19:24 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chen", "Dapeng", ""], ["Li", "Hongsheng", ""], ["Liu", "Xihui", ""], ["Shen", "Yantao", ""], ["Yuan", "Zejian", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1808.01575", "submitter": "Yang Feng", "authors": "Yang Feng, Lin Ma, Wei Liu, Tong Zhang, Jiebo Luo", "title": "Video Re-localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been developed to help people find the video contents they\nwant efficiently. However, there are still some unsolved problems in this area.\nFor example, given a query video and a reference video, how to accurately\nlocalize a segment in the reference video such that the segment semantically\ncorresponds to the query video? We define a distinctively new task, namely\n\\textbf{video re-localization}, to address this scenario. Video re-localization\nis an important emerging technology implicating many applications, such as fast\nseeking in videos, video copy detection, video surveillance, etc. Meanwhile, it\nis also a challenging research task because the visual appearance of a semantic\nconcept in videos can have large variations. The first hurdle to clear for the\nvideo re-localization task is the lack of existing datasets. It is labor\nexpensive to collect pairs of videos with semantic coherence or correspondence\nand label the corresponding segments. We first exploit and reorganize the\nvideos in ActivityNet to form a new dataset for video re-localization research,\nwhich consists of about 10,000 videos of diverse visual appearances associated\nwith localized boundary information. Subsequently, we propose an innovative\ncross gated bilinear matching model such that every time-step in the reference\nvideo is matched against the attentively weighted query video. Consequently,\nthe prediction of the starting and ending time is formulated as a\nclassification problem based on the matching results. Extensive experimental\nresults show that the proposed method outperforms the competing methods. Our\ncode is available at: https://github.com/fengyang0317/video_reloc.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 07:52:33 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Feng", "Yang", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1808.01595", "submitter": "Simon Koppers", "authors": "Simon Koppers, Luke Bloy, Jeffrey I. Berman, Chantal M.W. Tax, J.\n  Christopher Edgar and Dorit Merhof", "title": "Spherical Harmonic Residual Network for Diffusion Signal Harmonization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion imaging is an important method in the field of neuroscience, as it\nis sensitive to changes within the tissue microstructure of the human brain.\nHowever, a major challenge when using MRI to derive quantitative measures is\nthat the use of different scanners, as used in multi-site group studies,\nintroduces measurement variability. This can lead to an increased variance in\nquantitative metrics, even if the same brain is scanned.\n  Contrary to the assumption that these characteristics are comparable and\nsimilar, small changes in these values are observed in many clinical studies,\nhence harmonization of the signals is essential.\n  In this paper, we present a method that does not require additional\npreprocessing, such as segmentation or registration, and harmonizes the signal\nbased on a deep learning residual network. For this purpose, a training\ndatabase is required, which consist of the same subjects, scanned on different\nscanners.\n  The results show that harmonized signals are significantly more similar to\nthe ground truth signal compared to no harmonization, but also improve in\ncomparison to another deep learning method. The same effect is also\ndemonstrated in commonly used metrics derived from the diffusion MRI signal.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 11:05:23 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Koppers", "Simon", ""], ["Bloy", "Luke", ""], ["Berman", "Jeffrey I.", ""], ["Tax", "Chantal M. W.", ""], ["Edgar", "J. Christopher", ""], ["Merhof", "Dorit", ""]]}, {"id": "1808.01597", "submitter": "Jiaojiao Zhao", "authors": "Jiaojiao Zhao, Li Liu, Cees G.M. Snoek, Jungong Han, Ling Shao", "title": "Pixel-level Semantics Guided Image Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many image colorization algorithms have recently shown the capability\nof producing plausible color versions from gray-scale photographs, they still\nsuffer from the problems of context confusion and edge color bleeding. To\naddress context confusion, we propose to incorporate the pixel-level object\nsemantics to guide the image colorization. The rationale is that human beings\nperceive and distinguish colors based on the object's semantic categories. We\npropose a hierarchical neural network with two branches. One branch learns what\nthe object is while the other branch learns the object's colors. The network\njointly optimizes a semantic segmentation loss and a colorization loss. To\nattack edge color bleeding we generate more continuous color maps with sharp\nedges by adopting a joint bilateral upsamping layer at inference. Our network\nis trained on PASCAL VOC2012 and COCO-stuff with semantic segmentation labels\nand it produces more realistic and finer results compared to the colorization\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 11:20:12 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhao", "Jiaojiao", ""], ["Liu", "Li", ""], ["Snoek", "Cees G. M.", ""], ["Han", "Jungong", ""], ["Shao", "Ling", ""]]}, {"id": "1808.01606", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Fabio Tosi, Stefano Mattoccia", "title": "Learning monocular depth estimation with unsupervised trinocular\n  assumptions", "comments": "14 pages, 7 figures, 4 tables. Accepted to 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining accurate depth measurements out of a single image represents a\nfascinating solution to 3D sensing. CNNs led to considerable improvements in\nthis field, and recent trends replaced the need for ground-truth labels with\ngeometry-guided image reconstruction signals enabling unsupervised training.\nCurrently, for this purpose, state-of-the-art techniques rely on images\nacquired with a binocular stereo rig to predict inverse depth (i.e., disparity)\naccording to the aforementioned supervision principle. However, these methods\nsuffer from well-known problems near occlusions, left image border, etc\ninherited from the stereo setup. Therefore, in this paper, we tackle these\nissues by moving to a trinocular domain for training. Assuming the central\nimage as the reference, we train a CNN to infer disparity representations\npairing such image with frames on its left and right side. This strategy allows\nobtaining depth maps not affected by typical stereo artifacts. Moreover, being\ntrinocular datasets seldom available, we introduce a novel interleaved training\nprocedure enabling to enforce the trinocular assumption outlined from current\nbinocular datasets. Exhaustive experimental results on the KITTI dataset\nconfirm that our proposal outperforms state-of-the-art methods for unsupervised\nmonocular depth estimation trained on binocular stereo pairs as well as any\nknown methods relying on other cues.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 12:33:06 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Poggi", "Matteo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1808.01607", "submitter": "Nithin Reddy", "authors": "Nithin D Reddy", "title": "Classification of Dermoscopy Images using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is one of the most common forms of cancer and its incidence is\nprojected to rise over the next decade. Artificial intelligence is a viable\nsolution to the issue of providing quality care to patients in areas lacking\naccess to trained dermatologists. Considerable progress has been made in the\nuse of automated applications for accurate classification of skin lesions from\ndigital images. In this manuscript, we discuss the design and implementation of\na deep learning algorithm for classification of dermoscopy images from the\nHAM10000 Dataset. We trained a convolutional neural network based on the\nResNet50 architecture to accurately classify dermoscopy images of skin lesions\ninto one of seven disease categories. Using our custom model, we obtained a\nbalanced accuracy of 91% on the validation dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 12:40:23 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Reddy", "Nithin D", ""]]}, {"id": "1808.01623", "submitter": "Lipeng Ke", "authors": "Lipeng Ke, Ming-Ching Chang, Honggang Qi, Siwei Lyu", "title": "Multi-Scale Supervised Network for Human Pose Estimation", "comments": "Accepted by ICIP2018. arXiv admin note: text overlap with\n  arXiv:1803.09894", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is an important topic in computer vision with many\napplications including gesture and activity recognition. However, pose\nestimation from image is challenging due to appearance variations, occlusions,\nclutter background, and complex activities. To alleviate these problems, we\ndevelop a robust pose estimation method based on the recent deep conv-deconv\nmodules with two improvements: (1) multi-scale supervision of body keypoints,\nand (2) a global regression to improve structural consistency of keypoints. We\nrefine keypoint detection heatmaps using layer-wise multi-scale supervision to\nbetter capture local contexts. Pose inference via keypoint association is\noptimized globally using a regression network at the end. Our method can\neffectively disambiguate keypoint matches in close proximity including the\nmismatch of left-right body parts, and better infer occluded parts.\nExperimental results show that our method achieves competitive performance\namong state-of-the-art methods on the MPII and FLIC datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:13:10 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ke", "Lipeng", ""], ["Chang", "Ming-Ching", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1808.01625", "submitter": "Gr\\'egory Paul", "authors": "Christoph Mayer, Radu Timofte and Gr\\'egory Paul", "title": "Towards Closing the Gap in Weakly Supervised Semantic Segmentation with\n  DCNNs: Combining Local and Global Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating training sets for deep convolutional neural networks (DCNNs) is a\nbottleneck for modern real-world applications. This is a demanding task for\napplications where annotating training data is costly, such as in semantic\nsegmentation. In the literature, there is still a gap between the performance\nachieved by a network trained on full and on weak annotations. In this paper,\nwe establish a strategy to measure this gap and to identify the ingredients\nnecessary to reduce it.\n  On scribbles, we establish new state-of-the-art results: we obtain a mIoU of\n75.6% without, and 75.7% with CRF post-processing. We reduce the gap by 64.2%\nwhereas the current state-of-the-art reduces it only by 57.5%. Thanks to a\nsystematic study of the different ingredients involved in the weakly supervised\nscenario and an original experimental strategy, we unravel a counter-intuitive\nmechanism that is simple and amenable to generalisations to other\nweakly-supervised scenarios: averaging poor local predicted annotations with\nthe baseline ones and reuse them for training a DCNN yields new\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 14:19:19 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 22:57:42 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 22:39:29 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Mayer", "Christoph", ""], ["Timofte", "Radu", ""], ["Paul", "Gr\u00e9gory", ""]]}, {"id": "1808.01634", "submitter": "Fengdong Sun", "authors": "Fengdong Sun, Wenhui Li, Yuanyuan Guan", "title": "Self-Attention Recurrent Network for Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature maps in deep neural network generally contain different semantics.\nExisting methods often omit their characteristics that may lead to sub-optimal\nresults. In this paper, we propose a novel end-to-end deep saliency network\nwhich could effectively utilize multi-scale feature maps according to their\ncharacteristics. Shallow layers often contain more local information, and deep\nlayers have advantages in global semantics. Therefore, the network generates\nelaborate saliency maps by enhancing local and global information of feature\nmaps in different layers. On one hand, local information of shallow layers is\nenhanced by a recurrent structure which shared convolution kernel at different\ntime steps. On the other hand, global information of deep layers is utilized by\na self-attention module, which generates different attention weights for\nsalient objects and backgrounds thus achieve better performance. Experimental\nresults on four widely used datasets demonstrate that our method has advantages\nin performance over existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 15:31:22 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Sun", "Fengdong", ""], ["Li", "Wenhui", ""], ["Guan", "Yuanyuan", ""]]}, {"id": "1808.01642", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Multi-Objective Cognitive Model: a supervised approach for multi-subject\n  fMRI analysis", "comments": "Neuroinformatics, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to decode the human brain, Multivariate Pattern (MVP) classification\ngenerates cognitive models by using functional Magnetic Resonance Imaging\n(fMRI) datasets. As a standard pipeline in the MVP analysis, brain patterns in\nmulti-subject fMRI dataset must be mapped to a shared space and then a\nclassification model is generated by employing the mapped patterns. However,\nthe MVP models may not provide stable performance on a new fMRI dataset because\nthe standard pipeline uses disjoint steps for generating these models. Indeed,\neach step in the pipeline includes an objective function with independent\noptimization approach, where the best solution of each step may not be optimum\nfor the next steps. For tackling the mentioned issue, this paper introduces the\nMulti-Objective Cognitive Model (MOCM) that utilizes an integrated objective\nfunction for MVP analysis rather than just using those disjoint steps. For\nsolving the integrated problem, we proposed a customized multi-objective\noptimization approach, where all possible solutions are firstly generated, and\nthen our method ranks and selects the robust solutions as the final results.\nEmpirical studies confirm that the proposed method can generate superior\nperformance in comparison with other techniques.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 16:19:56 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1808.01673", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Nishant Ravikumar, Andreas Maier", "title": "Dilated Convolutions in Neural Networks for Left Atrial Segmentation in\n  3D Gadolinium Enhanced-MRI", "comments": "Accepted in SATCOM-MICCAI 2018 Workshop", "journal-ref": "STACOM 2018 Proceedings", "doi": "10.1007/978-3-030-12029-0_35", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the left atrial chamber and assessing its morphology, are\nessential for improving our understanding of atrial fibrillation, the most\ncommon type of cardiac arrhythmia. Automation of this process in 3D gadolinium\nenhanced-MRI (GE-MRI) data is desirable, as manual delineation is\ntime-consuming, challenging and observer-dependent. Recently, deep\nconvolutional neural networks (CNNs) have gained tremendous traction and\nachieved state-of-the-art results in medical image segmentation. However, it is\ndifficult to incorporate local and global information without using contracting\n(pooling) layers, which in turn reduces segmentation accuracy for smaller\nstructures. In this paper, we propose a 3D CNN for volumetric segmentation of\nthe left atrial chamber in LGE-MRI. Our network is based on the well known\nU-Net architecture. We employ a 3D fully convolutional network, with dilated\nconvolutions in the lowest level of the network, and residual connections\nbetween encoder blocks to incorporate local and global knowledge. The results\nshow that including global context through the use of dilated convolutions,\nhelps in domain adaptation, and the overall segmentation accuracy is improved\nin comparison to a 3D U-Net.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 19:04:17 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1808.01675", "submitter": "Zhangsihao Yang", "authors": "Zhangsihao Yang, Haoliang Jiang, Zou Lan", "title": "3D Conceptual Design Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a data-driven methodology to achieve a fast design\nsupport, in order to generate or develop novel designs covering multiple object\ncategories. This methodology implements two state-of-the-art Variational\nAutoencoder dealing with 3D model data. Our methodology constructs a\nself-defined loss function. The loss function, containing the outputs of\ncertain layers in the autoencoder, obtains combination of different latent\nfeatures from different 3D model categories.\n  Additionally, this article provide detail explanation to utilize the\nPrinceton ModelNet40 database, a comprehensive clean collection of 3D CAD\nmodels for objects. After convert the original 3D mesh file to voxel and point\ncloud data type, we enable to feed our autoencoder with data of the same size\nof dimension. The novelty of this work is to leverage the power of deep\nlearning methods as an efficient latent feature extractor to explore unknown\ndesigning areas. Through this project, we expect the output can show a clear\nand smooth interpretation of model from different categories to develop a fast\ndesign support to generate novel shapes. This final report will explore 1) the\ntheoretical ideas, 2) the progresses to implement Variantional Autoencoder to\nattain implicit features from input shapes, 3) the results of output shapes\nduring training in selected domains of both 3D voxel data and 3D point cloud\ndata, and 4) our conclusion and future work to achieve the more outstanding\ngoal.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 19:09:12 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Yang", "Zhangsihao", ""], ["Jiang", "Haoliang", ""], ["Lan", "Zou", ""]]}, {"id": "1808.01676", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Shreyas Malakarjun Patil, Nishant Ravikumar, Andreas\n  Maier", "title": "A Multi-task Framework for Skin Lesion Detection and Segmentation", "comments": "Accepted in ISIC-MICCAI 2018 Workshop", "journal-ref": "OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic\n  Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and segmentation of skin lesions is crucial for timely\ndiagnosis and treatment, necessary to improve the survival rate of patients.\nHowever, manual delineation is time consuming and subject to intra- and\ninter-observer variations among dermatologists. This underlines the need for an\naccurate and automatic approach to skin lesion segmentation. To tackle this\nissue, we propose a multi-task convolutional neural network (CNN) based, joint\ndetection and segmentation framework, designed to initially localize the lesion\nand subsequently, segment it. A `Faster region-based convolutional neural\nnetwork' (Faster-RCNN) which comprises a region proposal network (RPN), is used\nto generate bounding boxes/region proposals, for lesion localization in each\nimage. The proposed regions are subsequently refined using a softmax classifier\nand a bounding-box regressor. The refined bounding boxes are finally cropped\nand segmented using `SkinNet', a modified version of U-Net. We trained and\nevaluated the performance of our network, using the ISBI 2017 challenge and the\nPH2 datasets, and compared it with the state-of-the-art, using the official\ntest data released as part of the challenge for the former. Our approach\noutperformed others in terms of Dice coefficients ($>0.93$), Jaccard index\n($>0.88$), accuracy ($>0.96$) and sensitivity ($>0.95$), across five-fold cross\nvalidation experiments.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 19:13:15 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Patil", "Shreyas Malakarjun", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1808.01680", "submitter": "Toan Nguyen", "authors": "Toan Nguyen, Aditi Roy, Nasir Memon", "title": "Kid on The Phone! Toward Automatic Detection of Children on Mobile\n  Devices", "comments": "Under peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that children can be exposed to smart devices at a very\nearly age. This has important implications on research in children-computer\ninteraction, children online safety and early education. Many systems have been\nbuilt based on such research. In this work, we present multiple techniques to\nautomatically detect the presence of a child on a smart device, which could be\nused as the first step on such systems. Our methods distinguish children from\nadults based on behavioral differences while operating a touch-enabled modern\ncomputing device. Behavioral differences are extracted from data recorded by\nthe touchscreen and built-in sensors. To evaluate the effectiveness of the\nproposed methods, a new data set has been created from 50 children and adults\nwho interacted with off-the-shelf applications on smart phones. Results show\nthat it is possible to achieve 99% accuracy and less than 0.5% error rate after\n8 consecutive touch gestures using only touch information or 5 seconds of\nsensor reading. If information is used from multiple sensors, then only after 3\ngestures, similar performance could be achieved.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 19:59:35 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Nguyen", "Toan", ""], ["Roy", "Aditi", ""], ["Memon", "Nasir", ""]]}, {"id": "1808.01686", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Elin Farnell, Michael Kirby, Chris Peterson", "title": "Too many secants: a hierarchical approach to secant-based dimensionality\n  reduction on large data sets", "comments": "To appear in the Proceedings of the 2018 IEEE High Performance\n  Extreme Computing Conference, Waltham, MA USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in many data analysis settings is the problem of\ndiscerning the \"natural\" dimension of a data set. That is, when a data set is\ndrawn from a manifold (possibly with noise), a meaningful aspect of the data is\nthe dimension of that manifold. Various approaches exist for estimating this\ndimension, such as the method of Secant-Avoidance Projection (SAP).\nIntuitively, the SAP algorithm seeks to determine a projection which best\npreserves the lengths of all secants between points in a data set; by applying\nthe algorithm to find the best projections to vector spaces of various\ndimensions, one may infer the dimension of the manifold of origination. That\nis, one may learn the dimension at which it is possible to construct a\ndiffeomorphic copy of the data in a lower-dimensional Euclidean space. Using\nWhitney's embedding theorem, we can relate this information to the natural\ndimension of the data. A drawback of the SAP algorithm is that a data set with\n$T$ points has $O(T^2)$ secants, making the computation and storage of all\nsecants infeasible for very large data sets. In this paper, we propose a novel\nalgorithm that generalizes the SAP algorithm with an emphasis on addressing\nthis issue. That is, we propose a hierarchical secant-based\ndimensionality-reduction method, which can be employed for data sets where\nexplicitly calculating all secants is not feasible.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 21:27:32 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Kvinge", "Henry", ""], ["Farnell", "Elin", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""]]}, {"id": "1808.01688", "submitter": "Huan Zhang", "authors": "Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, Yupeng Gao", "title": "Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the\n  Robustness of 18 Deep Image Classification Models", "comments": "Accepted by the European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction accuracy has been the long-lasting and sole standard for\ncomparing the performance of different image classification models, including\nthe ImageNet competition. However, recent studies have highlighted the lack of\nrobustness in well-trained deep neural networks to adversarial examples.\nVisually imperceptible perturbations to natural images can easily be crafted\nand mislead the image classifiers towards misclassification. To demystify the\ntrade-offs between robustness and accuracy, in this paper we thoroughly\nbenchmark 18 ImageNet models using multiple robustness metrics, including the\ndistortion, success rate and transferability of adversarial examples between\n306 pairs of models. Our extensive experimental results reveal several new\ninsights: (1) linear scaling law - the empirical $\\ell_2$ and $\\ell_\\infty$\ndistortion metrics scale linearly with the logarithm of classification error;\n(2) model architecture is a more critical factor to robustness than model size,\nand the disclosed accuracy-robustness Pareto frontier can be used as an\nevaluation criterion for ImageNet model designers; (3) for a similar network\narchitecture, increasing network depth slightly improves robustness in\n$\\ell_\\infty$ distortion; (4) there exist models (in VGG family) that exhibit\nhigh adversarial transferability, while most adversarial examples crafted from\none model can only be transferred within the same family. Experiment code is\npublicly available at \\url{https://github.com/huanzhang12/Adversarial_Survey}.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 21:43:01 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 00:35:26 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Su", "Dong", ""], ["Zhang", "Huan", ""], ["Chen", "Hongge", ""], ["Yi", "Jinfeng", ""], ["Chen", "Pin-Yu", ""], ["Gao", "Yupeng", ""]]}, {"id": "1808.01694", "submitter": "Nils Gessert", "authors": "Nils Gessert, Thilo Sentker, Frederic Madesta, R\\\"udiger Schmitz,\n  Helge Kniep, Ivo Baltruschat, Ren\\'e Werner, Alexander Schlaefer", "title": "Skin Lesion Diagnosis using Ensembles, Unscaled Multi-Crop Evaluation\n  and Loss Weighting", "comments": "ISIC Skin Image Analysis Workshop and Challenge @ MICCAI 2018. Second\n  place at challenge, best with public data, see\n  https://challenge2018.isic-archive.com/leaderboards/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the methods of our submission to the ISIC 2018\nchallenge for skin lesion diagnosis (Task 3). The dataset consists of 10000\nimages with seven image-level classes to be distinguished by an automated\nalgorithm. We employ an ensemble of convolutional neural networks for this\ntask. In particular, we fine-tune pretrained state-of-the-art deep learning\nmodels such as Densenet, SENet and ResNeXt. We identify heavy class imbalance\nas a key problem for this challenge and consider multiple balancing approaches\nsuch as loss weighting and balanced batch sampling. Another important feature\nof our pipeline is the use of a vast amount of unscaled crops for evaluation.\nLast, we consider meta learning approaches for the final predictions. Our team\nplaced second at the challenge while being the best approach using only\npublicly available data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 22:23:23 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gessert", "Nils", ""], ["Sentker", "Thilo", ""], ["Madesta", "Frederic", ""], ["Schmitz", "R\u00fcdiger", ""], ["Kniep", "Helge", ""], ["Baltruschat", "Ivo", ""], ["Werner", "Ren\u00e9", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1808.01721", "submitter": "Bin Chen Dr.", "authors": "Bin Chen, Wei Guo, Bin Li, Rober K. F. Teng, Mingjun Dai, Jianping\n  Luo, Hui Wang", "title": "A Study of Deep Feature Fusion based Methods for Classifying Multi-lead\n  ECG", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic classification method has been studied to effectively detect and\nrecognize Electrocardiogram (ECG). Based on the synchronizing and orthogonal\nrelationships of multiple leads, we propose a Multi-branch Convolution and\nResidual Network (MBCRNet) with three kinds of feature fusion methods for\nautomatic detection of normal and abnormal ECG signals. Experiments are\nconducted on the Chinese Cardiovascular Disease Database (CCDD). Through\n10-fold cross-validation, we achieve an average accuracy of 87.04% and a\nsensitivity of 89.93%, which outperforms previous methods under the same\ndatabase. It is also shown that the multi-lead feature fusion network can\nimprove the classification accuracy over the network only with the single lead\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 03:23:53 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chen", "Bin", ""], ["Guo", "Wei", ""], ["Li", "Bin", ""], ["Teng", "Rober K. F.", ""], ["Dai", "Mingjun", ""], ["Luo", "Jianping", ""], ["Wang", "Hui", ""]]}, {"id": "1808.01725", "submitter": "Wu Tz-Ying", "authors": "Tz-Ying Wu, Juan-Ting Lin, Tsun-Hsuang Wang, Chan-Wei Hu, Juan Carlos\n  Niebles, Min Sun", "title": "Liquid Pouring Monitoring via Rich Sensory Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have the amazing ability to perform very subtle manipulation task\nusing a closed-loop control system with imprecise mechanics (i.e., our body\nparts) but rich sensory information (e.g., vision, tactile, etc.). In the\nclosed-loop system, the ability to monitor the state of the task via rich\nsensory information is important but often less studied. In this work, we take\nliquid pouring as a concrete example and aim at learning to continuously\nmonitor whether liquid pouring is successful (e.g., no spilling) or not via\nrich sensory inputs. We mimic humans' rich sensories using synchronized\nobservation from a chest-mounted camera and a wrist-mounted IMU sensor. Given\nmany success and failure demonstrations of liquid pouring, we train a\nhierarchical LSTM with late fusion for monitoring. To improve the robustness of\nthe system, we propose two auxiliary tasks during training: inferring (1) the\ninitial state of containers and (2) forecasting the one-step future 3D\ntrajectory of the hand with an adversarial training procedure. These tasks\nencourage our method to learn representation sensitive to container states and\nhow objects are manipulated in 3D. With these novel components, our method\nachieves ~8% and ~11% better monitoring accuracy than the baseline method\nwithout auxiliary tasks on unseen containers and unseen users respectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 03:59:00 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wu", "Tz-Ying", ""], ["Lin", "Juan-Ting", ""], ["Wang", "Tsun-Hsuang", ""], ["Hu", "Chan-Wei", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1808.01727", "submitter": "Sujoy Paul", "authors": "Sujoy Paul and Sourya Roy and Amit K. Roy-Chowdhury", "title": "Incorporating Scalability in Unsupervised Spatio-Temporal Feature\n  Learning", "comments": "International Conference on Acoustics, Speech, and Signal Processing\n  (ICASSP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are efficient learning machines which leverage upon a\nlarge amount of manually labeled data for learning discriminative features.\nHowever, acquiring substantial amount of supervised data, especially for videos\ncan be a tedious job across various computer vision tasks. This necessitates\nlearning of visual features from videos in an unsupervised setting. In this\npaper, we propose a computationally simple, yet effective, framework to learn\nspatio-temporal feature embedding from unlabeled videos. We train a\nConvolutional 3D Siamese network using positive and negative pairs mined from\nvideos under certain probabilistic assumptions. Experimental results on three\ndatasets demonstrate that our proposed framework is able to learn weights which\ncan be used for same as well as cross dataset and tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 04:11:07 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 00:44:53 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Paul", "Sujoy", ""], ["Roy", "Sourya", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1808.01752", "submitter": "Chuanqi Tan", "authors": "Chuanqi Tan, Fuchun Sun and Wenchang Zhang", "title": "Deep Transfer Learning for EEG-based Brain Computer Interface", "comments": "In Proceedings of IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP) 2018, 15-20 April 2018, Alberta, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalography classifier is the most important component of\nbrain-computer interface based systems. There are two major problems hindering\nthe improvement of it. First, traditional methods do not fully exploit\nmultimodal information. Second, large-scale annotated EEG datasets are almost\nimpossible to acquire because biological data acquisition is challenging and\nquality annotation is costly. Herein, we propose a novel deep transfer learning\napproach to solve these two problems. First, we model cognitive events based on\nEEG data by characterizing the data using EEG optical flow, which is designed\nto preserve multimodal EEG information in a uniform representation. Second, we\ndesign a deep transfer learning framework which is suitable for transferring\nknowledge by joint training, which contains a adversarial network and a special\nloss function. The experiments demonstrate that our approach, when applied to\nEEG classification tasks, has many advantages, such as robustness and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 07:23:34 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Tan", "Chuanqi", ""], ["Sun", "Fuchun", ""], ["Zhang", "Wenchang", ""]]}, {"id": "1808.01753", "submitter": "Vivek B S", "authors": "Vivek B.S., Konda Reddy Mopuri, and R. Venkatesh Babu", "title": "Gray-box Adversarial Training", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial samples are perturbed inputs crafted to mislead the machine\nlearning systems. A training mechanism, called adversarial training, which\npresents adversarial samples along with clean samples has been introduced to\nlearn robust models. In order to scale adversarial training for large datasets,\nthese perturbations can only be crafted using fast and simple methods (e.g.,\ngradient ascent). However, it is shown that adversarial training converges to a\ndegenerate minimum, where the model appears to be robust by generating weaker\nadversaries. As a result, the models are vulnerable to simple black-box\nattacks. In this paper we, (i) demonstrate the shortcomings of existing\nevaluation policy, (ii) introduce novel variants of white-box and black-box\nattacks, dubbed gray-box adversarial attacks\" based on which we propose novel\nevaluation method to assess the robustness of the learned models, and (iii)\npropose a novel variant of adversarial training, named Graybox Adversarial\nTraining\" that uses intermediate versions of the models to seed the\nadversaries. Experimental evaluation demonstrates that the models trained using\nour method exhibit better robustness compared to both undefended and\nadversarially trained model\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 07:26:44 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["S.", "Vivek B.", ""], ["Mopuri", "Konda Reddy", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1808.01785", "submitter": "Sibo Song", "authors": "Sibo Song, Yueru Chen, Ngai-Man Cheung, C.-C. Jay Kuo", "title": "Defense Against Adversarial Attacks with Saak Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known to be vulnerable to adversarial\nperturbations, which imposes a serious threat to DNN-based decision systems. In\nthis paper, we propose to apply the lossy Saak transform to adversarially\nperturbed images as a preprocessing tool to defend against adversarial attacks.\nSaak transform is a recently-proposed state-of-the-art for computing the\nspatial-spectral representations of input images. Empirically, we observe that\noutputs of the Saak transform are very discriminative in differentiating\nadversarial examples from clean ones. Therefore, we propose a Saak transform\nbased preprocessing method with three steps: 1) transforming an input image to\na joint spatial-spectral representation via the forward Saak transform, 2)\napply filtering to its high-frequency components, and, 3) reconstructing the\nimage via the inverse Saak transform. The processed image is found to be robust\nagainst adversarial perturbations. We conduct extensive experiments to\ninvestigate various settings of the Saak transform and filtering functions.\nWithout harming the decision performance on clean images, our method\noutperforms state-of-the-art adversarial defense methods by a substantial\nmargin on both the CIFAR-10 and ImageNet datasets. Importantly, our results\nsuggest that adversarial perturbations can be effectively and efficiently\ndefended using state-of-the-art frequency analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 09:01:41 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Song", "Sibo", ""], ["Chen", "Yueru", ""], ["Cheung", "Ngai-Man", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1808.01821", "submitter": "Kohei Uehara", "authors": "Kohei Uehara, Antonio Tejero-De-Pablos, Yoshitaka Ushiku, Tatsuya\n  Harada", "title": "Visual Question Generation for Class Acquisition of Unknown Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image recognition methods only consider objects belonging to\nalready learned classes. However, since training a recognition model with every\nobject class in the world is unfeasible, a way of getting information on\nunknown objects (i.e., objects whose class has not been learned) is necessary.\nA way for an image recognition system to learn new classes could be asking a\nhuman about objects that are unknown. In this paper, we propose a method for\ngenerating questions about unknown objects in an image, as means to get\ninformation about classes that have not been learned. Our method consists of a\nmodule for proposing objects, a module for identifying unknown objects, and a\nmodule for generating questions about unknown objects. The experimental results\nvia human evaluation show that our method can successfully get information\nabout unknown objects in an image dataset. Our code and dataset are available\nat https://github.com/mil-tokyo/vqg-unknown.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 11:14:35 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Uehara", "Kohei", ""], ["Tejero-De-Pablos", "Antonio", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1808.01834", "submitter": "Lingni Ma", "authors": "Lingni Ma and J\\\"org St\\\"uckler and Tao Wu and Daniel Cremers", "title": "Detailed Dense Inference with Convolutional Neural Networks via Discrete\n  Wavelet Transform", "comments": "This work was first submitted to NIPS 2017, May 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense pixelwise prediction such as semantic segmentation is an up-to-date\nchallenge for deep convolutional neural networks (CNNs). Many state-of-the-art\napproaches either tackle the loss of high-resolution information due to pooling\nin the encoder stage, or use dilated convolutions or high-resolution lanes to\nmaintain detailed feature maps and predictions. Motivated by the structural\nanalogy between multi-resolution wavelet analysis and the pooling/unpooling\nlayers of CNNs, we introduce discrete wavelet transform (DWT) into the CNN\nencoder-decoder architecture and propose WCNN. The high-frequency wavelet\ncoefficients are computed at encoder, which are later used at the decoder to\nunpooled jointly with coarse-resolution feature maps through the inverse DWT.\nThe DWT/iDWT is further used to develop two wavelet pyramids to capture the\nglobal context, where the multi-resolution DWT is applied to successively\nreduce the spatial resolution and increase the receptive field. Experiment with\nthe Cityscape dataset, the proposed WCNNs are computationally efficient and\nyield improvements the accuracy for high-resolution dense pixelwise prediction.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 11:57:15 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ma", "Lingni", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Wu", "Tao", ""], ["Cremers", "Daniel", ""]]}, {"id": "1808.01837", "submitter": "Stephanie Tan", "authors": "Stephanie Tan, Hayley Hung", "title": "Improving Temporal Interpolation of Head and Body Pose using Gaussian\n  Process Regression in a Matrix Completion Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a model for head and body pose estimation (HBPE) when\nlabelled samples are highly sparse. The current state-of-the-art multimodal\napproach to HBPE utilizes the matrix completion method in a transductive\nsetting to predict pose labels for unobserved samples. Based on this approach,\nthe proposed method tackles HBPE when manually annotated ground truth labels\nare temporally sparse. We posit that the current state of the art approach\noversimplifies the temporal sparsity assumption by using Laplacian smoothing.\nOur final solution uses: i) Gaussian process regression in place of Laplacian\nsmoothing, ii) head and body coupling, and iii) nuclear norm minimization in\nthe matrix completion setting. The model is applied to the challenging SALSA\ndataset for benchmark against the state-of-the-art method. Our presented\nformulation outperforms the state-of-the-art significantly in this particular\nsetting, e.g. at 5% ground truth labels as training data, head pose accuracy\nand body pose accuracy is approximately 62% and 70%, respectively. As well as\nfitting a more flexible model to missing labels in time, we posit that our\napproach also loosens the head and body coupling constraint, allowing for a\nmore expressive model of the head and body pose typically seen during\nconversational interaction in groups. This provides a new baseline to improve\nupon for future integration of multimodal sensor data for the purpose of HBPE.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 12:05:53 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Tan", "Stephanie", ""], ["Hung", "Hayley", ""]]}, {"id": "1808.01838", "submitter": "Eddy Ilg", "authors": "Eddy Ilg, Tonmoy Saikia, Margret Keuper and Thomas Brox", "title": "Occlusions, Motion and Depth Boundaries with a Generic Network for\n  Disparity, Optical Flow or Scene Flow Estimation", "comments": "Accepted to ECCV 2018 as poster. See video at:\n  https://www.youtube.com/watch?v=SwOdSaBRysI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusions play an important role in disparity and optical flow estimation,\nsince matching costs are not available in occluded areas and occlusions\nindicate depth or motion boundaries. Moreover, occlusions are relevant for\nmotion segmentation and scene flow estimation. In this paper, we present an\nefficient learning-based approach to estimate occlusion areas jointly with\ndisparities or optical flow. The estimated occlusions and motion boundaries\nclearly improve over the state-of-the-art. Moreover, we present networks with\nstate-of-the-art performance on the popular KITTI benchmark and good generic\nperformance. Making use of the estimated occlusions, we also show improved\nresults on motion segmentation and scene flow estimation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 12:10:50 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 09:26:30 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Ilg", "Eddy", ""], ["Saikia", "Tonmoy", ""], ["Keuper", "Margret", ""], ["Brox", "Thomas", ""]]}, {"id": "1808.01853", "submitter": "Sungsoo Ha", "authors": "Sungsoo Ha, Klaus Mueller", "title": "Metal Artifact Reduction in Cone-Beam X-Ray CT via Ray Profile\n  Correction", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computed tomography (CT), metal implants increase the inconsistencies\nbetween the measured data and the linear attenuation assumption made by\nanalytic CT reconstruction algorithms. The inconsistencies give rise to dark\nand bright bands and streaks in the reconstructed image, collectively called\nmetal artifacts. These artifacts make it difficult for radiologists to render\ncorrect diagnostic decisions. We describe a data-driven metal artifact\nreduction (MAR) algorithm for image-guided spine surgery that applies to\nscenarios in which a prior CT scan of the patient is available. We tested the\nproposed method with two clinical datasets that were both obtained during spine\nsurgery. Using the proposed method, we were not only able to remove the dark\nand bright streaks caused by the implanted screws but we also recovered the\nanatomical structures hidden by these artifacts. This results in an improved\ncapability of surgeons to confirm the correctness of the implanted pedicle\nscrew placements.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 12:41:44 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ha", "Sungsoo", ""], ["Mueller", "Klaus", ""]]}, {"id": "1808.01900", "submitter": "Huizhong Zhou", "authors": "Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox", "title": "DeepTAM: Deep Tracking and Mapping", "comments": "Accepted to ECCV 2018 as oral. Project page:\n  https://lmb.informatik.uni-freiburg.de/people/zhouh/deeptam/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for keyframe-based dense camera tracking and depth map\nestimation that is entirely learned. For tracking, we estimate small pose\nincrements between the current camera image and a synthetic viewpoint. This\nsignificantly simplifies the learning problem and alleviates the dataset bias\nfor camera motions. Further, we show that generating a large number of pose\nhypotheses leads to more accurate predictions. For mapping, we accumulate\ninformation in a cost volume centered at the current depth estimate. The\nmapping network then combines the cost volume and the keyframe image to update\nthe depth prediction, thereby effectively making use of depth measurements and\nimage-based priors. Our approach yields state-of-the-art results with few\nimages and is robust with respect to noisy camera poses. We demonstrate that\nthe performance of our 6 DOF tracking competes with RGB-D tracking algorithms.\nWe compare favorably against strong classic and deep learning powered dense\ndepth algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 13:43:31 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 16:44:49 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zhou", "Huizhong", ""], ["Ummenhofer", "Benjamin", ""], ["Brox", "Thomas", ""]]}, {"id": "1808.01911", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang, Junbin Gao, Xue Li", "title": "Where-and-When to Look: Deep Siamese Attention Networks for Video-based\n  Person Re-identification", "comments": "Appearing in IEEE Transactions on Multimedia. arXiv admin note: text\n  overlap with arXiv:1606.01609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (re-id) is a central application in\nsurveillance systems with significant concern in security. Matching persons\nacross disjoint camera views in their video fragments is inherently challenging\ndue to the large visual variations and uncontrolled frame rates. There are two\nsteps crucial to person re-id, namely discriminative feature learning and\nmetric learning. However, existing approaches consider the two steps\nindependently, and they do not make full use of the temporal and spatial\ninformation in videos. In this paper, we propose a Siamese attention\narchitecture that jointly learns spatiotemporal video representations and their\nsimilarity metrics. The network extracts local convolutional features from\nregions of each frame, and enhance their discriminative capability by focusing\non distinct regions when measuring the similarity with another pedestrian\nvideo. The attention mechanism is embedded into spatial gated recurrent units\nto selectively propagate relevant features and memorize their spatial\ndependencies through the network. The model essentially learns which parts\n(\\emph{where}) from which frames (\\emph{when}) are relevant and distinctive for\nmatching persons and attaches higher importance therein. The proposed Siamese\nmodel is end-to-end trainable to jointly learn comparable hidden\nrepresentations for paired pedestrian videos and their similarity value.\nExtensive experiments on three benchmark datasets show the effectiveness of\neach component of the proposed deep network while outperforming\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 01:07:03 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 04:09:48 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Gao", "Junbin", ""], ["Li", "Xue", ""]]}, {"id": "1808.01942", "submitter": "Xiang Xu", "authors": "Xiang Xu, Xiaofang Wang, Kris M. Kitani", "title": "Error Correction Maximization for Deep Image Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use the concept of the Hamming bound to derive the optimal\ncriteria for learning hash codes with a deep network. In particular, when the\nnumber of binary hash codes (typically the number of image categories) and code\nlength are known, it is possible to derive an upper bound on the minimum\nHamming distance between the hash codes. This upper bound can then be used to\ndefine the loss function for learning hash codes. By encouraging the margin\n(minimum Hamming distance) between the hash codes of different image categories\nto match the upper bound, we are able to learn theoretically optimal hash\ncodes. Our experiments show that our method significantly outperforms competing\ndeep learning-based approaches and obtains top performance on benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:48:15 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Xu", "Xiang", ""], ["Wang", "Xiaofang", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1808.01944", "submitter": "Nicolo' Savioli", "authors": "Nicol\\'o Savioli, Giovanni Montana, Pablo Lamata", "title": "V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial\n  Segmentation", "comments": "9 pages, 4 figures, In Proceedings of MICCAI 2018 Atrial Segmentation\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder\nthat causes changes in the anatomy of the atria. A better characterization of\nthese changes is desirable for the definition of clinical biomarkers,\nfurthermore, thus there is a need for its fully automatic segmentation from\nclinical images. In this work, we present an architecture based on\n3D-convolution kernels, a Volumetric Fully Convolution Neural Network (V-FCNN),\nable to segment the entire volume in a one-shot, and consequently integrate the\nimplicit spatial redundancy present in high-resolution images. A loss function\nbased on the mixture of both Mean Square Error (MSE) and Dice Loss (DL) is\nused, in an attempt to combine the ability to capture the bulk shape as well as\nthe reduction of local errors products by over-segmentation. Results\ndemonstrate a reasonable performance in the middle region of the atria along\nwith the impact of the challenges of capturing the variability of the pulmonary\nveins or the identification of the valve plane that separates the atria to the\nventricle. A final dice of $92.5\\%$ in $54$ patients ($4752$ atria test slices\nin total) is shown.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:51:33 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 09:27:11 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Savioli", "Nicol\u00f3", ""], ["Montana", "Giovanni", ""], ["Lamata", "Pablo", ""]]}, {"id": "1808.01946", "submitter": "Benjam\\'in Guti\\'errez-Becker", "authors": "Benjamin Gutierrez-Becker and Sergios Gatidis and Daniel Gutmann and\n  Annette Peters and Christopher Schlett Fabian Bamberg and Christian Wachinger", "title": "Deep Shape Analysis on Abdominal Organs for Diabetes Prediction", "comments": "Accepted for publication at the ShapeMI MICCAI Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological analysis of organs based on images is a key task in medical\nimaging computing. Several approaches have been proposed for the quantitative\nassessment of morphological changes, and they have been widely used for the\nanalysis of the effects of aging, disease and other factors in organ\nmorphology. In this work, we propose a deep neural network for predicting\ndiabetes on abdominal shapes. The network directly operates on raw point clouds\nwithout requiring mesh processing or shape alignment. Instead of relying on\nhand-crafted shape descriptors, an optimal representation is learned in the\nend-to-end training stage of the network. For comparison, we extend the\nstate-of-the-art shape descriptor BrainPrint to the AbdomenPrint. Our results\ndemonstrate that the network learns shape representations that better separates\nhealthy and diabetic individuals than traditional representations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:52:50 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gutierrez-Becker", "Benjamin", ""], ["Gatidis", "Sergios", ""], ["Gutmann", "Daniel", ""], ["Peters", "Annette", ""], ["Bamberg", "Christopher Schlett Fabian", ""], ["Wachinger", "Christian", ""]]}, {"id": "1808.01976", "submitter": "Jonas Rauber", "authors": "Wieland Brendel, Jonas Rauber, Alexey Kurakin, Nicolas Papernot, Behar\n  Veliqi, Marcel Salath\\'e, Sharada P. Mohanty, Matthias Bethge", "title": "Adversarial Vision Challenge", "comments": "https://www.crowdai.org/challenges/adversarial-vision-challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate\nmeasurable progress towards robust machine vision models and more generally\napplicable adversarial attacks. This document is an updated version of our\ncompetition proposal that was accepted in the competition track of 32nd\nConference on Neural Information Processing Systems (NIPS 2018).\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:13:43 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 18:21:49 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Brendel", "Wieland", ""], ["Rauber", "Jonas", ""], ["Kurakin", "Alexey", ""], ["Papernot", "Nicolas", ""], ["Veliqi", "Behar", ""], ["Salath\u00e9", "Marcel", ""], ["Mohanty", "Sharada P.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1808.01990", "submitter": "Fatih Cakir", "authors": "Fatih Cakir, Kun He, Stan Sclaroff", "title": "Hashing with Binary Matrix Pursuit", "comments": "23 pages, 4 figures. In Proceedings of European Conference on\n  Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:51:36 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Cakir", "Fatih", ""], ["He", "Kun", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1808.01992", "submitter": "Zhiding Yu", "authors": "Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, B.\n  V. K. Vijaya Kumar, Jan Kautz", "title": "Simultaneous Edge Alignment and Learning", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is among the most fundamental vision problems for its role in\nperceptual grouping and its wide applications. Recent advances in\nrepresentation learning have led to considerable improvements in this area.\nMany state of the art edge detection models are learned with fully\nconvolutional networks (FCNs). However, FCN-based edge learning tends to be\nvulnerable to misaligned labels due to the delicate structure of edges. While\nsuch problem was considered in evaluation benchmarks, similar issue has not\nbeen explicitly addressed in general edge learning. In this paper, we show that\nlabel misalignment can cause considerably degraded edge learning quality, and\naddress this issue by proposing a simultaneous edge alignment and learning\nframework. To this end, we formulate a probabilistic model where edge alignment\nis treated as latent variable optimization, and is learned end-to-end during\nnetwork training. Experiments show several applications of this work, including\nimproved edge detection with state of the art performance, and automatic\nrefinement of noisy annotations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:58:42 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:42:05 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 05:36:51 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yu", "Zhiding", ""], ["Liu", "Weiyang", ""], ["Zou", "Yang", ""], ["Feng", "Chen", ""], ["Ramalingam", "Srikumar", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Kautz", "Jan", ""]]}, {"id": "1808.02056", "submitter": "Xiang Li", "authors": "Jiasha Liu, Xiang Li, Hui Ren, Quanzheng Li", "title": "Multi-Estimator Full Left Ventricle Quantification through Ensemble\n  Learning", "comments": "Jiasha Liu, Xiang Li and Hui Ren contribute equally to this work", "journal-ref": null, "doi": "10.1007/978-3-030-12029-0_49", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease accounts for 1 in every 4 deaths in United States.\nAccurate estimation of structural and functional cardiac parameters is crucial\nfor both diagnosis and disease management. In this work, we develop an ensemble\nlearning framework for more accurate and robust left ventricle (LV)\nquantification. The framework combines two 1st-level modules: direct estimation\nmodule and a segmentation module. The direct estimation module utilizes\nConvolutional Neural Network (CNN) to achieve end-to-end quantification. The\nCNN is trained by taking 2D cardiac images as input and cardiac parameters as\noutput. The segmentation module utilizes a U-Net architecture for obtaining\npixel-wise prediction of the epicardium and endocardium of LV from the\nbackground. The binary U-Net output is then analyzed by a separate CNN for\nestimating the cardiac parameters. We then employ linear regression between the\n1st-level predictor and ground truth to learn a 2nd-level predictor that\nensembles the results from 1st-level modules for the final estimation.\nPreliminary results by testing the proposed framework on the LVQuan18 dataset\nshow superior performance of the ensemble learning model over the two base\nmodules.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 18:25:46 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Liu", "Jiasha", ""], ["Li", "Xiang", ""], ["Ren", "Hui", ""], ["Li", "Quanzheng", ""]]}, {"id": "1808.02084", "submitter": "Zaiwei Zhang", "authors": "Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth,\n  Etienne Vouga, Qixing Huang", "title": "Deep Generative Modeling for Scene Synthesis via Hybrid Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep generative scene modeling technique for indoor\nenvironments. Our goal is to train a generative model using a feed-forward\nneural network that maps a prior distribution (e.g., a normal distribution) to\nthe distribution of primary objects in indoor scenes. We introduce a 3D object\narrangement representation that models the locations and orientations of\nobjects, based on their size and shape attributes. Moreover, our scene\nrepresentation is applicable for 3D objects with different multiplicities\n(repetition counts), selected from a database. We show a principled way to\ntrain this model by combining discriminator losses for both a 3D object\narrangement representation and a 2D image-based representation. We demonstrate\nthe effectiveness of our scene representation and the deep learning method on\nbenchmark datasets. We also show the applications of this generative model in\nscene interpolation and scene completion.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 19:42:24 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zhang", "Zaiwei", ""], ["Yang", "Zhenpei", ""], ["Ma", "Chongyang", ""], ["Luo", "Linjie", ""], ["Huth", "Alexander", ""], ["Vouga", "Etienne", ""], ["Huang", "Qixing", ""]]}, {"id": "1808.02096", "submitter": "Huiguang He", "authors": "Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng,\n  Bao-Liang Lu, Huiguang He", "title": "Semi-supervised Deep Generative Modelling of Incomplete Multi-Modality\n  Emotional Data", "comments": "arXiv admin note: text overlap with arXiv:1704.07548, 2018 ACM\n  Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240528", "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are threefold challenges in emotion recognition. First, it is difficult\nto recognize human's emotional states only considering a single modality.\nSecond, it is expensive to manually annotate the emotional data. Third,\nemotional data often suffers from missing modalities due to unforeseeable\nsensor malfunction or configuration issues. In this paper, we address all these\nproblems under a novel multi-view deep generative framework. Specifically, we\npropose to model the statistical relationships of multi-modality emotional data\nusing multiple modality-specific generative networks with a shared latent\nspace. By imposing a Gaussian mixture assumption on the posterior approximation\nof the shared latent variables, our framework can learn the joint deep\nrepresentation from multiple modalities and evaluate the importance of each\nmodality simultaneously. To solve the labeled-data-scarcity problem, we extend\nour multi-view model to semi-supervised learning scenario by casting the\nsemi-supervised classification problem as a specialized missing data imputation\ntask. To address the missing-modality problem, we further extend our\nsemi-supervised multi-view model to deal with incomplete data, where a missing\nview is treated as a latent variable and integrated out during inference. This\nway, the proposed overall framework can utilize all available (both labeled and\nunlabeled, as well as both complete and incomplete) data to improve its\ngeneralization ability. The experiments conducted on two real multi-modal\nemotion datasets demonstrated the superiority of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:07:36 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Du", "Changde", ""], ["Du", "Changying", ""], ["Wang", "Hao", ""], ["Li", "Jinpeng", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""], ["He", "Huiguang", ""]]}, {"id": "1808.02104", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu and Sarah Ostadabbas", "title": "Inner Space Preserving Generative Pose Machine", "comments": "http://www.northeastern.edu/ostadabbas/2018/07/23/inner-space-preserving-generative-pose-machine/", "journal-ref": "European Conference on Computer Vision (ECCV2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based generative methods, such as generative adversarial networks\n(GANs) have already been able to generate realistic images with much context\ncontrol, specially when they are conditioned. However, most successful\nframeworks share a common procedure which performs an image-to-image\ntranslation with pose of figures in the image untouched. When the objective is\nreposing a figure in an image while preserving the rest of the image, the\nstate-of-the-art mainly assumes a single rigid body with simple background and\nlimited pose shift, which can hardly be extended to the images under normal\nsettings. In this paper, we introduce an image \"inner space\" preserving model\nthat assigns an interpretable low-dimensional pose descriptor (LDPD) to an\narticulated figure in the image. Figure reposing is then generated by passing\nthe LDPD and the original image through multi-stage augmented hourglass\nnetworks in a conditional GAN structure, called inner space preserving\ngenerative pose machine (ISP-GPM). We evaluated ISP-GPM on reposing human\nfigures, which are highly articulated with versatile variations. Test of a\nstate-of-the-art pose estimator on our reposed dataset gave an accuracy over\n80% on PCK0.5 metric. The results also elucidated that our ISP-GPM is able to\npreserve the background with high accuracy while reasonably recovering the area\nblocked by the figure to be reposed.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 20:45:50 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Liu", "Shuangjun", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1808.02122", "submitter": "Ali Pour Yazdanpanah", "authors": "Ali Pour Yazdanpanah, Onur Afacan, Simon K. Warfield", "title": "Non-Learning based Deep Parallel MRI Reconstruction (NLDpMRI)", "comments": null, "journal-ref": null, "doi": "10.1117/12.2511653", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand\nand scan time directly depends on the number of acquired k-space samples.\nRecently, the deep learning-based MRI reconstruction techniques were suggested\nto accelerate MR image acquisition. The most common issues in any deep\nlearning-based MRI reconstruction approaches are generalizability and\ntransferability. For different MRI scanner configurations using these\napproaches, the network must be trained from scratch every time with new\ntraining dataset, acquired under new configurations, to be able to provide good\nreconstruction performance. Here, we propose a new generalized parallel imaging\nmethod based on deep neural networks called NLDpMRI to reduce any structured\naliasing ambiguities related to the different k-space undersampling patterns\nfor accelerated data acquisition. Two loss functions including non-regularized\nand regularized are proposed for parallel MRI reconstruction using deep network\noptimization and we reconstruct MR images by optimizing the proposed loss\nfunctions over the network parameters. Unlike any deep learning-based MRI\nreconstruction approaches, our method doesn't include any training step that\nthe network learns from a large number of training samples and it only needs\nthe single undersampled multi-coil k-space data for reconstruction. Also, the\nproposed method can handle k-space data with different undersampling patterns,\nand the different number of coils. Experimental results show that the proposed\nmethod outperforms the current state-of-the-art GRAPPA method and the deep\nlearning-based variational network method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:26:06 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 17:15:39 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 23:22:59 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Yazdanpanah", "Ali Pour", ""], ["Afacan", "Onur", ""], ["Warfield", "Simon K.", ""]]}, {"id": "1808.02128", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Jongmin Lee, Deunsol Jung, Bohyung Han, Minsu Cho", "title": "Attentive Semantic Alignment with Offset-Aware Correlation Kernels", "comments": "ECCV 2018 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic correspondence is the problem of establishing correspondences across\nimages depicting different instances of the same object or scene class. One of\nrecent approaches to this problem is to estimate parameters of a global\ntransformation model that densely aligns one image to the other. Since an\nentire correlation map between all feature pairs across images is typically\nused to predict such a global transformation, noisy features from different\nbackgrounds, clutter, and occlusion distract the predictor from correct\nestimation of the alignment. This is a challenging issue, in particular, in the\nproblem of semantic correspondence where a large degree of image variations is\noften involved. In this paper, we introduce an attentive semantic alignment\nmethod that focuses on reliable correlations, filtering out distractors. For\neffective attention, we also propose an offset-aware correlation kernel that\nlearns to capture translation-invariant local transformations in computing\ncorrelation values over spatial locations. Experiments demonstrate the\neffectiveness of the attentive model and offset-aware kernel, and the proposed\nmodel combining both techniques achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:42:57 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 07:55:18 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Lee", "Jongmin", ""], ["Jung", "Deunsol", ""], ["Han", "Bohyung", ""], ["Cho", "Minsu", ""]]}, {"id": "1808.02130", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Tobias Weyand, Jack Sim, Bohyung Han", "title": "CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning\n  of Maps", "comments": "ECCV 2018 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image geolocalization is the task of identifying the location depicted in a\nphoto based only on its visual information. This task is inherently challenging\nsince many photos have only few, possibly ambiguous cues to their geolocation.\nRecent work has cast this task as a classification problem by partitioning the\nearth into a set of discrete cells that correspond to geographic regions. The\ngranularity of this partitioning presents a critical trade-off; using fewer but\nlarger cells results in lower location accuracy while using more but smaller\ncells reduces the number of training examples per class and increases model\nsize, making the model prone to overfitting. To tackle this issue, we propose a\nsimple but effective algorithm, combinatorial partitioning, which generates a\nlarge number of fine-grained output classes by intersecting multiple\ncoarse-grained partitionings of the earth. Each classifier votes for the\nfine-grained classes that overlap with their respective coarse-grained ones.\nThis technique allows us to predict locations at a fine scale while maintaining\nsufficient training examples per class. Our algorithm achieves the\nstate-of-the-art performance in location recognition on multiple benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 21:47:11 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Weyand", "Tobias", ""], ["Sim", "Jack", ""], ["Han", "Bohyung", ""]]}, {"id": "1808.02152", "submitter": "Tao Hu", "authors": "Tao Hu, Jizheng Xu, Cong Huang, Honggang Qi, Qingming Huang, Yan Lu", "title": "Weakly Supervised Bilinear Attention Network for Fine-Grained Visual\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fine-grained visual classification, objects usually share similar\ngeometric structure but present variant local appearance and different pose.\nTherefore, localizing and extracting discriminative local features play a\ncrucial role in accurate category prediction. Existing works either pay\nattention to limited object parts or train isolated networks for locating and\nclassification. In this paper, we propose Weakly Supervised Bilinear Attention\nNetwork (WS-BAN) to solve these issues. It jointly generates a set of attention\nmaps (region-of-interest maps) to indicate the locations of object's parts and\nextracts sequential part features by Bilinear Attention Pooling (BAP). Besides,\nwe propose attention regularization and attention dropout to weakly supervise\nthe generating process of attention maps. WS-BAN can be trained end-to-end and\nachieves the state-of-the-art performance on multiple fine-grained\nclassification datasets, including CUB-200-2011, Stanford Car and\nFGVC-Aircraft, which demonstrated its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 23:22:56 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 09:59:30 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Hu", "Tao", ""], ["Xu", "Jizheng", ""], ["Huang", "Cong", ""], ["Qi", "Honggang", ""], ["Huang", "Qingming", ""], ["Lu", "Yan", ""]]}, {"id": "1808.02155", "submitter": "Kihwan Kim", "authors": "Ben Eckart and Kihwan Kim and Jan Kautz", "title": "EOE: Expected Overlap Estimation over Unstructured Point Cloud Data", "comments": "The paper will be presented in 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an iterative overlap estimation technique to augment existing\npoint cloud registration algorithms that can achieve high performance in\ndifficult real-world situations where large pose displacement and\nnon-overlapping geometry would otherwise cause traditional methods to fail. Our\napproach estimates overlapping regions through an iterative Expectation\nMaximization procedure that encodes the sensor field-of-view into the\nregistration process. The proposed technique, Expected Overlap Estimation\n(EOE), is derived from the observation that differences in field-of-view\nviolate the iid assumption implicitly held by all maximum likelihood based\nregistration techniques. We demonstrate how our approach can augment many\npopular registration methods with minimal computational overhead. Through\nexperimentation on both synthetic and real-world datasets, we find that adding\nan explicit overlap estimation step can aid robust outlier handling and\nincrease the accuracy of both ICP-based and GMM-based registration methods,\nespecially in large unstructured domains and where the amount of overlap\nbetween point clouds is very small.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 23:39:27 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Eckart", "Ben", ""], ["Kim", "Kihwan", ""], ["Kautz", "Jan", ""]]}, {"id": "1808.02167", "submitter": "Chun-Fu (Richard) Chen", "authors": "Chun-Fu Chen, Quanfu Fan, Marco Pistoia, Gwo Giun Lee", "title": "Efficient Fusion of Sparse and Complementary Convolutions", "comments": "10 pages, updated with correct numbers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to create compact convolutional neural networks\n(CNNs) by exploiting sparse convolutions. Different from previous works that\nlearn sparsity in models, we directly employ hand-crafted kernels with regular\nsparse patterns, which result in the computational gain in practice without\nsophisticated and dedicated software or hardware. The core of our approach is\nan efficient network module that linearly combines sparse kernels to yield\nfeature representations as strong as those from regular kernels. We integrate\nthis module into various network architectures and demonstrate its\neffectiveness on three vision tasks, object classification, localization and\ndetection. For object classification and localization, our approach achieves\ncomparable or better performance than several baselines and related works while\nproviding lower computational costs with fewer parameters (on average, a\n$2-4\\times$ reduction of convolutional parameters and computation). For object\ndetection, our approach leads to a VGG-16-based Faster RCNN detector that is\n12.4$\\times$ smaller and about 3$\\times$ faster than the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 00:39:56 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 15:11:37 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2018 03:18:39 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Chen", "Chun-Fu", ""], ["Fan", "Quanfu", ""], ["Pistoia", "Marco", ""], ["Lee", "Gwo Giun", ""]]}, {"id": "1808.02194", "submitter": "Zhiqiang Tang", "authors": "Zhiqiang Tang, Xi Peng, Shijie Geng, Lingfei Wu, Shaoting Zhang and\n  Dimitris Metaxas", "title": "Quantized Densely Connected U-Nets for Efficient Landmark Localization", "comments": "ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose quantized densely connected U-Nets for efficient\nvisual landmark localization. The idea is that features of the same semantic\nmeanings are globally reused across the stacked U-Nets. This dense connectivity\nlargely improves the information flow, yielding improved localization accuracy.\nHowever, a vanilla dense design would suffer from critical efficiency issue in\nboth training and testing. To solve this problem, we first propose order-K\ndense connectivity to trim off long-distance shortcuts; then, we use a\nmemory-efficient implementation to significantly boost the training efficiency\nand investigate an iterative refinement that may slice the model size in half.\nFinally, to reduce the memory consumption and high precision operations both in\ntraining and testing, we further quantize weights, inputs, and gradients of our\nlocalization network to low bit-width numbers. We validate our approach in two\ntasks: human pose estimation and face alignment. The results show that our\napproach achieves state-of-the-art localization accuracy, but using ~70% fewer\nparameters, ~98% less model size and saving ~75% training memory compared with\nother benchmark localizers. The code is available at\nhttps://github.com/zhiqiangdon/CU-Net.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 03:22:44 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 00:40:06 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Tang", "Zhiqiang", ""], ["Peng", "Xi", ""], ["Geng", "Shijie", ""], ["Wu", "Lingfei", ""], ["Zhang", "Shaoting", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1808.02201", "submitter": "Siyuan Huang", "authors": "Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, Song-Chun\n  Zhu", "title": "Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational framework to jointly parse a single RGB image and\nreconstruct a holistic 3D configuration composed by a set of CAD models using a\nstochastic grammar model. Specifically, we introduce a Holistic Scene Grammar\n(HSG) to represent the 3D scene structure, which characterizes a joint\ndistribution over the functional and geometric space of indoor scenes. The\nproposed HSG captures three essential and often latent dimensions of the indoor\nscenes: i) latent human context, describing the affordance and the\nfunctionality of a room arrangement, ii) geometric constraints over the scene\nconfigurations, and iii) physical constraints that guarantee physically\nplausible parsing and reconstruction. We solve this joint parsing and\nreconstruction problem in an analysis-by-synthesis fashion, seeking to minimize\nthe differences between the input image and the rendered images generated by\nour 3D representation, over the space of depth, surface normal, and object\nsegmentation map. The optimal configuration, represented by a parse graph, is\ninferred using Markov chain Monte Carlo (MCMC), which efficiently traverses\nthrough the non-differentiable solution space, jointly optimizing object\nlocalization, 3D layout, and hidden human context. Experimental results\ndemonstrate that the proposed algorithm improves the generalization ability and\nsignificantly outperforms prior methods on 3D layout estimation, 3D object\ndetection, and holistic scene understanding.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 03:49:15 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Huang", "Siyuan", ""], ["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Xiao", "Yinxue", ""], ["Xu", "Yuanlu", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1808.02212", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Jianming Zhang, Haoxiang Li, Joon-Young Lee, Xin Lu,\n  and Amit K. Roy-Chowdhury", "title": "Contemplating Visual Emotions: Understanding and Overcoming Dataset Bias", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning approaches to visual emotion recognition offer great\npromise, current methods consider training and testing models on small scale\ndatasets covering limited visual emotion concepts. Our analysis identifies an\nimportant but long overlooked issue of existing visual emotion benchmarks in\nthe form of dataset biases. We design a series of tests to show and measure how\nsuch dataset biases obstruct learning a generalizable emotion recognition\nmodel. Based on our analysis, we propose a webly supervised approach by\nleveraging a large quantity of stock image data. Our approach uses a simple yet\neffective curriculum guided training strategy for learning discriminative\nemotion features. We discover that the models learned using our large scale\nstock image dataset exhibit significantly better generalization ability than\nthe existing datasets without the manual collection of even a single label.\nMoreover, visual representation learned using our approach holds a lot of\npromise across a variety of tasks on different image and video datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:00:51 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Panda", "Rameswar", ""], ["Zhang", "Jianming", ""], ["Li", "Haoxiang", ""], ["Lee", "Joon-Young", ""], ["Lu", "Xin", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1808.02229", "submitter": "Guangxu Zhu", "authors": "Jiayao Zhang and Guangxu Zhu and Robert W. Heath Jr. and Kaibin Huang", "title": "Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep\n  Learning", "comments": "Submitted to IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning algorithms have been adopted in a range of\nsignal-processing applications spanning computer vision, natural language\nprocessing, and artificial intelligence. Many relevant problems involve\nsubspace-structured features, orthogonality constrained or low-rank constrained\nobjective functions, or subspace distances. These mathematical characteristics\nare expressed naturally using the Grassmann manifold. Unfortunately, this fact\nis not yet explored in many traditional learning algorithms. In the last few\nyears, there have been growing interests in studying Grassmann manifold to\ntackle new learning problems. Such attempts have been reassured by substantial\nperformance improvements in both classic learning and learning using deep\nneural networks. We term the former as shallow and the latter deep Grassmannian\nlearning. The aim of this paper is to introduce the emerging area of\nGrassmannian learning by surveying common mathematical problems and primary\nsolution approaches, and overviewing various applications. We hope to inspire\npractitioners in different fields to adopt the powerful tool of Grassmannian\nlearning in their research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 06:54:06 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 02:19:08 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhang", "Jiayao", ""], ["Zhu", "Guangxu", ""], ["Heath", "Robert W.", "Jr."], ["Huang", "Kaibin", ""]]}, {"id": "1808.02244", "submitter": "Qi Zhang", "authors": "Qi Zhang, Chunping Zhang, Jinbo Ling, Qing Wang, Jingyi Yu", "title": "A Generic Multi-Projection-Center Model and Calibration Method for Light\n  Field Cameras", "comments": "accepted by T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras can capture both spatial and angular information of light\nrays, enabling 3D reconstruction by a single exposure. The geometry of 3D\nreconstruction is affected by intrinsic parameters of a light field camera\nsignificantly. In the paper, we propose a multi-projection-center (MPC) model\nwith 6 intrinsic parameters to characterize light field cameras based on\ntraditional two-parallel-plane (TPP) representation. The MPC model can\ngenerally parameterize light field in different imaging formations, including\nconventional and focused light field cameras. By the constraints of 4D ray and\n3D geometry, a 3D projective transformation is deduced to describe the\nrelationship between geometric structure and the MPC coordinates. Based on the\nMPC model and projective transformation, we propose a calibration algorithm to\nverify our light field camera model. Our calibration method includes a\nclose-form solution and a non-linear optimization by minimizing re-projection\nerrors. Experimental results on both simulated and real scene data have\nverified the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:52:38 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zhang", "Qi", ""], ["Zhang", "Chunping", ""], ["Ling", "Jinbo", ""], ["Wang", "Qing", ""], ["Yu", "Jingyi", ""]]}, {"id": "1808.02246", "submitter": "Tianrui Liu", "authors": "Tianrui Liu, Mohamed Elmikaty, Tania Stathaki", "title": "SAM-RCNN: Scale-Aware Multi-Resolution Multi-Channel Pedestrian\n  Detection", "comments": "published in British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have enabled significant improvements in\npedestrian detection owing to the strong representation ability of the CNN\nfeatures. Recently, aggregating features from multiple layers of a CNN has been\nconsidered as an effective approach, however, the same approach regarding\nfeature representation is used for detecting pedestrians of varying scales.\nConsequently, it is not guaranteed that the feature representation for\npedestrians of a particular scale is optimised. In this paper, we propose a\nScale-Aware Multi-resolution (SAM) method for pedestrian detection which can\nadaptively select multi-resolution convolutional features according to\npedestrian sizes. The proposed SAM method extracts the appropriate CNN features\nthat have strong representation ability as well as sufficient feature\nresolution, given the size of the pedestrian candidate output from a region\nproposal network. Moreover, we propose an enhanced SAM method, termed as SAM+,\nwhich incorporates complementary features channels and achieves further\nperformance improvement. Evaluations on the challenging Caltech and KITTI\npedestrian benchmarks demonstrate the superiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 07:58:41 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Liu", "Tianrui", ""], ["Elmikaty", "Mohamed", ""], ["Stathaki", "Tania", ""]]}, {"id": "1808.02267", "submitter": "Jiawang Bian", "authors": "JiaWang Bian, Ruihan Yang, Yun Liu, Le Zhang, Ming-Ming Cheng, Ian\n  Reid, WenHai Wu", "title": "MatchBench: An Evaluation of Feature Matchers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature matching is one of the most fundamental and active research areas in\ncomputer vision. A comprehensive evaluation of feature matchers is necessary,\nsince it would advance both the development of this field and also high-level\napplications such as Structure-from-Motion or Visual SLAM. However, to the best\nof our knowledge, no previous work targets the evaluation of feature matchers\nwhile they only focus on evaluating feature detectors and descriptors. This\nleads to a critical absence in this field that there is no standard datasets\nand evaluation metrics to evaluate different feature matchers fairly. To this\nend, we present the first uniform feature matching benchmark to facilitate the\nevaluation of feature matchers. In the proposed benchmark, matchers are\nevaluated in different aspects, involving matching ability, correspondence\nsufficiency, and efficiency. Also, their performances are investigated in\ndifferent scenes and in different matching types. Subsequently, we carry out an\nextensive evaluation of different state-of-the-art matchers on the benchmark\nand make in-depth analyses based on the reported results. This can be used to\ndesign practical matching systems in real applications and also advocates the\npotential future research directions in the field of feature matching.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 09:03:24 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Bian", "JiaWang", ""], ["Yang", "Ruihan", ""], ["Liu", "Yun", ""], ["Zhang", "Le", ""], ["Cheng", "Ming-Ming", ""], ["Reid", "Ian", ""], ["Wu", "WenHai", ""]]}, {"id": "1808.02289", "submitter": "Ana Garc\\'ia del Molino", "authors": "Ana Garcia del Molino and Joo-Hwee Lim and Ah-Hwee Tan", "title": "Predicting Visual Context for Unsupervised Event Segmentation in\n  Continuous Photo-streams", "comments": "Accepted for publication at the 2018 ACM Multimedia Conference (MM\n  '18)", "journal-ref": null, "doi": "10.1145/3240508.3240624", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting video content into events provides semantic structures for\nindexing, retrieval, and summarization. Since motion cues are not available in\ncontinuous photo-streams, and annotations in lifelogging are scarce and costly,\nthe frames are usually clustered into events by comparing the visual features\nbetween them in an unsupervised way. However, such methodologies are\nineffective to deal with heterogeneous events, e.g. taking a walk, and\ntemporary changes in the sight direction, e.g. at a meeting. To address these\nlimitations, we propose Contextual Event Segmentation (CES), a novel\nsegmentation paradigm that uses an LSTM-based generative network to model the\nphoto-stream sequences, predict their visual context, and track their\nevolution. CES decides whether a frame is an event boundary by comparing the\nvisual context generated from the frames in the past, to the visual context\npredicted from the future. We implemented CES on a new and massive lifelogging\ndataset consisting of more than 1.5 million images spanning over 1,723 days.\nExperiments on the popular EDUB-Seg dataset show that our model outperforms the\nstate-of-the-art by over 16% in f-measure. Furthermore, CES' performance is\nonly 3 points below that of human annotators.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 10:03:55 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["del Molino", "Ana Garcia", ""], ["Lim", "Joo-Hwee", ""], ["Tan", "Ah-Hwee", ""]]}, {"id": "1808.02299", "submitter": "Sergio A Velastin", "authors": "Jorge E. Espinosa (1), Sergio A. Velastin (2 and 3 and 4) and John W.\n  Branch (5) ((1) Politecnico Colombiano Jaime Isaza Cadavid Colombia, (2)\n  Universidad Carlos III de Madrid Spain, (3) Cortexica Vision Systems Ltd. UK,\n  (4) Queen Mary University of London UK, (5) Universidad Nacional de Colombia\n  Medellin Colombia)", "title": "Motorcycle detection and classification in urban Scenarios using a model\n  based on Faster R-CNN", "comments": "Presented at 9th International Conference on Pattern Recognition\n  Systems, ICPRS-18, 22-24 May 2018, Valparaiso, Chile", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Deep Learning Convolutional Neural Network model\nbased on Faster-RCNN for motorcycle detection and classification on urban\nenvironments. The model is evaluated in occluded scenarios where more than 60%\nof the vehicles present a degree of occlusion. For training and evaluation, we\nintroduce a new dataset of 7500 annotated images, captured under real traffic\nscenes, using a drone mounted camera. Several tests were carried out to design\nthe network, achieving promising results of 75% in average precision (AP), even\nwith the high number of occluded motorbikes, the low angle of capture and the\nmoving camera. The model is also evaluated on low occlusions datasets, reaching\nresults of up to 92% in AP.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 10:50:55 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 18:04:57 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Espinosa", "Jorge E.", "", "2 and 3 and 4"], ["Velastin", "Sergio A.", "", "2 and 3 and 4"], ["Branch", "John W.", ""]]}, {"id": "1808.02312", "submitter": "Ke Li", "authors": "Ke Li, Kaiyue Pang, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M.\n  Hospedales, Honggang Zhang", "title": "Universal Perceptual Grouping", "comments": "Accepted ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we aim to develop a universal sketch grouper. That is, a grouper\nthat can be applied to sketches of any category in any domain to group\nconstituent strokes/segments into semantically meaningful object parts. The\nfirst obstacle to this goal is the lack of large-scale datasets with grouping\nannotation. To overcome this, we contribute the largest sketch perceptual\ngrouping (SPG) dataset to date, consisting of 20,000 unique sketches evenly\ndistributed over 25 object categories. Furthermore, we propose a novel deep\nuniversal perceptual grouping model. The model is learned with both generative\nand discriminative losses. The generative losses improve the generalisation\nability of the model to unseen object categories and datasets. The\ndiscriminative losses include a local grouping loss and a novel global grouping\nloss to enforce global grouping consistency. We show that the proposed model\nsignificantly outperforms the state-of-the-art groupers. Further, we show that\nour grouper is useful for a number of sketch analysis tasks including sketch\nsynthesis and fine-grained sketch-based image retrieval (FG-SBIR).\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 11:50:32 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Li", "Ke", ""], ["Pang", "Kaiyue", ""], ["Song", "Jifei", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Zhang", "Honggang", ""]]}, {"id": "1808.02313", "submitter": "Kaiyue Pang", "authors": "Kaiyue Pang, Da Li, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M.\n  Hospedales", "title": "Deep Factorised Inverse-Sketching", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling human free-hand sketches has become topical recently, driven by\npractical applications such as fine-grained sketch based image retrieval\n(FG-SBIR). Sketches are clearly related to photo edge-maps, but a human\nfree-hand sketch of a photo is not simply a clean rendering of that photo's\nedge map. Instead there is a fundamental process of abstraction and iconic\nrendering, where overall geometry is warped and salient details are selectively\nincluded. In this paper we study this sketching process and attempt to invert\nit. We model this inversion by translating iconic free-hand sketches to\ncontours that resemble more geometrically realistic projections of object\nboundaries, and separately factorise out the salient added details. This\nfactorised re-representation makes it easier to match a free-hand sketch to a\nphoto instance of an object. Specifically, we propose a novel unsupervised\nimage style transfer model based on enforcing a cyclic embedding consistency\nconstraint. A deep FG-SBIR model is then formulated to accommodate\ncomplementary discriminative detail from each factorised sketch for better\nmatching with the corresponding photo. Our method is evaluated both\nqualitatively and quantitatively to demonstrate its superiority over a number\nof state-of-the-art alternatives for style transfer and FG-SBIR.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 11:51:20 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Pang", "Kaiyue", ""], ["Li", "Da", ""], ["Song", "Jifei", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1808.02324", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Mark Dras, Len Hamey, Deborah Richards, Stephen\n  Wan, Cecile Paris", "title": "Automatic Recognition of Student Engagement using Deep Learning and\n  Facial Expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engagement is a key indicator of the quality of learning experience, and one\nthat plays a major role in developing intelligent educational interfaces. Any\nsuch interface requires the ability to recognise the level of engagement in\norder to respond appropriately; however, there is very little existing data to\nlearn from, and new data is expensive and difficult to acquire. This paper\npresents a deep learning model to improve engagement recognition from images\nthat overcomes the data sparsity challenge by pre-training on readily available\nbasic facial expression data, before training on specialised engagement data.\nIn the first of two steps, a facial expression recognition model is trained to\nprovide a rich face representation using deep learning. In the second step, we\nuse the model's weights to initialize our deep learning based model to\nrecognize engagement; we term this the engagement model. We train the model on\nour new engagement recognition dataset with 4627 engaged and disengaged\nsamples. We find that the engagement model outperforms effective deep learning\narchitectures that we apply for the first time to engagement recognition, as\nwell as approaches using histogram of oriented gradients and support vector\nmachines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 12:38:20 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 11:32:39 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 09:01:53 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 03:29:39 GMT"}, {"version": "v5", "created": "Mon, 8 Jul 2019 13:29:02 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Dras", "Mark", ""], ["Hamey", "Len", ""], ["Richards", "Deborah", ""], ["Wan", "Stephen", ""], ["Paris", "Cecile", ""]]}, {"id": "1808.02350", "submitter": "Ahmad El Sallab Dr", "authors": "Waleed Ali, Sherif Abdelkarim, Mohamed Zahran, Mahmoud Zidan and Ahmad\n  El Sallab", "title": "YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection\n  from LiDAR Point Cloud", "comments": "Paper accepted in ECCV 2018, \"3D Reconstruction meets Semantics\"\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and classification in 3D is a key task in Automated Driving\n(AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction\nof the surrounding environment, while the task of 3D object bounding box\ndetection in real time remains a strong algorithmic challenge. In this paper,\nwe build on the success of the one-shot regression meta-architecture in the 2D\nperspective image space and extend it to generate oriented 3D object bounding\nboxes from LiDAR point cloud. Our main contribution is in extending the loss\nfunction of YOLO v2 to include the yaw angle, the 3D box center in Cartesian\ncoordinates and the height of the box as a direct regression problem. This\nformulation enables real-time performance, which is essential for automated\ndriving. Our results are showing promising figures on KITTI benchmark,\nachieving real-time performance (40 fps) on Titan X GPU.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 13:19:24 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ali", "Waleed", ""], ["Abdelkarim", "Sherif", ""], ["Zahran", "Mohamed", ""], ["Zidan", "Mahmoud", ""], ["Sallab", "Ahmad El", ""]]}, {"id": "1808.02355", "submitter": "Konstantinos Zormpas-Petridis", "authors": "Konstantinos Zormpas-Petridis, Henrik Failmezger, Ioannis Roxanis,\n  Matthew Blackledge, Yann Jamin, Yinyin Yuan", "title": "Capturing global spatial context for accurate cell classification in\n  skin cancer histology", "comments": "Accepted by MICCAI COMPAY 2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectacular response observed in clinical trials of immunotherapy in\npatients with previously uncurable Melanoma, a highly aggressive form of skin\ncancer, calls for a better understanding of the cancer-immune interface.\nComputational pathology provides a unique opportunity to spatially dissect such\ninterface on digitised pathological slides. Accurate cellular classification is\na key to ensure meaningful results, but is often challenging even with\nstate-of-art machine learning and deep learning methods.\n  We propose a hierarchical framework, which mirrors the way pathologists\nperceive tumour architecture and define tumour heterogeneity to improve cell\nclassification methods that rely solely on cell nuclei morphology. The SLIC\nsuperpixel algorithm was used to segment and classify tumour regions in low\nresolution H&E-stained histological images of melanoma skin cancer to provide a\nglobal context. Classification of superpixels into tumour, stroma, epidermis\nand lumen/white space, yielded a 97.7% training set accuracy and 95.7% testing\nset accuracy in 58 whole-tumour images of the TCGA melanoma dataset. The\nsuperpixel classification was projected down to high resolution images to\nenhance the performance of a single cell classifier, based on cell nuclear\nmorphological features, and resulted in increasing its accuracy from 86.4% to\n91.6%. Furthermore, a voting scheme was proposed to use global context as\nbiological a priori knowledge, pushing the accuracy further to 92.8%.\n  This study demonstrates how using the global spatial context can accurately\ncharacterise the tumour microenvironment and allow us to extend significantly\nbeyond single-cell morphological classification.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 13:31:09 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zormpas-Petridis", "Konstantinos", ""], ["Failmezger", "Henrik", ""], ["Roxanis", "Ioannis", ""], ["Blackledge", "Matthew", ""], ["Jamin", "Yann", ""], ["Yuan", "Yinyin", ""]]}, {"id": "1808.02357", "submitter": "Shayan Gharib", "authors": "Shayan Gharib, Honain Derrar, Daisuke Niizumi, Tuukka Senttula, Janne\n  Tommola, Toni Heittola, Tuomas Virtanen, Heikki Huttunen", "title": "Acoustic Scene Classification: A Competition Review", "comments": "This work has been accepted in IEEE International Workshop on Machine\n  Learning for Signal Processing (MLSP 2018). Copyright may be transferred\n  without notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of acoustic scene classification, i.e.,\ncategorization of audio sequences into mutually exclusive classes based on\ntheir spectral content. We describe the methods and results discovered during a\ncompetition organized in the context of a graduate machine learning course;\nboth by the students and external participants. We identify the most suitable\nmethods and study the impact of each by performing an ablation study of the\nmixture of approaches. We also compare the results with a neural network\nbaseline, and show the improvement over that. Finally, we discuss the impact of\nusing a competition as a part of a university course, and justify its\nimportance in the curriculum based on student feedback.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:40:17 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Gharib", "Shayan", ""], ["Derrar", "Honain", ""], ["Niizumi", "Daisuke", ""], ["Senttula", "Tuukka", ""], ["Tommola", "Janne", ""], ["Heittola", "Toni", ""], ["Virtanen", "Tuomas", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1808.02373", "submitter": "Pingping Zhang Mr", "authors": "Pingping Zhang, Huchuan Lu, Chunhua Shen", "title": "Troy: Give Attention to Saliency and for Saliency", "comments": "All of authors agree to withdrawal this paper because we have noticed\n  several important errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition, our work has text overlap with arXiv:1804.06242,\narXiv:1705.00938 by other authors. We want to rewrite this paper for avoiding\nthis fact.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 11:57:27 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:59:29 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1808.02408", "submitter": "Antal Horvath", "authors": "Antal Horvath, Charidimos Tsagkas, Simon Andermatt, Simon Pezold,\n  Katrin Parmar, Philippe Cattin", "title": "Spinal Cord Gray Matter-White Matter Segmentation on Magnetic Resonance\n  AMIRA Images with MD-GRU", "comments": "MICCAI 2018 CSI workshop, 12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The small butterfly shaped structure of spinal cord (SC) gray matter (GM) is\nchallenging to image and to delinate from its surrounding white matter (WM).\nSegmenting GM is up to a point a trade-off between accuracy and precision. We\npropose a new pipeline for GM-WM magnetic resonance (MR) image acquisition and\nsegmentation. We report superior results as compared to the ones recently\nreported in the SC GM segmentation challenge and show even better results using\nthe averaged magnetization inversion recovery acquisitions (AMIRA) sequence.\nScan-rescan experiments with the AMIRA sequence show high reproducibility in\nterms of Dice coefficient, Hausdorff distance and relative standard deviation.\nWe use a recurrent neural network (RNN) with multi-dimensional gated recurrent\nunits (MD-GRU) to train segmentation models on the AMIRA dataset of 855 slices.\nWe added a generalized dice loss to the cross entropy loss that MD-GRU uses and\nwere able to improve the results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 14:53:49 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Horvath", "Antal", ""], ["Tsagkas", "Charidimos", ""], ["Andermatt", "Simon", ""], ["Pezold", "Simon", ""], ["Parmar", "Katrin", ""], ["Cattin", "Philippe", ""]]}, {"id": "1808.02414", "submitter": "Michal Polic", "authors": "Michal Polic, Wolfgang F\\\"orstner and Tomas Pajdla", "title": "Fast and Accurate Camera Covariance Computation for Large 3D\n  Reconstruction", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Estimating uncertainty of camera parameters computed in Structure from Motion\n(SfM) is an important tool for evaluating the quality of the reconstruction and\nguiding the reconstruction process. Yet, the quality of the estimated\nparameters of large reconstructions has been rarely evaluated due to the\ncomputational challenges. We present a new algorithm which employs the sparsity\nof the uncertainty propagation and speeds the computation up about ten times\n\\wrt previous approaches. Our computation is accurate and does not use any\napproximations. We can compute uncertainties of thousands of cameras in tens of\nseconds on a standard PC. We also demonstrate that our approach can be\neffectively used for reconstructions of any size by applying it to smaller\nsub-reconstructions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 15:14:24 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Polic", "Michal", ""], ["F\u00f6rstner", "Wolfgang", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1808.02443", "submitter": "Eliza Mace", "authors": "Eliza Mace, Keith Manville, Monica Barbu-McInnis, Michael Laielli,\n  Matthew Klaric, Samuel Dooley", "title": "Overhead Detection: Beyond 8-bits and RGB", "comments": "10 pages, 8 figures, 2 tables", "journal-ref": "Naval Applications of Machine Learning, February 13, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses the challenging and publicly available SpaceNet dataset to\nestablish a performance baseline for a state-of-the-art object detector in\nsatellite imagery. Specifically, we examine how various features of the data\naffect building detection accuracy with respect to the Intersection over Union\nmetric. We demonstrate that the performance of the R-FCN detection algorithm on\nimagery with a 1.5 meter ground sample distance and three spectral bands\nincreases by over 32% by using 13-bit data, as opposed to 8-bit data at the\nsame spatial and spectral resolution. We also establish accuracy trends with\nrespect to building size and scene density. Finally, we propose and evaluate\nmultiple methods for integrating additional spectral information into\noff-the-shelf deep learning architectures. Interestingly, our methods are\nrobust to the choice of spectral bands and we note no significant performance\nimprovement when adding additional bands.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 16:12:02 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Mace", "Eliza", ""], ["Manville", "Keith", ""], ["Barbu-McInnis", "Monica", ""], ["Laielli", "Michael", ""], ["Klaric", "Matthew", ""], ["Dooley", "Samuel", ""]]}, {"id": "1808.02455", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane\n  Idoumghar, Pierre-Alain Muller", "title": "Data augmentation using synthetic data for time series classification\n  with deep residual networks", "comments": "Accepted at AALTD'18 workshop in ECML/PKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation in deep neural networks is the process of generating\nartificial data in order to reduce the variance of the classifier with the goal\nto reduce the number of errors. This idea has been shown to improve deep neural\nnetwork's generalization capabilities in many computer vision tasks such as\nimage recognition and object localization. Apart from these applications, deep\nConvolutional Neural Networks (CNNs) have also recently gained popularity in\nthe Time Series Classification (TSC) community. However, unlike in image\nrecognition problems, data augmentation techniques have not yet been\ninvestigated thoroughly for the TSC task. This is surprising as the accuracy of\ndeep learning models for TSC could potentially be improved, especially for\nsmall datasets that exhibit overfitting, when a data augmentation method is\nadopted. In this paper, we fill this gap by investigating the application of a\nrecently proposed data augmentation technique based on the Dynamic Time Warping\ndistance, for a deep learning model for TSC. To evaluate the potential of\naugmenting the training set, we performed extensive experiments using the UCR\nTSC benchmark. Our preliminary experiments reveal that data augmentation can\ndrastically increase deep CNN's accuracy on some datasets and significantly\nimprove the deep model's accuracy when the method is used in an ensemble\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 16:48:21 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1808.02473", "submitter": "Changqing Zou Dr.", "authors": "Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang,\n  Chengying Gao, Baoquan Chen, Hao Zhang", "title": "SketchyScene: Richly-Annotated Scene Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute the first large-scale dataset of scene sketches, SketchyScene,\nwith the goal of advancing research on sketch understanding at both the object\nand scene level. The dataset is created through a novel and carefully designed\ncrowdsourcing pipeline, enabling users to efficiently generate large quantities\nof realistic and diverse scene sketches. SketchyScene contains more than 29,000\nscene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+\nobject sketches. All objects in the scene sketches have ground-truth semantic\nand instance masks. The dataset is also highly scalable and extensible, easily\nallowing augmenting and/or changing scene composition. We demonstrate the\npotential impact of SketchyScene by training new computational models for\nsemantic segmentation of scene sketches and showing how the new dataset enables\nseveral applications including image retrieval, sketch colorization, editing,\nand captioning, etc. The dataset and code can be found at\nhttps://github.com/SketchyScene/SketchyScene.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 17:47:55 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zou", "Changqing", ""], ["Yu", "Qian", ""], ["Du", "Ruofei", ""], ["Mo", "Haoran", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Gao", "Chengying", ""], ["Chen", "Baoquan", ""], ["Zhang", "Hao", ""]]}, {"id": "1808.02474", "submitter": "Yuhong Guo", "authors": "Meng Ye and Yuhong Guo", "title": "Multi-Label Zero-Shot Learning with Transfer-Aware Label Embedding\n  Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning transfers knowledge from seen classes to novel unseen\nclasses to reduce human labor of labelling data for building new classifiers.\nMuch effort on zero-shot learning however has focused on the standard\nmulti-class setting, the more challenging multi-label zero-shot problem has\nreceived limited attention. In this paper we propose a transfer-aware embedding\nprojection approach to tackle multi-label zero-shot learning. The approach\nprojects the label embedding vectors into a low-dimensional space to induce\nbetter inter-label relationships and explicitly facilitate information transfer\nfrom seen labels to unseen labels, while simultaneously learning a max-margin\nmulti-label classifier with the projected label embeddings. Auxiliary\ninformation can be conveniently incorporated to guide the label embedding\nprojection to further improve label relation structures for zero-shot knowledge\ntransfer. We conduct experiments for zero-shot multi-label image\nclassification. The results demonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 17:48:40 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ye", "Meng", ""], ["Guo", "Yuhong", ""]]}, {"id": "1808.02518", "submitter": "Max Ferguson", "authors": "Max Ferguson, Ronay Ak, Yung-Tsun Tina Lee, Kincho H. Law", "title": "Detection and Segmentation of Manufacturing Defects with Convolutional\n  Neural Networks and Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control is a fundamental component of many manufacturing processes,\nespecially those involving casting or welding. However, manual quality control\nprocedures are often time-consuming and error-prone. In order to meet the\ngrowing demand for high-quality products, the use of intelligent visual\ninspection systems is becoming essential in production lines. Recently,\nConvolutional Neural Networks (CNNs) have shown outstanding performance in both\nimage classification and localization tasks. In this article, a system is\nproposed for the identification of casting defects in X-ray images, based on\nthe Mask Region-based CNN architecture. The proposed defect detection system\nsimultaneously performs defect detection and segmentation on input images,\nmaking it suitable for a range of defect detection tasks. It is shown that\ntraining the network to simultaneously perform defect detection and defect\ninstance segmentation, results in a higher defect detection accuracy than\ntraining on defect detection alone. Transfer learning is leveraged to reduce\nthe training data demands and increase the prediction accuracy of the trained\nmodel. More specifically, the model is first trained with two large\nopenly-available image datasets before finetuning on a relatively small metal\ncasting X-ray dataset. The accuracy of the trained model exceeds state-of-the\nart performance on the GRIMA database of X-ray images (GDXray) Castings dataset\nand is fast enough to be used in a production setting. The system also performs\nwell on the GDXray Welds dataset. A number of in-depth studies are conducted to\nexplore how transfer learning, multi-task learning, and multi-class learning\ninfluence the performance of the trained system.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 18:57:41 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 02:39:44 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ferguson", "Max", ""], ["Ak", "Ronay", ""], ["Lee", "Yung-Tsun Tina", ""], ["Law", "Kincho H.", ""]]}, {"id": "1808.02531", "submitter": "Mina Bishay", "authors": "Mina Bishay, Petar Palasek, Stefan Priebe, and Ioannis Patras", "title": "SchiNet: Automatic Estimation of Symptoms of Schizophrenia from Facial\n  Behaviour Analysis", "comments": "13 pages, IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients with schizophrenia often display impairments in the expression of\nemotion and speech and those are observed in their facial behaviour. Automatic\nanalysis of patients' facial expressions that is aimed at estimating symptoms\nof schizophrenia has received attention recently. However, the datasets that\nare typically used for training and evaluating the developed methods, contain\nonly a small number of patients (4-34) and are recorded while the subjects were\nperforming controlled tasks such as listening to life vignettes, or answering\nemotional questions. In this paper, we use videos of professional-patient\ninterviews, in which symptoms were assessed in a standardised way as they\nshould/may be assessed in practice, and which were recorded in realistic\nconditions (i.e. varying illumination levels and camera viewpoints) at the\npatients' homes or at mental health services. We automatically analyse the\nfacial behaviour of 91 out-patients - this is almost 3 times the number of\npatients in other studies - and propose SchiNet, a novel neural network\narchitecture that estimates expression-related symptoms in two different\nassessment interviews. We evaluate the proposed SchiNet for patient-independent\nprediction of symptoms of schizophrenia. Experimental results show that some\nautomatically detected facial expressions are significantly correlated to\nsymptoms of schizophrenia, and that the proposed network for estimating symptom\nseverity delivers promising results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 19:31:11 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Bishay", "Mina", ""], ["Palasek", "Petar", ""], ["Priebe", "Stefan", ""], ["Patras", "Ioannis", ""]]}, {"id": "1808.02536", "submitter": "Da Zhang", "authors": "Da Zhang, Xiyang Dai, Yuan-Fang Wang", "title": "Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling\n  for Activity Detection", "comments": "ACCV 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing instances at different scales simultaneously is a fundamental\nchallenge in visual detection problems. While spatial multi-scale modeling has\nbeen well studied in object detection, how to effectively apply a multi-scale\narchitecture to temporal models for activity detection is still under-explored.\nIn this paper, we identify three unique challenges that need to be specifically\nhandled for temporal activity detection compared to its spatial counterpart. To\naddress all these issues, we propose Dynamic Temporal Pyramid Network (DTPN), a\nnew activity detection framework with a multi-scale pyramidal architecture\nfeaturing three novel designs: (1) We sample input video frames dynamically\nwith varying frame per seconds (FPS) to construct a natural pyramidal input for\nvideo of an arbitrary length. (2) We design a two-branch multi-scale temporal\nfeature hierarchy to deal with the inherent temporal scale variation of\nactivity instances. (3) We further exploit the temporal context of activities\nby appropriately fusing multi-scale feature maps, and demonstrate that both\nlocal and global temporal contexts are important. By combining all these\ncomponents into a uniform network, we end up with a single-shot activity\ndetector involving single-pass inferencing and end-to-end training. Extensive\nexperiments show that the proposed DTPN achieves state-of-the-art performance\non the challenging ActvityNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 20:02:36 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 22:31:35 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Zhang", "Da", ""], ["Dai", "Xiyang", ""], ["Wang", "Yuan-Fang", ""]]}, {"id": "1808.02559", "submitter": "Youngjae Yu", "authors": "Youngjae Yu, Jongseok Kim, Gunhee Kim", "title": "A Joint Sequence Fusion Model for Video Question Answering and Retrieval", "comments": "To appear in ECCV 2018. 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach named JSFusion (Joint Sequence Fusion) that can\nmeasure semantic similarity between any pairs of multimodal sequence data (e.g.\na video clip and a language sentence). Our multimodal matching network consists\nof two key components. First, the Joint Semantic Tensor composes a dense\npairwise representation of two sequence data into a 3D tensor. Then, the\nConvolutional Hierarchical Decoder computes their similarity score by\ndiscovering hidden hierarchical matches between the two sequence modalities.\nBoth modules leverage hierarchical attention mechanisms that learn to promote\nwell-matched representation patterns while prune out misaligned ones in a\nbottom-up manner. Although the JSFusion is a universal model to be applicable\nto any multimodal sequence data, this work focuses on video-language tasks\nincluding multimodal retrieval and video QA. We evaluate the JSFusion model in\nthree retrieval and VQA tasks in LSMDC, for which our model achieves the best\nperformance reported so far. We also perform multiple-choice and movie\nretrieval tasks for the MSR-VTT dataset, on which our approach outperforms many\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 21:33:37 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Yu", "Youngjae", ""], ["Kim", "Jongseok", ""], ["Kim", "Gunhee", ""]]}, {"id": "1808.02564", "submitter": "Thibaud Ehret", "authors": "Thibaud Ehret, Axel Davy, Jean-Michel Morel and Mauricio Delbracio", "title": "Image Anomalies: a Review and Synthesis of Detection Methods", "comments": "Thibaud Ehret and Axel Davy contributed equally to this work", "journal-ref": null, "doi": "10.1007/s10851-019-00885-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the broad variety of methods that have been proposed for anomaly\ndetection in images. Most methods found in the literature have in mind a\nparticular application. Yet we show that the methods can be classified mainly\nby the structural assumption they make on the \"normal\" image. Five different\nstructural assumptions emerge. Our analysis leads us to reformulate the best\nrepresentative algorithms by attaching to them an a contrario detection that\ncontrols the number of false positives and thus derive universal detection\nthresholds. By combining the most general structural assumptions expressing the\nbackground's normality with the best proposed statistical detection tools, we\nend up proposing generic algorithms that seem to generalize or reconcile most\nmethods. We compare the six best representatives of our proposed classes of\nalgorithms on anomalous images taken from classic papers on the subject, and on\na synthetic database. Our conclusion is that it is possible to perform\nautomatic anomaly detection on a single image.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 22:06:44 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 08:51:44 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ehret", "Thibaud", ""], ["Davy", "Axel", ""], ["Morel", "Jean-Michel", ""], ["Delbracio", "Mauricio", ""]]}, {"id": "1808.02595", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu and Sarah Ostadabbas", "title": "A Semi-Supervised Data Augmentation Approach using 3D Graphical Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have been rapidly adopted across a wide range of\nfields because of their accuracy and flexibility, but require large labeled\ntraining datasets. This presents a fundamental problem for applications with\nlimited, expensive, or private data (i.e. small data), such as human pose and\nbehavior estimation/tracking which could be highly personalized. In this paper,\nwe present a semi-supervised data augmentation approach that can synthesize\nlarge scale labeled training datasets using 3D graphical engines based on a\nphysically-valid low dimensional pose descriptor. To evaluate the performance\nof our synthesized datasets in training deep learning-based models, we\ngenerated a large synthetic human pose dataset, called ScanAva using 3D scans\nof only 7 individuals based on our proposed augmentation approach. A\nstate-of-the-art human pose estimation deep learning model then was trained\nfrom scratch using our ScanAva dataset and could achieve the pose estimation\naccuracy of 91.2% at PCK0.5 criteria after applying an efficient domain\nadaptation on the synthetic images, in which its pose estimation accuracy was\ncomparable to the same model trained on large scale pose data from real humans\nsuch as MPII dataset and much higher than the model trained on other synthetic\nhuman dataset such as SURREAL.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 01:48:38 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:55:27 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Shuangjun", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1808.02603", "submitter": "Deyu Meng", "authors": "Mingrui Geng and Yun Deng and Qian Zhao and Qi Xie and Dong Zeng and\n  Dong Zeng and Wangmeng Zuo and Deyu Meng", "title": "Unsupervised/Semi-supervised Deep Learning for Low-dose CT Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning(DL) methods have been proposed for the low-dose\ncomputed tomography(LdCT) enhancement, and obtain good trade-off between\ncomputational efficiency and image quality. Most of them need large number of\npre-collected ground-truth/high-dose sinograms with less noise, and train the\nnetwork in a supervised end-to-end manner. This may bring major limitations on\nthese methods because the number of such low-dose/high-dose training sinogram\npairs would affect the network's capability and sometimes the ground-truth\nsinograms are hard to be obtained in large scale. Since large number of\nlow-dose ones are relatively easy to obtain, it should be critical to make\nthese sources play roles in network training in an unsupervised learning\nmanner. To address this issue, we propose an unsupervised DL method for LdCT\nenhancement that incorporates unlabeled LdCT sinograms directly into the\nnetwork training. The proposed method effectively considers the structure\ncharacteristics and noise distribution in the measured LdCT sinogram, and then\nlearns the proper gradient of the LdCT sinogram in a pure unsupervised manner.\nSimilar to the labeled ground-truth, the gradient information in an unlabeled\nLdCT sinogram can be used for sufficient network training. The experiments on\nthe patient data show effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 02:23:37 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Geng", "Mingrui", ""], ["Deng", "Yun", ""], ["Zhao", "Qian", ""], ["Xie", "Qi", ""], ["Zeng", "Dong", ""], ["Zeng", "Dong", ""], ["Zuo", "Wangmeng", ""], ["Meng", "Deyu", ""]]}, {"id": "1808.02631", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Chunhua Shen, Ian Reid", "title": "Training Compact Neural Networks with Binary Weights and Low Precision\n  Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to train a network with binary weights and\nlow-bitwidth activations, designed especially for mobile devices with limited\npower consumption. Most previous works on quantizing CNNs uncritically assume\nthe same architecture, though with reduced precision. However, we take the view\nthat for best performance it is possible (and even likely) that a different\narchitecture may be better suited to dealing with low precision weights and\nactivations.\n  Specifically, we propose a \"network expansion\" strategy in which we aggregate\na set of homogeneous low-precision branches to implicitly reconstruct the\nfull-precision intermediate feature maps. Moreover, we also propose a\ngroup-wise feature approximation strategy which is very flexible and highly\naccurate. Experiments on ImageNet classification tasks demonstrate the superior\nperformance of the proposed model, named Group-Net, over various popular\narchitectures. In particular, with binary weights and activations, we\noutperform the previous best binary neural network in terms of accuracy as well\nas saving more than 5 times computational complexity on ImageNet with ResNet-18\nand ResNet-50.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 05:32:23 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1808.02632", "submitter": "Pan Lu", "authors": "Peng Gao, Pan Lu, Hongsheng Li, Shuang Li, Yikang Li, Steven Hoi,\n  Xiaogang Wang", "title": "Question-Guided Hybrid Convolution for Visual Question Answering", "comments": "17 pages, 4 figures, accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC)\nnetwork for Visual Question Answering (VQA). Most state-of-the-art VQA methods\nfuse the high-level textual and visual features from the neural network and\nabandon the visual spatial information when learning multi-modal features.To\naddress these problems, question-guided kernels generated from the input\nquestion are designed to convolute with visual features for capturing the\ntextual and visual relationship in the early stage. The question-guided\nconvolution can tightly couple the textual and visual information but also\nintroduce more parameters when learning kernels. We apply the group\nconvolution, which consists of question-independent kernels and\nquestion-dependent kernels, to reduce the parameter size and alleviate\nover-fitting. The hybrid convolution can generate discriminative multi-modal\nfeatures with fewer parameters. The proposed approach is also complementary to\nexisting bilinear pooling fusion and attention based VQA methods. By\nintegrating with them, our method could further boost the performance.\nExtensive experiments on public VQA datasets validate the effectiveness of\nQGHC.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 05:39:00 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Gao", "Peng", ""], ["Lu", "Pan", ""], ["Li", "Hongsheng", ""], ["Li", "Shuang", ""], ["Li", "Yikang", ""], ["Hoi", "Steven", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1808.02651", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai,\n  Alec Jacobson", "title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically\n  Differentiable Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning image classifiers are vulnerable to adversarial\nattacks, inputs with perturbations designed to intentionally trigger\nmisclassification. Current adversarial methods directly alter pixel colors and\nevaluate against pixel norm-balls: pixel perturbations smaller than a specified\nmagnitude, according to a measurement norm. This evaluation, however, has\nlimited practical utility since perturbations in the pixel space do not\ncorrespond to underlying real-world phenomena of image formation that lead to\nthem and has no security motivation attached. Pixels in natural images are\nmeasurements of light that has interacted with the geometry of a physical\nscene. As such, we propose the direct perturbation of physical parameters that\nunderly image formation: lighting and geometry. As such, we propose a novel\nevaluation measure, parametric norm-balls, by directly perturbing physical\nparameters that underly image formation. One enabling contribution we present\nis a physically-based differentiable renderer that allows us to propagate pixel\ngradients to the parametric space of lighting and geometry. Our approach\nenables physically-based adversarial attacks, and our differentiable renderer\nleverages models from the interactive rendering literature to balance the\nperformance and accuracy trade-offs necessary for a memory-efficient and\nscalable adversarial data augmentation workflow.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 08:01:18 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 22:12:01 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Tao", "Michael", ""], ["Li", "Chun-Liang", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "1808.02668", "submitter": "Corentin Kervadec", "authors": "Valentin Vielzeuf, Corentin Kervadec, St\\'ephane Pateux, Alexis\n  Lechervy, Fr\\'ed\\'eric Jurie", "title": "An Occam's Razor View on Learning Audiovisual Emotion Recognition with\n  Small Training Sets", "comments": null, "journal-ref": "ICMI (EmotiW) 2018, Oct 2018, Boulder, Colorado, United States", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a light-weight and accurate deep neural model for\naudiovisual emotion recognition. To design this model, the authors followed a\nphilosophy of simplicity, drastically limiting the number of parameters to\nlearn from the target datasets, always choosing the simplest earning methods:\ni) transfer learning and low-dimensional space embedding allows to reduce the\ndimensionality of the representations. ii) The isual temporal information is\nhandled by a simple score-per-frame selection process, averaged across time.\niii) A simple frame selection echanism is also proposed to weight the images of\na sequence. iv) The fusion of the different modalities is performed at\nprediction level (late usion). We also highlight the inherent challenges of the\nAFEW dataset and the difficulty of model selection with as few as 383\nvalidation equences. The proposed real-time emotion classifier achieved a\nstate-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at\nhe Emotion in the Wild 2018 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 08:43:43 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Kervadec", "Corentin", ""], ["Pateux", "St\u00e9phane", ""], ["Lechervy", "Alexis", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1808.02775", "submitter": "Hidenobu Matsuki", "authors": "Hidenobu Matsuki, Lukas von Stumberg, Vladyslav Usenko, J\\\"org\n  St\\\"uckler, Daniel Cremers", "title": "Omnidirectional DSO: Direct Sparse Odometry with Fisheye Cameras", "comments": "Accepted by IEEE Robotics and Automation Letters (RA-L), 2018 and\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),\n  2018", "journal-ref": null, "doi": "10.1109/LRA.2018.2855443", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel real-time direct monocular visual odometry for\nomnidirectional cameras. Our method extends direct sparse odometry (DSO) by\nusing the unified omnidirectional model as a projection function, which can be\napplied to fisheye cameras with a field-of-view (FoV) well above 180 degrees.\nThis formulation allows for using the full area of the input image even with\nstrong distortion, while most existing visual odometry methods can only use a\nrectified and cropped part of it. Model parameters within an active keyframe\nwindow are jointly optimized, including the intrinsic/extrinsic camera\nparameters, 3D position of points, and affine brightness parameters. Thanks to\nthe wide FoV, image overlap between frames becomes bigger and points are more\nspatially distributed. Our results demonstrate that our method provides\nincreased accuracy and robustness over state-of-the-art visual odometry\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 13:34:31 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Matsuki", "Hidenobu", ""], ["von Stumberg", "Lukas", ""], ["Usenko", "Vladyslav", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1808.02792", "submitter": "Isaac Gerg", "authors": "Isaac D Gerg", "title": "Multiband SAS Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Advances in unmanned synthetic aperture sonar (SAS) imaging platforms allow\nfor the simultaneous collection of multiband SAS imagery. The imagery is\ncollected over several octaves and the phenomenology's interactions with the\nsea floor vary greatly over this range -- higher frequencies resolve proud and\nfine structure of the seafloor while lower frequencies resolve subsurface\nfeatures and often induce internal resonance in man-made objects. Currently,\nanalysts examine multiband imagery by viewing a single band at a time. This\nmethod makes it difficult to ascertain correlations between any pair of bands\ncollected over the same location. To mitigate this issue, we propose methods\nwhich ingest high frequency (HF) and low frequency (LF) SAS imagery and\ngenerates a color composite creating what we call a multiband SAS (MSAS) image.\nThe MSAS image contains the relevant portions of the HF and LF images required\nby an analyst to interpret the scene and are defined using a spatial saliency\nmetric computed for each image. We then combine the saliency and acoustic\nbackscatter measures to form the final MSAS image.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 14:19:38 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Gerg", "Isaac D", ""]]}, {"id": "1808.02837", "submitter": "Rui Fan", "authors": "Rui Fan, Mohammud Junaid Bocus, Naim Dahnoun", "title": "A Novel Disparity Transformation Algorithm for Road Segmentation", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disparity information provided by stereo cameras has enabled advanced\ndriver assistance systems to estimate road area more accurately and\neffectively. In this paper, a novel disparity transformation algorithm is\nproposed to extract road areas from dense disparity maps by making the\ndisparity value of the road pixels become similar. The transformation is\nachieved using two parameters: roll angle and fitted disparity value with\nrespect to each row. To achieve a better processing efficiency, golden section\nsearch and dynamic programming are utilised to estimate the roll angle and the\nfitted disparity value, respectively. By performing a rotation around the\nestimated roll angle, the disparity distribution of each row becomes very\ncompact. This further improves the accuracy of the road model estimation, as\ndemonstrated by the various experimental results in this paper. Finally, the\nOtsu's thresholding method is applied to the transformed disparity map and the\nroads can be accurately segmented at pixel level.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:50:22 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Fan", "Rui", ""], ["Bocus", "Mohammud Junaid", ""], ["Dahnoun", "Naim", ""]]}, {"id": "1808.02848", "submitter": "Thomas Peron", "authors": "Thomas Peron, Francisco A. Rodrigues, Luciano da F. Costa", "title": "Pattern Recognition Approach to Violin Shapes of MIMO database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the landmarks established by the Cremonese school in the 16th century,\nthe history of violin design has been marked by experimentation. While great\neffort has been invested since the early 19th century by the scientific\ncommunity on researching violin acoustics, substantially less attention has\nbeen given to the statistical characterization of how the violin shape evolved\nover time. In this paper we study the morphology of violins retrieved from the\nMusical Instrument Museums Online (MIMO) database -- the largest freely\naccessible platform providing information about instruments held in public\nmuseums. From the violin images, we derive a set of measurements that reflect\nrelevant geometrical features of the instruments. The application of Principal\nComponent Analysis (PCA) uncovered similarities between violin makers and their\nrespective copyists, as well as among luthiers belonging to the same family\nlineage, in the context of historical narrative. Combined with a time-windowed\napproach, thin plate splines visualizations revealed that the average violin\noutline has remained mostly stable over time, not adhering to any particular\ntrends of design across different periods in music history.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 16:21:08 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Peron", "Thomas", ""], ["Rodrigues", "Francisco A.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1808.02856", "submitter": "Matthew Trager", "authors": "Matthew Trager, Brian Osserman, Jean Ponce", "title": "On the Solvability of Viewing Graphs", "comments": "22 pages, 8 figures, presented at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of fundamental matrices relating pairs of cameras in some configuration\ncan be represented as edges of a \"viewing graph\". Whether or not these\nfundamental matrices are generically sufficient to recover the global camera\nconfiguration depends on the structure of this graph. We study\ncharacterizations of \"solvable\" viewing graphs and present several new results\nthat can be applied to determine which pairs of views may be used to recover\nall camera parameters. We also discuss strategies for verifying the solvability\nof a graph computationally.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 16:46:21 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 15:12:46 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Trager", "Matthew", ""], ["Osserman", "Brian", ""], ["Ponce", "Jean", ""]]}, {"id": "1808.02861", "submitter": "Ramprasaath R. Selvaraju", "authors": "Ramprasaath R. Selvaraju, Prithvijit Chattopadhyay, Mohamed Elhoseiny,\n  Tilak Sharma, Dhruv Batra, Devi Parikh, Stefan Lee", "title": "Choose Your Neuron: Incorporating Domain Knowledge through\n  Neuron-Importance", "comments": "In Proceedings of ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual neurons in convolutional neural networks supervised for\nimage-level classification tasks have been shown to implicitly learn\nsemantically meaningful concepts ranging from simple textures and shapes to\nwhole or partial objects - forming a \"dictionary\" of concepts acquired through\nthe learning process. In this work we introduce a simple, efficient zero-shot\nlearning approach based on this observation. Our approach, which we call Neuron\nImportance-AwareWeight Transfer (NIWT), learns to map domain knowledge about\nnovel \"unseen\" classes onto this dictionary of learned concepts and then\noptimizes for network parameters that can effectively combine these concepts -\nessentially learning classifiers by discovering and composing learned semantic\nconcepts in deep networks. Our approach shows improvements over previous\napproaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.\nWe demonstrate our approach on a diverse set of semantic inputs as external\ndomain knowledge including attributes and natural language captions. Moreover\nby learning inverse mappings, NIWT can provide visual and textual explanations\nfor the predictions made by the newly learned classifiers and provide neuron\nnames. Our code is available at\nhttps://github.com/ramprs/neuron-importance-zsl.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 17:00:43 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Selvaraju", "Ramprasaath R.", ""], ["Chattopadhyay", "Prithvijit", ""], ["Elhoseiny", "Mohamed", ""], ["Sharma", "Tilak", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1808.02868", "submitter": "Isaac Gerg", "authors": "Isaac Gerg and David Williams", "title": "Additional Representations for Improving Synthetic Aperture Sonar\n  Classification Using Convolutional Neural Networks", "comments": "Accepted for the Institute of Acoustics 4th International Conference\n  on Synthetic Aperture Sonar and Radar Sept 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification in synthetic aperture sonar (SAS) imagery is usually a\ndata starved and class imbalanced problem. There are few objects of interest\npresent among much benign seafloor. Despite these problems, current\nclassification techniques discard a large portion of the collected SAS\ninformation. In particular, a beamformed SAS image, which we call a single-look\ncomplex (SLC) image, contains complex pixels composed of real and imaginary\nparts. For human consumption, the SLC is converted to a magnitude-phase\nrepresentation and the phase information is discarded. Even more problematic,\nthe magnitude information usually exhibits a large dynamic range (>80dB) and\nmust be dynamic range compressed for human display. Often it is this dynamic\nrange compressed representation, originally designed for human consumption,\nwhich is fed into a classifier. Consequently, the classification process is\ncompletely void of the phase information. In this work, we show improvements in\nclassification performance using the phase information from the SLC as well as\ninformation from an alternate source: photographs. We perform statistical\ntesting to demonstrate the validity of our results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 17:15:12 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 12:37:05 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 13:22:39 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2018 15:29:26 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Gerg", "Isaac", ""], ["Williams", "David", ""]]}, {"id": "1808.02870", "submitter": "Terry Taewoong Um", "authors": "Terry Taewoong Um, Franz Michael Josef Pfister, Daniel Christian\n  Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, Dana\n  Kuli\\'c", "title": "Parkinson's Disease Assessment from a Wrist-Worn Wearable Sensor in\n  Free-Living Conditions: Deep Ensemble Learning and Visualization", "comments": "This is a pre-print of an article published in Annals of Biomedical\n  Engineering (ABME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's Disease (PD) is characterized by disorders in motor function such\nas freezing of gait, rest tremor, rigidity, and slowed and hyposcaled\nmovements. Medication with dopaminergic medication may alleviate those motor\nsymptoms, however, side-effects may include uncontrolled movements, known as\ndyskinesia. In this paper, an automatic PD motor-state assessment in\nfree-living conditions is proposed using an accelerometer in a wrist-worn\nwearable sensor. In particular, an ensemble of convolutional neural networks\n(CNNs) is applied to capture the large variability of daily-living activities\nand overcome the dissimilarity between training and test patients due to the\ninter-patient variability. In addition, class activation map (CAM), a\nvisualization technique for CNNs, is applied for providing an interpretation of\nthe results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 17:17:15 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Um", "Terry Taewoong", ""], ["Pfister", "Franz Michael Josef", ""], ["Pichler", "Daniel Christian", ""], ["Endo", "Satoshi", ""], ["Lang", "Muriel", ""], ["Hirche", "Sandra", ""], ["Fietzek", "Urban", ""], ["Kuli\u0107", "Dana", ""]]}, {"id": "1808.02874", "submitter": "Johannes Rieke", "authors": "Johannes Rieke, Fabian Eitel, Martin Weygandt, John-Dylan Haynes,\n  Kerstin Ritter", "title": "Visualizing Convolutional Networks for MRI-based Diagnosis of\n  Alzheimer's Disease", "comments": "MLCN 2018", "journal-ref": null, "doi": "10.1007/978-3-030-02628-8_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing and interpreting convolutional neural networks (CNNs) is an\nimportant task to increase trust in automatic medical decision making systems.\nIn this study, we train a 3D CNN to detect Alzheimer's disease based on\nstructural MRI scans of the brain. Then, we apply four different gradient-based\nand occlusion-based visualization methods that explain the network's\nclassification decisions by highlighting relevant areas in the input image. We\ncompare the methods qualitatively and quantitatively. We find that all four\nmethods focus on brain regions known to be involved in Alzheimer's disease,\nsuch as inferior and middle temporal gyrus. While the occlusion-based methods\nfocus more on specific regions, the gradient-based methods pick up distributed\nrelevance patterns. Additionally, we find that the distribution of relevance\nvaries across patients, with some having a stronger focus on the temporal lobe,\nwhereas for others more cortical areas are relevant. In summary, we show that\napplying different visualization methods is important to understand the\ndecisions of a CNN, a step that is crucial to increase clinical impact and\ntrust in computer-based decision support systems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 17:36:10 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Rieke", "Johannes", ""], ["Eitel", "Fabian", ""], ["Weygandt", "Martin", ""], ["Haynes", "John-Dylan", ""], ["Ritter", "Kerstin", ""]]}, {"id": "1808.02917", "submitter": "Jinming Duan", "authors": "Jinming Duan, Weicheng Xie, Ryan Wen Liu, Christopher Tench, Irene\n  Gottlob, Frank Proudlock, Li Bai", "title": "OCT segmentation: Integrating open parametric contour model of the\n  retinal layers and shape constraint to the Mumford-Shah functional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel retinal layer boundary model for\nsegmentation of optical coherence tomography (OCT) images. The retinal layer\nboundary model consists of 9 open parametric contours representing the 9\nretinal layers in OCT images. An intensity-based Mumford-Shah (MS) variational\nfunctional is first defined to evolve the retinal layer boundary model to\nsegment the 9 layers simultaneously. By making use of the normals of open\nparametric contours, we construct equal sized adjacent narrowbands that are\ndivided by each contour. Regional information in each narrowband can thus be\nintegrated into the MS energy functional such that its optimisation is robust\nagainst different initialisations. A statistical prior is also imposed on the\nshape of the segmented parametric contours for the functional. As such, by\nminimising the MS energy functional the parametric contours can be driven\ntowards the true boundaries of retinal layers, while the similarity of the\ncontours with respect to training OCT shapes is preserved. Experimental results\non real OCT images demonstrate that the method is accurate and robust to low\nquality OCT images with low contrast and high-level speckle noise, and it\noutperforms the recent geodesic distance based method for segmenting 9 layers\nof the retina in OCT images.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 19:29:21 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Duan", "Jinming", ""], ["Xie", "Weicheng", ""], ["Liu", "Ryan Wen", ""], ["Tench", "Christopher", ""], ["Gottlob", "Irene", ""], ["Proudlock", "Frank", ""], ["Bai", "Li", ""]]}, {"id": "1808.02956", "submitter": "Dongrui Wu", "authors": "Chenfeng Guo and Dongrui Wu", "title": "Feature Dimensionality Reduction for Video Affect Classification: A\n  Comparative Study", "comments": "1st Asian Affective Computing and Intelligent Interaction Conference,\n  Beijing, May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing has become a very important research area in\nhuman-machine interaction. However, affects are subjective, subtle, and\nuncertain. So, it is very difficult to obtain a large number of labeled\ntraining samples, compared with the number of possible features we could\nextract. Thus, dimensionality reduction is critical in affective computing.\nThis paper presents our preliminary study on dimensionality reduction for\naffect classification. Five popular dimensionality reduction approaches are\nintroduced and compared. Experiments on the DEAP dataset showed that no\napproach can universally outperform others, and performing classification using\nthe raw features directly may not always be a bad choice.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:29:45 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Guo", "Chenfeng", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.02992", "submitter": "Wenbing Huang", "authors": "Lijie Fan, Wenbing Huang, Chuang Gan, Junzhou Huang, Boqing Gong", "title": "Controllable Image-to-Video Translation: A Case Study on Facial\n  Expression Generation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in deep learning have made it possible to generate\nphoto-realistic images by using neural networks and even to extrapolate video\nframes from an input video clip. In this paper, for the sake of both furthering\nthis exploration and our own interest in a realistic application, we study\nimage-to-video translation and particularly focus on the videos of facial\nexpressions. This problem challenges the deep neural networks by another\ntemporal dimension comparing to the image-to-image translation. Moreover, its\nsingle input image fails most existing video generation methods that rely on\nrecurrent models. We propose a user-controllable approach so as to generate\nvideo clips of various lengths from a single face image. The lengths and types\nof the expressions are controlled by users. To this end, we design a novel\nneural network architecture that can incorporate the user input into its skip\nconnections and propose several improvements to the adversarial training method\nfor the neural network. Experiments and user studies verify the effectiveness\nof our approach. Especially, we would like to highlight that even for the face\nimages in the wild (downloaded from the Web and the authors' own photos), our\nmodel can generate high-quality facial expression videos of which about 50\\%\nare labeled as real by Amazon Mechanical Turk workers.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 01:38:01 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Fan", "Lijie", ""], ["Huang", "Wenbing", ""], ["Gan", "Chuang", ""], ["Huang", "Junzhou", ""], ["Gong", "Boqing", ""]]}, {"id": "1808.02996", "submitter": "Hiroki Miyamoto", "authors": "Hiroki Miyamoto, Kazuki Uehara, Masahiro Murakawa, Hidenori Sakanashi,\n  Hirokazu Nosato, Toru Kouyama, Ryosuke Nakamura", "title": "Object Detection in Satellite Imagery using 2-Step Convolutional Neural\n  Networks", "comments": "4 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient object detection method from satellite\nimagery. Among a number of machine learning algorithms, we proposed a\ncombination of two convolutional neural networks (CNN) aimed at high precision\nand high recall, respectively. We validated our models using golf courses as\ntarget objects. The proposed deep learning method demonstrated higher accuracy\nthan previous object identification methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 02:12:43 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Miyamoto", "Hiroki", ""], ["Uehara", "Kazuki", ""], ["Murakawa", "Masahiro", ""], ["Sakanashi", "Hidenori", ""], ["Nosato", "Hirokazu", ""], ["Kouyama", "Toru", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1808.03002", "submitter": "Xiaofeng Xie", "authors": "Xiaofeng Xie, ZhuLiang Yu, Zhenghui Gu and Yuanqing Li", "title": "An Iterative Boundary Random Walks Algorithm for Interactive Image\n  Segmentation", "comments": "9 pages, 9 figures, 1 tabel. Project\n  URL:https://xiaofengxie.github.io/project/my-project-name/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interactive image segmentation algorithm can provide an intelligent ways\nto understand the intention of user input. Many interactive methods have the\nproblem of that ask for large number of user input. To efficient produce\nintuitive segmentation under limited user input is important for industrial\napplication. In this paper, we reveal a positive feedback system on image\nsegmentation to show the pixels of self-learning. Two approaches, iterative\nrandom walks and boundary random walks, are proposed for segmentation\npotential, which is the key step in feedback system. Experiment results on\nimage segmentation indicates that proposed algorithms can obtain more efficient\ninput to random walks. And higher segmentation performance can be obtained by\napplying the iterative boundary random walks algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 02:50:59 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Xie", "Xiaofeng", ""], ["Yu", "ZhuLiang", ""], ["Gu", "Zhenghui", ""], ["Li", "Yuanqing", ""]]}, {"id": "1808.03015", "submitter": "Ji He", "authors": "Ji He and Jianhua Ma", "title": "Radon Inversion via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radon transform is widely used in physical and life sciences and one of its\nmajor applications is the X-ray computed tomography (X-ray CT), which is\nsignificant in modern health examination. The Radon inversion or image\nreconstruction is challenging due to the potentially defective radon\nprojections. Conventionally, the reconstruction process contains several ad hoc\nstages to approximate the corresponding Radon inversion. Each of the stages is\nhighly dependent on the results of the previous stage. In this paper, we\npropose a novel unified framework for Radon inversion via deep learning (DL).\nThe Radon inversion can be approximated by the proposed framework with an\nend-to-end fashion instead of processing step-by-step with multiple stages. For\nsimplicity, the proposed framework is short as iRadonMap (inverse Radon\ntransform approximation). Specifically, we implement the iRadonMap as an\nappropriative neural network, of which the architecture can be divided into two\nsegments. In the first segment, a learnable fully-connected filtering layer is\nused to filter the radon projections along the view-angle direction, which is\nfollowed by a learnable sinusoidal back-projection layer to transfer the\nfiltered radon projections into an image. The second segment is a common neural\nnetwork architecture to further improve the reconstruction performance in the\nimage domain. The iRadonMap is overall optimized by training a large number of\ngeneric images from ImageNet database. To evaluate the performance of the\niRadonMap, clinical patient data is used. Qualitative results show promising\nreconstruction performance of the iRadonMap.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 04:19:08 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["He", "Ji", ""], ["Ma", "Jianhua", ""]]}, {"id": "1808.03041", "submitter": "Fei Wen", "authors": "Fei Wen, Danping Zou, Rendong Ying, and Peilin Liu", "title": "Efficient Outlier Removal in Large Scale Global Structure-from-Motion", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the outlier removal problem in large-scale global\nstructure-from-motion. In such applications, global outlier removal is very\nuseful to mitigate the deterioration caused by mismatches in the feature point\nmatching step. Unlike existing outlier removal methods, we exploit the\nstructure in multiview geometry problems to propose a dimension reduced\nformulation, based on which two methods have been developed. The first method\nconsiders a convex relaxed $\\ell_1$ minimization and is solved by a single\nlinear programming (LP), whilst the second one approximately solves the ideal\n$\\ell_0$ minimization by an iteratively reweighted method. The dimension\nreduction results in a significant speedup of the new algorithms. Further, the\niteratively reweighted method can significantly reduce the possibility of\nremoving true inliers. Realistic multiview reconstruction experiments\ndemonstrated that, compared with state-of-the-art algorithms, the new\nalgorithms are much more efficient and meanwhile can give improved solution.\nMatlab code for reproducing the results is available at\n\\textit{https://github.com/FWen/OUTLR.git}.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 07:05:18 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 05:31:29 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 07:49:52 GMT"}, {"version": "v4", "created": "Fri, 15 Feb 2019 08:03:37 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Wen", "Fei", ""], ["Zou", "Danping", ""], ["Ying", "Rendong", ""], ["Liu", "Peilin", ""]]}, {"id": "1808.03053", "submitter": "Valentin Brimkov", "authors": "Boris Brimkov and Valentin E. Brimkov", "title": "Optimal conditions for connectedness of discretized sets", "comments": "9 pages, 1 figure with 2 subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing a discretization of a given set is a major problem in various\ntheoretical and applied disciplines. An offset discretization of a set $X$ is\nobtained by taking the integer points inside a closed neighborhood of $X$ of a\ncertain radius. In this note we determine a minimum threshold for the offset\nradius, beyond which the discretization of a disconnected set is always\nconnected. The results hold for a broad class of disconnected and unbounded\nsubsets of $R^n$, and generalize several previous results. Algorithmic aspects\nand possible applications are briefly discussed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 08:17:46 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Brimkov", "Boris", ""], ["Brimkov", "Valentin E.", ""]]}, {"id": "1808.03082", "submitter": "Alptekin Temizel", "authors": "Cihan \\\"Ong\\\"un, Alptekin Temizel", "title": "Paired 3D Model Generation with Conditional Generative Adversarial\n  Networks", "comments": "Published in ECCV 2018 Workshops, Springer, LNCS. Cite this paper as:\n  Ongun C., Temizel A. (2019) Paired 3D Model Generation with Conditional\n  Generative Adversarial Networks. In: Leal-Taixe L., Roth S. (eds) Computer\n  Vision-ECCV 2018 Workshops. ECCV 2018. Lecture Notes in Computer Science, vol\n  11129. Springer, Cham", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are shown to be successful at\ngenerating new and realistic samples including 3D object models. Conditional\nGAN, a variant of GANs, allows generating samples in given conditions. However,\nobjects generated for each condition are different and it does not allow\ngeneration of the same object in different conditions. In this paper, we first\nadapt conditional GAN, which is originally designed for 2D image generation, to\nthe problem of generating 3D models in different rotations. We then propose a\nnew approach to guide the network to generate the same 3D sample in different\nand controllable rotation angles (sample pairs). Unlike previous studies, the\nproposed method does not require modification of the standard conditional GAN\narchitecture and it can be integrated into the training step of any conditional\nGAN. Experimental results and visual comparison of 3D models show that the\nproposed method is successful at generating model pairs in different\nconditions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 10:58:18 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 12:37:59 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["\u00d6ng\u00fcn", "Cihan", ""], ["Temizel", "Alptekin", ""]]}, {"id": "1808.03114", "submitter": "Alex B\\\"auerle", "authors": "Alex B\\\"auerle, Heiko Neumann and Timo Ropinski", "title": "Classifier-Guided Visual Correction of Noisy Labels for Image\n  Classification Tasks", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.13973", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data plays an essential role in modern applications of machine\nlearning. However, gathering labeled training data is time-consuming.\nTherefore, labeling is often outsourced to less experienced users, or\ncompletely automated. This can introduce errors, which compromise valuable\ntraining data, and lead to suboptimal training results. We thus propose a novel\napproach that uses the power of pretrained classifiers to visually guide users\nto noisy labels, and let them interactively check error candidates, to\niteratively improve the training data set. To systematically investigate\ntraining data, we propose a categorization of labeling errors into three\ndifferent types, based on an analysis of potential pitfalls in label\nacquisition processes. For each of these types, we present approaches to\ndetect, reason about, and resolve error candidates, as we propose measures and\nvisual guidance techniques to support machine learning users. Our approach has\nbeen used to spot errors in well-known machine learning benchmark data sets,\nand we tested its usability during a user evaluation. While initially developed\nfor images, the techniques presented in this paper are independent of the\nclassification algorithm, and can also be extended to many other types of\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 12:34:33 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 12:07:13 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 12:20:21 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2020 13:55:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["B\u00e4uerle", "Alex", ""], ["Neumann", "Heiko", ""], ["Ropinski", "Timo", ""]]}, {"id": "1808.03195", "submitter": "Benjamin Bischke", "authors": "Benjamin Bischke, Patrick Helber, Florian K\\\"onig, Damian Borth,\n  Andreas Dengel", "title": "Overcoming Missing and Incomplete Modalities with Generative Adversarial\n  Networks for Building Footprint Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of information acquired with different modalities, spatial\nresolution and spectral bands has shown to improve predictive accuracies. Data\nfusion is therefore one of the key challenges in remote sensing. Most prior\nwork focusing on multi-modal fusion, assumes that modalities are always\navailable during inference. This assumption limits the applications of\nmulti-modal models since in practice the data collection process is likely to\ngenerate data with missing, incomplete or corrupted modalities. In this paper,\nwe show that Generative Adversarial Networks can be effectively used to\novercome the problems that arise when modalities are missing or incomplete.\nFocusing on semantic segmentation of building footprints with missing\nmodalities, our approach achieves an improvement of about 2% on the\nIntersection over Union (IoU) against the same network that relies only on the\navailable modality.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 15:24:56 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Bischke", "Benjamin", ""], ["Helber", "Patrick", ""], ["K\u00f6nig", "Florian", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1808.03232", "submitter": "Simone Meyer", "authors": "Simone Meyer, Victor Cornill\\`ere, Abdelaziz Djelouah, Christopher\n  Schroers, Markus Gross", "title": "Deep Video Color Propagation", "comments": "BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for color propagation in videos rely on some form of\nmatching between consecutive video frames. Using appearance descriptors, colors\nare then propagated both spatially and temporally. These methods, however, are\ncomputationally expensive and do not take advantage of semantic information of\nthe scene. In this work we propose a deep learning framework for color\npropagation that combines a local strategy, to propagate colors frame-by-frame\nensuring temporal stability, and a global strategy, using semantics for color\npropagation within a longer range. Our evaluation shows the superiority of our\nstrategy over existing video and image color propagation methods as well as\nneural photo-realistic style transfer approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:54:18 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Meyer", "Simone", ""], ["Cornill\u00e8re", "Victor", ""], ["Djelouah", "Abdelaziz", ""], ["Schroers", "Christopher", ""], ["Gross", "Markus", ""]]}, {"id": "1808.03240", "submitter": "Yuanzheng Ci", "authors": "Yuanzheng Ci, Xinzhu Ma, Zhihui Wang, Haojie Li and Zhongxuan Luo", "title": "User-Guided Deep Anime Line Art Colorization with Conditional\n  Adversarial Networks", "comments": "Accepted for publication at the 2018 ACM Multimedia Conference (MM\n  '18)", "journal-ref": null, "doi": "10.1145/3240508.3240661", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scribble colors based line art colorization is a challenging computer vision\nproblem since neither greyscale values nor semantic information is presented in\nline arts, and the lack of authentic illustration-line art training pairs also\nincreases difficulty of model generalization. Recently, several Generative\nAdversarial Nets (GANs) based methods have achieved great success. They can\ngenerate colorized illustrations conditioned on given line art and color hints.\nHowever, these methods fail to capture the authentic illustration distributions\nand are hence perceptually unsatisfying in the sense that they often lack\naccurate shading. To address these challenges, we propose a novel deep\nconditional adversarial architecture for scribble based anime line art\ncolorization. Specifically, we integrate the conditional framework with WGAN-GP\ncriteria as well as the perceptual loss to enable us to robustly train a deep\nnetwork that makes the synthesized images more natural and real. We also\nintroduce a local features network that is independent of synthetic data. With\nGANs conditioned on features from such network, we notably increase the\ngeneralization capability over \"in the wild\" line arts. Furthermore, we collect\ntwo datasets that provide high-quality colorful illustrations and authentic\nline arts for training and benchmarking. With the proposed model trained on our\nillustration dataset, we demonstrate that images synthesized by the presented\napproach are considerably more realistic and precise than alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:17:47 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 05:25:00 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Ci", "Yuanzheng", ""], ["Ma", "Xinzhu", ""], ["Wang", "Zhihui", ""], ["Li", "Haojie", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1808.03247", "submitter": "Jiajun Wu", "authors": "Shaoxiong Wang, Jiajun Wu, Xingyuan Sun, Wenzhen Yuan, William T.\n  Freeman, Joshua B. Tenenbaum, Edward H. Adelson", "title": "3D Shape Perception from Monocular Vision, Touch, and Shape Priors", "comments": "IROS 2018. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving accurate 3D object shape is important for robots to interact with\nthe physical world. Current research along this direction has been primarily\nrelying on visual observations. Vision, however useful, has inherent\nlimitations due to occlusions and the 2D-3D ambiguities, especially for\nperception with a monocular camera. In contrast, touch gets precise local shape\ninformation, though its efficiency for reconstructing the entire shape could be\nlow. In this paper, we propose a novel paradigm that efficiently perceives\naccurate 3D object shape by incorporating visual and tactile observations, as\nwell as prior knowledge of common object shapes learned from large-scale shape\nrepositories. We use vision first, applying neural networks with learned shape\npriors to predict an object's 3D shape from a single-view color image. We then\nuse tactile sensing to refine the shape; the robot actively touches the object\nregions where the visual prediction has high uncertainty. Our method\nefficiently builds the 3D shape of common objects from a color image and a\nsmall number of tactile explorations (around 10). Our setup is easy to apply\nand has potentials to help robots better perform grasping or manipulation tasks\non real-world objects.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:30:49 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Wang", "Shaoxiong", ""], ["Wu", "Jiajun", ""], ["Sun", "Xingyuan", ""], ["Yuan", "Wenzhen", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Adelson", "Edward H.", ""]]}, {"id": "1808.03305", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Richard Zemel, John K. Tsotsos", "title": "The Elephant in the Room", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We showcase a family of common failures of state-of-the art object detectors.\nThese are obtained by replacing image sub-regions by another sub-image that\ncontains a trained object. We call this \"object transplanting\". Modifying an\nimage in this manner is shown to have a non-local impact on object detection.\nSlight changes in object position can affect its identity according to an\nobject detector as well as that of other objects in the image. We provide some\nanalysis and suggest possible reasons for the reported phenomena.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 18:58:59 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Zemel", "Richard", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1808.03338", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Daniel McDuff", "title": "DeepMag: Source Specific Motion Magnification Using Gradient Ascent", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important physical phenomena involve subtle signals that are difficult\nto observe with the unaided eye, yet visualizing them can be very informative.\nCurrent motion magnification techniques can reveal these small temporal\nvariations in video, but require precise prior knowledge about the target\nsignal, and cannot deal with interference motions at a similar frequency. We\npresent DeepMag an end-to-end deep neural video-processing framework based on\ngradient ascent that enables automated magnification of subtle color and motion\nsignals from a specific source, even in the presence of large motions of\nvarious velocities. While the approach is generalizable, the advantages of\nDeepMag are highlighted via the task of video-based physiological\nvisualization. Through systematic quantitative and qualitative evaluation of\nthe approach on videos with different levels of head motion, we compare the\nmagnification of pulse and respiration to existing state-of-the-art methods.\nOur method produces magnified videos with substantially fewer artifacts and\nblurring whilst magnifying the physiological changes by a similar degree.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 20:36:57 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Chen", "Weixuan", ""], ["McDuff", "Daniel", ""]]}, {"id": "1808.03344", "submitter": "Xuechen Zhang", "authors": "Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang, Jing-Hao Xue", "title": "Deep Learning for Single Image Super-Resolution: A Brief Review", "comments": "Accepted by IEEE Transactions on Multimedia (TMM)", "journal-ref": null, "doi": "10.1109/TMM.2019.2919431", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) is a notoriously challenging ill-posed\nproblem, which aims to obtain a high-resolution (HR) output from one of its\nlow-resolution (LR) versions. To solve the SISR problem, recently powerful deep\nlearning algorithms have been employed and achieved the state-of-the-art\nperformance. In this survey, we review representative deep learning-based SISR\nmethods, and group them into two categories according to their major\ncontributions to two essential aspects of SISR: the exploration of efficient\nneural network architectures for SISR, and the development of effective\noptimization objectives for deep SISR learning. For each category, a baseline\nis firstly established and several critical limitations of the baseline are\nsummarized. Then representative works on overcoming these limitations are\npresented based on their original contents as well as our critical\nunderstandings and analyses, and relevant comparisons are conducted from a\nvariety of perspectives. Finally we conclude this review with some vital\ncurrent challenges and future trends in SISR leveraging deep learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 20:51:51 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 08:32:19 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 02:48:41 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Yang", "Wenming", ""], ["Zhang", "Xuechen", ""], ["Tian", "Yapeng", ""], ["Wang", "Wei", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "1808.03399", "submitter": "Napa Sae-Bae", "authors": "NapaSae-Bae and NasirMemon and Pitikhate Sooraksa", "title": "Distinctiveness, complexity, and repeatability of online signature\n  templates", "comments": "Preprint version of Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2018.07.024", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes three measures to quantify the characteristics of online\nsignature templates in terms of distinctiveness, complexity and repeatability.\nA distinctiveness measure of a signature template is computed from a set of\nenrolled signature samples and a statistical assumption about random\nsignatures. Secondly, a complexity measure of the template is derived from a\nset of enrolled signature samples. Finally, given a signature template, a\nmeasure to quantify the repeatability of the online signature is derived from a\nvalidation set of samples. These three measures can then be used as an\nindicator for the performance of the system in rejecting random forgery samples\nand skilled forgery samples and the performance of users in providing accepted\ngenuine samples, respectively. The effectiveness of these three measures and\ntheir applications are demonstrated through experiments performed on three\nonline signature datasets and one keystroke dynamics dataset using different\nverification algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 03:01:45 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["NapaSae-Bae", "", ""], ["NasirMemon", "", ""], ["Sooraksa", "Pitikhate", ""]]}, {"id": "1808.03405", "submitter": "Wenhan Luo", "authors": "Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang", "title": "End-to-end Active Object Tracking and Its Real-world Deployment via\n  Reinforcement Learning", "comments": "To appear in Transactions on Pattern Analysis and Machine\n  Intelligence. arXiv admin note: text overlap with arXiv:1705.10561", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study active object tracking, where a tracker takes visual observations\n(i.e., frame sequences) as input and produces the corresponding camera control\nsignals as output (e.g., move forward, turn left, etc.). Conventional methods\ntackle tracking and camera control tasks separately, and the resulting system\nis difficult to tune jointly. These methods also require significant human\nefforts for image labeling and expensive trial-and-error system tuning in the\nreal world. To address these issues, we propose, in this paper, an end-to-end\nsolution via deep reinforcement learning. A ConvNet-LSTM function approximator\nis adopted for the direct frame-to-action prediction. We further propose an\nenvironment augmentation technique and a customized reward function, which are\ncrucial for successful training. The tracker trained in simulators (ViZDoom and\nUnreal Engine) demonstrates good generalization behaviors in the case of unseen\nobject moving paths, unseen object appearances, unseen backgrounds, and\ndistracting objects. The system is robust and can restore tracking after\noccasional lost of the target being tracked. We also find that the tracking\nability, obtained solely from simulators, can potentially transfer to\nreal-world scenarios. We demonstrate successful examples of such transfer, via\nexperiments over the VOT dataset and the deployment of a real-world robot using\nthe proposed active tracker trained in simulation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 04:04:19 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 09:20:10 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Luo", "Wenhan", ""], ["Sun", "Peng", ""], ["Zhong", "Fangwei", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""], ["Wang", "Yizhou", ""]]}, {"id": "1808.03417", "submitter": "Tony Tung", "authors": "Zorah Laehner, Daniel Cremers, Tony Tung", "title": "DeepWrinkles: Accurate and Realistic Clothing Modeling", "comments": "18 pages, 12 figures, 15th European Conference on Computer Vision\n  (ECCV) 2018, Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to generate accurate and realistic clothing\ndeformation from real data capture. Previous methods for realistic cloth\nmodeling mainly rely on intensive computation of physics-based simulation (with\nnumerous heuristic parameters), while models reconstructed from visual\nobservations typically suffer from lack of geometric details. Here, we propose\nan original framework consisting of two modules that work jointly to represent\nglobal shape deformation as well as surface details with high fidelity. Global\nshape deformations are recovered from a subspace model learned from 3D data of\nclothed people in motion, while high frequency details are added to normal maps\ncreated using a conditional Generative Adversarial Network whose architecture\nis designed to enforce realism and temporal consistency. This leads to\nunprecedented high-quality rendering of clothing deformation sequences, where\nfine wrinkles from (real) high resolution observations can be recovered. In\naddition, as the model is learned independently from body shape and pose, the\nframework is suitable for applications that require retargeting (e.g., body\nanimation). Our experiments show original high quality results with a flexible\nmodel. We claim an entirely data-driven approach to realistic cloth wrinkle\ngeneration is possible.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:45:36 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Laehner", "Zorah", ""], ["Cremers", "Daniel", ""], ["Tung", "Tony", ""]]}, {"id": "1808.03426", "submitter": "Sang-Hyuk Jung", "authors": "Yeong Chan Lee, Sang-Hyuk Jung, and Hong-Hee Won", "title": "WonDerM: Skin Lesion Classification with Fine-tuned Neural Networks", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As skin cancer is one of the most frequent cancers globally, accurate,\nnon-invasive dermoscopy-based diagnosis becomes essential and promising. A task\nof the Part 3 of the ISIC Skin Image Analysis Challenge at MICCAI 2018 is to\npredict seven disease classes with skin lesion images, including melanoma\n(MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis /\nBowen's disease (intraepithelial carcinoma) (AKIEC), benign keratosis (solar\nlentigo / seborrheic keratosis / lichen planus-like keratosis) (BKL),\ndermatofibroma (DF) and vascular lesion (VASC) as defined by the International\nDermatology Society.\n  In this work, we design the WonDerM pipeline, that resamples the preprocessed\nskin lesion images, builds neural network architecture fine-tuned with\nsegmentation task data (the Part 1), and uses an ensemble method to classify\nthe seven skin diseases. Our model achieved an accuracy of 0.899 and 0.785 in\nthe validation set and test set, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 06:40:58 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 10:13:16 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 05:18:12 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Lee", "Yeong Chan", ""], ["Jung", "Sang-Hyuk", ""], ["Won", "Hong-Hee", ""]]}, {"id": "1808.03457", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Zhilei Liu, Jianfei Cai, Yunsheng Wu, Lizhuang Ma", "title": "Facial Action Unit Detection Using Attention and Relation Learning", "comments": "This paper is accepted by IEEE Transactions on Affective Computing", "journal-ref": null, "doi": "10.1109/TAFFC.2019.2948635", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism has recently attracted increasing attentions in the field\nof facial action unit (AU) detection. By finding the region of interest of each\nAU with the attention mechanism, AU-related local features can be captured.\nMost of the existing attention based AU detection works use prior knowledge to\npredefine fixed attentions or refine the predefined attentions within a small\nrange, which limits their capacity to model various AUs. In this paper, we\npropose an end-to-end deep learning based attention and relation learning\nframework for AU detection with only AU labels, which has not been explored\nbefore. In particular, multi-scale features shared by each AU are learned\nfirstly, and then both channel-wise and spatial attentions are adaptively\nlearned to select and extract AU-related local features. Moreover, pixel-level\nrelations for AUs are further captured to refine spatial attentions so as to\nextract more relevant local features. Without changing the network\narchitecture, our framework can be easily extended for AU intensity estimation.\nExtensive experiments show that our framework (i) soundly outperforms the\nstate-of-the-art methods for both AU detection and AU intensity estimation on\nthe challenging BP4D, DISFA, FERA 2015 and BP4D+ benchmarks, (ii) can\nadaptively capture the correlated regions of each AU, and (iii) also works well\nunder severe occlusions and large poses.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 08:51:30 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 09:10:22 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 07:36:31 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Shao", "Zhiwen", ""], ["Liu", "Zhilei", ""], ["Cai", "Jianfei", ""], ["Wu", "Yunsheng", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1808.03485", "submitter": "Arno Solin", "authors": "Santiago Cort\\'es, Arno Solin, Juho Kannala", "title": "Deep Learning Based Speed Estimation for Constraining Strapdown Inertial\n  Navigation on Smartphones", "comments": "To appear in IEEE International Workshop on Machine Learning for\n  Signal Processing (MLSP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strapdown inertial navigation systems are sensitive to the quality of the\ndata provided by the accelerometer and gyroscope. Low-grade IMUs in handheld\nsmart-devices pose a problem for inertial odometry on these devices. We propose\na scheme for constraining the inertial odometry problem by complementing\nnon-linear state estimation by a CNN-based deep-learning model for inferring\nthe momentary speed based on a window of IMU samples. We show the feasibility\nof the model using a wide range of data from an iPhone, and present\nproof-of-concept results for how the model can be combined with an inertial\nnavigation system for three-dimensional inertial navigation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 11:03:58 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Cort\u00e9s", "Santiago", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1808.03513", "submitter": "Krzysztof Domino", "authors": "Przemys{\\l}aw G{\\l}omb, Krzysztof Domino, Micha{\\l} Romaszewski,\n  Micha{\\l} Cholewa", "title": "Band selection with Higher Order Multivariate Cumulants for small target\n  detection in hyperspectral images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the small target detection problem a pattern to be located is on the order\nof magnitude less numerous than other patterns present in the dataset. This\napplies both to the case of supervised detection, where the known template is\nexpected to match in just a few areas and unsupervised anomaly detection, as\nanomalies are rare by definition. This problem is frequently related to the\nimaging applications, i.e. detection within the scene acquired by a camera. To\nmaximize available data about the scene, hyperspectral cameras are used; at\neach pixel, they record spectral data in hundreds of narrow bands.\n  The typical feature of hyperspectral imaging is that characteristic\nproperties of target materials are visible in the small number of bands, where\nlight of certain wavelength interacts with characteristic molecules. A\ntarget-independent band selection method based on statistical principles is a\nversatile tool for solving this problem in different practical applications.\n  Combination of a regular background and a rare standing out anomaly will\nproduce a distortion in the joint distribution of hyperspectral pixels. Higher\nOrder Cumulants Tensors are a natural `window' into this distribution, allowing\nto measure properties and suggest candidate bands for removal. While there have\nbeen attempts at producing band selection algorithms based on the 3 rd\ncumulant's tensor i.e. the joint skewness, the literature lacks a systematic\nanalysis of how the order of the cumulant tensor used affects effectiveness of\nband selection in detection applications. In this paper we present an analysis\nof a general algorithm for band selection based on higher order cumulants. We\ndiscuss its usability related to the observed breaking points in performance,\ndepending both on method order and the desired number of bands. Finally we\nperform experiments and evaluate these methods in a hyperspectral detection\nscenario.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 12:50:52 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["G\u0142omb", "Przemys\u0142aw", ""], ["Domino", "Krzysztof", ""], ["Romaszewski", "Micha\u0142", ""], ["Cholewa", "Micha\u0142", ""]]}, {"id": "1808.03550", "submitter": "Nantheera Anantrasirichai", "authors": "N. Anantrasirichai, Alin Achim, David Bull", "title": "Atmospheric turbulence mitigation for sequences with moving objects\n  using recursive image fusion", "comments": "IEEE International Conference on Image Processing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new method for mitigating the effects of atmospheric\ndistortion on observed sequences that include large moving objects. In order to\nprovide accurate detail from objects behind the distorting layer, we solve the\nspace-variant distortion problem using recursive image fusion based on the Dual\nTree Complex Wavelet Transform (DT-CWT). The moving objects are detected and\ntracked using the improved Gaussian mixture models (GMM) and Kalman filtering.\nNew fusion rules are introduced which work on the magnitudes and angles of the\nDT-CWT coefficients independently to achieve a sharp image and to reduce\natmospheric distortion, respectively. The subjective results show that the\nproposed method achieves better video quality than other existing methods with\ncompetitive speed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 13:59:54 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Anantrasirichai", "N.", ""], ["Achim", "Alin", ""], ["Bull", "David", ""]]}, {"id": "1808.03571", "submitter": "Michael Kellman", "authors": "Michael R. Kellman, Emrah Bostan, Nicole Repina, and Laura Waller", "title": "Physics-based Learned Design: Optimized Coded-Illumination for\n  Quantitative Phase Imaging", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded-illumination can enable quantitative phase microscopy of transparent\nsamples with minimal hardware requirements. Intensity images are captured with\ndifferent source patterns and a non-linear phase retrieval optimization\nreconstructs the image. The non-linear nature of the processing makes\noptimizing the illumination pattern designs complicated. Traditional techniques\nfor experimental design (e.g. condition number optimization, spectral analysis)\nconsider only linear measurement formation models and linear reconstructions.\nDeep neural networks (DNNs) can efficiently represent the non-linear process\nand can be optimized over via training in an end-to-end framework. However,\nDNNs typically require a large amount of training examples and parameters to\nproperly learn the phase retrieval process, without making use of the known\nphysical models. Here, we aim to use both our knowledge of the physics and the\npower of machine learning together. We develop a new data-driven approach to\noptimizing coded-illumination patterns for a LED array microscope for a given\nphase reconstruction algorithm. Our method incorporates both the physics of the\nmeasurement scheme and the non-linearity of the reconstruction algorithm into\nthe design problem. This enables efficient parameterization, which allows us to\nuse only a small number of training examples to learn designs that generalize\nwell in the experimental setting without retraining. We show experimental\nresults for both a well-characterized phase target and mouse fibroblast cells\nusing coded-illumination patterns optimized for a sparsity-based phase\nreconstruction algorithm. Our learned design results using 2 measurements\ndemonstrate similar accuracy to Fourier Ptychography with 69 measurements.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 14:58:28 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 03:31:36 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 04:09:12 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Kellman", "Michael R.", ""], ["Bostan", "Emrah", ""], ["Repina", "Nicole", ""], ["Waller", "Laura", ""]]}, {"id": "1808.03575", "submitter": "Qizhu Li", "authors": "Qizhu Li, Anurag Arnab, Philip H.S. Torr", "title": "Weakly- and Semi-Supervised Panoptic Segmentation", "comments": "ECCV 2018. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weakly supervised model that jointly performs both semantic- and\ninstance-segmentation -- a particularly relevant problem given the substantial\ncost of obtaining pixel-perfect annotation for these tasks. In contrast to many\npopular instance segmentation approaches based on object detectors, our method\ndoes not predict any overlapping instances. Moreover, we are able to segment\nboth \"thing\" and \"stuff\" classes, and thus explain all the pixels in the image.\n\"Thing\" classes are weakly-supervised with bounding boxes, and \"stuff\" with\nimage-level tags. We obtain state-of-the-art results on Pascal VOC, for both\nfull and weak supervision (which achieves about 95% of fully-supervised\nperformance). Furthermore, we present the first weakly-supervised results on\nCityscapes for both semantic- and instance-segmentation. Finally, we use our\nweakly supervised framework to analyse the relationship between annotation\nquality and predictive performance, which is of interest to dataset creators.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:04:14 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 19:56:21 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 00:57:49 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Li", "Qizhu", ""], ["Arnab", "Anurag", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1808.03578", "submitter": "Noah Frazier-Logue", "authors": "Noah Frazier-Logue and Stephen Jos\\'e Hanson", "title": "Dropout is a special case of the stochastic delta rule: faster and more\n  accurate deep learning", "comments": "6 pages, 7 figures; submitted to ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layer neural networks have lead to remarkable performance on many kinds\nof benchmark tasks in text, speech and image processing. Nonlinear parameter\nestimation in hierarchical models is known to be subject to overfitting and\nmisspecification. One approach to these estimation and related problems (local\nminima, colinearity, feature discovery etc.) is called Dropout (Hinton, et al\n2012, Baldi et al 2016). The Dropout algorithm removes hidden units according\nto a Bernoulli random variable with probability $p$ prior to each update,\ncreating random \"shocks\" to the network that are averaged over updates. In this\npaper we will show that Dropout is a special case of a more general model\npublished originally in 1990 called the Stochastic Delta Rule, or SDR (Hanson,\n1990). SDR redefines each weight in the network as a random variable with mean\n$\\mu_{w_{ij}}$ and standard deviation $\\sigma_{w_{ij}}$. Each weight random\nvariable is sampled on each forward activation, consequently creating an\nexponential number of potential networks with shared weights. Both parameters\nare updated according to prediction error, thus resulting in weight noise\ninjections that reflect a local history of prediction error and local model\naveraging. SDR therefore implements a more sensitive local gradient-dependent\nsimulated annealing per weight converging in the limit to a Bayes optimal\nnetwork. Tests on standard benchmarks (CIFAR) using a modified version of\nDenseNet shows the SDR outperforms standard Dropout in test error by approx.\n$17\\%$ with DenseNet-BC 250 on CIFAR-100 and approx. $12-14\\%$ in smaller\nnetworks. We also show that SDR reaches the same accuracy that Dropout attains\nin 100 epochs in as few as 35 epochs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:06:05 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 20:58:09 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Frazier-Logue", "Noah", ""], ["Hanson", "Stephen Jos\u00e9", ""]]}, {"id": "1808.03609", "submitter": "Pulak Purkait", "authors": "Pulak Purkait and Ujwal Bonde and Christopher Zach", "title": "Weakly supervised learning of indoor geometry by dual warping", "comments": "3DV 2018, to appear, International Conference on 3D Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major element of depth perception and 3D understanding is the ability to\npredict the 3D layout of a scene and its contained objects for a novel pose.\nIndoor environments are particularly suitable for novel view prediction, since\nthe set of objects in such environments is relatively restricted. In this work\nwe address the task of 3D prediction especially for indoor scenes by leveraging\nonly weak supervision. In the literature 3D scene prediction is usually solved\nvia a 3D voxel grid. However, such methods are limited to estimating rather\ncoarse 3D voxel grids, since predicting entire voxel spaces has large\ncomputational costs. Hence, our method operates in image-space rather than in\nvoxel space, and the task of 3D estimation essentially becomes a depth image\ncompletion problem. We propose a novel approach to easily generate training\ndata containing depth maps with realistic occlusions, and subsequently train a\nnetwork for completing those occluded regions. Using multiple publicly\navailable dataset~\\cite{song2017semantic,Silberman:ECCV12} we benchmark our\nmethod against existing approaches and are able to obtain superior performance.\nWe further demonstrate the flexibility of our method by presenting results for\nnew view synthesis of RGB-D images.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 16:31:51 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Purkait", "Pulak", ""], ["Bonde", "Ujwal", ""], ["Zach", "Christopher", ""]]}, {"id": "1808.03735", "submitter": "Bochen Guan", "authors": "Bochen Guan, Hanrong Ye, Hong Liu, William A. Sethares", "title": "Video Logo Retrieval based on local Features", "comments": "Accepted by ICIP 20. Contact author: Bochen Guan (gbochen@wisc.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the frequency and duration of logos in videos is important and\nchallenging in the advertisement industry as a way of estimating the impact of\nad purchases. Since logos occupy only a small area in the videos, the popular\nmethods of image retrieval could fail. This paper develops an algorithm called\nVideo Logo Retrieval (VLR), which is an image-to-video retrieval algorithm\nbased on the spatial distribution of local image descriptors that measure the\ndistance between the query image (the logo) and a collection of video images.\nVLR uses local features to overcome the weakness of global feature-based models\nsuch as convolutional neural networks (CNN). Meanwhile, VLR is flexible and\ndoes not require training after setting some hyper-parameters. The performance\nof VLR is evaluated on two challenging open benchmark tasks (SoccerNet and\nStandford I2V), and compared with other state-of-the-art logo retrieval or\ndetection algorithms. Overall, VLR shows significantly higher accuracy compared\nwith the existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 01:27:40 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 22:14:51 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 21:34:18 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 21:55:34 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Guan", "Bochen", ""], ["Ye", "Hanrong", ""], ["Liu", "Hong", ""], ["Sethares", "William A.", ""]]}, {"id": "1808.03749", "submitter": "Hongyang Li", "authors": "Hongyang Li, Xiaoyang Guo, Bo Dai, Wanli Ouyang, Xiaogang Wang", "title": "Neural Network Encapsulation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A capsule is a collection of neurons which represents different variants of a\npattern in the network. The routing scheme ensures only certain capsules which\nresemble lower counterparts in the higher layer should be activated. However,\nthe computational complexity becomes a bottleneck for scaling up to larger\nnetworks, as lower capsules need to correspond to each and every higher\ncapsule. To resolve this limitation, we approximate the routing process with\ntwo branches: a master branch which collects primary information from its\ndirect contact in the lower layer and an aide branch that replenishes master\nbased on pattern variants encoded in other lower capsules. Compared with\nprevious iterative and unsupervised routing scheme, these two branches are\ncommunicated in a fast, supervised and one-time pass fashion. The complexity\nand runtime of the model are therefore decreased by a large margin. Motivated\nby the routing to make higher capsule have agreement with lower capsule, we\nextend the mechanism as a compensation for the rapid loss of information in\nnearby layers. We devise a feedback agreement unit to send back higher capsules\nas feedback. It could be regarded as an additional regularization to the\nnetwork. The feedback agreement is achieved by comparing the optimal transport\ndivergence between two distributions (lower and higher capsules). Such an\nadd-on witnesses a unanimous gain in both capsule and vanilla networks. Our\nproposed EncapNet performs favorably better against previous state-of-the-arts\non CIFAR10/100, SVHN and a subset of ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 04:36:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Hongyang", ""], ["Guo", "Xiaoyang", ""], ["Dai", "Bo", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1808.03766", "submitter": "Victor Escorcia", "authors": "Bernard Ghanem and Juan Carlos Niebles and Cees Snoek and Fabian Caba\n  Heilbron and Humam Alwassel and Victor Escorcia and Ranjay Krishna and\n  Shyamal Buch and Cuong Duc Dao", "title": "The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary", "comments": "CVPR Workshop 2018 challenge summary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3rd annual installment of the ActivityNet Large- Scale Activity\nRecognition Challenge, held as a full-day workshop in CVPR 2018, focused on the\nrecognition of daily life, high-level, goal-oriented activities from\nuser-generated videos as those found in internet video portals. The 2018\nchallenge hosted six diverse tasks which aimed to push the limits of semantic\nvisual understanding of videos as well as bridge visual content with human\ncaptions. Three out of the six tasks were based on the ActivityNet dataset,\nwhich was introduced in CVPR 2015 and organized hierarchically in a semantic\ntaxonomy. These tasks focused on tracing evidence of activities in time in the\nform of proposals, class labels, and captions. In this installment of the\nchallenge, we hosted three guest tasks to enrich the understanding of visual\ninformation in videos. The guest tasks focused on complementary aspects of the\nactivity recognition problem at large scale and involved three challenging and\nrecently compiled datasets: the Kinetics-600 dataset from Google DeepMind, the\nAVA dataset from Berkeley and Google, and the Moments in Time dataset from MIT\nand IBM Research.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 07:55:51 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 04:28:38 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Ghanem", "Bernard", ""], ["Niebles", "Juan Carlos", ""], ["Snoek", "Cees", ""], ["Heilbron", "Fabian Caba", ""], ["Alwassel", "Humam", ""], ["Escorcia", "Victor", ""], ["Krishna", "Ranjay", ""], ["Buch", "Shyamal", ""], ["Dao", "Cuong Duc", ""]]}, {"id": "1808.03823", "submitter": "Kai Xu", "authors": "Biao Leng, Cheng Zhang, Xiaocheng Zhou, Cheng Xu, Kai Xu", "title": "Learning Discriminative 3D Shape Representations by View Discerning\n  Networks", "comments": "Accepted by IEEE Transactions on Visualization and Computer Graphics.\n  Corresponding Author: Kai Xu (kevin.kai.xu@gmail.com)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view-based 3D shape recognition, extracting discriminative visual\nrepresentation of 3D shapes from projected images is considered the core\nproblem. Projections with low discriminative ability can adversely influence\nthe final 3D shape representation. Especially under the real situations with\nbackground clutter and object occlusion, the adverse effect is even more\nsevere. To resolve this problem, we propose a novel deep neural network, View\nDiscerning Network, which learns to judge the quality of views and adjust their\ncontributions to the representation of shapes. In this network, a Score\nGeneration Unit is devised to evaluate the quality of each projected image with\nscore vectors. These score vectors are used to weight the image features and\nthe weighted features perform much better than original features in 3D shape\nrecognition task. In particular, we introduce two structures of Score\nGeneration Unit, Channel-wise Score Unit and Part-wise Score Unit, to assess\nthe quality of feature maps from different perspectives. Our network aggregates\nfeatures and scores in an end-to-end framework, so that final shape descriptors\nare directly obtained from its output. Our experiments on ModelNet and ShapeNet\nCore55 show that View Discerning Network outperforms the state-of-the-arts in\nterms of the retrieval task, with excellent robustness against background\nclutter and object occlusion.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:09:45 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 21:22:36 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Leng", "Biao", ""], ["Zhang", "Cheng", ""], ["Zhou", "Xiaocheng", ""], ["Xu", "Cheng", ""], ["Xu", "Kai", ""]]}, {"id": "1808.03833", "submitter": "Abhinav Valada", "authors": "Abhinav Valada, Rohit Mohan, Wolfram Burgard", "title": "Self-Supervised Model Adaptation for Multimodal Semantic Segmentation", "comments": "A Live demo is available at http://deepscene.cs.uni-freiburg.de and\n  the code as well as the models are available at\n  https://github.com/DeepSceneSeg", "journal-ref": "International Journal of Computer Vision, 2019", "doi": "10.1007/s11263-019-01188-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to reliably perceive and understand the scene is an integral enabler\nfor robots to operate in the real-world. This problem is inherently challenging\ndue to the multitude of object types as well as appearance changes caused by\nvarying illumination and weather conditions. Leveraging complementary\nmodalities can enable learning of semantically richer representations that are\nresilient to such perturbations. Despite the tremendous progress in recent\nyears, most multimodal convolutional neural network approaches directly\nconcatenate feature maps from individual modality streams rendering the model\nincapable of focusing only on relevant complementary information for fusion. To\naddress this limitation, we propose a mutimodal semantic segmentation framework\nthat dynamically adapts the fusion of modality-specific features while being\nsensitive to the object category, spatial location and scene context in a\nself-supervised manner. Specifically, we propose an architecture consisting of\ntwo modality-specific encoder streams that fuse intermediate encoder\nrepresentations into a single decoder using our proposed self-supervised model\nadaptation fusion mechanism which optimally combines complementary features. As\nintermediate representations are not aligned across modalities, we introduce an\nattention scheme for better correlation. In addition, we propose a\ncomputationally efficient unimodal segmentation architecture termed AdapNet++\nthat incorporates a new encoder with multiscale residual units and an efficient\natrous spatial pyramid pooling that has a larger effective receptive field with\nmore than 10x fewer parameters, complemented with a strong decoder with a\nmulti-resolution supervision scheme that recovers high-resolution details.\nComprehensive empirical evaluations on several benchmarks demonstrate that both\nour unimodal and multimodal architectures achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:36:38 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 20:52:40 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 16:58:57 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Valada", "Abhinav", ""], ["Mohan", "Rohit", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1808.03844", "submitter": "Christopher Bridge", "authors": "Christopher P. Bridge, Michael Rosenthal, Bradley Wright, Gopal\n  Kotecha, Florian Fintelmann, Fabian Troschel, Nityanand Miskin, Khanant\n  Desai, William Wrobel, Ana Babic, Natalia Khalaf, Lauren Brais, Marisa Welch,\n  Caitlin Zellers, Neil Tenenholtz, Mark Michalski, Brian Wolpin, and Katherine\n  Andriole", "title": "Fully-Automated Analysis of Body Composition from CT in Cancer Patients\n  Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01201-4_22", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The amounts of muscle and fat in a person's body, known as body composition,\nare correlated with cancer risks, cancer survival, and cardiovascular risk. The\ncurrent gold standard for measuring body composition requires time-consuming\nmanual segmentation of CT images by an expert reader. In this work, we describe\na two-step process to fully automate the analysis of CT body composition using\na DenseNet to select the CT slice and U-Net to perform segmentation. We train\nand test our methods on independent cohorts. Our results show Dice scores\n(0.95-0.98) and correlation coefficients (R=0.99) that are favorable compared\nto human readers. These results suggest that fully automated body composition\nanalysis is feasible, which could enable both clinical use and large-scale\npopulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 18:10:22 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Bridge", "Christopher P.", ""], ["Rosenthal", "Michael", ""], ["Wright", "Bradley", ""], ["Kotecha", "Gopal", ""], ["Fintelmann", "Florian", ""], ["Troschel", "Fabian", ""], ["Miskin", "Nityanand", ""], ["Desai", "Khanant", ""], ["Wrobel", "William", ""], ["Babic", "Ana", ""], ["Khalaf", "Natalia", ""], ["Brais", "Lauren", ""], ["Welch", "Marisa", ""], ["Zellers", "Caitlin", ""], ["Tenenholtz", "Neil", ""], ["Michalski", "Mark", ""], ["Wolpin", "Brian", ""], ["Andriole", "Katherine", ""]]}, {"id": "1808.03887", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu and Pheng-Ann Heng", "title": "Semi-supervised Skin Lesion Segmentation via Transformation Consistent\n  Self-ensembling Model", "comments": "BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic skin lesion segmentation on dermoscopic images is an essential\ncomponent in computer-aided diagnosis of melanoma. Recently, many fully\nsupervised deep learning based methods have been proposed for automatic skin\nlesion segmentation. However, these approaches require massive pixel-wise\nannotation from experienced dermatologists, which is very costly and\ntime-consuming. In this paper, we present a novel semi-supervised method for\nskin lesion segmentation by leveraging both labeled and unlabeled data. The\nnetwork is optimized by the weighted combination of a common supervised loss\nfor labeled inputs only and a regularization loss for both labeled and\nunlabeled data. In this paper, we present a novel semi-supervised method for\nskin lesion segmentation, where the network is optimized by the weighted\ncombination of a common supervised loss for labeled inputs only and a\nregularization loss for both labeled and unlabeled data. Our method encourages\na consistent prediction for unlabeled images using the outputs of the\nnetwork-in-training under different regularizations, so that it can utilize the\nunlabeled data. To utilize the unlabeled data, our method encourages the\nconsistent predictions of the network-in-training for the same input under\ndifferent regularizations. Aiming for the semi-supervised segmentation problem,\nwe enhance the effect of regularization for pixel-level predictions by\nintroducing a transformation, including rotation and flipping, consistent\nscheme in our self-ensembling model. With only 300 labeled training samples,\nour method sets a new record on the benchmark of the International Skin Imaging\nCollaboration (ISIC) 2017 skin lesion segmentation challenge. Such a result\nclearly surpasses fully-supervised state-of-the-arts that are trained with 2000\nlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 03:57:29 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Li", "Xiaomeng", ""], ["Yu", "Lequan", ""], ["Chen", "Hao", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1808.03899", "submitter": "Yue Pan", "authors": "Yue Pan, Bisheng Yang, Fuxun Liang and Zhen Dong", "title": "Iterative Global Similarity Points : A robust coarse-to-fine integration\n  solution for pairwise 3D point cloud registration", "comments": "Accepted to International Conference on 3DVision (3DV) 2018 [8 pages,\n  6 figures and 3 tables]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a coarse-to-fine integration solution inspired by\nthe classical ICP algorithm, to pairwise 3D point cloud registration with two\nimprovements of hybrid metric spaces (eg, BSC feature and Euclidean geometry\nspaces) and globally optimal correspondences matching. First, we detect the\nkeypoints of point clouds and use the Binary Shape Context (BSC) descriptor to\nencode their local features. Then, we formulate the correspondence matching\ntask as an energy function, which models the global similarity of keypoints on\nthe hybrid spaces of BSC feature and Euclidean geometry. Next, we estimate the\nglobally optimal correspondences through optimizing the energy function by the\nKuhn-Munkres algorithm and then calculate the transformation based on the\ncorrespondences. Finally,we iteratively refine the transformation between two\npoint clouds by conducting optimal correspondences matching and transformation\ncalculation in a mutually reinforcing manner, to achieve the coarse-to-fine\nregistration under an unified framework.The proposed method is evaluated and\ncompared to several state-of-the-art methods on selected challenging datasets\nwith repetitive, symmetric and incomplete structures.Comprehensive experiments\ndemonstrate that the proposed IGSP algorithm obtains good performance and\noutperforms the state-of-the-art methods in terms of both rotation and\ntranslation errors.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 06:19:37 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Pan", "Yue", ""], ["Yang", "Bisheng", ""], ["Liang", "Fuxun", ""], ["Dong", "Zhen", ""]]}, {"id": "1808.03935", "submitter": "Hui Feng Prof.", "authors": "Hui Feng, Shanshan Wang, Shuzhi Sam Ge", "title": "Fine-grained visual recognition with salient feature detection", "comments": "10 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision based fine-grained recognition has received great attention\nin recent years. Existing works focus on discriminative part localization and\nfeature learning. In this paper, to improve the performance of fine-grained\nrecognition, we try to precisely locate as many salient parts of object as\npossible at first. Then, we figure out the classification probability that can\nbe obtained by using separate parts for object classification. Finally, through\nextracting efficient features from each part and combining them, then feeding\nto a classifier for recognition, an improved accuracy over state-of-art\nalgorithms has been obtained on CUB200-2011 bird dataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 13:05:52 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:53:20 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Feng", "Hui", ""], ["Wang", "Shanshan", ""], ["Ge", "Shuzhi Sam", ""]]}, {"id": "1808.03941", "submitter": "Yi Zhang", "authors": "Maosong Ran, Jinrong Hu, Yang Chen, Hu Chen, Huaiqiang Sun, Jiliu\n  Zhou, Yi Zhang", "title": "Denoising of 3-D Magnetic Resonance Images Using a Residual\n  Encoder-Decoder Wasserstein Generative Adversarial Network", "comments": "To appear on Medical Image Analysis. 29 pages, 15 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images\nis a critical step in medical image analysis. Over the past few years, many\nalgorithms with impressive performances have been proposed. In this paper,\ninspired by the idea of deep learning, we introduce an MRI denoising method\nbased on the residual encoder-decoder Wasserstein generative adversarial\nnetwork (RED-WGAN). Specifically, to explore the structure similarity between\nneighboring slices, a 3D configuration is utilized as the basic processing\nunit. Residual autoencoders combined with deconvolution operations are\nintroduced into the generator network. Furthermore, to alleviate the\noversmoothing shortcoming of the traditional mean squared error (MSE) loss\nfunction, the perceptual similarity, which is implemented by calculating the\ndistances in the feature space extracted by a pretrained VGG-19 network, is\nincorporated with the MSE and adversarial losses to form the new loss function.\nExtensive experiments are implemented to assess the performance of the proposed\nmethod. The experimental results show that the proposed RED-WGAN achieves\nperformance superior to several state-of-the-art methods in both simulated and\nreal clinical data. In particular, our method demonstrates powerful abilities\nin both noise suppression and structure preservation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 13:30:27 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 03:42:19 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Ran", "Maosong", ""], ["Hu", "Jinrong", ""], ["Chen", "Yang", ""], ["Chen", "Hu", ""], ["Sun", "Huaiqiang", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "1808.03944", "submitter": "Chengjia Wang", "authors": "Chengjia Wang, Gillian Macnaught, Giorgos Papanastasiou, Tom\n  MacGillivray, and David Newby", "title": "Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the cycle-consistent generative adversarial networks (CycleGAN) has\nbeen widely used for synthesis of multi-domain medical images. The\ndomain-specific nonlinear deformations captured by CycleGAN make the\nsynthesized images difficult to be used for some applications, for example,\ngenerating pseudo-CT for PET-MR attenuation correction. This paper presents a\ndeformation-invariant CycleGAN (DicycleGAN) method using deformable\nconvolutional layers and new cycle-consistency losses. Its robustness dealing\nwith data that suffer from domain-specific nonlinear deformations has been\nevaluated through comparison experiments performed on a multi-sequence brain MR\ndataset and a multi-modality abdominal dataset. Our method has displayed its\nability to generate synthesized data that is aligned with the source while\nmaintaining a proper quality of signal compared to CycleGAN-generated data. The\nproposed model also obtained comparable performance with CycleGAN when data\nfrom the source and target domains are alignable through simple affine\ntransformations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 13:49:19 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Chengjia", ""], ["Macnaught", "Gillian", ""], ["Papanastasiou", "Giorgos", ""], ["MacGillivray", "Tom", ""], ["Newby", "David", ""]]}, {"id": "1808.03959", "submitter": "Yuchao Dai Dr.", "authors": "Yiran Zhong, Hongdong Li, Yuchao Dai", "title": "Open-World Stereo Video Matching with Deep RNN", "comments": "Accepted by European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 15:41:54 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhong", "Yiran", ""], ["Li", "Hongdong", ""], ["Dai", "Yuchao", ""]]}, {"id": "1808.03969", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Ryota Hinami, Shin'ichi Satoh", "title": "Reconfigurable Inverted Index", "comments": "ACMMM 2018 (oral). Code: https://github.com/matsui528/rii", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approximate nearest neighbor search systems suffer from two\nfundamental problems that are of practical importance but have not received\nsufficient attention from the research community. First, although existing\nsystems perform well for the whole database, it is difficult to run a search\nover a subset of the database. Second, there has been no discussion concerning\nthe performance decrement after many items have been newly added to a system.\nWe develop a reconfigurable inverted index (Rii) to resolve these two issues.\nBased on the standard IVFADC system, we design a data layout such that items\nare stored linearly. This enables us to efficiently run a subset search by\nswitching the search method to a linear PQ scan if the size of a subset is\nsmall. Owing to the linear layout, the data structure can be dynamically\nadjusted after new items are added, maintaining the fast speed of the system.\nExtensive comparisons show that Rii achieves a comparable performance with\nstate-of-the art systems such as Faiss.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 16:47:47 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Matsui", "Yusuke", ""], ["Hinami", "Ryota", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1808.03986", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Sandeep Kumar, Vinod K. Kurmi, Vinay P. Namboodiri", "title": "Multimodal Differential Network for Visual Question Generation", "comments": "EMNLP 2018 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating natural questions from an image is a semantic task that requires\nusing visual and language modality to learn multimodal representations. Images\ncan have multiple visual and language contexts that are relevant for generating\nquestions namely places, captions, and tags. In this paper, we propose the use\nof exemplars for obtaining the relevant context. We obtain this by using a\nMultimodal Differential Network to produce natural and engaging questions. The\ngenerated questions show a remarkable similarity to the natural questions as\nvalidated by a human study. Further, we observe that the proposed approach\nsubstantially improves over state-of-the-art benchmarks on the quantitative\nmetrics (BLEU, METEOR, ROUGE, and CIDEr).\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 18:56:56 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 10:23:19 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Patro", "Badri N.", ""], ["Kumar", "Sandeep", ""], ["Kurmi", "Vinod K.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1808.04000", "submitter": "Mehmet G\\\"unel", "authors": "Mehmet G\\\"unel, Erkut Erdem, Aykut Erdem", "title": "Language Guided Fashion Image Manipulation with Feature-wise\n  Transformations", "comments": "Accepted to ECCV 2018, First Workshop on Computer Vision For Fashion,\n  Art and Design (extended version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing techniques for editing an outfit image through natural sentences\nand accordingly generating new outfits has promising applications for art,\nfashion and design. However, it is considered as a certainly challenging task\nsince image manipulation should be carried out only on the relevant parts of\nthe image while keeping the remaining sections untouched. Moreover, this\nmanipulation process should generate an image that is as realistic as possible.\nIn this work, we propose FiLMedGAN, which leverages feature-wise linear\nmodulation (FiLM) to relate and transform visual features with natural language\nrepresentations without using extra spatial information. Our experiments\ndemonstrate that this approach, when combined with skip connections and total\nvariation regularization, produces more plausible results than the baseline\nwork, and has a better localization capability when generating new outfits\nconsistent with the target description.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 20:18:41 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["G\u00fcnel", "Mehmet", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""]]}, {"id": "1808.04018", "submitter": "Huynh Manh", "authors": "Huynh Manh, Gita Alaghband", "title": "Scene-LSTM: A Model for Human Trajectory Prediction", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a human movement trajectory prediction system that incorporates\nthe scene information (Scene-LSTM) as well as human movement trajectories\n(Pedestrian movement LSTM) in the prediction process within static crowded\nscenes. We superimpose a two-level grid structure (scene is divided into grid\ncells each modeled by a scene-LSTM, which are further divided into smaller\nsub-grids for finer spatial granularity) and explore common human trajectories\noccurring in the grid cell (e.g., making a right or left turn onto sidewalks\ncoming out of an alley; or standing still at bus/train stops). Two coupled LSTM\nnetworks, Pedestrian movement LSTMs (one per target) and the corresponding\nScene-LSTMs (one per grid-cell) are trained simultaneously to predict the next\nmovements. We show that such common path information greatly influences\nprediction of future movement. We further design a scene data filter that holds\nimportant non-linear movement information. The scene data filter allows us to\nselect the relevant parts of the information from the grid cell's memory\nrelative to a target's state. We evaluate and compare two versions of our\nmethod with the Linear and several existing LSTM-based methods on five crowded\nvideo sequences from the UCY [1] and ETH [2] datasets. The results show that\nour method reduces the location displacement errors compared to related methods\nand specifically about 80% reduction compared to social interaction methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 23:19:36 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 16:10:52 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Manh", "Huynh", ""], ["Alaghband", "Gita", ""]]}, {"id": "1808.04028", "submitter": "Yuchao Dai Dr.", "authors": "Yiran Zhong and Yuchao Dai and Hongdong Li", "title": "3D Geometry-Aware Semantic Labeling of Outdoor Street Scenes", "comments": "Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of how to better exploit 3D\ngeometric information for dense semantic image labeling. Existing methods often\ntreat the available 3D geometry information (e.g., 3D depth-map) simply as an\nadditional image channel besides the R-G-B color channels, and apply the same\ntechnique for RGB image labeling. In this paper, we demonstrate that directly\nperforming 3D convolution in the framework of a residual connected 3D voxel\ntop-down modulation network can lead to superior results. Specifically, we\npropose a 3D semantic labeling method to label outdoor street scenes whenever a\ndense depth map is available. Experiments on the \"Synthia\" and \"Cityscape\"\ndatasets show our method outperforms the state-of-the-art methods, suggesting\nsuch a simple 3D representation is effective in incorporating 3D geometric\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 00:13:19 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1808.04063", "submitter": "Yatao Zhong", "authors": "Yatao Zhong, Bicheng Xu, Guang-Tong Zhou, Luke Bornn, Greg Mori", "title": "Time Perception Machine: Temporal Point Processes for the When, Where\n  and What of Activity Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous powerful point process models have been developed to understand\ntemporal patterns in sequential data from fields such as health-care,\nelectronic commerce, social networks, and natural disaster forecasting. In this\npaper, we develop novel models for learning the temporal distribution of human\nactivities in streaming data (e.g., videos and person trajectories). We propose\nan integrated framework of neural networks and temporal point processes for\npredicting when the next activity will happen. Because point processes are\nlimited to taking event frames as input, we propose a simple yet effective\nmechanism to extract features at frames of interest while also preserving the\nrich information in the remaining frames. We evaluate our model on two\nchallenging datasets. The results show that our model outperforms traditional\nstatistical point process approaches significantly, demonstrating its\neffectiveness in capturing the underlying temporal dynamics as well as the\ncorrelation within sequential activities. Furthermore, we also extend our model\nto a joint estimation framework for predicting the timing, spatial location,\nand category of the activity simultaneously, to answer the when, where, and\nwhat of activity prediction.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 04:48:07 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:36:27 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhong", "Yatao", ""], ["Xu", "Bicheng", ""], ["Zhou", "Guang-Tong", ""], ["Bornn", "Luke", ""], ["Mori", "Greg", ""]]}, {"id": "1808.04068", "submitter": "Bo Dong", "authors": "Bo Dong and Xinnian Wang", "title": "A Transfer Learning based Feature-Weak-Relevant Method for Image\n  Clustering", "comments": "17 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image clustering is to group a set of images into disjoint clusters in a way\nthat images in the same cluster are more similar to each other than to those in\nother clusters, which is an unsupervised or semi-supervised learning process.\nIt is a crucial and challenging task in machine learning and computer vision.\nThe performances of existing image clustering methods have close relations with\nfeatures used for clustering, even if unsupervised coding based methods have\nimproved the performances a lot. To reduce the effect of clustering features,\nwe propose a feature-weak-relevant method for image clustering. The proposed\nmethod converts an unsupervised clustering process into an alternative\niterative process of unsupervised learning and transfer learning. The\nclustering process firstly starts up from handcrafted features based image\nclustering to estimate an initial label for every image, and secondly use a\nproposed sampling strategy to choose images with reliable labels to feed a\ntransfer-learning model to learn representative features that can be used for\nnext round of unsupervised learning. In this manner, image clustering is\niteratively optimized. What's more, the handcrafted features are used to boot\nup the clustering process, and just have a little effect on the final\nperformance; therefore, the proposed method is feature-weak-relevant.\nExperimental results on six kinds of public available datasets show that the\nproposed method outperforms state of the art methods and depends less on the\nemployed features at the same time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 05:39:28 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:35:15 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Dong", "Bo", ""], ["Wang", "Xinnian", ""]]}, {"id": "1808.04108", "submitter": "Chia-Hung Wan", "authors": "Chia-Hung Wan, Shun-Po Chuang and Hung-Yi Lee", "title": "Towards Audio to Scene Image Synthesis using Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can imagine a scene from a sound. We want machines to do so by using\nconditional generative adversarial networks (GANs). By applying the techniques\nincluding spectral norm, projection discriminator and auxiliary classifier,\ncompared with naive conditional GAN, the model can generate images with better\nquality in terms of both subjective and objective evaluations. Almost\nthree-fourth of people agree that our model have the ability to generate images\nrelated to sounds. By inputting different volumes of the same sound, our model\noutput different scales of changes based on the volumes, showing that our model\ntruly knows the relationship between sounds and images to some extent.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 08:46:42 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wan", "Chia-Hung", ""], ["Chuang", "Shun-Po", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1808.04134", "submitter": "Wenchao Wang", "authors": "Wenchao Wang, Jianshu Zhang, Jun Du, Zi-Rui Wang and Yixing Zhu", "title": "DenseRAN for Offline Handwritten Chinese Character Recognition", "comments": "Accepted by ICFHR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, great success has been achieved in offline handwritten Chinese\ncharacter recognition by using deep learning methods. Chinese characters are\nmainly logographic and consist of basic radicals, however, previous research\nmostly treated each Chinese character as a whole without explicitly considering\nits internal two-dimensional structure and radicals. In this study, we propose\na novel radical analysis network with densely connected architecture (DenseRAN)\nto analyze Chinese character radicals and its two-dimensional structures\nsimultaneously. DenseRAN first encodes input image to high-level visual\nfeatures by employing DenseNet as an encoder. Then a decoder based on recurrent\nneural networks is employed, aiming at generating captions of Chinese\ncharacters by detecting radicals and two-dimensional structures through\nattention mechanism. The manner of treating a Chinese character as a\ncomposition of two-dimensional structures and radicals can reduce the size of\nvocabulary and enable DenseRAN to possess the capability of recognizing unseen\nChinese character classes, only if the corresponding radicals have been seen in\ntraining set. Evaluated on ICDAR-2013 competition database, the proposed\napproach significantly outperforms whole-character modeling approach with a\nrelative character error rate (CER) reduction of 18.54%. Meanwhile, for the\ncase of recognizing 3277 unseen Chinese characters in CASIA-HWDB1.2 database,\nDenseRAN can achieve a character accuracy of about 41% while the traditional\nwhole-character method has no capability to handle them.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 10:16:08 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Wenchao", ""], ["Zhang", "Jianshu", ""], ["Du", "Jun", ""], ["Wang", "Zi-Rui", ""], ["Zhu", "Yixing", ""]]}, {"id": "1808.04138", "submitter": "Wenchao Wang", "authors": "Wenchao Wang, Jun Du and Zi-Rui Wang", "title": "Parsimonious HMMs for Offline Handwritten Chinese Text Recognition", "comments": "Accepted by ICFHR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, hidden Markov models (HMMs) have achieved promising results for\noffline handwritten Chinese text recognition. However, due to the large\nvocabulary of Chinese characters with each modeled by a uniform and fixed\nnumber of hidden states, a high demand of memory and computation is required.\nIn this study, to address this issue, we present parsimonious HMMs via the\nstate tying which can fully utilize the similarities among different Chinese\ncharacters. Two-step algorithm with the data-driven question-set is adopted to\ngenerate the tied-state pool using the likelihood measure. The proposed\nparsimonious HMMs with both Gaussian mixture models (GMMs) and deep neural\nnetworks (DNNs) as the emission distributions not only lead to a compact model\nbut also improve the recognition accuracy via the data sharing for the tied\nstates and the confusion decreasing among state classes. Tested on ICDAR-2013\ncompetition database, in the best configured case, the new parsimonious DNN-HMM\ncan yield a relative character error rate (CER) reduction of 6.2%, 25%\nreduction of model size and 60% reduction of decoding time over the\nconventional DNN-HMM. In the compact setting case of average 1-state HMM, our\nparsimonious DNN-HMM significantly outperforms the conventional DNN-HMM with a\nrelative CER reduction of 35.5%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 10:26:56 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wang", "Wenchao", ""], ["Du", "Jun", ""], ["Wang", "Zi-Rui", ""]]}, {"id": "1808.04181", "submitter": "Thomas Probst", "authors": "Thomas Probst, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool", "title": "Incremental Non-Rigid Structure-from-Motion with Unknown Focal Length", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perspective camera and the isometric surface prior have recently gathered\nincreased attention for Non-Rigid Structure-from-Motion (NRSfM). Despite the\nrecent progress, several challenges remain, particularly the computational\ncomplexity and the unknown camera focal length. In this paper we present a\nmethod for incremental Non-Rigid Structure-from-Motion (NRSfM) with the\nperspective camera model and the isometric surface prior with unknown focal\nlength. In the template-based case, we provide a method to estimate four\nparameters of the camera intrinsics. For the template-less scenario of NRSfM,\nwe propose a method to upgrade reconstructions obtained for one focal length to\nanother based on local rigidity and the so-called Maximum Depth Heuristics\n(MDH). On its basis we propose a method to simultaneously recover the focal\nlength and the non-rigid shapes. We further solve the problem of incorporating\na large number of points and adding more views in MDH-based NRSfM and\nefficiently solve them with Second-Order Cone Programming (SOCP). This does not\nrequire any shape initialization and produces results orders of times faster\nthan many methods. We provide evaluations on standard sequences with\nground-truth and qualitative reconstructions on challenging YouTube videos.\nThese evaluations show that our method performs better in both speed and\naccuracy than the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 12:53:10 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Probst", "Thomas", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "1808.04187", "submitter": "Nils Gessert", "authors": "Nils Gessert, Matthias Lutz, Markus Heyder, Sarah Latus, David M.\n  Leistner, Youssef S. Abdelwahed, Alexander Schlaefer", "title": "Automatic Plaque Detection in IVOCT Pullbacks Using Convolutional Neural\n  Networks", "comments": "Accepted for Publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2865659", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary heart disease is a common cause of death despite being preventable.\nTo treat the underlying plaque deposits in the arterial walls, intravascular\noptical coherence tomography can be used by experts to detect and characterize\nthe lesions. In clinical routine, hundreds of images are acquired for each\npatient which requires automatic plaque detection for fast and accurate\ndecision support. So far, automatic approaches rely on classic machine learning\nmethods and deep learning solutions have rarely been studied. Given the success\nof deep learning methods with other imaging modalities, a thorough\nunderstanding of deep learning-based plaque detection for future clinical\ndecision support systems is required. We address this issue with a new dataset\nconsisting of in-vivo patient images labeled by three trained experts. Using\nthis dataset, we employ state-of-the-art deep learning models that directly\nlearn plaque classification from the images. For improved performance, we study\ndifferent transfer learning approaches. Furthermore, we investigate the use of\ncartesian and polar image representations and employ data augmentation\ntechniques tailored to each representation. We fuse both representations in a\nmulti-path architecture for more effective feature exploitation. Last, we\naddress the challenge of plaque differentiation in addition to detection.\nOverall, we find that our combined model performs best with an accuracy of\n91.7%, a sensitivity of 90.9% and a specificity of 92.4%. Our results indicate\nthat building a deep learning-based clinical decision support system for plaque\ndetection is feasible.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 13:01:14 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 09:59:41 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Gessert", "Nils", ""], ["Lutz", "Matthias", ""], ["Heyder", "Markus", ""], ["Latus", "Sarah", ""], ["Leistner", "David M.", ""], ["Abdelwahed", "Youssef S.", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1808.04205", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang", "title": "Partial Adversarial Domain Adaptation", "comments": "14 pages, ECCV 2018 poster. arXiv admin note: text overlap with\n  arXiv:1707.07901", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adversarial learning aligns the feature distributions across the\nsource and target domains in a two-player minimax game. Existing domain\nadversarial networks generally assume identical label space across different\ndomains. In the presence of big data, there is strong motivation of\ntransferring deep models from existing big domains to unknown small domains.\nThis paper introduces partial domain adaptation as a new domain adaptation\nscenario, which relaxes the fully shared label space assumption to that the\nsource label space subsumes the target label space. Previous methods typically\nmatch the whole source domain to the target domain, which are vulnerable to\nnegative transfer for the partial domain adaptation problem due to the large\nmismatch between label spaces. We present Partial Adversarial Domain Adaptation\n(PADA), which simultaneously alleviates negative transfer by down-weighing the\ndata of outlier source classes for training both source classifier and domain\nadversary, and promotes positive transfer by matching the feature distributions\nin the shared label space. Experiments show that PADA exceeds state-of-the-art\nresults for partial domain adaptation tasks on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 17:18:33 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Cao", "Zhangjie", ""], ["Ma", "Lijia", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""]]}, {"id": "1808.04228", "submitter": "Zhan Yang", "authors": "Zhan Yang, Osolo Ian Raymond, ChengYuan Zhang, Ying Wan, Jun Long", "title": "DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human\n  Activity Recognition", "comments": "19 pages, 5 figures, 6 tables, accepted by IEEE Access", "journal-ref": "IEEE ACCESS, vol. 6, pp. 56750-56764, 2018", "doi": "10.1109/ACCESS.2018.2873315", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) are currently popular in human\nactivity recognition applications. However, in the face of modern artificial\nintelligence sensor-based games, many research achievements cannot be\npractically applied on portable devices. DCNNs are typically resource-intensive\nand too large to be deployed on portable devices, thus this limits the\npractical application of complex activity detection. In addition, since\nportable devices do not possess high-performance Graphic Processing Units\n(GPUs), there is hardly any improvement in Action Game (ACT) experience.\nBesides, in order to deal with multi-sensor collaboration, all previous human\nactivity recognition models typically treated the representations from\ndifferent sensor signal sources equally. However, distinct types of activities\nshould adopt different fusion strategies. In this paper, a novel scheme is\nproposed. This scheme is used to train 2-bit Convolutional Neural Networks with\nweights and activations constrained to {-0.5,0,0.5}. It takes into account the\ncorrelation between different sensor signal sources and the activity types.\nThis model, which we refer to as DFTerNet, aims at producing a more reliable\ninference and better trade-offs for practical applications. Our basic idea is\nto exploit quantization of weights and activations directly in pre-trained\nfilter banks and adopt dynamic fusion strategies for different activity types.\nExperiments demonstrate that by using dynamic fusion strategy can exceed the\nbaseline model performance by up to ~5% on activity recognition like\nOPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we\nwere able to achieve performances closer to that of full-precision counterpart.\nThese results were also verified using the UniMiB-SHAR dataset. In addition,\nthe proposed method can achieve ~9x acceleration on CPUs and ~11x memory\nsaving.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:33:34 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 04:47:24 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""], ["Zhang", "ChengYuan", ""], ["Wan", "Ying", ""], ["Long", "Jun", ""]]}, {"id": "1808.04234", "submitter": "Oscar Chen", "authors": "Shitao Tang, Litong Feng, Zhangkui Kuang, Yimin Chen, Wei Zhang", "title": "Fast Video Shot Transition Localization with Deep Structured Models", "comments": "16 pages, 3 figures, submitted to ACCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of video shot transition is a crucial pre-processing step in video\nanalysis. Previous studies are restricted on detecting sudden content changes\nbetween frames through similarity measurement and multi-scale operations are\nwidely utilized to deal with transitions of various lengths. However,\nlocalization of gradual transitions are still under-explored due to the high\nvisual similarity between adjacent frames. Cut shot transitions are abrupt\nsemantic breaks while gradual shot transitions contain low-level\nspatial-temporal patterns caused by video effects in addition to the gradual\nsemantic breaks, e.g. dissolve. In order to address the problem, we propose a\nstructured network which is able to detect these two shot transitions using\ntargeted models separately. Considering speed performance trade-offs, we design\na smart framework. With one TITAN GPU, the proposed method can achieve a\n30\\(\\times\\) real-time speed. Experiments on public TRECVID07 and RAI databases\nshow that our method outperforms the state-of-the-art methods. In order to\ntrain a high-performance shot transition detector, we contribute a new database\nClipShots, which contains 128636 cut transitions and 38120 gradual transitions\nfrom 4039 online videos. ClipShots intentionally collect short videos for more\nhard cases caused by hand-held camera vibrations, large object motions, and\nocclusion.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:04:03 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Tang", "Shitao", ""], ["Feng", "Litong", ""], ["Kuang", "Zhangkui", ""], ["Chen", "Yimin", ""], ["Zhang", "Wei", ""]]}, {"id": "1808.04256", "submitter": "Chenyu You", "authors": "Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan,\n  Shenghong Ju, Zhen Zhao, Zhuiyang Zhang, Wenxiang Cong, Michael W. Vannier,\n  Punam K. Saha, Ge Wang", "title": "CT Super-resolution GAN Constrained by the Identical, Residual, and\n  Cycle Learning Ensemble(GAN-CIRCLE)", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging 2019", "doi": "10.1109/TMI.2019.2922960", "report-no": "TMI-2019-0250", "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) is widely used in screening, diagnosis, and\nimage-guided therapy for both clinical and research purposes. Since CT involves\nionizing radiation, an overarching thrust of related technical research is\ndevelopment of novel methods enabling ultrahigh quality imaging with fine\nstructural details while reducing the X-ray radiation. In this paper, we\npresent a semi-supervised deep learning approach to accurately recover\nhigh-resolution (HR) CT images from low-resolution (LR) counterparts.\nSpecifically, with the generative adversarial network (GAN) as the building\nblock, we enforce the cycle-consistency in terms of the Wasserstein distance to\nestablish a nonlinear end-to-end mapping from noisy LR input images to denoised\nand deblurred HR outputs. We also include the joint constraints in the loss\nfunction to facilitate structural preservation. In this deep imaging process,\nwe incorporate deep convolutional neural network (CNN), residual learning, and\nnetwork in network techniques for feature extraction and restoration. In\ncontrast to the current trend of increasing network depth and complexity to\nboost the CT imaging performance, which limit its real-world applications by\nimposing considerable computational and memory overheads, we apply a parallel\n$1\\times1$ CNN to compress the output of the hidden layer and optimize the\nnumber of layers and the number of filters for each convolutional layer.\nQuantitative and qualitative evaluations demonstrate that our proposed model is\naccurate, efficient and robust for super-resolution (SR) image restoration from\nnoisy LR input images. In particular, we validate our composite SR networks on\nthree large-scale CT datasets, and obtain promising results as compared to the\nother state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:33:23 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 17:56:08 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 20:28:30 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["You", "Chenyu", ""], ["Li", "Guang", ""], ["Zhang", "Yi", ""], ["Zhang", "Xiaoliu", ""], ["Shan", "Hongming", ""], ["Ju", "Shenghong", ""], ["Zhao", "Zhen", ""], ["Zhang", "Zhuiyang", ""], ["Cong", "Wenxiang", ""], ["Vannier", "Michael W.", ""], ["Saha", "Punam K.", ""], ["Wang", "Ge", ""]]}, {"id": "1808.04277", "submitter": "Teresa Ara\\'ujo", "authors": "Guilherme Aresta, Teresa Ara\\'ujo, Scotty Kwok, Sai Saketh\n  Chennamsetty, Mohammed Safwan, Varghese Alex, Bahram Marami, Marcel Prastawa,\n  Monica Chan, Michael Donovan, Gerardo Fernandez, Jack Zeineh, Matthias Kohl,\n  Christoph Walz, Florian Ludwig, Stefan Braunewell, Maximilian Baust, Quoc\n  Dang Vu, Minh Nguyen Nhat To, Eal Kim, Jin Tae Kwak, Sameh Galal, Veronica\n  Sanchez-Freire, Nadia Brancati, Maria Frucci, Daniel Riccio, Yaqi Wang,\n  Lingling Sun, Kaiqiang Ma, Jiannan Fang, Ismael Kone, Lahsen Boulmane,\n  Aur\\'elio Campilho, Catarina Eloy, Ant\\'onio Pol\\'onia, Paulo Aguiar", "title": "BACH: Grand Challenge on Breast Cancer Histology Images", "comments": "Accepted for publication at Medical Image Analysis (Elsevier).\n  Publication licensed under the Creative Commons CC-BY-NC-ND 4.0 license\n  http://creativecommons.org/licenses/by-nc-nd/4.0/", "journal-ref": "Medical Image Analysis, 2019", "doi": "10.1016/j.media.2019.05.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common invasive cancer in women, affecting more\nthan 10% of women worldwide. Microscopic analysis of a biopsy remains one of\nthe most important methods to diagnose the type of breast cancer. This requires\nspecialized analysis by pathologists, in a task that i) is highly time- and\ncost-consuming and ii) often leads to nonconsensual results. The relevance and\npotential of automatic classification algorithms using hematoxylin-eosin\nstained histopathological images has already been demonstrated, but the\nreported results are still sub-optimal for clinical use. With the goal of\nadvancing the state-of-the-art in automatic classification, the Grand Challenge\non BreAst Cancer Histology images (BACH) was organized in conjunction with the\n15th International Conference on Image Analysis and Recognition (ICIAR 2018). A\nlarge annotated dataset, composed of both microscopy and whole-slide images,\nwas specifically compiled and made publicly available for the BACH challenge.\nFollowing a positive response from the scientific community, a total of 64\nsubmissions, out of 677 registrations, effectively entered the competition.\nFrom the submitted algorithms it was possible to push forward the\nstate-of-the-art in terms of accuracy (87%) in automatic classification of\nbreast cancer with histopathological images. Convolutional neuronal networks\nwere the most successful methodology in the BACH challenge. Detailed analysis\nof the collective results allowed the identification of remaining challenges in\nthe field and recommendations for future developments. The BACH dataset remains\npublically available as to promote further improvements to the field of\nautomatic classification in digital pathology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:48:46 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 15:41:37 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Aresta", "Guilherme", ""], ["Ara\u00fajo", "Teresa", ""], ["Kwok", "Scotty", ""], ["Chennamsetty", "Sai Saketh", ""], ["Safwan", "Mohammed", ""], ["Alex", "Varghese", ""], ["Marami", "Bahram", ""], ["Prastawa", "Marcel", ""], ["Chan", "Monica", ""], ["Donovan", "Michael", ""], ["Fernandez", "Gerardo", ""], ["Zeineh", "Jack", ""], ["Kohl", "Matthias", ""], ["Walz", "Christoph", ""], ["Ludwig", "Florian", ""], ["Braunewell", "Stefan", ""], ["Baust", "Maximilian", ""], ["Vu", "Quoc Dang", ""], ["To", "Minh Nguyen Nhat", ""], ["Kim", "Eal", ""], ["Kwak", "Jin Tae", ""], ["Galal", "Sameh", ""], ["Sanchez-Freire", "Veronica", ""], ["Brancati", "Nadia", ""], ["Frucci", "Maria", ""], ["Riccio", "Daniel", ""], ["Wang", "Yaqi", ""], ["Sun", "Lingling", ""], ["Ma", "Kaiqiang", ""], ["Fang", "Jiannan", ""], ["Kone", "Ismael", ""], ["Boulmane", "Lahsen", ""], ["Campilho", "Aur\u00e9lio", ""], ["Eloy", "Catarina", ""], ["Pol\u00f3nia", "Ant\u00f3nio", ""], ["Aguiar", "Paulo", ""]]}, {"id": "1808.04285", "submitter": "Aruni RoyChowdhury", "authors": "SouYoung Jin, Aruni RoyChowdhury, Huaizu Jiang, Ashish Singh, Aditya\n  Prasad, Deep Chakraborty, Erik Learned-Miller", "title": "Unsupervised Hard Example Mining from Videos for Improved Object\n  Detection", "comments": "14 pages, 7 figures, accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important gains have recently been obtained in object detection by using\ntraining objectives that focus on {\\em hard negative} examples, i.e., negative\nexamples that are currently rated as positive or ambiguous by the detector.\nThese examples can strongly influence parameters when the network is trained to\ncorrect them. Unfortunately, they are often sparse in the training data, and\nare expensive to obtain. In this work, we show how large numbers of hard\nnegatives can be obtained {\\em automatically} by analyzing the output of a\ntrained detector on video sequences. In particular, detections that are {\\em\nisolated in time}, i.e., that have no associated preceding or following\ndetections, are likely to be hard negatives. We describe simple procedures for\nmining large numbers of such hard negatives (and also hard {\\em positives})\nfrom unlabeled video data. Our experiments show that retraining detectors on\nthese automatically obtained examples often significantly improves performance.\nWe present experiments on multiple architectures and multiple data sets,\nincluding face detection, pedestrian detection and other object categories.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:21:22 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Jin", "SouYoung", ""], ["RoyChowdhury", "Aruni", ""], ["Jiang", "Huaizu", ""], ["Singh", "Ashish", ""], ["Prasad", "Aditya", ""], ["Chakraborty", "Deep", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1808.04287", "submitter": "Paul Jasek", "authors": "Paul Jasek and Bernard Abayowa", "title": "Visual Sensor Network Reconfiguration with Deep Reinforcement Learning", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for reconfiguration of dynamic visual sensor networks\nwith deep reinforcement learning (RL). Our RL agent uses a modified\nasynchronous advantage actor-critic framework and the recently proposed\nRelational Network module at the foundation of its network architecture. To\naddress the issue of sample inefficiency in current approaches to model-free\nreinforcement learning, we train our system in an abstract simulation\nenvironment that represents inputs from a dynamic scene. Our system is\nvalidated using inputs from a real-world scenario and preexisting object\ndetection and tracking algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:24:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Jasek", "Paul", ""], ["Abayowa", "Bernard", ""]]}, {"id": "1808.04303", "submitter": "Sukho Lee", "authors": "Hyein Kim, Jungho Yoon, Byeongseon Jeong, and Sukho Lee", "title": "Rank-1 Convolutional Neural Network", "comments": "The paper is in 2 Column style 8 pages. It will be submitted to\n  CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a convolutional neural network(CNN) with 3-D rank-1\nfilters which are composed by the outer product of 1-D filters. After being\ntrained, the 3-D rank-1 filters can be decomposed into 1-D filters in the test\ntime for fast inference. The reason that we train 3-D rank-1 filters in the\ntraining stage instead of consecutive 1-D filters is that a better gradient\nflow can be obtained with this setting, which makes the training possible even\nin the case where the network with consecutive 1-D filters cannot be trained.\nThe 3-D rank-1 filters are updated by both the gradient flow and the outer\nproduct of the 1-D filters in every epoch, where the gradient flow tries to\nobtain a solution which minimizes the loss function, while the outer product\noperation tries to make the parameters of the filter to live on a rank-1\nsub-space. Furthermore, we show that the convolution with the rank-1 filters\nresults in low rank outputs, constraining the final output of the CNN also to\nlive on a low dimensional subspace.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:55:41 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Kim", "Hyein", ""], ["Yoon", "Jungho", ""], ["Jeong", "Byeongseon", ""], ["Lee", "Sukho", ""]]}, {"id": "1808.04311", "submitter": "Xiaofan Zhang", "authors": "Junsong Wang, Qiuwen Lou, Xiaofan Zhang, Chao Zhu, Yonghua Lin, Deming\n  Chen", "title": "Design Flow of Accelerating Hybrid Extremely Low Bit-width Neural\n  Network in Embedded FPGA", "comments": "Accepted by International Conference on Field-Programmable Logic and\n  Applications (FPL'2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network accelerators with low latency and low energy consumption are\ndesirable for edge computing. To create such accelerators, we propose a design\nflow for accelerating the extremely low bit-width neural network (ELB-NN) in\nembedded FPGAs with hybrid quantization schemes. This flow covers both network\ntraining and FPGA-based network deployment, which facilitates the design space\nexploration and simplifies the tradeoff between network accuracy and\ncomputation efficiency. Using this flow helps hardware designers to deliver a\nnetwork accelerator in edge devices under strict resource and power\nconstraints. We present the proposed flow by supporting hybrid ELB settings\nwithin a neural network. Results show that our design can deliver very high\nperformance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while\nrunning large-scale neural networks for less than 5W using embedded FPGA. To\nthe best of our knowledge, it is the most energy efficient solution in\ncomparison to GPU or other FPGA implementations reported so far in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:24:57 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 18:16:49 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Wang", "Junsong", ""], ["Lou", "Qiuwen", ""], ["Zhang", "Xiaofan", ""], ["Zhu", "Chao", ""], ["Lin", "Yonghua", ""], ["Chen", "Deming", ""]]}, {"id": "1808.04325", "submitter": "James Tompkin", "authors": "Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie, Kwang In Kim, James\n  Tompkin", "title": "Improving Shape Deformation in Unsupervised Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation techniques are able to map local\ntexture between two domains, but they are typically unsuccessful when the\ndomains require larger shape change. Inspired by semantic segmentation, we\nintroduce a discriminator with dilated convolutions that is able to use\ninformation from across the entire image to train a more context-aware\ngenerator. This is coupled with a multi-scale perceptual loss that is better\nable to represent error in the underlying shape of objects. We demonstrate that\nthis design is more capable of representing shape deformation in a challenging\ntoy dataset, plus in complex mappings with significant dataset variation\nbetween humans, dolls, and anime faces, and between cats and dogs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 16:33:46 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 21:24:25 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gokaslan", "Aaron", ""], ["Ramanujan", "Vivek", ""], ["Ritchie", "Daniel", ""], ["Kim", "Kwang In", ""], ["Tompkin", "James", ""]]}, {"id": "1808.04336", "submitter": "Pravakar Roy", "authors": "Pravakar Roy, Abhijeet Kislay, Patrick A. Plonski, James Luby and\n  Volkan Isler", "title": "Vision-Based Preharvest Yield Mapping for Apple Orchards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end computer vision system for mapping yield in an apple\norchard using images captured from a single camera. Our proposed system is\nplatform independent and does not require any specific lighting conditions. Our\nmain technical contributions are 1)~a semi-supervised clustering algorithm that\nutilizes colors to identify apples and 2)~an unsupervised clustering method\nthat utilizes spatial properties to estimate fruit counts from apple clusters\nhaving arbitrarily complex geometry. Additionally, we utilize camera motion to\nmerge the counts across multiple views. We verified the performance of our\nalgorithms by conducting multiple field trials on three tree rows consisting of\n$252$ trees at the University of Minnesota Horticultural Research Center.\nResults indicate that the detection method achieves $F_1$-measure $.95 -.97$\nfor multiple color varieties and lighting conditions. The counting method\nachieves an accuracy of $89\\%-98\\%$. Additionally, we report merged fruit\ncounts from both sides of the tree rows. Our yield estimation method achieves\nan overall accuracy of $91.98\\% - 94.81\\%$ across different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:23:16 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Roy", "Pravakar", ""], ["Kislay", "Abhijeet", ""], ["Plonski", "Patrick A.", ""], ["Luby", "James", ""], ["Isler", "Volkan", ""]]}, {"id": "1808.04355", "submitter": "Deepak Pathak", "authors": "Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor\n  Darrell, Alexei A. Efros", "title": "Large-Scale Study of Curiosity-Driven Learning", "comments": "First three authors contributed equally and ordered alphabetically.\n  Website at https://pathak22.github.io/large-scale-curiosity/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms rely on carefully engineering environment\nrewards that are extrinsic to the agent. However, annotating each environment\nwith hand-designed, dense rewards is not scalable, motivating the need for\ndeveloping reward functions that are intrinsic to the agent. Curiosity is a\ntype of intrinsic reward function which uses prediction error as reward signal.\nIn this paper: (a) We perform the first large-scale study of purely\ncuriosity-driven learning, i.e. without any extrinsic rewards, across 54\nstandard benchmark environments, including the Atari game suite. Our results\nshow surprisingly good performance, and a high degree of alignment between the\nintrinsic curiosity objective and the hand-designed extrinsic rewards of many\ngame environments. (b) We investigate the effect of using different feature\nspaces for computing prediction error and show that random features are\nsufficient for many popular RL game benchmarks, but learned features appear to\ngeneralize better (e.g. to novel game levels in Super Mario Bros.). (c) We\ndemonstrate limitations of the prediction-based rewards in stochastic setups.\nGame-play videos and code are at\nhttps://pathak22.github.io/large-scale-curiosity/\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:58:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Burda", "Yuri", ""], ["Edwards", "Harri", ""], ["Pathak", "Deepak", ""], ["Storkey", "Amos", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1808.04357", "submitter": "Jiarui Fang", "authors": "Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh", "title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "comments": "10 pages. Journal of Parallel and Distributed Computing, 2019", "journal-ref": null, "doi": "10.1016/j.jpdc.2019.05.016", "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism has become a dominant method to scale Deep Neural Network\n(DNN) training across multiple nodes. Since synchronizing a large number of\ngradients of the local model can be a bottleneck for large-scale distributed\ntraining, compressing communication data has gained widespread attention\nrecently. Among several recent proposed compression algorithms, Residual\nGradient Compression (RGC) is one of the most successful approaches---it can\nsignificantly compress the transmitting message size (0.1\\% of the gradient\nsize) of each node and still achieve correct accuracy and the same convergence\nspeed. However, the literature on compressing deep networks focuses almost\nexclusively on achieving good theoretical compression rate, while the\nefficiency of RGC in real distributed implementation has been less\ninvestigated. In this paper, we develop an RGC-based system that is able to\nreduce the end-to-end training time on real-world multi-GPU systems. Our\nproposed design called RedSync, which introduces a set of optimizations to\nreduce communication bandwidth requirement while introducing limited overhead.\nWe evaluate the performance of RedSync on two different multiple GPU platforms,\nincluding 128 GPUs of a supercomputer and an 8-GPU server. Our test cases\ninclude image classification tasks on Cifar10 and ImageNet, and language\nmodeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high\ncommunication to computation ratio, which have long been considered with poor\nscalability, RedSync brings significant performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:02:47 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:25:36 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 09:48:26 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fang", "Jiarui", ""], ["Fu", "Haohuan", ""], ["Yang", "Guangwen", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1808.04359", "submitter": "Akshat Agarwal", "authors": "Akshat Agarwal, Swaminathan Gurumurthy, Vasu Sharma, Mike Lewis, Katia\n  Sycara", "title": "Community Regularization of Visually-Grounded Dialog", "comments": "7 pages, ICML/AAMAS Adaptive Learning Agents Workshop 2018 and CVPR\n  Visual Dialog Workshop 2018. Code available at\n  https://github.com/agakshat/visualdialog-pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of conducting visually grounded dialog involves learning\ngoal-oriented cooperative dialog between autonomous agents who exchange\ninformation about a scene through several rounds of questions and answers in\nnatural language. We posit that requiring artificial agents to adhere to the\nrules of human language, while also requiring them to maximize information\nexchange through dialog is an ill-posed problem. We observe that humans do not\nstray from a common language because they are social creatures who live in\ncommunities, and have to communicate with many people everyday, so it is far\neasier to stick to a common language even at the cost of some efficiency loss.\nUsing this as inspiration, we propose and evaluate a multi-agent\ncommunity-based dialog framework where each agent interacts with, and learns\nfrom, multiple agents, and show that this community-enforced regularization\nresults in more relevant and coherent dialog (as judged by human evaluators)\nwithout sacrificing task performance (as judged by quantitative metrics).\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 22:09:43 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 20:01:29 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Agarwal", "Akshat", ""], ["Gurumurthy", "Swaminathan", ""], ["Sharma", "Vasu", ""], ["Lewis", "Mike", ""], ["Sycara", "Katia", ""]]}, {"id": "1808.04362", "submitter": "Pascal Sturmfels", "authors": "Pascal Sturmfels, Saige Rutherford, Mike Angstadt, Mark Peterson,\n  Chandra Sripada, Jenna Wiens", "title": "A Domain Guided CNN Architecture for Predicting Age from Structural\n  Brain Images", "comments": "Machine Learning for Healthcare 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the wide success of convolutional neural networks (CNNs) applied to\nnatural images, researchers have begun to apply them to neuroimaging data. To\ndate, however, exploration of novel CNN architectures tailored to neuroimaging\ndata has been limited. Several recent works fail to leverage the 3D structure\nof the brain, instead treating the brain as a set of independent 2D slices.\nApproaches that do utilize 3D convolutions rely on architectures developed for\nobject recognition tasks in natural 2D images. Such architectures make\nassumptions about the input that may not hold for neuroimaging. For example,\nexisting architectures assume that patterns in the brain exhibit translation\ninvariance. However, a pattern in the brain may have different meaning\ndepending on where in the brain it is located. There is a need to explore novel\narchitectures that are tailored to brain images. We present two simple\nmodifications to existing CNN architectures based on brain image structure.\nApplied to the task of brain age prediction, our network achieves a mean\nabsolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline\nthat achieves a MAE of 1.6 years. Our results suggest that lessons learned from\ndeveloping models on natural images may not directly transfer to neuroimaging\ntasks. Instead, there remains a large space of unexplored questions regarding\nmodel development in this area, whose answers may differ from conventional\nwisdom.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 19:43:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Sturmfels", "Pascal", ""], ["Rutherford", "Saige", ""], ["Angstadt", "Mike", ""], ["Peterson", "Mark", ""], ["Sripada", "Chandra", ""], ["Wiens", "Jenna", ""]]}, {"id": "1808.04399", "submitter": "Aleix Martinez", "authors": "Ramprakash Srinivasan, Aleix M. Martinez", "title": "Cross-Cultural and Cultural-Specific Production and Perception of Facial\n  Expressions of Emotion in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of emotion from facial expressions is an intense area\nof research, with a potentially long list of important application. Yet, the\nstudy of emotion requires knowing which facial expressions are used within and\nacross cultures in the wild, not in controlled lab conditions; but such studies\ndo not exist. Which and how many cross-cultural and cultural-specific facial\nexpressions do people commonly use? And, what affect variables does each\nexpression communicate to observers? If we are to design technology that\nunderstands the emotion of users, we need answers to these two fundamental\nquestions. In this paper, we present the first large-scale study of the\nproduction and visual perception of facial expressions of emotion in the wild.\nWe find that of the 16,384 possible facial configurations that people can\ntheoretically produce, only 35 are successfully used to transmit emotive\ninformation across cultures, and only 8 within a smaller number of cultures.\nCrucially, we find that visual analysis of cross-cultural expressions yields\nconsistent perception of emotion categories and valence, but not arousal. In\ncontrast, visual analysis of cultural-specific expressions yields consistent\nperception of valence and arousal, but not of emotion categories. Additionally,\nwe find that the number of expressions used to communicate each emotion is also\ndifferent, e.g., 17 expressions transmit happiness, but only 1 is used to\nconvey disgust.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 18:43:59 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Srinivasan", "Ramprakash", ""], ["Martinez", "Aleix M.", ""]]}, {"id": "1808.04432", "submitter": "Sheng Li", "authors": "Longfei Liu, Sheng Li, Yisong Chen and Guoping Wang", "title": "X-GANs: Image Reconstruction Made Easy for Extreme Cases", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction including image restoration and denoising is a\nchallenging problem in the field of image computing. We present a new method,\ncalled X-GANs, for reconstruction of arbitrary corrupted resource based on a\nvariant of conditional generative adversarial networks (conditional GANs). In\nour method, a novel generator and multi-scale discriminators are proposed, as\nwell as the combined adversarial losses, which integrate a VGG perceptual loss,\nan adversarial perceptual loss, and an elaborate corresponding point loss\ntogether based on the analysis of image feature. Our conditional GANs have\nenabled a variety of applications in image reconstruction, including image\ndenoising, image restoration from quite a sparse sampling, image inpainting,\nimage recovery from the severely polluted block or even color-noise dominated\nimages, which are extreme cases and haven't been addressed in the status quo.\nWe have significantly improved the accuracy and quality of image\nreconstruction. Extensive perceptual experiments on datasets ranging from human\nfaces to natural scenes demonstrate that images reconstructed by the presented\napproach are considerably more realistic than alternative work. Our method can\nalso be extended to handle high-ratio image compression.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 10:36:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Liu", "Longfei", ""], ["Li", "Sheng", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""]]}, {"id": "1808.04433", "submitter": "Nizar Ouarti Prof.", "authors": "Nizar Ouarti and David Carmona", "title": "Out of the Black Box: Properties of deep neural networks and their\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are powerful machine learning approaches that have\nexhibited excellent results on many classification tasks. However, they are\nconsidered as black boxes and some of their properties remain to be formalized.\nIn the context of image recognition, it is still an arduous task to understand\nwhy an image is recognized or not. In this study, we formalize some properties\nshared by eight state-of-the-art deep neural networks in order to grasp the\nprinciples allowing a given deep neural network to classify an image. Our\nresults, tested on these eight networks, show that an image can be sub-divided\ninto several regions (patches) responding at different degrees of probability\n(local property). With the same patch, some locations in the image can answer\ntwo (or three) orders of magnitude higher than other locations (spatial\nproperty). Some locations are activators and others inhibitors\n(activation-inhibition property). The repetition of the same patch can increase\n(or decrease) the probability of recognition of an object (cumulative\nproperty). Furthermore, we propose a new approach called Deepception that\nexploits these properties to deceive a deep neural network. We obtain for the\nVGG-VDD-19 neural network a fooling ratio of 88\\%. Thanks to our\n\"Psychophysics\" approach, no prior knowledge on the networks architectures is\nrequired.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 09:30:52 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Ouarti", "Nizar", ""], ["Carmona", "David", ""]]}, {"id": "1808.04436", "submitter": "Xiaojiang Li", "authors": "Xiaojiang Li, Bill Yang Cai, Waishan Qiu, Jinhua Zhao, Carlo Ratti", "title": "A novel method for predicting and mapping the presence of sun glare\n  using Google Street View", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sun glare is one of the major environmental hazards that cause traffic\naccidents. Every year, many people died and injured in traffic accidents\nrelated to sun glare. Providing accurate information about when and where sun\nglare happens would be helpful to prevent sun glare caused traffic accidents\nand save lives. In this study, we proposed to use publicly accessible Google\nStreet View (GSV) panorama images to estimate and predict the occurrence of sun\nglare. GSV images have view sight similar to drivers, which would make GSV\nimages suitable for estimating the visibility of sun glare to drivers. A\nrecently developed convolutional neural network algorithm was used to segment\nGSV images and predict obstructions on sun glare. Based on the predicted\nobstructions for given locations, we further estimated the time windows of sun\nglare by estimating the sun positions and the relative angles between drivers\nand the sun for those locations. We conducted a case study in Cambridge,\nMassachusetts, USA. Results show that the method can predict the presence of\nsun glare precisely. The proposed method would provide an important tool for\ndrivers and traffic planners to mitigate the sun glare and decrease the\npotential traffic accidents caused by the sun glare.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 11:44:36 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Xiaojiang", ""], ["Cai", "Bill Yang", ""], ["Qiu", "Waishan", ""], ["Zhao", "Jinhua", ""], ["Ratti", "Carlo", ""]]}, {"id": "1808.04437", "submitter": "Jun Li", "authors": "Jun Li, Chang Xu, Wankou Yang, Changyin Sun, Dacheng Tao, Hong Zhang", "title": "Discriminative multi-view Privileged Information learning for image\n  re-ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multi-view re-ranking methods usually perform asymmetrical\nmatching between the region of interest (ROI) in the query image and the whole\ntarget image for similarity computation. Due to the inconsistency in the visual\nappearance, this practice tends to degrade the retrieval accuracy particularly\nwhen the image ROI, which is usually interpreted as the image objectness,\naccounts for a smaller region in the image. Since Privileged Information (PI),\nwhich can be viewed as the image prior, enables well characterizing the image\nobjectness, we are aiming at leveraging PI for further improving the\nperformance of the multi-view re-ranking accuracy in this paper. Towards this\nend, we propose a discriminative multi-view re-ranking approach in which both\nthe original global image visual contents and the local auxiliary PI features\nare simultaneously integrated into a unified training framework for generating\nthe latent subspaces with sufficient discriminating power. For the on-the-fly\nre-ranking, since the multi-view PI features are unavailable, we only project\nthe original multi-view image representations onto the latent subspace, and\nthus the re-ranking can be achieved by computing and sorting the distances from\nthe multi-view embeddings to the separating hyperplane. Extensive experimental\nevaluations on the two public benchmarks Oxford5k and Paris6k reveal our\napproach provides further performance boost for accurate image re-ranking,\nwhilst the comparative study demonstrates the advantage of our method against\nother multi-view re-ranking methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 18:57:14 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Jun", ""], ["Xu", "Chang", ""], ["Yang", "Wankou", ""], ["Sun", "Changyin", ""], ["Tao", "Dacheng", ""], ["Zhang", "Hong", ""]]}, {"id": "1808.04439", "submitter": "Ayagoz Mussabayeva", "authors": "Ayagoz Mussabayeva, Alexey Kroshnin, Anvar Kurmukov, Yulia Dodonova,\n  Li Shen, Shan Cong, Lei Wang and Boris A. Gutman", "title": "Image Registration and Predictive Modeling: Learning the Metric on the\n  Space of Diffeomorphisms", "comments": "Accepted to ShapeMI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for metric optimization in the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework, by treating the induced\nRiemannian metric on the space of diffeomorphisms as a kernel in a machine\nlearning context. For simplicity, we choose the kernel Fischer Linear\nDiscriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters\nin an Expectation-Maximization framework, we define model fidelity via the\nhinge loss of the decision function. The resulting algorithm optimizes the\nparameters of the LDDMM norm-inducing differential operator as a solution to a\ngroup-wise registration and classification problem. In practice, this may lead\nto a biology-aware registration, focusing its attention on the predictive task\nat hand such as identifying the effects of disease. We first tested our\nalgorithm on a synthetic dataset, showing that our parameter selection improves\nregistration quality and classification accuracy. We then tested the algorithm\non 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our\nSchizpohrenia-Control predictive model showed significant improvement in ROC\nAUC compared to baseline parameters.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 15:25:10 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Mussabayeva", "Ayagoz", ""], ["Kroshnin", "Alexey", ""], ["Kurmukov", "Anvar", ""], ["Dodonova", "Yulia", ""], ["Shen", "Li", ""], ["Cong", "Shan", ""], ["Wang", "Lei", ""], ["Gutman", "Boris A.", ""]]}, {"id": "1808.04440", "submitter": "Evangello Flouty", "authors": "Evangello Flouty, Odysseas Zisimopoulos, and Danail Stoyanov", "title": "FaceOff: Anonymizing Videos in the Operating Rooms", "comments": "MICCAI 2018: OR 2.0 Context-Aware Operating Theaters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video capture in the surgical operating room (OR) is increasingly possible\nand has potential for use with computer assisted interventions (CAI), surgical\ndata science and within smart OR integration. Captured video innately carries\nsensitive information that should not be completely visible in order to\npreserve the patient's and the clinical teams' identities. When surgical video\nstreams are stored on a server, the videos must be anonymized prior to storage\nif taken outside of the hospital. In this article, we describe how a deep\nlearning model, Faster R-CNN, can be used for this purpose and help to\nanonymize video data captured in the OR. The model detects and blurs faces in\nan effort to preserve anonymity. After testing an existing face detection\ntrained model, a new dataset tailored to the surgical environment, with faces\nobstructed by surgical masks and caps, was collected for fine-tuning to achieve\nhigher face-detection rates in the OR. We also propose a temporal\nregularisation kernel to improve recall rates. The fine-tuned model achieves a\nface detection recall of 88.05 % and 93.45 % before and after applying\ntemporal-smoothing respectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 09:36:08 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Flouty", "Evangello", ""], ["Zisimopoulos", "Odysseas", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1808.04441", "submitter": "Aaron Pries", "authors": "Aaron Pries, Peter J. Schreier, Artur Lamm, Stefan Pede, J\\\"urgen\n  Schmidt", "title": "Deep Morphing: Detecting bone structures in fluoroscopic X-ray images\n  with prior knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose approaches based on deep learning to localize objects in images\nwhen only a small training dataset is available and the images have low\nquality. That applies to many problems in medical image processing, and in\nparticular to the analysis of fluoroscopic (low-dose) X-ray images, where the\nimages have low contrast. We solve the problem by incorporating high-level\ninformation about the objects, which could be a simple geometrical model, like\na circular outline, or a more complex statistical model. A simple geometrical\nrepresentation can sufficiently describe some objects and only requires minimal\nlabeling. Statistical shape models can be used to represent more complex\nobjects. We propose computationally efficient two-stage approaches, which we\ncall deep morphing, for both representations by fitting the representation to\nthe output of a deep segmentation network.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 17:55:31 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 17:02:27 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pries", "Aaron", ""], ["Schreier", "Peter J.", ""], ["Lamm", "Artur", ""], ["Pede", "Stefan", ""], ["Schmidt", "J\u00fcrgen", ""]]}, {"id": "1808.04446", "submitter": "Mathieu Seurin", "authors": "Florian Strub and Mathieu Seurin and Ethan Perez and Harm de Vries and\n  J\\'er\\'emie Mary and Philippe Preux and Aaron Courville and Olivier Pietquin", "title": "Visual Reasoning with Multi-hop Feature Modulation", "comments": "In Proc of ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in computer vision and natural language processing have\nspurred interest in challenging multi-modal tasks such as visual\nquestion-answering and visual dialogue. For such tasks, one successful approach\nis to condition image-based convolutional network computation on language via\nFeature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and\nshifting. We propose to generate the parameters of FiLM layers going up the\nhierarchy of a convolutional network in a multi-hop fashion rather than all at\nonce, as in prior work. By alternating between attending to the language input\nand generating FiLM layer parameters, this approach is better able to scale to\nsettings with longer input sequences such as dialogue. We demonstrate that\nmulti-hop FiLM generation achieves state-of-the-art for the short input\nsequence task ReferIt --- on-par with single-hop FiLM generation --- while also\nsignificantly outperforming prior state-of-the-art and single-hop FiLM\ngeneration on the GuessWhat?! visual dialogue task.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 14:32:02 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 11:36:42 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Strub", "Florian", ""], ["Seurin", "Mathieu", ""], ["Perez", "Ethan", ""], ["de Vries", "Harm", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Preux", "Philippe", ""], ["Courville", "Aaron", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1808.04447", "submitter": "Akshay Chaudhari", "authors": "Akshay Chaudhari, Zhongnan Fang, Jin Hyung Lee, Garry Gold, Brian\n  Hargreaves", "title": "Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological\n  and Quantitative Magnetic Resonance Imaging", "comments": "Accepted for the Machine Learning for Medical Image Reconstruction\n  Workshop at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining magnetic resonance images (MRI) with high resolution and generating\nquantitative image-based biomarkers for assessing tissue biochemistry is\ncrucial in clinical and research applications. How- ever, acquiring\nquantitative biomarkers requires high signal-to-noise ratio (SNR), which is at\nodds with high-resolution in MRI, especially in a single rapid sequence. In\nthis paper, we demonstrate how super-resolution can be utilized to maintain\nadequate SNR for accurate quantification of the T2 relaxation time biomarker,\nwhile simultaneously generating high- resolution images. We compare the\nefficacy of resolution enhancement using metrics such as peak SNR and\nstructural similarity. We assess accuracy of cartilage T2 relaxation times by\ncomparing against a standard reference method. Our evaluation suggests that SR\ncan successfully maintain high-resolution and generate accurate biomarkers for\naccelerating MRI scans and enhancing the value of clinical and research MRI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:09:11 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Chaudhari", "Akshay", ""], ["Fang", "Zhongnan", ""], ["Lee", "Jin Hyung", ""], ["Gold", "Garry", ""], ["Hargreaves", "Brian", ""]]}, {"id": "1808.04449", "submitter": "Maarten Bieshaar", "authors": "Maarten Bieshaar, Malte Depping, Jan Schneegans, Bernhard Sick", "title": "Starting Movement Detection of Cyclists Using Smart Devices", "comments": "10 pages, accepted for publication at DSAA 2018, Turin, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In near future, vulnerable road users (VRUs) such as cyclists and pedestrians\nwill be equipped with smart devices and wearables which are capable to\ncommunicate with intelligent vehicles and other traffic participants. Road\nusers are then able to cooperate on different levels, such as in cooperative\nintention detection for advanced VRU protection. Smart devices can be used to\ndetect intentions, e.g., an occluded cyclist intending to cross the road, to\nwarn vehicles of VRUs, and prevent potential collisions. This article presents\na human activity recognition approach to detect the starting movement of\ncyclists wearing smart devices. We propose a novel two-stage feature selection\nprocedure using a score specialized for robust starting detection reducing the\nfalse positive detections and leading to understandable and interpretable\nfeatures. The detection is modelled as a classification problem and realized by\nmeans of a machine learning classifier. We introduce an auxiliary class, that\nmodels starting movements and allows to integrate early movement indicators,\ni.e., body part movements indicating future behaviour. In this way we improve\nthe robustness and reduce the detection time of the classifier. Our empirical\nstudies with real-world data originating from experiments which involve 49 test\nsubjects and consists of 84 starting motions show that we are able to detect\nthe starting movements early. Our approach reaches an F1-score of 67 % within\n0.33 s after the first movement of the bicycle wheel. Investigations concerning\nthe device wearing location show that for devices worn in the trouser pocket\nthe detector has less false detections and detects starting movements faster on\naverage. We found that we can further improve the results when we train\ndistinct classifiers for different wearing locations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 09:20:13 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Bieshaar", "Maarten", ""], ["Depping", "Malte", ""], ["Schneegans", "Jan", ""], ["Sick", "Bernhard", ""]]}, {"id": "1808.04450", "submitter": "Yecheng Lyu", "authors": "Yecheng Lyu, Lin Bai and Xinming Huang", "title": "Road Segmentation Using CNN and Distributed LSTM", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1804.05164", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automated driving systems (ADS) and advanced driver-assistance systems\n(ADAS), an efficient road segmentation is necessary to perceive the drivable\nregion and build an occupancy map for path planning. The existing algorithms\nimplement gigantic convolutional neural networks (CNNs) that are\ncomputationally expensive and time consuming. In this paper, we introduced\ndistributed LSTM, a neural network widely used in audio and video processing,\nto process rows and columns in images and feature maps. We then propose a new\nnetwork combining the convolutional and distributed LSTM layers to solve the\nroad segmentation problem. In the end, the network is trained and tested in\nKITTI road benchmark. The result shows that the combined structure enhances the\nfeature extraction and processing but takes less processing time than pure CNN\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 12:35:35 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 23:38:10 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Lyu", "Yecheng", ""], ["Bai", "Lin", ""], ["Huang", "Xinming", ""]]}, {"id": "1808.04451", "submitter": "Maarten Bieshaar", "authors": "Jan Schneegans, Maarten Bieshaar", "title": "Smart Device based Initial Movement Detection of Cyclists using\n  Convolutional Neuronal Networks", "comments": "12 pages, accepted for publication at OC-DDC 2018, W\\\"urzburg,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For future traffic scenarios, we envision interconnected traffic\nparticipants, who exchange information about their current state, e.g.,\nposition, their predicted intentions, allowing to act in a cooperative manner.\nVulnerable road users (VRUs), e.g., pedestrians and cyclists, will be equipped\nwith smart device that can be used to detect their intentions and transmit\nthese detected intention to approaching cars such that their drivers can be\nwarned. In this article, we focus on detecting the initial movement of cyclist\nusing smart devices. Smart devices provide the necessary sensors, namely\naccelerometer and gyroscope, and therefore pose an excellent instrument to\ndetect movement transitions (e.g., waiting to moving) fast. Convolutional\nNeural Networks prove to be the state-of-the-art solution for many problems\nwith an ever increasing range of applications. Therefore, we model the initial\nmovement detection as a classification problem. In terms of Organic Computing\n(OC) it be seen as a step towards self-awareness and self-adaptation. We apply\nresidual network architectures to the task of detecting the initial starting\nmovement of cyclists.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 09:35:33 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Schneegans", "Jan", ""], ["Bieshaar", "Maarten", ""]]}, {"id": "1808.04456", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Khushmeen Sakloth, Charles Siegel, Abhinav Vishnu, Jim\n  Pfaendtner", "title": "Multimodal Deep Neural Networks using Both Engineered and Learned\n  Representations for Biodegradability Prediction", "comments": "Submitted to a peer-reviewed ML conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms excel at extracting patterns from raw data, and with\nlarge datasets, they have been very successful in computer vision and natural\nlanguage applications. However, in other domains, large datasets on which to\nlearn representations from may not exist. In this work, we develop a novel\nmultimodal CNN-MLP neural network architecture that utilizes both\ndomain-specific feature engineering as well as learned representations from raw\ndata. We illustrate the effectiveness of such network designs in the chemical\nsciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP\nnetwork is more accurate than either standalone network designs, and achieves\nan error classification rate of 0.125 that is 27% lower than the current\nstate-of-the-art. Thus, our work indicates that combining traditional feature\nengineering with representation learning can be effective, particularly in\nsituations where labeled data is limited.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 20:36:08 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 18:27:08 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Goh", "Garrett B.", ""], ["Sakloth", "Khushmeen", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Pfaendtner", "Jim", ""]]}, {"id": "1808.04469", "submitter": "Hong Xuan", "authors": "Hong Xuan, Richard Souvenir, Robert Pless", "title": "Deep Randomized Ensembles for Metric Learning", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning embedding functions, which map semantically related inputs to nearby\nlocations in a feature space supports a variety of classification and\ninformation retrieval tasks. In this work, we propose a novel, generalizable\nand fast method to define a family of embedding functions that can be used as\nan ensemble to give improved results. Each embedding function is learned by\nrandomly bagging the training labels into small subsets. We show experimentally\nthat these embedding ensembles create effective embedding functions. The\nensemble output defines a metric space that improves state of the art\nperformance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes\nRetrieval and VehicleID.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 21:11:56 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 16:58:59 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Xuan", "Hong", ""], ["Souvenir", "Richard", ""], ["Pless", "Robert", ""]]}, {"id": "1808.04480", "submitter": "Benjamin Schnieders", "authors": "Benjamin Schnieders and Karl Tuyls", "title": "Fast Convergence for Object Detection by Learning how to Combine Error\n  Functions", "comments": "Accepted for publication at IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an innovative method to improve the convergence\nspeed and accuracy of object detection neural networks. Our approach,\nCONVERGE-FAST-AUXNET, is based on employing multiple, dependent loss metrics\nand weighting them optimally using an on-line trained auxiliary network.\nExperiments are performed in the well-known RoboCup@Work challenge environment.\nA fully convolutional segmentation network is trained on detecting objects'\npickup points. We empirically obtain an approximate measure for the rate of\nsuccess of a robotic pickup operation based on the accuracy of the object\ndetection network. Our experiments show that adding an optimally weighted\nEuclidean distance loss to a network trained on the commonly used Intersection\nover Union (IoU) metric reduces the convergence time by 42.48%. The estimated\npickup rate is improved by 39.90%. Compared to state-of-the-art task weighting\nmethods, the improvement is 24.5% in convergence, and 15.8% on the estimated\npickup rate.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 22:20:34 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Schnieders", "Benjamin", ""], ["Tuyls", "Karl", ""]]}, {"id": "1808.04487", "submitter": "Andreas Mang", "authors": "Andreas Mang and Amir Gholami and Christos Davatzikos and George Biros", "title": "CLAIRE: A distributed-memory solver for constrained large deformation\n  diffeomorphic image registration", "comments": "37 pages;", "journal-ref": "SIAM Journal on Scientific Computing, 41(5):C548-C584, 2019", "doi": "10.1137/18M1207818", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With this work, we release CLAIRE, a distributed-memory implementation of an\neffective solver for constrained large deformation diffeomorphic image\nregistration problems in three dimensions. We consider an optimal control\nformulation. We invert for a stationary velocity field that parameterizes the\ndeformation map. Our solver is based on a globalized, preconditioned, inexact\nreduced space Gauss--Newton--Krylov scheme. We exploit state-of-the-art\ntechniques in scientific computing to develop an effective solver that scales\nto thousands of distributed memory nodes on high-end clusters. We present the\nformulation, discuss algorithmic features, describe the software package, and\nintroduce an improved preconditioner for the reduced space Hessian to speed up\nthe convergence of our solver. We test registration performance on synthetic\nand real data. We demonstrate registration accuracy on several neuroimaging\ndatasets. We compare the performance of our scheme against different flavors of\nthe Demons algorithm for diffeomorphic image registration. We study convergence\nof our preconditioner and our overall algorithm. We report scalability results\non state-of-the-art supercomputing platforms. We demonstrate that we can solve\nregistration problems for clinically relevant data sizes in two to four minutes\non a standard compute node with 20 cores, attaining excellent data fidelity.\nWith the present work we achieve a speedup of (on average) 5$\\times$ with a\npeak performance of up to 17$\\times$ compared to our former work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 22:59:25 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 21:50:57 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mang", "Andreas", ""], ["Gholami", "Amir", ""], ["Davatzikos", "Christos", ""], ["Biros", "George", ""]]}, {"id": "1808.04495", "submitter": "Jialei Chen", "authors": "Jialei Chen, Yujia Xie, Kan Wang, Zih Huei Wang, Geet Lahoti, Chuck\n  Zhang, Mani A Vannan, Ben Wang, Zhen Qian", "title": "Generative Invertible Networks (GIN): Pathophysiology-Interpretable\n  Feature Mapping and Virtual Patient Generation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-00928-1_61", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods play increasingly important roles in pre-procedural\nplanning for complex surgeries and interventions. Very often, however,\nresearchers find the historical records of emerging surgical techniques, such\nas the transcatheter aortic valve replacement (TAVR), are highly scarce in\nquantity. In this paper, we address this challenge by proposing novel\ngenerative invertible networks (GIN) to select features and generate\nhigh-quality virtual patients that may potentially serve as an additional data\nsource for machine learning. Combining a convolutional neural network (CNN) and\ngenerative adversarial networks (GAN), GIN discovers the pathophysiologic\nmeaning of the feature space. Moreover, a test of predicting the surgical\noutcome directly using the selected features results in a high accuracy of\n81.55%, which suggests little pathophysiologic information has been lost while\nconducting the feature selection. This demonstrates GIN can generate virtual\npatients not only visually authentic but also pathophysiologically\ninterpretable.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 00:18:33 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Chen", "Jialei", ""], ["Xie", "Yujia", ""], ["Wang", "Kan", ""], ["Wang", "Zih Huei", ""], ["Lahoti", "Geet", ""], ["Zhang", "Chuck", ""], ["Vannan", "Mani A", ""], ["Wang", "Ben", ""], ["Qian", "Zhen", ""]]}, {"id": "1808.04500", "submitter": "Felix Lau", "authors": "Felix Lau, Tom Hendriks, Jesse Lieman-Sifry, Berk Norman, Sean Sall,\n  Daniel Golden", "title": "ScarGAN: Chained Generative Adversarial Networks to Simulate\n  Pathological Tissue on Cardiovascular MR Scans", "comments": "12 pages, 5 figures. To appear in MICCAI DLMIA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images with specific pathologies are scarce, but a large amount of\ndata is usually required for a deep convolutional neural network (DCNN) to\nachieve good accuracy. We consider the problem of segmenting the left\nventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular\nmagnetic resonance (CMR) scans of which only some of the scans have scar\ntissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using\nchained generative adversarial networks (GAN). Our novel approach factorizes\nthe simulation process into 3 steps: 1) a mask generator to simulate the shape\nof the scar tissue; 2) a domain-specific heuristic to produce the initial\nsimulated scar tissue from the simulated shape; 3) a refining generator to add\ndetails to the simulated scar tissue. Unlike other approaches that generate\nsamples from scratch, we simulate scar tissue on normal scans resulting in\nhighly realistic samples. We show that experienced radiologists are unable to\ndistinguish between real and simulated scar tissue. Training a U-Net with\nadditional scans with scar tissue simulated by ScarGAN increases the percentage\nof scar pixels correctly included in LV myocardium prediction from 75.9% to\n80.5%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 01:10:00 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Lau", "Felix", ""], ["Hendriks", "Tom", ""], ["Lieman-Sifry", "Jesse", ""], ["Norman", "Berk", ""], ["Sall", "Sean", ""], ["Golden", "Daniel", ""]]}, {"id": "1808.04503", "submitter": "Junhong Xu", "authors": "Junhong Xu, Qiwei Liu, Hanqing Guo, Aaron Kageza, Saeed AlQarni,\n  Shaoen Wu", "title": "Shared Multi-Task Imitation Learning for Indoor Self-Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep imitation learning enables robots to learn from expert demonstrations to\nperform tasks such as lane following or obstacle avoidance. However, in the\ntraditional imitation learning framework, one model only learns one task, and\nthus it lacks of the capability to support a robot to perform various different\nnavigation tasks with one model in indoor environments. This paper proposes a\nnew framework, Shared Multi-headed Imitation Learning(SMIL), that allows a\nrobot to perform multiple tasks with one model without switching among\ndifferent models. We model each task as a sub-policy and design a multi-headed\npolicy to learn the shared information among related tasks by summing up\nactivations from all sub-policies. Compared to single or non-shared\nmulti-headed policies, this framework is able to leverage correlated\ninformation among tasks to increase performance.We have implemented this\nframework using a robot based on NVIDIA TX2 and performed extensive experiments\nin indoor environments with different baseline solutions. The results\ndemonstrate that SMIL has doubled the performance over nonshared multi-headed\npolicy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 01:38:47 GMT"}], "update_date": "2018-08-19", "authors_parsed": [["Xu", "Junhong", ""], ["Liu", "Qiwei", ""], ["Guo", "Hanqing", ""], ["Kageza", "Aaron", ""], ["AlQarni", "Saeed", ""], ["Wu", "Shaoen", ""]]}, {"id": "1808.04505", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Wenxi Wu, Yuefang Gao, Le Dong, Xiaonan Luo, Liang Lin", "title": "Fine-Grained Representation Learning and Recognition by Exploiting\n  Hierarchical Semantic Embedding", "comments": "Accepted at ACM MM 2018 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object categories inherently form a hierarchy with different levels of\nconcept abstraction, especially for fine-grained categories. For example, birds\n(Aves) can be categorized according to a four-level hierarchy of order, family,\ngenus, and species. This hierarchy encodes rich correlations among various\ncategories across different levels, which can effectively regularize the\nsemantic space and thus make prediction less ambiguous. However, previous\nstudies of fine-grained image recognition primarily focus on categories of one\ncertain level and usually overlook this correlation information. In this work,\nwe investigate simultaneously predicting categories of different levels in the\nhierarchy and integrating this structured correlation information into the deep\nneural network by developing a novel Hierarchical Semantic Embedding (HSE)\nframework. Specifically, the HSE framework sequentially predicts the category\nscore vector of each level in the hierarchy, from highest to lowest. At each\nlevel, it incorporates the predicted score vector of the higher level as prior\nknowledge to learn finer-grained feature representation. During training, the\npredicted score vector of the higher level is also employed to regularize label\nprediction by using it as soft targets of corresponding sub-categories. To\nevaluate the proposed framework, we organize the 200 bird species of the\nCaltech-UCSD birds dataset with the four-level category hierarchy and construct\na large-scale butterfly dataset that also covers four level categories.\nExtensive experiments on these two and the newly-released VegFru datasets\ndemonstrate the superiority of our HSE framework over the baseline methods and\nexisting competitors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 02:09:34 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Chen", "Tianshui", ""], ["Wu", "Wenxi", ""], ["Gao", "Yuefang", ""], ["Dong", "Le", ""], ["Luo", "Xiaonan", ""], ["Lin", "Liang", ""]]}, {"id": "1808.04521", "submitter": "Zhanxuan Hu", "authors": "Zhanxuan Hu, Feiping Nie, Rong Wang, Xuelong Li", "title": "Low Rank Regularization: A Review", "comments": "16 pages,4 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank regularization, in essence, involves introducing a low rank or\napproximately low rank assumption for matrix we aim to learn, which has\nachieved great success in many fields including machine learning, data mining\nand computer version. Over the last decade, much progress has been made in\ntheories and practical applications. Nevertheless, the intersection between\nthem is very slight. In order to construct a bridge between practical\napplications and theoretical research, in this paper we provide a comprehensive\nsurvey for low rank regularization. We first review several traditional machine\nlearning models using low rank regularization, and then show their (or their\nvariants) applications in solving practical issues, such as non-rigid structure\nfrom motion and image denoising. Subsequently, we summarize the regularizers\nand optimization methods that achieve great success in traditional machine\nlearning tasks but are rarely seen in solving practical issues. Finally, we\nprovide a discussion and comparison for some representative regularizers\nincluding convex and non-convex relaxations. Extensive experimental results\ndemonstrate that non-convex regularizers can provide a large advantage over the\nnuclear norm, the regularizer widely used in solving practical issues.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 04:38:58 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 14:45:07 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 03:05:30 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hu", "Zhanxuan", ""], ["Nie", "Feiping", ""], ["Wang", "Rong", ""], ["Li", "Xuelong", ""]]}, {"id": "1808.04537", "submitter": "Sifei Liu", "authors": "Xueting Li, Sifei Liu, Jan Kautz, Ming-Hsuan Yang", "title": "Learning Linear Transformations for Fast Arbitrary Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random pair of images, an arbitrary style transfer method extracts\nthe feel from the reference image to synthesize an output based on the look of\nthe other content image. Recent arbitrary style transfer methods transfer\nsecond order statistics from reference image onto content image via a\nmultiplication between content image features and a transformation matrix,\nwhich is computed from features with a pre-determined algorithm. These\nalgorithms either require computationally expensive operations, or fail to\nmodel the feature covariance and produce artifacts in synthesized images.\nGeneralized from these methods, in this work, we derive the form of\ntransformation matrix theoretically and present an arbitrary style transfer\napproach that learns the transformation matrix with a feed-forward network. Our\nalgorithm is highly efficient yet allows a flexible combination of multi-level\nstyles while preserving content affinity during style transfer process. We\ndemonstrate the effectiveness of our approach on four tasks: artistic style\ntransfer, video and photo-realistic style transfer as well as domain\nadaptation, including comparisons with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 05:45:20 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Xueting", ""], ["Liu", "Sifei", ""], ["Kautz", "Jan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1808.04538", "submitter": "Satya Krishna Gorti", "authors": "Satya Krishna Gorti and Jeremy Ma", "title": "Text-to-Image-to-Text Translation using Cycle Consistent Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-Image translation has been an active area of research in the recent\npast. The ability for a network to learn the meaning of a sentence and generate\nan accurate image that depicts the sentence shows ability of the model to think\nmore like humans. Popular methods on text to image translation make use of\nGenerative Adversarial Networks (GANs) to generate high quality images based on\ntext input, but the generated images don't always reflect the meaning of the\nsentence given to the model as input. We address this issue by using a\ncaptioning network to caption on generated images and exploit the distance\nbetween ground truth captions and generated captions to improve the network\nfurther. We show extensive comparisons between our method and existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 05:45:25 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gorti", "Satya Krishna", ""], ["Ma", "Jeremy", ""]]}, {"id": "1808.04545", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, Eli\n  Shechtman, Sunil Hadap, Ersin Yumer, Honglak Lee", "title": "MT-VAE: Learning Motion Transformations to Generate Multimodal Human\n  Dynamics", "comments": "Published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term human motion can be represented as a series of motion\nmodes---motion sequences that capture short-term temporal dynamics---with\ntransitions between them. We leverage this structure and present a novel Motion\nTransformation Variational Auto-Encoders (MT-VAE) for learning motion sequence\ngeneration. Our model jointly learns a feature embedding for motion modes (that\nthe motion sequence can be reconstructed from) and a feature transformation\nthat represents the transition of one motion mode to the next motion mode. Our\nmodel is able to generate multiple diverse and plausible motion sequences in\nthe future from the same input. We apply our approach to both facial and full\nbody motion, and demonstrate applications like analogy-based motion transfer\nand video synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 06:21:03 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Yan", "Xinchen", ""], ["Rastogi", "Akash", ""], ["Villegas", "Ruben", ""], ["Sunkavalli", "Kalyan", ""], ["Shechtman", "Eli", ""], ["Hadap", "Sunil", ""], ["Yumer", "Ersin", ""], ["Lee", "Honglak", ""]]}, {"id": "1808.04551", "submitter": "Geethu Miriam Jacob", "authors": "Geethu Miriam Jacob and Sukhendu Das", "title": "Moving Object Segmentation in Jittery Videos by Stabilizing Trajectories\n  Modeled in Kendall's Shape Space", "comments": "13 pages, 3 figures, Published in British Machine Vision Conference\n  2017 (BMVC-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving Object Segmentation is a challenging task for jittery/wobbly videos.\nFor jittery videos, the non-smooth camera motion makes discrimination between\nforeground objects and background layers hard to solve. While most recent works\nfor moving video object segmentation fail in this scenario, our method\ngenerates an accurate segmentation of a single moving object. The proposed\nmethod performs a sparse segmentation, where frame-wise labels are assigned\nonly to trajectory coordinates, followed by the pixel-wise labeling of frames.\nThe sparse segmentation involving stabilization and clustering of trajectories\nin a 3-stage iterative process. At the 1st stage, the trajectories are\nclustered using pairwise Procrustes distance as a cue for creating an affinity\nmatrix. The 2nd stage performs a block-wise Procrustes analysis of the\ntrajectories and estimates Frechet means (in Kendall's shape space) of the\nclusters. The Frechet means represent the average trajectories of the motion\nclusters. An optimization function has been formulated to stabilize the Frechet\nmeans, yielding stabilized trajectories at the 3rd stage. The accuracy of the\nmotion clusters are iteratively refined, producing distinct groups of\nstabilized trajectories. Next, the labels obtained from the sparse segmentation\nare propagated for pixel-wise labeling of the frames, using a GraphCut based\nenergy formulation. Use of Procrustes analysis and energy minimization in\nKendall's shape space for moving object segmentation in jittery videos, is the\nnovelty of this work. Second contribution comes from experiments performed on a\ndataset formed of 20 real-world natural jittery videos, with manually annotated\nground truth. Experiments are done with controlled levels of artificial jitter\non videos of SegTrack2 dataset. Qualitative and quantitative results indicate\nthe superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 06:41:21 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Jacob", "Geethu Miriam", ""], ["Das", "Sukhendu", ""]]}, {"id": "1808.04560", "submitter": "Wenjing Wang", "authors": "Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu", "title": "Deep Retinex Decomposition for Low-Light Enhancement", "comments": "BMVC 2018(Oral). Dataset and Project page:\n  https://daooshee.github.io/BMVC2018website/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinex model is an effective tool for low-light image enhancement. It\nassumes that observed images can be decomposed into the reflectance and\nillumination. Most existing Retinex-based methods have carefully designed\nhand-crafted constraints and parameters for this highly ill-posed\ndecomposition, which may be limited by model capacity when applied in various\nscenes. In this paper, we collect a LOw-Light dataset (LOL) containing\nlow/normal-light image pairs and propose a deep Retinex-Net learned on this\ndataset, including a Decom-Net for decomposition and an Enhance-Net for\nillumination adjustment. In the training process for Decom-Net, there is no\nground truth of decomposed reflectance and illumination. The network is learned\nwith only key constraints including the consistent reflectance shared by paired\nlow/normal-light images, and the smoothness of illumination. Based on the\ndecomposition, subsequent lightness enhancement is conducted on illumination by\nan enhancement network called Enhance-Net, and for joint denoising there is a\ndenoising operation on reflectance. The Retinex-Net is end-to-end trainable, so\nthat the learned decomposition is by nature good for lightness adjustment.\nExtensive experiments demonstrate that our method not only achieves visually\npleasing quality for low-light enhancement but also provides a good\nrepresentation of image decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 07:20:55 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Wei", "Chen", ""], ["Wang", "Wenjing", ""], ["Yang", "Wenhan", ""], ["Liu", "Jiaying", ""]]}, {"id": "1808.04571", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa, and Afzel\n  Noore", "title": "Learning A Shared Transform Model for Skull to Digital Face Image\n  Matching", "comments": "Accepted in IEEE International Conference on Biometrics: Theory,\n  Applications and Systems (BTAS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human skull identification is an arduous task, traditionally requiring the\nexpertise of forensic artists and anthropologists. This paper is an effort to\nautomate the process of matching skull images to digital face images, thereby\nestablishing an identity of the skeletal remains. In order to achieve this, a\nnovel Shared Transform Model is proposed for learning discriminative\nrepresentations. The model learns robust features while reducing the\nintra-class variations between skulls and digital face images. Such a model can\nassist law enforcement agencies by speeding up the process of skull\nidentification, and reducing the manual load. Experimental evaluation performed\non two pre-defined protocols of the publicly available IdentifyMe dataset\ndemonstrates the efficacy of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 07:58:42 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Noore", "Afzel", ""]]}, {"id": "1808.04576", "submitter": "Antonio Garcia-Uceda Juarez", "authors": "A. Garcia-Uceda Juarez, H.A.W.M. Tiddens, M. de Bruijne", "title": "Automatic Airway Segmentation in chest CT using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the airway tree from chest computed tomography (CT) images is\ncritical for quantitative assessment of airway diseases including\nbronchiectasis and chronic obstructive pulmonary disease (COPD). However,\nobtaining an accurate segmentation of airways from CT scans is difficult due to\nthe high complexity of airway structures. Recently, deep convolutional neural\nnetworks (CNNs) have become the state-of-the-art for many segmentation tasks,\nand in particular the so-called Unet architecture for biomedical images.\nHowever, its application to the segmentation of airways still remains a\nchallenging task. This work presents a simple but robust approach based on a 3D\nUnet to perform segmentation of airways from chest CTs. The method is trained\non a dataset composed of 12 CTs, and tested on another 6 CTs. We evaluate the\ninfluence of different loss functions and data augmentation techniques, and\nreach an average dice coefficient of 0.8 between the ground-truth and our\nautomated segmentations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 08:13:03 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Juarez", "A. Garcia-Uceda", ""], ["Tiddens", "H. A. W. M.", ""], ["de Bruijne", "M.", ""]]}, {"id": "1808.04589", "submitter": "Andrew Beers", "authors": "Andrew Beers, James Brown, Ken Chang, Katharina Hoebel, Elizabeth\n  Gerstner, Bruce Rosen, Jayashree Kalpathy-Cramer", "title": "DeepNeuro: an open-source deep learning toolbox for neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating neural networks from theory to clinical practice has unique\nchallenges, specifically in the field of neuroimaging. In this paper, we\npresent DeepNeuro, a deep learning framework that is best-suited to putting\ndeep learning algorithms for neuroimaging in practical usage with a minimum of\nfriction. We show how this framework can be used to both design and train\nneural network architectures, as well as modify state-of-the-art architectures\nin a flexible and intuitive way. We display the pre- and postprocessing\nfunctions common in the medical imaging community that DeepNeuro offers to\nensure consistent performance of networks across variable users, institutions,\nand scanners. And we show how pipelines created in DeepNeuro can be concisely\npackaged into shareable Docker containers and command-line interfaces using\nDeepNeuro's pipeline resources.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 09:03:39 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Beers", "Andrew", ""], ["Brown", "James", ""], ["Chang", "Ken", ""], ["Hoebel", "Katharina", ""], ["Gerstner", "Elizabeth", ""], ["Rosen", "Bruce", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "1808.04593", "submitter": "Simion-Vlad Bogolin", "authors": "Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu", "title": "Unsupervised learning of foreground object detection", "comments": "International Journal of Computer Vision (IJCV), 2019", "journal-ref": null, "doi": "10.1007/s11263-019-01183-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning poses one of the most difficult challenges in computer\nvision today. The task has an immense practical value with many applications in\nartificial intelligence and emerging technologies, as large quantities of\nunlabeled videos can be collected at relatively low cost. In this paper, we\naddress the unsupervised learning problem in the context of detecting the main\nforeground objects in single images. We train a student deep network to predict\nthe output of a teacher pathway that performs unsupervised object discovery in\nvideos or large image collections. Our approach is different from published\nmethods on unsupervised object discovery. We move the unsupervised learning\nphase during training time, then at test time we apply the standard\nfeed-forward processing along the student pathway. This strategy has the\nbenefit of allowing increased generalization possibilities during training,\nwhile remaining fast at testing. Our unsupervised learning algorithm can run\nover several generations of student-teacher training. Thus, a group of student\nnetworks trained in the first generation collectively create the teacher at the\nnext generation. In experiments our method achieves top results on three\ncurrent datasets for object discovery in video, unsupervised image segmentation\nand saliency detection. At test time the proposed system is fast, being one to\ntwo orders of magnitude faster than published unsupervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 09:12:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Croitoru", "Ioana", ""], ["Bogolin", "Simion-Vlad", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1808.04610", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Harish Katti, Mohan Kankanhalli, Ramanathan\n  Subramanian", "title": "Looking Beyond a Clever Narrative: Visual Context and Attention are\n  Primary Drivers of Affect in Video Advertisements", "comments": "Accepted for publication in the Proceedings of 20th ACM International\n  Conference on Multimodal Interaction, Boulder, CO, USA", "journal-ref": null, "doi": "10.1145/3242969.3242988", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion evoked by an advertisement plays a key role in influencing brand\nrecall and eventual consumer choices. Automatic ad affect recognition has\nseveral useful applications. However, the use of content-based feature\nrepresentations does not give insights into how affect is modulated by aspects\nsuch as the ad scene setting, salient object attributes and their interactions.\nNeither do such approaches inform us on how humans prioritize visual\ninformation for ad understanding. Our work addresses these lacunae by\ndecomposing video content into detected objects, coarse scene structure, object\nstatistics and actively attended objects identified via eye-gaze. We measure\nthe importance of each of these information channels by systematically\nincorporating related information into ad affect prediction models. Contrary to\nthe popular notion that ad affect hinges on the narrative and the clever use of\nlinguistic and social cues, we find that actively attended objects and the\ncoarse scene structure better encode affective information as compared to\nindividual scene objects or conspicuous background elements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 10:16:30 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Shukla", "Abhinav", ""], ["Katti", "Harish", ""], ["Kankanhalli", "Mohan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1808.04640", "submitter": "Nikita Moriakov", "authors": "Nikita Moriakov, Koen Michielsen, Jonas Adler, Ritse Mann, Ioannis\n  Sechopoulos, Jonas Teuwen", "title": "Deep Learning Framework for Digital Breast Tomosynthesis Reconstruction", "comments": "4 pages, 2 figures, submitted to SPIE", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital breast tomosynthesis is rapidly replacing digital mammography as the\nbasic x-ray technique for evaluation of the breasts. However, the sparse\nsampling and limited angular range gives rise to different artifacts, which\nmanufacturers try to solve in several ways. In this study we propose an\nextension of the Learned Primal-Dual algorithm for digital breast\ntomosynthesis. The Learned Primal-Dual algorithm is a deep neural network\nconsisting of several `reconstruction blocks', which take in raw sinogram data\nas the initial input, perform a forward and a backward pass by taking\nprojections and back-projections, and use a convolutional neural network to\nproduce an intermediate reconstruction result which is then improved further by\nthe successive reconstruction block. We extend the architecture by providing\nbreast thickness measurements as a mask to the neural network and allow it to\nlearn how to use this thickness mask. We have trained the algorithm on digital\nphantoms and the corresponding noise-free/noisy projections, and then tested\nthe algorithm on digital phantoms for varying level of noise. Reconstruction\nperformance of the algorithms was compared visually, using MSE loss and\nStructural Similarity Index. Results indicate that the proposed algorithm\noutperforms the baseline iterative reconstruction algorithm in terms of\nreconstruction quality for both breast edges and internal structures and is\nrobust to noise.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 11:41:32 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Moriakov", "Nikita", ""], ["Michielsen", "Koen", ""], ["Adler", "Jonas", ""], ["Mann", "Ritse", ""], ["Sechopoulos", "Ioannis", ""], ["Teuwen", "Jonas", ""]]}, {"id": "1808.04699", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Alexei A. Efros, Stella X. Yu", "title": "Improving Generalization via Scalable Neighborhood Component Analysis", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current major approaches to visual recognition follow an end-to-end\nformulation that classifies an input image into one of the pre-determined set\nof semantic categories. Parametric softmax classifiers are a common choice for\nsuch a closed world with fixed categories, especially when big labeled data is\navailable during training. However, this becomes problematic for open-set\nscenarios where new categories are encountered with very few examples for\nlearning a generalizable parametric classifier. We adopt a non-parametric\napproach for visual recognition by optimizing feature embeddings instead of\nparametric classifiers. We use a deep neural network to learn the visual\nfeature that preserves the neighborhood structure in the semantic space, based\non the Neighborhood Component Analysis (NCA) criterion. Limited by its\ncomputational bottlenecks, we devise a mechanism to use augmented memory to\nscale NCA for large datasets and very deep networks. Our experiments deliver\nnot only remarkable performance on ImageNet classification for such a simple\nnon-parametric method, but most importantly a more generalizable feature\nrepresentation for sub-category discovery and few-shot recognition.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 14:03:47 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Wu", "Zhirong", ""], ["Efros", "Alexei A.", ""], ["Yu", "Stella X.", ""]]}, {"id": "1808.04702", "submitter": "Bo Xiong", "authors": "Bo Xiong, Suyog Dutt Jain, Kristen Grauman", "title": "Pixel Objectness: Learning to Segment Generic Objects Automatically in\n  Images and Videos", "comments": "To appear in PAMI. arXiv admin note: text overlap with\n  arXiv:1701.05349, arXiv:1701.05384", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end learning framework for segmenting generic objects in\nboth images and videos. Given a novel image or video, our approach produces a\npixel-level mask for all \"object-like\" regions---even for object categories\nnever seen during training. We formulate the task as a structured prediction\nproblem of assigning an object/background label to each pixel, implemented\nusing a deep fully convolutional network. When applied to a video, our model\nfurther incorporates a motion stream, and the network learns to combine both\nappearance and motion and attempts to extract all prominent objects whether\nthey are moving or not. Beyond the core model, a second contribution of our\napproach is how it leverages varying strengths of training annotations.\nPixel-level annotations are quite difficult to obtain, yet crucial for training\na deep network approach for segmentation. Thus we propose ways to exploit\nweakly labeled data for learning dense foreground segmentation. For images, we\nshow the value in mixing object category examples with image-level labels\ntogether with relatively few images with boundary-level annotations. For video,\nwe show how to bootstrap weakly annotated videos together with the network\ntrained for image segmentation. Through experiments on multiple challenging\nimage and video segmentation benchmarks, our method offers consistently strong\nresults and improves the state-of-the-art for fully automatic segmentation of\ngeneric (unseen) objects. In addition, we demonstrate how our approach benefits\nimage retrieval and image retargeting, both of which flourish when given our\nhigh-quality foreground maps. Code, models, and videos are\nat:http://vision.cs.utexas.edu/projects/pixelobjectness/\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 21:53:22 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 02:46:57 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Xiong", "Bo", ""], ["Jain", "Suyog Dutt", ""], ["Grauman", "Kristen", ""]]}, {"id": "1808.04745", "submitter": "Sebastian Kaltwang", "authors": "Sebastian Kaltwang, Sina Samangooei, John Redford, Andrew Blake", "title": "Imagining the Unseen: Learning a Distribution over Incomplete Images\n  with Dense Latent Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are composed as a hierarchy of object parts. We use this insight to\ncreate a generative graphical model that defines a hierarchical distribution\nover image parts. Typically, this leads to intractable inference due to loops\nin the graph. We propose an alternative model structure, the Dense Latent Tree\n(DLT), which avoids loops and allows for efficient exact inference, while\nmaintaining a dense connectivity between parts of the hierarchy. The usefulness\nof DLTs is shown for the example task of image completion on partially observed\nMNIST and Fashion-MNIST data. We verify having successfully learned a\nhierarchical model of images by visualising its latent states.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:24:39 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Kaltwang", "Sebastian", ""], ["Samangooei", "Sina", ""], ["Redford", "John", ""], ["Blake", "Andrew", ""]]}, {"id": "1808.04754", "submitter": "Bill Yang Cai", "authors": "Bill Yang Cai, Xiaojiang Li, Ian Seiferling, Carlo Ratti", "title": "Treepedia 2.0: Applying Deep Learning for Large-scale Quantification of\n  Urban Tree Cover", "comments": "Accepted and will appear in IEEE BigData Congress 2018 Conference\n  Proceedings", "journal-ref": null, "doi": "10.1109/bigdatacongress.2018.00014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have made it possible to quantify urban\nmetrics at fine resolution, and over large extents using street-level images.\nHere, we focus on measuring urban tree cover using Google Street View (GSV)\nimages. First, we provide a small-scale labelled validation dataset and propose\nstandard metrics to compare the performance of automated estimations of street\ntree cover using GSV. We apply state-of-the-art deep learning models, and\ncompare their performance to a previously established benchmark of an\nunsupervised method. Our training procedure for deep learning models is novel;\nwe utilize the abundance of openly available and similarly labelled\nstreet-level image datasets to pre-train our model. We then perform additional\ntraining on a small training dataset consisting of GSV images. We find that\ndeep learning models significantly outperform the unsupervised benchmark\nmethod. Our semantic segmentation model increased mean intersection-over-union\n(IoU) from 44.10% to 60.42% relative to the unsupervised method and our\nend-to-end model decreased Mean Absolute Error from 10.04% to 4.67%. We also\nemploy a recently developed method called gradient-weighted class activation\nmap (Grad-CAM) to interpret the features learned by the end-to-end model. This\ntechnique confirms that the end-to-end model has accurately learned to identify\ntree cover area as key features for predicting percentage tree cover. Our paper\nprovides an example of applying advanced deep learning techniques on a\nlarge-scale, geo-tagged and image-based dataset to efficiently estimate\nimportant urban metrics. The results demonstrate that deep learning models are\nhighly accurate, can be interpretable, and can also be efficient in terms of\ndata-labelling effort and computational resources.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:34:22 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Cai", "Bill Yang", ""], ["Li", "Xiaojiang", ""], ["Seiferling", "Ian", ""], ["Ratti", "Carlo", ""]]}, {"id": "1808.04795", "submitter": "Xiaoyuan Guo", "authors": "Xiaoyuan Guo, Hanyi Yu, Blair Rossetti, George Teodoro, Daniel Brat\n  and Jun Kong", "title": "Clumped Nuclei Segmentation with Adjacent Point Match and Local Shape\n  based Intensity Analysis for Overlapped Nuclei in Fluorescence In-Situ\n  Hybridization Images", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly clumped nuclei clusters captured in fluorescence in situ hybridization\nmicroscopy images are common histology entities under investigations in a wide\nspectrum of tissue-related biomedical investigations. Due to their large scale\nin presence, computer based image analysis is used to facilitate such analysis\nwith improved analysis efficiency and reproducibility. To ensure the quality of\ndownstream biomedical analyses, it is essential to segment clustered nuclei\nwith high quality. However, this presents a technical challenge commonly\nencountered in a large number of biomedical research, as nuclei are often\noverlapped due to a high cell density. In this paper, we propose an\nsegmentation algorithm that identifies point pair connection candidates and\nevaluates adjacent point connections with a formulated ellipse fitting quality\nindicator. After connection relationships are determined, we recover the\nresulting dividing paths by following points with specific eigenvalues from\nHessian in a constrained searching space. We validate our algorithm with 560\nimage patches from two classes of tumor regions of seven brain tumor patients.\nBoth qualitative and quantitative experimental results suggest that our\nalgorithm is promising for dividing overlapped nuclei in fluorescence in situ\nhybridization microscopy images widely used in various biomedical research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 16:58:59 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Guo", "Xiaoyuan", ""], ["Yu", "Hanyi", ""], ["Rossetti", "Blair", ""], ["Teodoro", "George", ""], ["Brat", "Daniel", ""], ["Kong", "Jun", ""]]}, {"id": "1808.04803", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Hierarchical binary CNNs for landmark localization with limited\n  resources", "comments": "Accepted to IEEE TPAMI18: Best of ICCV 2017 SI. Previously portions\n  of this work appeared as arXiv:1703.00862, which was the conference version", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2866051", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to design architectures that retain the groundbreaking\nperformance of Convolutional Neural Networks (CNNs) for landmark localization\nand at the same time are lightweight, compact and suitable for applications\nwith limited computational resources. To this end, we make the following\ncontributions: (a) we are the first to study the effect of neural network\nbinarization on localization tasks, namely human pose estimation and face\nalignment. We exhaustively evaluate various design choices, identify\nperformance bottlenecks, and more importantly propose multiple orthogonal ways\nto boost performance. (b) Based on our analysis, we propose a novel\nhierarchical, parallel and multi-scale residual architecture that yields large\nperformance improvement over the standard bottleneck block while having the\nsame number of parameters, thus bridging the gap between the original network\nand its binarized counterpart. (c) We perform a large number of ablation\nstudies that shed light on the properties and the performance of the proposed\nblock. (d) We present results for experiments on the most challenging datasets\nfor human pose estimation and face alignment, reporting in many cases\nstate-of-the-art performance. (e) We further provide additional results for the\nproblem of facial part segmentation. Code can be downloaded from\nhttps://www.adrianbulat.com/binary-cnn-landmark\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 17:32:29 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1808.04818", "submitter": "Chengyang Li", "authors": "Chengyang Li, Dan Song, Ruofeng Tong, Min Tang", "title": "Multispectral Pedestrian Detection via Simultaneous Detection and\n  Segmentation", "comments": "British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral pedestrian detection has attracted increasing attention from\nthe research community due to its crucial competence for many around-the-clock\napplications (e.g., video surveillance and autonomous driving), especially\nunder insufficient illumination conditions. We create a human baseline over the\nKAIST dataset and reveal that there is still a large gap between current top\ndetectors and human performance. To narrow this gap, we propose a network\nfusion architecture, which consists of a multispectral proposal network to\ngenerate pedestrian proposals, and a subsequent multispectral classification\nnetwork to distinguish pedestrian instances from hard negatives. The unified\nnetwork is learned by jointly optimizing pedestrian detection and semantic\nsegmentation tasks. The final detections are obtained by integrating the\noutputs from different modalities as well as the two stages. The approach\nsignificantly outperforms state-of-the-art methods on the KAIST dataset while\nremain fast. Additionally, we contribute a sanitized version of training\nannotations for the KAIST dataset, and examine the effects caused by different\nkinds of annotation errors. Future research of this problem will benefit from\nthe sanitized version which eliminates the interference of annotation errors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 17:59:12 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Chengyang", ""], ["Song", "Dan", ""], ["Tong", "Ruofeng", ""], ["Tang", "Min", ""]]}, {"id": "1808.04848", "submitter": "Mark Skouson", "authors": "Mark B. Skouson and Brett J. Borghetti and Robert C. Leishman", "title": "URSA: A Neural Network for Unordered Point Clouds Using Constellations", "comments": "11 pages, 6 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a neural network layer, named Ursa, that uses a\nconstellation of points to learn classification information from point cloud\ndata. Unlike other machine learning classification problems where the task is\nto classify an individual high-dimensional observation, in a point-cloud\nclassification problem the goal is to classify a set of d-dimensional\nobservations. Because a point cloud is a set, there is no ordering to the\ncollection of points in a point-cloud classification problem. Thus, the\nchallenge of classifying point clouds inputs is in building a classifier which\nis agnostic to the ordering of the observations, yet preserves the\nd-dimensional information of each point in the set. This research presents\nUrsa, a new layer type for an artificial neural network which achieves these\ntwo properties. Similar to new methods for this task, this architecture works\ndirectly on d-dimensional points rather than first converting the points to a\nd-dimensional volume. The Ursa layer is followed by a series of dense layers to\nclassify 2D and 3D objects from point clouds. Experiments on ModelNet40 and\nMNIST data show classification results comparable with current methods, while\nreducing the training parameters by over 50 percent.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 18:30:30 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 18:08:13 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Skouson", "Mark B.", ""], ["Borghetti", "Brett J.", ""], ["Leishman", "Robert C.", ""]]}, {"id": "1808.04859", "submitter": "Hao Tang", "authors": "Hao Tang, Wei Wang, Dan Xu, Yan Yan, Nicu Sebe", "title": "GestureGAN for Hand Gesture-to-Gesture Translation in the Wild", "comments": "9 pages, 7 figures, accepted to ACM MM 2018 as an oral paper, fix\n  typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture-to-gesture translation in the wild is a challenging task since\nhand gestures can have arbitrary poses, sizes, locations and self-occlusions.\nTherefore, this task requires a high-level understanding of the mapping between\nthe input source gesture and the output target gesture. To tackle this problem,\nwe propose a novel hand Gesture Generative Adversarial Network (GestureGAN).\nGestureGAN consists of a single generator $G$ and a discriminator $D$, which\ntakes as input a conditional hand image and a target hand skeleton image.\nGestureGAN utilizes the hand skeleton information explicitly, and learns the\ngesture-to-gesture mapping through two novel losses, the color loss and the\ncycle-consistency loss. The proposed color loss handles the issue of \"channel\npollution\" while back-propagating the gradients. In addition, we present the\nFr\\'echet ResNet Distance (FRD) to evaluate the quality of generated images.\nExtensive experiments on two widely used benchmark datasets demonstrate that\nthe proposed GestureGAN achieves state-of-the-art performance on the\nunconstrained hand gesture-to-gesture translation task. Meanwhile, the\ngenerated images are in high-quality and are photo-realistic, allowing them to\nbe used as data augmentation to improve the performance of a hand gesture\nclassifier. Our model and code are available at\nhttps://github.com/Ha0Tang/GestureGAN.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 18:57:22 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 11:01:02 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Wei", ""], ["Xu", "Dan", ""], ["Yan", "Yan", ""], ["Sebe", "Nicu", ""]]}, {"id": "1808.04878", "submitter": "Ozan Candogan", "authors": "Baris Ata, Alexandre Belloni, Ozan Candogan", "title": "Latent Agents in Networks: Estimation and Pricing", "comments": "91 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV econ.TH math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on a setting where agents in a social network consume a product that\nexhibits positive local network externalities. A seller has access to data on\npast consumption decisions/prices for a subset of observable agents, and can\ntarget these agents with appropriate discounts to exploit network effects and\nincrease her revenues. A novel feature of the model is that the observable\nagents potentially interact with additional latent agents. These latent agents\ncan purchase the same product from a different channel, and are not observed by\nthe seller. Observable agents influence each other both directly and indirectly\nthrough the influence they exert on the latent agents. The seller knows the\nconnection structure of neither the observable nor the latent part of the\nnetwork.\n  Due to the presence of network externalities, an agent's consumption decision\ndepends not only on the price offered to her, but also on the consumption\ndecisions of (and in turn the prices offered to) her neighbors in the\nunderlying network. We investigate how the seller can use the available data to\nestimate the matrix that captures the dependence of observable agents'\nconsumption decisions on the prices offered to them. We provide an algorithm\nfor estimating this matrix under an approximate sparsity condition, and obtain\nconvergence rates for the proposed estimator despite the high dimensionality\nthat allows more agents than observations. Importantly, we then show that this\napproximate sparsity condition holds under standard conditions present in the\nliterature and hence our algorithms are applicable to a large class of\nnetworks. We establish that by using the estimated matrix the seller can\nconstruct prices that lead to a small revenue loss relative to\nrevenue-maximizing prices under complete information, and the optimality gap\nvanishes relative to the size of the network.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:57:55 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 16:51:27 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Ata", "Baris", ""], ["Belloni", "Alexandre", ""], ["Candogan", "Ozan", ""]]}, {"id": "1808.04909", "submitter": "Jonas Teuwen", "authors": "Joris van Vugt, Elena Marchiori, Ritse Mann, Albert Gubern-M\\'erida,\n  Nikita Moriakov, Jonas Teuwen", "title": "Vendor-independent soft tissue lesion detection using weakly supervised\n  and unsupervised adversarial domain adaptation", "comments": "Submitted to SPIE MI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided detection aims to improve breast cancer screening programs by\nhelping radiologists to evaluate digital mammography (DM) exams. DM exams are\ngenerated by devices from different vendors, with diverse characteristics\nbetween and even within vendors. Physical properties of these devices and\npostprocessing of the images can greatly influence the resulting mammogram.\nThis results in the fact that a deep learning model trained on data from one\nvendor cannot readily be applied to data from another vendor. This paper\ninvestigates the use of tailored transfer learning methods based on adversarial\nlearning to tackle this problem. We consider a database of DM exams (mostly\nbilateral and two views) generated by Hologic and Siemens vendors. We analyze\ntwo transfer learning settings: 1) unsupervised transfer, where Hologic data\nwith soft lesion annotation at pixel level and Siemens unlabelled data are used\nto annotate images in the latter data; 2) weak supervised transfer, where exam\nlevel labels for images from the Siemens mammograph are available. We propose\ntailored variants of recent state-of-the-art methods for transfer learning\nwhich take into account the class imbalance and incorporate knowledge provided\nby the annotations at exam level. Results of experiments indicate the\nbeneficial effect of transfer learning in both transfer settings. Notably, at\n0.02 false positives per image, we achieve a sensitivity of 0.37, compared to\n0.30 of a baseline with no transfer. Results indicate that using exam level\nannotations gives an additional increase in sensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 21:58:28 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["van Vugt", "Joris", ""], ["Marchiori", "Elena", ""], ["Mann", "Ritse", ""], ["Gubern-M\u00e9rida", "Albert", ""], ["Moriakov", "Nikita", ""], ["Teuwen", "Jonas", ""]]}, {"id": "1808.04929", "submitter": "Lucian Trestioreanu", "authors": "Lucian Trestioreanu", "title": "Holographic Visualisation of Radiology Data and Automated Machine\n  Learning-based Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within this thesis we propose a platform for combining Augmented Reality (AR)\nhardware with machine learning in a user-oriented pipeline, offering to the\nmedical staff an intuitive 3D visualization of volumetric Computed Tomography\n(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside\nthe AR headset, that does not need human intervention for loading, processing\nand segmentation of medical images. The AR visualization, based on Microsoft\nHoloLens, employs a modular and thus scalable frontend-backend architecture for\nreal-time visualizations on multiple AR headsets. As Convolutional Neural\nNetworks (CNNs) have lastly demonstrated superior performance for the machine\nlearning task of image semantic segmentation, the pipeline also includes a\nfully automated CNN algorithm for the segmentation of the liver from CT scans.\nThe model is based on the Deep Retinal Image Understanding (DRIU) model which\nis a Fully Convolutional Network with side outputs from feature maps with\ndifferent resolution, extracted at different stages of the network. The\nalgorithm is 2.5D which means that the input is a set of consecutive scan\nslices. The experiments have been performed on the Liver Tumor Segmentation\nChallenge (LiTS) dataset for liver segmentation and demonstrated good results\nand flexibility. While multiple approaches exist in the domain, only few of\nthem have focused on overcoming the practical aspects which still largely hold\nthis technology away from the operating rooms. In line with this, we also are\nnext planning an evaluation from medical doctors and radiologists in a\nreal-world environment.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 00:20:35 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Trestioreanu", "Lucian", ""]]}, {"id": "1808.04952", "submitter": "Hao Pan", "authors": "Yuqi Yang, Shilin Liu, Hao Pan, Yang Liu, Xin Tong", "title": "PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel\n  Frames", "comments": "15 pages, 18 figures. CVPR 2020. Project page:\n  https://haopan.github.io/surfacecnn.html", "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 13578-13587", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface meshes are widely used shape representations and capture finer\ngeometry data than point clouds or volumetric grids, but are challenging to\napply CNNs directly due to their non-Euclidean structure. We use parallel\nframes on surface to define PFCNNs that enable effective feature learning on\nsurface meshes by mimicking standard convolutions faithfully. In particular,\nthe convolution of PFCNN not only maps local surface patches onto flat tangent\nplanes, but also aligns the tangent planes such that they locally form a flat\nEuclidean structure, thus enabling recovery of standard convolutions. The\nalignment is achieved by the tool of locally flat connections borrowed from\ndiscrete differential geometry, which can be efficiently encoded and computed\nby parallel frame fields. In addition, the lack of canonical axis on surface is\nhandled by sampling with the frame directions. Experiments show that for tasks\nincluding classification, segmentation and registration on deformable geometric\ndomains, as well as semantic scene segmentation on rigid domains, PFCNNs\nachieve robust and superior performances without using sophisticated input\nfeatures than state-of-the-art surface based CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 02:39:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 05:58:56 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yang", "Yuqi", ""], ["Liu", "Shilin", ""], ["Pan", "Hao", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1808.04974", "submitter": "Yonghyun Kim", "authors": "Yonghyun Kim, Bong-Nam Kang and Daijin Kim", "title": "SAN: Learning Relationship between Convolutional Features for\n  Multi-Scale Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the recent successful methods in accurate object detection build on\nthe convolutional neural networks (CNN). However, due to the lack of scale\nnormalization in CNN-based detection methods, the activated channels in the\nfeature space can be completely different according to a scale and this\ndifference makes it hard for the classifier to learn samples. We propose a\nScale Aware Network (SAN) that maps the convolutional features from the\ndifferent scales onto a scale-invariant subspace to make CNN-based detection\nmethods more robust to the scale variation, and also construct a unique\nlearning method which considers purely the relationship between channels\nwithout the spatial information for the efficient learning of SAN. To show the\nvalidity of our method, we visualize how convolutional features change\naccording to the scale through a channel activation matrix and experimentally\nshow that SAN reduces the feature differences in the scale space. We evaluate\nour method on VOC PASCAL and MS COCO dataset. We demonstrate SAN by conducting\nseveral experiments on structures and parameters. The proposed SAN can be\ngenerally applied to many CNN-based detection methods to enhance the detection\naccuracy with a slight increase in the computing time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 05:35:44 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Kim", "Yonghyun", ""], ["Kang", "Bong-Nam", ""], ["Kim", "Daijin", ""]]}, {"id": "1808.04976", "submitter": "Bong-Nam Kang", "authors": "Bong-Nam Kang, Yonghyun Kim, Daijin Kim", "title": "Pairwise Relational Networks for Face Recognition", "comments": "to appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing face recognition using deep neural networks is difficult to know\nwhat kind of features are used to discriminate the identities of face images\nclearly. To investigate the effective features for face recognition, we propose\na novel face recognition method, called a pairwise relational network (PRN),\nthat obtains local appearance patches around landmark points on the feature\nmap, and captures the pairwise relation between a pair of local appearance\npatches. The PRN is trained to capture unique and discriminative pairwise\nrelations among different identities. Because the existence and meaning of\npairwise relations should be identity dependent, we add a face identity state\nfeature, which obtains from the long short-term memory (LSTM) units network\nwith the sequential local appearance patches on the feature maps, to the PRN.\nTo further improve accuracy of face recognition, we combined the global\nappearance representation with the pairwise relational feature. Experimental\nresults on the LFW show that the PRN using only pairwise relations achieved\n99.65% accuracy and the PRN using both pairwise relations and face identity\nstate feature achieved 99.76% accuracy. On the YTF, both the PRN using only\npairwise relations and the PRN using pairwise relations and the face identity\nstate feature achieved the state-of-the-art (95.7% and 96.3%). The PRN also\nachieved comparable results to the state-of-the-art for both face verification\nand face identification tasks on the IJB-A, and the state-of-the-art on the\nIJB-B.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 05:52:31 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Kang", "Bong-Nam", ""], ["Kim", "Yonghyun", ""], ["Kim", "Daijin", ""]]}, {"id": "1808.04999", "submitter": "Xiaotian Li", "authors": "Xiaotian Li, Juha Ylioinas, Jakob Verbeek and Juho Kannala", "title": "Scene Coordinate Regression with Angle-Based Reprojection Loss for\n  Camera Relocalization", "comments": "ECCV 2018 Workshop (Geometry Meets Deep Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based camera relocalization is an important problem in computer vision\nand robotics. Recent works utilize convolutional neural networks (CNNs) to\nregress for pixels in a query image their corresponding 3D world coordinates in\nthe scene. The final pose is then solved via a RANSAC-based optimization scheme\nusing the predicted coordinates. Usually, the CNN is trained with ground truth\nscene coordinates, but it has also been shown that the network can discover 3D\nscene geometry automatically by minimizing single-view reprojection loss.\nHowever, due to the deficiencies of the reprojection loss, the network needs to\nbe carefully initialized. In this paper, we present a new angle-based\nreprojection loss, which resolves the issues of the original reprojection loss.\nWith this new loss function, the network can be trained without careful\ninitialization, and the system achieves more accurate results. The new loss\nalso enables us to utilize available multi-view constraints, which further\nimprove performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 08:25:15 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 22:34:22 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Li", "Xiaotian", ""], ["Ylioinas", "Juha", ""], ["Verbeek", "Jakob", ""], ["Kannala", "Juho", ""]]}, {"id": "1808.05022", "submitter": "Federico Magliani", "authors": "Federico Magliani and Tomaso Fontanini and Andrea Prati", "title": "A Dense-Depth Representation for VLAD descriptors in Content-Based Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent advances brought by deep learning allowed to improve the\nperformance in image retrieval tasks. Through the many convolutional layers,\navailable in a Convolutional Neural Network (CNN), it is possible to obtain a\nhierarchy of features from the evaluated image. At every step, the patches\nextracted are smaller than the previous levels and more representative.\nFollowing this idea, this paper introduces a new detector applied on the\nfeature maps extracted from pre-trained CNN. Specifically, this approach lets\nto increase the number of features in order to increase the performance of the\naggregation algorithms like the most famous and used VLAD embedding. The\nproposed approach is tested on different public datasets: Holidays, Oxford5k,\nParis6k and UKB.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 10:01:33 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Magliani", "Federico", ""], ["Fontanini", "Tomaso", ""], ["Prati", "Andrea", ""]]}, {"id": "1808.05071", "submitter": "Tomas Majtner", "authors": "Tom\\'a\\v{s} Majtner, Buda Baji\\'c, Sule Yildirim, Jon Yngve Hardeberg,\n  Joakim Lindblad, Nata\\v{s}a Sladoje", "title": "Ensemble of Convolutional Neural Networks for Dermoscopic Images\n  Classification", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we are presenting our automated prediction system for disease\nclassification within dermoscopic images. The proposed solution is based on\ndeep learning, where we employed transfer learning strategy on VGG16 and\nGoogLeNet architectures. The key feature of our solution is preprocessing based\nprimarily on image augmentation and colour normalization. The solution was\nevaluated on Task 3: Lesion Diagnosis of the ISIC 2018: Skin Lesion Analysis\nTowards Melanoma Detection.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 13:36:41 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Majtner", "Tom\u00e1\u0161", ""], ["Baji\u0107", "Buda", ""], ["Yildirim", "Sule", ""], ["Hardeberg", "Jon Yngve", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "1808.05075", "submitter": "Leuleseged Alemu", "authors": "Leulseged Tesfaye Alemu, Marcello Pelillo", "title": "Multi-feature Fusion for Image Retrieval Using Constrained Dominant Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aggregating different image features for image retrieval has recently shown\nits effectiveness. While highly effective, though, the question of how to\nuplift the impact of the best features for a specific query image persists as\nan open computer vision problem. In this paper, we propose a computationally\nefficient approach to fuse several hand-crafted and deep features, based on the\nprobabilistic distribution of a given membership score of a constrained cluster\nin an unsupervised manner. First, we introduce an incremental nearest neighbor\n(NN) selection method, whereby we dynamically select k-NN to the query. We then\nbuild several graphs from the obtained NN sets and employ constrained dominant\nsets (CDS) on each graph G to assign edge weights which consider the intrinsic\nmanifold structure of the graph, and detect false matches to the query.\nFinally, we elaborate the computation of feature positive-impact weight (PIW)\nbased on the dispersive degree of the characteristics vector. To this end, we\nexploit the entropy of a cluster membership-score distribution. In addition,\nthe final NN set bypasses a heuristic voting scheme. Experiments on several\nretrieval benchmark datasets show that our method can improve the\nstate-of-the-art result.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 13:41:22 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Alemu", "Leulseged Tesfaye", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1808.05085", "submitter": "Zhaoyang Zhang", "authors": "Zhaoyang Zhang, Zhanghui Kuang, Ping Luo, Litong Feng, Wei Zhang", "title": "Temporal Sequence Distillation: Towards Few-Frame Action Recognition in\n  Videos", "comments": "Accepted by ACM Multimedia", "journal-ref": null, "doi": "10.1145/3240508.3240534", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Analytics Software as a Service (VA SaaS) has been rapidly growing in\nrecent years. VA SaaS is typically accessed by users using a lightweight\nclient. Because the transmission bandwidth between the client and cloud is\nusually limited and expensive, it brings great benefits to design cloud video\nanalysis algorithms with a limited data transmission requirement. Although\nconsiderable research has been devoted to video analysis, to our best\nknowledge, little of them has paid attention to the transmission bandwidth\nlimitation in SaaS. As the first attempt in this direction, this work\nintroduces a problem of few-frame action recognition, which aims at maintaining\nhigh recognition accuracy, when accessing only a few frames during both\ntraining and test. Unlike previous work that processed dense frames, we present\nTemporal Sequence Distillation (TSD), which distills a long video sequence into\na very short one for transmission. By end-to-end training with 3D CNNs for\nvideo action recognition, TSD learns a compact and discriminative temporal and\nspatial representation of video frames. On Kinetics dataset, TSD+I3D typically\nrequires only 50\\% of the number of frames compared to I3D, a state-of-the-art\nvideo action recognition algorithm, to achieve almost the same accuracies. The\nproposed TSD has three appealing advantages. Firstly, TSD has a lightweight\narchitecture and can be deployed in the client, eg. mobile devices, to produce\ncompressed representative frames to save transmission bandwidth. Secondly, TSD\nsignificantly reduces the computations to run video action recognition with\ncompressed frames on the cloud, while maintaining high recognition accuracies.\nThirdly, TSD can be plugged in as a preprocessing module of any existing 3D\nCNNs. Extensive experiments show the effectiveness and characteristics of TSD.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 13:59:00 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhang", "Zhaoyang", ""], ["Kuang", "Zhanghui", ""], ["Luo", "Ping", ""], ["Feng", "Litong", ""], ["Zhang", "Wei", ""]]}, {"id": "1808.05130", "submitter": "Ilkay Oksuz", "authors": "Ilkay Oksuz, Bram Ruijsink, Esther Puyol-Anton, Aurelien Bustin,\n  Gastao Cruz, Claudia Prieto, Daniel Rueckert, Julia A. Schnabel, Andrew P.\n  King", "title": "Deep Learning using K-space Based Data Augmentation for Automated\n  Cardiac MR Motion Artefact Detection", "comments": "Accepted for MICCAI2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment of medical images is essential for complete automation of\nimage processing pipelines. For large population studies such as the UK\nBiobank, artefacts such as those caused by heart motion are problematic and\nmanual identification is tedious and time-consuming. Therefore, there is an\nurgent need for automatic image quality assessment techniques. In this paper,\nwe propose a method to automatically detect the presence of motion-related\nartefacts in cardiac magnetic resonance (CMR) images. As this is a highly\nimbalanced classification problem (due to the high number of good quality\nimages compared to the low number of images with motion artefacts), we propose\na novel k-space based training data augmentation approach in order to address\nthis problem. Our method is based on 3D spatio-temporal Convolutional Neural\nNetworks, and is able to detect 2D+time short axis images with motion artefacts\nin less than 1ms. We test our algorithm on a subset of the UK Biobank dataset\nconsisting of 3465 CMR images and achieve not only high accuracy in detection\nof motion artefacts, but also high precision and recall. We compare our\napproach to a range of state-of-the-art quality assessment methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:22:21 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 09:14:46 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Oksuz", "Ilkay", ""], ["Ruijsink", "Bram", ""], ["Puyol-Anton", "Esther", ""], ["Bustin", "Aurelien", ""], ["Cruz", "Gastao", ""], ["Prieto", "Claudia", ""], ["Rueckert", "Daniel", ""], ["Schnabel", "Julia A.", ""], ["King", "Andrew P.", ""]]}, {"id": "1808.05137", "submitter": "Daniele Faccio", "authors": "Alessandro Boccolini, Alessandro Fedrizzi, Daniele Faccio", "title": "Ghost imaging with the human eye", "comments": null, "journal-ref": null, "doi": "10.1364/OE.27.009258", "report-no": null, "categories": "q-bio.NC cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational ghost imaging relies on the decomposition of an image into\npatterns that are summed together with weights that measure the overlap of each\npattern with the scene being imaged. These tasks rely on a computer. Here we\ndemonstrate that the computational integration can be performed directly with\nthe human eye. We use this human ghost imaging technique to evaluate the\ntemporal response of the eye and establish the image persistence time to be\naround 20 ms followed by a further 20 ms exponential decay. These persistence\ntimes are in agreement with previous studies but can now potentially be\nextended to include a more precise characterisation of visual stimuli and\nprovide a new experimental tool for the study of visual perception.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:04:53 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Boccolini", "Alessandro", ""], ["Fedrizzi", "Alessandro", ""], ["Faccio", "Daniele", ""]]}, {"id": "1808.05174", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Shugao Ma, Deva Ramanan, Yaser Sheikh", "title": "Recycle-GAN: Unsupervised Video Retargeting", "comments": "ECCV 2018; Please refer to project webpage for videos -\n  http://www.cs.cmu.edu/~aayushb/Recycle-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach for unsupervised video retargeting that\ntranslates content from one domain to another while preserving the style native\nto a domain, i.e., if contents of John Oliver's speech were to be transferred\nto Stephen Colbert, then the generated content/speech should be in Stephen\nColbert's style. Our approach combines both spatial and temporal information\nalong with adversarial losses for content translation and style preservation.\nIn this work, we first study the advantages of using spatiotemporal constraints\nover spatial constraints for effective retargeting. We then demonstrate the\nproposed approach for the problems where information in both space and time\nmatters such as face-to-face translation, flower-to-flower, wind and cloud\nsynthesis, sunrise and sunset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:34:08 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Bansal", "Aayush", ""], ["Ma", "Shugao", ""], ["Ramanan", "Deva", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1808.05205", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Tanveer Syeda-Mahmood, Mehdi Moradi", "title": "Building medical image classifiers with very limited data using\n  segmentation networks", "comments": "This paper was accepted by Medical Image Analysis", "journal-ref": "Medical Image Analysis 49 (2018) 105-116", "doi": "10.1016/j.media.2018.07.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown promising results in medical image analysis, however,\nthe lack of very large annotated datasets confines its full potential. Although\ntransfer learning with ImageNet pre-trained classification models can alleviate\nthe problem, constrained image sizes and model complexities can lead to\nunnecessary increase in computational cost and decrease in performance. As many\ncommon morphological features are usually shared by different classification\ntasks of an organ, it is greatly beneficial if we can extract such features to\nimprove classification with limited samples. Therefore, inspired by the idea of\ncurriculum learning, we propose a strategy for building medical image\nclassifiers using features from segmentation networks. By using a segmentation\nnetwork pre-trained on similar data as the classification task, the machine can\nfirst learn the simpler shape and structural concepts before tackling the\nactual classification problem which usually involves more complicated concepts.\nUsing our proposed framework on a 3D three-class brain tumor type\nclassification problem, we achieved 82% accuracy on 191 testing samples with 91\ntraining samples. When applying to a 2D nine-class cardiac semantic level\nclassification problem, we achieved 86% accuracy on 263 testing samples with\n108 training samples. Comparisons with ImageNet pre-trained classifiers and\nclassifiers trained from scratch are presented.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 17:52:09 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Syeda-Mahmood", "Tanveer", ""], ["Moradi", "Mehdi", ""]]}, {"id": "1808.05238", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Yufang Huang, Liang Zeng, Xuming Chen, Yong Liu, Zhen\n  Qian, Nan Du, Wei Fan, Xiaohui Xie", "title": "AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume\n  Segmentation of Head and Neck Anatomy", "comments": "6 figures, 4 videos in GitHub and YouTube. Accepted by Medical\n  Physics. Code and videos are available on GitHub. Video:\n  https://www.youtube.com/watch?v=_PpIUIm4XLU", "journal-ref": null, "doi": "10.1002/mp.13300", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Methods: Our deep learning model, called AnatomyNet, segments OARs from head\nand neck CT images in an end-to-end fashion, receiving whole-volume HaN CT\nimages as input and generating masks of all OARs of interest in one shot.\nAnatomyNet is built upon the popular 3D U-net architecture, but extends it in\nthree important ways: 1) a new encoding scheme to allow auto-segmentation on\nwhole-volume CT images instead of local patches or subsets of slices, 2)\nincorporating 3D squeeze-and-excitation residual blocks in encoding layers for\nbetter feature representation, and 3) a new loss function combining Dice scores\nand focal loss to facilitate the training of the neural model. These features\nare designed to address two main challenges in deep-learning-based HaN\nsegmentation: a) segmenting small anatomies (i.e., optic chiasm and optic\nnerves) occupying only a few slices, and b) training with inconsistent data\nannotations with missing ground truth for some anatomical structures.\n  Results: We collected 261 HaN CT images to train AnatomyNet, and used MICCAI\nHead and Neck Auto Segmentation Challenge 2015 as a benchmark dataset to\nevaluate the performance of AnatomyNet. The objective is to segment nine\nanatomies: brain stem, chiasm, mandible, optic nerve left, optic nerve right,\nparotid gland left, parotid gland right, submandibular gland left, and\nsubmandibular gland right. Compared to previous state-of-the-art results from\nthe MICCAI 2015 competition, AnatomyNet increases Dice similarity coefficient\nby 3.3% on average. AnatomyNet takes about 0.12 seconds to fully segment a head\nand neck CT image of dimension 178 x 302 x 225, significantly faster than\nprevious methods. In addition, the model is able to process whole-volume CT\nimages and delineate all OARs in one pass, requiring little pre- or\npost-processing.\nhttps://github.com/wentaozhu/AnatomyNet-for-anatomical-segmentation.git.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 18:03:12 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 00:23:48 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Zhu", "Wentao", ""], ["Huang", "Yufang", ""], ["Zeng", "Liang", ""], ["Chen", "Xuming", ""], ["Liu", "Yong", ""], ["Qian", "Zhen", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1808.05240", "submitter": "Penghang Yin", "authors": "Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi,\n  Jack Xin", "title": "Blended Coarse Gradient Descent for Full Quantization of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized deep neural networks (QDNNs) are attractive due to their much lower\nmemory storage and faster inference speed than their regular full precision\ncounterparts. To maintain the same performance level especially at low\nbit-widths, QDNNs must be retrained. Their training involves piecewise constant\nactivation functions and discrete weights, hence mathematical challenges arise.\nWe introduce the notion of coarse gradient and propose the blended coarse\ngradient descent (BCGD) algorithm, for training fully quantized neural\nnetworks. Coarse gradient is generally not a gradient of any function but an\nartificial ascent direction. The weight update of BCGD goes by coarse gradient\ncorrection of a weighted average of the full precision weights and their\nquantization (the so-called blending), which yields sufficient descent in the\nobjective value and thus accelerates the training. Our experiments demonstrate\nthat this simple blending technique is very effective for quantization at\nextremely low bit-width such as binarization. In full quantization of ResNet-18\nfor ImageNet classification task, BCGD gives 64.36\\% top-1 accuracy with binary\nweights across all layers and 4-bit adaptive activation. If the weights in the\nfirst and last layers are kept in full precision, this number increases to\n65.46\\%. As theoretical justification, we show convergence analysis of coarse\ngradient descent for a two-linear-layer neural network model with Gaussian\ninput data, and prove that the expected coarse gradient correlates positively\nwith the underlying true gradient.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 18:13:12 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 01:37:23 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 07:04:22 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2019 06:00:43 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Yin", "Penghang", ""], ["Zhang", "Shuai", ""], ["Lyu", "Jiancheng", ""], ["Osher", "Stanley", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "1808.05279", "submitter": "Isaac Gerg", "authors": "Brian Reinhardt, Isaac Gerg, Daniel Brown, Joonho Park", "title": "Measuring Human Assessed Complexity in Synthetic Aperture Sonar Imagery\n  Using the Elo Rating System", "comments": "Accepted for the Institute of Acoustics 4th International Conference\n  on Synthetic Apertures Sonar and Radar Sept 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of automatic target recognition from synthetic aperture sonar\ndata is heavily dependent on the complexity of the beamformed imagery. Several\nmechanisms can contribute to this, including unwanted vehicle dynamics, the\nbathymetry of the scene, and the presence of natural and manmade clutter. To\nunderstand the impact of the environmental complexity on image perception,\nresearchers have taken approaches rooted in information theory, or heuristics.\nDespite these efforts, a quantitative measure for complexity has not been\nrelated to the phenomenology from which it is derived.\n  By using subject matter experts (SMEs) we derive a complexity metric for a\nset of imagery which accounts for the underlying phenomenology. The goal of\nthis work is to develop an understanding of how several common information\ntheoretic and heuristic measures are related to the SME perceived complexity in\nsynthetic aperture sonar imagery. To achieve this, an ensemble of 10-meter x\n10-meter images were cropped from a high-frequency SAS data set that spans\nmultiple environments. The SME's were presented pairs of images from which they\ncould rate the relative image complexity. These comparisons were then converted\ninto the desired sequential ranking using a method first developed by A. Elo\nfor establishing rankings of chess players.\n  The Elo method produced a plausible rank ordering across the broad dataset.\nThe heuristic and information theoretical metrics were then compared to the\nimage rank from which they were derived. The metrics with the highest degree of\ncorrelation were those relating to spatial information, e.g. variations in\npixel intensity, with an R-squared value of approximately 0.9. However, this\nagreement was dependent on the scale from which the spatial variation was\nmeasured. Results will also be presented for many other measures including\nlacunarity, image compression, and entropy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 20:06:38 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Reinhardt", "Brian", ""], ["Gerg", "Isaac", ""], ["Brown", "Daniel", ""], ["Park", "Joonho", ""]]}, {"id": "1808.05285", "submitter": "Denis Gudovskiy", "authors": "Denis A. Gudovskiy, Alec Hodgkinson, Luca Rigazio", "title": "DNN Feature Map Compression using Learned Representation over GF(2)", "comments": "CEFRL2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a method to compress intermediate feature maps of\ndeep neural networks (DNNs) to decrease memory storage and bandwidth\nrequirements during inference. Unlike previous works, the proposed method is\nbased on converting fixed-point activations into vectors over the smallest\nGF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers\nembedded into a DNN. Such an end-to-end learned representation finds more\ncompact feature maps by exploiting quantization redundancies within the\nfixed-point activations along the channel or spatial dimensions. We apply the\nproposed network architectures derived from modified SqueezeNet and MobileNetV2\nto the tasks of ImageNet classification and PASCAL VOC object detection.\nCompared to prior approaches, the conducted experiments show a factor of 2\ndecrease in memory requirements with minor degradation in accuracy while adding\nonly bitwise computations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 21:13:06 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Gudovskiy", "Denis A.", ""], ["Hodgkinson", "Alec", ""], ["Rigazio", "Luca", ""]]}, {"id": "1808.05323", "submitter": "Yudong Guo", "authors": "Yudong Guo, Lin Cai, Juyong Zhang", "title": "3D Face From X: Learning Face Shape from Diverse Sources", "comments": "Accepted by IEEE Transactions on Image Processing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to jointly learn a 3D face parametric model and 3D\nface reconstruction from diverse sources. Previous methods usually learn 3D\nface modeling from one kind of source, such as scanned data or in-the-wild\nimages. Although 3D scanned data contain accurate geometric information of face\nshapes, the capture system is expensive and such datasets usually contain a\nsmall number of subjects. On the other hand, in-the-wild face images are easily\nobtained and there are a large number of facial images. However, facial images\ndo not contain explicit geometric information. In this paper, we propose a\nmethod to learn a unified face model from diverse sources. Besides scanned face\ndata and face images, we also utilize a large number of RGB-D images captured\nwith an iPhone X to bridge the gap between the two sources. Experimental\nresults demonstrate that with training data from more sources, we can learn a\nmore powerful face model.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 01:59:15 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 00:57:16 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 09:20:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Guo", "Yudong", ""], ["Cai", "Lin", ""], ["Zhang", "Juyong", ""]]}, {"id": "1808.05331", "submitter": "Risheng Liu", "authors": "Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, Zhongxuan\n  Luo", "title": "On the Convergence of Learning-based Iterative Methods for Nonconvex\n  Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous tasks at the core of statistics, learning and vision areas are\nspecific cases of ill-posed inverse problems. Recently, learning-based (e.g.,\ndeep) iterative methods have been empirically shown to be useful for these\nproblems. Nevertheless, integrating learnable structures into iterations is\nstill a laborious process, which can only be guided by intuitions or empirical\ninsights. Moreover, there is a lack of rigorous analysis about the convergence\nbehaviors of these reimplemented iterations, and thus the significance of such\nmethods is a little bit vague. This paper moves beyond these limits and\nproposes Flexible Iterative Modularization Algorithm (FIMA), a generic and\nprovable paradigm for nonconvex inverse problems. Our theoretical analysis\nreveals that FIMA allows us to generate globally convergent trajectories for\nlearning-based iterative methods. Meanwhile, the devised scheduling policies on\nflexible modules should also be beneficial for classical numerical methods in\nthe nonconvex scenario. Extensive experiments on real applications verify the\nsuperiority of FIMA.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:02:59 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Liu", "Risheng", ""], ["Cheng", "Shichao", ""], ["He", "Yi", ""], ["Fan", "Xin", ""], ["Lin", "Zhouchen", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1808.05336", "submitter": "Sunil Prakash", "authors": "Sunil Prakash and Gaelan Gu", "title": "Simultaneous Localization And Mapping with depth Prediction using\n  Capsule Networks for UAVs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an novel implementation of a simultaneous\nlocalization and mapping (SLAM) system based on a monocular camera from an\nunmanned aerial vehicle (UAV) using Depth prediction performed with Capsule\nNetworks (CapsNet), which possess improvements over the drawbacks of the more\nwidely-used Convolutional Neural Networks (CNN). An Extended Kalman Filter will\nassist in estimating the position of the UAV so that we are able to update the\nbelief for the environment. Results will be evaluated on a benchmark dataset to\nportray the accuracy of our intended approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:39:25 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Prakash", "Sunil", ""], ["Gu", "Gaelan", ""]]}, {"id": "1808.05380", "submitter": "Tejo Chalasani", "authors": "Tejo Chalasani and Jan Ondrej and Aljosa Smolic", "title": "Egocentric Gesture Recognition for Head-Mounted AR devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural interaction with virtual objects in AR/VR environments makes for a\nsmooth user experience. Gestures are a natural extension from real world to\naugmented space to achieve these interactions. Finding discriminating\nspatio-temporal features relevant to gestures and hands in ego-view is the\nprimary challenge for recognising egocentric gestures. In this work we propose\na data driven end-to-end deep learning approach to address the problem of\negocentric gesture recognition, which combines an ego-hand encoder network to\nfind ego-hand features, and a recurrent neural network to discern temporally\ndiscriminating features. Since deep learning networks are data intensive, we\npropose a novel data augmentation technique using green screen capture to\nalleviate the problem of ground truth annotation. In addition we publish a\ndataset of 10 gestures performed in a natural fashion in front of a green\nscreen for training and the same 10 gestures performed in different natural\nscenes without green screen for validation. We also present the results of our\nnetwork's performance in comparison to the state-of-the-art using the AirGest\ndataset\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:00:56 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Chalasani", "Tejo", ""], ["Ondrej", "Jan", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1808.05382", "submitter": "Mario R\\\"uttgers", "authors": "Mario R\\\"uttgers, Sangseung Lee and Donghyun You", "title": "Typhoon track prediction using satellite images in a Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracks of typhoons are predicted using satellite images as input for a\nGenerative Adversarial Network (GAN). The satellite images have time gaps of 6\nhours and are marked with a red square at the location of the typhoon center.\nThe GAN uses images from the past to generate an image one time step ahead. The\ngenerated image shows the future location of the typhoon center, as well as the\nfuture cloud structures. The errors between predicted and real typhoon centers\nare measured quantitatively in kilometers. 42.4% of all typhoon center\npredictions have absolute errors of less than 80 km, 32.1% lie within a range\nof 80 - 120 km and the remaining 25.5% have accuracies above 120 km. The\nrelative error sets the above mentioned absolute error in relation to the\ndistance that has been traveled by a typhoon over the past 6 hours. High\nrelative errors are found in three types of situations, when a typhoon moves on\nthe open sea far away from land, when a typhoon changes its course suddenly and\nwhen a typhoon is about to hit the mainland. The cloud structure prediction is\nevaluated qualitatively. It is shown that the GAN is able to predict trends in\ncloud motion. In order to improve both, the typhoon center and cloud motion\nprediction, the present study suggests to add information about the sea surface\ntemperature, surface pressure and velocity fields to the input data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:19:57 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["R\u00fcttgers", "Mario", ""], ["Lee", "Sangseung", ""], ["You", "Donghyun", ""]]}, {"id": "1808.05387", "submitter": "Pierre Matysiak", "authors": "Pierre Matysiak, Mair\\'ead Grogan, Mika\\\"el Le Pendu, Martin Alain,\n  Aljosa Smolic", "title": "A Pipeline for Lenslet Light Field Quality Enhancement", "comments": "IEEE International Conference on Image Processing 2018, 5 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, light fields have become a major research topic and their\napplications span across the entire spectrum of classical image processing.\nAmong the different methods used to capture a light field are the lenslet\ncameras, such as those developed by Lytro. While these cameras give a lot of\nfreedom to the user, they also create light field views that suffer from a\nnumber of artefacts. As a result, it is common to ignore a significant subset\nof these views when doing high-level light field processing. We propose a\npipeline to process light field views, first with an enhanced processing of RAW\nimages to extract subaperture images, then a colour correction process using a\nrecent colour transfer algorithm, and finally a denoising process using a state\nof the art light field denoising approach. We show that our method improves the\nlight field quality on many levels, by reducing ghosting artefacts and noise,\nas well as retrieving more accurate and homogeneous colours across the\nsub-aperture images.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:30:11 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Matysiak", "Pierre", ""], ["Grogan", "Mair\u00e9ad", ""], ["Pendu", "Mika\u00ebl Le", ""], ["Alain", "Martin", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1808.05399", "submitter": "Yu Yang", "authors": "Yu Yanga, Xiao-Jun Wu, and Josef Kittler", "title": "Landmark Weighting for 3DMM Shape Fitting", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face is a 3D object with shape and surface texture. 3D Morphable Model\n(3DMM) is a powerful tool for reconstructing the 3D face from a single 2D face\nimage. In the shape fitting process, 3DMM estimates the correspondence between\n2D and 3D landmarks. Most traditional 3DMM fitting methods fail to reconstruct\nan accurate model because face shape fitting is a difficult non-linear\noptimization problem. In this paper we show that landmark weighting is\ninstrumental to improve the accuracy of shape reconstruction and propose a\nnovel 3D Morphable Model Fitting method. Different from previous works that\ntreat all landmarks equally, we take into consideration the estimated errors\nfor each pair of 2D and 3D corresponding landmarks. The landmark points are\nweighted in the optimization cost function based on these errors. Obviously,\nthese landmarks have different semantics because they locate on different\nfacial components. In the context of the solution of fitting is approximated,\nthere are deviations in landmarks matching. However, these landmarks with\ndifferent semantics have different effects on reconstructing 3D faces. Thus, it\nis necessary to consider each landmark individually. To our knowledge, we are\nthe first to analyze each feature point for 3D face reconstruction by 3DMM. The\nweight is adaptive with the estimation residuals of landmarks. Experimental\nresults show that the proposed method significantly reduces the reconstruction\nerror and improves the authenticity of the 3D model expression.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:59:03 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Yanga", "Yu", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1808.05469", "submitter": "Krishna Regmi", "authors": "Krishna Regmi, Ali Borji", "title": "Cross-view image synthesis using geometry-guided conditional GANs", "comments": "Under review as a journal paper at CVIU. arXiv admin note:\n  substantial text overlap with arXiv:1803.03396", "journal-ref": null, "doi": "10.1016/j.cviu.2019.07.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of generating images across two drastically different\nviews, namely ground (street) and aerial (overhead) views. Image synthesis by\nitself is a very challenging computer vision task and is even more so when\ngeneration is conditioned on an image in another view. Due the difference in\nviewpoints, there is small overlapping field of view and little common content\nbetween these two views. Here, we try to preserve the pixel information between\nthe views so that the generated image is a realistic representation of cross\nview input image. For this, we propose to use homography as a guide to map the\nimages between the views based on the common field of view to preserve the\ndetails in the input image. We then use generative adversarial networks to\ninpaint the missing regions in the transformed image and add realism to it. Our\nexhaustive evaluation and model comparison demonstrate that utilizing geometry\nconstraints adds fine details to the generated images and can be a better\napproach for cross view image synthesis than purely pixel based synthesis\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 21:24:26 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 05:34:36 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Regmi", "Krishna", ""], ["Borji", "Ali", ""]]}, {"id": "1808.05488", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional\n  Network Inference on Video Streams", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.04313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few years have brought advances in computer vision at an amazing\npace, grounded on new findings in deep neural network construction and training\nas well as the availability of large labeled datasets. Applying these networks\nto images demands a high computational effort and pushes the use of\nstate-of-the-art networks on real-time video data out of reach of embedded\nplatforms. Many recent works focus on reducing network complexity for real-time\ninference on embedded computing platforms. We adopt an orthogonal viewpoint and\npropose a novel algorithm exploiting the spatio-temporal sparsity of pixel\nchanges. This optimized inference procedure resulted in an average speed-up of\n9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of <0.1%\nand no retraining of the network for a semantic segmentation application.\nSimilarly, an average speed-up of 7.0x has been achieved for a pose detection\nDNN and a reduction of 5x of the number of arithmetic operations to be\nperformed for object detection on static camera video surveillance data. These\nthroughput gains combined with a lower power consumption result in an energy\nefficiency of 511 GOp/s/W compared to 70 GOp/s/W for the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:27:29 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 17:07:31 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1808.05492", "submitter": "Marc Masana Castrillo", "authors": "Marc Masana, Idoia Ruiz, Joan Serrat, Joost van de Weijer, Antonio M.\n  Lopez", "title": "Metric Learning for Novelty and Anomaly Detection", "comments": "Accepted at BMVC 2018, 10 pages main article and 4 pages\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When neural networks process images which do not resemble the distribution\nseen during training, so called out-of-distribution images, they often make\nwrong predictions, and do so too confidently. The capability to detect\nout-of-distribution images is therefore crucial for many real-world\napplications. We divide out-of-distribution detection between novelty detection\n---images of classes which are not in the training set but are related to\nthose---, and anomaly detection ---images with classes which are unrelated to\nthe training set. By related we mean they contain the same type of objects,\nlike digits in MNIST and SVHN. Most existing work has focused on anomaly\ndetection, and has addressed this problem considering networks trained with the\ncross-entropy loss. Differently from them, we propose to use metric learning\nwhich does not have the drawback of the softmax layer (inherent to\ncross-entropy methods), which forces the network to divide its prediction power\nover the learned classes. We perform extensive experiments and evaluate both\nnovelty and anomaly detection, even in a relevant application such as traffic\nsign recognition, obtaining comparable or better results than previous works.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 13:53:14 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Masana", "Marc", ""], ["Ruiz", "Idoia", ""], ["Serrat", "Joan", ""], ["van de Weijer", "Joost", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1808.05498", "submitter": "Ge Gao", "authors": "Ge Gao, Mikko Lauri, Jianwei Zhang and Simone Frintrop", "title": "Occlusion Resistant Object Rotation Regression from Point Cloud Segments", "comments": "Proceeding of the ECCV18 workshop on Recovering 6D Object Pose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rotation estimation of known rigid objects is important for robotic\napplications such as dexterous manipulation. Most existing methods for rotation\nestimation use intermediate representations such as templates, global or local\nfeature descriptors, or object coordinates, which require multiple steps in\norder to infer the object pose. We propose to directly regress a pose vector\nfrom raw point cloud segments using a convolutional neural network.\nExperimental results show that our method can potentially achieve competitive\nperformance compared to a state-of-the-art method, while also showing more\nrobustness against occlusion. Our method does not require any post processing\nsuch as refinement with the iterative closest point algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:03:58 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 14:57:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Gao", "Ge", ""], ["Lauri", "Mikko", ""], ["Zhang", "Jianwei", ""], ["Frintrop", "Simone", ""]]}, {"id": "1808.05499", "submitter": "Meng Zheng", "authors": "Meng Zheng, Srikrishna Karanam, Richard J. Radke", "title": "Measuring the Temporal Behavior of Real-World Person Re-Identification", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing real-world person re-identification (re-id) systems requires\nattention to operational aspects not typically considered in academic research.\nTypically, the probe image or image sequence is matched to a gallery set with a\nfixed candidate list. On the other hand, in real-world applications of re-id,\nwe would search for a person of interest in a gallery set that is continuously\npopulated by new candidates over time. A key question of interest for the\noperator of such a system is: how long is a correct match to a probe likely to\nremain in a rank-k shortlist of candidates? In this paper, we propose to\ndistill this information into what we call a Rank Persistence Curve (RPC),\nwhich unlike a conventional cumulative match characteristic (CMC) curve helps\ndirectly compare the temporal performance of different re-id algorithms. To\ncarefully illustrate the concept, we collected a new multi-shot person re-id\ndataset called RPIfield. The RPIfield dataset is constructed using a network of\n12 cameras with 112 explicitly time-stamped actor paths among about 4000\ndistractors. We then evaluate the temporal performance of different re-id\nalgorithms using the proposed RPCs using single and pairwise camera videos from\nRPIfield, and discuss considerations for future research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:07:03 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Radke", "Richard J.", ""]]}, {"id": "1808.05500", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M. Jorge Cardoso,\n  Marc Modat, Sebastien Ourselin, Lauge S{\\o}rensen", "title": "Robust training of recurrent neural networks to handle missing data for\n  disease progression modeling", "comments": "9 pages, 1 figure, MIDL conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease progression modeling (DPM) using longitudinal data is a challenging\ntask in machine learning for healthcare that can provide clinicians with better\ntools for diagnosis and monitoring of disease. Existing DPM algorithms neglect\ntemporal dependencies among measurements and make parametric assumptions about\nbiomarker trajectories. In addition, they do not model multiple biomarkers\njointly and need to align subjects' trajectories. In this paper, recurrent\nneural networks (RNNs) are utilized to address these issues. However, in many\ncases, longitudinal cohorts contain incomplete data, which hinders the\napplication of standard RNNs and requires a pre-processing step such as\nimputation of the missing values. We, therefore, propose a generalized training\nrule for the most widely used RNN architecture, long short-term memory (LSTM)\nnetworks, that can handle missing values in both target and predictor\nvariables. This algorithm is applied for modeling the progression of\nAlzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The\nresults show that the proposed LSTM algorithm achieves a lower mean absolute\nerror for prediction of measurements across all considered MRI biomarkers\ncompared to using standard LSTM networks with data imputation or using a\nregression-based DPM method. Moreover, applying linear discriminant analysis to\nthe biomarkers' values predicted by the proposed algorithm results in a larger\narea under the receiver operating characteristic curve (AUC) for clinical\ndiagnosis of AD compared to the same alternatives, and the AUC is comparable to\nstate-of-the-art AUCs from a recent cross-sectional medical image\nclassification challenge. This paper shows that built-in handling of missing\nvalues in LSTM network training paves the way for application of RNNs in\ndisease progression modeling.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:09:22 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Cardoso", "M. Jorge", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""], ["S\u00f8rensen", "Lauge", ""]]}, {"id": "1808.05508", "submitter": "Boyu Lu", "authors": "Boyu Lu, Jun-Cheng Chen, Carlos D. Castillo, and Rama Chellappa", "title": "An Experimental Evaluation of Covariates Effects on Unconstrained Face\n  Verification", "comments": null, "journal-ref": null, "doi": "10.1109/TBIOM.2018.2890577", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariates are factors that have a debilitating influence on face\nverification performance. In this paper, we comprehensively study two covariate\nrelated problems for unconstrained face verification: first, how covariates\naffect the performance of deep neural networks on the large-scale unconstrained\nface verification problem; second, how to utilize covariates to improve\nverification performance. To study the first problem, we implement five\nstate-of-the-art deep convolutional networks (DCNNs) for face verification and\nevaluate them on three challenging covariates datasets. In total, seven\ncovariates are considered: pose (yaw and roll), age, facial hair, gender,\nindoor/outdoor, occlusion (nose and mouth visibility, eyes visibility, and\nforehead visibility), and skin tone. These covariates cover both intrinsic\nsubject-specific characteristics and extrinsic factors of faces. Some of the\nresults confirm and extend the findings of previous studies, others are new\nfindings that were rarely mentioned previously or did not show consistent\ntrends. For the second problem, we demonstrate that with the assistance of\ngender information, the quality of a pre-curated noisy large-scale face dataset\nfor face recognition can be further improved. After retraining the face\nrecognition model using the curated data, performance improvement is observed\nat low False Acceptance Rates (FARs) (FAR=$10^{-5}$, $10^{-6}$, $10^{-7}$).\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:24:37 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Lu", "Boyu", ""], ["Chen", "Jun-Cheng", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1808.05517", "submitter": "Jianbo Guo", "authors": "Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong Chen, Jianguo Li", "title": "Network Decoupling: From Regular to Depthwise Separable Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise separable convolution has shown great efficiency in network design,\nbut requires time-consuming training procedure with full training-set\navailable. This paper first analyzes the mathematical relationship between\nregular convolutions and depthwise separable convolutions, and proves that the\nformer one could be approximated with the latter one in closed form. We show\ndepthwise separable convolutions are principal components of regular\nconvolutions. And then we propose network decoupling (ND), a training-free\nmethod to accelerate convolutional neural networks (CNNs) by transferring\npre-trained CNN models into the MobileNet-like depthwise separable convolution\nstructure, with a promising speedup yet negligible accuracy loss. We further\nverify through experiments that the proposed method is orthogonal to other\ntraining-free methods like channel decomposition, spatial decomposition, etc.\nCombining the proposed method with them will bring even larger CNN speedup. For\ninstance, ND itself achieves about 2X speedup for the widely used VGG16, and\ncombined with other methods, it reaches 3.7X speedup with graceful accuracy\ndegradation. We demonstrate that ND is widely applicable to classification\nnetworks like ResNet, and object detection network like SSD300.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:39:10 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Guo", "Jianbo", ""], ["Li", "Yuxi", ""], ["Lin", "Weiyao", ""], ["Chen", "Yurong", ""], ["Li", "Jianguo", ""]]}, {"id": "1808.05560", "submitter": "Qingpeng Li", "authors": "Qingpeng Li and Lichao Mou and Qizhi Xu and Yun Zhang and Xiao Xiang\n  Zhu", "title": "R$^3$-Net: A Deep Network for Multi-oriented Vehicle Detection in Aerial\n  Images and Videos", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2895362", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection is a significant and challenging task in aerial remote\nsensing applications. Most existing methods detect vehicles with regular\nrectangle boxes and fail to offer the orientation of vehicles. However, the\norientation information is crucial for several practical applications, such as\nthe trajectory and motion estimation of vehicles. In this paper, we propose a\nnovel deep network, called rotatable region-based residual network (R$^3$-Net),\nto detect multi-oriented vehicles in aerial images and videos. More specially,\nR$^3$-Net is utilized to generate rotatable rectangular target boxes in a half\ncoordinate system. First, we use a rotatable region proposal network (R-RPN) to\ngenerate rotatable region of interests (R-RoIs) from feature maps produced by a\ndeep convolutional neural network. Here, a proposed batch averaging rotatable\nanchor (BAR anchor) strategy is applied to initialize the shape of vehicle\ncandidates. Next, we propose a rotatable detection network (R-DN) for the final\nclassification and regression of the R-RoIs. In R-DN, a novel rotatable\nposition sensitive pooling (R-PS pooling) is designed to keep the position and\norientation information simultaneously while downsampling the feature maps of\nR-RoIs. In our model, R-RPN and R-DN can be trained jointly. We test our\nnetwork on two open vehicle detection image datasets, namely DLR 3K Munich\nDataset and VEDAI Dataset, demonstrating the high precision and robustness of\nour method. In addition, further experiments on aerial videos show the good\ngeneralization capability of the proposed method and its potential for vehicle\ntracking in aerial videos. The demo video is available at\nhttps://youtu.be/xCYD-tYudN0.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:04:27 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Li", "Qingpeng", ""], ["Mou", "Lichao", ""], ["Xu", "Qizhi", ""], ["Zhang", "Yun", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1808.05561", "submitter": "Samuel Albanie", "authors": "Samuel Albanie, Arsha Nagrani, Andrea Vedaldi, Andrew Zisserman", "title": "Emotion Recognition in Speech using Cross-Modal Transfer in the Wild", "comments": "Conference paper at ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining large, human labelled speech datasets to train models for emotion\nrecognition is a notoriously challenging task, hindered by annotation cost and\nlabel ambiguity. In this work, we consider the task of learning embeddings for\nspeech classification without access to any form of labelled audio. We base our\napproach on a simple hypothesis: that the emotional content of speech\ncorrelates with the facial expression of the speaker. By exploiting this\nrelationship, we show that annotations of expression can be transferred from\nthe visual domain (faces) to the speech domain (voices) through cross-modal\ndistillation. We make the following contributions: (i) we develop a strong\nteacher network for facial emotion recognition that achieves the state of the\nart on a standard benchmark; (ii) we use the teacher to train a student, tabula\nrasa, to learn representations (embeddings) for speech emotion recognition\nwithout access to labelled audio data; and (iii) we show that the speech\nemotion embedding can be used for speech emotion recognition on external\nbenchmark datasets. Code, models and data are available.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:10:23 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Albanie", "Samuel", ""], ["Nagrani", "Arsha", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1808.05577", "submitter": "Stefano B. Blumberg", "authors": "Stefano B. Blumberg, Ryutaro Tanno, Iasonas Kokkinos, Daniel C.\n  Alexander", "title": "Deeper Image Quality Transfer: Training Low-Memory Neural Networks for\n  3D Images", "comments": "Accepted in: MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the memory demands that come with the processing of\n3-dimensional, high-resolution, multi-channeled medical images in deep\nlearning. We exploit memory-efficient backpropagation techniques, to reduce the\nmemory complexity of network training from being linear in the network's depth,\nto being roughly constant $ - $ permitting us to elongate deep architectures\nwith negligible memory increase. We evaluate our methodology in the paradigm of\nImage Quality Transfer, whilst noting its potential application to various\ntasks that use deep learning. We study the impact of depth on accuracy and show\nthat deeper models have more predictive power, which may exploit larger\ntraining sets. We obtain substantially better results than the previous\nstate-of-the-art model with a slight memory increase, reducing the\nroot-mean-squared-error by $ 13\\% $. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:42:10 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Blumberg", "Stefano B.", ""], ["Tanno", "Ryutaro", ""], ["Kokkinos", "Iasonas", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1808.05584", "submitter": "Zhao Zhong", "authors": "Zhao Zhong, Zichen Yang, Boyang Deng, Junjie Yan, Wei Wu, Jing Shao,\n  Cheng-Lin Liu", "title": "BlockQNN: Efficient Block-wise Neural Network Architecture Generation", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have gained a remarkable success in computer\nvision. However, most usable network architectures are hand-crafted and usually\nrequire expertise and elaborate design. In this paper, we provide a block-wise\nnetwork generation pipeline called BlockQNN which automatically builds\nhigh-performance networks using the Q-Learning paradigm with epsilon-greedy\nexploration strategy. The optimal network block is constructed by the learning\nagent which is trained to choose component layers sequentially. We stack the\nblock to construct the whole auto-generated network. To accelerate the\ngeneration process, we also propose a distributed asynchronous framework and an\nearly stop strategy. The block-wise generation brings unique advantages: (1) it\nyields state-of-the-art results in comparison to the hand-crafted networks on\nimage classification, particularly, the best network generated by BlockQNN\nachieves 2.35% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction\nof the search space in designing networks, spending only 3 days with 32 GPUs. A\nfaster version can yield a comparable result with only 1 GPU in 20 hours. (3)\nit has strong generalizability in that the network built on CIFAR also performs\nwell on the larger-scale dataset. The best network achieves very competitive\naccuracy of 82.0% top-1 and 96.0% top-5 on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 17:02:24 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zhong", "Zhao", ""], ["Yang", "Zichen", ""], ["Deng", "Boyang", ""], ["Yan", "Junjie", ""], ["Wu", "Wei", ""], ["Shao", "Jing", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1808.05727", "submitter": "Viral Thakar", "authors": "Viral Thakar, Walid Ahmed, Mohammad M Soltani, Jia Yuan Yu", "title": "Ensemble-based Adaptive Single-shot Multi-box Detector", "comments": "6 pages, 2 figures, to appear in the Proceedings of the ISNCC 2018,\n  19-21 June 2018, Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two improvements to the SSD---single shot multibox detector.\nFirst, we propose an adaptive approach for default box selection in SSD. This\nuses data to reduce the uncertainty in the selection of best aspect ratios for\nthe default boxes and improves performance of SSD for datasets containing small\nand complex objects (e.g., equipments at construction sites). We do so by\nfinding the distribution of aspect ratios of the given training dataset, and\nthen choosing representative values. Secondly, we propose an ensemble\nalgorithm, using SSD as components, which improves the performance of SSD,\nespecially for small amount of training datasets. Compared to the conventional\nSSD algorithm, adaptive box selection improves mean average precision by 3%,\nwhile ensemble-based SSD improves it by 8%.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:02:42 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Thakar", "Viral", ""], ["Ahmed", "Walid", ""], ["Soltani", "Mohammad M", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1808.05730", "submitter": "Viral Thakar", "authors": "Viral Thakar, Himani Saini, Walid Ahmed, Mohammad M Soltani, Ahmed\n  Aly, Jia Yuan Yu", "title": "Efficient Single-Shot Multibox Detector for Construction Site Monitoring", "comments": "6 pages, 4 figures, to appear in the Proceedings of the ISC2 2018,\n  16-19 September 2018, Kansas, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asset monitoring in construction sites is an intricate, manually intensive\ntask, that can highly benefit from automated solutions engineered using deep\nneural networks. We use Single-Shot Multibox Detector --- SSD, for its fine\nbalance between speed and accuracy, to leverage ubiquitously available images\nand videos from the surveillance cameras on the construction sites and automate\nthe monitoring tasks, hence enabling project managers to better track the\nperformance and optimize the utilization of each resource. We propose to\nimprove the performance of SSD by clustering the predicted boxes instead of a\ngreedy approach like non-maximum suppression. We do so using Affinity\nPropagation Clustering --- APC to cluster the predicted boxes based on the\nsimilarity index computed using the spatial features as well as location of\npredicted boxes. In our attempts, we have been able to improve the mean average\nprecision of SSD by 3.77% on custom dataset consist of images from construction\nsites and by 1.67% on PASCAL VOC Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:19:31 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 00:23:29 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Thakar", "Viral", ""], ["Saini", "Himani", ""], ["Ahmed", "Walid", ""], ["Soltani", "Mohammad M", ""], ["Aly", "Ahmed", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1808.05732", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Katherine L. Bouman, William T. Freeman, Natalia S.\n  Rost, Mert R. Sabuncu, Polina Golland", "title": "Medical Image Imputation from Image Collections", "comments": "Accepted at IEEE Transactions on Medical Imaging (\\c{opyright} 2018\n  IEEE)", "journal-ref": null, "doi": "10.1109/TMI.2018.2866692", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for creating high resolution anatomically plausible\nimages consistent with acquired clinical brain MRI scans with large inter-slice\nspacing. Although large data sets of clinical images contain a wealth of\ninformation, time constraints during acquisition result in sparse scans that\nfail to capture much of the anatomy. These characteristics often render\ncomputational analysis impractical as many image analysis algorithms tend to\nfail when applied to such images. Highly specialized algorithms that explicitly\nhandle sparse slice spacing do not generalize well across problem domains. In\ncontrast, we aim to enable application of existing algorithms that were\noriginally developed for high resolution research scans to significantly\nundersampled scans. We introduce a generative model that captures fine-scale\nanatomical structure across subjects in clinical image collections and derive\nan algorithm for filling in the missing data in scans with large inter-slice\nspacing. Our experimental results demonstrate that the resulting method\noutperforms state-of-the-art upsampling super-resolution techniques, and\npromises to facilitate subsequent analysis not previously possible with scans\nof this quality. Our implementation is freely available at\nhttps://github.com/adalca/papago .\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:46:33 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Bouman", "Katherine L.", ""], ["Freeman", "William T.", ""], ["Rost", "Natalia S.", ""], ["Sabuncu", "Mert R.", ""], ["Golland", "Polina", ""]]}, {"id": "1808.05734", "submitter": "Wenxue Cui", "authors": "Wenxue Cui, Tao Zhang, Shengping Zhang, Feng Jiang, Wangmeng Zuo,\n  Debin Zhao", "title": "Convolutional Neural Networks based Intra Prediction for HEVC", "comments": "10 pages, This is the extended edition of poster paper accepted by\n  DCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional intra prediction methods for HEVC rely on using the nearest\nreference lines for predicting a block, which ignore much richer context\nbetween the current block and its neighboring blocks and therefore cause\ninaccurate prediction especially when weak spatial correlation exists between\nthe current block and the reference lines. To overcome this problem, in this\npaper, an intra prediction convolutional neural network (IPCNN) is proposed for\nintra prediction, which exploits the rich context of the current block and\ntherefore is capable of improving the accuracy of predicting the current block.\nMeanwhile, the predictions of the three nearest blocks can also be refined. To\nthe best of our knowledge, this is the first paper that directly applies CNNs\nto intra prediction for HEVC. Experimental results validate the effectiveness\nof applying CNNs to intra prediction and achieved significant performance\nimprovement compared to traditional intra prediction methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:50:49 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Cui", "Wenxue", ""], ["Zhang", "Tao", ""], ["Zhang", "Shengping", ""], ["Jiang", "Feng", ""], ["Zuo", "Wangmeng", ""], ["Zhao", "Debin", ""]]}, {"id": "1808.05744", "submitter": "Mingchen Gao", "authors": "Yan Shen, Mingchen Gao", "title": "Dynamic Routing on Deep Neural Network for Thoracic Disease\n  Classification and Sensitive Area Localization", "comments": null, "journal-ref": "MLMI 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate a new deep neural network architecture for automatic\nthoracic disease detection on chest X-rays. Deep neural networks have shown\ngreat success in a plethora of visual recognition tasks such as image\nclassification and object detection by stacking multiple layers of\nconvolutional neural networks (CNN) in a feed-forward manner. However, the\nperformance gain by going deeper has reached bottlenecks as a result of the\ntrade-off between model complexity and discrimination power. We address this\nproblem by utilizing the recently developed routing-by agreement mechanism in\nour architecture. A novel characteristic of our network structure is that it\nextends routing to two types of layer connections (1) connection between\nfeature maps in dense layers, (2) connection between primary capsules and\nprediction capsules in final classification layer. We show that our networks\nachieve comparable results with much fewer layers in the measurement of AUC\nscore. We further show the combined benefits of model interpretability by\ngenerating Gradient-weighted Class Activation Mapping (Grad-CAM) for\nlocalization. We demonstrate our results on the NIH chestX-ray14 dataset that\nconsists of 112,120 images on 30,805 unique patients including 14 kinds of lung\ndiseases.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 04:00:25 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Shen", "Yan", ""], ["Gao", "Mingchen", ""]]}, {"id": "1808.05754", "submitter": "C. H. Huck Yang", "authors": "C.-H. Huck Yang, Fangyu Liu, Jia-Hong Huang, Meng Tian, Hiromasa\n  Morikawa, I-Hung Lin, Yi-Chieh Liu, Hao-Hsiang Yang, Jesper Tegner", "title": "Auto-Classification of Retinal Diseases in the Limit of Sparse Data\n  Using a Two-Streams Machine Learning Model", "comments": "A extension work of a workshop paper arXiv admin note: substantial\n  text overlap with arXiv:1806.06423", "journal-ref": "Asian Conference on Computer Vision (ACCV), Artificial\n  Intelligence for Retinal Image Analysis Workshop, December 2-6, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Automatic clinical diagnosis of retinal diseases has emerged as a promising\napproach to facilitate discovery in areas with limited access to specialists.\nBased on the fact that fundus structure and vascular disorders are the main\ncharacteristics of retinal diseases, we propose a novel visual-assisted\ndiagnosis hybrid model mixing the support vector machine (SVM) and deep neural\nnetworks (DNNs). Furthermore, we present a new clinical retina dataset, called\nEyeNet2, for ophthalmology incorporating 52 retina diseases classes. Using\nEyeNet2, our model achieves 90.43\\% diagnosis accuracy, and the model\nperformance is comparable to the professional ophthalmologists.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 12:53:53 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 12:44:11 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 12:41:39 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 21:42:29 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Liu", "Fangyu", ""], ["Huang", "Jia-Hong", ""], ["Tian", "Meng", ""], ["Morikawa", "Hiromasa", ""], ["Lin", "I-Hung", ""], ["Liu", "Yi-Chieh", ""], ["Yang", "Hao-Hsiang", ""], ["Tegner", "Jesper", ""]]}, {"id": "1808.05756", "submitter": "Xiaoliang Wang", "authors": "Xiaoliang Wang, Peng Cheng, Xinchuan Liu, Benedict Uzochukwu", "title": "Fast and Accurate, Convolutional Neural Network Based Approach for\n  Object Detection from UAV", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.01114", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs), have intrigued different people from all\nwalks of life, because of their pervasive computing capabilities. UAV equipped\nwith vision techniques, could be leveraged to establish navigation autonomous\ncontrol for UAV itself. Also, object detection from UAV could be used to\nbroaden the utilization of drone to provide ubiquitous surveillance and\nmonitoring services towards military operation, urban administration and\nagriculture management. As the data-driven technologies evolved, machine\nlearning algorithm, especially the deep learning approach has been intensively\nutilized to solve different traditional computer vision research problems.\nModern Convolutional Neural Networks based object detectors could be divided\ninto two major categories: one-stage object detector and two-stage object\ndetector. In this study, we utilize some representative CNN based object\ndetectors to execute the computer vision task over Stanford Drone Dataset\n(SDD). State-of-the-art performance has been achieved in utilizing focal loss\ndense detector RetinaNet based approach for object detection from UAV in a fast\nand accurate manner.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 13:22:00 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 14:07:31 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Wang", "Xiaoliang", ""], ["Cheng", "Peng", ""], ["Liu", "Xinchuan", ""], ["Uzochukwu", "Benedict", ""]]}, {"id": "1808.05779", "submitter": "Sangil Jung", "authors": "Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Youngjun Kwak,\n  Jae-Joon Han, Sung Ju Hwang, Changkyu Choi", "title": "Learning to Quantize Deep Networks by Optimizing Quantization Intervals\n  with Task Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing bit-widths of activations and weights of deep networks makes it\nefficient to compute and store them in memory, which is crucial in their\ndeployments to resource-limited devices, such as mobile phones. However,\ndecreasing bit-widths with quantization generally yields drastically degraded\naccuracy. To tackle this problem, we propose to learn to quantize activations\nand weights via a trainable quantizer that transforms and discretizes them.\nSpecifically, we parameterize the quantization intervals and obtain their\noptimal values by directly minimizing the task loss of the network. This\nquantization-interval-learning (QIL) allows the quantized networks to maintain\nthe accuracy of the full-precision (32-bit) networks with bit-width as low as\n4-bit and minimize the accuracy degeneration with further bit-width reduction\n(i.e., 3 and 2-bit). Moreover, our quantizer can be trained on a heterogeneous\ndataset, and thus can be used to quantize pretrained networks without access to\ntheir training data. We demonstrate the effectiveness of our trainable\nquantizer on ImageNet dataset with various network architectures such as\nResNet-18, -34 and AlexNet, on which it outperforms existing methods to achieve\nthe state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 07:28:17 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 07:05:34 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 02:18:04 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Jung", "Sangil", ""], ["Son", "Changyong", ""], ["Lee", "Seohyung", ""], ["Son", "Jinwoo", ""], ["Kwak", "Youngjun", ""], ["Han", "Jae-Joon", ""], ["Hwang", "Sung Ju", ""], ["Choi", "Changkyu", ""]]}, {"id": "1808.05805", "submitter": "Mahdi Hamad", "authors": "Mingchuan Zhou, Mahdi Hamad, Jakob Weiss, Abouzar Eslami, Kai Huang,\n  Mathias Maier, Chris P. Lohmann, Nassir Navab, Alois Knoll, M. Ali Nasseri", "title": "Towards Robotic Eye Surgery: Marker-free, Online Hand-eye Calibration\n  using Optical Coherence Tomography Images", "comments": "*The first two authors contributed equally to this paper. Accepted by\n  IEEE Robotics and Automation Letters (RA-L), 2018", "journal-ref": null, "doi": "10.1109/LRA.2018.2858744", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ophthalmic microsurgery is known to be a challenging operation, which\nrequires very precise and dexterous manipulation. Image guided robot-assisted\nsurgery (RAS) is a promising solution that brings significant improvements in\noutcomes and reduces the physical limitations of human surgeons. However, this\ntechnology must be further developed before it can be routinely used in\nclinics. One of the problems is the lack of proper calibration between the\nrobotic manipulator and appropriate imaging device. In this work, we developed\na flexible framework for hand-eye calibration of an ophthalmic robot with a\nmicroscope-integrated Optical Coherence Tomography (MIOCT) without any markers.\nThe proposed method consists of three main steps: a) we estimate the OCT\ncalibration parameters; b) with micro-scale displacements controlled by the\nrobot, we detect and segment the needle tip in 3D-OCT volume; c) we find the\ntransformation between the coordinate system of the OCT camera and the\ncoordinate system of the robot. We verified the capability of our framework in\nex-vivo pig eye experiments and compared the results with a reference method\n(marker-based). In all experiments, our method showed a small difference from\nthe marker based method, with a mean calibration error of 9.2 $\\mu$m and 7.0\n$\\mu$m, respectively. Additionally, the noise test shows the robustness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 09:19:40 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Zhou", "Mingchuan", ""], ["Hamad", "Mahdi", ""], ["Weiss", "Jakob", ""], ["Eslami", "Abouzar", ""], ["Huang", "Kai", ""], ["Maier", "Mathias", ""], ["Lohmann", "Chris P.", ""], ["Navab", "Nassir", ""], ["Knoll", "Alois", ""], ["Nasseri", "M. Ali", ""]]}, {"id": "1808.05819", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Vladan Radosavljevic, Henggang Cui, Thi Nguyen,\n  Fang-Chieh Chou, Tsung-Han Lin, Nitin Singh, Jeff Schneider", "title": "Uncertainty-aware Short-term Motion Prediction of Traffic Actors for\n  Autonomous Driving", "comments": "Accepted for publication at IEEE Winter Conference on Applications of\n  Computer Vision (WACV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address one of the crucial aspects necessary for safe and efficient\noperations of autonomous vehicles, namely predicting future state of traffic\nactors in the autonomous vehicle's surroundings. We introduce a deep\nlearning-based approach that takes into account a current world state and\nproduces raster images of each actor's vicinity. The rasters are then used as\ninputs to deep convolutional models to infer future movement of actors while\nalso accounting for and capturing inherent uncertainty of the prediction task.\nExtensive experiments on real-world data strongly suggest benefits of the\nproposed approach. Moreover, following completion of the offline tests the\nsystem was successfully tested onboard self-driving vehicles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 10:37:51 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 14:20:22 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 06:35:56 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Djuric", "Nemanja", ""], ["Radosavljevic", "Vladan", ""], ["Cui", "Henggang", ""], ["Nguyen", "Thi", ""], ["Chou", "Fang-Chieh", ""], ["Lin", "Tsung-Han", ""], ["Singh", "Nitin", ""], ["Schneider", "Jeff", ""]]}, {"id": "1808.05848", "submitter": "Junsheng Fu", "authors": "Junsheng Fu, Said Pertuz, Jiri Matas, Joni-Kristian K\\\"am\\\"ar\\\"ainen", "title": "Performance Analysis and Robustification of Single-query 6-DoF Camera\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single-query 6-DoF camera pose estimation with reference images\nand a point cloud, i.e. the problem of estimating the position and orientation\nof a camera by using reference images and a point cloud. In this work, we\nperform a systematic comparison of three state-of-the-art strategies for 6-DoF\ncamera pose estimation, i.e. feature-based, photometric-based and\nmutual-information-based approaches. The performance of the studied methods is\nevaluated on two standard datasets in terms of success rate, translation error\nand max orientation error. Building on the results analysis, we propose a\nhybrid approach that combines feature-based and mutual-information-based pose\nestimation methods since it provides complementary properties for pose\nestimation. Experiments show that (1) in cases with large environmental\nvariance, the hybrid approach outperforms feature-based and\nmutual-information-based approaches by an average of 25.1% and 5.8% in terms of\nsuccess rate, respectively; (2) in cases where query and reference images are\ncaptured at similar imaging conditions, the hybrid approach performs similarly\nas the feature-based approach, but outperforms both photometric-based and\nmutual-information-based approaches with a clear margin; (3) the feature-based\napproach is consistently more accurate than mutual-information-based and\nphotometric-based approaches when at least 4 consistent matching points are\nfound between the query and reference images.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 13:21:24 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Fu", "Junsheng", ""], ["Pertuz", "Said", ""], ["Matas", "Jiri", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""]]}, {"id": "1808.05853", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Transfer Learning Enhanced Common Spatial Pattern Filtering for Brain\n  Computer Interfaces (BCIs): Overview and a New Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalogram (EEG) is the most widely used input for brain\ncomputer interfaces (BCIs), and common spatial pattern (CSP) is frequently used\nto spatially filter it to increase its signal-to-noise ratio. However, CSP is a\nsupervised filter, which needs some subject-specific calibration data to\ndesign. This is time-consuming and not user-friendly. A promising approach for\nshortening or even completely eliminating this calibration session is transfer\nlearning, which leverages relevant data or knowledge from other subjects or\ntasks. This paper reviews three existing approaches for incorporating transfer\nlearning into CSP, and also proposes a new transfer learning enhanced CSP\napproach. Experiments on motor imagery classification demonstrate their\neffectiveness. Particularly, our proposed approach achieves the best\nperformance when the number of target domain calibration samples is small.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:52:12 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.05864", "submitter": "Daqing Liu", "authors": "Daqing Liu, Zheng-Jun Zha, Hanwang Zhang, Yongdong Zhang, Feng Wu", "title": "Context-Aware Visual Policy Network for Sequence-Level Image Captioning", "comments": "9 pages, 6 figures, ACM MM 2018 oral", "journal-ref": null, "doi": "10.1145/3240508.3240632", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many vision-language tasks can be reduced to the problem of sequence\nprediction for natural language output. In particular, recent advances in image\ncaptioning use deep reinforcement learning (RL) to alleviate the \"exposure\nbias\" during training: ground-truth subsequence is exposed in every step\nprediction, which introduces bias in test when only predicted subsequence is\nseen. However, existing RL-based image captioning methods only focus on the\nlanguage policy while not the visual policy (e.g., visual attention), and thus\nfail to capture the visual context that are crucial for compositional reasoning\nsuch as visual relationships (e.g., \"man riding horse\") and comparisons (e.g.,\n\"smaller cat\"). To fill the gap, we propose a Context-Aware Visual Policy\nnetwork (CAVP) for sequence-level image captioning. At every time step, CAVP\nexplicitly accounts for the previous visual attentions as the context, and then\ndecides whether the context is helpful for the current word generation given\nthe current visual attention. Compared against traditional visual attention\nthat only fixes a single image region at every step, CAVP can attend to complex\nvisual compositions over time. The whole image captioning model --- CAVP and\nits subsequent language policy network --- can be efficiently optimized\nend-to-end by using an actor-critic policy gradient method with respect to any\ncaption evaluation metric. We demonstrate the effectiveness of CAVP by\nstate-of-the-art performances on MS-COCO offline split and online server, using\nvarious metrics and sensible visualizations of qualitative visual context. The\ncode is available at https://github.com/daqingliu/CAVP\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 11:45:45 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 20:54:31 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2018 13:32:28 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Liu", "Daqing", ""], ["Zha", "Zheng-Jun", ""], ["Zhang", "Hanwang", ""], ["Zhang", "Yongdong", ""], ["Wu", "Feng", ""]]}, {"id": "1808.05870", "submitter": "Maxime Soler", "authors": "Maxime Soler, M\\'elanie Plainchault, Bruno Conche, Julien Tierny", "title": "Lifted Wasserstein Matcher for Fast and Robust Topology Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust and efficient method for tracking topological\nfeatures in time-varying scalar data. Structures are tracked based on the\noptimal matching between persistence diagrams with respect to the Wasserstein\nmetric. This fundamentally relies on solving the assignment problem, a special\ncase of optimal transport, for all consecutive timesteps. Our approach relies\non two main contributions. First, we revisit the seminal assignment algorithm\nby Kuhn and Munkres which we specifically adapt to the problem of matching\npersistence diagrams in an efficient way. Second, we propose an extension of\nthe Wasserstein metric that significantly improves the geometrical stability of\nthe matching of domain-embedded persistence pairs. We show that this\ngeometrical lifting has the additional positive side-effect of improving the\nassignment matrix sparsity and therefore computing time. The global framework\nimplements a coarse-grained parallelism by computing persistence diagrams and\nfinding optimal matchings in parallel for every couple of consecutive\ntimesteps. Critical trajectories are constructed by associating successively\nmatched persistence pairs over time. Merging and splitting events are detected\nwith a geometrical threshold in a post-processing stage. Extensive experiments\non real-life datasets show that our matching approach is an order of magnitude\nfaster than the seminal Munkres algorithm. Moreover, compared to a modern\napproximation method, our method provides competitive runtimes while yielding\nexact results. We demonstrate the utility of our global framework by extracting\ncritical point trajectories from various simulated time-varying datasets and\ncompare it to the existing methods based on associated overlaps of volumes.\nRobustness to noise and temporal resolution downsampling is empirically\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 13:54:05 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 11:17:11 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 09:23:15 GMT"}, {"version": "v4", "created": "Wed, 2 Jan 2019 08:02:12 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Soler", "Maxime", ""], ["Plainchault", "M\u00e9lanie", ""], ["Conche", "Bruno", ""], ["Tierny", "Julien", ""]]}, {"id": "1808.05883", "submitter": "Wouter Bulten", "authors": "Wouter Bulten, P\\'eter B\\'andi, Jeffrey Hoven, Rob van de Loo,\n  Johannes Lotz, Nick Weiss, Jeroen van der Laak, Bram van Ginneken, Christina\n  Hulsbergen-van de Kaa, Geert Litjens", "title": "Epithelium segmentation using deep learning in H&E-stained prostate\n  specimens with immunohistochemistry as reference standard", "comments": null, "journal-ref": "Nature Scientific Reports 9, 864 (2019)", "doi": "10.1038/s41598-018-37257-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer (PCa) is graded by pathologists by examining the\narchitectural pattern of cancerous epithelial tissue on hematoxylin and eosin\n(H&E) stained slides. Given the importance of gland morphology, automatically\ndifferentiating between glandular epithelial tissue and other tissues is an\nimportant prerequisite for the development of automated methods for detecting\nPCa. We propose a new method, using deep learning, for automatically segmenting\nepithelial tissue in digitized prostatectomy slides. We employed\nimmunohistochemistry (IHC) to render the ground truth less subjective and more\nprecise compared to manual outlining on H&E slides, especially in areas with\nhigh-grade and poorly differentiated PCa. Our dataset consisted of 102 tissue\nblocks, including both low and high grade PCa. From each block a single new\nsection was cut, stained with H&E, scanned, restained using P63 and CK8/18 to\nhighlight the epithelial structure, and scanned again. The H&E slides were\nco-registered to the IHC slides. On a subset of the IHC slides we applied color\ndeconvolution, corrected stain errors manually, and trained a U-Net to perform\nsegmentation of epithelial structures. Whole-slide segmentation masks generated\nby the IHC U-Net were used to train a second U-Net on H&E. Our system makes\nprecise cell-level segmentations and segments both intact glands as well as\nindividual (tumor) epithelial cells. We achieved an F1-score of 0.895 on a\nhold-out test set and 0.827 on an external reference set from a different\ncenter. We envision this segmentation as being the first part of a fully\nautomated prostate cancer detection and grading pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:36:43 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 13:28:49 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Bulten", "Wouter", ""], ["B\u00e1ndi", "P\u00e9ter", ""], ["Hoven", "Jeffrey", ""], ["van de Loo", "Rob", ""], ["Lotz", "Johannes", ""], ["Weiss", "Nick", ""], ["van der Laak", "Jeroen", ""], ["van Ginneken", "Bram", ""], ["de Kaa", "Christina Hulsbergen-van", ""], ["Litjens", "Geert", ""]]}, {"id": "1808.05896", "submitter": "David Tellez", "authors": "David Tellez, Maschenka Balkenhol, Irene Otte-Holler, Rob van de Loo,\n  Rob Vogels, Peter Bult, Carla Wauters, Willem Vreuls, Suzanne Mol, Nico\n  Karssemeijer, Geert Litjens, Jeroen van der Laak, Francesco Ciompi", "title": "Whole-Slide Mitosis Detection in H&E Breast Histology Using PHH3 as a\n  Reference to Train Distilled Stain-Invariant Convolutional Networks", "comments": "Accepted to appear in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2820199", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual counting of mitotic tumor cells in tissue sections constitutes one of\nthe strongest prognostic markers for breast cancer. This procedure, however, is\ntime-consuming and error-prone. We developed a method to automatically detect\nmitotic figures in breast cancer tissue sections based on convolutional neural\nnetworks (CNNs). Application of CNNs to hematoxylin and eosin (H&E) stained\nhistological tissue sections is hampered by: (1) noisy and expensive reference\nstandards established by pathologists, (2) lack of generalization due to\nstaining variation across laboratories, and (3) high computational requirements\nneeded to process gigapixel whole-slide images (WSIs). In this paper, we\npresent a method to train and evaluate CNNs to specifically solve these issues\nin the context of mitosis detection in breast cancer WSIs. First, by combining\nimage analysis of mitotic activity in phosphohistone-H3 (PHH3) restained slides\nand registration, we built a reference standard for mitosis detection in entire\nH&E WSIs requiring minimal manual annotation effort. Second, we designed a data\naugmentation strategy that creates diverse and realistic H&E stain variations\nby modifying the hematoxylin and eosin color channels directly. Using it during\ntraining combined with network ensembling resulted in a stain invariant mitosis\ndetector. Third, we applied knowledge distillation to reduce the computational\nrequirements of the mitosis detection ensemble with a negligible loss of\nperformance. The system was trained in a single-center cohort and evaluated in\nan independent multicenter cohort from The Cancer Genome Atlas on the three\ntasks of the Tumor Proliferation Assessment Challenge (TUPAC). We obtained a\nperformance within the top-3 best methods for most of the tasks of the\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 15:14:20 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Tellez", "David", ""], ["Balkenhol", "Maschenka", ""], ["Otte-Holler", "Irene", ""], ["van de Loo", "Rob", ""], ["Vogels", "Rob", ""], ["Bult", "Peter", ""], ["Wauters", "Carla", ""], ["Vreuls", "Willem", ""], ["Mol", "Suzanne", ""], ["Karssemeijer", "Nico", ""], ["Litjens", "Geert", ""], ["van der Laak", "Jeroen", ""], ["Ciompi", "Francesco", ""]]}, {"id": "1808.05902", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues, Mariana Louren\\c{c}o, Bernardete Ribeiro, Francisco\n  Pereira", "title": "Learning Supervised Topic Models for Classification and Regression from\n  Crowds", "comments": "14 pages", "journal-ref": "Rodrigues, F., Lourenco, M., Ribeiro, B. and Pereira, F.C., 2017.\n  Learning supervised topic models for classification and regression from\n  crowds. IEEE transactions on pattern analysis and machine intelligence,\n  39(12), pp.2409-2422", "doi": "10.1109/TPAMI.2017.2648786", "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need to analyze large collections of documents has led to great\ndevelopments in topic modeling. Since documents are frequently associated with\nother related variables, such as labels or ratings, much interest has been\nplaced on supervised topic models. However, the nature of most annotation\ntasks, prone to ambiguity and noise, often with high volumes of documents, deem\nlearning under a single-annotator assumption unrealistic or unpractical for\nmost real-world applications. In this article, we propose two supervised topic\nmodels, one for classification and another for regression problems, which\naccount for the heterogeneity and biases among different annotators that are\nencountered in practice when learning from crowds. We develop an efficient\nstochastic variational inference algorithm that is able to scale to very large\ndatasets, and we empirically demonstrate the advantages of the proposed model\nover state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 15:32:24 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Louren\u00e7o", "Mariana", ""], ["Ribeiro", "Bernardete", ""], ["Pereira", "Francisco", ""]]}, {"id": "1808.05941", "submitter": "Nitin Khanna Dr.", "authors": "Sharad Joshi, Suraj Saxena, Nitin Khanna", "title": "First Steps Toward CNN based Source Classification of Document Images\n  Shared Over Messaging App", "comments": "10 pages", "journal-ref": null, "doi": "10.1016/j.image.2019.05.020", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of source smartphone corresponding to a document image can be\nhelpful in a variety of applications including copyright infringement,\nownership attribution, leak identification and usage restriction. In this\nletter, we investigate a convolutional neural network-based approach to solve\nsource smartphone identification problem for printed text documents which have\nbeen captured by smartphone cameras and shared over messaging platform. In\nabsence of any publicly available dataset addressing this problem, we introduce\na new image dataset consisting of 315 images of documents printed in three\ndifferent fonts, captured using 21 smartphones and shared over WhatsApp.\nExperiments conducted on this dataset demonstrate that, in all scenarios, the\nproposed system performs as well as or better than the state-of-the-art system\nbased on handcrafted features and classification of letters extracted from\ndocument images. The new dataset and code of the proposed system will be made\npublicly available along with this letter's publication, presently they are\nsubmitted for review.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 17:49:39 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Joshi", "Sharad", ""], ["Saxena", "Suraj", ""], ["Khanna", "Nitin", ""]]}, {"id": "1808.05942", "submitter": "Mohamed Omran", "authors": "Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler,\n  Bernt Schiele", "title": "Neural Body Fitting: Unifying Deep Learning and Model-Based Human Pose\n  and Shape Estimation", "comments": "3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct prediction of 3D body pose and shape remains a challenge even for\nhighly parameterized deep learning models. Mapping from the 2D image space to\nthe prediction space is difficult: perspective ambiguities make the loss\nfunction noisy and training data is scarce. In this paper, we propose a novel\napproach (Neural Body Fitting (NBF)). It integrates a statistical body model\nwithin a CNN, leveraging reliable bottom-up semantic body part segmentation and\nrobust top-down body model constraints. NBF is fully differentiable and can be\ntrained using 2D and 3D annotations. In detailed experiments, we analyze how\nthe components of our model affect performance, especially the use of part\nsegmentations as an explicit intermediate representation, and present a robust,\nefficiently trainable framework for 3D human pose estimation from 2D images\nwith competitive results on standard benchmarks. Code will be made available at\nhttp://github.com/mohomran/neural_body_fitting\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 17:56:10 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Omran", "Mohamed", ""], ["Lassner", "Christoph", ""], ["Pons-Moll", "Gerard", ""], ["Gehler", "Peter V.", ""], ["Schiele", "Bernt", ""]]}, {"id": "1808.05965", "submitter": "Chun-Guang Li", "authors": "Chun-Guang Li, Chong You, and Ren\\'e Vidal", "title": "On Geometric Analysis of Affine Sparse Subspace Clustering", "comments": "15 pages, 6 figures, 2 tables. To appear on IEEE Journal of Selected\n  Topics in Signal Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2867446", "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC) is a state-of-the-art method for segmenting\na set of data points drawn from a union of subspaces into their respective\nsubspaces. It is now well understood that SSC produces subspace-preserving data\naffinity under broad geometric conditions but suffers from a connectivity\nissue. In this paper, we develop a novel geometric analysis for a variant of\nSSC, named affine SSC (ASSC), for the problem of clustering data from a union\nof affine subspaces. Our contributions include a new concept called affine\nindependence for capturing the arrangement of a collection of affine subspaces.\nUnder the affine independence assumption, we show that ASSC is guaranteed to\nproduce subspace-preserving affinity. Moreover, inspired by the phenomenon that\nthe $\\ell_1$ regularization no longer induces sparsity when the solution is\nnonnegative, we further show that subspace-preserving recovery can be achieved\nunder much weaker conditions for all data points other than the extreme points\nof samples from each subspace. In addition, we confirm a curious observation\nthat the affinity produced by ASSC may be subspace-dense---which could\nguarantee the subspace-preserving affinity of ASSC to produce correct\nclustering under rather weak conditions. We validate the theoretical findings\non carefully designed synthetic data and evaluate the performance of ASSC on\nseveral real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 18:11:37 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 15:38:25 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 04:07:34 GMT"}, {"version": "v4", "created": "Wed, 21 Nov 2018 07:39:36 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Li", "Chun-Guang", ""], ["You", "Chong", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1808.06030", "submitter": "Kai Li", "authors": "Kai Li, Zhengming Ding, Kunpeng Li, Yulun Zhang, Yun Fu", "title": "Support Neighbor Loss for Person Re-Identification", "comments": "Accepted by ACM Multimedia (ACM MM) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) has recently been tremendously boosted due\nto the advancement of deep convolutional neural networks (CNN). The majority of\ndeep re-ID methods focus on designing new CNN architectures, while less\nattention is paid on investigating the loss functions. Verification loss and\nidentification loss are two types of losses widely used to train various deep\nre-ID models, both of which however have limitations. Verification loss guides\nthe networks to generate feature embeddings of which the intra-class variance\nis decreased while the inter-class ones is enlarged. However, training networks\nwith verification loss tends to be of slow convergence and unstable performance\nwhen the number of training samples is large. On the other hand, identification\nloss has good separating and scalable property. But its neglect to explicitly\nreduce the intra-class variance limits its performance on re-ID, because the\nsame person may have significant appearance disparity across different camera\nviews. To avoid the limitations of the two types of losses, we propose a new\nloss, called support neighbor (SN) loss. Rather than being derived from data\nsample pairs or triplets, SN loss is calculated based on the positive and\nnegative support neighbor sets of each anchor sample, which contain more\nvaluable contextual information and neighborhood structure that are beneficial\nfor more stable performance. To ensure scalability and separability, a\nsoftmax-like function is formulated to push apart the positive and negative\nsupport sets. To reduce intra-class variance, the distance between the anchor's\nnearest positive neighbor and furthest positive sample is penalized.\nIntegrating SN loss on top of Resnet50, superior re-ID results to the\nstate-of-the-art ones are obtained on several widely used datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 01:40:56 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Li", "Kai", ""], ["Ding", "Zhengming", ""], ["Li", "Kunpeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "1808.06032", "submitter": "Yufei Wang", "authors": "Yufei Wang, Zhe Lin, Xiaohui Shen, Jianming Zhang, Scott Cohen", "title": "Concept Mask: Large-Scale Segmentation from Semantic Concepts", "comments": "Accepted to ECCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on semantic segmentation typically consider a small number of\nlabels, ranging from tens to a few hundreds. With a large number of labels,\ntraining and evaluation of such task become extremely challenging due to\ncorrelation between labels and lack of datasets with complete annotations. We\nformulate semantic segmentation as a problem of image segmentation given a\nsemantic concept, and propose a novel system which can potentially handle an\nunlimited number of concepts, including objects, parts, stuff, and attributes.\nWe achieve this using a weakly and semi-supervised framework leveraging\nmultiple datasets with different levels of supervision. We first train a deep\nneural network on a 6M stock image dataset with only image-level labels to\nlearn visual-semantic embedding on 18K concepts. Then, we refine and extend the\nembedding network to predict an attention map, using a curated dataset with\nbounding box annotations on 750 concepts. Finally, we train an attention-driven\nclass agnostic segmentation network using an 80-category fully annotated\ndataset. We perform extensive experiments to validate that the proposed system\nperforms competitively to the state of the art on fully supervised concepts,\nand is capable of producing accurate segmentations for weakly learned and\nunseen concepts.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 02:26:03 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Yufei", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Zhang", "Jianming", ""], ["Cohen", "Scott", ""]]}, {"id": "1808.06041", "submitter": "Xavier-Lewis Palmer", "authors": "Darlington Ahiale Akogo, Vincent Appiah, Xavier-Lewis Palmer", "title": "CellLineNet: End-to-End Learning and Transfer Learning For Multiclass\n  Epithelial Breast cell Line Classification via a Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Computer Vision for Analyzing and Classifying cells and tissues often require\nrigorous lab procedures and so automated Computer Vision solutions have been\nsought. Most work in such field usually requires Feature Extractions before the\nanalysis of such features via Machine Learning and Machine Vision algorithms.\nWe developed a Convolutional Neural Network that classifies 5 types of\nepithelial breast cell lines comprised of two human cancer lines, 2 normal\nimmortalized lines, and 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A\nand HC11) without requiring feature extraction. The Multiclass Cell Line\nClassification Convolutional Neural Network extends our earlier work on a\nBinary Breast Cancer Cell Line Classification model. CellLineNet is 31-layer\nConvolutional Neural Network trained, validated and tested on a 3,252 image\ndataset of 5 types of Epithelial Breast cell Lines (MDA-MB-468, MCF7, 10A, 12A\nand HC11) in an end-to-end fashion. End-to-End Learning enables CellLineNet to\nidentify and learn on its own, visual features and regularities most important\nto Breast Cancer Cell Line Classification from the dataset of images. Using\nTransfer Learning, the 28-layer MobileNet Convolutional Neural Network\narchitecture with pre-trained ImageNet weights is extended and fine tuned to\nthe Multiclass Epithelial Breast cell Line Classification problem. CellLineNet\nsimply requires an imaged Cell Line as input and it outputs the type of breast\nepithelial cell line (MDA-MB-468, MCF7, 10A, 12A or HC11) as predicted\nprobabilities for the 5 classes. CellLineNet scored a 96.67% Accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 05:05:28 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Akogo", "Darlington Ahiale", ""], ["Appiah", "Vincent", ""], ["Palmer", "Xavier-Lewis", ""]]}, {"id": "1808.06048", "submitter": "Zheng Zhu", "authors": "Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, Weiming Hu", "title": "Distractor-aware Siamese Networks for Visual Object Tracking", "comments": "ECCV 2018, main paper and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Siamese networks have drawn great attention in visual tracking\ncommunity because of their balanced accuracy and speed. However, features used\nin most Siamese tracking approaches can only discriminate foreground from the\nnon-semantic backgrounds. The semantic backgrounds are always considered as\ndistractors, which hinders the robustness of Siamese trackers. In this paper,\nwe focus on learning distractor-aware Siamese networks for accurate and\nlong-term tracking. To this end, features used in traditional Siamese trackers\nare analyzed at first. We observe that the imbalanced distribution of training\ndata makes the learned features less discriminative. During the off-line\ntraining phase, an effective sampling strategy is introduced to control this\ndistribution and make the model focus on the semantic distractors. During\ninference, a novel distractor-aware module is designed to perform incremental\nlearning, which can effectively transfer the general embedding to the current\nvideo domain. In addition, we extend the proposed approach for long-term\ntracking by introducing a simple yet effective local-to-global search region\nstrategy. Extensive experiments on benchmarks show that our approach\nsignificantly outperforms the state-of-the-arts, yielding 9.6% relative gain in\nVOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker\ncan perform at 160 FPS on short-term benchmarks and 110 FPS on long-term\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 06:38:10 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zhu", "Zheng", ""], ["Wang", "Qiang", ""], ["Li", "Bo", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""], ["Hu", "Weiming", ""]]}, {"id": "1808.06088", "submitter": "Jingfeng Wu", "authors": "Bing Yu, Jingfeng Wu, Jinwen Ma and Zhanxing Zhu", "title": "Tangent-Normal Adversarial Regularization for Semi-supervised Learning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with standard supervised learning, the key difficulty in\nsemi-supervised learning is how to make full use of the unlabeled data. A\nrecently proposed method, virtual adversarial training (VAT), smartly performs\nadversarial training without label information to impose a local smoothness on\nthe classifier, which is especially beneficial to semi-supervised learning. In\nthis work, we propose tangent-normal adversarial regularization (TNAR) as an\nextension of VAT by taking the data manifold into consideration. The proposed\nTNAR is composed by two complementary parts, the tangent adversarial\nregularization (TAR) and the normal adversarial regularization (NAR). In TAR,\nVAT is applied along the tangent space of the data manifold, aiming to enforce\nlocal invariance of the classifier on the manifold, while in NAR, VAT is\nperformed on the normal space orthogonal to the tangent space, intending to\nimpose robustness on the classifier against the noise causing the observed data\ndeviating from the underlying data manifold. Demonstrated by experiments on\nboth artificial and practical datasets, our proposed TAR and NAR complement\nwith each other, and jointly outperforms other state-of-the-art methods for\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 14:30:57 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 13:44:47 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 14:57:07 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yu", "Bing", ""], ["Wu", "Jingfeng", ""], ["Ma", "Jinwen", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1808.06133", "submitter": "Ze Wang", "authors": "Ze Wang, Zehao Xiao, Kai Xie, Qiang Qiu, Xiantong Zhen, Xianbin Cao", "title": "In Defense of Single-column Networks for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting usually addressed by density estimation becomes an\nincreasingly important topic in computer vision due to its widespread\napplications in video surveillance, urban planning, and intelligence gathering.\nHowever, it is essentially a challenging task because of the greatly varied\nsizes of objects, coupled with severe occlusions and vague appearance of\nextremely small individuals. Existing methods heavily rely on multi-column\nlearning architectures to extract multi-scale features, which however suffer\nfrom heavy computational cost, especially undesired for crowd counting. In this\npaper, we propose the single-column counting network (SCNet) for efficient\ncrowd counting without relying on multi-column networks. SCNet consists of\nresidual fusion modules (RFMs) for multi-scale feature extraction, a pyramid\npooling module (PPM) for information fusion, and a sub-pixel convolutional\nmodule (SPCM) followed by a bilinear upsampling layer for resolution recovery.\nThose proposed modules enable our SCNet to fully capture multi-scale features\nin a compact single-column architecture and estimate high-resolution density\nmap in an efficient way. In addition, we provide a principled paradigm for\ndensity map generation and data augmentation for training, which shows further\nimproved performance. Extensive experiments on three benchmark datasets show\nthat our SCNet delivers new state-of-the-art performance and surpasses previous\nmethods by large margins, which demonstrates the great effectiveness of SCNet\nas a single-column network for crowd counting.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 21:08:57 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Ze", ""], ["Xiao", "Zehao", ""], ["Xie", "Kai", ""], ["Qiu", "Qiang", ""], ["Zhen", "Xiantong", ""], ["Cao", "Xianbin", ""]]}, {"id": "1808.06155", "submitter": "Xiaoxiang Zhu", "authors": "Muhammad Shahzad, Michael Maurer, Friedrich Fraundorfer, Yuanyuan\n  Wang, Xiao Xiang Zhu", "title": "Buildings Detection in VHR SAR Images Using Fully Convolution Neural\n  Networks", "comments": "Accepted publication in IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2018.2864716", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the highly challenging problem of automatically\ndetecting man-made structures especially buildings in very high resolution\n(VHR) synthetic aperture radar (SAR) images. In this context, the paper has two\nmajor contributions: Firstly, it presents a novel and generic workflow that\ninitially classifies the spaceborne TomoSAR point clouds $ - $ generated by\nprocessing VHR SAR image stacks using advanced interferometric techniques known\nas SAR tomography (TomoSAR) $ - $ into buildings and non-buildings with the aid\nof auxiliary information (i.e., either using openly available 2-D building\nfootprints or adopting an optical image classification scheme) and later back\nproject the extracted building points onto the SAR imaging coordinates to\nproduce automatic large-scale benchmark labelled (buildings/non-buildings) SAR\ndatasets. Secondly, these labelled datasets (i.e., building masks) have been\nutilized to construct and train the state-of-the-art deep Fully Convolution\nNeural Networks with an additional Conditional Random Field represented as a\nRecurrent Neural Network to detect building regions in a single VHR SAR image.\nSuch a cascaded formation has been successfully employed in computer vision and\nremote sensing fields for optical image classification but, to our knowledge,\nhas not been applied to SAR images. The results of the building detection are\nillustrated and validated over a TerraSAR-X VHR spotlight SAR image covering\napproximately 39 km$ ^2 $ $ - $ almost the whole city of Berlin $ - $ with mean\npixel accuracies of around 93.84%\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 12:47:28 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Shahzad", "Muhammad", ""], ["Maurer", "Michael", ""], ["Fraundorfer", "Friedrich", ""], ["Wang", "Yuanyuan", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1808.06178", "submitter": "Mohammad Reza Mohammadi Dr.", "authors": "Mohammad Reza Mohammadi", "title": "Deep Multiple Instance Learning for Airplane Detection in High\n  Resolution Imagery", "comments": "12 pages, 12 figures, 5 tables", "journal-ref": null, "doi": "10.1007/s00138-020-01153-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic airplane detection in aerial imagery has a variety of applications.\nTwo of the significant challenges in this task are variations in the scale and\ndirection of the airplanes. To solve these challenges, we present a\nrotation-and-scale invariant airplane proposal generator. We call this\ngenerator symmetric line segments (SLS) that is developed based on the\nsymmetric and regular boundaries of airplanes from the top view. Then, the\ngenerated proposals are used to train a deep convolutional neural network for\nremoving non-airplane proposals. Since each airplane can have multiple SLS\nproposals, where some of them are not in the direction of the fuselage, we\ncollect all proposals corresponding to one ground-truth as a positive bag and\nthe others as the negative instances. To have multiple instance deep learning,\nwe modify the loss function of the network to learn from each positive bag at\nleast one instance as well as all negative instances. Finally, we employ\nnon-maximum suppression to remove duplicate detections. Our experiments on NWPU\nVHR-10 and DOTA datasets show that our method is a promising approach for\nautomatic airplane detection in very high-resolution images. Moreover, we\nestimate the direction of the airplanes using box-level annotations as an extra\nachievement.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 07:36:22 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 04:40:36 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mohammadi", "Mohammad Reza", ""]]}, {"id": "1808.06194", "submitter": "Yuanxin Ye", "authors": "Yuanxin Ye, Lorenzo Bruzzone, Jie Shan, Francesca Bovolo and Qing Zhu", "title": "Fast and Robust Matching for Multimodal Remote Sensing Image\n  Registration", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2924684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While image registration has been studied in remote sensing community for\ndecades, registering multimodal data [e.g., optical, LiDAR, SAR, and map]\nremains a challenging problem because of significant nonlinear intensity\ndifferences between such data. To address this problem, this paper presents a\nfast and robust matching framework integrating local descriptors for multimodal\nregistration. In the proposed framework, a local descriptor, such as Histogram\nof Oriented Gradient (HOG), Local Self Similarity (LSS), or Speeded-Up Robust\nFeature (SURF), is first extracted at each pixel to form a pixel-wise feature\nrepresentation of an image. Then we define a similarity measure based on the\nfeature representation in frequency domain using the 3 Dimensional Fast Fourier\nTransform (3DFFT) technique, followed by a template matching scheme to detect\ncontrol points between images. In this procedure, we also propose a novel\npixel-wise feature representation using orientated gradients of images, which\nis named channel features of orientated gradients (CFOG). This novel feature is\nan extension of the pixel-wise HOG descriptors, and outperforms that both in\nmatching performance and computational efficiency. The major advantage of the\nproposed framework includes: (1) structural similarity representation using the\npixel-wise feature description and (2) high computational efficiency due to the\nuse of 3DFFT. Experimental results on different types of multimodal images show\nthe superior matching performance of the proposed framework than the\nstate-of-the-art methods.The proposed matching framework have been used in the\nsoftware products of a Chinese listed company. The matlab code is available in\nthis manuscript.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 10:19:06 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 12:36:25 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 08:35:17 GMT"}, {"version": "v4", "created": "Sat, 17 Nov 2018 09:14:20 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 02:35:05 GMT"}, {"version": "v6", "created": "Sat, 9 Jan 2021 09:30:30 GMT"}, {"version": "v7", "created": "Sat, 23 Jan 2021 13:03:39 GMT"}, {"version": "v8", "created": "Wed, 31 Mar 2021 08:28:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ye", "Yuanxin", ""], ["Bruzzone", "Lorenzo", ""], ["Shan", "Jie", ""], ["Bovolo", "Francesca", ""], ["Zhu", "Qing", ""]]}, {"id": "1808.06207", "submitter": "Jie Chen", "authors": "Jie Chen, Cheen-Hau Tan, and Lap-Pui Chau", "title": "Haze Density Estimation via Modeling of Scattering Coefficients of\n  Iso-depth Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision based haze density estimation is of practical implications for the\npurpose of precaution alarm and emergency reactions toward disastrous hazy\nweathers. In this paper, we introduce a haze density estimation framework based\non modeling of scattering coefficients of iso-depth regions. A haze density\nmetric of Normalized Scattering Coefficient (NSC) is proposed to measure\ncurrent haze density level with reference to two reference scales. Iso-depth\nregions are determined via superpixel segmentation. Efficient searching and\nmatching of iso-depth units could be carried out for measurements via\nunstationary cameras. A robust dark SP selection method is used to produce\nreliable predictions for most out-door scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 13:17:51 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Chen", "Jie", ""], ["Tan", "Cheen-Hau", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1808.06210", "submitter": "Erjin Zhou", "authors": "Erjin Zhou, Zhimin Cao, Jian Sun", "title": "GridFace: Face Rectification via Learning Local Homography\n  Transformations", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method, called GridFace, to reduce facial\ngeometric variations and improve the recognition performance. Our method\nrectifies the face by local homography transformations, which are estimated by\na face rectification network. To encourage the image generation with canonical\nviews, we apply a regularization based on the natural face distribution. We\nlearn the rectification network and recognition network in an end-to-end\nmanner. Extensive experiments show our method greatly reduces geometric\nvariations, and gains significant improvements in unconstrained face\nrecognition scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 13:37:11 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zhou", "Erjin", ""], ["Cao", "Zhimin", ""], ["Sun", "Jian", ""]]}, {"id": "1808.06220", "submitter": "Yuan Xie", "authors": "Bingqian Lin, Yuan Xie, Yanyun Qu, Cuihua Li, Xiaodan Liang", "title": "Jointly Deep Multi-View Learning for Clustering Analysis", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Joint framework for Deep Multi-view\nClustering (DMJC), where multiple deep embedded features, multi-view fusion\nmechanism and clustering assignments can be learned simultaneously. Our key\nidea is that the joint learning strategy can sufficiently exploit\nclustering-friendly multi-view features and useful multi-view complementary\ninformation to improve the clustering performance. How to realize the\nmulti-view fusion in such a joint framework is the primary challenge. To do so,\nwe design two ingenious variants of deep multi-view joint clustering models\nunder the proposed framework, where multi-view fusion is implemented by two\ndifferent schemes. The first model, called DMJC-S, performs multi-view fusion\nin an implicit way via a novel multi-view soft assignment distribution. The\nsecond model, termed DMJC-T, defines a novel multi-view auxiliary target\ndistribution to conduct the multi-view fusion explicitly. Both DMJC-S and\nDMJC-T are optimized under a KL divergence like clustering objective.\nExperiments on six challenging image datasets demonstrate the superiority of\nboth DMJC-S and DMJC-T over single/multi-view baselines and the\nstate-of-the-art multiview clustering methods, which proves the effectiveness\nof the proposed DMJC framework. To our best knowledge, this is the first work\nto model the multi-view clustering in a deep joint framework, which will\nprovide a meaningful thinking in unsupervised multi-view learning.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 15:17:34 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 09:09:28 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Lin", "Bingqian", ""], ["Xie", "Yuan", ""], ["Qu", "Yanyun", ""], ["Li", "Cuihua", ""], ["Liang", "Xiaodan", ""]]}, {"id": "1808.06250", "submitter": "Tavi Halperin", "authors": "Tavi Halperin, Ariel Ephrat, Shmuel Peleg", "title": "Dynamic Temporal Alignment of Speech to Lips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many speech segments in movies are re-recorded in a studio during\npostproduction, to compensate for poor sound quality as recorded on location.\nManual alignment of the newly-recorded speech with the original lip movements\nis a tedious task. We present an audio-to-video alignment method for automating\nspeech to lips alignment, stretching and compressing the audio signal to match\nthe lip movements. This alignment is based on deep audio-visual features,\nmapping the lips video and the speech signal to a shared representation. Using\nthis shared representation we compute the lip-sync error between every short\nspeech period and every video frame, followed by the determination of the\noptimal corresponding frame for each short sound period over the entire video\nclip. We demonstrate successful alignment both quantitatively, using a human\nperception-inspired metric, as well as qualitatively. The strongest advantage\nof our audio-to-video approach is in cases where the original voice in unclear,\nand where a constant shift of the sound can not give a perfect alignment. In\nthese cases state-of-the-art methods will fail.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 19:58:05 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Halperin", "Tavi", ""], ["Ephrat", "Ariel", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1808.06253", "submitter": "Gr\\'egoire Payen de La Garanderie", "authors": "Gr\\'egoire Payen de La Garanderie, Amir Atapour Abarghouei, Toby P.\n  Breckon", "title": "Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular\n  Depth Estimation to 360{\\deg} Panoramic Imagery", "comments": "This work is accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent automotive vision work has focused almost exclusively on processing\nforward-facing cameras. However, future autonomous vehicles will not be viable\nwithout a more comprehensive surround sensing, akin to a human driver, as can\nbe provided by 360{\\deg} panoramic cameras. We present an approach to adapt\ncontemporary deep network architectures developed on conventional rectilinear\nimagery to work on equirectangular 360{\\deg} panoramic imagery. To address the\nlack of annotated panoramic automotive datasets availability, we adapt a\ncontemporary automotive dataset, via style and projection transformations, to\nfacilitate the cross-domain retraining of contemporary algorithms for panoramic\nimagery. Following this approach we retrain and adapt existing architectures to\nrecover scene depth and 3D pose of vehicles from monocular panoramic imagery\nwithout any panoramic training labels or calibration parameters. Our approach\nis evaluated qualitatively on crowd-sourced panoramic images and quantitatively\nusing an automotive environment simulator to provide the first benchmark for\nsuch techniques within panoramic imagery.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 20:38:08 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["de La Garanderie", "Gr\u00e9goire Payen", ""], ["Abarghouei", "Amir Atapour", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1808.06280", "submitter": "Lijie Niu", "authors": "Jianjun Lei, Lijie Niu, Huazhu Fu, Bo Peng, Qingming Huang, and\n  Chunping Hou", "title": "Person Re-Identification by Semantic Region Representation and Topology\n  Constraint", "comments": "13 pages, 10 figures, Accepted by IEEE Transactions on Circuits and\n  Systems for Video Technology 2018", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2866260", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a popular research topic which aims at matching\nthe specific person in a multi-camera network automatically. Feature\nrepresentation and metric learning are two important issues for person\nre-identification. In this paper, we propose a novel person re-identification\nmethod, which consists of a reliable representation called Semantic Region\nRepresentation (SRR), and an effective metric learning with Mapping Space\nTopology Constraint (MSTC). The SRR integrates semantic representations to\nachieve effective similarity comparison between the corresponding regions via\nparsing the body into multiple parts, which focuses on the foreground context\nagainst the background interference. To learn a discriminant metric, the MSTC\nis proposed to take into account the topological relationship among all samples\nin the feature space. It considers two-fold constraints: the distribution of\npositive pairs should be more compact than the average distribution of negative\npairs with regard to the same probe, while the average distance between\ndifferent classes should be larger than that between same classes. These two\naspects cooperate to maintain the compactness of the intra-class as well as the\nsparsity of the inter-class. Extensive experiments conducted on five\nchallenging person re-identification datasets, VIPeR, SYSU-sReID, QUML GRID,\nCUHK03, and Market-1501, show that the proposed method achieves competitive\nperformance with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 01:25:09 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Lei", "Jianjun", ""], ["Niu", "Lijie", ""], ["Fu", "Huazhu", ""], ["Peng", "Bo", ""], ["Huang", "Qingming", ""], ["Hou", "Chunping", ""]]}, {"id": "1808.06281", "submitter": "Prajjwal Bhargava", "authors": "Prajjwal Bhargava", "title": "Incremental Learning in Person Re-Identification", "comments": "The code can be found at\n  https://github.com/prajjwal1/person-reid-incremental", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification is still a challenging task in Computer Vision due\nto a variety of reasons. On the other side, Incremental Learning is still an\nissue since deep learning models tend to face the problem of over catastrophic\nforgetting when trained on subsequent tasks. In this paper, we propose a model\nthat can be used for multiple tasks in Person Re-Identification, provide\nstate-of-the-art results on a variety of tasks and still achieve considerable\naccuracy subsequently. We evaluated our model on two datasets Market 1501 and\nDuke MTMC. Extensive experiments show that this method can achieve Incremental\nLearning in Person ReID efficiently as well as for other tasks in computer\nvision as well.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 01:37:16 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 05:39:16 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 09:30:15 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2019 05:01:48 GMT"}, {"version": "v5", "created": "Thu, 8 Aug 2019 12:08:30 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Bhargava", "Prajjwal", ""]]}, {"id": "1808.06323", "submitter": "Aniruddha Mazumdar", "authors": "Aniruddha Mazumdar, Jaya Singh, Yosha Singh Tomar, Prabin Kumar Bora", "title": "Universal Image Manipulation Detection using Deep Siamese Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of different types of image editing operations carried out on an\nimage is an important problem in image forensics. It gives the information\nabout the processing history of an image, and also can expose forgeries present\nin an image. There have been few methods proposed to detect different types of\nimage editing operations in a single framework. However, all the operations\nhave to be known a priori in the training phase. But, in real-forensics\nscenarios it may not be possible to know about the editing operations carried\nout on an image. To solve this problem, we propose a novel deep learning-based\nmethod which can differentiate between different types of image editing\noperations. The proposed method classifies image patches in a pair-wise fashion\nas either similarly or differently processed using a deep siamese neural\nnetwork. Once the network learns feature that can discriminate between\ndifferent image editing operations, it can differentiate between different\nimage editing operations not present in the training stage. The experimental\nresults show the efficacy of the proposed method in detecting/discriminating\ndifferent image editing operations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 06:40:05 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 11:27:35 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mazumdar", "Aniruddha", ""], ["Singh", "Jaya", ""], ["Tomar", "Yosha Singh", ""], ["Bora", "Prabin Kumar", ""]]}, {"id": "1808.06352", "submitter": "Sajad Saeedi", "authors": "Sajad Saeedi (1), Bruno Bodin (2), Harry Wagstaff (2), Andy Nisbet\n  (3), Luigi Nardi (4), John Mawer (3), Nicolas Melot (1), Oscar Palomar (3),\n  Emanuele Vespa (1), Tom Spink (2), Cosmin Gorgovan (3), Andrew Webb (3),\n  James Clarkson (3), Erik Tomusk (2), Thomas Debrunner (1), Kuba Kaszyk (2),\n  Pablo Gonzalez-de-Aledo (1), Andrey Rodchenko (3), Graham Riley (3), Christos\n  Kotselidis (3), Bj\\\"orn Franke (2), Michael F. P. O'Boyle (2), Andrew J.\n  Davison (1), Paul H. J. Kelly (1), Mikel Luj\\'an (3), Steve Furber (3) ((1)\n  Department of Computing, Imperial College London, UK, (2) School of\n  Informatics, University of Edinburgh, UK, (3) School of Computer Science,\n  University of Manchester, UK, (4) Electrical Engineering - Computer Systems,\n  Stanford University, USA)", "title": "Navigating the Landscape for Real-time Localisation and Mapping for\n  Robotics and Virtual and Augmented Reality", "comments": "Proceedings of the IEEE 2018", "journal-ref": null, "doi": "10.1109/JPROC.2018.2856739", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual understanding of 3D environments in real-time, at low power, is a huge\ncomputational challenge. Often referred to as SLAM (Simultaneous Localisation\nand Mapping), it is central to applications spanning domestic and industrial\nrobotics, autonomous vehicles, virtual and augmented reality. This paper\ndescribes the results of a major research effort to assemble the algorithms,\narchitectures, tools, and systems software needed to enable delivery of SLAM,\nby supporting applications specialists in selecting and configuring the\nappropriate algorithm and the appropriate hardware, and compilation pathway, to\nmeet their performance, accuracy, and energy consumption goals. The major\ncontributions we present are (1) tools and methodology for systematic\nquantitative evaluation of SLAM algorithms, (2) automated,\nmachine-learning-guided exploration of the algorithmic and implementation\ndesign space with respect to multiple objectives, (3) end-to-end simulation\ntools to enable optimisation of heterogeneous, accelerated architectures for\nthe specific algorithmic requirements of the various SLAM algorithmic\napproaches, and (4) tools for delivering, where appropriate, accelerated,\nadaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:06:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Saeedi", "Sajad", ""], ["Bodin", "Bruno", ""], ["Wagstaff", "Harry", ""], ["Nisbet", "Andy", ""], ["Nardi", "Luigi", ""], ["Mawer", "John", ""], ["Melot", "Nicolas", ""], ["Palomar", "Oscar", ""], ["Vespa", "Emanuele", ""], ["Spink", "Tom", ""], ["Gorgovan", "Cosmin", ""], ["Webb", "Andrew", ""], ["Clarkson", "James", ""], ["Tomusk", "Erik", ""], ["Debrunner", "Thomas", ""], ["Kaszyk", "Kuba", ""], ["Gonzalez-de-Aledo", "Pablo", ""], ["Rodchenko", "Andrey", ""], ["Riley", "Graham", ""], ["Kotselidis", "Christos", ""], ["Franke", "Bj\u00f6rn", ""], ["O'Boyle", "Michael F. P.", ""], ["Davison", "Andrew J.", ""], ["Kelly", "Paul H. J.", ""], ["Luj\u00e1n", "Mikel", ""], ["Furber", "Steve", ""]]}, {"id": "1808.06368", "submitter": "Raul Gomez", "authors": "Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas", "title": "Learning to Learn from Web Data through Deep Semantic Embeddings", "comments": "ECCV MULA Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to learn a multimodal image and text embedding from\nWeb and Social Media data, aiming to leverage the semantic knowledge learnt in\nthe text domain and transfer it to a visual model for semantic image retrieval.\nWe demonstrate that the pipeline can learn from images with associated text\nwithout supervision and perform a thourough analysis of five different text\nembeddings in three different benchmarks. We show that the embeddings learnt\nwith Web and Social Media data have competitive performances over supervised\nmethods in the text based image retrieval task, and we clearly outperform state\nof the art in the MIRFlickr dataset when training in the target data. Further\nwe demonstrate how semantic multimodal image retrieval can be performed using\nthe learnt embeddings, going beyond classical instance-level retrieval\nproblems. Finally, we present a new dataset, InstaCities1M, composed by\nInstagram images and their associated texts that can be used for fair\ncomparison of image-text embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 09:58:23 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Gomez", "Raul", ""], ["Gomez", "Lluis", ""], ["Gibert", "Jaume", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1808.06369", "submitter": "Raul Gomez", "authors": "Raul Gomez, Lluis Gomez, Jaume Gibert, Dimosthenis Karatzas", "title": "Learning from #Barcelona Instagram data what Locals and Tourists post\n  about its Neighbourhoods", "comments": "ECCV MULA Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive tourism is becoming a big problem for some cities, such as Barcelona,\ndue to its concentration in some neighborhoods. In this work we gather\nInstagram data related to Barcelona consisting on images-captions pairs and,\nusing the text as a supervisory signal, we learn relations between images,\nwords and neighborhoods. Our goal is to learn which visual elements appear in\nphotos when people is posting about each neighborhood. We perform a language\nseparate treatment of the data and show that it can be extrapolated to a\ntourists and locals separate analysis, and that tourism is reflected in Social\nMedia at a neighborhood level. The presented pipeline allows analyzing the\ndifferences between the images that tourists and locals associate to the\ndifferent neighborhoods. The proposed method, which can be extended to other\ncities or subjects, proves that Instagram data can be used to train multi-modal\n(image and text) machine learning models that are useful to analyze\npublications about a city at a neighborhood level. We publish the collected\ndataset, InstaBarcelona and the code used in the analysis.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 10:04:55 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Gomez", "Raul", ""], ["Gomez", "Lluis", ""], ["Gibert", "Jaume", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1808.06389", "submitter": "Eddy Ilg", "authors": "Osama Makansi, Eddy Ilg and Thomas Brox", "title": "FusionNet and AugmentedFlowNet: Selective Proxy Ground Truth for\n  Training on Unlabeled Images", "comments": "See video at: https://www.youtube.com/watch?v=HdMeb20Rybs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that convolutional neural networks (CNNs) can be used\nto estimate optical flow with high quality and fast runtime. This makes them\npreferable for real-world applications. However, such networks require very\nlarge training datasets. Engineering the training data is difficult and/or\nlaborious. This paper shows how to augment a network trained on an existing\nsynthetic dataset with large amounts of additional unlabelled data. In\nparticular, we introduce a selection mechanism to assemble from multiple\nestimates a joint optical flow field, which outperforms that of all input\nmethods. The latter can be used as proxy-ground-truth to train a network on\nreal-world data and to adapt it to specific domains of interest. Our\nexperimental results show that the performance of networks improves\nconsiderably, both, in cross-domain and in domain-specific scenarios. As a\nconsequence, we obtain state-of-the-art results on the KITTI benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:18:06 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Makansi", "Osama", ""], ["Ilg", "Eddy", ""], ["Brox", "Thomas", ""]]}, {"id": "1808.06396", "submitter": "Adrian Popescu", "authors": "Eden Belouadah, Adrian Popescu", "title": "DeeSIL: Deep-Shallow Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental Learning (IL) is an interesting AI problem when the algorithm is\nassumed to work on a budget. This is especially true when IL is modeled using a\ndeep learning approach, where two com- plex challenges arise due to limited\nmemory, which induces catastrophic forgetting and delays related to the\nretraining needed in order to incorpo- rate new classes. Here we introduce\nDeeSIL, an adaptation of a known transfer learning scheme that combines a fixed\ndeep representation used as feature extractor and learning independent shallow\nclassifiers to in- crease recognition capacity. This scheme tackles the two\naforementioned challenges since it works well with a limited memory budget and\neach new concept can be added within a minute. Moreover, since no deep re-\ntraining is needed when the model is incremented, DeeSIL can integrate larger\namounts of initial data that provide more transferable features. Performance is\nevaluated on ImageNet LSVRC 2012 against three state of the art algorithms.\nResults show that, at scale, DeeSIL performance is 23 and 33 points higher than\nthe best baseline when using the same and more initial data respectively.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:39:09 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Belouadah", "Eden", ""], ["Popescu", "Adrian", ""]]}, {"id": "1808.06428", "submitter": "Anabik Pal Mr.", "authors": "Anabik Pal, Akshay Chaturvedi, Utpal Garain, Aditi Chandra, Raghunath\n  Chatterjee, and Swapan Senapati", "title": "CapsDeMM: Capsule network for Detection of Munro's Microabscess in skin\n  biopsy images", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for automatic detection of Munro's\nMicroabscess in stratum corneum (SC) of human skin biopsy in order to realize a\nmachine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils\nin presence of nucleated cells is solved using the recent advances of deep\nlearning algorithms. Separation of SC layer, extraction of patches from the\nlayer followed by classification of patches with respect to presence or absence\nof neutrophils form the basis of the overall approach which is effected through\nan integration of a U-Net based segmentation network and a capsule network for\nclassification. The novel design of the present capsule net leads to a drastic\nreduction in the number of parameters without any noticeable compromise in the\noverall performance. The research further addresses the challenge of dealing\nwith Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The\npromising result coming out of an experiment on a dataset consisting of 273\nreal-life images shows that a practical system is possible based on the present\nresearch. The implementation of our system is available at\nhttps://github.com/Anabik/CapsDeMM.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:50:41 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 13:31:15 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Pal", "Anabik", ""], ["Chaturvedi", "Akshay", ""], ["Garain", "Utpal", ""], ["Chandra", "Aditi", ""], ["Chatterjee", "Raghunath", ""], ["Senapati", "Swapan", ""]]}, {"id": "1808.06469", "submitter": "Christina Koutsoumpa", "authors": "Christina Koutsoumpa, Jennifer Keegan, David Firmin, Guang-Zhong Yang\n  and Duncan Gillies", "title": "Translational Motion Compensation for Soft Tissue Velocity Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Advancements in MRI Tissue Phase Velocity Mapping (TPM) allow for\nthe acquisition of higher quality velocity cardiac images providing better\nassessment of regional myocardial deformation for accurate disease diagnosis,\npre-operative planning and post-operative patient surveillance. Translation of\nTPM velocities from the scanner's reference coordinate system to the regional\ncardiac coordinate system requires decoupling of translational motion and\nmotion due to myocardial deformation. Despite existing techniques for\nrespiratory motion compensation in TPM, there is still a remaining\ntranslational velocity component due to the global motion of the beating heart.\nTo compensate for translational motion in cardiac TPM, we propose an\nimage-processing method, which we have evaluated on synthetic data and applied\non in vivo TPM data. Methods: Translational motion is estimated from a suitable\nregion of velocities automatically defined in the left-ventricular volume. The\nregion is generated by dilating the medial axis of myocardial masks in each\nslice and the translational velocity is estimated by integration in this\nregion. The method was evaluated on synthetic data and in vivo data corrupted\nwith a translational velocity component (200% of the maximum measured\nvelocity). Accuracy and robustness were examined and the method was applied on\n10 in vivo datasets. Results: The results from synthetic and in vivo corrupted\ndata show excellent performance with an estimation error less than 0.3% and\nhigh robustness in both cases. The effectiveness of the method is confirmed\nwith visual observation of results from the 10 datasets. Conclusion: The\nproposed method is accurate and suitable for translational motion correction of\nthe left ventricular velocity fields. The current method for translational\nmotion compensation could be applied to any annular contracting (tissue)\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 14:22:05 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Koutsoumpa", "Christina", ""], ["Keegan", "Jennifer", ""], ["Firmin", "David", ""], ["Yang", "Guang-Zhong", ""], ["Gillies", "Duncan", ""]]}, {"id": "1808.06495", "submitter": "Hiroyuki Kobayashi", "authors": "Hiroyuki Kobayashi, Hitoshi Kiya", "title": "Bitstream-Based JPEG Image Encryption with File-Size Preserving", "comments": "to appear in 2018 IEEE 7th Global Conference on Consumer Electronics,\n  Nara, JAPAN, 10th Oct., 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An encryption scheme of JPEG images in the bitstream domain is proposed. The\nproposed scheme preserves the JPEG format even after encrypting the images, and\nthe file size of encrypted images is the exact same as that of the original\nJPEG images. Several methods for encrypting JPEG images in the bitstream domain\nhave been proposed. However, since some marker codes are generated or lost in\nthe encryption process, the file size of JPEG bitstreams is generally changed\ndue to the encryption operations. The proposed method inputs JPEG bitstreams\nand selectively encrypts the additional bit components of the Huffman code in\nthe bitstreams. This feature allows us to have encrypted images with the same\ndata size as that recoded in the image transmission process, when JPEG images\nare replaced with the encrypted ones by the hooking, so that the image\ntransmission are successfully carried out after the hooking.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:37:24 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1808.06516", "submitter": "Jos\\'e M. F\\'acil", "authors": "Daniel Olid, Jos\\'e M. F\\'acil and Javier Civera", "title": "Single-View Place Recognition under Seasonal Changes", "comments": "Accepted at 10th Planning, Perception and Navigation for Intelligent\n  Vehicles (PPNIV'18), Workshop at IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view place recognition, that we can define as finding an image that\ncorresponds to the same place as a given query image, is a key capability for\nautonomous navigation and mapping. Although there has been a considerable\namount of research in the topic, the high degree of image variability (with\nviewpoint, illumination or occlusions for example) makes it a research\nchallenge.\n  One of the particular challenges, that we address in this work, is weather\nvariation. Seasonal changes can produce drastic appearance changes, that\nclassic low-level features do not model properly. Our contributions in this\npaper are twofold. First we pre-process and propose a partition for the\nNordland dataset, frequently used for place recognition research without\nconsensus on the partitions. And second, we evaluate several neural network\narchitectures such as pre-trained, siamese and triplet for this problem. Our\nbest results outperform the state of the art of the field. A video showing our\nresults can be found in https://youtu.be/VrlxsYZoHDM. The partitioned version\nof the Nordland dataset at http://webdiis.unizar.es/~jmfacil/pr-nordland/.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:35:29 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Olid", "Daniel", ""], ["F\u00e1cil", "Jos\u00e9 M.", ""], ["Civera", "Javier", ""]]}, {"id": "1808.06519", "submitter": "Mauricio Orbes Arteaga", "authors": "Mauricio Orbes-Arteaga, M. Jorge Cardoso, Lauge S{\\o}rensen, Marc\n  Modat, S\\'ebastien Ourselin, Mads Nielsen, Akshay Pai", "title": "Simultaneous synthesis of FLAIR and segmentation of white matter\n  hypointensities from T1 MRIs", "comments": "Conference on Medical Imaging with Deep Learning MIDL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting vascular pathologies such as white matter lesions in Brain\nmagnetic resonance images (MRIs) require acquisition of multiple sequences such\nas T1-weighted (T1-w) --on which lesions appear hypointense-- and fluid\nattenuated inversion recovery (FLAIR) sequence --where lesions appear\nhyperintense--. However, most of the existing retrospective datasets do not\nconsist of FLAIR sequences. Existing missing modality imputation methods\nseparate the process of imputation, and the process of segmentation. In this\npaper, we propose a method to link both modality imputation and segmentation\nusing convolutional neural networks. We show that by jointly optimizing the\nimputation network and the segmentation network, the method not only produces\nmore realistic synthetic FLAIR images from T1-w images, but also improves the\nsegmentation of WMH from T1-w images only.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:43:06 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Orbes-Arteaga", "Mauricio", ""], ["Cardoso", "M. Jorge", ""], ["S\u00f8rensen", "Lauge", ""], ["Modat", "Marc", ""], ["Ourselin", "S\u00e9bastien", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""]]}, {"id": "1808.06521", "submitter": "Zhiqiang Tang", "authors": "Zhiqiang Tang, Xi Peng, Shijie Geng, Yizhe Zhu and Dimitris N. Metaxas", "title": "CU-Net: Coupled U-Nets", "comments": "BMVC 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new connectivity pattern for the U-Net architecture. Given\nseveral stacked U-Nets, we couple each U-Net pair through the connections of\ntheir semantic blocks, resulting in the coupled U-Nets (CU-Net). The coupling\nconnections could make the information flow more efficiently across U-Nets. The\nfeature reuse across U-Nets makes each U-Net very parameter efficient. We\nevaluate the coupled U-Nets on two benchmark datasets of human pose estimation.\nBoth the accuracy and model parameter number are compared. The CU-Net obtains\ncomparable accuracy as state-of-the-art methods. However, it only has at least\n60% fewer parameters than other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:45:26 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Tang", "Zhiqiang", ""], ["Peng", "Xi", ""], ["Geng", "Shijie", ""], ["Zhu", "Yizhe", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1808.06556", "submitter": "Kai Xu", "authors": "Yawei Zhao, Kai Xu, Xinwang Liu, En Zhu, Xinzhong Zhu, Jianping Yin", "title": "Triangle Lasso for Simultaneous Clustering and Optimization in Graph\n  Datasets", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, network lasso has drawn many attentions due to its remarkable\nperformance on simultaneous clustering and optimization. However, it usually\nsuffers from the imperfect data (noise, missing values etc), and yields\nsub-optimal solutions. The reason is that it finds the similar instances\naccording to their features directly, which is usually impacted by the\nimperfect data, and thus returns sub-optimal results. In this paper, we propose\ntriangle lasso to avoid its disadvantage. Triangle lasso finds the similar\ninstances according to their neighbours. If two instances have many common\nneighbours, they tend to become similar. Although some instances are profiled\nby the imperfect data, it is still able to find the similar counterparts.\nFurthermore, we develop an efficient algorithm based on Alternating Direction\nMethod of Multipliers (ADMM) to obtain a moderately accurate solution. In\naddition, we present a dual method to obtain the accurate solution with the low\nadditional time consumption. We demonstrate through extensive numerical\nexperiments that triangle lasso is robust to the imperfect data. It usually\nyields a better performance than the state-of-the-art method when performing\ndata analysis tasks in practical scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 16:31:30 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zhao", "Yawei", ""], ["Xu", "Kai", ""], ["Liu", "Xinwang", ""], ["Zhu", "En", ""], ["Zhu", "Xinzhong", ""], ["Yin", "Jianping", ""]]}, {"id": "1808.06560", "submitter": "Shuchin Aeron", "authors": "Anuththari Gamage, Brian Rappaport, Shuchin Aeron, Xiaozhe Hu", "title": "Multi-View Graph Embedding Using Randomized Shortest Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data sets often provide multiple types of information about the\nsame set of entities. This data is well represented by multi-view graphs, which\nconsist of several distinct sets of edges over the same nodes. These can be\nused to analyze how entities interact from different viewpoints. Combining\nmultiple views improves the quality of inferences drawn from the underlying\ndata, which has increased interest in developing efficient multi-view graph\nembedding methods. We propose an algorithm, C-RSP, that generates a common (C)\nembedding of a multi-view graph using Randomized Shortest Paths (RSP). This\nalgorithm generates a dissimilarity measure between nodes by minimizing the\nexpected cost of a random walk between any two nodes across all views of a\nmulti-view graph, in doing so encoding both the local and global structure of\nthe graph. We test C-RSP on both real and synthetic data and show that it\noutperforms benchmark algorithms at embedding and clustering tasks while\nremaining computationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 16:40:35 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Gamage", "Anuththari", ""], ["Rappaport", "Brian", ""], ["Aeron", "Shuchin", ""], ["Hu", "Xiaozhe", ""]]}, {"id": "1808.06562", "submitter": "Tal Remez", "authors": "Tal Remez, Or Litany, Raja Giryes, and Alex M. Bronstein", "title": "Class-Aware Fully-Convolutional Gaussian and Poisson Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2859044", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully-convolutional neural-network architecture for image\ndenoising which is simple yet powerful. Its structure allows to exploit the\ngradual nature of the denoising process, in which shallow layers handle local\nnoise statistics, while deeper layers recover edges and enhance textures. Our\nmethod advances the state-of-the-art when trained for different noise levels\nand distributions (both Gaussian and Poisson). In addition, we show that making\nthe denoiser class-aware by exploiting semantic class information boosts\nperformance, enhances textures and reduces artifacts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 16:46:09 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Remez", "Tal", ""], ["Litany", "Or", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1808.06586", "submitter": "Xiaoyang Guo", "authors": "Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, Xiaogang Wang", "title": "Learning Monocular Depth by Distilling Cross-domain Stereo Networks", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation aims at estimating a pixelwise depth map for a\nsingle image, which has wide applications in scene understanding and autonomous\ndriving. Existing supervised and unsupervised methods face great challenges.\nSupervised methods require large amounts of depth measurement data, which are\ngenerally difficult to obtain, while unsupervised methods are usually limited\nin estimation accuracy. Synthetic data generated by graphics engines provide a\npossible solution for collecting large amounts of depth data. However, the\nlarge domain gaps between synthetic and realistic data make directly training\nwith them challenging. In this paper, we propose to use the stereo matching\nnetwork as a proxy to learn depth from synthetic data and use predicted stereo\ndisparity maps for supervising the monocular depth estimation network.\nCross-domain synthetic data could be fully utilized in this novel framework.\nDifferent strategies are proposed to ensure learned depth perception capability\nwell transferred across different domains. Our extensive experiments show\nstate-of-the-art results of monocular depth estimation on KITTI dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:47:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Li", "Hongsheng", ""], ["Yi", "Shuai", ""], ["Ren", "Jimmy", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1808.06601", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan\n  Kautz, Bryan Catanzaro", "title": "Video-to-Video Synthesis", "comments": "In NeurIPS, 2018. Code, models, and more results are available at\n  https://github.com/NVIDIA/vid2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of video-to-video synthesis, whose goal is to learn a\nmapping function from an input source video (e.g., a sequence of semantic\nsegmentation masks) to an output photorealistic video that precisely depicts\nthe content of the source video. While its image counterpart, the\nimage-to-image synthesis problem, is a popular topic, the video-to-video\nsynthesis problem is less explored in the literature. Without understanding\ntemporal dynamics, directly applying existing image synthesis approaches to an\ninput video often results in temporally incoherent videos of low visual\nquality. In this paper, we propose a novel video-to-video synthesis approach\nunder the generative adversarial learning framework. Through carefully-designed\ngenerator and discriminator architectures, coupled with a spatio-temporal\nadversarial objective, we achieve high-resolution, photorealistic, temporally\ncoherent video results on a diverse set of input formats including segmentation\nmasks, sketches, and poses. Experiments on multiple benchmarks show the\nadvantage of our method compared to strong baselines. In particular, our model\nis capable of synthesizing 2K resolution videos of street scenes up to 30\nseconds long, which significantly advances the state-of-the-art of video\nsynthesis. Finally, we apply our approach to future video prediction,\noutperforming several state-of-the-art competing systems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:58:42 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 15:12:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Zhu", "Jun-Yan", ""], ["Liu", "Guilin", ""], ["Tao", "Andrew", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1808.06661", "submitter": "Bin Wang", "authors": "Bin Wang and Yanan Sun and Bing Xue and Mengjie Zhang", "title": "A Hybrid Differential Evolution Approach to Designing Deep Convolutional\n  Neural Networks for Image Classification", "comments": "Accepted by The Australasian Joint Conference on Artificial\n  Intelligence 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have demonstrated their superiority in\nimage classification, and evolutionary computation (EC) methods have recently\nbeen surging to automatically design the architectures of CNNs to save the\ntedious work of manually designing CNNs. In this paper, a new hybrid\ndifferential evolution (DE) algorithm with a newly added crossover operator is\nproposed to evolve the architectures of CNNs of any lengths, which is named\nDECNN. There are three new ideas in the proposed DECNN method. Firstly, an\nexisting effective encoding scheme is refined to cater for variable-length CNN\narchitectures; Secondly, the new mutation and crossover operators are developed\nfor variable-length DE to optimise the hyperparameters of CNNs; Finally, the\nnew second crossover is introduced to evolve the depth of the CNN\narchitectures. The proposed algorithm is tested on six widely-used benchmark\ndatasets and the results are compared to 12 state-of-the-art methods, which\nshows the proposed method is vigorously competitive to the state-of-the-art\nalgorithms. Furthermore, the proposed method is also compared with a method\nusing particle swarm optimisation with a similar encoding strategy named IPPSO,\nand the proposed DECNN outperforms IPPSO in terms of the accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:24:45 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 01:32:59 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1808.06671", "submitter": "Christoph Mayer", "authors": "Christoph Mayer and Radu Timofte", "title": "Adversarial Sampling for Active Learning", "comments": "Accepted at WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes asal, a new GAN based active learning method that\ngenerates high entropy samples. Instead of directly annotating the synthetic\nsamples, ASAL searches similar samples from the pool and includes them for\ntraining. Hence, the quality of new samples is high and annotations are\nreliable. To the best of our knowledge, ASAL is the first GAN based AL method\napplicable to multi-class problems that outperforms random sample selection.\nAnother benefit of ASAL is its small run-time complexity (sub-linear) compared\nto traditional uncertainty sampling (linear). We present a comprehensive set of\nexperiments on multiple traditional data sets and show that ASAL outperforms\nsimilar methods and clearly exceeds the established baseline (random sampling).\nIn the discussion section we analyze in which situations ASAL performs best and\nwhy it is sometimes hard to outperform random sample selection.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:53:19 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 13:47:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mayer", "Christoph", ""], ["Timofte", "Radu", ""]]}, {"id": "1808.06675", "submitter": "Soham Saha", "authors": "Soham Saha, Girish Varma, C.V.Jawahar", "title": "Class2Str: End to End Latent Hierarchy Learning", "comments": "6 pages, ICPR 2018, Beijing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep neural networks for image classification typically consists of a\nconvolutional feature extractor followed by a fully connected classifier\nnetwork. The predicted and the ground truth labels are represented as one hot\nvectors. Such a representation assumes that all classes are equally dissimilar.\nHowever, classes have visual similarities and often form a hierarchy. Learning\nthis latent hierarchy explicitly in the architecture could provide invaluable\ninsights. We propose an alternate architecture to the classifier network called\nthe Latent Hierarchy (LH) Classifier and an end to end learned Class2Str\nmapping which discovers a latent hierarchy of the classes. We show that for\nsome of the best performing architectures on CIFAR and Imagenet datasets, the\nproposed replacement and training by LH classifier recovers the accuracy, with\na fraction of the number of parameters in the classifier part. Compared to the\nprevious work of HDCNN, which also learns a 2 level hierarchy, we are able to\nlearn a hierarchy at an arbitrary number of levels as well as obtain an\naccuracy improvement on the Imagenet classification task over them. We also\nverify that many visually similar classes are grouped together, under the\nlearnt hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 19:58:35 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Saha", "Soham", ""], ["Varma", "Girish", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1808.06686", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Wael AbdAlmageed, Yue Wu and Prem Natarajan", "title": "Deep Multimodal Image-Repurposing Detection", "comments": "To be published at ACM Multimeda 2018 (orals)", "journal-ref": null, "doi": "10.1145/3240508.3240707", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nefarious actors on social media and other platforms often spread rumors and\nfalsehoods through images whose metadata (e.g., captions) have been modified to\nprovide visual substantiation of the rumor/falsehood. This type of modification\nis referred to as image repurposing, in which often an unmanipulated image is\npublished along with incorrect or manipulated metadata to serve the actor's\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\ndataset, a substantially challenging dataset over that which has been\npreviously available to support research into image repurposing detection. The\nnew dataset includes location, person, and organization manipulations on\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\nmultimodal learning model for assessing the integrity of an image by combining\ninformation extracted from the image with related information from a knowledge\nbase. The proposed method is compared against state-of-the-art techniques on\nexisting datasets as well as MEIR, where it outperforms existing methods across\nthe board, with AUC improvement up to 0.23.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 20:47:56 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Sabir", "Ekraam", ""], ["AbdAlmageed", "Wael", ""], ["Wu", "Yue", ""], ["Natarajan", "Prem", ""]]}, {"id": "1808.06698", "submitter": "Kai Xu", "authors": "Songle Chen, Lintao Zheng, Yan Zhang, Zhixin Sun, Kai Xu", "title": "VERAM: View-Enhanced Recurrent Attention Model for 3D Shape\n  Classification", "comments": "Accepted by IEEE Transactions on Visualization and Computer Graphics.\n  Corresponding Author: Kai Xu (kevin.kai.xu@gmail.com)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view deep neural network is perhaps the most successful approach in 3D\nshape classification. However, the fusion of multi-view features based on max\nor average pooling lacks a view selection mechanism, limiting its application\nin, e.g., multi-view active object recognition by a robot. This paper presents\nVERAM, a recurrent attention model capable of actively selecting a sequence of\nviews for highly accurate 3D shape classification. VERAM addresses an important\nissue commonly found in existing attention-based models, i.e., the unbalanced\ntraining of the subnetworks corresponding to next view estimation and shape\nclassification. The classification subnetwork is easily overfitted while the\nview estimation one is usually poorly trained, leading to a suboptimal\nclassification performance. This is surmounted by three essential\nview-enhancement strategies: 1) enhancing the information flow of gradient\nbackpropagation for the view estimation subnetwork, 2) devising a highly\ninformative reward function for the reinforcement training of view estimation\nand 3) formulating a novel loss function that explicitly circumvents view\nduplication. Taking grayscale image as input and AlexNet as CNN architecture,\nVERAM with 9 views achieves instance-level and class-level accuracy of 95:5%\nand 95:3% on ModelNet10, 93:7% and 92:1% on ModelNet40, both are the\nstate-of-the-art performance under the same number of views.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 21:08:02 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Chen", "Songle", ""], ["Zheng", "Lintao", ""], ["Zhang", "Yan", ""], ["Sun", "Zhixin", ""], ["Xu", "Kai", ""]]}, {"id": "1808.06739", "submitter": "Tianqi Liu", "authors": "Tianqi Liu, Bo Liu", "title": "Constrained-size Tensorflow Models for YouTube-8M Video Understanding\n  Challenge", "comments": "Accepted paper at 2018 ECCV Youtube8M workshop:\n  https://research.google.com/youtube8m/workshop2018/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our 7th place solution to the second YouTube-8M video\nunderstanding competition which challenges participates to build a\nconstrained-size model to classify millions of YouTube videos into thousands of\nclasses. Our final model consists of four single models aggregated into one\ntensorflow graph. For each single model, we use the same network architecture\nas in the winning solution of the first YouTube-8M video understanding\ncompetition, namely Gated NetVLAD. We train the single models separately in\ntensorflow's default float32 precision, then replace weights with float16\nprecision and ensemble them in the evaluation and inference stages., achieving\n48.5% compression rate without loss of precision. Our best model achieved\n88.324% GAP on private leaderboard. The code is publicly available at\nhttps://github.com/boliu61/youtube-8m\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 02:22:44 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 11:50:23 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 15:09:56 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Liu", "Tianqi", ""], ["Liu", "Bo", ""]]}, {"id": "1808.06749", "submitter": "Pei Lv", "authors": "Pei Lv, Shunhua Liu, Mingliang Xu, Bing Zhou", "title": "Abnormal Event Detection and Location for Dense Crowds using Repulsive\n  Forces and Sparse Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method based on repulsive forces and sparse\nreconstruction for the detection and location of abnormal events in crowded\nscenes. In order to avoid the challenging problem of accurately tracking each\nspecific individual in a dense or complex scene, we divide each frame of the\nsurveillance video into a fixed number of grids and select a single\nrepresentative point in each grid as the individual to track. The repulsive\nforce model, which can accurately reflect interactive behaviors of crowds, is\nused to calculate the interactive forces between grid particles in crowded\nscenes and to construct a force flow matrix using these discrete forces from a\nfixed number of continuous frames. The force flow matrix, which contains\nspatial and temporal information, is adopted to train a group of visual\ndictionaries by sparse coding. To further improve the detection efficiency and\navoid concept drift, we propose a fully unsupervised global and local dynamic\nupdating algorithm, based on sparse reconstruction and a group of word pools.\nFor anomaly location, since our method is based on a fixed grid, we can judge\nwhether anomalies occur in a region intuitively according to the reconstruction\nerror of the corresponding visual words. We experimentally verify the proposed\nmethod using the UMN dataset, the UCSD dataset and the Web dataset separately.\nThe results indicate that our method can not only detect abnormal events\naccurately, but can also pinpoint the location of anomalies.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 03:24:00 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Lv", "Pei", ""], ["Liu", "Shunhua", ""], ["Xu", "Mingliang", ""], ["Zhou", "Bing", ""]]}, {"id": "1808.06753", "submitter": "Kejie Qiu", "authors": "Kejie Qiu, Tong Qin, Hongwen Xie, Shaojie Shen", "title": "Estimating Metric Poses of Dynamic Objects Using Monocular\n  Visual-Inertial Fusion", "comments": "IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A monocular 3D object tracking system generally has only up-to-scale pose\nestimation results without any prior knowledge of the tracked object. In this\npaper, we propose a novel idea to recover the metric scale of an arbitrary\ndynamic object by optimizing the trajectory of the objects in the world frame,\nwithout motion assumptions. By introducing an additional constraint in the time\ndomain, our monocular visual-inertial tracking system can obtain continuous six\ndegree of freedom (6-DoF) pose estimation without scale ambiguity. Our method\nrequires neither fixed multi-camera nor depth sensor settings for scale\nobservability, instead, the IMU inside the monocular sensing suite provides\nscale information for both camera itself and the tracked object. We build the\nproposed system on top of our monocular visual-inertial system (VINS) to obtain\naccurate state estimation of the monocular camera in the world frame. The whole\nsystem consists of a 2D object tracker, an object region-based visual bundle\nadjustment (BA), VINS and a correlation analysis-based metric scale estimator.\nExperimental comparisons with ground truth demonstrate the tracking accuracy of\nour 3D tracking performance while a mobile augmented reality (AR) demo shows\nthe feasibility of potential applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 04:04:20 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Qiu", "Kejie", ""], ["Qin", "Tong", ""], ["Xie", "Hongwen", ""], ["Shen", "Shaojie", ""]]}, {"id": "1808.06759", "submitter": "Diego Patino", "authors": "Diego Pati\\~no and Jonathan Avenda\\~no and John Willian Branch", "title": "Automatic skin lesion segmentation on dermoscopic images by the means of\n  superpixel merging", "comments": "MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a superpixel-based strategy for segmenting skin lesion on\ndermoscopic images. The segmentation is carried out by over-segmenting the\noriginal image using the SLIC algorithm, and then merge the resulting\nsuperpixels into two regions: healthy skin and lesion. The mean RGB color of\neach superpixel was used as merging criterion. The presented method is capable\nof dealing with segmentation problems commonly found in dermoscopic images such\nas hair removal, oil bubbles, changes in illumination, and reflections images\nwithout any additional steps. The method was evaluated on the PH2 and ISIC 2017\ndataset with results comparable to the state-of-art.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 04:18:37 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Pati\u00f1o", "Diego", ""], ["Avenda\u00f1o", "Jonathan", ""], ["Branch", "John Willian", ""]]}, {"id": "1808.06801", "submitter": "Yuxin Peng", "authors": "Mingkuan Yuan and Yuxin Peng", "title": "Text-to-image Synthesis via Symmetrical Distillation Networks", "comments": "9 pages, accepted as an oral paper of ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image synthesis aims to automatically generate images according to\ntext descriptions given by users, which is a highly challenging task. The main\nissues of text-to-image synthesis lie in two gaps: the heterogeneous and\nhomogeneous gaps. The heterogeneous gap is between the high-level concepts of\ntext descriptions and the pixel-level contents of images, while the homogeneous\ngap exists between synthetic image distributions and real image distributions.\nFor addressing these problems, we exploit the excellent capability of generic\ndiscriminative models (e.g. VGG19), which can guide the training process of a\nnew generative model on multiple levels to bridge the two gaps. The high-level\nrepresentations can teach the generative model to extract necessary visual\ninformation from text descriptions, which can bridge the heterogeneous gap. The\nmid-level and low-level representations can lead it to learn structures and\ndetails of images respectively, which relieves the homogeneous gap. Therefore,\nwe propose Symmetrical Distillation Networks (SDN) composed of a source\ndiscriminative model as \"teacher\" and a target generative model as \"student\".\nThe target generative model has a symmetrical structure with the source\ndiscriminative model, in order to transfer hierarchical knowledge accessibly.\nMoreover, we decompose the training process into two stages with different\ndistillation paradigms for promoting the performance of the target generative\nmodel. Experiments on two widely-used datasets are conducted to verify the\neffectiveness of our proposed SDN.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 08:54:23 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Yuan", "Mingkuan", ""], ["Peng", "Yuxin", ""]]}, {"id": "1808.06840", "submitter": "Dario Rethage", "authors": "Dario Rethage, Johanna Wald, J\\\"urgen Sturm, Nassir Navab and Federico\n  Tombari", "title": "Fully-Convolutional Point Networks for Large-Scale Point Clouds", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a general-purpose, fully-convolutional network\narchitecture for efficiently processing large-scale 3D data. One striking\ncharacteristic of our approach is its ability to process unorganized 3D\nrepresentations such as point clouds as input, then transforming them\ninternally to ordered structures to be processed via 3D convolutions. In\ncontrast to conventional approaches that maintain either unorganized or\norganized representations, from input to output, our approach has the advantage\nof operating on memory efficient input data representations while at the same\ntime exploiting the natural structure of convolutional operations to avoid the\nredundant computing and storing of spatial information in the network. The\nnetwork eliminates the need to pre- or post process the raw sensor data. This,\ntogether with the fully-convolutional nature of the network, makes it an\nend-to-end method able to process point clouds of huge spaces or even entire\nrooms with up to 200k points at once. Another advantage is that our network can\nproduce either an ordered output or map predictions directly onto the input\ncloud, thus making it suitable as a general-purpose point cloud descriptor\napplicable to many 3D tasks. We demonstrate our network's ability to\neffectively learn both low-level features as well as complex compositional\nrelationships by evaluating it on benchmark datasets for semantic voxel\nsegmentation, semantic part segmentation and 3D scene captioning.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 10:58:57 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Rethage", "Dario", ""], ["Wald", "Johanna", ""], ["Sturm", "J\u00fcrgen", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1808.06843", "submitter": "Dario Rethage", "authors": "Dario Rethage, Federico Tombari, Felix Achilles and Nassir Navab", "title": "Deep Learned Full-3D Object Completion from Single View", "comments": "CVPR 2016 - Scene Understanding Workshop (SUNw)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D geometry is a very informative cue when interacting with and navigating an\nenvironment. This writing proposes a new approach to 3D reconstruction and\nscene understanding, which implicitly learns 3D geometry from depth maps\npairing a deep convolutional neural network architecture with an auto-encoder.\nA data set of synthetic depth views and voxelized 3D representations is built\nbased on ModelNet, a large-scale collection of CAD models, to train networks.\nThe proposed method offers a significant advantage over current, explicit\nreconstruction methods in that it learns key geometric features offline and\nmakes use of those to predict the most probable reconstruction of an unseen\nobject. The relatively small network, consisting of roughly 4 million weights,\nachieves a 92.9% reconstruction accuracy at a 30x30x30 resolution through the\nuse of a pre-trained decompression layer. This is roughly 1/4 the weights of\nthe current leading network. The fast execution time of the model makes it\nsuitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 11:07:08 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Rethage", "Dario", ""], ["Tombari", "Federico", ""], ["Achilles", "Felix", ""], ["Navab", "Nassir", ""]]}, {"id": "1808.06847", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Mingyi Shi, Jing Liao, Dani Lischinski, Baoquan Chen,\n  Daniel Cohen-Or", "title": "Deep Video-Based Performance Cloning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new video-based performance cloning technique. After training a\ndeep generative network using a reference video capturing the appearance and\ndynamics of a target actor, we are able to generate videos where this actor\nreenacts other performances. All of the training data and the driving\nperformances are provided as ordinary video segments, without motion capture or\ndepth information. Our generative model is realized as a deep neural network\nwith two branches, both of which train the same space-time conditional\ngenerator, using shared weights. One branch, responsible for learning to\ngenerate the appearance of the target actor in various poses, uses\n\\emph{paired} training data, self-generated from the reference video. The\nsecond branch uses unpaired data to improve generation of temporally coherent\nvideo renditions of unseen pose sequences. We demonstrate a variety of\npromising results, where our method is able to generate temporally coherent\nvideos, for challenging scenarios where the reference and driving videos\nconsist of very different dance performances. Supplementary video:\nhttps://youtu.be/JpwsEeqNhhA.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 11:20:49 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Aberman", "Kfir", ""], ["Shi", "Mingyi", ""], ["Liao", "Jing", ""], ["Lischinski", "Dani", ""], ["Chen", "Baoquan", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1808.06866", "submitter": "Yang He", "authors": "Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang", "title": "Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks", "comments": "Accepted to IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a Soft Filter Pruning (SFP) method to accelerate the\ninference procedure of deep Convolutional Neural Networks (CNNs). Specifically,\nthe proposed SFP enables the pruned filters to be updated when training the\nmodel after pruning. SFP has two advantages over previous works: (1) Larger\nmodel capacity. Updating previously pruned filters provides our approach with\nlarger optimization space than fixing the filters to zero. Therefore, the\nnetwork trained by our method has a larger model capacity to learn from the\ntraining data. (2) Less dependence on the pre-trained model. Large capacity\nenables SFP to train from scratch and prune the model simultaneously. In\ncontrast, previous filter pruning methods should be conducted on the basis of\nthe pre-trained model to guarantee their performance. Empirically, SFP from\nscratch outperforms the previous filter pruning methods. Moreover, our approach\nhas been demonstrated effective for many advanced CNN architectures. Notably,\non ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2%\ntop-5 accuracy improvement, which has advanced the state-of-the-art. Code is\npublicly available on GitHub: https://github.com/he-y/soft-filter-pruning\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 12:22:38 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["He", "Yang", ""], ["Kang", "Guoliang", ""], ["Dong", "Xuanyi", ""], ["Fu", "Yanwei", ""], ["Yang", "Yi", ""]]}, {"id": "1808.06882", "submitter": "A. Sophia Koepke", "authors": "Olivia Wiles, A. Sophia Koepke, Andrew Zisserman", "title": "Self-supervised learning of a facial attribute embedding from video", "comments": "To appear in BMVC 2018. Supplementary material can be found at\n  http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/fabnet.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised framework for learning facial attributes by\nsimply watching videos of a human face speaking, laughing, and moving over\ntime. To perform this task, we introduce a network, Facial Attributes-Net\n(FAb-Net), that is trained to embed multiple frames from the same video\nface-track into a common low-dimensional space. With this approach, we make\nthree contributions: first, we show that the network can leverage information\nfrom multiple source frames by predicting confidence/attention masks for each\nframe; second, we demonstrate that using a curriculum learning regime improves\nthe learned embedding; finally, we demonstrate that the network learns a\nmeaningful face embedding that encodes information about head pose, facial\nlandmarks and facial expression, i.e. facial attributes, without having been\nsupervised with any labelled data. We are comparable or superior to\nstate-of-the-art self-supervised methods on these tasks and approach the\nperformance of supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 13:01:46 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Wiles", "Olivia", ""], ["Koepke", "A. Sophia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1808.06887", "submitter": "Abhinav Valada", "authors": "Noha Radwan, Wolfram Burgard and Abhinav Valada", "title": "Multimodal Interaction-aware Motion Prediction for Autonomous Street\n  Crossing", "comments": "The International Journal of Robotics Research (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mobile robots navigating on sidewalks, it is essential to be able to\nsafely cross street intersections. Most existing approaches rely on the\nrecognition of the traffic light signal to make an informed crossing decision.\nAlthough these approaches have been crucial enablers for urban navigation, the\ncapabilities of robots employing such approaches are still limited to\nnavigating only on streets containing signalized intersections. In this paper,\nwe address this challenge and propose a multimodal convolutional neural network\nframework to predict the safety of a street intersection for crossing. Our\narchitecture consists of two subnetworks; an interaction-aware trajectory\nestimation stream IA-TCNN, that predicts the future states of all observed\ntraffic participants in the scene, and a traffic light recognition stream\nAtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior\nof the observable dynamic agents in the scene without explicitly assigning\npriorities to the interactions among them. While AtteNet utilizes\nSqueeze-Excitation blocks to learn a content-aware mechanism for selecting the\nrelevant features from the data, thereby improving the noise robustness.\nLearned representations from the traffic light recognition stream are fused\nwith the estimated trajectories from the motion prediction stream to learn the\ncrossing decision. Furthermore, we extend our previously introduced Freiburg\nStreet Crossing dataset with sequences captured at different types of\nintersections, demonstrating complex interactions among the traffic\nparticipants. Extensive experimental evaluations on public benchmark datasets\nand our proposed dataset demonstrate that our network achieves state-of-the-art\nperformance for each of the subtasks, as well as for the crossing safety\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 13:18:27 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 20:27:51 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 13:47:16 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 16:32:54 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 16:41:58 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Radwan", "Noha", ""], ["Burgard", "Wolfram", ""], ["Valada", "Abhinav", ""]]}, {"id": "1808.06914", "submitter": "Shivam Singh", "authors": "Shivam Singh and Stuti Pathak", "title": "Segmentation of Microscopy Data for finding Nuclei in Divergent Images", "comments": "7 pages, 7 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:1807.04459, arXiv:1802.10548, arXiv:1807.10165 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every year millions of people die due to disease of Cancer. Due to its\ninvasive nature it is very complex to cure even in primary stages. Hence, only\nmethod to survive this disease completely is via forecasting by analyzing the\nearly mutation in cells of the patient biopsy. Cell Segmentation can be used to\nfind cell which have left their nuclei. This enables faster cure and high rate\nof survival. Cell counting is a hard, yet tedious task that would greatly\nbenefit from automation. To accomplish this task, segmentation of cells need to\nbe accurate. In this paper, we have improved the learning of training data by\nour network. It can annotate precise masks on test data. we examine the\nstrength of activation functions in medical image segmentation task by\nimproving learning rates by our proposed Carving Technique. Identifying the\ncells nuclei is the starting point for most analyses, identifying nuclei allows\nresearchers to identify each individual cell in a sample, and by measuring how\ncells react to various treatments, the researcher can understand the underlying\nbiological processes at work. Experimental results shows the efficiency of the\nproposed work.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 11:01:48 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 01:45:09 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Singh", "Shivam", ""], ["Pathak", "Stuti", ""]]}, {"id": "1808.07017", "submitter": "Yahya Muhammad", "authors": "Muhammad Yahya, Jawad Ali Shah, Arif Warsi, Kushsairy Kadir, Sheroz\n  Khan, M Izani", "title": "Real Time Elbow Angle Estimation Using Single RGB Camera", "comments": "10 pages, 7 figures, 1 table, jounal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The use of motion capture has increased from last decade in a varied spectrum\nof applications like film special effects, controlling games and robots,\nrehabilitation system, animations etc. The current human motion capture\ntechniques use markers, structured environment, and high resolution cameras in\na dedicated environment. Because of rapid movement, elbow angle estimation is\nobserved as the most difficult problem in human motion capture system. In this\npaper, we take elbow angle estimation as our research subject and propose a\nnovel, markerless and cost-effective solution that uses RGB camera for\nestimating elbow angle in real time using part affinity field. We have\nrecruited five (5) participants to perform cup to mouth movement and at the\nsame time measured the angle by both RGB camera and Microsoft Kinect. The\nexperimental results illustrate that markerless and cost-effective RGB camera\nhas a median RMS errors of 3.06{\\deg} and 0.95{\\deg} in sagittal and coronal\nplane respectively as compared to Microsoft Kinect.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 17:00:42 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Yahya", "Muhammad", ""], ["Shah", "Jawad Ali", ""], ["Warsi", "Arif", ""], ["Kadir", "Kushsairy", ""], ["Khan", "Sheroz", ""], ["Izani", "M", ""]]}, {"id": "1808.07096", "submitter": "Mireille Boutin", "authors": "Haiyin Wang and Mireille Boutin and Jeffrey Trask and Jan Allebach", "title": "Three Efficient, Low-Complexity Algorithms for Automatic Color Trapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color separations (most often cyan, magenta, yellow, and black) are commonly\nused in printing to reproduce multi-color images. For mechanical reasons, these\ncolor separations are generally not perfectly aligned with respect to each\nother when they are rendered by their respective imaging stations. This\nphenomenon, called color plane misregistration, causes gap and halo artifacts\nin the printed image. Color trapping is an image processing technique that aims\nto reduce these artifacts by modifying the susceptible edge boundaries to\ncreate small, unnoticeable overlaps between the color planes. We propose three\nlow-complexity algorithms for automatic color trapping which hide the effects\nof small color plane mis-registrations. Our algorithms are designed for\nsoftware or embedded firmware implementation. The trapping method they follow\nis based on a hardware-friendly technique proposed by J. Trask (JTHBCT03) which\nis too computationally expensive for software or firmware implementation. The\nfirst two algorithms are based on the use of look-up tables (LUTs). The first\nLUT-based algorithm corrects all registration errors of one pixel in extent and\nreduces several cases of misregistration errors of two pixels in extent using\nonly 727 Kbytes of storage space. This algorithm is particularly attractive for\nimplementation in the embedded firmware of low-cost formatter-based printers.\nThe second LUT-based algorithm corrects all types of misregistration errors of\nup to two pixels in extent using 3.7 Mbytes of storage space. The third\nalgorithm is a hybrid one that combines LUTs and feature extraction to minimize\nthe storage requirements (724 Kbytes) while still correcting all\nmisregistration errors of up to two pixels in extent. This algorithm is\nsuitable for both embedded firmware implementation on low-cost formatter-based\nprinters and software implementation on host-based printers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 19:27:39 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Wang", "Haiyin", ""], ["Boutin", "Mireille", ""], ["Trask", "Jeffrey", ""], ["Allebach", "Jan", ""]]}, {"id": "1808.07110", "submitter": "Muneeb Aadil", "authors": "Muneeb Aadil, Rafia Rahim, Sibt ul Hussain", "title": "Improving Super-Resolution Methods via Incremental Residual Learning", "comments": "Accepted at IEEE ICIP'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) have shown promising\nperformance in super-resolution (SR). However, these methods operate primarily\non Low Resolution (LR) inputs for memory efficiency but this limits, as we\ndemonstrate, their ability to (i) model high frequency information; and (ii)\nsmoothly translate from LR to High Resolution (HR) space. To this end, we\npropose a novel Incremental Residual Learning (IRL) framework to address these\nmentioned issues. In IRL, first we select a typical SR pre-trained network as a\nmaster branch. Next we sequentially train and add residual branches to the main\nbranch, where each residual branch is learned to model accumulated residuals of\nall previous branches. We plug state of the art methods in IRL framework and\ndemonstrate consistent performance improvement on public benchmark datasets to\nset a new state of the art for SR at only approximately 20% increase in\ntraining time.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 20:01:07 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 07:21:59 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Aadil", "Muneeb", ""], ["Rahim", "Rafia", ""], ["Hussain", "Sibt ul", ""]]}, {"id": "1808.07150", "submitter": "Matthias Joachim Ehrhardt", "authors": "Matthias J. Ehrhardt, Pawel Markiewicz, Carola-Bibiane Sch\\\"onlieb", "title": "Faster PET Reconstruction with Non-Smooth Priors by Randomization and\n  Preconditioning", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/ab3d07", "report-no": null, "categories": "physics.med-ph cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncompressed clinical data from modern positron emission tomography (PET)\nscanners are very large, exceeding 350 million data points (projection bins).\nThe last decades have seen tremendous advancements in mathematical imaging\ntools many of which lead to non-smooth (i.e. non-differentiable) optimization\nproblems which are much harder to solve than smooth optimization problems. Most\nof these tools have not been translated to clinical PET data, as the\nstate-of-the-art algorithms for non-smooth problems do not scale well to large\ndata. In this work, inspired by big data machine learning applications, we use\nadvanced randomized optimization algorithms to solve the PET reconstruction\nproblem for a very large class of non-smooth priors which includes for example\ntotal variation, total generalized variation, directional total variation and\nvarious different physical constraints. The proposed algorithm randomly uses\nsubsets of the data and only updates the variables associated with these. While\nthis idea often leads to divergent algorithms, we show that the proposed\nalgorithm does indeed converge for any proper subset selection. Numerically, we\nshow on real PET data (FDG and florbetapir) from a Siemens Biograph mMR that\nabout ten projections and backprojections are sufficient to solve the MAP\noptimisation problem related to many popular non-smooth priors; thus showing\nthat the proposed algorithm is fast enough to bring these models into routine\nclinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 22:19:37 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 15:58:10 GMT"}, {"version": "v3", "created": "Fri, 14 Dec 2018 19:19:13 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 17:28:26 GMT"}, {"version": "v5", "created": "Fri, 2 Aug 2019 21:20:52 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ehrhardt", "Matthias J.", ""], ["Markiewicz", "Pawel", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1808.07182", "submitter": "Dylan Drover", "authors": "Dylan Drover, Rohith MV, Ching-Hang Chen, Amit Agrawal, Ambrish Tyagi,\n  and Cong Phuoc Huynh", "title": "Can 3D Pose be Learned from 2D Projections Alone?", "comments": "Appearing in ECCVW 2018 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D pose estimation from a single image is a challenging task in computer\nvision. We present a weakly supervised approach to estimate 3D pose points,\ngiven only 2D pose landmarks. Our method does not require correspondences\nbetween 2D and 3D points to build explicit 3D priors. We utilize an adversarial\nframework to impose a prior on the 3D structure, learned solely from their\nrandom 2D projections. Given a set of 2D pose landmarks, the generator network\nhypothesizes their depths to obtain a 3D skeleton. We propose a novel Random\nProjection layer, which randomly projects the generated 3D skeleton and sends\nthe resulting 2D pose to the discriminator. The discriminator improves by\ndiscriminating between the generated poses and pose samples from a real\ndistribution of 2D poses. Training does not require correspondence between the\n2D inputs to either the generator or the discriminator. We apply our approach\nto the task of 3D human pose estimation. Results on Human3.6M dataset\ndemonstrates that our approach outperforms many previous supervised and weakly\nsupervised approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 01:57:33 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Drover", "Dylan", ""], ["MV", "Rohith", ""], ["Chen", "Ching-Hang", ""], ["Agrawal", "Amit", ""], ["Tyagi", "Ambrish", ""], ["Huynh", "Cong Phuoc", ""]]}, {"id": "1808.07209", "submitter": "Yadan Luo", "authors": "Yadan Luo, Ziwei Wang, Zi Huang, Yang Yang, Cong Zhao", "title": "Coarse-to-Fine Annotation Enrichment for Semantic Segmentation Learning", "comments": "CIKM 2018 International Conference on Information and Knowledge\n  Management", "journal-ref": null, "doi": "10.1145/3269206.3271672", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rich high-quality annotated data is critical for semantic segmentation\nlearning, yet acquiring dense and pixel-wise ground-truth is both labor- and\ntime-consuming. Coarse annotations (e.g., scribbles, coarse polygons) offer an\neconomical alternative, with which training phase could hardly generate\nsatisfactory performance unfortunately. In order to generate high-quality\nannotated data with a low time cost for accurate segmentation, in this paper,\nwe propose a novel annotation enrichment strategy, which expands existing\ncoarse annotations of training data to a finer scale. Extensive experiments on\nthe Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural\nnetworks trained with the enriched annotations from our framework yield a\nsignificant improvement over that trained with the original coarse labels. It\nis highly competitive to the performance obtained by using human annotated\ndense annotations. The proposed method also outperforms among other\nstate-of-the-art weakly-supervised segmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 03:55:33 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Luo", "Yadan", ""], ["Wang", "Ziwei", ""], ["Huang", "Zi", ""], ["Yang", "Yang", ""], ["Zhao", "Cong", ""]]}, {"id": "1808.07256", "submitter": "Karanbir Chahal", "authors": "Karanbir Singh Chahal and Kuntal Dey", "title": "A Survey of Modern Object Detection Literature using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is the identification of an object in the image along with\nits localisation and classification. It has wide spread applications and is a\ncritical component for vision based software systems. This paper seeks to\nperform a rigorous survey of modern object detection algorithms that use deep\nlearning. As part of the survey, the topics explored include various\nalgorithms, quality metrics, speed/size trade offs and training methodologies.\nThis paper focuses on the two types of object detection algorithms- the SSD\nclass of single step detectors and the Faster R-CNN class of two step\ndetectors. Techniques to construct detectors that are portable and fast on low\npowered devices are also addressed by exploring new lightweight convolutional\nbase architectures. Ultimately, a rigorous review of the strengths and\nweaknesses of each detector leads us to the present state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:43:50 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Chahal", "Karanbir Singh", ""], ["Dey", "Kuntal", ""]]}, {"id": "1808.07258", "submitter": "Chieh Hubert Lin", "authors": "Chia-Che Chang, Chieh Hubert Lin, Che-Rung Lee, Da-Cheng Juan, Wei\n  Wei, Hwann-Tzong Chen", "title": "Escaping from Collapsing Modes in a Constrained Space", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) often suffer from unpredictable\nmode-collapsing during training. We study the issue of mode collapse of\nBoundary Equilibrium Generative Adversarial Network (BEGAN), which is one of\nthe state-of-the-art generative models. Despite its potential of generating\nhigh-quality images, we find that BEGAN tends to collapse at some modes after a\nperiod of training. We propose a new model, called \\emph{BEGAN with a\nConstrained Space} (BEGAN-CS), which includes a latent-space constraint in the\nloss function. We show that BEGAN-CS can significantly improve training\nstability and suppress mode collapse without either increasing the model\ncomplexity or degrading the image quality. Further, we visualize the\ndistribution of latent vectors to elucidate the effect of latent-space\nconstraint. The experimental results show that our method has additional\nadvantages of being able to train on small datasets and to generate images\nsimilar to a given real image yet with variations of designated attributes\non-the-fly.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 07:51:22 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Chang", "Chia-Che", ""], ["Lin", "Chieh Hubert", ""], ["Lee", "Che-Rung", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1808.07269", "submitter": "Kazuhiro Terao", "authors": "MicroBooNE collaboration: C. Adams, M. Alrashed, R. An, J. Anthony, J.\n  Asaadi, A. Ashkenazi, M. Auger, S. Balasubramanian, B. Baller, C. Barnes, G.\n  Barr, M. Bass, F. Bay, A. Bhat, K. Bhattacharya, M. Bishai, A. Blake, T.\n  Bolton, L. Camilleri, D. Caratelli, I. Caro Terrazas, R. Carr, R. Castillo\n  Fernandez, F. Cavanna, G. Cerati, Y. Chen, E. Church, D. Cianci, E. Cohen,\n  G.H. Collin, J.M. Conrad, M. Convery, L. Cooper-Troendle, J.I. Crespo-Anadon,\n  M. Del Tutto, D. Devitt, A. Diaz, K. Duffy, S. Dytman, B. Eberly, A.\n  Ereditato, L. Escudero Sanchez, J. Esquivel, J.J. Evans, A.A. Fadeeva, R.S.\n  Fitzpatrick, B.T. Fleming, D. Franco, A.P. Furmanski, D. Garcia-Gamez, G.T.\n  Garvey, V. Genty, D. Goeldi, S. Gollapinni, O. Goodwin, E. Gramellini, H.\n  Greenlee, R. Grosso, R. Guenette, P. Guzowski, A. Hackenburg, P. Hamilton, O.\n  Hen, J. Hewes, C. Hill, G.A. Horton-Smith, A. Hourlier, E.-C. Huang, C.\n  James, J. Jan de Vries, L. Jiang, R.A. Johnson, J. Joshi, H. Jostlein, Y.-J.\n  Jwa, G. Karagiorgi, W. Ketchum, B. Kirby, M. Kirby, T. Kobilarcik, I. Kreslo,\n  Y. Li, A. Lister, B.R. Littlejohn, S. Lockwitz, D. Lorca, W.C. Louis, M.\n  Luethi, B. Lundberg, X. Luo, A. Marchionni, S. Marcocci, C. Mariani, J.\n  Marshall, J. Martin-Albo, D.A. Martinez Caicedo, A. Mastbaum, V. Meddage, T.\n  Mettler, G.B. Mills, K. Mistry, A. Mogan, J. Moon, M. Mooney, C.D. Moore, J.\n  Mousseau, M. Murphy, R. Murrells, D. Naples, P. Nienaber, J. Nowak, O.\n  Palamara, V. Pandey, V. Paolone, A. Papadopoulou, V. Papavassiliou, S.F.\n  Pate, Z. Pavlovic, E. Piasetzky, D. Porzio, G. Pulliam, X. Qian, J.L. Raaf,\n  A. Rafique, L. Rochester, M. Ross-Lonergan, C. Rudolf von Rohr, B. Russell,\n  D.W. Schmitz, A. Schukraft, W. Seligman, M.H. Shaevitz, R. Sharankova, J.\n  Sinclair, A. Smith, E.L. Snider, M. Soderberg, S. Soldner-Rembold, S.R.\n  Soleti, P. Spentzouris, J. Spitz, J. St. John, T. Strauss, K. Sutton, S.\n  Sword-Fehlberg, A.M. Szelc, N. Tagg, W. Tang, K. Terao, M. Thomson, R.T.\n  Thornton, M. Toups, Y.-T. Tsai, S. Tufanli, T. Usher, W. Van De Pontseele,\n  R.G. Van de Water, B. Viren, M. Weber, H. Wei, D.A. Wickremasinghe, K.\n  Wierman, Z. Williams, S. Wolbers, T. Wongjirad, K. Woodruff, T. Yang, G.\n  Yarbrough, L.E. Yates, G.P. Zeller, J. Zennamo, C. Zhang", "title": "A Deep Neural Network for Pixel-Level Electromagnetic Particle\n  Identification in the MicroBooNE Liquid Argon Time Projection Chamber", "comments": null, "journal-ref": "Phys. Rev. D 99, 092001 (2019)", "doi": "10.1103/PhysRevD.99.092001", "report-no": null, "categories": "hep-ex cs.CV physics.data-an physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a convolutional neural network (CNN) that can make a\npixel-level prediction of objects in image data recorded by a liquid argon time\nprojection chamber (LArTPC) for the first time. We describe the network design,\ntraining techniques, and software tools developed to train this network. The\ngoal of this work is to develop a complete deep neural network based data\nreconstruction chain for the MicroBooNE detector. We show the first\ndemonstration of a network's validity on real LArTPC data using MicroBooNE\ncollection plane images. The demonstration is performed for stopping muon and a\n$\\nu_\\mu$ charged current neutral pion data samples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:27:28 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["MicroBooNE collaboration", "", ""], ["Adams", "C.", ""], ["Alrashed", "M.", ""], ["An", "R.", ""], ["Anthony", "J.", ""], ["Asaadi", "J.", ""], ["Ashkenazi", "A.", ""], ["Auger", "M.", ""], ["Balasubramanian", "S.", ""], ["Baller", "B.", ""], ["Barnes", "C.", ""], ["Barr", "G.", ""], ["Bass", "M.", ""], ["Bay", "F.", ""], ["Bhat", "A.", ""], ["Bhattacharya", "K.", ""], ["Bishai", "M.", ""], ["Blake", "A.", ""], ["Bolton", "T.", ""], ["Camilleri", "L.", ""], ["Caratelli", "D.", ""], ["Terrazas", "I. Caro", ""], ["Carr", "R.", ""], ["Fernandez", "R. Castillo", ""], ["Cavanna", "F.", ""], ["Cerati", "G.", ""], ["Chen", "Y.", ""], ["Church", "E.", ""], ["Cianci", "D.", ""], ["Cohen", "E.", ""], ["Collin", "G. H.", ""], ["Conrad", "J. M.", ""], ["Convery", "M.", ""], ["Cooper-Troendle", "L.", ""], ["Crespo-Anadon", "J. I.", ""], ["Del Tutto", "M.", ""], ["Devitt", "D.", ""], ["Diaz", "A.", ""], ["Duffy", "K.", ""], ["Dytman", "S.", ""], ["Eberly", "B.", ""], ["Ereditato", "A.", ""], ["Sanchez", "L. Escudero", ""], ["Esquivel", "J.", ""], ["Evans", "J. J.", ""], ["Fadeeva", "A. A.", ""], ["Fitzpatrick", "R. S.", ""], ["Fleming", "B. T.", ""], ["Franco", "D.", ""], ["Furmanski", "A. P.", ""], ["Garcia-Gamez", "D.", ""], ["Garvey", "G. T.", ""], ["Genty", "V.", ""], ["Goeldi", "D.", ""], ["Gollapinni", "S.", ""], ["Goodwin", "O.", ""], ["Gramellini", "E.", ""], ["Greenlee", "H.", ""], ["Grosso", "R.", ""], ["Guenette", "R.", ""], ["Guzowski", "P.", ""], ["Hackenburg", "A.", ""], ["Hamilton", "P.", ""], ["Hen", "O.", ""], ["Hewes", "J.", ""], ["Hill", "C.", ""], ["Horton-Smith", "G. A.", ""], ["Hourlier", "A.", ""], ["Huang", "E. -C.", ""], ["James", "C.", ""], ["de Vries", "J. Jan", ""], ["Jiang", "L.", ""], ["Johnson", "R. A.", ""], ["Joshi", "J.", ""], ["Jostlein", "H.", ""], ["Jwa", "Y. -J.", ""], ["Karagiorgi", "G.", ""], ["Ketchum", "W.", ""], ["Kirby", "B.", ""], ["Kirby", "M.", ""], ["Kobilarcik", "T.", ""], ["Kreslo", "I.", ""], ["Li", "Y.", ""], ["Lister", "A.", ""], ["Littlejohn", "B. R.", ""], ["Lockwitz", "S.", ""], ["Lorca", "D.", ""], ["Louis", "W. C.", ""], ["Luethi", "M.", ""], ["Lundberg", "B.", ""], ["Luo", "X.", ""], ["Marchionni", "A.", ""], ["Marcocci", "S.", ""], ["Mariani", "C.", ""], ["Marshall", "J.", ""], ["Martin-Albo", "J.", ""], ["Caicedo", "D. A. Martinez", ""], ["Mastbaum", "A.", ""], ["Meddage", "V.", ""], ["Mettler", "T.", ""], ["Mills", "G. B.", ""], ["Mistry", "K.", ""], ["Mogan", "A.", ""], ["Moon", "J.", ""], ["Mooney", "M.", ""], ["Moore", "C. D.", ""], ["Mousseau", "J.", ""], ["Murphy", "M.", ""], ["Murrells", "R.", ""], ["Naples", "D.", ""], ["Nienaber", "P.", ""], ["Nowak", "J.", ""], ["Palamara", "O.", ""], ["Pandey", "V.", ""], ["Paolone", "V.", ""], ["Papadopoulou", "A.", ""], ["Papavassiliou", "V.", ""], ["Pate", "S. F.", ""], ["Pavlovic", "Z.", ""], ["Piasetzky", "E.", ""], ["Porzio", "D.", ""], ["Pulliam", "G.", ""], ["Qian", "X.", ""], ["Raaf", "J. L.", ""], ["Rafique", "A.", ""], ["Rochester", "L.", ""], ["Ross-Lonergan", "M.", ""], ["von Rohr", "C. Rudolf", ""], ["Russell", "B.", ""], ["Schmitz", "D. W.", ""], ["Schukraft", "A.", ""], ["Seligman", "W.", ""], ["Shaevitz", "M. H.", ""], ["Sharankova", "R.", ""], ["Sinclair", "J.", ""], ["Smith", "A.", ""], ["Snider", "E. L.", ""], ["Soderberg", "M.", ""], ["Soldner-Rembold", "S.", ""], ["Soleti", "S. R.", ""], ["Spentzouris", "P.", ""], ["Spitz", "J.", ""], ["John", "J. St.", ""], ["Strauss", "T.", ""], ["Sutton", "K.", ""], ["Sword-Fehlberg", "S.", ""], ["Szelc", "A. M.", ""], ["Tagg", "N.", ""], ["Tang", "W.", ""], ["Terao", "K.", ""], ["Thomson", "M.", ""], ["Thornton", "R. T.", ""], ["Toups", "M.", ""], ["Tsai", "Y. -T.", ""], ["Tufanli", "S.", ""], ["Usher", "T.", ""], ["Van De Pontseele", "W.", ""], ["Van de Water", "R. G.", ""], ["Viren", "B.", ""], ["Weber", "M.", ""], ["Wei", "H.", ""], ["Wickremasinghe", "D. A.", ""], ["Wierman", "K.", ""], ["Williams", "Z.", ""], ["Wolbers", "S.", ""], ["Wongjirad", "T.", ""], ["Woodruff", "K.", ""], ["Yang", "T.", ""], ["Yarbrough", "G.", ""], ["Yates", "L. E.", ""], ["Zeller", "G. P.", ""], ["Zennamo", "J.", ""], ["Zhang", "C.", ""]]}, {"id": "1808.07272", "submitter": "Sibo Song", "authors": "Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal", "title": "Deep Adaptive Temporal Pooling for Activity Recognition", "comments": "Accepted by ACM Multimedia 2018", "journal-ref": null, "doi": "10.1145/3240508.3240713", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved competitive accuracy for human\nactivity recognition. However, there is room for improvement, especially in\nmodeling long-term temporal importance and determining the activity relevance\nof different temporal segments in a video. To address this problem, we propose\na learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP).\nDATP applies a self-attention mechanism to adaptively pool the classification\nscores of different video segments. Specifically, using frame-level features,\nDATP regresses importance of different temporal segments and generates weights\nfor them. Remarkably, DATP is trained using only the video-level label. There\nis no need of additional supervision except video-level activity class label.\nWe conduct extensive experiments to investigate various input features and\ndifferent weight models. Experimental results show that DATP can learn to\nassign large weights to key video segments. More importantly, DATP can improve\ntraining of frame-level feature extractor. This is because relevant temporal\nsegments are assigned large weights during back-propagation. Overall, we\nachieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:29:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Song", "Sibo", ""], ["Cheung", "Ngai-Man", ""], ["Chandrasekhar", "Vijay", ""], ["Mandal", "Bappaditya", ""]]}, {"id": "1808.07275", "submitter": "Valentin Vielzeuf", "authors": "Valentin Vielzeuf, Alexis Lechervy, St\\'ephane Pateux, Fr\\'ed\\'eric\n  Jurie", "title": "CentralNet: a Multilayer Approach for Multimodal Fusion", "comments": null, "journal-ref": "European Conference on Computer Vision Workshops: Multimodal\n  Learning and Applications, Sep 2018, Munich, Germany.\n  https://mula2018.github.io/", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel multimodal fusion approach, aiming to produce\nbest possible decisions by integrating information coming from multiple media.\nWhile most of the past multimodal approaches either work by projecting the\nfeatures of different modalities into the same space, or by coordinating the\nrepresentations of each modality through the use of constraints, our approach\nborrows from both visions. More specifically, assuming each modality can be\nprocessed by a separated deep convolutional network, allowing to take decisions\nindependently from each modality, we introduce a central network linking the\nmodality specific networks. This central network not only provides a common\nfeature embedding but also regularizes the modality specific networks through\nthe use of multi-task learning. The proposed approach is validated on 4\ndifferent computer vision tasks on which it consistently improves the accuracy\nof existing multimodal fusion approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:37:55 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Lechervy", "Alexis", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1808.07277", "submitter": "Wassim Swaileh", "authors": "Wassim Swaileh and Thierry Paquet", "title": "A syllable based model for handwriting recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new modeling approach of texts for handwriting\nrecognition based on syllables. We propose a supervised syllabification\napproach for the French and English languages for building a vocabulary of\nsyllables. Statistical n-gram language models of syllables are trained on\nFrench and English Wikipedia corpora. The handwriting recognition system, based\non optical HMM context independent character models, performs a two pass\ndecoding, integrating the proposed syllabic models. Evaluation is carried out\non the French RIMES dataset and English IAM dataset by analyzing the\nperformance for various coverage of the syllable models. We also compare the\nsyllable models with lexicon and character n-gram models. The proposed approach\nreaches interesting performances thanks to its capacity to cover a large amount\nof out of vocabulary words working with a limited amount of syllables combined\nwith statistical n-gram of reasonable order.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 08:45:43 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Swaileh", "Wassim", ""], ["Paquet", "Thierry", ""]]}, {"id": "1808.07301", "submitter": "Yanbei Chen", "authors": "Yanbei Chen, Xiatian Zhu and Shaogang Gong", "title": "Deep Association Learning for Unsupervised Video Person\n  Re-identification", "comments": "Accepted by BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have started to dominate the research progress of\nvideo-based person re-identification (re-id). However, existing methods mostly\nconsider supervised learning, which requires exhaustive manual efforts for\nlabelling cross-view pairwise data. Therefore, they severely lack scalability\nand practicality in real-world video surveillance applications. In this work,\nto address the video person re-id task, we formulate a novel Deep Association\nLearning (DAL) scheme, the first end-to-end deep learning method using none of\nthe identity labels in model initialisation and training. DAL learns a deep\nre-id matching model by jointly optimising two margin-based association losses\nin an end-to-end manner, which effectively constrains the association of each\nframe to the best-matched intra-camera representation and cross-camera\nrepresentation. Existing standard CNNs can be readily employed within our DAL\nscheme. Experiment results demonstrate that our proposed DAL significantly\noutperforms current state-of-the-art unsupervised video person re-id methods on\nthree benchmarks: PRID 2011, iLIDS-VID and MARS.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 10:16:43 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Chen", "Yanbei", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1808.07330", "submitter": "Muktabh Mayank Srivastava", "authors": "Pranaydeep Singh, Srikrishna Varadarajan, Ankit Narayan Singh, Muktabh\n  Mayank Srivastava", "title": "Multidomain Document Layout Understanding using Few Shot Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We try to address the problem of document layout understanding using a simple\nalgorithm which generalizes across multiple domains while training on just few\nexamples per domain. We approach this problem via supervised object detection\nmethod and propose a methodology to overcome the requirement of large datasets.\nWe use the concept of transfer learning by pre-training our object detector on\na simple artificial (source) dataset and fine-tuning it on a tiny domain\nspecific (target) dataset. We show that this methodology works for multiple\ndomains with training samples as less as 10 documents. We demonstrate the\neffect of each component of the methodology in the end result and show the\nsuperiority of this methodology over simple object detectors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 12:23:51 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Singh", "Pranaydeep", ""], ["Varadarajan", "Srikrishna", ""], ["Singh", "Ankit Narayan", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1808.07334", "submitter": "Thomas Gietzen", "authors": "Thomas Gietzen and Robert Brylka and Jascha Achenbach and Katja zum\n  Hebel and Elmar Sch\\\"omer and Mario Botsch and Ulrich Schwanecke and Ralf\n  Schulze", "title": "A method for automatic forensic facial reconstruction based on dense\n  statistics of soft tissue thickness", "comments": "19 pages, 12 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0210257", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for automated estimation of a human face\ngiven a skull remain. The proposed method is based on three statistical models.\nA volumetric (tetrahedral) skull model encoding the variations of different\nskulls, a surface head model encoding the head variations, and a dense\nstatistic of facial soft tissue thickness (FSTT). All data are automatically\nderived from computed tomography (CT) head scans and optical face scans. In\norder to obtain a proper dense FSTT statistic, we register a skull model to\neach skull extracted from a CT scan and determine the FSTT value for each\nvertex of the skull model towards the associated extracted skin surface. The\nFSTT values at predefined landmarks from our statistic are well in agreement\nwith data from the literature.\n  To recover a face from a skull remain, we first fit our skull model to the\ngiven skull. Next, we generate spheres with radius of the respective FSTT value\nobtained from our statistic at each vertex of the registered skull. Finally, we\nfit a head model to the union of all spheres. The proposed automated method\nenables a probabilistic face-estimation that facilitates forensic recovery even\nfrom incomplete skull remains. The FSTT statistic allows the generation of\nplausible head variants, which can be adjusted intuitively using principal\ncomponent analysis. We validate our face recovery process using an anonymized\nhead CT scan. The estimation generated from the given skull visually compares\nwell with the skin surface extracted from the CT scan itself.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 12:43:51 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Gietzen", "Thomas", ""], ["Brylka", "Robert", ""], ["Achenbach", "Jascha", ""], ["Hebel", "Katja zum", ""], ["Sch\u00f6mer", "Elmar", ""], ["Botsch", "Mario", ""], ["Schwanecke", "Ulrich", ""], ["Schulze", "Ralf", ""]]}, {"id": "1808.07349", "submitter": "Zhenxi Li", "authors": "Zhenxi Li, Guillaume-Alexandre Bilodeau and Wassim Bouachir", "title": "Multi-Branch Siamese Networks with Online Selection for Object Tracking", "comments": "ISVC2018, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust object tracking algorithm based on a\nbranch selection mechanism to choose the most efficient object representations\nfrom multi-branch siamese networks. While most deep learning trackers use a\nsingle CNN for target representation, the proposed Multi-Branch Siamese Tracker\n(MBST) employs multiple branches of CNNs pre-trained for different tasks, and\nused for various target representations in our tracking method. With our branch\nselection mechanism, the appropriate CNN branch is selected depending on the\ntarget characteristics in an online manner. By using the most adequate target\nrepresentation with respect to the tracked object, our method achieves\nreal-time tracking, while obtaining improved performance compared to standard\nSiamese network trackers on object tracking benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 13:30:22 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 12:05:05 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 20:29:27 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Zhenxi", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bouachir", "Wassim", ""]]}, {"id": "1808.07366", "submitter": "Renata Khasanova", "authors": "Renata Khasanova and Pascal Frossard", "title": "Isometric Transformation Invariant Graph-based Deep Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.00356", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning transformation invariant representations of visual data is an\nimportant problem in computer vision. Deep convolutional networks have\ndemonstrated remarkable results for image and video classification tasks.\nHowever, they have achieved only limited success in the classification of\nimages that undergo geometric transformations. In this work we present a novel\nTransformation Invariant Graph-based Network (TIGraNet), which learns\ngraph-based features that are inherently invariant to isometric transformations\nsuch as rotation and translation of input images. In particular, images are\nrepresented as signals on graphs, which permits to replace classical\nconvolution and pooling layers in deep networks with graph spectral convolution\nand dynamic graph pooling layers that together contribute to invariance to\nisometric transformation. Our experiments show high performance on rotated and\ntranslated images from the test set compared to classical architectures that\nare very sensitive to transformations in the data. The inherent invariance\nproperties of our framework provide key advantages, such as increased\nresiliency to data variability and sustained performance with limited training\nsets. Our code is available online.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:32:52 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Khasanova", "Renata", ""], ["Frossard", "Pascal", ""]]}, {"id": "1808.07371", "submitter": "Caroline Chan", "authors": "Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros", "title": "Everybody Dance Now", "comments": "In ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple method for \"do as I do\" motion transfer: given a\nsource video of a person dancing, we can transfer that performance to a novel\n(amateur) target after only a few minutes of the target subject performing\nstandard moves. We approach this problem as video-to-video translation using\npose as an intermediate representation. To transfer the motion, we extract\nposes from the source subject and apply the learned pose-to-appearance mapping\nto generate the target subject. We predict two consecutive frames for\ntemporally coherent video results and introduce a separate pipeline for\nrealistic face synthesis. Although our method is quite simple, it produces\nsurprisingly compelling results (see video). This motivates us to also provide\na forensics tool for reliable synthetic content detection, which is able to\ndistinguish videos synthesized by our system from real data. In addition, we\nrelease a first-of-its-kind open-source dataset of videos that can be legally\nused for training and motion transfer.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 13:58:36 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 21:10:54 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chan", "Caroline", ""], ["Ginosar", "Shiry", ""], ["Zhou", "Tinghui", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1808.07413", "submitter": "Aykut Erdem", "authors": "Levent Karacan, Zeynep Akata, Aykut Erdem, Erkut Erdem", "title": "Manipulating Attributes of Natural Scenes via Hallucination", "comments": "Accepted for publication in ACM Transactions on Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we explore building a two-stage framework for enabling users\nto directly manipulate high-level attributes of a natural scene. The key to our\napproach is a deep generative network which can hallucinate images of a scene\nas if they were taken at a different season (e.g. during winter), weather\ncondition (e.g. in a cloudy day) or time of the day (e.g. at sunset). Once the\nscene is hallucinated with the given attributes, the corresponding look is then\ntransferred to the input image while preserving the semantic details intact,\ngiving a photo-realistic manipulation result. As the proposed framework\nhallucinates what the scene will look like, it does not require any reference\nstyle image as commonly utilized in most of the appearance or style transfer\napproaches. Moreover, it allows to simultaneously manipulate a given scene\naccording to a diverse set of transient attributes within a single model,\neliminating the need of training multiple networks per each translation task.\nOur comprehensive set of qualitative and quantitative results demonstrate the\neffectiveness of our approach against the competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 16:01:18 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 09:52:05 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 06:18:45 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Karacan", "Levent", ""], ["Akata", "Zeynep", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""]]}, {"id": "1808.07416", "submitter": "Victor Vaquero", "authors": "Victor Vaquero, German Ros, Francesc Moreno-Noguer, Antonio M. Lopez,\n  Alberto Sanfeliu", "title": "Joint Coarse-And-Fine Reasoning for Deep Optical Flow", "comments": "Accepted in IEEE ICIP 2017. IEEE Copyrights: Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses", "journal-ref": null, "doi": "10.1109/ICIP.2017.8296744", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel representation for dense pixel-wise estimation tasks using\nCNNs that boosts accuracy and reduces training time, by explicitly exploiting\njoint coarse-and-fine reasoning. The coarse reasoning is performed over a\ndiscrete classification space to obtain a general rough solution, while the\nfine details of the solution are obtained over a continuous regression space.\nIn our approach both components are jointly estimated, which proved to be\nbeneficial for improving estimation accuracy. Additionally, we propose a new\nnetwork architecture, which combines coarse and fine components by treating the\nfine estimation as a refinement built on top of the coarse solution, and\ntherefore adding details to the general prediction. We apply our approach to\nthe challenging problem of optical flow estimation and empirically validate it\nagainst state-of-the-art CNN-based solutions trained from scratch and tested on\nlarge optical flow datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 16:07:11 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Vaquero", "Victor", ""], ["Ros", "German", ""], ["Moreno-Noguer", "Francesc", ""], ["Lopez", "Antonio M.", ""], ["Sanfeliu", "Alberto", ""]]}, {"id": "1808.07456", "submitter": "Siyu Huang", "authors": "Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann", "title": "Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance", "comments": "The code is available at\n  http://github.com/siyuhuang/crowdcount-stackpool", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the cross-scale similarity in crowd counting\nscenario, in which the regions of different scales often exhibit high visual\nsimilarity. This feature is universal both within an image and across different\nimages, indicating the importance of scale invariance of a crowd counting\nmodel. Motivated by this, in this paper we propose simple but effective\nvariants of pooling module, i.e., multi-kernel pooling and stacked pooling, to\nboost the scale invariance of convolutional neural networks (CNNs), benefiting\nmuch the crowd density estimation and counting. Specifically, the multi-kernel\npooling comprises of pooling kernels with multiple receptive fields to capture\nthe responses at multi-scale local ranges. The stacked pooling is an equivalent\nform of multi-kernel pooling, while, it reduces considerable computing cost.\nOur proposed pooling modules do not introduce extra parameters into model and\ncan easily take place of the vanilla pooling layer in implementation. In\nempirical study on two benchmark crowd counting datasets, the stacked pooling\nbeats the vanilla pooling layer in most cases.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 17:46:06 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Huang", "Siyu", ""], ["Li", "Xi", ""], ["Cheng", "Zhi-Qi", ""], ["Zhang", "Zhongfei", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1808.07471", "submitter": "Yang He", "authors": "Yang He, Xuanyi Dong, Guoliang Kang, Yanwei Fu, Chenggang Yan, and Yi\n  Yang", "title": "Asymptotic Soft Filter Pruning for Deep Convolutional Neural Networks", "comments": "Extended Journal Version of arXiv:1808.06866", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deeper and wider Convolutional Neural Networks (CNNs) achieve superior\nperformance but bring expensive computation cost. Accelerating such\nover-parameterized neural network has received increased attention. A typical\npruning algorithm is a three-stage pipeline, i.e., training, pruning, and\nretraining. Prevailing approaches fix the pruned filters to zero during\nretraining, and thus significantly reduce the optimization space. Besides, they\ndirectly prune a large number of filters at first, which would cause\nunrecoverable information loss. To solve these problems, we propose an\nAsymptotic Soft Filter Pruning (ASFP) method to accelerate the inference\nprocedure of the deep neural networks. First, we update the pruned filters\nduring the retraining stage. As a result, the optimization space of the pruned\nmodel would not be reduced but be the same as that of the original model. In\nthis way, the model has enough capacity to learn from the training data.\nSecond, we prune the network asymptotically. We prune few filters at first and\nasymptotically prune more filters during the training procedure. With\nasymptotic pruning, the information of the training set would be gradually\nconcentrated in the remaining filters, so the subsequent training and pruning\nprocess would be stable. Experiments show the effectiveness of our ASFP on\nimage classification benchmarks. Notably, on ILSVRC-2012, our ASFP reduces more\nthan 40% FLOPs on ResNet-50 with only 0.14% top-5 accuracy degradation, which\nis higher than the soft filter pruning (SFP) by 8%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 00:32:41 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 00:55:45 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 10:01:41 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2019 02:10:29 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["He", "Yang", ""], ["Dong", "Xuanyi", ""], ["Kang", "Guoliang", ""], ["Fu", "Yanwei", ""], ["Yan", "Chenggang", ""], ["Yang", "Yi", ""]]}, {"id": "1808.07503", "submitter": "Tsung-Yu Lin", "authors": "Tsung-Yu Lin, Subhransu Maji, Piotr Koniusz", "title": "Second-order Democratic Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated second-order features extracted from deep convolutional networks\nhave been shown to be effective for texture generation, fine-grained\nrecognition, material classification, and scene understanding. In this paper,\nwe study a class of orderless aggregation functions designed to minimize\ninterference or equalize contributions in the context of second-order features\nand we show that they can be computed just as efficiently as their first-order\ncounterparts and they have favorable properties over aggregation by summation.\nAnother line of work has shown that matrix power normalization after\naggregation can significantly improve the generalization of second-order\nrepresentations. We show that matrix power normalization implicitly equalizes\ncontributions during aggregation thus establishing a connection between matrix\nnormalization techniques and prior work on minimizing interference. Based on\nthe analysis we present {\\gamma}-democratic aggregators that interpolate\nbetween sum ({\\gamma}=1) and democratic pooling ({\\gamma}=0) outperforming both\non several classification tasks. Moreover, unlike power normalization, the\n{\\gamma}-democratic aggregations can be computed in a low dimensional space by\nsketching that allows the use of very high-dimensional second-order features.\nThis results in a state-of-the-art performance on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 18:07:26 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Lin", "Tsung-Yu", ""], ["Maji", "Subhransu", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1808.07507", "submitter": "Unaiza Ahsan", "authors": "Unaiza Ahsan, Rishi Madhok and Irfan Essa", "title": "Video Jigsaw: Unsupervised Learning of Spatiotemporal Context for Video\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised learning method to jointly reason about spatial\nand temporal context for video recognition. Recent self-supervised approaches\nhave used spatial context [9, 34] as well as temporal coherency [32] but a\ncombination of the two requires extensive preprocessing such as tracking\nobjects through millions of video frames [59] or computing optical flow to\ndetermine frame regions with high motion [30]. We propose to combine spatial\nand temporal context in one self-supervised framework without any heavy\npreprocessing. We divide multiple video frames into grids of patches and train\na network to solve jigsaw puzzles on these patches from multiple frames. So the\nnetwork is trained to correctly identify the position of a patch within a video\nframe as well as the position of a patch over time. We also propose a novel\npermutation strategy that outperforms random permutations while significantly\nreducing computational and memory constraints. We use our trained network for\ntransfer learning tasks such as video activity recognition and demonstrate the\nstrength of our approach on two benchmark video action recognition datasets\nwithout using a single frame from these datasets for unsupervised pretraining\nof our proposed video jigsaw network.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 18:18:44 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Ahsan", "Unaiza", ""], ["Madhok", "Rishi", ""], ["Essa", "Irfan", ""]]}, {"id": "1808.07518", "submitter": "Iljoo Baek", "authors": "Iljoo Baek, Mengwen He", "title": "Vehicles Lane-changing Behavior Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The lane-level localization accuracy is very important for autonomous\nvehicles. The Global Navigation Satellite System (GNSS), e.g. GPS, is a generic\nlocalization method for vehicles, but is vulnerable to the multi-path\ninterference in the urban environment. Integrating the vision-based relative\nlocalization result and a digital map with the GNSS is a common and cheap way\nto increase the global localization accuracy and thus to realize the lane-level\nlocalization. This project is to develop a mono-camera based lane-changing\nbehavior detection module for the correction of lateral GPS localization. We\nimplemented a Support Vector Machine (SVM) based framework to directly classify\nthe driving behavior, including the lane keeping, left and right lane changing,\nfrom a sampled data of the raw image captured by the mono-camera installed\nbehind the window shield. The training data was collected from the driving\naround Carnegie Mellon University, and we compared the trained SVM models w/\nand w/o the Principle Component Analysis (PCA) dimension reduction technique.\nThe performance of the SVM based classification method was compared with the\nCNN method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 18:37:10 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Baek", "Iljoo", ""], ["He", "Mengwen", ""]]}, {"id": "1808.07528", "submitter": "Richard Chen", "authors": "Richard Chen, Faisal Mahmood, Alan Yuille, Nicholas J. Durr", "title": "Rethinking Monocular Depth Estimation with Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is an extensively studied computer vision problem\nwith a vast variety of applications. Deep learning-based methods have\ndemonstrated promise for both supervised and unsupervised depth estimation from\nmonocular images. Most existing approaches treat depth estimation as a\nregression problem with a local pixel-wise loss function. In this work, we\ninnovate beyond existing approaches by using adversarial training to learn a\ncontext-aware, non-local loss function. Such an approach penalizes the joint\nconfiguration of predicted depth values at the patch-level instead of the\npixel-level, which allows networks to incorporate more global information. In\nthis framework, the generator learns a mapping between RGB images and its\ncorresponding depth map, while the discriminator learns to distinguish depth\nmap and RGB pairs from ground truth. This conditional GAN depth estimation\nframework is stabilized using spectral normalization to prevent mode collapse\nwhen learning from diverse datasets. We test this approach using a diverse set\nof generators that include U-Net and joint CNN-CRF. We benchmark this approach\non the NYUv2, Make3D and KITTI datasets, and observe that adversarial training\nreduces relative error by several fold, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 19:11:41 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 06:54:44 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 18:37:26 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Chen", "Richard", ""], ["Mahmood", "Faisal", ""], ["Yuille", "Alan", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1808.07535", "submitter": "Seunghoon Hong", "authors": "Seunghoon Hong and Xinchen Yan and Thomas Huang and Honglak Lee", "title": "Learning Hierarchical Semantic Image Manipulation through Structured\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding, reasoning, and manipulating semantic concepts of images have\nbeen a fundamental research problem for decades. Previous work mainly focused\non direct manipulation on natural image manifold through color strokes,\nkey-points, textures, and holes-to-fill. In this work, we present a novel\nhierarchical framework for semantic image manipulation. Key to our hierarchical\nframework is that we employ a structured semantic layout as our intermediate\nrepresentation for manipulation. Initialized with coarse-level bounding boxes,\nour structure generator first creates pixel-wise semantic layout capturing the\nobject shape, object-object interactions, and object-scene relations. Then our\nimage generator fills in the pixel-level textures guided by the semantic\nlayout. Such framework allows a user to manipulate images at object-level by\nadding, removing, and moving one bounding box at a time. Experimental\nevaluations demonstrate the advantages of the hierarchical manipulation\nframework over existing image generation and context hole-filing models, both\nqualitatively and quantitatively. Benefits of the hierarchical framework are\nfurther demonstrated in applications such as semantic object manipulation,\ninteractive image editing, and data-driven image manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 19:33:31 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 00:33:27 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Hong", "Seunghoon", ""], ["Yan", "Xinchen", ""], ["Huang", "Thomas", ""], ["Lee", "Honglak", ""]]}, {"id": "1808.07553", "submitter": "Yi Hong", "authors": "Sharmin Pathan and Yi Hong", "title": "Predictive Image Regression for Longitudinal Studies with Missing Data", "comments": "1st Conference on Medical Imaging with Deep Learning (MIDL 2018),\n  Amsterdam, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a predictive regression model for longitudinal\nimages with missing data based on large deformation diffeomorphic metric\nmapping (LDDMM) and deep neural networks. Instead of directly predicting image\nscans, our model predicts a vector momentum sequence associated with a baseline\nimage. This momentum sequence parameterizes the original image sequence in the\nLDDMM framework and lies in the tangent space of the baseline image, which is\nEuclidean. A recurrent network with long term-short memory (LSTM) units encodes\nthe time-varying changes in the vector-momentum sequence, and a convolutional\nneural network (CNN) encodes the baseline image of the vector momenta. Features\nextracted by the LSTM and CNN are fed into a decoder network to reconstruct the\nvector momentum sequence, which is used for the image sequence prediction by\ndeforming the baseline image with LDDMM shooting. To handle the missing images\nat some time points, we adopt a binary mask to ignore their reconstructions in\nthe loss calculation. We evaluate our model on synthetically generated images\nand the brain MRIs from the OASIS dataset. Experimental results demonstrate the\npromising predictions of the spatiotemporal changes in both datasets,\nirrespective of large or subtle changes in longitudinal image sequences.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 20:31:54 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Pathan", "Sharmin", ""], ["Hong", "Yi", ""]]}, {"id": "1808.07592", "submitter": "Nathan J. Olliverre", "authors": "Nathan J Olliverre, Guang Yang, Gregory Slabaugh, Constantino Carlos\n  Reyes-Aldasoro and Eduardo Alonso", "title": "Generating Magnetic Resonance Spectroscopy Imaging Data of Brain Tumours\n  from Linear, Non-Linear and Deep Learning Models", "comments": "9 pages, 4 figures, 2 tables, to be presented at Simulation and\n  Synthesis in Medical Imaging (SASHIMI) 2018 and published in the book\n  Simulation and Synthesis in Medical Imaging, Lecture Notes in Computer\n  Science series, Volume 11037", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Magnetic Resonance Spectroscopy (MRS) provides valuable information to help\nwith the identification and understanding of brain tumors, yet MRS is not a\nwidely available medical imaging modality. Aiming to counter this issue, this\nresearch draws on the advancements in machine learning techniques in other\nfields for the generation of artificial data. The generated methods were tested\nthrough the evaluation of their output against that of a real-world labelled\nMRS brain tumor data-set. Furthermore the resultant output from the generative\ntechniques were each used to train separate traditional classifiers which were\ntested on a subset of the real MRS brain tumor dataset. The results suggest\nthat there exist methods capable of producing accurate, ground truth based MRS\nvoxels. These findings indicate that through generative techniques, large\ndatasets can be made available for training deep, learning models for the use\nin brain tumor diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 00:02:31 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Olliverre", "Nathan J", ""], ["Yang", "Guang", ""], ["Slabaugh", "Gregory", ""], ["Reyes-Aldasoro", "Constantino Carlos", ""], ["Alonso", "Eduardo", ""]]}, {"id": "1808.07659", "submitter": "Haoxuan You", "authors": "Haoxuan You, Yifan Feng, Rongrong Ji, Yue Gao", "title": "PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for\n  3D Shape Recognition", "comments": "ACM Multimedia Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object recognition has attracted wide research attention in the field of\nmultimedia and computer vision. With the recent proliferation of deep learning,\nvarious deep models with different representations have achieved the\nstate-of-the-art performance. Among them, point cloud and multi-view based 3D\nshape representations are promising recently, and their corresponding deep\nmodels have shown significant performance on 3D shape recognition. However,\nthere is little effort concentrating point cloud data and multi-view data for\n3D shape representation, which is, in our consideration, beneficial and\ncompensated to each other. In this paper, we propose the Point-View Network\n(PVNet), the first framework integrating both the point cloud and the\nmulti-view data towards joint 3D shape recognition. More specifically, an\nembedding attention fusion scheme is proposed that could employ high-level\nfeatures from the multi-view data to model the intrinsic correlation and\ndiscriminability of different structure features from the point cloud data. In\nparticular, the discriminative descriptions are quantified and leveraged as the\nsoft attention mask to further refine the structure feature of the 3D shape. We\nhave evaluated the proposed method on the ModelNet40 dataset for 3D shape\nclassification and retrieval tasks. Experimental results and comparisons with\nstate-of-the-art methods demonstrate that our framework can achieve superior\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 08:10:08 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["You", "Haoxuan", ""], ["Feng", "Yifan", ""], ["Ji", "Rongrong", ""], ["Gao", "Yue", ""]]}, {"id": "1808.07675", "submitter": "Devis Tuia", "authors": "Michele Volpi, Devis Tuia", "title": "Deep multi-task learning for a geographically-regularized semantic\n  segmentation of aerial images", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 144, pages\n  48-60, 2018", "doi": "10.1016/j.isprsjprs.2018.06.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When approaching the semantic segmentation of overhead imagery in the\ndecimeter spatial resolution range, successful strategies usually combine\npowerful methods to learn the visual appearance of the semantic classes (e.g.\nconvolutional neural networks) with strategies for spatial regularization (e.g.\ngraphical models such as conditional random fields). In this paper, we propose\na method to learn evidence in the form of semantic class likelihoods, semantic\nboundaries across classes and shallow-to-deep visual features, each one modeled\nby a multi-task convolutional neural network architecture. We combine this\nbottom-up information with top-down spatial regularization encoded by a\nconditional random field model optimizing the label space across a hierarchy of\nsegments with constraints related to structural, spatial and data-dependent\npairwise relationships between regions. Our results show that such strategy\nprovide better regularization than a series of strong baselines reflecting\nstate-of-the-art technologies. The proposed strategy offers a flexible and\nprincipled framework to include several sources of visual and structural\ninformation, while allowing for different degrees of spatial regularization\naccounting for priors about the expected output structures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 09:41:47 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Volpi", "Michele", ""], ["Tuia", "Devis", ""]]}, {"id": "1808.07703", "submitter": "Petra Bevandi\\'c", "authors": "Petra Bevandi\\'c, Ivan Kre\\v{s}o, Marin Or\\v{s}i\\'c, Sini\\v{s}a\n  \\v{S}egvi\\'c", "title": "Discriminative out-of-distribution detection for semantic segmentation", "comments": "This paper has been withdrawn from AutoNUE workshop at ECCV 2018 due\n  to ECCV registration being closed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification and segmentation datasets assume a closed-world scenario\nin which predictions are expressed as distribution over a predetermined set of\nvisual classes. However, such assumption implies unavoidable and often\nunnoticeable failures in presence of out-of-distribution (OOD) input. These\nfailures are bound to happen in most real-life applications since current\nvisual ontologies are far from being comprehensive. We propose to address this\nissue by discriminative detection of OOD pixels in input data. Different from\nrecent approaches, we avoid to bring any decisions by only observing the\ntraining dataset of the primary model trained to solve the desired computer\nvision task. Instead, we train a dedicated OOD model which discriminates the\nprimary training set from a much larger \"background\" dataset which approximates\nthe variety of the visual world. We perform our experiments on high resolution\nnatural images in a dense prediction setup. We use several road driving\ndatasets as our training distribution, while we approximate the background\ndistribution with the ILSVRC dataset. We evaluate our approach on WildDash\ntest, which is currently the only public test dataset that includes\nout-of-distribution images. The obtained results show that the proposed\napproach succeeds to identify out-of-distribution pixels while outperforming\nprevious work by a wide margin.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 11:28:34 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 09:19:58 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bevandi\u0107", "Petra", ""], ["Kre\u0161o", "Ivan", ""], ["Or\u0161i\u0107", "Marin", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "1808.07712", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh and Suman Saha and Fabio Cuzzolin", "title": "Predicting Action Tubes", "comments": "ECCV workshop; Anticipating Human Behaviour 2018; 16 page 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a method to predict an entire `action tube' (a set\nof temporally linked bounding boxes) in a trimmed video just by observing a\nsmaller subset of it. Predicting where an action is going to take place in the\nnear future is essential to many computer vision based applications such as\nautonomous driving or surgical robotics. Importantly, it has to be done in\nreal-time and in an online fashion. We propose a Tube Prediction network\n(TPnet) which jointly predicts the past, present and future bounding boxes\nalong with their action classification scores. At test time TPnet is used in a\n(temporal) sliding window setting, and its predictions are put into a tube\nestimation framework to construct/predict the video long action tubes not only\nfor the observed part of the video but also for the unobserved part.\nAdditionally, the proposed action tube predictor helps in completing action\ntubes for unobserved segments of the video. We quantitatively demonstrate the\nlatter ability, and the fact that TPnet improves state-of-the-art detection\nperformance, on one of the standard action detection benchmarks - J-HMDB-21\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 12:11:06 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Singh", "Gurkirt", ""], ["Saha", "Suman", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1808.07746", "submitter": "Mohsen Hajabdollahi", "authors": "Mohsen Hajabdollahi, Reza Esfandiarpoor, Pejman Khadivi, S.M.Reza\n  Soroushmehr, Nader Karimi, Kayvan Najarian, Shadrokh Samavi", "title": "Segmentation of Bleeding Regions in Wireless Capsule Endoscopy for\n  Detection of Informative Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless capsule endoscopy (WCE) is an effective mean for diagnosis of\ngastrointestinal disorders. Detection of informative scenes in WCE video could\nreduce the length of transmitted videos and help the diagnosis procedure. In\nthis paper, we investigate the problem of simplification of neural networks for\nautomatic bleeding region detection inside capsule endoscopy device. Suitable\ncolor channels are selected as neural networks inputs, and image classification\nis conducted using a multi-layer perceptron (MLP) and a convolutional neural\nnetwork (CNN) separately. Both CNN and MLP structures are simplified to reduce\nthe number of computational operations. Performances of two simplified networks\nare evaluated on a WCE bleeding image dataset using the DICE score. Simulation\nresults show that applying simplification methods on both MLP and CNN\nstructures reduces the number of computational operations significantly with\nAUC greater than 0.97. Although CNN performs better in comparison with\nsimplified MLP, the simplified MLP segments bleeding regions with a\nsignificantly smaller number of computational operations. Concerning the\nimportance of having a simple structure or a more accurate model, each of the\ndesigned structures could be selected for inside capsule implementation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 13:39:04 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Hajabdollahi", "Mohsen", ""], ["Esfandiarpoor", "Reza", ""], ["Khadivi", "Pejman", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Najarian", "Kayvan", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1808.07773", "submitter": "Amanjot Kaur", "authors": "Abhinav Dhall, Amanjot Kaur, Roland Goecke, Tom Gedeon", "title": "EmotiW 2018: Audio-Video, Student Engagement and Group-Level Affect\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details the sixth Emotion Recognition in the Wild (EmotiW)\nchallenge. EmotiW 2018 is a grand challenge in the ACM International Conference\non Multimodal Interaction 2018, Colorado, USA. The challenge aims at providing\na common platform to researchers working in the affective computing community\nto benchmark their algorithms on `in the wild' data. This year EmotiW contains\nthree sub-challenges: a) Audio-video based emotion recognition; b) Student\nengagement prediction; and c) Group-level emotion recognition. The databases,\nprotocols and baselines are discussed in detail.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:19:11 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Dhall", "Abhinav", ""], ["Kaur", "Amanjot", ""], ["Goecke", "Roland", ""], ["Gedeon", "Tom", ""]]}, {"id": "1808.07784", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman, Frederik Ebert, Alexei A. Efros, Sergey Levine", "title": "Time-Agnostic Prediction: Predicting Predictable Video Frames", "comments": "8 pages, plus appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction is arguably one of the most basic functions of an intelligent\nsystem. In general, the problem of predicting events in the future or between\ntwo waypoints is exceedingly difficult. However, most phenomena naturally pass\nthrough relatively predictable bottlenecks---while we cannot predict the\nprecise trajectory of a robot arm between being at rest and holding an object\nup, we can be certain that it must have picked the object up. To exploit this,\nwe decouple visual prediction from a rigid notion of time. While conventional\napproaches predict frames at regularly spaced temporal intervals, our\ntime-agnostic predictors (TAP) are not tied to specific times so that they may\ninstead discover predictable \"bottleneck\" frames no matter when they occur. We\nevaluate our approach for future and intermediate frame prediction across three\nrobotic manipulation tasks. Our predictions are not only of higher visual\nquality, but also correspond to coherent semantic subgoals in temporally\nextended tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:52:40 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 22:46:41 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 19:22:25 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Ebert", "Frederik", ""], ["Efros", "Alexei A.", ""], ["Levine", "Sergey", ""]]}, {"id": "1808.07793", "submitter": "Niluthpol Mithun", "authors": "Niluthpol Chowdhury Mithun, Rameswar Panda, Evangelos E. Papalexakis,\n  Amit K. Roy-Chowdhury", "title": "Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval", "comments": "ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:07:52 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Panda", "Rameswar", ""], ["Papalexakis", "Evangelos E.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1808.07819", "submitter": "Sanketh Vedula", "authors": "Sanketh Vedula, Ortal Senouf, Grigoriy Zurakhov, Alex M. Bronstein,\n  Michael Zibulevsky, Oleg Michailovich, Dan Adam, Diana Gaitini", "title": "High quality ultrasonic multi-line transmission through deep learning", "comments": "To appear in Proceedings of MLMIR workshop, MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Frame rate is a crucial consideration in cardiac ultrasound imaging and 3D\nsonography. Several methods have been proposed in the medical ultrasound\nliterature aiming at accelerating the image acquisition. In this paper, we\nconsider one such method called \\textit{multi-line transmission} (MLT), in\nwhich several evenly separated focused beams are transmitted simultaneously.\nWhile MLT reduces the acquisition time, it comes at the expense of a heavy loss\nof contrast due to the interactions between the beams (cross-talk artifact). In\nthis paper, we introduce a data-driven method to reduce the artifacts arising\nin MLT. To this end, we propose to train an end-to-end convolutional neural\nnetwork consisting of correction layers followed by a constant apodization\nlayer. The network is trained on pairs of raw data obtained through MLT and the\ncorresponding \\textit{single-line transmission} (SLT) data. Experimental\nevaluation demonstrates significant improvement both in the visual image\nquality and in objective measures such as contrast ratio and contrast-to-noise\nratio, while preserving resolution unlike traditional apodization-based\nmethods. We show that the proposed method is able to generalize well across\ndifferent patients and anatomies on real and phantom data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 16:07:59 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Vedula", "Sanketh", ""], ["Senouf", "Ortal", ""], ["Zurakhov", "Grigoriy", ""], ["Bronstein", "Alex M.", ""], ["Zibulevsky", "Michael", ""], ["Michailovich", "Oleg", ""], ["Adam", "Dan", ""], ["Gaitini", "Diana", ""]]}, {"id": "1808.07823", "submitter": "Sanketh Vedula", "authors": "Ortal Senouf, Sanketh Vedula, Grigoriy Zurakhov, Alex M. Bronstein,\n  Michael Zibulevsky, Oleg Michailovich, Dan Adam, David Blondheim", "title": "High frame-rate cardiac ultrasound imaging with deep learning", "comments": "To appear in the Proceedings of MICCAI, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cardiac ultrasound imaging requires a high frame rate in order to capture\nrapid motion. This can be achieved by multi-line acquisition (MLA), where\nseveral narrow-focused received lines are obtained from each wide-focused\ntransmitted line. This shortens the acquisition time at the expense of\nintroducing block artifacts. In this paper, we propose a data-driven\nlearning-based approach to improve the MLA image quality. We train an\nend-to-end convolutional neural network on pairs of real ultrasound cardiac\ndata, acquired through MLA and the corresponding single-line acquisition (SLA).\nThe network achieves a significant improvement in image quality for both $5-$\nand $7-$line MLA resulting in a decorrelation measure similar to that of SLA\nwhile having the frame rate of MLA.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 16:21:18 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Senouf", "Ortal", ""], ["Vedula", "Sanketh", ""], ["Zurakhov", "Grigoriy", ""], ["Bronstein", "Alex M.", ""], ["Zibulevsky", "Michael", ""], ["Michailovich", "Oleg", ""], ["Adam", "Dan", ""], ["Blondheim", "David", ""]]}, {"id": "1808.07935", "submitter": "Victor Vaquero", "authors": "Victor Vaquero, Ivan del Pino, Francesc Moreno-Noguer, Joan Sol\\`a,\n  Alberto Sanfeliu, Juan Andrade-Cetto", "title": "Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking\n  in Driving Scenarios", "comments": "Presented in IEEE ECMR 2017. IEEE Copyrights: Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses", "journal-ref": null, "doi": "10.1109/ECMR.2017.8098657", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection and tracking is a core ingredient for developing autonomous\ndriving applications in urban scenarios. Recent image-based Deep Learning (DL)\ntechniques are obtaining breakthrough results in these perceptive tasks.\nHowever, DL research has not yet advanced much towards processing 3D point\nclouds from lidar range-finders. These sensors are very common in autonomous\nvehicles since, despite not providing as semantically rich information as\nimages, their performance is more robust under harsh weather conditions than\nvision sensors. In this paper we present a full vehicle detection and tracking\nsystem that works with 3D lidar information only. Our detection step uses a\nConvolutional Neural Network (CNN) that receives as input a featured\nrepresentation of the 3D information provided by a Velodyne HDL-64 sensor and\nreturns a per-point classification of whether it belongs to a vehicle or not.\nThe classified point cloud is then geometrically processed to generate\nobservations for a multi-object tracking system implemented via a number of\nMulti-Hypothesis Extended Kalman Filters (MH-EKF) that estimate the position\nand velocity of the surrounding vehicles. The system is thoroughly evaluated on\nthe KITTI tracking dataset, and we show the performance boost provided by our\nCNN-based vehicle detector over a standard geometric approach. Our lidar-based\napproach uses about a 4% of the data needed for an image-based detector with\nsimilarly competitive results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 20:30:04 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Vaquero", "Victor", ""], ["del Pino", "Ivan", ""], ["Moreno-Noguer", "Francesc", ""], ["Sol\u00e0", "Joan", ""], ["Sanfeliu", "Alberto", ""], ["Andrade-Cetto", "Juan", ""]]}, {"id": "1808.07954", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Arash Mohammadi, Konstantinos N. Plataniotis,\n  Anastasia Oikonomou, and Habib Benali", "title": "From Hand-Crafted to Deep Learning-based Cancer Radiomics: Challenges\n  and Opportunities", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2019.2900993", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in signal processing and machine learning coupled with\ndevelopments of electronic medical record keeping in hospitals and the\navailability of extensive set of medical images through internal/external\ncommunication systems, have resulted in a recent surge of significant interest\nin \"Radiomics\". Radiomics is an emerging and relatively new research field,\nwhich refers to extracting semi-quantitative and/or quantitative features from\nmedical images with the goal of developing predictive and/or prognostic models,\nand is expected to become a critical component for integration of image-derived\ninformation for personalized treatment in the near future. The conventional\nRadiomics workflow is typically based on extracting pre-designed features (also\nreferred to as hand-crafted or engineered features) from a segmented region of\ninterest. Nevertheless, recent advancements in deep learning have caused trends\ntowards deep learning-based Radiomics (also referred to as discovery\nRadiomics). Considering the advantages of these two approaches, there are also\nhybrid solutions developed to exploit the potentials of multiple data sources.\nConsidering the variety of approaches to Radiomics, further improvements\nrequire a comprehensive and integrated sketch, which is the goal of this\narticle. This manuscript provides a unique interdisciplinary perspective on\nRadiomics by discussing state-of-the-art signal processing solutions in the\ncontext of Radiomics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 21:39:12 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 13:08:27 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 00:34:41 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Afshar", "Parnian", ""], ["Mohammadi", "Arash", ""], ["Plataniotis", "Konstantinos N.", ""], ["Oikonomou", "Anastasia", ""], ["Benali", "Habib", ""]]}, {"id": "1808.07962", "submitter": "Siyuan Qi", "authors": "Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, Song-Chun Zhu", "title": "Learning Human-Object Interactions by Graph Parsing Neural Networks", "comments": "This paper is published in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of detecting and recognizing human-object\ninteractions (HOI) in images and videos. We introduce the Graph Parsing Neural\nNetwork (GPNN), a framework that incorporates structural knowledge while being\ndifferentiable end-to-end. For a given scene, GPNN infers a parse graph that\nincludes i) the HOI graph structure represented by an adjacency matrix, and ii)\nthe node labels. Within a message passing inference framework, GPNN iteratively\ncomputes the adjacency matrices and node labels. We extensively evaluate our\nmodel on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO,\nand CAD-120 datasets. Our approach significantly outperforms state-of-art\nmethods, verifying that GPNN is scalable to large datasets and applies to\nspatial-temporal settings. The code is available at\nhttps://github.com/SiyuanQi/gpnn.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 23:04:22 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Qi", "Siyuan", ""], ["Wang", "Wenguan", ""], ["Jia", "Baoxiong", ""], ["Shen", "Jianbing", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1808.07967", "submitter": "Enzo Ferrante", "authors": "Alejandro Debus and Enzo Ferrante", "title": "Left ventricle quantification through spatio-temporal CNNs", "comments": "Accepted for publication at Statistical Atlases and Computational\n  Modeling of the Heart (STACOM) workshop @ MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular diseases are among the leading causes of death globally.\nCardiac left ventricle (LV) quantification is known to be one of the most\nimportant tasks for the identification and diagnosis of such pathologies. In\nthis paper, we propose a deep learning method that incorporates 3D\nspatio-temporal convolutions to perform direct left ventricle quantification\nfrom cardiac MR sequences. Instead of analysing slices independently, we\nprocess stacks of temporally adjacent slices by means of 3D convolutional\nkernels which fuse the spatio-temporal information, incorporating the temporal\ndynamics of the heart to the learned model. We show that incorporating such\ninformation by means of spatio-temporal convolutions into standard LV\nquantification architectures improves the accuracy of the predictions when\ncompared with single-slice models, achieving competitive results for all\ncardiac indices and significantly breaking the state of the art (Xue et al.,\n2018, MedIA) for cardiac phase estimation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 23:37:07 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Debus", "Alejandro", ""], ["Ferrante", "Enzo", ""]]}, {"id": "1808.07993", "submitter": "Tao Kong", "authors": "Tao Kong, Fuchun Sun, Wenbing Huang and Huaping Liu", "title": "Deep Feature Pyramid Reconfiguration for Object Detection", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detectors usually learn multi-scale representations\nto get better results by employing feature pyramids. However, the current\ndesigns for feature pyramids are still inefficient to integrate the semantic\ninformation over different scales. In this paper, we begin by investigating\ncurrent feature pyramids solutions, and then reformulate the feature pyramid\nconstruction as the feature reconfiguration process. Finally, we propose a\nnovel reconfiguration architecture to combine low-level representations with\nhigh-level semantic features in a highly-nonlinear yet efficient way. In\nparticular, our architecture which consists of global attention and local\nreconfigurations, is able to gather task-oriented features across different\nspatial locations and scales, globally and locally. Both the global attention\nand local reconfiguration are lightweight, in-place, and end-to-end trainable.\nUsing this method in the basic SSD system, our models achieve consistent and\nsignificant boosts compared with the original model and its other variations,\nwithout losing real-time processing speed.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 03:17:17 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Kong", "Tao", ""], ["Sun", "Fuchun", ""], ["Huang", "Wenbing", ""], ["Liu", "Huaping", ""]]}, {"id": "1808.08024", "submitter": "Devis Tuia", "authors": "Devis Tuia, Michele Volpi, Gabriele Moser", "title": "Decision fusion with multiple spatial supports by conditional random\n  fields", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 56(6),\n  3277-3289, 2018", "doi": "10.1109/TGRS.2018.2797316", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of remotely sensed images into land cover or land use is\nhighly dependent on geographical information at least at two levels. First,\nland cover classes are observed in a spatially smooth domain separated by sharp\nregion boundaries. Second, land classes and observation scale are also tightly\nintertwined: they tend to be consistent within areas of homogeneous appearance,\nor regions, in the sense that all pixels within a roof should be classified as\nroof, independently on the spatial support used for the classification. In this\npaper, we follow these two observations and encode them as priors in an energy\nminimization framework based on conditional random fields (CRFs), where\nclassification results obtained at pixel and region levels are\nprobabilistically fused. The aim is to enforce the final maps to be consistent\nnot only in their own spatial supports (pixel and region) but also across\nsupports, i.e., by getting the predictions on the pixel lattice and on the set\nof regions to agree. To this end, we define an energy function with three\nterms: 1) a data term for the individual elements in each support\n(support-specific nodes); 2) spatial regularization terms in a neighborhood for\neach of the supports (support-specific edges); and 3) a regularization term\nbetween individual pixels and the region containing each of them (intersupports\nedges). We utilize these priors in a unified energy minimization problem that\ncan be optimized by standard solvers. The proposed 2LCRF model consists of a\nCRF defined over a bipartite graph, i.e., two interconnected layers within a\nsingle graph accounting for interlattice connections.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 06:52:50 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Tuia", "Devis", ""], ["Volpi", "Michele", ""], ["Moser", "Gabriele", ""]]}, {"id": "1808.08093", "submitter": "Lazar Kats", "authors": "Lazar Kats, Marilena Vered, Ayelet Zlotogorski-Hurvitz, Itai Harpaz", "title": "Atherosclerotic carotid plaques on panoramic imaging: an automatic\n  detection using deep learning with small dataset", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke is the second most frequent cause of death worldwide with a\nconsiderable economic burden on the health systems. In about 15% of strokes,\natherosclerotic carotid plaques (ACPs) constitute the main etiological factor.\nEarly detection of ACPs may have a key-role for preventing strokes by managing\nthe patient a-priory to the occurrence of the damage. ACPs can be detected on\npanoramic images. As these are one of the most common images performed for\nroutine dental practice, they can be used as a source of available data for\ncomputerized methods of automatic detection in order to significantly increase\ntimely diagnosis of ACPs. Recently, there has been a definite breakthrough in\nthe field of analysis of medical images due to the use of deep learning based\non neural networks. These methods, however have been barely used in dentistry.\nIn this study we used the Faster Region-based Convolutional Network (Faster\nR-CNN) for deep learning. We aimed to assess the operation of the algorithm on\na small database of 65 panoramic images. Due to a small amount of available\ntraining data, we had to use data augmentation by changing the brightness and\nrandomly flipping and rotating cropped regions of interest in multiple angles.\nReceiver Operating Characteristic (ROC) analysis was performed to calculate the\naccuracy of detection. ACP was detected with a sensitivity of 75%, specificity\nof 80% and an accuracy of 83%. The ROC analysis showed a significant Area Under\nCurve (AUC) difference from 0.5. Our novelty lies in that we have showed the\nefficiency of the Faster R-CNN algorithm in detecting ACPs on routine panoramic\nimages based on a small database. There is a need to further improve the\napplication of the algorithm to the level of introducing this methodology in\nroutine dental practice in order to enable us to prevent stroke events.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 11:25:36 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Kats", "Lazar", ""], ["Vered", "Marilena", ""], ["Zlotogorski-Hurvitz", "Ayelet", ""], ["Harpaz", "Itai", ""]]}, {"id": "1808.08114", "submitter": "Jo Schlemper", "authors": "Jo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard\n  Kainz, Ben Glocker, Daniel Rueckert", "title": "Attention Gated Networks: Learning to Leverage Salient Regions in\n  Medical Images", "comments": "Accepted for Medical Image Analysis (Special Issue on Medical Imaging\n  with Deep Learning). arXiv admin note: substantial text overlap with\n  arXiv:1804.03999, arXiv:1804.05338", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel attention gate (AG) model for medical image analysis that\nautomatically learns to focus on target structures of varying shapes and sizes.\nModels trained with AGs implicitly learn to suppress irrelevant regions in an\ninput image while highlighting salient features useful for a specific task.\nThis enables us to eliminate the necessity of using explicit external\ntissue/organ localisation modules when using convolutional neural networks\n(CNNs). AGs can be easily integrated into standard CNN models such as VGG or\nU-Net architectures with minimal computational overhead while increasing the\nmodel sensitivity and prediction accuracy. The proposed AG models are evaluated\non a variety of tasks, including medical image classification and segmentation.\nFor classification, we demonstrate the use case of AGs in scan plane detection\nfor fetal ultrasound screening. We show that the proposed attention mechanism\ncan provide efficient object localisation while improving the overall\nprediction performance by reducing false positives. For segmentation, the\nproposed architecture is evaluated on two large 3D CT abdominal datasets with\nmanual annotations for multiple organs. Experimental results show that AG\nmodels consistently improve the prediction performance of the base\narchitectures across different datasets and training sizes while preserving\ncomputational efficiency. Moreover, AGs guide the model activations to be\nfocused around salient regions, which provides better insights into how model\npredictions are made. The source code for the proposed AG models is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 19:17:23 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 00:29:45 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Schlemper", "Jo", ""], ["Oktay", "Ozan", ""], ["Schaap", "Michiel", ""], ["Heinrich", "Mattias", ""], ["Kainz", "Bernhard", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1808.08121", "submitter": "Mojtaba Heidarysafa", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Donald E. Brown, Kiana Jafari\n  Meimandi, Laura E. Barnes", "title": "An Improvement of Data Classification Using Random Multimodel Deep\n  Learning (RMDL)", "comments": "published in International Journal of Machine Learning and Computing\n  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890", "journal-ref": null, "doi": "10.18178/ijmlc.2018.8.4.703", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth in the number of complex datasets every year requires\nmore enhancement in machine learning methods to provide robust and accurate\ndata classification. Lately, deep learning approaches have achieved surpassing\nresults in comparison to previous machine learning algorithms. However, finding\nthe suitable structure for these models has been a challenge for researchers.\nThis paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,\ndeep learning approach for classification. RMDL solves the problem of finding\nthe best deep learning structure and architecture while simultaneously\nimproving robustness and accuracy through ensembles of deep learning\narchitectures. In short, RMDL trains multiple randomly generated models of Deep\nNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural\nNetwork (RNN) in parallel and combines their results to produce better result\nof any of those models individually. In this paper, we describe RMDL model and\ncompare the results for image and text classification as well as face\nrecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for\nimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text\nclassification. Lastly, we used ORL dataset to compare the model performance on\nface recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 00:38:14 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Meimandi", "Kiana Jafari", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1808.08127", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Nassir Navab, Christian Wachinger", "title": "Recalibrating Fully Convolutional Networks with Spatial and Channel\n  'Squeeze & Excitation' Blocks", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging.\n  arXiv admin note: text overlap with arXiv:1803.02579", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of semantic segmentation tasks, fully convolutional neural\nnetworks (F-CNNs) have been successfully leveraged to achieve state-of-the-art\nperformance. Architectural innovations of F-CNNs have mainly been on improving\nspatial encoding or network connectivity to aid gradient flow. In this article,\nwe aim towards an alternate direction of recalibrating the learned feature maps\nadaptively; boosting meaningful features while suppressing weak ones. The\nrecalibration is achieved by simple computational blocks that can be easily\nintegrated in F-CNNs architectures. We draw our inspiration from the recently\nproposed 'squeeze & excitation' (SE) modules for channel recalibration for\nimage classification. Towards this end, we introduce three variants of SE\nmodules for segmentation, (i) squeezing spatially and exciting channel-wise,\n(ii) squeezing channel-wise and exciting spatially and (iii) joint spatial and\nchannel 'squeeze & excitation'. We effectively incorporate the proposed SE\nblocks in three state-of-the-art F-CNNs and demonstrate a consistent\nimprovement of segmentation accuracy on three challenging benchmark datasets.\nImportantly, SE blocks only lead to a minimal increase in model complexity of\nabout 1.5%, while the Dice score increases by 4-9% in the case of U-Net. Hence,\nwe believe that SE blocks can be an integral part of future F-CNN\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 13:45:03 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1808.08180", "submitter": "Vinkle Kumar Srivastav", "authors": "Vinkle Srivastav, Thibaut Issenhuth, Abdolrahim Kadkhodamohammadi,\n  Michel de Mathelin, Afshin Gangi, Nicolas Padoy", "title": "MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose\n  Estimation", "comments": "The paper was presented in the MICCAI-LABELS 2018\n  (https://labels.tue-image.nl/previous-editions/labels-2018/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person detection and pose estimation is a key requirement to develop\nintelligent context-aware assistance systems. To foster the development of\nhuman pose estimation methods and their applications in the Operating Room\n(OR), we release the Multi-View Operating Room (MVOR) dataset, the first public\ndataset recorded during real clinical interventions. It consists of 732\nsynchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR.\nIt also includes the visual challenges present in such environments, such as\nocclusions and clutter. We provide camera calibration parameters, color and\ndepth frames, human bounding boxes, and 2D/3D pose annotations. In this paper,\nwe present the dataset, its annotations, as well as baseline results from\nseveral recent person detection and 2D/3D pose estimation methods. Since we\nneed to blur some parts of the images to hide identity and nudity in the\nreleased dataset, we also present a comparative study of how the baselines have\nbeen impacted by the blurring. Results show a large margin for improvement and\nsuggest that the MVOR dataset can be useful to compare the performance of the\ndifferent methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 15:47:48 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:17:43 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Srivastav", "Vinkle", ""], ["Issenhuth", "Thibaut", ""], ["Kadkhodamohammadi", "Abdolrahim", ""], ["de Mathelin", "Michel", ""], ["Gangi", "Afshin", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1808.08186", "submitter": "Kumar Sankar Ray", "authors": "Rajesh Misra, Kumar S. Ray", "title": "Dual approach for object tracking based on optical flow and swarm\n  intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision,object tracking is a very old and complex problem.Though\nthere are several existing algorithms for object tracking, still there are\nseveral challenges remain to be solved. For instance, variation of illumination\nof light, noise, occlusion, sudden start and stop of moving object, shading\netc,make the object tracking a complex problem not only for dynamic background\nbut also for static background. In this paper we propose a dual approach for\nobject tracking based on optical flow and swarm Intelligence.The optical flow\nbased KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the\ntarget object from first frame to last frame of a video sequence;whereas swarm\nIntelligence based PSO (Particle Swarm Optimization) tracker simultaneously\ntracks the boundary information of the target object from second frame to last\nframe of the same video sequence.This dual function of tracking makes the\ntrackers very much robust with respect to the above stated problems. The\nflexibility of our approach is that it can be successfully applicable in\nvariable background as well as static background.We compare the performance of\nthe proposed dual tracking algorithm with several benchmark datasets and obtain\nvery competitive results in general and in most of the cases we obtained\nsuperior results using dual tracking algorithm. We also compare the performance\nof the proposed dual tracker with some existing PSO based algorithms for\ntracking and achieved better results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 14:17:45 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Misra", "Rajesh", ""], ["Ray", "Kumar S.", ""]]}, {"id": "1808.08210", "submitter": "Stanley Chan", "authors": "Xiran Wang, Jason Juang, Stanley H. Chan", "title": "Automatic Foreground Extraction from Imperfect Backgrounds using\n  Multi-Agent Consensus Equilibrium", "comments": null, "journal-ref": null, "doi": "10.1016/j.jvcir.2020.102907", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting accurate foreground objects from a scene is an essential step for\nmany video applications. Traditional background subtraction algorithms can\ngenerate coarse estimates, but generating high quality masks requires\nprofessional softwares with significant human interventions, e.g., providing\ntrimaps or labeling key frames. We propose an automatic foreground extraction\nmethod in applications where a static but imperfect background is available.\nExamples include filming and surveillance where the background can be captured\nbefore the objects enter the scene or after they leave the scene. Our proposed\nmethod is very robust and produces significantly better estimates than\nstate-of-the-art background subtraction, video segmentation and alpha matting\nmethods. The key innovation of our method is a novel information fusion\ntechnique. The fusion framework allows us to integrate the individual strengths\nof alpha matting, background subtraction and image denoising to produce an\noverall better estimate. Such integration is particularly important when\nhandling complex scenes with imperfect background. We show how the framework is\ndeveloped, and how the individual components are built. Extensive experiments\nand ablation studies are conducted to evaluate the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 16:58:00 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 21:26:49 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 20:01:58 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Xiran", ""], ["Juang", "Jason", ""], ["Chan", "Stanley H.", ""]]}, {"id": "1808.08272", "submitter": "Fangqi Li", "authors": "Fang-Qi Li, Xu-Die Ren, Hao-Nan Guo", "title": "Probabilistic Model of Object Detection Based on Convolutional Neural\n  Network", "comments": "8 pages, 8 figures, International Conference on Communication, Signal\n  Processing and Systems (CSPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of a CNN detector and a search framework forms the basis for\nlocal object/pattern detection. To handle the waste of regional information and\nthe defective compromise between efficiency and accuracy, this paper proposes a\nprobabilistic model with a powerful search framework. By mapping an image into\na probabilistic distribution of objects, this new model gives more informative\noutputs with less computation. The setting and analytic traits are elaborated\nin this paper, followed by a series of experiments carried out on FDDB, which\nshow that the proposed model is sound, efficient and analytic.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 18:11:56 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Fang-Qi", ""], ["Ren", "Xu-Die", ""], ["Guo", "Hao-Nan", ""]]}, {"id": "1808.08273", "submitter": "Yeman Brhane Hagos", "authors": "Yeman Brhane Hagos, Albert Gubern Merida, and Jonas Teuwen", "title": "Improving Breast Cancer Detection using Symmetry Information with Deep\n  Learning", "comments": "8 pages, 7 figures, accepted in MICCAI 2018 Breast Image Analysis\n  (BIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have had a huge success in many areas of\ncomputer vision and medical image analysis. However, there is still an immense\npotential for performance improvement in mammogram breast cancer detection\nComputer-Aided Detection (CAD) systems by integrating all the information that\nthe radiologist utilizes, such as symmetry and temporal data. In this work, we\nproposed a patch based multi-input CNN that learns symmetrical difference to\ndetect breast masses. The network was trained on a large-scale dataset of 28294\nmammogram images. The performance was compared to a baseline architecture\nwithout symmetry context using Area Under the ROC Curve (AUC) and Competition\nPerformance Metric (CPM). At candidate level, AUC value of 0.933 with 95%\nconfidence interval of [0.920, 0.954] was obtained when symmetry information is\nincorporated in comparison with baseline architecture which yielded AUC value\nof 0.929 with [0.919, 0.947] confidence interval. By incorporating symmetrical\ninformation, although there was no a significant candidate level performance\nagain (p = 0.111), we have found a compelling result at exam level with CPM\nvalue of 0.733 (p = 0.001). We believe that including temporal data, and adding\nbenign class to the dataset could improve the detection performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 11:04:42 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Hagos", "Yeman Brhane", ""], ["Merida", "Albert Gubern", ""], ["Teuwen", "Jonas", ""]]}, {"id": "1808.08275", "submitter": "Soumi Ray", "authors": "Soumi Ray, Vinod Kumar", "title": "Binary Image Features Proposed to Empower Computer Vision", "comments": null, "journal-ref": null, "doi": "10.1002/ima.22418", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This literature has proposed three fast and easy computable image features to\nimprove computer vision by offering more human-like vision power. These\nfeatures are not based on image pixels absolute or relative intensity; neither\nbased on shape or colour. So, no complex pixel by pixel calculation is\nrequired. For human eyes, pixel by pixel calculation is like seeing an image\nwith maximum zoom which is done only when a higher level of details is\nrequired. Normally, first we look at an image to get an overall idea about it\nto know whether it deserves further investigation or not. This capacity of\ngetting an idea at a glance is analysed and three basic features are proposed\nto empower computer vision. Potential of proposed features is tested and\nestablished through different medical dataset. Achieved accuracy in\nclassification demonstrates possibilities and potential of the use of the\nproposed features in image processing.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 06:39:58 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ray", "Soumi", ""], ["Kumar", "Vinod", ""]]}, {"id": "1808.08277", "submitter": "Xupeng Chen", "authors": "Xupeng Chen, Binbin Shi", "title": "Deep Mask For X-ray Based Heart Disease Classification", "comments": "outdated work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a deep learning model to detect and classify heart disease using\n$X-ray$. We collect data from several hospitals and public datasets. After\npreprocess we get 3026 images including disease type VSD, ASD, TOF and normal\ncontrol. The main problem we have to solve is to enable the network to\naccurately learn the characteristics of the heart, to ensure the reliability of\nthe network while increasing accuracy. By learning the doctor's diagnostic\nexperience, labeling the image and using tools to extract masks of heart\nregion, we train a U-net to generate a mask to give more attention. It forces\nthe model to focus on the characteristics of the heart region and obtain more\nreliable results.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 21:41:45 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 04:05:57 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Xupeng", ""], ["Shi", "Binbin", ""]]}, {"id": "1808.08279", "submitter": "Navid Alemi Koohbanani", "authors": "Navid Alemi Koohababni, Mostafa Jahanifar, Ali Gooya, and Nasir\n  Rajpoot", "title": "Nuclei Detection Using Mixture Density Networks", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclei detection is an important task in the histology domain as it is a main\nstep toward further analysis such as cell counting, cell segmentation, study of\ncell connections, etc. This is a challenging task due to the complex texture of\nhistology image, variation in shape, and touching cells. To tackle these\nhurdles, many approaches have been proposed in the literature where deep\nlearning methods stand on top in terms of performance. Hence, in this paper, we\npropose a novel framework for nuclei detection based on Mixture Density\nNetworks (MDNs). These networks are suitable to map a single input to several\npossible outputs and we utilize this property to detect multiple seeds in a\nsingle image patch. A new modified form of a cost function is proposed for\ntraining and handling patches with missing nuclei. The probability maps of the\nnuclei in the individual patches are next combined to generate the final\nimage-wide result. The experimental results show the state-of-the-art\nperformance on complex colorectal adenocarcinoma dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 05:59:19 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Koohababni", "Navid Alemi", ""], ["Jahanifar", "Mostafa", ""], ["Gooya", "Ali", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1808.08280", "submitter": "Suman Sedai", "authors": "Suman Sedai, Dwarikanath Mahapatra, Zongyuan Ge, Rajib Chakravorty and\n  Rahil Garnavi", "title": "Deep multiscale convolutional feature learning for weakly supervised\n  localization of chest pathologies in X-ray images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of chest pathologies in chest X-ray images is a challenging task\nbecause of their varying sizes and appearances. We propose a novel weakly\nsupervised method to localize chest pathologies using class aware deep\nmultiscale feature learning. Our method leverages intermediate feature maps\nfrom CNN layers at different stages of a deep network during the training of a\nclassification model using image level annotations of pathologies. During the\ntraining phase, a set of \\emph{layer relevance weights} are learned for each\npathology class and the CNN is optimized to perform pathology classification by\nconvex combination of feature maps from both shallow and deep layers using the\nlearned weights. During the test phase, to localize the predicted pathology,\nthe multiscale attention map is obtained by convex combination of class\nactivation maps from each stage using the \\emph{layer relevance weights}\nlearned during the training phase. We have validated our method using 112000\nX-ray images and compared with the state-of-the-art localization methods. We\nexperimentally demonstrate that the proposed weakly supervised method can\nimprove the localization performance of small pathologies such as nodule and\nmass while giving comparable performance for bigger pathologies e.g.,\nCardiomegaly\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 06:08:45 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Sedai", "Suman", ""], ["Mahapatra", "Dwarikanath", ""], ["Ge", "Zongyuan", ""], ["Chakravorty", "Rajib", ""], ["Garnavi", "Rahil", ""]]}, {"id": "1808.08282", "submitter": "Mahdieh Abbasi", "authors": "Mahdieh Abbasi, Arezoo Rajabi, Azadeh Sadat Mozafari, Rakesh B. Bobba,\n  Christian Gagne", "title": "Controlling Over-generalization and its Effect on Adversarial Examples\n  Generation and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) significantly improve the\nstate-of-the-art for many applications, especially in computer vision. However,\nCNNs still suffer from a tendency to confidently classify out-distribution\nsamples from unknown classes into pre-defined known classes. Further, they are\nalso vulnerable to adversarial examples. We are relating these two issues\nthrough the tendency of CNNs to over-generalize for areas of the input space\nnot covered well by the training set. We show that a CNN augmented with an\nextra output class can act as a simple yet effective end-to-end model for\ncontrolling over-generalization. As an appropriate training set for the extra\nclass, we introduce two resources that are computationally efficient to obtain:\na representative natural out-distribution set and interpolated in-distribution\nsamples. To help select a representative natural out-distribution set among\navailable ones, we propose a simple measurement to assess an out-distribution\nset's fitness. We also demonstrate that training such an augmented CNN with\nrepresentative out-distribution natural datasets and some interpolated samples\nallows it to better handle a wide range of unseen out-distribution samples and\nblack-box adversarial examples without training it on any adversaries. Finally,\nwe show that generation of white-box adversarial attacks using our proposed\naugmented CNN can become harder, as the attack algorithms have to get around\nthe rejection regions when generating actual adversaries.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 02:05:57 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 22:43:58 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Abbasi", "Mahdieh", ""], ["Rajabi", "Arezoo", ""], ["Mozafari", "Azadeh Sadat", ""], ["Bobba", "Rakesh B.", ""], ["Gagne", "Christian", ""]]}, {"id": "1808.08296", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Nicha C. Dvornek, Juntang Zhuang, Pamela Ventola and\n  James S. Duncan", "title": "Brain Biomarker Interpretation in ASD Using Deep Learning and fMRI", "comments": "8 pagers, accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder.\nFinding the biomarkers associated with ASD is extremely helpful to understand\nthe underlying roots of the disorder and can lead to earlier diagnosis and more\ntargeted treatment. Although Deep Neural Networks (DNNs) have been applied in\nfunctional magnetic resonance imaging (fMRI) to identify ASD, understanding the\ndata-driven computational decision making procedure has not been previously\nexplored. Therefore, in this work, we address the problem of interpreting\nreliable biomarkers associated with identifying ASD; specifically, we propose a\n2-stage method that classifies ASD and control subjects using fMRI images and\ninterprets the saliency features activated by the classifier. First, we trained\nan accurate DNN classifier. Then, for detecting the biomarkers, different from\nthe DNN visualization works in computer vision, we take advantage of the\nanatomical structure of brain fMRI and develop a frequency-normalized sampling\nmethod to corrupt images. Furthermore, in the ASD vs. control subjects\nclassification scenario, we provide a new approach to detect and characterize\nimportant brain features into three categories. The biomarkers we found by the\nproposed method are robust and consistent with previous findings in the\nliterature. We also validate the detected biomarkers by neurological function\ndecoding and comparing with the DNN activation maps.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 06:24:56 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Dvornek", "Nicha C.", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "1808.08304", "submitter": "Saad Nadeem", "authors": "Rena Elkin, Saad Nadeem, Eldad Haber, Klara Steklova, Hedok Lee,\n  Helene Benveniste and Allen Tannenbaum", "title": "GlymphVIS: Visualizing Glymphatic Transport Pathways Using Regularized\n  Optimal Transport", "comments": "MICCAI 2018, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The glymphatic system (GS) is a transit passage that facilitates brain\nmetabolic waste removal and its dysfunction has been associated with\nneurodegenerative diseases such as Alzheimer's disease. The GS has been studied\nby acquiring temporal contrast enhanced magnetic resonance imaging (MRI)\nsequences of a rodent brain, and tracking the cerebrospinal fluid injected\ncontrast agent as it flows through the GS. We present here a novel\nvisualization framework, GlymphVIS, which uses regularized optimal transport\n(OT) to study the flow behavior between time points at which the images are\ntaken. Using this regularized OT approach, we can incorporate diffusion, handle\nnoise, and accurately capture and visualize the time varying dynamics in GS\ntransport. Moreover, we are able to reduce the registration mean-squared and\ninfinity-norm error across time points by up to a factor of 5 as compared to\nthe current state-of-the-art method. Our visualization pipeline yields flow\npatterns that align well with experts' current findings of the glymphatic\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 20:43:37 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 16:27:39 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Elkin", "Rena", ""], ["Nadeem", "Saad", ""], ["Haber", "Eldad", ""], ["Steklova", "Klara", ""], ["Lee", "Hedok", ""], ["Benveniste", "Helene", ""], ["Tannenbaum", "Allen", ""]]}, {"id": "1808.08307", "submitter": "Saad Nadeem", "authors": "Wookjin Choi, Saad Nadeem, Sadegh Riyahi, Joseph O. Deasy, Allen\n  Tannenbaum, and Wei Lu", "title": "Reproducible and Interpretable Spiculation Quantification for Lung\n  Cancer Screening", "comments": "Computer Methods and Programs in Biomedicine (Journal Extension of\n  MICCAI ShapeMI 2018 Workshop paper). **First two authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiculations are important predictors of lung cancer malignancy, which are\nspikes on the surface of the pulmonary nodules. In this study, we proposed an\ninterpretable and parameter-free technique to quantify the spiculation using\narea distortion metric obtained by the conformal (angle-preserving) spherical\nparameterization. We exploit the insight that for an angle-preserved spherical\nmapping of a given nodule, the corresponding negative area distortion precisely\ncharacterizes the spiculations on that nodule. We introduced novel spiculation\nscores based on the area distortion metric and spiculation measures. We also\nsemi-automatically segment lung nodule (for reproducibility) as well as vessel\nand wall attachment to differentiate the real spiculations from lobulation and\nattachment. A simple pathological malignancy prediction model is also\nintroduced. We used the publicly-available LIDC-IDRI dataset pathologists\n(strong-label) and radiologists (weak-label) ratings to train and test\nradiomics models containing this feature, and then externally validate the\nmodels. We achieved AUC$=$0.80 and 0.76, respectively, with the models trained\non the 811 weakly-labeled LIDC datasets and tested on the 72 strongly-labeled\nLIDC and 73 LUNGx datasets; the previous best model for LUNGx had AUC$=$0.68.\nThe number-of-spiculations feature was found to be highly correlated\n(Spearman's rank correlation coefficient $\\rho = 0.44$) with the radiologists'\nspiculation score. We developed a reproducible and interpretable,\nparameter-free technique for quantifying spiculations on nodules. The\nspiculation quantification measures was then applied to the radiomics framework\nfor pathological malignancy prediction with reproducible semi-automatic\nsegmentation of nodule. Using our interpretable features (size, attachment,\nspiculation, lobulation), we were able to achieve higher performance than\nprevious models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 20:59:56 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 16:30:30 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 15:50:47 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Choi", "Wookjin", ""], ["Nadeem", "Saad", ""], ["Riyahi", "Sadegh", ""], ["Deasy", "Joseph O.", ""], ["Tannenbaum", "Allen", ""], ["Lu", "Wei", ""]]}, {"id": "1808.08308", "submitter": "Joseph Chuang", "authors": "Joseph Chuang, Eric Tsai, Kevin Huang, Jay Fetter", "title": "ParaNet - Using Dense Blocks for Early Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DenseNets have been shown to be a competitive model among recent\nconvolutional network architectures. These networks utilize Dense Blocks, which\nare groups of densely connected layers where the output of a hidden layer is\nfed in as the input of every other layer following it. In this paper, we aim to\nimprove certain aspects of DenseNet, especially when it comes to practicality.\nWe introduce ParaNet, a new architecture that constructs three pipelines which\nallow for early inference. We additionally introduce a cascading mechanism such\nthat different pipelines are able to share parameters, as well as logit\nmatching between the outputs of the pipelines. We separately evaluate each of\nthe newly introduced mechanisms of ParaNet, then evaluate our proposed\narchitecture on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:01:17 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Chuang", "Joseph", ""], ["Tsai", "Eric", ""], ["Huang", "Kevin", ""], ["Fetter", "Jay", ""]]}, {"id": "1808.08312", "submitter": "Saad Nadeem", "authors": "Sadegh Riyahi, Wookjin Choi, Chia-Ju Liu, Saad Nadeem, Shan Tan,\n  Hualiang Zhong, Wengen Chen, Abraham J. Wu, James G. Mechalakos, Joseph O.\n  Deasy and Wei Lu", "title": "Quantification of Local Metabolic Tumor Volume Changes by Registering\n  Blended PET-CT Images for Prediction of Pathologic Tumor Response", "comments": "MICCAI 2018: Workshop on Data-Driven Treatment Response Assessment\n  (Oral Presentation), 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of local metabolic tumor volume (MTV) chan-ges after\nChemo-radiotherapy would allow accurate tumor response evaluation. Currently,\nlocal MTV changes in esophageal (soft-tissue) cancer are measured by\nregistering follow-up PET to baseline PET using the same transformation\nobtained by deformable registration of follow-up CT to baseline CT. Such\napproach is suboptimal because PET and CT capture fundamentally different\nproperties (metabolic vs. anatomy) of a tumor. In this work we combined PET and\nCT images into a single blended PET-CT image and registered follow-up blended\nPET-CT image to baseline blended PET-CT image. B-spline regularized\ndiffeomorphic registration was used to characterize the large MTV shrinkage.\nJacobian of the resulting transformation was computed to measure the local MTV\nchanges. Radiomic features (intensity and texture) were then extracted from the\nJacobian map to predict pathologic tumor response. Local MTV changes calculated\nusing blended PET-CT registration achieved the highest correlation with ground\ntruth segmentation (R=0.88) compared to PET-PET (R=0.80) and CT-CT (R=0.67)\nregistrations. Moreover, using blended PET-CT registration, the multivariate\nprediction model achieved the highest accuracy with only one Jacobian\nco-occurrence texture feature (accuracy=82.3%). This novel framework can\nreplace the conventional approach that applies CT-CT transformation to the PET\ndata for longitudinal evaluation of tumor response.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:17:11 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Riyahi", "Sadegh", ""], ["Choi", "Wookjin", ""], ["Liu", "Chia-Ju", ""], ["Nadeem", "Saad", ""], ["Tan", "Shan", ""], ["Zhong", "Hualiang", ""], ["Chen", "Wengen", ""], ["Wu", "Abraham J.", ""], ["Mechalakos", "James G.", ""], ["Deasy", "Joseph O.", ""], ["Lu", "Wei", ""]]}, {"id": "1808.08319", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent\n  Buch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis,\n  Caner Sahin, Fabian Manhardt, Federico Tombari, Tae-Kyun Kim, Jiri Matas,\n  Carsten Rother", "title": "BOP: Benchmark for 6D Object Pose Estimation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a benchmark for 6D pose estimation of a rigid object from a single\nRGB-D input image. The training data consists of a texture-mapped 3D object\nmodel or images of the object in known 6D poses. The benchmark comprises of: i)\neight datasets in a unified format that cover different practical scenarios,\nincluding two new datasets focusing on varying lighting conditions, ii) an\nevaluation methodology with a pose-error function that deals with pose\nambiguities, iii) a comprehensive evaluation of 15 diverse recent methods that\ncaptures the status quo of the field, and iv) an online evaluation system that\nis open for continuous submission of new results. The evaluation shows that\nmethods based on point-pair features currently perform best, outperforming\ntemplate matching methods, learning-based methods and methods based on 3D local\nfeatures. The project website is available at bop.felk.cvut.cz.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:38:53 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Hodan", "Tomas", ""], ["Michel", "Frank", ""], ["Brachmann", "Eric", ""], ["Kehl", "Wadim", ""], ["Buch", "Anders Glent", ""], ["Kraft", "Dirk", ""], ["Drost", "Bertram", ""], ["Vidal", "Joel", ""], ["Ihrke", "Stephan", ""], ["Zabulis", "Xenophon", ""], ["Sahin", "Caner", ""], ["Manhardt", "Fabian", ""], ["Tombari", "Federico", ""], ["Kim", "Tae-Kyun", ""], ["Matas", "Jiri", ""], ["Rother", "Carsten", ""]]}, {"id": "1808.08348", "submitter": "Kazuto Ichimaru", "authors": "Kazuto Ichimaru, Ryo Furukawa, Hiroshi Kawasaki", "title": "Multi-scale CNN stereo and pattern removal technique for underwater\n  active stereo system", "comments": "International Conference on 3D Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demands on capturing dynamic scenes of underwater environments are rapidly\ngrowing. Passive stereo is applicable to capture dynamic scenes, however the\nshape with textureless surfaces or irregular reflections cannot be recovered by\nthe technique. In our system, we add a pattern projector to the stereo camera\npair so that artificial textures are augmented on the objects. To use the\nsystem at underwater environments, several problems should be compensated,\ni.e., refraction, disturbance by fluctuation and bubbles. Further, since\nsurface of the objects are interfered by the bubbles, projected patterns, etc.,\nthose noises and patterns should be removed from captured images to recover\noriginal texture. To solve these problems, we propose three approaches; a\ndepth-dependent calibration, Convolutional Neural Network(CNN)-stereo method\nand CNN-based texture recovery method. A depth-dependent calibration is our\nanalysis to find the acceptable depth range for approximation by center\nprojection to find the certain target depth for calibration. In terms of CNN\nstereo, unlike common CNNbased stereo methods which do not consider strong\ndisturbances like refraction or bubbles, we designed a novel CNN architecture\nfor stereo matching using multi-scale information, which is intended to be\nrobust against such disturbances. Finally, we propose a multi-scale method for\nbubble and a projected-pattern removal method using CNNs to recover original\ntextures. Experimental results are shown to prove the effectiveness of our\nmethod compared with the state of the art techniques. Furthermore,\nreconstruction of a live swimming fish is demonstrated to confirm the\nfeasibility of our techniques.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 03:17:16 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Ichimaru", "Kazuto", ""], ["Furukawa", "Ryo", ""], ["Kawasaki", "Hiroshi", ""]]}, {"id": "1808.08374", "submitter": "He Huang", "authors": "He Huang, Yujing Shen, Jiankai Sun, Cewu Lu", "title": "NavigationNet: A Large-scale Interactive Indoor Navigation Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor navigation aims at performing navigation within buildings. In scenes\nlike home and factory, most intelligent mobile devices require an functionality\nof routing to guide itself precisely through indoor scenes to complete various\ntasks in order to serve human. In most scenarios, we expected an intelligent\ndevice capable of navigating itself in unseen environment. Although several\nsolutions have been proposed to deal with this issue, they usually require\npre-installed beacons or a map pre-built with SLAM, which means that they are\nnot capable of working in novel environments. To address this, we proposed\nNavigationNet, a computer vision dataset and benchmark to allow the utilization\nof deep reinforcement learning on scene-understanding-based indoor navigation.\nWe also proposed and formalized several typical indoor routing problems that\nare suitable for deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 08:24:10 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Huang", "He", ""], ["Shen", "Yujing", ""], ["Sun", "Jiankai", ""], ["Lu", "Cewu", ""]]}, {"id": "1808.08378", "submitter": "John McCormac", "authors": "John McCormac, Ronald Clark, Michael Bloesch, Andrew J. Davison,\n  Stefan Leutenegger", "title": "Fusion++: Volumetric Object-Level SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an online object-level SLAM system which builds a persistent and\naccurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera\nbrowses a cluttered indoor scene, Mask-RCNN instance segmentations are used to\ninitialise compact per-object Truncated Signed Distance Function (TSDF)\nreconstructions with object size-dependent resolutions and a novel 3D\nforeground mask. Reconstructed objects are stored in an optimisable 6DoF pose\ngraph which is our only persistent map representation. Objects are\nincrementally refined via depth fusion, and are used for tracking,\nrelocalisation and loop closure detection. Loop closures cause adjustments in\nthe relative pose estimates of object instances, but no intra-object warping.\nEach object also carries semantic information which is refined over time and an\nexistence probability to account for spurious instance predictions. We\ndemonstrate our approach on a hand-held RGB-D sequence from a cluttered office\nscene with a large number and variety of object instances, highlighting how the\nsystem closes loops and makes good use of existing objects on repeated loops.\nWe quantitatively evaluate the trajectory error of our system against a\nbaseline approach on the RGB-D SLAM benchmark, and qualitatively compare\nreconstruction quality of discovered objects on the YCB video dataset.\nPerformance evaluation shows our approach is highly memory efficient and runs\nonline at 4-8Hz (excluding relocalisation) despite not being optimised at the\nsoftware level.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 08:37:08 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 17:36:59 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["McCormac", "John", ""], ["Clark", "Ronald", ""], ["Bloesch", "Michael", ""], ["Davison", "Andrew J.", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "1808.08393", "submitter": "Ahsan Adeel", "authors": "Fengling Jiang, Bin Kong, Ahsan Adeel, Yun Xiao, and Amir Hussain", "title": "Saliency Detection via Bidirectional Absorbing Markov Chain", "comments": "To appear in the 9th International Conference on Brain Inspired\n  Cognitive Systems (BICS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional saliency detection via Markov chain only considers boundaries\nnodes. However, in addition to boundaries cues, background prior and foreground\nprior cues play a complementary role to enhance saliency detection. In this\npaper, we propose an absorbing Markov chain based saliency detection method\nconsidering both boundary information and foreground prior cues. The proposed\napproach combines both boundaries and foreground prior cues through\nbidirectional Markov chain. Specifically, the image is first segmented into\nsuperpixels and four boundaries nodes (duplicated as virtual nodes) are\nselected. Subsequently, the absorption time upon transition node's random walk\nto the absorbing state is calculated to obtain foreground possibility.\nSimultaneously, foreground prior as the virtual absorbing nodes is used to\ncalculate the absorption time and obtain the background possibility. Finally,\ntwo obtained results are fused to obtain the combined saliency map using cost\nfunction for further optimization at multi-scale. Experimental results\ndemonstrate the outperformance of our proposed model on 4 benchmark datasets as\ncompared to 17 state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 09:36:04 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Jiang", "Fengling", ""], ["Kong", "Bin", ""], ["Adeel", "Ahsan", ""], ["Xiao", "Yun", ""], ["Hussain", "Amir", ""]]}, {"id": "1808.08395", "submitter": "Jiang Zhang", "authors": "Jiang Zhang, Yuanqing Xia, Ganghui Shen", "title": "A Novel Deep Neural Network Architecture for Mars Visual Navigation", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems on May 17th, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, emerging deep learning techniques are leveraged to deal with\nMars visual navigation problem. Specifically, to achieve precise landing and\nautonomous navigation, a novel deep neural network architecture with double\nbranches and non-recurrent structure is designed, which can represent both\nglobal and local deep features of Martian environment images effectively. By\nemploying this architecture, Mars rover can determine the optimal navigation\npolicy to the target point directly from original Martian environment images.\nMoreover, compared with the existing state-of-the-art algorithm, the training\ntime is reduced by 45.8%. Finally, experiment results demonstrate that the\nproposed deep neural network architecture achieves better performance and\nfaster convergence than the existing ones and generalizes well to unknown\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 09:40:06 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zhang", "Jiang", ""], ["Xia", "Yuanqing", ""], ["Shen", "Ganghui", ""]]}, {"id": "1808.08396", "submitter": "Luisa Verdoliva", "authors": "Davide Cozzolino and Luisa Verdoliva", "title": "Noiseprint: a CNN-based camera model fingerprint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic analyses of digital images rely heavily on the traces of in-camera\nand out-camera processes left on the acquired images. Such traces represent a\nsort of camera fingerprint. If one is able to recover them, by suppressing the\nhigh-level scene content and other disturbances, a number of forensic tasks can\nbe easily accomplished. A notable example is the PRNU pattern, which can be\nregarded as a device fingerprint, and has received great attention in\nmultimedia forensics. In this paper we propose a method to extract a camera\nmodel fingerprint, called noiseprint, where the scene content is largely\nsuppressed and model-related artifacts are enhanced. This is obtained by means\nof a Siamese network, which is trained with pairs of image patches coming from\nthe same (label +1) or different (label -1) cameras. Although noiseprints can\nbe used for a large variety of forensic tasks, here we focus on image forgery\nlocalization. Experiments on several datasets widespread in the forensic\ncommunity show noiseprint-based methods to provide state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 09:44:26 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Cozzolino", "Davide", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1808.08402", "submitter": "Brian Kenji Iwana", "authors": "Shailza Jolly, Brian Kenji Iwana, Ryohei Kuroki, Seiichi Uchida", "title": "How do Convolutional Neural Networks Learn Design?", "comments": "Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to understand the design principles in book cover\nimages which are carefully crafted by experts. Book covers are designed in a\nunique way, specific to genres which convey important information to their\nreaders. By using Convolutional Neural Networks (CNN) to predict book genres\nfrom cover images, visual cues which distinguish genres can be highlighted and\nanalyzed. In order to understand these visual clues contributing towards the\ndecision of a genre, we present the application of Layer-wise Relevance\nPropagation (LRP) on the book cover image classification results. We use LRP to\nexplain the pixel-wise contributions of book cover design and highlight the\ndesign elements contributing towards particular genres. In addition, with the\nuse of state-of-the-art object and text detection methods, insights about\ngenre-specific book cover designs are discovered.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 10:34:05 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Jolly", "Shailza", ""], ["Iwana", "Brian Kenji", ""], ["Kuroki", "Ryohei", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1808.08410", "submitter": "Changhao Wu", "authors": "Changhao Wu, Shugong Xu, Guocong Song, Shunqing Zhang", "title": "How many labeled license plates are needed?", "comments": "12 pages, 3 figures, Chinese Conference on Pattern Recognition and\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a good deep learning model often requires a lot of annotated data.\nAs a large amount of labeled data is typically difficult to collect and even\nmore difficult to annotate, data augmentation and data generation are widely\nused in the process of training deep neural networks. However, there is no\nclear common understanding on how much labeled data is needed to get\nsatisfactory performance. In this paper, we try to address such a question\nusing vehicle license plate character recognition as an example application. We\napply computer graphic scripts and Generative Adversarial Networks to generate\nand augment a large number of annotated, synthesized license plate images with\nrealistic colors, fonts, and character composition from a small number of real,\nmanually labeled license plate images. Generated and augmented data are mixed\nand used as training data for the license plate recognition network modified\nfrom DenseNet. The experimental results show that the model trained from the\ngenerated mixed training data has good generalization ability, and the proposed\napproach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even\nwith a very limited number of original real license plates. In addition, the\naccuracy improvement caused by data generation becomes more significant when\nthe number of labeled images is reduced. Data augmentation also plays a more\nsignificant role when the number of labeled images is increased.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 11:11:42 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Wu", "Changhao", ""], ["Xu", "Shugong", ""], ["Song", "Guocong", ""], ["Zhang", "Shunqing", ""]]}, {"id": "1808.08426", "submitter": "Giovanni Poggi", "authors": "Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva", "title": "Analysis of adversarial attacks against CNN-based image forgery\n  detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ubiquitous diffusion of social networks, images are becoming a\ndominant and powerful communication channel. Not surprisingly, they are also\nincreasingly subject to manipulations aimed at distorting information and\nspreading fake news. In recent years, the scientific community has devoted\nmajor efforts to contrast this menace, and many image forgery detectors have\nbeen proposed. Currently, due to the success of deep learning in many\nmultimedia processing tasks, there is high interest towards CNN-based\ndetectors, and early results are already very promising. Recent studies in\ncomputer vision, however, have shown CNNs to be highly vulnerable to\nadversarial attacks, small perturbations of the input data which drive the\nnetwork towards erroneous classification. In this paper we analyze the\nvulnerability of CNN-based image forensics methods to adversarial attacks,\nconsidering several detectors and several types of attack, and testing\nperformance on a wide range of common manipulations, both easily and hardly\ndetectable.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 13:40:36 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gragnaniello", "Diego", ""], ["Marra", "Francesco", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1808.08473", "submitter": "Siyuan Qi", "authors": "Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, Song-Chun Zhu", "title": "Human-centric Indoor Scene Synthesis Using Stochastic Grammar", "comments": "This paper is published in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a human-centric method to sample and synthesize 3D room layouts\nand 2D images thereof, to obtain large-scale 2D/3D image data with perfect\nper-pixel ground truth. An attributed spatial And-Or graph (S-AOG) is proposed\nto represent indoor scenes. The S-AOG is a probabilistic grammar model, in\nwhich the terminal nodes are object entities. Human contexts as contextual\nrelations are encoded by Markov Random Fields (MRF) on the terminal nodes. We\nlearn the distributions from an indoor scene dataset and sample new layouts\nusing Monte Carlo Markov Chain. Experiments demonstrate that our method can\nrobustly sample a large variety of realistic room layouts based on three\ncriteria: (i) visual realism comparing to a state-of-the-art room arrangement\nmethod, (ii) accuracy of the affordance maps with respect to groundtruth, and\n(ii) the functionality and naturalness of synthesized rooms evaluated by human\nsubjects. The code is available at\nhttps://github.com/SiyuanQi/human-centric-scene-synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 21:44:18 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Huang", "Siyuan", ""], ["Jiang", "Chenfanfu", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1808.08480", "submitter": "Michel Fornaciali", "authors": "Alceu Bissoto, F\\'abio Perez, Vin\\'icius Ribeiro, Michel Fornaciali,\n  Sandra Avila, Eduardo Valle", "title": "Deep-Learning Ensembles for Skin-Lesion Segmentation, Analysis,\n  Classification: RECOD Titans at ISIC Challenge 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This extended abstract describes the participation of RECOD Titans in parts 1\nto 3 of the ISIC Challenge 2018 \"Skin Lesion Analysis Towards Melanoma\nDetection\" (MICCAI 2018). Although our team has a long experience with melanoma\nclassification and moderate experience with lesion segmentation, the ISIC\nChallenge 2018 was the very first time we worked on lesion attribute detection.\nFor each task we submitted 3 different ensemble approaches, varying\ncombinations of models and datasets. Our best results on the official testing\nset, regarding the official metric of each task, were: 0.728 (segmentation),\n0.344 (attribute detection) and 0.803 (classification). Those submissions\nreached, respectively, the 56th, 14th and 9th places.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 22:50:43 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bissoto", "Alceu", ""], ["Perez", "F\u00e1bio", ""], ["Ribeiro", "Vin\u00edcius", ""], ["Fornaciali", "Michel", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1808.08483", "submitter": "Mark Sabini", "authors": "Mark Sabini, Gili Rusak", "title": "Painting Outside the Box: Image Outpainting with GANs", "comments": "5 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenging task of image outpainting (extrapolation) has received\ncomparatively little attention in relation to its cousin, image inpainting\n(completion). Accordingly, we present a deep learning approach based on Iizuka\net al. for adversarially training a network to hallucinate past image\nboundaries. We use a three-phase training schedule to stably train a DCGAN\narchitecture on a subset of the Places365 dataset. In line with Iizuka et al.,\nwe also use local discriminators to enhance the quality of our output. Once\ntrained, our model is able to outpaint $128 \\times 128$ color images relatively\nrealistically, thus allowing for recursive outpainting. Our results show that\ndeep learning approaches to image outpainting are both feasible and promising.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 23:38:13 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Sabini", "Mark", ""], ["Rusak", "Gili", ""]]}, {"id": "1808.08509", "submitter": "Vandit Jain", "authors": "Vandit Jain, Prakhar Bansal, Abhinav Kumar Singh and Rajeev Srivastava", "title": "Efficient Single Image Super Resolution using Enhanced Learned Group\n  Convolutions", "comments": "Accepted in International Conference on Neural Information Processing\n  (ICONIP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have demonstrated great results for the\nsingle-image super-resolution (SISR) problem. Currently, most CNN algorithms\npromote deep and computationally expensive models to solve SISR. However, we\npropose a novel SISR method that uses relatively less number of computations.\nOn training, we get group convolutions that have unused connections removed. We\nhave refined this system specifically for the task at hand by removing\nunnecessary modules from original CondenseNet. Further, a reconstruction\nnetwork consisting of deconvolutional layers has been used in order to upscale\nto high resolution. All these steps significantly reduce the number of\ncomputations required at testing time. Along with this, bicubic upsampled input\nis added to the network output for easier learning. Our model is named\nSRCondenseNet. We evaluate the method using various benchmark datasets and show\nthat it performs favourably against the state-of-the-art methods in terms of\nboth accuracy and number of computations required.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 06:16:29 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Jain", "Vandit", ""], ["Bansal", "Prakhar", ""], ["Singh", "Abhinav Kumar", ""], ["Srivastava", "Rajeev", ""]]}, {"id": "1808.08528", "submitter": "Ahmet Sayar", "authors": "Suleyman Eken and Ahmet Sayar", "title": "A MapReduce based Big-data Framework for Object Extraction from Mosaic\n  Satellite Images", "comments": "Proceedings of Doctoral Consortium on Internet of Things At Roma,\n  Italy 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework stitching of vector representations of large scale\nraster mosaic images in distributed computing model. In this way, the negative\neffect of the lack of resources of the central system and scalability problem\ncan be eliminated. The product obtained by this study can be used in\napplications requiring spatial and temporal analysis on big satellite map\nimages. This study also shows that big data frameworks are not only used in\napplications of text-based data mining and machine learning algorithms, but\nalso used in applications of algorithms in image processing. The effectiveness\nof the product realized with this project is also going to be proven by\nscalability and performance tests performed on real world LandSat-8 satellite\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 10:32:40 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Eken", "Suleyman", ""], ["Sayar", "Ahmet", ""]]}, {"id": "1808.08531", "submitter": "Dongyu Liu", "authors": "Dongyu Liu, Weiwei Cui, Kai Jin, Yuxiao Guo, Huamin Qu", "title": "DeepTracker: Visualizing the Training Process of Convolutional Neural\n  Networks", "comments": "Published at ACM Transactions on Intelligent Systems and Technology\n  (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have achieved remarkable success in\nvarious fields. However, training an excellent CNN is practically a\ntrial-and-error process that consumes a tremendous amount of time and computer\nresources. To accelerate the training process and reduce the number of trials,\nexperts need to understand what has occurred in the training process and why\nthe resulting CNN behaves as such. However, current popular training platforms,\nsuch as TensorFlow, only provide very little and general information, such as\ntraining/validation errors, which is far from enough to serve this purpose. To\nbridge this gap and help domain experts with their training tasks in a\npractical environment, we propose a visual analytics system, DeepTracker, to\nfacilitate the exploration of the rich dynamics of CNN training processes and\nto identify the unusual patterns that are hidden behind the huge amount of\ntraining log. Specifically,we combine a hierarchical index mechanism and a set\nof hierarchical small multiples to help experts explore the entire training log\nfrom different levels of detail. We also introduce a novel cube-style\nvisualization to reveal the complex correlations among multiple types of\nheterogeneous training data including neuron weights, validation images, and\ntraining iterations. Three case studies are conducted to demonstrate how\nDeepTracker provides its users with valuable knowledge in an industry-level CNN\ntraining process, namely in our case, training ResNet-50 on the ImageNet\ndataset. We show that our method can be easily applied to other\nstate-of-the-art \"very deep\" CNN models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 11:09:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Liu", "Dongyu", ""], ["Cui", "Weiwei", ""], ["Jin", "Kai", ""], ["Guo", "Yuxiao", ""], ["Qu", "Huamin", ""]]}, {"id": "1808.08544", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Kazuya Iwami, Satoshi Ikehata, Kiyoharu Aizawa", "title": "Scale Drift Correction of Camera Geo-Localization using Geo-Tagged\n  Images", "comments": "ECCV Workshop CVRSUAD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera geo-localization from a monocular video is a fundamental task for\nvideo analysis and autonomous navigation. Although 3D reconstruction is a key\ntechnique to obtain camera poses, monocular 3D reconstruction in a large\nenvironment tends to result in the accumulation of errors in rotation,\ntranslation, and especially in scale: a problem known as scale drift. To\novercome these errors, we propose a novel framework that integrates incremental\nstructure from motion (SfM) and a scale drift correction method utilizing\ngeo-tagged images, such as those provided by Google Street View. Our correction\nmethod begins by obtaining sparse 6-DoF correspondences between the\nreconstructed 3D map coordinate system and the world coordinate system, by\nusing geo-tagged images. Then, it corrects scale drift by applying pose graph\noptimization over Sim(3) constraints and bundle adjustment. Experimental\nevaluations on large-scale datasets show that the proposed framework not only\nsufficiently corrects scale drift, but also achieves accurate geo-localization\nin a kilometer-scale environment.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 12:43:34 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Iwami", "Kazuya", ""], ["Ikehata", "Satoshi", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1808.08545", "submitter": "Xi-Le Zhao", "authors": "Ye-Tao Wang, Xi-Le Zhao, Tai-Xiang Jiang, Liang-Jian Deng, Yi Chang\n  and Ting-Zhu Huang", "title": "Rain Streak Removal for Single Image via Kernel Guided CNN", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2020.3015897", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streak removal is an important issue and has recently been investigated\nextensively. Existing methods, especially the newly emerged deep learning\nmethods, could remove the rain streaks well in many cases. However the\nessential factor in the generative procedure of the rain streaks, i.e., the\nmotion blur, which leads to the line pattern appearances, were neglected by the\ndeep learning rain streaks approaches and this resulted in over-derain or\nunder-derain results. In this paper, we propose a novel rain streak removal\napproach using a kernel guided convolutional neural network (KGCNN), achieving\nthe state-of-the-art performance with simple network architectures. We first\nmodel the rain streak interference with its motion blur mechanism. Then, our\nframework starts with learning the motion blur kernel, which is determined by\ntwo factors including angle and length, by a plain neural network, denoted as\nparameter net, from a patch of the texture component. Then, after a\ndimensionality stretching operation, the learned motion blur kernel is\nstretched into a degradation map with the same spatial size as the rainy patch.\nThe stretched degradation map together with the texture patch is subsequently\ninput into a derain convolutional network, which is a typical ResNet\narchitecture and trained to output the rain streaks with the guidance of the\nlearned motion blur kernel. Experiments conducted on extensive synthetic and\nreal data demonstrate the effectiveness of the proposed method, which preserves\nthe texture and the contrast while removing the rain streaks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 12:53:18 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 09:11:42 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Wang", "Ye-Tao", ""], ["Zhao", "Xi-Le", ""], ["Jiang", "Tai-Xiang", ""], ["Deng", "Liang-Jian", ""], ["Chang", "Yi", ""], ["Huang", "Ting-Zhu", ""]]}, {"id": "1808.08560", "submitter": "Amir Soleimani", "authors": "Amir Soleimani, Nasser M. Nasrabadi, Elias Griffith, Jason Ralph,\n  Simon Maskell", "title": "Convolutional Neural Networks for Aerial Vehicle Detection and\n  Recognition", "comments": "This paper has been accepted in the National Aerospace Electronics\n  Conference (NAECON) 2018 and would be indexed in IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of aerial vehicle recognition using a\ntext-guided deep convolutional neural network classifier. The network receives\nan aerial image and a desired class, and makes a yes or no output by matching\nthe image and the textual description of the desired class. We train and test\nour model on a synthetic aerial dataset and our desired classes consist of the\ncombination of the class types and colors of the vehicles. This strategy helps\nwhen considering more classes in testing than in training.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 14:29:57 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Soleimani", "Amir", ""], ["Nasrabadi", "Nasser M.", ""], ["Griffith", "Elias", ""], ["Ralph", "Jason", ""], ["Maskell", "Simon", ""]]}, {"id": "1808.08578", "submitter": "Jinming Duan", "authors": "Jinming Duan, Ghalib Bello, Jo Schlemper, Wenjia Bai, Timothy J W\n  Dawes, Carlo Biffi, Antonio de Marvao, Georgia Doumou, Declan P O'Regan,\n  Daniel Rueckert", "title": "Automatic 3D bi-ventricular segmentation of cardiac images by a\n  shape-refined multi-task deep learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have achieved state-of-the-art performance in\ncardiac magnetic resonance (CMR) image segmentation. However, most approaches\nhave focused on learning image intensity features for segmentation, whereas the\nincorporation of anatomical shape priors has received less attention. In this\npaper, we combine a multi-task deep learning approach with atlas propagation to\ndevelop a shape-constrained bi-ventricular segmentation pipeline for short-axis\nCMR volumetric images. The pipeline first employs a fully convolutional network\n(FCN) that learns segmentation and landmark localisation tasks simultaneously.\nThe architecture of the proposed FCN uses a 2.5D representation, thus combining\nthe computational advantage of 2D FCNs networks and the capability of\naddressing 3D spatial consistency without compromising segmentation accuracy.\nMoreover, the refinement step is designed to explicitly enforce a shape\nconstraint and improve segmentation quality. This step is effective for\novercoming image artefacts (e.g. due to different breath-hold positions and\nlarge slice thickness), which preclude the creation of anatomically meaningful\n3D cardiac shapes. The proposed pipeline is fully automated, due to network's\nability to infer landmarks, which are then used downstream in the pipeline to\ninitialise atlas propagation. We validate the pipeline on 1831 healthy subjects\nand 649 subjects with pulmonary hypertension. Extensive numerical experiments\non the two datasets demonstrate that our proposed method is robust and capable\nof producing accurate, high-resolution and anatomically smooth bi-ventricular\n3D models, despite the artefacts in input CMR volumes.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 15:42:50 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 13:18:46 GMT"}, {"version": "v3", "created": "Sat, 13 Jul 2019 19:39:39 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Duan", "Jinming", ""], ["Bello", "Ghalib", ""], ["Schlemper", "Jo", ""], ["Bai", "Wenjia", ""], ["Dawes", "Timothy J W", ""], ["Biffi", "Carlo", ""], ["de Marvao", "Antonio", ""], ["Doumou", "Georgia", ""], ["O'Regan", "Declan P", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1808.08601", "submitter": "Zhengqi Li", "authors": "Zhengqi Li and Noah Snavely", "title": "CGIntrinsics: Better Intrinsic Image Decomposition through\n  Physically-Based Rendering", "comments": "Paper for 'CGIntrinsics: Better Intrinsic Image Decomposition through\n  Physically-Based Rendering' published in ECCV, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic image decomposition is a challenging, long-standing computer vision\nproblem for which ground truth data is very difficult to acquire. We explore\nthe use of synthetic data for training CNN-based intrinsic image decomposition\nmodels, then applying these learned models to real-world images. To that end,\nwe present \\ICG, a new, large-scale dataset of physically-based rendered images\nof scenes with full ground truth decompositions. The rendering process we use\nis carefully designed to yield high-quality, realistic images, which we find to\nbe crucial for this problem domain. We also propose a new end-to-end training\nmethod that learns better decompositions by leveraging \\ICG, and optionally IIW\nand SAW, two recent datasets of sparse annotations on real-world images.\nSurprisingly, we find that a decomposition network trained solely on our\nsynthetic data outperforms the state-of-the-art on both IIW and SAW, and\nperformance improves even further when IIW and SAW data is added during\ntraining. Our work demonstrates the suprising effectiveness of\ncarefully-rendered synthetic data for the intrinsic images task.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 17:58:46 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2018 00:53:00 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 22:34:49 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Li", "Zhengqi", ""], ["Snavely", "Noah", ""]]}, {"id": "1808.08603", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Sung-Li Chiang, John Canny", "title": "Label and Sample: Efficient Training of Vehicle Object Detector from\n  Sparsely Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicle vision systems must deal with an extremely broad and\nchallenging set of scenes. They can potentially exploit an enormous amount of\ntraining data collected from vehicles in the field, but the volumes are too\nlarge to train offline naively. Not all training instances are equally valuable\nthough, and importance sampling can be used to prioritize which training images\nto collect. This approach assumes that objects in images are labeled with high\naccuracy. To generate accurate labels in the field, we exploit the\nspatio-temporal coherence of vehicle video. We use a near-to-far labeling\nstrategy by first labeling large, close objects in the video, and tracking them\nback in time to induce labels on small distant presentations of those objects.\nIn this paper we demonstrate the feasibility of this approach in several steps.\nFirst, we note that an optimal subset (relative to all the objects encountered\nand labeled) of labeled objects in images can be obtained by importance\nsampling using gradients of the recognition network. Next we show that these\ngradients can be approximated with very low error using the loss function,\nwhich is already available when the CNN is running inference. Then, we\ngeneralize these results to objects in a larger scene using an object detection\nsystem. Finally, we describe a self-labeling scheme using object tracking.\nObjects are tracked back in time (near-to-far) and labels of near objects are\nused to check accuracy of those objects in the far field. We then evaluate the\naccuracy of models trained on importance sampled data vs models trained on\ncomplete data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 18:10:23 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Pan", "Xinlei", ""], ["Chiang", "Sung-Li", ""], ["Canny", "John", ""]]}, {"id": "1808.08610", "submitter": "Snehasis Mukherjee", "authors": "Kushal Borkar and Snehasis Mukherjee", "title": "Single Image Dehazing Based on Generic Regularity", "comments": "Submitted to journal", "journal-ref": "This article is under consideration in the journal of Computer\n  Vision and Image Understanding, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel technique for single image dehazing. Most of the\nstate-of-the-art methods for single image dehazing relies either on Dark\nChannel Prior (DCP) or on Color line. The proposed method combines the two\ndifferent approaches. We initially compute the dark channel prior and then\napply a Nearest-Neighbor (NN) based regularization technique to obtain a smooth\ntransmission map of the hazy image. We consider the effect of airlight on the\nimage by using the color line model to assess the commitment of airlight in\neach patch of the image and interpolate at the local neighborhood where the\nestimate is unreliable. The NN based regularization of the DCP can remove the\nhaze, whereas, the color line based interpolation of airlight effect makes the\nproposed system robust against the variation of haze within an image due to\nmultiple light sources. The proposed method is tested on benchmark datasets and\nshows promising results compared to the state-of-the-art, including the deep\nlearning based methods, which require a huge computational setup. Moreover, the\nproposed method outperforms the recent deep learning based methods when applied\non images with sky regions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 18:47:08 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Borkar", "Kushal", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1808.08615", "submitter": "Ganapati Bhat", "authors": "Ganapati Bhat, Ranadeep Deb, Vatika Vardhan Chaurasia, Holly Shill,\n  Umit Y. Ogras", "title": "Online Human Activity Recognition using Low-Power Wearable Devices", "comments": "This is in proceedings of ICCAD 2018. The datasets are available at\n  https://github.com/gmbhat/human-activity-recognition", "journal-ref": null, "doi": "10.1145/3240765.3240833", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition~(HAR) has attracted significant research interest\ndue to its applications in health monitoring and patient rehabilitation. Recent\nresearch on HAR focuses on using smartphones due to their widespread use.\nHowever, this leads to inconvenient use, limited choice of sensors and\ninefficient use of resources, since smartphones are not designed for HAR. This\npaper presents the first HAR framework that can perform both online training\nand inference. The proposed framework starts with a novel technique that\ngenerates features using the fast Fourier and discrete wavelet transforms of a\ntextile-based stretch sensor and accelerometer. Using these features, we design\nan artificial neural network classifier which is trained online using the\npolicy gradient algorithm. Experiments on a low power IoT device (TI-CC2650\nMCU) with nine users show 97.7% accuracy in identifying six activities and\ntheir transitions with less than 12.5 mW power consumption.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 19:49:20 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 21:08:15 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Bhat", "Ganapati", ""], ["Deb", "Ranadeep", ""], ["Chaurasia", "Vatika Vardhan", ""], ["Shill", "Holly", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "1808.08671", "submitter": "Kwangsoo Shin", "authors": "Kwangsoo Shin, Junhyeong Jeon, Seungbin Lee, Boyoung Lim, Minsoo\n  Jeong, Jongho Nang", "title": "Approach for Video Classification with Multi-label on YouTube-8M Dataset", "comments": "Accepted at The 2nd Workshop on YouTube-8M Large-Scale Video\n  Understanding in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video traffic is increasing at a considerable rate due to the spread of\npersonal media and advancements in media technology. Accordingly, there is a\ngrowing need for techniques to automatically classify moving images. This paper\nuse NetVLAD and NetFV models and the Huber loss function for video\nclassification problem and YouTube-8M dataset to verify the experiment. We\ntried various attempts according to the dataset and optimize hyperparameters,\nultimately obtain a GAP score of 0.8668.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 02:56:56 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 15:14:56 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 14:21:29 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Shin", "Kwangsoo", ""], ["Jeon", "Junhyeong", ""], ["Lee", "Seungbin", ""], ["Lim", "Boyoung", ""], ["Jeong", "Minsoo", ""], ["Nang", "Jongho", ""]]}, {"id": "1808.08675", "submitter": "Somnuk Phon-Amnuaisuk", "authors": "Somnuk Phon-Amnuaisuk, Ken T. Murata, Praphan Pavarangkoon, Kazunori\n  Yamamoto, Takamichi Mizuhara", "title": "Exploring the Applications of Faster R-CNN and Single-Shot Multi-box\n  Detection in a Smart Nursery Domain", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of a baby detection task concerns detecting the presence of\na baby and other objects in a sequence of 2D images, tracking them and\nunderstanding the semantic contents of the scene. Recent advances in deep\nlearning and computer vision offer various powerful tools in general object\ndetection and can be applied to a baby detection task. In this paper, the\nFaster Region-based Convolutional Neural Network and the Single-Shot Multi-Box\nDetection approaches are explored. They are the two state-of-the-art object\ndetectors based on the region proposal tactic and the multi-box tactic. The\npresence of a baby in the scene obtained from these detectors, tested using\ndifferent pre-trained models, are discussed. This study is important since the\nbehaviors of these detectors in a baby detection task using different\npre-trained models are still not well understood. This exploratory study\nreveals many useful insights into the applications of these object detectors in\nthe smart nursery domain.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 03:16:32 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Phon-Amnuaisuk", "Somnuk", ""], ["Murata", "Ken T.", ""], ["Pavarangkoon", "Praphan", ""], ["Yamamoto", "Kazunori", ""], ["Mizuhara", "Takamichi", ""]]}, {"id": "1808.08685", "submitter": "Zixuan Huang", "authors": "Zixuan Huang, Junming Fan, Shenggan Cheng, Shuai Yi, Xiaogang Wang,\n  Hongsheng Li", "title": "HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse\n  Depth Completion", "comments": "IEEE Trans. on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth cues are important and have wide applications in various computer\nvision tasks. In autonomous driving, LIDAR sensors are adopted to acquire depth\nmeasurements around the vehicle to perceive the surrounding environments.\nHowever, depth maps obtained by LIDAR are generally sparse because of its\nhardware limitation. The task of depth completion attracts increasing\nattention, which aims at generating a dense depth map from an input sparse\ndepth map. To effectively utilize multi-scale features, we propose three novel\nsparsity-invariant operations, based on which, a sparsity-invariant multi-scale\nencoder-decoder network (HMS-Net) for handling sparse inputs and sparse feature\nmaps is also proposed. Additional RGB features could be incorporated to further\nimprove the depth completion performance. Our extensive experiments and\ncomponent analysis on two public benchmarks, KITTI depth completion benchmark\nand NYU-depth-v2 dataset, demonstrate the effectiveness of the proposed\napproach. As of Aug. 12th, 2018, on KITTI depth completion leaderboard, our\nproposed model without RGB guidance ranks first among all peer-reviewed methods\nwithout using RGB information, and our model with RGB guidance ranks second\namong all RGB-guided methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 04:43:34 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 18:19:25 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Huang", "Zixuan", ""], ["Fan", "Junming", ""], ["Cheng", "Shenggan", ""], ["Yi", "Shuai", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1808.08688", "submitter": "Yuchao Dai Dr.", "authors": "Xibin Song, Yuchao Dai, Xueying Qin", "title": "Deeply Supervised Depth Map Super-Resolution as Novel View Synthesis", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (T-CSVT) 2018", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2866399", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (DCNN) has been successfully applied to\ndepth map super-resolution and outperforms existing methods by a wide margin.\nHowever, there still exist two major issues with these DCNN based depth map\nsuper-resolution methods that hinder the performance: i) The low-resolution\ndepth maps either need to be up-sampled before feeding into the network or\nsubstantial deconvolution has to be used; and ii) The supervision\n(high-resolution depth maps) is only applied at the end of the network, thus it\nis difficult to handle large up-sampling factors, such as $\\times 8, \\times\n16$. In this paper, we propose a new framework to tackle the above problems.\nFirst, we propose to represent the task of depth map super-resolution as a\nseries of novel view synthesis sub-tasks. The novel view synthesis sub-task\naims at generating (synthesizing) a depth map from different camera pose, which\ncould be learned in parallel. Second, to handle large up-sampling factors, we\npresent a deeply supervised network structure to enforce strong supervision in\neach stage of the network. Third, a multi-scale fusion strategy is proposed to\neffectively exploit the feature maps at different scales and handle the\nblocking effect. In this way, our proposed framework could deal with\nchallenging depth map super-resolution efficiently under large up-sampling\nfactors (e.g. $\\times 8, \\times 16$). Our method only uses the low-resolution\ndepth map as input, and the support of color image is not needed, which greatly\nreduces the restriction of our method. Extensive experiments on various\nbenchmarking datasets demonstrate the superiority of our method over current\nstate-of-the-art depth map super-resolution methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 04:54:13 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Song", "Xibin", ""], ["Dai", "Yuchao", ""], ["Qin", "Xueying", ""]]}, {"id": "1808.08690", "submitter": "Yuchao Dai Dr.", "authors": "Yiran Zhong, Yuchao Dai, Hongdong Li", "title": "Stereo Computation for a Single Mixture Image", "comments": "Accepted by European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an original problem of \\emph{stereo computation from a\nsingle mixture image}-- a challenging problem that had not been researched\nbefore. The goal is to separate (\\ie, unmix) a single mixture image into two\nconstitute image layers, such that the two layers form a left-right stereo\nimage pair, from which a valid disparity map can be recovered. This is a\nseverely illposed problem, from one input image one effectively aims to recover\nthree (\\ie, left image, right image and a disparity map). In this work we give\na novel deep-learning based solution, by jointly solving the two subtasks of\nimage layer separation as well as stereo matching. Training our deep net is a\nsimple task, as it does not need to have disparity maps. Extensive experiments\ndemonstrate the efficacy of our method.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 05:03:37 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1808.08692", "submitter": "Zhenhua Chen", "authors": "Zhenhua Chen, David Crandall", "title": "Generalized Capsule Networks with Trainable Routing Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  CapsNet (Capsule Network) was first proposed by~\\citet{capsule} and later\nanother version of CapsNet was proposed by~\\citet{emrouting}. CapsNet has been\nproved effective in modeling spatial features with much fewer parameters.\nHowever, the routing procedures in both papers are not well incorporated into\nthe whole training process. The optimal number of routing procedure is misery\nwhich has to be found manually. To overcome this disadvantages of current\nrouting procedures in CapsNet, we embed the routing procedure into the\noptimization procedure with all other parameters in neural networks, namely,\nmake coupling coefficients in the routing procedure become completely\ntrainable. We call it Generalized CapsNet (G-CapsNet). We implement both\n\"full-connected\" version of G-CapsNet and \"convolutional\" version of G-CapsNet.\nG-CapsNet achieves a similar performance in the dataset MNIST as in the\noriginal papers. We also test two capsule packing method (cross feature maps or\nwith feature maps) from previous convolutional layers and see no evident\ndifference. Besides, we also explored possibility of stacking multiple capsule\nlayers. The code is shared on\n\\hyperlink{https://github.com/chenzhenhua986/CAFFE-CapsNet}{CAFFE-CapsNet}.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 05:44:19 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Chen", "Zhenhua", ""], ["Crandall", "David", ""]]}, {"id": "1808.08718", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Yuchen Fan, Jianchao Yang, Ning Xu, Zhaowen Wang, Xinchao\n  Wang, Thomas Huang", "title": "Wide Activation for Efficient and Accurate Image Super-Resolution", "comments": "tech report and factsheet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we demonstrate that with same parameters and computational\nbudgets, models with wider features before ReLU activation have significantly\nbetter performance for single image super-resolution (SISR). The resulted SR\nresidual network has a slim identity mapping pathway with wider (\\(2\\times\\) to\n\\(4\\times\\)) channels before activation in each residual block. To further\nwiden activation (\\(6\\times\\) to \\(9\\times\\)) without computational overhead,\nwe introduce linear low-rank convolution into SR networks and achieve even\nbetter accuracy-efficiency tradeoffs. In addition, compared with batch\nnormalization or no normalization, we find training with weight normalization\nleads to better accuracy for deep super-resolution networks. Our proposed SR\nnetwork \\textit{WDSR} achieves better results on large-scale DIV2K image\nsuper-resolution benchmark in terms of PSNR with same or lower computational\ncomplexity. Based on WDSR, our method also won 1st places in NTIRE 2018\nChallenge on Single Image Super-Resolution in all three realistic tracks.\nExperiments and ablation studies support the importance of wide activation for\nimage super-resolution. Code is released at:\nhttps://github.com/JiahuiYu/wdsr_ntire2018\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 07:48:21 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 02:42:59 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Yu", "Jiahui", ""], ["Fan", "Yuchen", ""], ["Yang", "Jianchao", ""], ["Xu", "Ning", ""], ["Wang", "Zhaowen", ""], ["Wang", "Xinchao", ""], ["Huang", "Thomas", ""]]}, {"id": "1808.08750", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, Heiko H.\n  Sch\\\"utt, Matthias Bethge, Felix A. Wichmann", "title": "Generalisation in humans and deep neural networks", "comments": "Added optimal probability aggregation method to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the robustness of humans and current convolutional deep neural\nnetworks (DNNs) on object recognition under twelve different types of image\ndegradations. First, using three well known DNNs (ResNet-152, VGG-19,\nGoogLeNet) we find the human visual system to be more robust to nearly all of\nthe tested image manipulations, and we observe progressively diverging\nclassification error-patterns between humans and DNNs when the signal gets\nweaker. Secondly, we show that DNNs trained directly on distorted images\nconsistently surpass human performance on the exact distortion types they were\ntrained on, yet they display extremely poor generalisation abilities when\ntested on other distortion types. For example, training on salt-and-pepper\nnoise does not imply robustness on uniform white noise and vice versa. Thus,\nchanges in the noise distribution between training and testing constitutes a\ncrucial challenge to deep learning vision systems that can be systematically\naddressed in a lifelong machine learning approach. Our new dataset consisting\nof 83K carefully measured human psychophysical trials provide a useful\nreference for lifelong robustness against image degradations set by the human\nvisual system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 09:17:57 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 16:26:58 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 09:05:30 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Geirhos", "Robert", ""], ["Temme", "Carlos R. Medina", ""], ["Rauber", "Jonas", ""], ["Sch\u00fctt", "Heiko H.", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "1808.08754", "submitter": "Ren Yang", "authors": "Jiaxin Lu, Mai Xu, Ren Yang, Zulin Wang", "title": "What Makes Natural Scene Memorable?", "comments": "Accepted to ACM MM Workshops", "journal-ref": "Proceedings of the 2018 Workshop on Understanding Subjective\n  Attributes of Data, with the Focus on Evoked Emotions", "doi": "10.1145/3267799.3267802", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on image memorability have shed light on the visual features\nthat make generic images, object images or face photographs memorable. However,\na clear understanding and reliable estimation of natural scene memorability\nremain elusive. In this paper, we provide an attempt to answer: \"what exactly\nmakes natural scene memorable\". Specifically, we first build LNSIM, a\nlarge-scale natural scene image memorability database (containing 2,632 images\nand memorability annotations). Then, we mine our database to investigate how\nlow-, middle- and high-level handcrafted features affect the memorability of\nnatural scene. In particular, we find that high-level feature of scene category\nis rather correlated with natural scene memorability. Thus, we propose a deep\nneural network based natural scene memorability (DeepNSM) predictor, which\ntakes advantage of scene category. Finally, the experimental results validate\nthe effectiveness of DeepNSM.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 09:38:16 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Lu", "Jiaxin", ""], ["Xu", "Mai", ""], ["Yang", "Ren", ""], ["Wang", "Zulin", ""]]}, {"id": "1808.08779", "submitter": "Liu Liu", "authors": "Liu Liu and Hongdong Li and Yuchao Dai", "title": "Stochastic Attraction-Repulsion Embedding for Large Scale Image\n  Localization", "comments": "ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of large-scale image-based localization (IBL)\nwhere the spatial location of a query image is determined by finding out the\nmost similar reference images in a large database. For solving this problem, a\ncritical task is to learn discriminative image representation that captures\ninformative information relevant for localization. We propose a novel\nrepresentation learning method having higher location-discriminating power. It\nprovides the following contributions: 1) we represent a place (location) as a\nset of exemplar images depicting the same landmarks and aim to maximize\nsimilarities among intra-place images while minimizing similarities among\ninter-place images; 2) we model a similarity measure as a probability\ndistribution on L_2-metric distances between intra-place and inter-place image\nrepresentations; 3) we propose a new Stochastic Attraction and Repulsion\nEmbedding (SARE) loss function minimizing the KL divergence between the learned\nand the actual probability distributions; 4) we give theoretical comparisons\nbetween SARE, triplet ranking and contrastive losses. It provides insights into\nwhy SARE is better by analyzing gradients. Our SARE loss is easy to implement\nand pluggable to any CNN. Experiments show that our proposed method improves\nthe localization performance on standard benchmarks by a large margin.\nDemonstrating the broad applicability of our method, we obtained the third\nplace out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our\ncode and model are available at https://github.com/Liumouliu/deepIBL.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:52:44 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 08:00:58 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Liu", "Liu", ""], ["Li", "Hongdong", ""], ["Dai", "Yuchao", ""]]}, {"id": "1808.08802", "submitter": "Xiao Song", "authors": "Xiao Song, Xu Zhao, Liangji Fang, Tianwei Lin", "title": "Discriminative Representation Combinations for Accurate Face Spoofing\n  Detection", "comments": "To be published in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three discriminative representations for face presentation attack detection\nare introduced in this paper. Firstly we design a descriptor called spatial\npyramid coding micro-texture (SPMT) feature to characterize local appearance\ninformation. Secondly we utilize the SSD, which is a deep learning framework\nfor detection, to excavate context cues and conduct end-to-end face\npresentation attack detection. Finally we design a descriptor called template\nface matched binocular depth (TFBD) feature to characterize stereo structures\nof real and fake faces. For accurate presentation attack detection, we also\ndesign two kinds of representation combinations. Firstly, we propose a\ndecision-level cascade strategy to combine SPMT with SSD. Secondly, we use a\nsimple score fusion strategy to combine face structure cues (TFBD) with local\nmicro-texture features (SPMT). To demonstrate the effectiveness of our design,\nwe evaluate the representation combination of SPMT and SSD on three public\ndatasets, which outperforms all other state-of-the-art methods. In addition, we\nevaluate the representation combination of SPMT and TFBD on our dataset and\nexcellent performance is also achieved.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 12:01:06 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 01:17:57 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Song", "Xiao", ""], ["Zhao", "Xu", ""], ["Fang", "Liangji", ""], ["Lin", "Tianwei", ""]]}, {"id": "1808.08803", "submitter": "Linchao Zhu", "authors": "Ke Ning, Linchao Zhu, Ming Cai, Yi Yang, Di Xie, Fei Wu", "title": "Attentive Sequence to Sequence Translation for Localizing Clips of\n  Interest by Natural Language Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel attentive sequence to sequence translator (ASST) for clip\nlocalization in videos by natural language descriptions. We make two\ncontributions. First, we propose a bi-directional Recurrent Neural Network\n(RNN) with a finely calibrated vision-language attentive mechanism to\ncomprehensively understand the free-formed natural language descriptions. The\nRNN parses natural language descriptions in two directions, and the attentive\nmodel attends every meaningful word or phrase to each frame, thereby resulting\nin a more detailed understanding of video content and description semantics.\nSecond, we design a hierarchical architecture for the network to jointly model\nlanguage descriptions and video content. Given a video-description pair, the\nnetwork generates a matrix representation, i.e., a sequence of vectors. Each\nvector in the matrix represents a video frame conditioned by the description.\nThe 2D representation not only preserves the temporal dependencies of frames\nbut also provides an effective way to perform frame-level video-language\nmatching. The hierarchical architecture exploits video content with multiple\ngranularities, ranging from subtle details to global context. Integration of\nthe multiple granularities yields a robust representation for multi-level\nvideo-language abstraction. We validate the effectiveness of our ASST on two\nlarge-scale datasets. Our ASST outperforms the state-of-the-art by $4.28\\%$ in\nRank$@1$ on the DiDeMo dataset. On the Charades-STA dataset, we significantly\nimprove the state-of-the-art by $13.41\\%$ in Rank$@1,IoU=0.5$.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 12:01:26 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Ning", "Ke", ""], ["Zhu", "Linchao", ""], ["Cai", "Ming", ""], ["Yang", "Yi", ""], ["Xie", "Di", ""], ["Wu", "Fei", ""]]}, {"id": "1808.08834", "submitter": "Ilchae Jung", "authors": "Ilchae Jung, Jeany Son, Mooyeol Baek, and Bohyung Han", "title": "Real-Time MDNet", "comments": "16 pages, 8 figures, accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and accurate visual tracking algorithm based on the\nmulti-domain convolutional neural network (MDNet). The proposed approach\naccelerates feature extraction procedure and learns more discriminative models\nfor instance classification; it enhances representation quality of target and\nbackground by maintaining a high resolution feature map with a large receptive\nfield per activation. We also introduce a novel loss term to differentiate\nforeground instances across multiple domains and learn a more discriminative\nembedding of target objects with similar semantics. The proposed techniques are\nintegrated into the pipeline of a well known CNN-based visual tracking\nalgorithm, MDNet. We accomplish approximately 25 times speed-up with almost\nidentical accuracy compared to MDNet. Our algorithm is evaluated in multiple\npopular tracking benchmark datasets including OTB2015, UAV123, and TempleColor,\nand outperforms the state-of-the-art real-time tracking methods consistently\neven without dataset-specific parameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 13:13:14 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Jung", "Ilchae", ""], ["Son", "Jeany", ""], ["Baek", "Mooyeol", ""], ["Han", "Bohyung", ""]]}, {"id": "1808.08867", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan", "title": "Facial Information Recovery from Heavily Damaged Images using Generative\n  Adversarial Network- PART 1", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Over the past decades, a large number of techniques have emerged in modern\nimaging systems to capture the exact information of the original scene\nregardless of shake, motion, lighting conditions and etc., These developments\nhave progressively addressed the acquisition of images in high speed and high\nresolutions. However, the various ineradicable real-time factors cause the\ndegradation of the information and the quality of the acquired images. The\navailable techniques are not intelligent enough to generalize this complex\nphenomenon. Hence, it is necessary to develop an intellectual framework to\nrecover the possible information presented in the original scene. In this\narticle, we propose a kernel free framework based on conditional-GAN to recover\nthe information from the heavily damaged images. The degradation of images is\nassumed to be occurred by the combination of a various blur. Learning parameter\nof the cGAN is optimized by multi-component loss function that includes\nimproved wasserstein loss with regression loss function. The generator module\nof this network is developed by using U-Net architecture with local Residual\nconnections and global skip connection. Local connections and a global skip\nconnection are implemented for the utilization of all stages of features.\nGenerated images show that the network has the potential to recover the\nprobable information of blurred images from the learned features. This research\nwork is carried out as a part of our IOP studio software 'Facial recognition\nmodule'.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 14:45:08 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Murugan", "Pushparaja", ""]]}, {"id": "1808.08885", "submitter": "Heyi Li", "authors": "Heyi Li, Dongdong Chen, Bill Nailon, Mike Davies, Dave Laurenson", "title": "Improved Breast Mass Segmentation in Mammograms with Conditional\n  Residual U-net", "comments": "To appear in MICCAI 2018, Breast Image Analysis Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of deep learning for breast mass segmentation in\nmammograms. By integrating the merits of residual learning and probabilistic\ngraphical modelling with standard U-Net, we propose a new deep network,\nConditional Residual U-Net (CRU-Net), to improve the U-Net segmentation\nperformance. Benefiting from the advantage of probabilistic graphical modelling\nin the pixel-level labelling, and the structure insights of a deep residual\nnetwork in the feature extraction, the CRU-Net provides excellent mass\nsegmentation performance. Evaluations based on INbreast and DDSM-BCRP datasets\ndemonstrate that the CRU-Net achieves the best mass segmentation performance\ncompared to the state-of-art methodologies. Moreover, neither tedious\npre-processing nor post-processing techniques are not required in our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 15:34:01 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Heyi", ""], ["Chen", "Dongdong", ""], ["Nailon", "Bill", ""], ["Davies", "Mike", ""], ["Laurenson", "Dave", ""]]}, {"id": "1808.08891", "submitter": "Anurag Illendula", "authors": "Anurag Illendula and Kv Manohar and Manish Reddy Yedulla", "title": "Which Emoji Talks Best for My Picture?", "comments": "Accepted at the 2018 IEEE/WIC/ACM International Conference on Web\n  Intelligence (WI '18), December 3-6, 2018, Santiago de Chile", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emojis have evolved as complementary sources for expressing emotion in\nsocial-media platforms where posts are mostly composed of texts and images. In\norder to increase the expressiveness of the social media posts, users associate\nrelevant emojis with their posts. Incorporating domain knowledge has improved\nmachine understanding of text. In this paper, we investigate whether domain\nknowledge for emoji can improve the accuracy of emoji recommendation task in\ncase of multimedia posts composed of image and text. Our emoji recommendation\ncan suggest accurate emojis by exploiting both visual and textual content from\nsocial media posts as well as domain knowledge from Emojinet. Experimental\nresults using pre-trained image classifiers and pre-trained word embedding\nmodels on Twitter dataset show that our results outperform the current\nstate-of-the-art by 9.6\\%. We also present a user study evaluation of our\nrecommendation system on a set of images chosen from MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 15:43:28 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Illendula", "Anurag", ""], ["Manohar", "Kv", ""], ["Yedulla", "Manish Reddy", ""]]}, {"id": "1808.08931", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang and Shuiwang Ji", "title": "Smoothed Dilated Convolutions for Improved Dense Prediction", "comments": "The original version was accepted by KDD2018. Code is publicly\n  available at https://github.com/divelab/dilated", "journal-ref": "In Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (pp. 2486-2495). 2018", "doi": "10.1145/3219819.3219944", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dilated convolutions, also known as atrous convolutions, have been widely\nexplored in deep convolutional neural networks (DCNNs) for various dense\nprediction tasks. However, dilated convolutions suffer from the gridding\nartifacts, which hampers the performance. In this work, we propose two simple\nyet effective degridding methods by studying a decomposition of dilated\nconvolutions. Unlike existing models, which explore solutions by focusing on a\nblock of cascaded dilated convolutional layers, our methods address the\ngridding artifacts by smoothing the dilated convolution itself. In addition, we\npoint out that the two degridding approaches are intrinsically related and\ndefine separable and shared (SS) operations, which generalize the proposed\nmethods. We further explore SS operations in view of operations on graphs and\npropose the SS output layer, which is able to smooth the entire DCNNs by only\nreplacing the output layer. We evaluate our degridding methods and the SS\noutput layer thoroughly, and visualize the smoothing effect through effective\nreceptive field analysis. Results show that our methods degridding yield\nconsistent improvements on the performance of dense prediction tasks, while\nadding negligible amounts of extra training parameters. And the SS output layer\nimproves the performance significantly and is very efficient in terms of number\nof training parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 17:13:38 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 23:04:40 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1808.08941", "submitter": "Stephan Baier", "authors": "Stephan Baier, Yunpu Ma, Volker Tresp", "title": "Improving Information Extraction from Images with Learned Semantic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require an understanding of an image that goes beyond the\nsimple detection and classification of its objects. In particular, a great deal\nof semantic information is carried in the relationships between objects. We\nhave previously shown that the combination of a visual model and a statistical\nsemantic prior model can improve on the task of mapping images to their\nassociated scene description. In this paper, we review the model and compare it\nto a novel conditional multi-way model for visual relationship detection, which\ndoes not include an explicitly trained visual prior model. We also discuss\npotential relationships between the proposed methods and memory models of the\nhuman brain.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 17:39:56 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Baier", "Stephan", ""], ["Ma", "Yunpu", ""], ["Tresp", "Volker", ""]]}, {"id": "1808.08993", "submitter": "Sheng He", "authors": "Sheng He and Lambert Schomaker", "title": "Open Set Chinese Character Recognition using Multi-typed Attributes", "comments": "29 pages, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recognition of Off-line Chinese characters is still a challenging problem,\nespecially in historical documents, not only in the number of classes extremely\nlarge in comparison to contemporary image retrieval methods, but also new\nunseen classes can be expected under open learning conditions (even for CNN).\nChinese character recognition with zero or a few training samples is a\ndifficult problem and has not been studied yet. In this paper, we propose a new\nChinese character recognition method by multi-type attributes, which are based\non pronunciation, structure and radicals of Chinese characters, applied to\ncharacter recognition in historical books. This intermediate attribute code has\na strong advantage over the common `one-hot' class representation because it\nallows for understanding complex and unseen patterns symbolically using\nattributes. First, each character is represented by four groups of attribute\ntypes to cover a wide range of character possibilities: Pinyin label, layout\nstructure, number of strokes, three different input methods such as Cangjie,\nZhengma and Wubi, as well as a four-corner encoding method. A convolutional\nneural network (CNN) is trained to learn these attributes. Subsequently,\ncharacters can be easily recognized by these attributes using a distance metric\nand a complete lexicon that is encoded in attribute space. We evaluate the\nproposed method on two open data sets: printed Chinese character recognition\nfor zero-shot learning, historical characters for few-shot learning and a\nclosed set: handwritten Chinese characters. Experimental results show a good\ngeneral classification of seen classes but also a very promising generalization\nability to unseen characters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 18:53:31 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["He", "Sheng", ""], ["Schomaker", "Lambert", ""]]}, {"id": "1808.09001", "submitter": "Eran Dahan", "authors": "Eran Dahan and Tzvi Diskin", "title": "COFGA: Classification Of Fine-Grained Features In Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification between thousands of classes in high-resolution images is one\nof the heavily studied problems in deep learning over the last decade. However,\nthe challenge of fine-grained multi-class classification of objects in aerial\nimages, especially in low resource cases, is still challenging and an active\narea of research in the literature. Solving this problem can give rise to\nvarious applications in the field of scene understanding and classification and\nre-identification of specific objects from aerial images. In this paper, we\nprovide a description of our dataset - COFGA of multi-class annotated objects\nin aerial images. We examine the results of existing state-of-the-art models\nand modified deep neural networks. Finally, we explain in detail the first\npublished competition for solving this task.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 19:09:51 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Dahan", "Eran", ""], ["Diskin", "Tzvi", ""]]}, {"id": "1808.09016", "submitter": "Xianshan Qu", "authors": "Xianshan Qu, Xiaopeng Li, John R. Rose", "title": "Review Helpfulness Assessment based on Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the implementation of a convolutional neural\nnetwork (CNN) used to assess online review helpfulness. To our knowledge, this\nis the first use of this architecture to address this problem. We explore the\nimpact of two related factors impacting CNN performance: different word\nembedding initializations and different input review lengths. We also propose\nan approach to combining rating star information with review text to further\nimprove prediction accuracy. We demonstrate that this can improve the overall\naccuracy by 2%. Finally, we evaluate the method on a benchmark dataset and show\nan improvement in accuracy relative to published results for traditional\nmethods of 2.5% for a model trained using only review text and 4.24% for a\nmodel trained on a combination of rating star information and review text.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 19:53:52 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Qu", "Xianshan", ""], ["Li", "Xiaopeng", ""], ["Rose", "John R.", ""]]}, {"id": "1808.09023", "submitter": "Mizanur Rahman", "authors": "Mizanur Rahman, Mhafuzul Islam, Jon Calhoun and Mashrur Chowdhury", "title": "Real-time Pedestrian Detection Approach with an Efficient Data\n  Communication Bandwidth Strategy", "comments": "20 pages, 6 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1506.02640 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle-to-Pedestrian (V2P) communication can significantly improve\npedestrian safety at a signalized intersection. It is unlikely that pedestrians\nwill carry a low latency communication enabled device and activate a pedestrian\nsafety application in their hand-held device all the time. Because of this\nlimitation, multiple traffic cameras at the signalized intersection can be used\nto accurately detect and locate pedestrians using deep learning and broadcast\nsafety alerts related to pedestrians to warn connected and automated vehicles\naround a signalized intersection. However, unavailability of high-performance\ncomputing infrastructure at the roadside and limited network bandwidth between\ntraffic cameras and the computing infrastructure limits the ability of\nreal-time data streaming and processing for pedestrian detection. In this\npaper, we develop an edge computing based real-time pedestrian detection\nstrategy combining pedestrian detection algorithm using deep learning and an\nefficient data communication approach to reduce bandwidth requirements while\nmaintaining a high object detection accuracy. We utilize a lossy compression\ntechnique on traffic camera data to determine the tradeoff between the\nreduction of the communication bandwidth requirements and a defined object\ndetection accuracy. The performance of the pedestrian-detection strategy is\nmeasured in terms of pedestrian classification accuracy with varying peak\nsignal-to-noise ratios. The analyses reveal that we detect pedestrians by\nmaintaining a defined detection accuracy with a peak signal-to-noise ratio\n(PSNR) 43 dB while reducing the communication bandwidth from 9.82 Mbits/sec to\n0.31 Mbits/sec, a 31x reduction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 20:13:47 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 21:06:52 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 04:43:18 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Rahman", "Mizanur", ""], ["Islam", "Mhafuzul", ""], ["Calhoun", "Jon", ""], ["Chowdhury", "Mashrur", ""]]}, {"id": "1808.09044", "submitter": "Lluis Gomez", "authors": "Llu\\'is G\\'omez and Andr\\'es Mafla and Mar\\c{c}al Rusi\\~nol and\n  Dimosthenis Karatzas", "title": "Single Shot Scene Text Retrieval", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual information found in scene images provides high level semantic\ninformation about the image and its context and it can be leveraged for better\nscene understanding. In this paper we address the problem of scene text\nretrieval: given a text query, the system must return all images containing the\nqueried text. The novelty of the proposed model consists in the usage of a\nsingle shot CNN architecture that predicts at the same time bounding boxes and\na compact text representation of the words in them. In this way, the text based\nimage retrieval task can be casted as a simple nearest neighbor search of the\nquery text representation over the outputs of the CNN over the entire image\ndatabase. Our experiments demonstrate that the proposed architecture\noutperforms previous state-of-the-art while it offers a significant increase in\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 21:59:26 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["G\u00f3mez", "Llu\u00eds", ""], ["Mafla", "Andr\u00e9s", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1808.09102", "submitter": "Pengze Liu", "authors": "Pengze Liu, Xihui Liu, Junjie Yan, Jing Shao", "title": "Localization Guided Learning for Pedestrian Attribute Recognition", "comments": "Accepted by BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute recognition has attracted many attentions due to its\nwide applications in scene understanding and person analysis from surveillance\nvideos. Existing methods try to use additional pose, part or viewpoint\ninformation to complement the global feature representation for attribute\nclassification. However, these methods face difficulties in localizing the\nareas corresponding to different attributes. To address this problem, we\npropose a novel Localization Guided Network which assigns attribute-specific\nweights to local features based on the affinity between proposals pre-extracted\nproposals and attribute locations. The advantage of our model is that our local\nfeatures are learned automatically for each attribute and emphasized by the\ninteraction with global features. We demonstrate the effectiveness of our\nLocalization Guided Network on two pedestrian attribute benchmarks (PA-100K and\nRAP). Our result surpasses the previous state-of-the-art in all five metrics on\nboth datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 03:39:43 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Liu", "Pengze", ""], ["Liu", "Xihui", ""], ["Yan", "Junjie", ""], ["Shao", "Jing", ""]]}, {"id": "1808.09128", "submitter": "Rui Fan", "authors": "Han Ma, Yixin Ma, Jianhao Jiao, M Usman Maqbool Bhutta, Mohammud\n  Junaid Bocus, Lujia Wang, Ming Liu, Rui Fan", "title": "Multiple Lane Detection Algorithm Based on Optimised Dense Disparity Map\n  Estimation", "comments": "5 pages, 7 figures, IEEE International Conference on Imaging Systems\n  and Techniques (IST) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection is very important for self-driving vehicles. In recent years,\ncomputer stereo vision has been prevalently used to enhance the accuracy of the\nlane detection systems. This paper mainly presents a multiple lane detection\nalgorithm developed based on optimised dense disparity map estimation, where\nthe disparity information obtained at time t_{n} is utilised to optimise the\nprocess of disparity estimation at time t_{n+1}. This is achieved by estimating\nthe road model at time t_{n} and then controlling the search range for the\ndisparity estimation at time t_{n+1}. The lanes are then detected using our\npreviously published algorithm, where the vanishing point information is used\nto model the lanes. The experimental results illustrate that the runtime of the\ndisparity estimation is reduced by around 37% and the accuracy of the lane\ndetection is about 99%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 05:42:37 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ma", "Han", ""], ["Ma", "Yixin", ""], ["Jiao", "Jianhao", ""], ["Bhutta", "M Usman Maqbool", ""], ["Bocus", "Mohammud Junaid", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""], ["Fan", "Rui", ""]]}, {"id": "1808.09162", "submitter": "Alessandro Betti", "authors": "Alessandro Betti, Marco Gori, Stefano Melacci", "title": "Cognitive Action Laws: The Case of Visual Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a theory for understanding perceptual learning processes\nwithin the general framework of laws of nature. Neural networks are regarded as\nsystems whose connections are Lagrangian variables, namely functions depending\non time. They are used to minimize the cognitive action, an appropriate\nfunctional index that measures the agent interactions with the environment. The\ncognitive action contains a potential and a kinetic term that nicely resemble\nthe classic formulation of regularization in machine learning. A special choice\nof the functional index, which leads to forth-order differential\nequations---Cognitive Action Laws (CAL)---exhibits a structure that mirrors\nclassic formulation of machine learning. In particular, unlike the action of\nmechanics, the stationarity condition corresponds with the global minimum.\nMoreover, it is proven that typical asymptotic learning conditions on the\nweights can coexist with the initialization provided that the system dynamics\nis driven under a policy referred to as information overloading control.\nFinally, the theory is experimented for the problem of feature extraction in\ncomputer vision.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 08:12:23 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""], ["Melacci", "Stefano", ""]]}, {"id": "1808.09166", "submitter": "Guodong Xu", "authors": "Guodong Xu, Chaoqiang Liu, Hui Ji", "title": "Removing out-of-focus blur from a single image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducing an all-in-focus image from an image with defocus regions is of\npractical value in many applications, eg, digital photography, and robotics.\nUsing the output of some existing defocus map estimator, existing approaches\nfirst segment a de-focused image into multiple regions blurred by Gaussian\nkernels with different variance each, and then de-blur each region using the\ncorresponding Gaussian kernel. In this paper, we proposed a blind deconvolution\nmethod specifically designed for removing defocus blurring from an image, by\nproviding effective solutions to two critical problems: 1) suppressing the\nartifacts caused by segmentation error by introducing an additional variable\nregularized by weighted $\\ell_0$-norm; and 2) more accurate defocus kernel\nestimation using non-parametric symmetry and low-rank based constraints on the\nkernel. The experiments on real datasets showed the advantages of the proposed\nmethod over existing ones, thanks to the effective treatments of the two\nimportant issues mentioned above during deconvolution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 08:28:23 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Xu", "Guodong", ""], ["Liu", "Chaoqiang", ""], ["Ji", "Hui", ""]]}, {"id": "1808.09170", "submitter": "Math\\'e Zeegers", "authors": "Math\\'e Zeegers, Felix Lucka and Kees Joost Batenburg", "title": "A Multi-channel DART Algorithm", "comments": "16 pages. 17 figures. Paper for IWCIA 2018 conference", "journal-ref": null, "doi": "10.1007/978-3-030-05288-1_13", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomography deals with the reconstruction of objects from their projections,\nacquired along a range of angles. Discrete tomography is concerned with objects\nthat consist of a small number of materials, which makes it possible to compute\naccurate reconstructions from highly limited projection data. For cases where\nthe allowed intensity values in the reconstruction are known a priori, the\ndiscrete algebraic reconstruction technique (DART) has shown to yield accurate\nreconstructions from few projections. However, a key limitation is that the\nbenefit of DART diminishes as the number of different materials increases. Many\ntomographic imaging techniques can simultaneously record tomographic data at\nmultiple channels, each corresponding to a different weighting of the materials\nin the object. Whenever projection data from more than one channel is\navailable, this additional information can potentially be exploited by the\nreconstruction algorithm. In this paper we present Multi-Channel DART\n(MC-DART), which deals effectively with multi-channel data. This class of\nalgorithms is a generalization of DART to multiple channels and combines the\ninformation for each separate channel-reconstruction in a multi-channel\nsegmentation step. We demonstrate that in a range of simulation experiments,\nMC-DART is capable of producing more accurate reconstructions compared to\nsingle-channel DART.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 08:41:55 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Zeegers", "Math\u00e9", ""], ["Lucka", "Felix", ""], ["Batenburg", "Kees Joost", ""]]}, {"id": "1808.09183", "submitter": "Wassim Swaileh", "authors": "Wassim Swaileh, Yann Soullard, Thierry Paquet", "title": "A Unified Multilingual Handwriting Recognition System using multigrams\n  sub-lexical units", "comments": "preprint", "journal-ref": "Pattern Recognition Letter 2018", "doi": "10.1016/j.patrec.2018.07.027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the design of a unified multilingual system for handwriting\nrecognition. Most of multi- lingual systems rests on specialized models that\nare trained on a single language and one of them is selected at test time.\nWhile some recognition systems are based on a unified optical model, dealing\nwith a unified language model remains a major issue, as traditional language\nmodels are generally trained on corpora composed of large word lexicons per\nlanguage. Here, we bring a solution by con- sidering language models based on\nsub-lexical units, called multigrams. Dealing with multigrams strongly reduces\nthe lexicon size and thus decreases the language model complexity. This makes\npos- sible the design of an end-to-end unified multilingual recognition system\nwhere both a single optical model and a single language model are trained on\nall the languages. We discuss the impact of the language unification on each\nmodel and show that our system reaches state-of-the-art methods perfor- mance\nwith a strong reduction of the complexity.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 09:06:13 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Swaileh", "Wassim", ""], ["Soullard", "Yann", ""], ["Paquet", "Thierry", ""]]}, {"id": "1808.09208", "submitter": "Muhammad Jameel Malik", "authors": "Jameel Malik, Ahmed Elhayek, Fabrizio Nunnari, Kiran Varanasi, Kiarash\n  Tamaddon, Alexis Heloir, Didier Stricker", "title": "DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning\n  from Synthetic Depth", "comments": "Accepted for publication in 3DV-2018 (http://3dv18.uniud.it/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulated hand pose and shape estimation is an important problem for\nvision-based applications such as augmented reality and animation. In contrast\nto the existing methods which optimize only for joint positions, we propose a\nfully supervised deep network which learns to jointly estimate a full 3D hand\nmesh representation and pose from a single depth image. To this end, a CNN\narchitecture is employed to estimate parametric representations i.e. hand pose,\nbone scales and complex shape parameters. Then, a novel hand pose and shape\nlayer, embedded inside our deep framework, produces 3D joint positions and hand\nmesh. Lack of sufficient training data with varying hand shapes limits the\ngeneralized performance of learning based methods. Also, manually annotating\nreal data is suboptimal. Therefore, we present SynHand5M: a million-scale\nsynthetic dataset with accurate joint annotations, segmentation masks and mesh\nfiles of depth maps. Among model based learning (hybrid) methods, we show\nimproved results on our dataset and two of the public benchmarks i.e. NYU and\nICVL. Also, by employing a joint training strategy with real and synthetic\ndata, we recover 3D hand mesh and pose from real images in 3.7ms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 10:29:20 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Malik", "Jameel", ""], ["Elhayek", "Ahmed", ""], ["Nunnari", "Fabrizio", ""], ["Varanasi", "Kiran", ""], ["Tamaddon", "Kiarash", ""], ["Heloir", "Alexis", ""], ["Stricker", "Didier", ""]]}, {"id": "1808.09211", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "St\\'ephane Lathuili\\`ere, Pablo Mesejo, Xavier Alameda-Pineda and Radu\n  Horaud", "title": "DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture\n  Model", "comments": "accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of how to robustly train a ConvNet for\nregression, or deep robust regression. Traditionally, deep regression employs\nthe L2 loss function, known to be sensitive to outliers, i.e. samples that\neither lie at an abnormal distance away from the majority of the training\nsamples, or that correspond to wrongly annotated targets. This means that,\nduring back-propagation, outliers may bias the training process due to the high\nmagnitude of their gradient. In this paper, we propose DeepGUM: a deep\nregression model that is robust to outliers thanks to the use of a\nGaussian-uniform mixture model. We derive an optimization algorithm that\nalternates between the unsupervised detection of outliers using\nexpectation-maximization, and the supervised training with cleaned samples\nusing stochastic gradient descent. DeepGUM is able to adapt to a continuously\nevolving outlier distribution, avoiding to manually impose any threshold on the\nproportion of outliers in the training set. Extensive experimental evaluations\non four different tasks (facial and fashion landmark detection, age and head\npose estimation) lead us to conclude that our novel robust technique provides\nreliability in the presence of various types of noise and protection against a\nhigh percentage of outliers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 10:32:43 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Lathuili\u00e8re", "St\u00e9phane", ""], ["Mesejo", "Pablo", ""], ["Alameda-Pineda", "Xavier", ""], ["Horaud", "Radu", ""]]}, {"id": "1808.09273", "submitter": "Jorge Espinosa", "authors": "Jorge E. Espinosa, Sergio A. Velastin and John W.Branch", "title": "Motorcycle Classification in Urban Scenarios using Convolutional Neural\n  Networks for Feature Extraction", "comments": "This paper were published by IET for the ICPRS 2017 Conference,\n  Madrid Spain July 2017", "journal-ref": "ISBN: 978-1-78561-652-5 July 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a motorcycle classification system for urban scenarios\nusing Convolutional Neural Network (CNN). Significant results on image\nclassification has been achieved using CNNs at the expense of a high\ncomputational cost for training with thousands or even millions of examples.\nNevertheless, features can be extracted from CNNs already trained. In this work\nAlexNet, included in the framework CaffeNet, is used to extract features from\nframes taken on a real urban scenario. The extracted features from the CNN are\nused to train a support vector machine (SVM) classifier to discriminate\nmotorcycles from other road users. The obtained results show a mean accuracy of\n99.40% and 99.29% on a classification task of three and five classes\nrespectively. Further experiments are performed on a validation set of images\nshowing a satisfactory classification.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 13:16:17 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Espinosa", "Jorge E.", ""], ["Velastin", "Sergio A.", ""], ["Branch", "John W.", ""]]}, {"id": "1808.09284", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Riquan Chen, Lin Nie, Xiaonan Luo, Xiaobai Liu, and\n  Liang Lin", "title": "Neural Task Planning with And-Or Graph Representations", "comments": "Submitted to TMM, under minor revision. arXiv admin note: text\n  overlap with arXiv:1707.04677", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on semantic task planning, i.e., predicting a sequence of\nactions toward accomplishing a specific task under a certain scene, which is a\nnew problem in computer vision research. The primary challenges are how to\nmodel task-specific knowledge and how to integrate this knowledge into the\nlearning procedure. In this work, we propose training a recurrent long\nshort-term memory (LSTM) network to address this problem, i.e., taking a scene\nimage (including pre-located objects) and the specified task as input and\nrecurrently predicting action sequences. However, training such a network\ngenerally requires large numbers of annotated samples to cover the semantic\nspace (e.g., diverse action decomposition and ordering). To overcome this\nissue, we introduce a knowledge and-or graph (AOG) for task description, which\nhierarchically represents a task as atomic actions. With this AOG\nrepresentation, we can produce many valid samples (i.e., action sequences\naccording to common sense) by training another auxiliary LSTM network with a\nsmall set of annotated samples. Furthermore, these generated samples (i.e.,\ntask-oriented action sequences) effectively facilitate training of the model\nfor semantic task planning. In our experiments, we create a new dataset that\ncontains diverse daily tasks and extensively evaluate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 02:57:23 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Chen", "Tianshui", ""], ["Chen", "Riquan", ""], ["Nie", "Lin", ""], ["Luo", "Xiaonan", ""], ["Liu", "Xiaobai", ""], ["Lin", "Liang", ""]]}, {"id": "1808.09297", "submitter": "Sebastian Bullinger", "authors": "Sebastian Bullinger, Christoph Bodensteiner, Michael Arens and Rainer\n  Stiefelhagen", "title": "Stereo 3D Object Trajectory Reconstruction", "comments": "Under Review. arXiv admin note: text overlap with arXiv:1711.06136", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to reconstruct the three-dimensional trajectory of a\nmoving instance of a known object category using stereo video data. We track\nthe two-dimensional shape of objects on pixel level exploiting instance-aware\nsemantic segmentation techniques and optical flow cues. We apply Structure from\nMotion (SfM) techniques to object and background images to determine for each\nframe initial camera poses relative to object instances and background\nstructures. We refine the initial SfM results by integrating stereo camera\nconstraints exploiting factor graphs. We compute the object trajectory by\ncombining object and background camera pose information. In contrast to stereo\nmatching methods, our approach leverages temporal adjacent views for object\npoint triangulation. As opposed to monocular trajectory reconstruction\napproaches, our method shows no degenerated cases. We evaluate our approach\nusing publicly available video data of vehicles in urban scenes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 15:04:04 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Bullinger", "Sebastian", ""], ["Bodensteiner", "Christoph", ""], ["Arens", "Michael", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1808.09316", "submitter": "Istv\\'an S\\'ar\\'andi", "authors": "Istv\\'an S\\'ar\\'andi, Timm Linder, Kai O. Arras, Bastian Leibe", "title": "How Robust is 3D Human Pose Estimation to Occlusion?", "comments": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS'18) - Workshop on Robotic Co-workers 4.0: Human Safety and\n  Comfort in Human-Robot Interactive Social Environments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is commonplace in realistic human-robot shared environments, yet\nits effects are not considered in standard 3D human pose estimation benchmarks.\nThis leaves the question open: how robust are state-of-the-art 3D pose\nestimation methods against partial occlusions? We study several types of\nsynthetic occlusions over the Human3.6M dataset and find a method with\nstate-of-the-art benchmark performance to be sensitive even to low amounts of\nocclusion. Addressing this issue is key to progress in applications such as\ncollaborative and service robotics. We take a first step in this direction by\nimproving occlusion-robustness through training data augmentation with\nsynthetic occlusions. This also turns out to be an effective regularizer that\nis beneficial even for non-occluded test cases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:14:42 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 12:08:09 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["S\u00e1r\u00e1ndi", "Istv\u00e1n", ""], ["Linder", "Timm", ""], ["Arras", "Kai O.", ""], ["Leibe", "Bastian", ""]]}, {"id": "1808.09347", "submitter": "Chen Chao", "authors": "Chao Chen and Zhihong Chen and Boyuan Jiang and Xinyu Jin", "title": "Joint Domain Alignment and Discriminative Feature Learning for\n  Unsupervised Deep Domain Adaptation", "comments": "This paper has been accepted by AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable effort has been devoted to deep domain adaptation in\ncomputer vision and machine learning communities. However, most of existing\nwork only concentrates on learning shared feature representation by minimizing\nthe distribution discrepancy across different domains. Due to the fact that all\nthe domain alignment approaches can only reduce, but not remove the domain\nshift. Target domain samples distributed near the edge of the clusters, or far\nfrom their corresponding class centers are easily to be misclassified by the\nhyperplane learned from the source domain. To alleviate this issue, we propose\nto joint domain alignment and discriminative feature learning, which could\nbenefit both domain alignment and final classification. Specifically, an\ninstance-based discriminative feature learning method and a center-based\ndiscriminative feature learning method are proposed, both of which guarantee\nthe domain invariant features with better intra-class compactness and\ninter-class separability. Extensive experiments show that learning the\ndiscriminative features in the shared feature space can significantly boost the\nperformance of deep domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:04:32 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 07:19:39 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Chen", "Chao", ""], ["Chen", "Zhihong", ""], ["Jiang", "Boyuan", ""], ["Jin", "Xinyu", ""]]}, {"id": "1808.09351", "submitter": "Jun-Yan Zhu", "authors": "Shunyu Yao, Tzu Ming Harry Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio\n  Torralba, William T. Freeman, Joshua B. Tenenbaum", "title": "3D-Aware Scene Manipulation via Inverse Graphics", "comments": "NeurIPS 2018. Code: https://github.com/ysymyth/3D-SDN Website:\n  http://3dsdn.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to obtain an interpretable, expressive, and disentangled scene\nrepresentation that contains comprehensive structural and textural information\nfor each object. Previous scene representations learned by neural networks are\noften uninterpretable, limited to a single object, or lacking 3D knowledge. In\nthis work, we propose 3D scene de-rendering networks (3D-SDN) to address the\nabove issues by integrating disentangled representations for semantics,\ngeometry, and appearance into a deep generative model. Our scene encoder\nperforms inverse graphics, translating a scene into a structured object-wise\nrepresentation. Our decoder has two components: a differentiable shape renderer\nand a neural texture generator. The disentanglement of semantics, geometry, and\nappearance supports 3D-aware scene manipulation, e.g., rotating and moving\nobjects freely while keeping the consistent shape and texture, and changing the\nobject appearance without affecting its shape. Experiments demonstrate that our\nediting scheme based on 3D-SDN is superior to its 2D counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:16:07 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 17:58:48 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 17:34:14 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 18:57:20 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Yao", "Shunyu", ""], ["Hsu", "Tzu Ming Harry", ""], ["Zhu", "Jun-Yan", ""], ["Wu", "Jiajun", ""], ["Torralba", "Antonio", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1808.09389", "submitter": "Dongdong Chen", "authors": "Dongdong Chen, Jiancheng Lv, Mike E. Davies", "title": "Learning Discriminative Representation with Signed Laplacian Restricted\n  Boltzmann Machine", "comments": "To appear in iTWIST'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the potential of a restricted Boltzmann Machine (RBM) for\ndiscriminative representation learning. By imposing the class information\npreservation constraints on the hidden layer of the RBM, we propose a Signed\nLaplacian Restricted Boltzmann Machine (SLRBM) for supervised discriminative\nrepresentation learning. The model utilizes the label information and preserves\nthe global data locality of data points simultaneously. Experimental results on\nthe benchmark data set show the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 16:27:47 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Chen", "Dongdong", ""], ["Lv", "Jiancheng", ""], ["Davies", "Mike E.", ""]]}, {"id": "1808.09411", "submitter": "Charles Hessel", "authors": "Charles Hessel and Jean-Michel Morel", "title": "Quantitative Evaluation of Base and Detail Decomposition Filters Based\n  on their Artifacts", "comments": "12 pages; 11 figures; 2 tables; supplementary material available\n  (link given in the paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces a quantitative evaluation of filters that seek to\nseparate an image into its large-scale variations, the base layer, and its\nfine-scale variations, the detail layer. Such methods have proliferated with\nthe development of HDR imaging and the proposition of many new tone-mapping\noperators. We argue that an objective quality measurement for all methods can\nbe based on their artifacts. To this aim, the four main recurrent artifacts are\ndescribed and mathematically characterized. Among them two are classic, the\nluminance halo and the staircase effect, but we show the relevance of two more,\nthe contrast halo and the compartmentalization effect. For each of these\nartifacts we design a test-pattern and its attached measurement formula. Then\nwe fuse these measurements into a single quality mark, and obtain in that way a\nranking method valid for all filters performing a base+detail decomposition.\nThis synthetic ranking is applied to seven filters representative of the\nliterature and shown to agree with expert artifact rejection criteria.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 17:02:20 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Hessel", "Charles", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "1808.09526", "submitter": "Victor Vaquero", "authors": "Victor Vaquero, Alberto Sanfeliu, Francesc Moreno-Noguer", "title": "Deep Lidar CNN to Understand the Dynamics of Moving Vehicles", "comments": "Presented in IEEE ICRA 2018. IEEE Copyrights: Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses. (V2 just corrected comments on arxiv submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception technologies in Autonomous Driving are experiencing their golden\nage due to the advances in Deep Learning. Yet, most of these systems rely on\nthe semantically rich information of RGB images. Deep Learning solutions\napplied to the data of other sensors typically mounted on autonomous cars (e.g.\nlidars or radars) are not explored much. In this paper we propose a novel\nsolution to understand the dynamics of moving vehicles of the scene from only\nlidar information. The main challenge of this problem stems from the fact that\nwe need to disambiguate the proprio-motion of the 'observer' vehicle from that\nof the external 'observed' vehicles. For this purpose, we devise a CNN\narchitecture which at testing time is fed with pairs of consecutive lidar\nscans. However, in order to properly learn the parameters of this network,\nduring training we introduce a series of so-called pretext tasks which also\nleverage on image data. These tasks include semantic information about\nvehicleness and a novel lidar-flow feature which combines standard image-based\noptical flow with lidar scans. We obtain very promising results and show that\nincluding distilled image information only during training, allows improving\nthe inference results of the network at test time, even when image data is no\nlonger used.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 20:27:16 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 13:03:16 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Vaquero", "Victor", ""], ["Sanfeliu", "Alberto", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1808.09559", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Panagiotis Linardos, Eva Mohedano, Monica Cherto, Cathal Gurrin and\n  Xavier Giro-i-Nieto", "title": "Temporal Saliency Adaptation in Egocentric Videos", "comments": "Extended abstract at the ECCV 2018 Workshop on Egocentric Perception,\n  Interaction and Computing (EPIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work adapts a deep neural model for image saliency prediction to the\ntemporal domain of egocentric video. We compute the saliency map for each video\nframe, firstly with an off-the-shelf model trained from static images, secondly\nby adding a a convolutional or conv-LSTM layers trained with a dataset for\nvideo saliency prediction. We study each configuration on EgoMon, a new dataset\nmade of seven egocentric videos recorded by three subjects in both free-viewing\nand task-driven set ups. Our results indicate that the temporal adaptation is\nbeneficial when the viewer is not moving and observing the scene from a narrow\nfield of view. Encouraged by this observation, we compute and publish the\nsaliency maps for the EPIC Kitchens dataset, in which viewers are cooking.\nSource code and models available at\nhttps://imatge-upc.github.io/saliency-2018-videosalgan/\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 22:24:10 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 20:54:52 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Linardos", "Panagiotis", ""], ["Mohedano", "Eva", ""], ["Cherto", "Monica", ""], ["Gurrin", "Cathal", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1808.09560", "submitter": "Luan Tran", "authors": "Luan Tran, Xiaoming Liu", "title": "On Learning 3D Face Morphable Model from In-the-wild Images", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). Conference version: arXiv:1804.03786 (CVPR'18). Source code:\n  https://github.com/tranluan/Nonlinear_Face_3DMM , Project webpage:\n  http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2927975", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a classic statistical model of 3D facial shape and albedo, 3D Morphable\nModel (3DMM) is widely used in facial analysis, e.g., model fitting, image\nsynthesis. Conventional 3DMM is learned from a set of 3D face scans with\nassociated well-controlled 2D face images, and represented by two sets of PCA\nbasis functions. Due to the type and amount of training data, as well as, the\nlinear bases, the representation power of 3DMM can be limited. To address these\nproblems, this paper proposes an innovative framework to learn a nonlinear 3DMM\nmodel from a large set of in-the-wild face images, without collecting 3D face\nscans. Specifically, given a face image as input, a network encoder estimates\nthe projection, lighting, shape and albedo parameters. Two decoders serve as\nthe nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape\nand albedo, respectively. With the projection parameter, lighting, 3D shape,\nand albedo, a novel analytically-differentiable rendering layer is designed to\nreconstruct the original input face. The entire network is end-to-end trainable\nwith only weak supervision. We demonstrate the superior representation power of\nour nonlinear 3DMM over its linear counterpart, and its contribution to face\nalignment, 3D reconstruction, and face editing.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 22:25:01 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 05:01:58 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Tran", "Luan", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1808.09568", "submitter": "Yu Luo", "authors": "Yu Luo, Jianbo Ye, Reginald B. Adams, Jr., Jia Li, Michelle G. Newman,\n  James Z. Wang", "title": "ARBEE: Towards Automated Recognition of Bodily Expression of Emotion In\n  the Wild", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-019-01215-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are arguably innately prepared to comprehend others' emotional\nexpressions from subtle body movements. If robots or computers can be empowered\nwith this capability, a number of robotic applications become possible.\nAutomatically recognizing human bodily expression in unconstrained situations,\nhowever, is daunting given the incomplete understanding of the relationship\nbetween emotional expressions and body movements. The current research, as a\nmultidisciplinary effort among computer and information sciences, psychology,\nand statistics, proposes a scalable and reliable crowdsourcing approach for\ncollecting in-the-wild perceived emotion data for computers to learn to\nrecognize body languages of humans. To accomplish this task, a large and\ngrowing annotated dataset with 9,876 video clips of body movements and 13,239\nhuman characters, named BoLD (Body Language Dataset), has been created.\nComprehensive statistical analysis of the dataset revealed many interesting\ninsights. A system to model the emotional expressions based on bodily\nmovements, named ARBEE (Automated Recognition of Bodily Expression of Emotion),\nhas also been developed and evaluated. Our analysis shows the effectiveness of\nLaban Movement Analysis (LMA) features in characterizing arousal, and our\nexperiments using LMA features further demonstrate computability of bodily\nexpression. We report and compare results of several other baseline methods\nwhich were developed for action recognition based on two different modalities,\nbody skeleton, and raw image. The dataset and findings presented in this work\nwill likely serve as a launchpad for future discoveries in body language\nunderstanding that will enable future robots to interact and collaborate more\neffectively with humans.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 22:39:21 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 22:46:10 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Luo", "Yu", ""], ["Ye", "Jianbo", ""], ["Adams,", "Reginald B.", "Jr."], ["Li", "Jia", ""], ["Newman", "Michelle G.", ""], ["Wang", "James Z.", ""]]}, {"id": "1808.09574", "submitter": "Maryam Jaberi", "authors": "Maryam Jaberi, Marianna Pensky, Hassan Foroosh", "title": "Probabilistic Sparse Subspace Clustering Using Delayed Association", "comments": null, "journal-ref": "ICPR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering and clustering subspaces in high-dimensional data is a\nfundamental problem of machine learning with a wide range of applications in\ndata mining, computer vision, and pattern recognition. Earlier methods divided\nthe problem into two separate stages of finding the similarity matrix and\nfinding clusters. Similar to some recent works, we integrate these two steps\nusing a joint optimization approach. We make the following contributions: (i)\nwe estimate the reliability of the cluster assignment for each point before\nassigning a point to a subspace. We group the data points into two groups of\n\"certain\" and \"uncertain\", with the assignment of latter group delayed until\ntheir subspace association certainty improves. (ii) We demonstrate that delayed\nassociation is better suited for clustering subspaces that have ambiguities,\ni.e. when subspaces intersect or data are contaminated with outliers/noise.\n(iii) We demonstrate experimentally that such delayed probabilistic association\nleads to a more accurate self-representation and final clusters. The proposed\nmethod has higher accuracy both for points that exclusively lie in one\nsubspace, and those that are on the intersection of subspaces. (iv) We show\nthat delayed association leads to huge reduction of computational cost, since\nit allows for incremental spectral clustering.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 23:03:55 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Jaberi", "Maryam", ""], ["Pensky", "Marianna", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1808.09648", "submitter": "Avikalp Srivastava", "authors": "Avikalp Srivastava, Hsin Wen Liu, Sumio Fujita", "title": "Adapting Visual Question Answering Models for Enhancing Multimodal\n  Community Q&A Platforms", "comments": "Submitted for review at CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question categorization and expert retrieval methods have been crucial for\ninformation organization and accessibility in community question & answering\n(CQA) platforms. Research in this area, however, has dealt with only the text\nmodality. With the increasing multimodal nature of web content, we focus on\nextending these methods for CQA questions accompanied by images. Specifically,\nwe leverage the success of representation learning for text and images in the\nvisual question answering (VQA) domain, and adapt the underlying concept and\narchitecture for automated category classification and expert retrieval on\nimage-based questions posted on Yahoo! Chiebukuro, the Japanese counterpart of\nYahoo! Answers.\n  To the best of our knowledge, this is the first work to tackle the\nmultimodality challenge in CQA, and to adapt VQA models for tasks on a more\necologically valid source of visual questions. Our analysis of the differences\nbetween visual QA and community QA data drives our proposal of novel\naugmentations of an attention method tailored for CQA, and use of auxiliary\ntasks for learning better grounding features. Our final model markedly\noutperforms the text-only and VQA model baselines for both tasks of\nclassification and expert retrieval on real-world multimodal CQA data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 05:53:17 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 20:24:44 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Srivastava", "Avikalp", ""], ["Liu", "Hsin Wen", ""], ["Fujita", "Sumio", ""]]}, {"id": "1808.09679", "submitter": "Christoph Haarburger", "authors": "Christoph Haarburger, Philippe Weitz, Oliver Rippel, Dorit Merhof", "title": "Image-based Survival Analysis for Lung Cancer Patients using CNNs", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI.2019.8759499", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional survival models such as the Cox proportional hazards model are\ntypically based on scalar or categorical clinical features. With the advent of\nincreasingly large image datasets, it has become feasible to incorporate\nquantitative image features into survival prediction. So far, this kind of\nanalysis is mostly based on radiomics features, i.e. a fixed set of features\nthat is mathematically defined a priori. To capture highly abstract\ninformation, it is desirable to learn the feature extraction using\nconvolutional neural networks. However, for tomographic medical images, model\ntraining is difficult because on the one hand, only few samples of 3D image\ndata fit into one batch at once and on the other hand, survival loss functions\nare essentially ordering measures that require large batch sizes. In this work,\nwe show that by simplifying survival analysis to median survival\nclassification, convolutional neural networks can be trained with small batch\nsizes and learn features that predict survival equally well as end-to-end\nhazard prediction networks. Our approach outperforms the previous state of the\nart in a publicly available lung cancer dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 08:30:46 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 12:47:57 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Haarburger", "Christoph", ""], ["Weitz", "Philippe", ""], ["Rippel", "Oliver", ""], ["Merhof", "Dorit", ""]]}, {"id": "1808.09697", "submitter": "Uche Nnolim", "authors": "Uche A. Nnolim", "title": "Fractional Multiscale Fusion-based De-hazing", "comments": "23 pages, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the results of a proposed multi-scale fusion-based\nsingle image de-hazing algorithm, which can also be used for underwater image\nenhancement. Furthermore, the algorithm was designed for very fast operation\nand minimal run-time. The proposed scheme is the faster than existing\nalgorithms for both de-hazing and underwater image enhancement and amenable to\ndigital hardware implementation. Results indicate mostly consistent and good\nresults for both categories of images when compared with other algorithms from\nthe literature.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 09:06:05 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Nnolim", "Uche A.", ""]]}, {"id": "1808.09714", "submitter": "Luisa Verdoliva", "authors": "Davide Cozzolino and Luisa Verdoliva", "title": "Camera-based Image Forgery Localization using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera fingerprints are precious tools for a number of image forensics tasks.\nA well-known example is the photo response non-uniformity (PRNU) noise pattern,\na powerful device fingerprint. Here, to address the image forgery localization\nproblem, we rely on noiseprint, a recently proposed CNN-based camera model\nfingerprint. The CNN is trained to minimize the distance between same-model\npatches, and maximize the distance otherwise. As a result, the noiseprint\naccounts for model-related artifacts just like the PRNU accounts for\ndevice-related non-uniformities. However, unlike the PRNU, it is only mildly\naffected by residuals of high-level scene content. The experiments show that\nthe proposed noiseprint-based forgery localization method improves over the\nPRNU-based reference.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 10:20:52 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Cozzolino", "Davide", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1808.09740", "submitter": "Yao Qin", "authors": "Yao Qin, Lorenzo Bruzzone, Biao Li and Yuanxin Ye", "title": "Cross-Domain Collaborative Learning via Cluster Canonical Correlation\n  Analysis and Random Walker for Hyperspectral Image Classification", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TGRS.2018.2889195", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel heterogenous domain adaptation (HDA) method for\nhyperspectral image classification with a limited amount of labeled samples in\nboth domains. The method is achieved in the way of cross-domain collaborative\nlearning (CDCL), which is addressed via cluster canonical correlation analysis\n(C-CCA) and random walker (RW) algorithms. To be specific, the proposed CDCL\nmethod is an iterative process of three main stages, i.e. twice of RW-based\npseudolabeling and cross domain learning via C-CCA. Firstly, given the\ninitially labeled target samples as training set ($\\mathbf{TS}$), the RW-based\npseudolabeling is employed to update $\\mathbf{TS}$ and extract target clusters\n($\\mathbf{TCs}$) by fusing the segmentation results obtained by RW and extended\nRW (ERW) classifiers. Secondly, cross domain learning via C-CCA is applied\nusing labeled source samples and $\\mathbf{TCs}$. The unlabeled target samples\nare then classified with the estimated probability maps using the model trained\nin the projected correlation subspace. Thirdly, both $\\mathbf{TS}$ and\nestimated probability maps are used for updating $\\mathbf{TS}$ again via\nRW-based pseudolabeling. When the iterative process finishes, the result\nobtained by the ERW classifier using the final $\\mathbf{TS}$ and estimated\nprobability maps is regarded as the final classification map. Experimental\nresults on four real HSIs demonstrate that the proposed method can achieve\nbetter performance compared with the state-of-the-art HDA and ERW methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 11:37:23 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 08:31:28 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Qin", "Yao", ""], ["Bruzzone", "Lorenzo", ""], ["Li", "Biao", ""], ["Ye", "Yuanxin", ""]]}, {"id": "1808.09769", "submitter": "Yao Qin", "authors": "Yao Qin, Lorenzo Bruzzone and Biao Li", "title": "Tensor Alignment Based Domain Adaptation for Hyperspectral Image\n  Classification", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TGRS.2019.2926069", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tensor alignment (TA) based domain adaptation method\nfor hyperspectral image (HSI) classification. To be specific, HSIs in both\ndomains are first segmented into superpixels and tensors of both domains are\nconstructed to include neighboring samples from single superpixel. Then we\nconsider the subspace invariance between two domains as projection matrices and\noriginal tensors are projected as core tensors with lower dimensions into the\ninvariant tensor subspace by applying Tucker decomposition. To preserve\ngeometric information in original tensors, we employ a manifold regularization\nterm for core tensors into the decomposition progress. The projection matrices\nand core tensors are solved in an alternating optimization manner and the\nconvergence of TA algorithm is analyzed. In addition, a post-processing\nstrategy is defined via pure samples extraction for each superpixel to further\nimprove classification performance. Experimental results on four real HSIs\ndemonstrate that the proposed method can achieve better performance compared\nwith the state-of-the-art subspace learning methods when a limited amount of\nsource labeled samples are available.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 12:51:59 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 16:15:57 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Qin", "Yao", ""], ["Bruzzone", "Lorenzo", ""], ["Li", "Biao", ""]]}, {"id": "1808.09796", "submitter": "Bingjie Xu", "authors": "Bingjie Xu, Junnan Li, Yongkang Wong, Mohan S. Kankanhalli, and Qi\n  Zhao", "title": "Interact as You Intend: Intention-Driven Human-Object Interaction\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent advances in instance-level detection tasks lay strong foundation\nfor genuine comprehension of the visual scenes. However, the ability to fully\ncomprehend a social scene is still in its preliminary stage. In this work, we\nfocus on detecting human-object interactions (HOIs) in social scene images,\nwhich is demanding in terms of research and increasingly useful for practical\napplications. To undertake social tasks interacting with objects, humans direct\ntheir attention and move their body based on their intention. Based on this\nobservation, we provide a unique computational perspective to explore human\nintention in HOI detection. Specifically, the proposed human intention-driven\nHOI detection (iHOI) framework models human pose with the relative distances\nfrom body joints to the object instances. It also utilizes human gaze to guide\nthe attended contextual regions in a weakly-supervised setting. In addition, we\npropose a hard negative sampling strategy to address the problem of\nmis-grouping. We perform extensive experiments on two benchmark datasets,\nnamely V-COCO and HICO-DET. The efficacy of each proposed component has also\nbeen validated.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:25:50 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 11:45:38 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Xu", "Bingjie", ""], ["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan S.", ""], ["Zhao", "Qi", ""]]}, {"id": "1808.09814", "submitter": "Carles Ventura", "authors": "Carles Ventura, Jordi Pont-Tuset, Sergi Caelles, Kevis-Kokitsi\n  Maninis, Luc Van Gool", "title": "Iterative Deep Learning for Road Topology Extraction", "comments": "BMVC 2018 camera ready. Code:\n  https://github.com/carlesventura/iterative-deep-learning. arXiv admin note:\n  substantial text overlap with arXiv:1712.01217", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the task of estimating the topology of road networks from\naerial images. Building on top of a global model that performs a dense\nsemantical classification of the pixels of the image, we design a Convolutional\nNeural Network (CNN) that predicts the local connectivity among the central\npixel of an input patch and its border points. By iterating this local\nconnectivity we sweep the whole image and infer the global topology of the road\nnetwork, inspired by a human delineating a complex network with the tip of\ntheir finger. We perform an extensive and comprehensive qualitative and\nquantitative evaluation on the road network estimation task, and show that our\nmethod also generalizes well when moving to networks of retinal vessels.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:32:44 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Ventura", "Carles", ""], ["Pont-Tuset", "Jordi", ""], ["Caelles", "Sergi", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Van Gool", "Luc", ""]]}, {"id": "1808.09825", "submitter": "Mandar Gogate", "authors": "Ahsan Adeel, Mandar Gogate, Amir Hussain", "title": "Contextual Audio-Visual Switching For Speech Enhancement in Real-World\n  Environments", "comments": "16 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1808.00046", "journal-ref": "Information Fusion, 2019", "doi": "10.1016/j.inffus.2019.08.008", "report-no": "ISSN 1566-2535", "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human speech processing is inherently multimodal, where visual cues (lip\nmovements) help to better understand the speech in noise. Lip-reading driven\nspeech enhancement significantly outperforms benchmark audio-only approaches at\nlow signal-to-noise ratios (SNRs). However, at high SNRs or low levels of\nbackground noise, visual cues become fairly less effective for speech\nenhancement. Therefore, a more optimal, context-aware audio-visual (AV) system\nis required, that contextually utilises both visual and noisy audio features\nand effectively accounts for different noisy conditions. In this paper, we\nintroduce a novel contextual AV switching component that contextually exploits\nAV cues with respect to different operating conditions to estimate clean audio,\nwithout requiring any SNR estimation. The switching module switches between\nvisual-only (V-only), audio-only (A-only), and both AV cues at low, high and\nmoderate SNR levels, respectively. The contextual AV switching component is\ndeveloped by integrating a convolutional neural network and long-short-term\nmemory network. For testing, the estimated clean audio features are utilised by\nthe developed novel enhanced visually derived Wiener filter for clean audio\npower spectrum estimation. The contextual AV speech enhancement method is\nevaluated under real-world scenarios using benchmark Grid and ChiME3 corpora.\nFor objective testing, perceptual evaluation of speech quality is used to\nevaluate the quality of the restored speech. For subjective testing, the\nstandard mean-opinion-score method is used. The critical analysis and\ncomparative study demonstrate the outperformance of proposed contextual AV\napproach, over A-only, V-only, spectral subtraction, and log-minimum mean\nsquare error based speech enhancement methods at both low and high SNRs,\nrevealing its capability to tackle spectro-temporal variation in any real-world\nnoisy condition.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 17:27:11 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Adeel", "Ahsan", ""], ["Gogate", "Mandar", ""], ["Hussain", "Amir", ""]]}, {"id": "1808.09829", "submitter": "Md. Mostafa Kamal Sarker", "authors": "Md. Mostafa Kamal Sarker, Hatem A. Rashwan, Estefania Talavera, Syeda\n  Furruka Banu, Petia Radeva, Domenec Puig", "title": "MACNet: Multi-scale Atrous Convolution Networks for Food Places\n  Classification in Egocentric Photo-streams", "comments": "10 pages, accepted in ECCV at EPIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  First-person (wearable) camera continually captures unscripted interactions\nof the camera user with objects, people, and scenes reflecting his personal and\nrelational tendencies. One of the preferences of people is their interaction\nwith food events. The regulation of food intake and its duration has a great\nimportance to protect against diseases. Consequently, this work aims to develop\na smart model that is able to determine the recurrences of a person on food\nplaces during a day. This model is based on a deep end-to-end model for\nautomatic food places recognition by analyzing egocentric photo-streams. In\nthis paper, we apply multi-scale Atrous convolution networks to extract the key\nfeatures related to food places of the input images. The proposed model is\nevaluated on an in-house private dataset called \"EgoFoodPlaces\". Experimental\nresults shows promising results of food places classification recognition in\negocentric photo-streams.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 13:51:00 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Sarker", "Md. Mostafa Kamal", ""], ["Rashwan", "Hatem A.", ""], ["Talavera", "Estefania", ""], ["Banu", "Syeda Furruka", ""], ["Radeva", "Petia", ""], ["Puig", "Domenec", ""]]}, {"id": "1808.09879", "submitter": "Clara Fernandez Labrador", "authors": "Clara Fernandez-Labrador, Jose M. Facil, Alejandro Perez-Yus, Cedric\n  Demonceaux, and Jose J. Guerrero", "title": "PanoRoom: From the Sphere to the 3D Layout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel FCN able to work with omnidirectional images that outputs\naccurate probability maps representing the main structure of indoor scenes,\nwhich is able to generalize on different data. Our approach handles occlusions\nand recovers complex shaped rooms more faithful to the actual shape of the real\nscenes. We outperform the state of the art not only in accuracy of the 3D\nmodels but also in speed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:19:03 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Fernandez-Labrador", "Clara", ""], ["Facil", "Jose M.", ""], ["Perez-Yus", "Alejandro", ""], ["Demonceaux", "Cedric", ""], ["Guerrero", "Jose J.", ""]]}, {"id": "1808.09892", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Oswald Lanz", "title": "Top-down Attention Recurrent VLAD Encoding for Action Recognition in\n  Videos", "comments": "Accepted to the 17th International Conference of the Italian\n  Association for Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent approaches for action recognition from video leverage deep\narchitectures to encode the video clip into a fixed length representation\nvector that is then used for classification. For this to be successful, the\nnetwork must be capable of suppressing irrelevant scene background and extract\nthe representation from the most discriminative part of the video. Our\ncontribution builds on the observation that spatio-temporal patterns\ncharacterizing actions in videos are highly correlated with objects and their\nlocation in the video. We propose Top-down Attention Action VLAD (TA-VLAD), a\ndeep recurrent architecture with built-in spatial attention that performs\ntemporally aggregated VLAD encoding for action recognition from videos. We\nadopt a top-down approach of attention, by using class specific activation maps\nobtained from a deep CNN pre-trained for image classification, to weight\nappearance features before encoding them into a fixed-length video descriptor\nusing Gated Recurrent Units. Our method achieves state of the art recognition\naccuracy on HMDB51 and UCF101 benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:41:36 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Lanz", "Oswald", ""]]}, {"id": "1808.09916", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Autoencoders, Kernels, and Multilayer Perceptrons for Electron\n  Micrograph Restoration and Compression", "comments": "16 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present 14 autoencoders, 15 kernels and 14 multilayer perceptrons for\nelectron micrograph restoration and compression. These have been trained for\ntransmission electron microscopy (TEM), scanning transmission electron\nmicroscopy (STEM) and for both (TEM+STEM). TEM autoencoders have been trained\nfor 1$\\times$, 4$\\times$, 16$\\times$ and 64$\\times$ compression, STEM\nautoencoders for 1$\\times$, 4$\\times$ and 16$\\times$ compression and TEM+STEM\nautoencoders for 1$\\times$, 2$\\times$, 4$\\times$, 8$\\times$, 16$\\times$,\n32$\\times$ and 64$\\times$ compression. Kernels and multilayer perceptrons have\nbeen trained to approximate the denoising effect of the 4$\\times$ compression\nautoencoders. Kernels for input sizes of 3, 5, 7, 11 and 15 have been fitted\nfor TEM, STEM and TEM+STEM. TEM multilayer perceptrons have been trained with 1\nhidden layer for input sizes of 3, 5 and 7 and with 2 hidden layers for input\nsizes of 5 and 7. STEM multilayer perceptrons have been trained with 1 hidden\nlayer for input sizes of 3, 5 and 7. TEM+STEM multilayer perceptrons have been\ntrained with 1 hidden layer for input sizes of 3, 5, 7 and 11 and with 2 hidden\nlayers for input sizes of 3 and 7. Our code, example usage and pre-trained\nmodels are available at\nhttps://github.com/Jeffrey-Ede/Denoising-Kernels-MLPs-Autoencoders\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 16:33:49 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "1808.09945", "submitter": "Alexandr A. Kalinin", "authors": "Roman Solovyev, Alexander Kustov, Dmitry Telpukhov, Vladimir Rukhlov,\n  Alexandr Kalinin", "title": "Fixed-Point Convolutional Neural Network for Real-Time Video Processing\n  in FPGA", "comments": "2019 IEEE Conference of Russian Young Researchers in Electrical and\n  Electronic Engineering (EIConRus)", "journal-ref": null, "doi": "10.1109/EIConRus.2019.8656778", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mobile neural networks with a reduced number of weights and parameters\ndo a good job with image classification tasks, but even they may be too complex\nto be implemented in an FPGA for video processing tasks. The article proposes\nneural network architecture for the practical task of recognizing images from a\ncamera, which has several advantages in terms of speed. This is achieved by\nreducing the number of weights, moving from a floating-point to a fixed-point\narithmetic, and due to a number of hardware-level optimizations associated with\nstoring weights in blocks, a shift register, and an adjustable number of\nconvolutional blocks that work in parallel. The article also proposed methods\nfor adapting the existing data set for solving a different task. As the\nexperiments showed, the proposed neural network copes well with real-time video\nprocessing even on the cheap FPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 17:51:39 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 15:51:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Solovyev", "Roman", ""], ["Kustov", "Alexander", ""], ["Telpukhov", "Dmitry", ""], ["Rukhlov", "Vladimir", ""], ["Kalinin", "Alexandr", ""]]}, {"id": "1808.09964", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Semi-Metrification of the Dynamic Time Warping Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic time warping (dtw) distance fails to satisfy the triangle\ninequality and the identity of indiscernibles. As a consequence, the\ndtw-distance is not warping-invariant, which in turn results in peculiarities\nin data mining applications. This article converts the dtw-distance to a\nsemi-metric and shows that its canonical extension is warping-invariant.\nEmpirical results indicate that the nearest-neighbor classifier in the proposed\nsemi-metric space performs comparably to the same classifier in the standard\ndtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result\nsuggests to further explore the semi-metric space for data mining applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:46:35 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 06:17:56 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1808.10002", "submitter": "Tian Ye", "authors": "Tian Ye, Xiaolong Wang, James Davidson, Abhinav Gupta", "title": "Interpretable Intuitive Physics Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have a remarkable ability to use physical commonsense and predict the\neffect of collisions. But do they understand the underlying factors? Can they\npredict if the underlying factors have changed? Interestingly, in most cases\nhumans can predict the effects of similar collisions with different conditions\nsuch as changes in mass, friction, etc. It is postulated this is primarily\nbecause we learn to model physics with meaningful latent variables. This does\nnot imply we can estimate the precise values of these meaningful variables\n(estimate exact values of mass or friction). Inspired by this observation, we\npropose an interpretable intuitive physics model where specific dimensions in\nthe bottleneck layers correspond to different physical properties. In order to\ndemonstrate that our system models these underlying physical properties, we\ntrain our model on collisions of different shapes (cube, cone, cylinder,\nspheres etc.) and test on collisions of unseen combinations of shapes.\nFurthermore, we demonstrate our model generalizes well even when similar scenes\nare simulated with different underlying properties.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:22:21 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ye", "Tian", ""], ["Wang", "Xiaolong", ""], ["Davidson", "James", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1808.10009", "submitter": "Aishwarya Padmakumar", "authors": "Aishwarya Padmakumar and Peter Stone and Raymond J. Mooney", "title": "Learning a Policy for Opportunistic Active Learning", "comments": "EMNLP 2018 Camera Ready", "journal-ref": "EMNLP 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning identifies data points to label that are expected to be the\nmost useful in improving a supervised model. Opportunistic active learning\nincorporates active learning into interactive tasks that constrain possible\nqueries during interactions. Prior work has shown that opportunistic active\nlearning can be used to improve grounding of natural language descriptions in\nan interactive object retrieval task. In this work, we use reinforcement\nlearning for such an object retrieval task, to learn a policy that effectively\ntrades off task completion with model improvement that would benefit future\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:40:26 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Padmakumar", "Aishwarya", ""], ["Stone", "Peter", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1808.10032", "submitter": "Luiz Zanlorensi", "authors": "Luiz A. Zanlorensi, Eduardo Luz, Rayson Laroca, Alceu S. Britto Jr.,\n  Luiz S. Oliveira, David Menotti", "title": "The Impact of Preprocessing on Deep Representations for Iris Recognition\n  on Unconstrained Environments", "comments": "Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2018", "journal-ref": null, "doi": "10.1109/SIBGRAPI.2018.00044", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of iris as a biometric trait is widely used because of its high level\nof distinction and uniqueness. Nowadays, one of the major research challenges\nrelies on the recognition of iris images obtained in visible spectrum under\nunconstrained environments. In this scenario, the acquired iris are affected by\ncapture distance, rotation, blur, motion blur, low contrast and specular\nreflection, creating noises that disturb the iris recognition systems. Besides\ndelineating the iris region, usually preprocessing techniques such as\nnormalization and segmentation of noisy iris images are employed to minimize\nthese problems. But these techniques inevitably run into some errors. In this\ncontext, we propose the use of deep representations, more specifically,\narchitectures based on VGG and ResNet-50 networks, for dealing with the images\nusing (and not) iris segmentation and normalization. We use transfer learning\nfrom the face domain and also propose a specific data augmentation technique\nfor iris images. Our results show that the approach using non-normalized and\nonly circle-delimited iris images reaches a new state of the art in the\nofficial protocol of the NICE.II competition, a subset of the UBIRIS database,\none of the most challenging databases on unconstrained environments, reporting\nan average Equal Error Rate (EER) of 13.98% which represents an absolute\nreduction of about 5%.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 20:22:41 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Zanlorensi", "Luiz A.", ""], ["Luz", "Eduardo", ""], ["Laroca", "Rayson", ""], ["Britto", "Alceu S.", "Jr."], ["Oliveira", "Luiz S.", ""], ["Menotti", "David", ""]]}, {"id": "1808.10044", "submitter": "Mohammad Farhadi Bajestani", "authors": "Mohammmad Farhadi Bajestani, Seyed Soroush Heidari Rahmat Abadi, Seyed\n  Mostafa Derakhshandeh Fard, Roozbeh Khodadadeh", "title": "AAD: Adaptive Anomaly Detection through traffic surveillance videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection through video analysis is of great importance to detect any\nanomalous vehicle/human behavior at a traffic intersection. While most existing\nworks use neural networks and conventional machine learning methods based on\nprovided dataset, we will use object recognition (Faster R-CNN) to identify\nobjects labels and their corresponding location in the video scene as the first\nstep to implement anomaly detection. Then, the optical flow will be utilized to\nidentify adaptive traffic flows in each region of the frame. Basically, we\npropose an alternative method for unusual activity detection using an adaptive\nanomaly detection framework. Compared to the baseline method described in the\nreference paper, our method is more efficient and yields the comparable\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 21:12:57 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Bajestani", "Mohammmad Farhadi", ""], ["Abadi", "Seyed Soroush Heidari Rahmat", ""], ["Fard", "Seyed Mostafa Derakhshandeh", ""], ["Khodadadeh", "Roozbeh", ""]]}, {"id": "1808.10072", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira Bermudez", "title": "Super-Resolution for Hyperspectral and Multispectral Image Fusion\n  Accounting for Seasonal Spectral Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion combines data from different heterogeneous sources to obtain\nmore precise information about an underlying scene. Hyperspectral-multispectral\n(HS-MS) image fusion is currently attracting great interest in remote sensing\nsince it allows the generation of high spatial resolution HS images,\ncircumventing the main limitation of this imaging modality. Existing HS-MS\nfusion algorithms, however, neglect the spectral variability often existing\nbetween images acquired at different time instants. This time difference causes\nvariations in spectral signatures of the underlying constituent materials due\nto different acquisition and seasonal conditions. This paper introduces a novel\nHS-MS image fusion strategy that combines an unmixing-based formulation with an\nexplicit parametric model for typical spectral variability between the two\nimages. Simulations with synthetic and real data show that the proposed\nstrategy leads to a significant performance improvement under spectral\nvariability and state-of-the-art performance otherwise.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 00:32:37 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 16:08:26 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1808.10075", "submitter": "Lei Zhang", "authors": "Lei Zhang, Peng Wang, Lingqiao Liu, Chunhua Shen, Wei Wei, Yannning\n  Zhang, Anton Van Den Hengel", "title": "Towards Effective Deep Embedding for Zero-Shot Learning", "comments": "Working in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) can be formulated as a cross-domain matching\nproblem: after being projected into a joint embedding space, a visual sample\nwill match against all candidate class-level semantic descriptions and be\nassigned to the nearest class. In this process, the embedding space underpins\nthe success of such matching and is crucial for ZSL. In this paper, we conduct\nan in-depth study on the construction of embedding space for ZSL and posit that\nan ideal embedding space should satisfy two criteria: intra-class compactness\nand inter-class separability. While the former encourages the embeddings of\nvisual samples of one class to distribute tightly close to the semantic\ndescription embedding of this class, the latter requires embeddings from\ndifferent classes to be well separated from each other. Towards this goal, we\npresent a simple but effective two-branch network to simultaneously map\nsemantic descriptions and visual samples into a joint space, on which visual\nembeddings are forced to regress to their class-level semantic embeddings and\nthe embeddings crossing classes are required to be distinguishable by a\ntrainable classifier. Furthermore, we extend our method to a transductive\nsetting to better handle the model bias problem in ZSL (i.e., samples from\nunseen classes tend to be categorized into seen classes) with minimal extra\nsupervision. Specifically, we propose a pseudo labeling strategy to\nprogressively incorporate the testing samples into the training process and\nthus balance the model between seen and unseen classes. Experimental results on\nfive standard ZSL datasets show the superior performance of the proposed method\nand its transductive extension.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 00:51:13 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 03:03:08 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Zhang", "Lei", ""], ["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Wei", "Wei", ""], ["Zhang", "Yannning", ""], ["Hengel", "Anton Van Den", ""]]}, {"id": "1808.10083", "submitter": "He Zhang", "authors": "He Zhang, Hanlin Mo, You Hao, Qi Li, and Hua Li", "title": "Differential and integral invariants under Mobius transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging problems in the domain of 2-D image or 3-D shape\nis to handle the non-rigid deformation. From the perspective of transformation\ngroups, the conformal transformation is a key part of the diffeomorphism.\nAccording to the Liouville Theorem, an important part of the conformal\ntransformation is the Mobius transformation, so we focus on Mobius\ntransformation and propose two differential expressions that are invariable\nunder 2-D and 3-D Mobius transformation respectively. Next, we analyze the\nabsoluteness and relativity of invariance on them and their components. After\nthat, we propose integral invariants under Mobius transformation based on the\ntwo differential expressions. Finally, we propose a conjecture about the\nstructure of differential invariants under conformal transformation according\nto our observation on the composition of the above two differential invariants.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 01:45:57 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Zhang", "He", ""], ["Mo", "Hanlin", ""], ["Hao", "You", ""], ["Li", "Qi", ""], ["Li", "Hua", ""]]}, {"id": "1808.10086", "submitter": "Md Mehedi Hasan PhD", "authors": "Md Mehedi Hasan, Tasneem Rahman, Kiok Ahn, Oksam Chae", "title": "Artifacts Detection and Error Block Analysis from Broadcasted Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of IPTV and HDTV technology, previous subtle errors in\nvideos are now becoming more prominent because of the structure oriented and\ncompression based artifacts. In this paper, we focus towards the development of\na real-time video quality check system. Light weighted edge gradient magnitude\ninformation is incorporated to acquire the statistical information and the\ndistorted frames are then estimated based on the characteristics of their\nsurrounding frames. Then we apply the prominent texture patterns to classify\nthem in different block errors and analyze them not only in video error\ndetection application but also in error concealment, restoration and retrieval.\nFinally, evaluating the performance through experiments on prominent datasets\nand broadcasted videos show that the proposed algorithm is very much efficient\nto detect errors for video broadcast and surveillance applications in terms of\ncomputation time and analysis of distorted frames.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 02:15:46 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Hasan", "Md Mehedi", ""], ["Rahman", "Tasneem", ""], ["Ahn", "Kiok", ""], ["Chae", "Oksam", ""]]}, {"id": "1808.10093", "submitter": "Satoshi Ikehata Mr.", "authors": "Satoshi Ikehata", "title": "CNN-PS: CNN-based Photometric Stereo for General Non-Convex Surfaces", "comments": "Accepted in ECCV 2018 (ECCV2018). Source code and supplementary are\n  available at https://github.com/satoshi-ikehata/CNN-PS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most conventional photometric stereo algorithms inversely solve a BRDF-based\nimage formation model. However, the actual imaging process is often far more\ncomplex due to the global light transport on the non-convex surfaces. This\npaper presents a photometric stereo network that directly learns relationships\nbetween the photometric stereo input and surface normals of a scene. For\nhandling unordered, arbitrary number of input images, we merge all the input\ndata to the intermediate representation called {\\it observation map} that has a\nfixed shape, is able to be fed into a CNN. To improve both training and\nprediction, we take into account the rotational pseudo-invariance of the\nobservation map that is derived from the isotropic constraint. For training the\nnetwork, we create a synthetic photometric stereo dataset that is generated by\na physics-based renderer, therefore the global light transport is considered.\nOur experimental results on both synthetic and real datasets show that our\nmethod outperforms conventional BRDF-based photometric stereo algorithms\nespecially when scenes are highly non-convex.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 02:48:51 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ikehata", "Satoshi", ""]]}, {"id": "1808.10146", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Oliver Wasenm\\\"uller, Didier Stricker", "title": "Dense Scene Flow from Stereo Disparity and Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow describes 3D motion in a 3D scene. It can either be modeled as a\nsingle task, or it can be reconstructed from the auxiliary tasks of stereo\ndepth and optical flow estimation. While the second method can achieve\nreal-time performance by using real-time auxiliary methods, it will typically\nproduce non-dense results. In this representation of a basic combination\napproach for scene flow estimation, we will tackle the problem of non-density\nby interpolation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 07:13:36 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1808.10180", "submitter": "Hyeonwoo Yu", "authors": "H. W. Yu and B. H. Lee", "title": "A Variational Feature Encoding Method of 3D Object for Probabilistic\n  Semantic SLAM", "comments": "to appear in the proceedings of IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a feature encoding method of complex 3D objects for\nhigh-level semantic features. Recent approaches to object recognition methods\nbecome important for semantic simultaneous localization and mapping (SLAM).\nHowever, there is a lack of consideration of the probabilistic observation\nmodel for 3D objects, as the shape of a 3D object basically follows a complex\nprobability distribution. Furthermore, since the mobile robot equipped with a\nrange sensor observes only a single view, much information of the object shape\nis discarded. These limitations are the major obstacles to semantic SLAM and\nview-independent loop closure using 3D object shapes as features. In order to\nenable the numerical analysis for the Bayesian inference, we approximate the\ntrue observation model of 3D objects to tractable distributions. Since the\nobservation likelihood can be obtained from the generative model, we formulate\nthe true generative model for 3D object with the Bayesian networks. To capture\nthese complex distributions, we apply a variational auto-encoder. To analyze\nthe approximated distributions and encoded features, we perform classification\nwith maximum likelihood estimation and shape retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 08:39:04 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Yu", "H. W.", ""], ["Lee", "B. H.", ""]]}, {"id": "1808.10232", "submitter": "Oliver Wasenm\\\"uller", "authors": "Oliver Wasenm\\\"uller, Ren\\'e Schuster, Didier Stricker, Karl Leiss,\n  J\\\"urger Pfister, Oleksandra Ganus, Julian Tatsch, Artem Savkin, Nikolas\n  Brasch", "title": "Automated Scene Flow Data Generation for Training and Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow describes the 3D position as well as the 3D motion of each pixel\nin an image. Such algorithms are the basis for many state-of-the-art autonomous\nor automated driving functions. For verification and training large amounts of\nground truth data is required, which is not available for real data. In this\npaper, we demonstrate a technology to create synthetic data with dense and\nprecise scene flow ground truth.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 11:37:01 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 11:48:46 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Wasenm\u00fcller", "Oliver", ""], ["Schuster", "Ren\u00e9", ""], ["Stricker", "Didier", ""], ["Leiss", "Karl", ""], ["Pfister", "J\u00fcrger", ""], ["Ganus", "Oleksandra", ""], ["Tatsch", "Julian", ""], ["Savkin", "Artem", ""], ["Brasch", "Nikolas", ""]]}, {"id": "1808.10322", "submitter": "Tolga Birdal", "authors": "Haowen Deng, Tolga Birdal, Slobodan Ilic", "title": "PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local\n  Descriptors", "comments": "Accepted for publication at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PPF-FoldNet for unsupervised learning of 3D local descriptors on\npure point cloud geometry. Based on the folding-based auto-encoding of well\nknown point pair features, PPF-FoldNet offers many desirable properties: it\nnecessitates neither supervision, nor a sensitive local reference frame,\nbenefits from point-set sparsity, is end-to-end, fast, and can extract powerful\nrotation invariant descriptors. Thanks to a novel feature visualization, its\nevolution can be monitored to provide interpretable insights. Our extensive\nexperiments demonstrate that despite having six degree-of-freedom invariance\nand lack of training labels, our network achieves state of the art results in\nstandard benchmark datasets and outperforms its competitors when rotations and\nvarying point densities are present. PPF-FoldNet achieves $9\\%$ higher recall\non standard benchmarks, $23\\%$ higher recall when rotations are introduced into\nthe same datasets and finally, a margin of $>35\\%$ is attained when point\ndensity is significantly decreased.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 14:45:26 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Deng", "Haowen", ""], ["Birdal", "Tolga", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1808.10356", "submitter": "Matan Ben-Yosef", "authors": "Matan Ben-Yosef and Daphna Weinshall", "title": "Gaussian Mixture Generative Adversarial Networks for Diverse Datasets,\n  and the Unsupervised Clustering of Images", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been shown to produce\nrealistically looking synthetic images with remarkable success, yet their\nperformance seems less impressive when the training set is highly diverse. In\norder to provide a better fit to the target data distribution when the dataset\nincludes many different classes, we propose a variant of the basic GAN model,\ncalled Gaussian Mixture GAN (GM-GAN), where the probability distribution over\nthe latent space is a mixture of Gaussians. We also propose a supervised\nvariant which is capable of conditional sample synthesis. In order to evaluate\nthe model's performance, we propose a new scoring method which separately takes\ninto account two (typically conflicting) measures - diversity vs. quality of\nthe generated data. Through a series of empirical experiments, using both\nsynthetic and real-world datasets, we quantitatively show that GM-GANs\noutperform baselines, both when evaluated using the commonly used Inception\nScore, and when evaluated using our own alternative scoring method. In\naddition, we qualitatively demonstrate how the \\textit{unsupervised} variant of\nGM-GAN tends to map latent vectors sampled from different Gaussians in the\nlatent space to samples of different classes in the data space. We show how\nthis phenomenon can be exploited for the task of unsupervised clustering, and\nprovide quantitative evaluation showing the superiority of our method for the\nunsupervised clustering of image datasets. Finally, we demonstrate a feature\nwhich further sets our model apart from other GAN models: the option to control\nthe quality-diversity trade-off by altering, post-training, the probability\ndistribution of the latent space. This allows one to sample higher quality and\nlower diversity samples, or vice versa, according to one's needs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:32:40 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ben-Yosef", "Matan", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1808.10383", "submitter": "Weizheng Yan", "authors": "Weizheng Yan, Han Zhang, Jing Sui, Dinggang Shen", "title": "Deep Chronnectome Learning via Full Bidirectional Long Short-Term Memory\n  Networks for MCI Diagnosis", "comments": "The paper has been accepted by MICCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain functional connectivity (FC) extracted from resting-state fMRI\n(RS-fMRI) has become a popular approach for disease diagnosis, where\ndiscriminating subjects with mild cognitive impairment (MCI) from normal\ncontrols (NC) is still one of the most challenging problems. Dynamic functional\nconnectivity (dFC), consisting of time-varying spatiotemporal dynamics, may\ncharacterize \"chronnectome\" diagnostic information for improving MCI\nclassification. However, most of the current dFC studies are based on detecting\ndiscrete major brain status via spatial clustering, which ignores rich\nspatiotemporal dynamics contained in such chronnectome. We propose Deep\nChronnectome Learning for exhaustively mining the comprehensive information,\nespecially the hidden higher-level features, i.e., the dFC time series that may\nadd critical diagnostic power for MCI classification. To this end, we devise a\nnew Fully-connected Bidirectional Long Short-Term Memory Network (Full-BiLSTM)\nto effectively learn the periodic brain status changes using both past and\nfuture information for each brief time segment and then fuse them to form the\nfinal output. We have applied our method to a rigorously built large-scale\nmulti-site database (i.e., with 164 data from NCs and 330 from MCIs, which can\nbe further augmented by 25 folds). Our method outperforms other\nstate-of-the-art approaches with an accuracy of 73.6% under solid\ncross-validations. We also made extensive comparisons among multiple variants\nof LSTM models. The results suggest high feasibility of our method with\npromising value also for other brain disorder diagnoses.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 16:24:11 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Yan", "Weizheng", ""], ["Zhang", "Han", ""], ["Sui", "Jing", ""], ["Shen", "Dinggang", ""]]}, {"id": "1808.10393", "submitter": "Ashish Mehta", "authors": "Ashish Mehta, Adithya Subramanian, Anbumani Subramanian", "title": "Learning End-to-end Autonomous Driving using Guided Auxiliary\n  Supervision", "comments": "12 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Learning to drive faithfully in highly stochastic urban settings remains an\nopen problem. To that end, we propose a Multi-task Learning from Demonstration\n(MT-LfD) framework which uses supervised auxiliary task prediction to guide the\nmain task of predicting the driving commands. Our framework involves an\nend-to-end trainable network for imitating the expert demonstrator's driving\ncommands. The network intermediately predicts visual affordances and action\nprimitives through direct supervision which provide the aforementioned\nauxiliary supervised guidance. We demonstrate that such joint learning and\nsupervised guidance facilitates hierarchical task decomposition, assisting the\nagent to learn faster, achieve better driving performance and increases\ntransparency of the otherwise black-box end-to-end network. We run our\nexperiments to validate the MT-LfD framework in CARLA, an open-source urban\ndriving simulator. We introduce multiple non-player agents in CARLA and induce\ntemporal noise in them for realistic stochasticity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 16:46:22 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Mehta", "Ashish", ""], ["Subramanian", "Adithya", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "1808.10437", "submitter": "Chen Gao", "authors": "Chen Gao, Yuliang Zou, Jia-Bin Huang", "title": "iCAN: Instance-Centric Attention Network for Human-Object Interaction\n  Detection", "comments": "BMVC 2018. Project webpage: https://gaochen315.github.io/iCAN/ Code:\n  https://github.com/vt-vl-lab/iCAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed rapid progress in detecting and recognizing\nindividual object instances. To understand the situation in a scene, however,\ncomputers need to recognize how humans interact with surrounding objects. In\nthis paper, we tackle the challenging task of detecting human-object\ninteractions (HOI). Our core idea is that the appearance of a person or an\nobject instance contains informative cues on which relevant parts of an image\nto attend to for facilitating interaction prediction. To exploit these cues, we\npropose an instance-centric attention module that learns to dynamically\nhighlight regions in an image conditioned on the appearance of each instance.\nSuch an attention-based network allows us to selectively aggregate features\nrelevant for recognizing HOIs. We validate the efficacy of the proposed network\non the Verb in COCO and HICO-DET datasets and show that our approach compares\nfavorably with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 17:58:33 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Gao", "Chen", ""], ["Zou", "Yuliang", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1808.10524", "submitter": "Sharif Amit Kamran", "authors": "Sourajit Saha, Sharif Amit Kamran, Ali Shihab Sabbir", "title": "Total Recall: Understanding Traffic Signs using Deep Hierarchical\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICCITECHN.2018.8631925", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Traffic Signs using intelligent systems can drastically reduce\nthe number of accidents happening world-wide. With the arrival of Self-driving\ncars it has become a staple challenge to solve the automatic recognition of\nTraffic and Hand-held signs in the major streets. Various machine learning\ntechniques like Random Forest, SVM as well as deep learning models has been\nproposed for classifying traffic signs. Though they reach state-of-the-art\nperformance on a particular data-set, but fall short of tackling multiple\nTraffic Sign Recognition benchmarks. In this paper, we propose a novel and\none-for-all architecture that aces multiple benchmarks with better overall\nscore than the state-of-the-art architectures. Our model is made of residual\nconvolutional blocks with hierarchical dilated skip connections joined in\nsteps. With this we score 99.33% Accuracy in German sign recognition benchmark\nand 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover,\nwe propose a newly devised dilated residual learning representation technique\nwhich is very low in both memory and computational complexity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:13:02 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 12:47:06 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Saha", "Sourajit", ""], ["Kamran", "Sharif Amit", ""], ["Sabbir", "Ali Shihab", ""]]}, {"id": "1808.10542", "submitter": "Victor Vaquero", "authors": "Victor Vaquero, Alberto Sanfeliu, Francesc Moreno-Noguer", "title": "Hallucinating Dense Optical Flow from Sparse Lidar for Autonomous\n  Vehicles", "comments": "Accepted in ICPR 2018. More information: www.victorvaquero.me", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to estimate dense optical flow from\nsparse lidar data acquired on an autonomous vehicle. This is intended to be\nused as a drop-in replacement of any image-based optical flow system when\nimages are not reliable due to e.g. adverse weather conditions or at night. In\norder to infer high resolution 2D flows from discrete range data we devise a\nthree-block architecture of multiscale filters that combines multiple\nintermediate objectives, both in the lidar and image domain. To train this\nnetwork we introduce a dataset with approximately 20K lidar samples of the\nKitti dataset which we have augmented with a pseudo ground-truth image-based\noptical flow computed using FlowNet2. We demonstrate the effectiveness of our\napproach on Kitti, and show that despite using the low-resolution and sparse\nmeasurements of the lidar, we can regress dense optical flow maps which are at\npar with those estimated with image-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 22:54:44 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Vaquero", "Victor", ""], ["Sanfeliu", "Alberto", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1808.10544", "submitter": "Changqing Zou Dr.", "authors": "Changqing Zou, Haoran Mo, Ruofei Du, Xing Wu, Chengying Gao, Hongbo Fu", "title": "LUCSS: Language-based User-customized Colourization of Scene Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LUCSS, a language-based system for interactive col- orization of\nscene sketches, based on their semantic understanding. LUCSS is built upon deep\nneural networks trained via a large-scale repository of scene sketches and\ncartoon-style color images with text descriptions. It con- sists of three\nsequential modules. First, given a scene sketch, the segmenta- tion module\nautomatically partitions an input sketch into individual object instances.\nNext, the captioning module generates the text description with spatial\nrelationships based on the instance-level segmentation results. Fi- nally, the\ninteractive colorization module allows users to edit the caption and produce\ncolored images based on the altered caption. Our experiments show the\neffectiveness of our approach and the desirability of its compo- nents to\nalternative choices.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 23:01:55 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Zou", "Changqing", ""], ["Mo", "Haoran", ""], ["Du", "Ruofei", ""], ["Wu", "Xing", ""], ["Gao", "Chengying", ""], ["Fu", "Hongbo", ""]]}, {"id": "1808.10564", "submitter": "Kang Zhou", "authors": "Kang Zhou, Zaiwang Gu, Wen Liu, Weixin Luo, Jun Cheng, Shenghua Gao,\n  Jiang Liu", "title": "Multi-Cell Multi-Task Convolutional Neural Networks for Diabetic\n  Retinopathy Grading", "comments": "Accepted by EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is a non-negligible eye disease among patients with\nDiabetes Mellitus, and automatic retinal image analysis algorithm for the DR\nscreening is in high demand. Considering the resolution of retinal image is\nvery high, where small pathological tissues can be detected only with large\nresolution image and large local receptive field are required to identify those\nlate stage disease, but directly training a neural network with very deep\narchitecture and high resolution image is both time computational expensive and\ndifficult because of gradient vanishing/exploding problem, we propose a\n\\textbf{Multi-Cell} architecture which gradually increases the depth of deep\nneural network and the resolution of input image, which both boosts the\ntraining time but also improves the classification accuracy. Further,\nconsidering the different stages of DR actually progress gradually, which means\nthe labels of different stages are related. To considering the relationships of\nimages with different stages, we propose a \\textbf{Multi-Task} learning\nstrategy which predicts the label with both classification and regression.\nExperimental results on the Kaggle dataset show that our method achieves a\nKappa of 0.841 on test set which is the 4-th rank of all state-of-the-arts\nmethods. Further, our Multi-Cell Multi-Task Convolutional Neural Networks\n(M$^2$CNN) solution is a general framework, which can be readily integrated\nwith many other deep neural network architectures.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 01:21:46 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 06:10:57 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhou", "Kang", ""], ["Gu", "Zaiwang", ""], ["Liu", "Wen", ""], ["Luo", "Weixin", ""], ["Cheng", "Jun", ""], ["Gao", "Shenghua", ""], ["Liu", "Jiang", ""]]}, {"id": "1808.10584", "submitter": "Harsh Jhamtani", "authors": "Harsh Jhamtani, Taylor Berg-Kirkpatrick", "title": "Learning to Describe Differences Between Pairs of Similar Images", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the task of automatically generating text to\ndescribe the differences between two similar images. We collect a new dataset\nby crowd-sourcing difference descriptions for pairs of image frames extracted\nfrom video-surveillance footage. Annotators were asked to succinctly describe\nall the differences in a short paragraph. As a result, our novel dataset\nprovides an opportunity to explore models that align language and vision, and\ncapture visual salience. The dataset may also be a useful benchmark for\ncoherent multi-sentence generation. We perform a firstpass visual analysis that\nexposes clusters of differing pixels as a proxy for object-level differences.\nWe propose a model that captures visual salience by using a latent variable to\nalign clusters of differing pixels with output sentences. We find that, for\nboth single-sentence generation and as well as multi-sentence generation, the\nproposed model outperforms the models that use attention alone.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 03:15:28 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Jhamtani", "Harsh", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1808.10646", "submitter": "Rongzhao Zhang", "authors": "Rongzhao Zhang, Han Zhang and Albert C. S. Chung", "title": "A Unified Mammogram Analysis Method via Hybrid Deep Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic mammogram classification and mass segmentation play a critical role\nin a computer-aided mammogram screening system. In this work, we present a\nunified mammogram analysis framework for both whole-mammogram classification\nand segmentation. Our model is designed based on a deep U-Net with residual\nconnections, and equipped with the novel hybrid deep supervision (HDS) scheme\nfor end-to-end multi-task learning. As an extension of deep supervision (DS),\nHDS not only can force the model to learn more discriminative features like DS,\nbut also seamlessly integrates segmentation and classification tasks into one\nmodel, thus the model can benefit from both pixel-wise and image-wise\nsupervisions. We extensively validate the proposed method on the widely-used\nINbreast dataset. Ablation study corroborates that pixel-wise and image-wise\nsupervisions are mutually beneficial, evidencing the efficacy of HDS. The\nresults of 5-fold cross validation indicate that our unified model matches\nstate-of-the-art performance on both mammogram segmentation and classification\ntasks, which achieves an average segmentation Dice similarity coefficient (DSC)\nof 0.85 and a classification accuracy of 0.89. The code is available at\nhttps://github.com/angrypudding/hybrid-ds.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:32:52 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Zhang", "Rongzhao", ""], ["Zhang", "Han", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "1808.10654", "submitter": "Amir Zamir", "authors": "Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik,\n  Silvio Savarese", "title": "Gibson Env: Real-World Perception for Embodied Agents", "comments": "Access the code, dataset, and project website at\n  http://gibsonenv.vision/ . CVPR 2018", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing visual perception models for active agents and sensorimotor\ncontrol are cumbersome to be done in the physical world, as existing algorithms\nare too slow to efficiently learn in real-time and robots are fragile and\ncostly. This has given rise to learning-in-simulation which consequently casts\na question on whether the results transfer to real-world. In this paper, we are\nconcerned with the problem of developing real-world perception for active\nagents, propose Gibson Virtual Environment for this purpose, and showcase\nsample perceptual tasks learned therein. Gibson is based on virtualizing real\nspaces, rather than using artificially designed ones, and currently includes\nover 1400 floor spaces from 572 full buildings. The main characteristics of\nGibson are: I. being from the real-world and reflecting its semantic\ncomplexity, II. having an internal synthesis mechanism, \"Goggles\", enabling\ndeploying the trained models in real-world without needing further domain\nadaptation, III. embodiment of agents and making them subject to constraints of\nphysics and space.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:56:43 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Xia", "Fei", ""], ["Zamir", "Amir", ""], ["He", "Zhi-Yang", ""], ["Sax", "Alexander", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1808.10710", "submitter": "Mateusz Trokielewicz", "authors": "Ewelina Bartuzi and Katarzyna Roszczewska and Mateusz Trokielewicz and\n  Rados{\\l}aw Bia{\\l}obrzeski", "title": "MobiBits: Multimodal Mobile Biometric Database", "comments": "Submitted for the BIOSIG2018 conference on June 18, 2018. Accepted\n  for publication on July 20, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel database comprising representations of five\ndifferent biometric characteristics, collected in a mobile, unconstrained or\nsemi-constrained setting with three different mobile devices, including\ncharacteristics previously unavailable in existing datasets, namely hand\nimages, thermal hand images, and thermal face images, all acquired with a\nmobile, off-the-shelf device. In addition to this collection of data we perform\nan extensive set of experiments providing insight on benchmark recognition\nperformance that can be achieved with these data, carried out with existing\ncommercial and academic biometric solutions. This is the first known to us\nmobile biometric database introducing samples of biometric traits such as\nthermal hand images and thermal face images. We hope that this contribution\nwill make a valuable addition to the already existing databases and enable new\nexperiments and studies in the field of mobile authentication. The MobiBits\ndatabase is made publicly available to the research community at no cost for\nnon-commercial purposes.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 12:38:09 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Bartuzi", "Ewelina", ""], ["Roszczewska", "Katarzyna", ""], ["Trokielewicz", "Mateusz", ""], ["Bia\u0142obrzeski", "Rados\u0142aw", ""]]}, {"id": "1808.10765", "submitter": "Vahid Mirjalili Dr", "authors": "Sudipta Banerjee, Vahid Mirjalili, Arun Ross", "title": "Spoofing PRNU Patterns of Iris Sensors while Preserving Iris Recognition", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of Photo Response Non-Uniformity (PRNU) is used to link an\nimage with its source, i.e., the sensor that produced it. In this work, we\ninvestigate if it is possible to modify an iris image acquired using one sensor\nin order to spoof the PRNU noise pattern of a different sensor. In this regard,\nwe develop an image perturbation routine that iteratively modifies blocks of\npixels in the original iris image such that its PRNU pattern approaches that of\na target sensor. Experiments indicate the efficacy of the proposed perturbation\nmethod in spoofing PRNU patterns present in an iris image whilst still\nretaining its biometric content.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:21:29 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 17:42:47 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Banerjee", "Sudipta", ""], ["Mirjalili", "Vahid", ""], ["Ross", "Arun", ""]]}, {"id": "1808.10822", "submitter": "Muhammad Kamran Janjua", "authors": "Shah Nawaz, Alessandro Calefati, Muhammad Kamran Janjua, Ignazio Gallo", "title": "Seeing Colors: Learning Semantic Text Encoding for Classification", "comments": "9 pages. Under review at IJDAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question we answer with this work is: can we convert a text document into\nan image to exploit best image classification models to classify documents? To\nanswer this question we present a novel text classification method which\nconverts a text document into an encoded image, using word embedding and\ncapabilities of Convolutional Neural Networks (CNNs), successfully employed in\nimage classification. We evaluate our approach by obtaining promising results\non some well-known benchmark datasets for text classification. This work allows\nthe application of many of the advanced CNN architectures developed for\nComputer Vision to Natural Language Processing. We test the proposed approach\non a multi-modal dataset, proving that it is possible to use a single deep\nmodel to represent text and image in the same feature space.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 15:55:56 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Nawaz", "Shah", ""], ["Calefati", "Alessandro", ""], ["Janjua", "Muhammad Kamran", ""], ["Gallo", "Ignazio", ""]]}, {"id": "1808.10848", "submitter": "Steven Guan", "authors": "Steven Guan, Amir Khan, Siddhartha Sikdar, Parag V. Chitnis", "title": "Fully Dense UNet for 2D Sparse Photoacoustic Tomography Artifact Removal", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2019.2912935", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic imaging is an emerging imaging modality that is based upon the\nphotoacoustic effect. In photoacoustic tomography (PAT), the induced acoustic\npressure waves are measured by an array of detectors and used to reconstruct an\nimage of the initial pressure distribution. A common challenge faced in PAT is\nthat the measured acoustic waves can only be sparsely sampled. Reconstructing\nsparsely sampled data using standard methods results in severe artifacts that\nobscure information within the image. We propose a modified convolutional\nneural network (CNN) architecture termed Fully Dense UNet (FD-UNet) for\nremoving artifacts from 2D PAT images reconstructed from sparse data and\ncompare the proposed CNN with the standard UNet in terms of reconstructed image\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:23:44 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 14:49:28 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 14:37:03 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Guan", "Steven", ""], ["Khan", "Amir", ""], ["Sikdar", "Siddhartha", ""], ["Chitnis", "Parag V.", ""]]}, {"id": "1808.10858", "submitter": "Theerawit Wilaiprasitporn", "authors": "Worawate Ausawalaithong, Sanparith Marukatat, Arjaree Thirach and\n  Theerawit Wilaiprasitporn", "title": "Automatic Lung Cancer Prediction from Chest X-ray Images Using Deep\n  Learning Approach", "comments": null, "journal-ref": "2018 11th Biomedical Engineering International Conference\n  (BMEiCON)", "doi": "10.1109/BMEiCON.2018.8609997", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since, cancer is curable when diagnosed at an early stage, lung cancer\nscreening plays an important role in preventive care. Although both low dose\ncomputed tomography (LDCT) and computed tomography (CT) scans provide more\nmedical information than normal chest x-rays, there is very limited access to\nthese technologies in rural areas. Recently, there is a trend in using\ncomputer-aided diagnosis (CADx) to assist in screening and diagnosing of cancer\nfrom biomedical images. In this study, the 121-layer convolutional neural\nnetwork also known as DenseNet-121 by G. Huang et. al., along with the transfer\nlearning scheme was explored as a means to classify lung cancer using chest\nX-ray images. The model was trained on a lung nodules dataset before training\non the lung cancer dataset to alleviate the problem of a small dataset. The\nproposed model yields 74.43$\\pm$6.01\\% of mean accuracy, 74.96$\\pm$9.85\\% of\nmean specificity, and 74.68$\\pm$15.33\\% of mean sensitivity. The proposed model\nalso provides a heatmap for identifying the location of the lung nodule. These\nfindings are promising for further development of chest x-ray-based lung cancer\ndiagnosis using the deep learning approach. Moreover, these findings solve the\nproblem of small dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:36:59 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ausawalaithong", "Worawate", ""], ["Marukatat", "Sanparith", ""], ["Thirach", "Arjaree", ""], ["Wilaiprasitporn", "Theerawit", ""]]}, {"id": "1808.10862", "submitter": "Yuri G. Gordienko", "authors": "Nikita Gordienko, Peng Gang, Yuri Gordienko, Wei Zeng, Oleg Alienin,\n  Oleksandr Rokovyi, and Sergii Stirenko", "title": "Open Source Dataset and Machine Learning Techniques for Automatic\n  Recognition of Historical Graffiti", "comments": "11 pages, 9 figures, accepted for 25th International Conference on\n  Neural Information Processing (ICONIP 2018), 14-16 December, 2018 (Siem Reap,\n  Cambodia)", "journal-ref": "In: Cheng L., Leung A., Ozawa S. (eds) Neural Information\n  Processing. ICONIP 2018. Lecture Notes in Computer Science, vol. 11305, pp.\n  414-424. Springer, Cham", "doi": "10.1007/978-3-030-04221-9_37", "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are presented for automatic recognition of the\nhistorical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia\ncathedral in Kyiv (Ukraine). A new image dataset of these carved Glagolitic and\nCyrillic letters (CGCL) was assembled and pre-processed for recognition and\nprediction by machine learning methods. The dataset consists of more than 4000\nimages for 34 types of letters. The explanatory data analysis of CGCL and\nnotMNIST datasets shown that the carved letters can hardly be differentiated by\ndimensionality reduction methods, for example, by t-distributed stochastic\nneighbor embedding (tSNE) due to the worse letter representation by stone\ncarving in comparison to hand writing. The multinomial logistic regression\n(MLR) and a 2D convolutional neural network (CNN) models were applied. The MLR\nmodel demonstrated the area under curve (AUC) values for receiver operating\ncharacteristic (ROC) are not lower than 0.92 and 0.60 for notMNIST and CGCL,\nrespectively. The CNN model gave AUC values close to 0.99 for both notMNIST and\nCGCL (despite the much smaller size and quality of CGCL in comparison to\nnotMNIST) under condition of the high lossy data augmentation. CGCL dataset was\npublished to be available for the data science community as an open source\nresource.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:43:21 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Gordienko", "Nikita", ""], ["Gang", "Peng", ""], ["Gordienko", "Yuri", ""], ["Zeng", "Wei", ""], ["Alienin", "Oleg", ""], ["Rokovyi", "Oleksandr", ""], ["Stirenko", "Sergii", ""]]}]