[{"id": "1601.00022", "submitter": "Hongzhi Li", "authors": "Hongzhi Li, Joseph G. Ellis, Shih-Fu Chang", "title": "Event Specific Multimodal Pattern Mining with Image-Caption Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel framework and algorithms for discovering\nimage patch patterns from a large corpus of weakly supervised image-caption\npairs generated from news events. Current pattern mining techniques attempt to\nfind patterns that are representative and discriminative, we stipulate that our\ndiscovered patterns must also be recognizable by humans and preferably with\nmeaningful names. We propose a new multimodal pattern mining approach that\nleverages the descriptive captions often accompanying news images to learn\nsemantically meaningful image patch patterns. The mutltimodal patterns are then\nnamed using words mined from the associated image captions for each pattern. A\nnovel evaluation framework is provided that demonstrates our patterns are 26.2%\nmore semantically meaningful than those discovered by the state of the art\nvision only pipeline, and that we can provide tags for the discovered images\npatches with 54.5% accuracy with no direct supervision. Our methods also\ndiscover named patterns beyond those covered by the existing image datasets\nlike ImageNet. To the best of our knowledge this is the first algorithm\ndeveloped to automatically mine image patch patterns that have strong semantic\nmeaning specific to high-level news events, and then evaluate these patterns\nbased on that criteria.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:14:22 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 01:55:22 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Li", "Hongzhi", ""], ["Ellis", "Joseph G.", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1601.00025", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "comments": "(TPAMI) Transactions on Pattern Analysis and Machine Intelligence\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People typically learn through exposure to visual concepts associated with\nlinguistic descriptions. For instance, teaching visual object categories to\nchildren is often accompanied by descriptions in text or speech. In a machine\nlearning context, these observations motivates us to ask whether this learning\nprocess could be computationally modeled to learn visual classifiers. More\nspecifically, the main question of this work is how to utilize purely textual\ndescription of visual classes with no training images, to learn explicit visual\nclassifiers for them. We propose and investigate two baseline formulations,\nbased on regression and domain transfer, that predict a linear classifier.\nThen, we propose a new constrained optimization formulation that combines a\nregression function and a knowledge transfer function with additional\nconstraints to predict the parameters of a linear classifier. We also propose a\ngeneric kernelized models where a kernel classifier is predicted in the form\ndefined by the representer theorem. The kernelized models allow defining and\nutilizing any two RKHS (Reproducing Kernel Hilbert Space) kernel functions in\nthe visual space and text space, respectively. We finally propose a kernel\nfunction between unstructured text descriptions that builds on distributional\nsemantics, which shows an advantage in our setting and could be useful for\nother applications. We applied all the studied models to predict visual\nclassifiers on two fine-grained and challenging categorization datasets (CU\nBirds and Flower Datasets), and the results indicate successful predictions of\nour final model over several baselines that we designed.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:23:34 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:13:59 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1601.00027", "submitter": "Thomas Fuchs", "authors": "Thomas J. Fuchs and Joachim M. Buhmann", "title": "Computational Pathology: Challenges and Promises for Tissue Analysis", "comments": null, "journal-ref": "Computerized Medical Imaging and Graphics, vol. 35, 7-8, p.\n  515-530, 2011", "doi": "10.1016/j.compmedimag.2011.02.006", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The histological assessment of human tissue has emerged as the key challenge\nfor detection and treatment of cancer. A plethora of different data sources\nranging from tissue microarray data to gene expression, proteomics or\nmetabolomics data provide a detailed overview of the health status of a\npatient. Medical doctors need to assess these information sources and they rely\non data driven automatic analysis tools. Methods for classification, grouping\nand segmentation of heterogeneous data sources as well as regression of noisy\ndependencies and estimation of survival probabilities enter the processing\nworkflow of a pathology diagnosis system at various stages. This paper reports\non state-of-the-art of the design and effectiveness of computational pathology\nworkflows and it discusses future research directions in this emergent field of\nmedical informatics and diagnostic machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:33:44 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Fuchs", "Thomas J.", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1601.00072", "submitter": "Mishal Almazrooie Mr", "authors": "Mishal Almazrooie, Mogana Vadiveloo, and Rosni Abdullah", "title": "GPU-Based Fuzzy C-Means Clustering Algorithm for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fast and practical GPU-based implementation of Fuzzy\nC-Means(FCM) clustering algorithm for image segmentation is proposed. First, an\nextensive analysis is conducted to study the dependency among the image pixels\nin the algorithm for parallelization. The proposed GPU-based FCM has been\ntested on digital brain simulated dataset to segment white matter(WM), gray\nmatter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The execution\ntime of the sequential FCM is 519 seconds for an image dataset with the size of\n1MB. While the proposed GPU-based FCM requires only 2.33 seconds for the\nsimilar size of image dataset. An estimated 245-fold speedup is measured for\nthe data size of 40 KB on a CUDA device that has 448 processors.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 11:18:31 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 02:27:45 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 09:47:29 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Almazrooie", "Mishal", ""], ["Vadiveloo", "Mogana", ""], ["Abdullah", "Rosni", ""]]}, {"id": "1601.00088", "submitter": "Stanley Chan", "authors": "Stanley H. Chan, Todd Zickler, Yue M. Lu", "title": "Understanding Symmetric Smoothing Filters: A Gaussian Mixture Model\n  Perspective", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TIP.2017.2731208", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many patch-based image denoising algorithms can be formulated as applying a\nsmoothing filter to the noisy image. Expressed as matrices, the smoothing\nfilters must be row normalized so that each row sums to unity. Surprisingly, if\nwe apply a column normalization before the row normalization, the performance\nof the smoothing filter can often be significantly improved. Prior works showed\nthat such performance gain is related to the Sinkhorn-Knopp balancing\nalgorithm, an iterative procedure that symmetrizes a row-stochastic matrix to a\ndoubly-stochastic matrix. However, a complete understanding of the performance\ngain phenomenon is still lacking.\n  In this paper, we study the performance gain phenomenon from a statistical\nlearning perspective. We show that Sinkhorn-Knopp is equivalent to an\nExpectation-Maximization (EM) algorithm of learning a Gaussian mixture model of\nthe image patches. By establishing the correspondence between the steps of\nSinkhorn-Knopp and the EM algorithm, we provide a geometrical interpretation of\nthe symmetrization process. This observation allows us to develop a new\ndenoising algorithm called Gaussian mixture model symmetric smoothing filter\n(GSF). GSF is an extension of the Sinkhorn-Knopp and is a generalization of the\noriginal smoothing filters. Despite its simple formulation, GSF outperforms\nmany existing smoothing filters and has a similar performance compared to\nseveral state-of-the-art denoising algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 15:44:12 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 02:10:56 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Chan", "Stanley H.", ""], ["Zickler", "Todd", ""], ["Lu", "Yue M.", ""]]}, {"id": "1601.00119", "submitter": "John McKay", "authors": "John McKay, Raghu Raj, Vishal Monga, Jason Isaacs", "title": "Discriminative Sparsity for Sonar ATR", "comments": "Conference Paper for Oceans 2015 Washington DC (IEEE and MTS\n  organizers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in Sonar image capture have enabled researchers to apply\nsophisticated object identification algorithms in order to locate targets of\ninterest in images such as mines. Despite progress in this field, modern sonar\nautomatic target recognition (ATR) approaches lack robustness to the amount of\nnoise one would expect in real-world scenarios, the capability to handle\nblurring incurred from the physics of image capture, and the ability to excel\nwith relatively few training samples. We address these challenges by adapting\nmodern sparsity-based techniques with dictionaries comprising of training from\neach class. We develop new discriminative (as opposed to generative) sparse\nrepresentations which can help automatically classify targets in Sonar imaging.\nUsing a simulated SAS data set from the Naval Surface Warfare Center (NSWC), we\nobtained compelling classification rates for multi-class problems even in cases\nwith considerable noise and sparsity in training samples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 21:57:45 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["McKay", "John", ""], ["Raj", "Raghu", ""], ["Monga", "Vishal", ""], ["Isaacs", "Jason", ""]]}, {"id": "1601.00149", "submitter": "Xinglin Piao", "authors": "Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, Zhouchen Lin and\n  Baocai Yin", "title": "Tensor Sparse and Low-Rank based Submodule Clustering Method for\n  Multi-way Data", "comments": "We want to withdraw this paper because we need more mathematical\n  derivation and experiments to support our method. Therefore, we think this\n  paper is not suitable to be published in this period", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new submodule clustering method via sparse and low-rank representation for\nmulti-way data is proposed in this paper. Instead of reshaping multi-way data\ninto vectors, this method maintains their natural orders to preserve data\nintrinsic structures, e.g., image data kept as matrices. To implement\nclustering, the multi-way data, viewed as tensors, are represented by the\nproposed tensor sparse and low-rank model to obtain its submodule\nrepresentation, called a free module, which is finally used for spectral\nclustering. The proposed method extends the conventional subspace clustering\nmethod based on sparse and low-rank representation to multi-way data submodule\nclustering by combining t-product operator. The new method is tested on several\npublic datasets, including synthetical data, video sequences and toy images.\nThe experiments show that the new method outperforms the state-of-the-art\nmethods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation\n(LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation\n(RobustLatLRR) and Sparse Submodule Clustering method (SSmC).\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 07:27:48 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 08:31:16 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 02:58:35 GMT"}, {"version": "v4", "created": "Thu, 15 Sep 2016 16:45:09 GMT"}, {"version": "v5", "created": "Sat, 24 Sep 2016 14:32:34 GMT"}, {"version": "v6", "created": "Tue, 27 Sep 2016 05:02:16 GMT"}, {"version": "v7", "created": "Wed, 28 Sep 2016 08:03:53 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Piao", "Xinglin", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Lin", "Zhouchen", ""], ["Yin", "Baocai", ""]]}, {"id": "1601.00199", "submitter": "Joan Alabort-i-Medina", "authors": "Joan Alabort-i-Medina and Stefanos Zafeiriou", "title": "A Unified Framework for Compositional Fitting of Active Appearance\n  Models", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Appearance Models (AAMs) are one of the most popular and\nwell-established techniques for modeling deformable objects in computer vision.\nIn this paper, we study the problem of fitting AAMs using Compositional\nGradient Descent (CGD) algorithms. We present a unified and complete view of\nthese algorithms and classify them with respect to three main characteristics:\ni) cost function; ii) type of composition; and iii) optimization method.\nFurthermore, we extend the previous view by: a) proposing a novel Bayesian cost\nfunction that can be interpreted as a general probabilistic formulation of the\nwell-known project-out loss; b) introducing two new types of composition,\nasymmetric and bidirectional, that combine the gradients of both image and\nappearance model to derive better conver- gent and more robust CGD algorithms;\nand c) providing new valuable insights into existent CGD algorithms by\nreinterpreting them as direct applications of the Schur complement and the\nWiberg method. Finally, in order to encourage open research and facilitate\nfuture comparisons with our work, we make the implementa- tion of the\nalgorithms studied in this paper publicly available as part of the Menpo\nProject.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 17:50:13 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Alabort-i-Medina", "Joan", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1601.00210", "submitter": "Omar Al-Kadi", "authors": "O. S. Al-Kadi and D. Watson", "title": "Susceptibility of texture measures to noise: an application to lung\n  tumor CT images", "comments": "8th International Conference on BioInformatics and BioEngineering,\n  Greece, 2008", "journal-ref": null, "doi": "10.1109/BIBE.2008.4696789", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Five different texture methods are used to investigate their susceptibility\nto subtle noise occurring in lung tumor Computed Tomography (CT) images caused\nby acquisition and reconstruction deficiencies. Noise of Gaussian and Rayleigh\ndistributions with varying mean and variance was encountered in the analyzed CT\nimages. Fisher and Bhattacharyya distance measures were used to differentiate\nbetween an original extracted lung tumor region of interest (ROI) with a\nfiltered and noisy reconstructed versions. Through examining the texture\ncharacteristics of the lung tumor areas by five different texture measures, it\nwas determined that the autocovariance measure was least affected and the gray\nlevel co-occurrence matrix was the most affected by noise. Depending on the\nselected ROI size, it was concluded that the number of extracted features from\neach texture measure increases susceptibility to noise.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 19:08:41 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Al-Kadi", "O. S.", ""], ["Watson", "D.", ""]]}, {"id": "1601.00211", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "A fractal dimension based optimal wavelet packet analysis technique for\n  classification of meningioma brain tumours", "comments": "IEEE International Conference on Image Processing, Egypt, 2009", "journal-ref": null, "doi": "10.1109/ICIP.2009.5414534", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the heterogeneous nature of tissue texture, using a single resolution\napproach for optimum classification might not suffice. In contrast, a\nmultiresolution wavelet packet analysis can decompose the input signal into a\nset of frequency subbands giving the opportunity to characterise the texture at\nthe appropriate frequency channel. An adaptive best bases algorithm for optimal\nbases selection for meningioma histopathological images is proposed, via\napplying the fractal dimension (FD) as the bases selection criterion in a\ntree-structured manner. Thereby, the most significant subband that better\nidentifies texture discontinuities will only be chosen for further\ndecomposition, and its fractal signature would represent the extracted feature\nvector for classification. The best basis selection using the FD outperformed\nthe energy based selection approaches, achieving an overall classification\naccuracy of 91.25% as compared to 83.44% and 73.75% for the co-occurrence\nmatrix and energy texture signatures; respectively.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 19:14:08 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1601.00212", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "Supervised Texture Segmentation: A Comparative Study", "comments": "IEEE Jordan Conf. on Applied Electrical Engineering and Computing\n  Technologies, Jordan, 2011", "journal-ref": null, "doi": "10.1109/AEECT.2011.6132529", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to compare between four different types of feature extraction\napproaches in terms of texture segmentation. The feature extraction methods\nthat were used for segmentation are Gabor filters (GF), Gaussian Markov random\nfields (GMRF), run-length matrix (RLM) and co-occurrence matrix (GLCM). It was\nshown that the GF performed best in terms of quality of segmentation while the\nGLCM localises the texture boundaries better as compared to the other methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 19:24:24 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1601.00260", "submitter": "Gholamreza Anbarjafari", "authors": "Pejman Rasti, Hasan Demirel, Gholamreza Anbarjafari", "title": "Image Resolution Enhancement by Using Interpolation Followed by\n  Iterative Back Projection", "comments": "4 pages, Signal Processing and Communications Applications Conference\n  (SIU), 2013", "journal-ref": null, "doi": "10.1109/SIU.2013.6531593", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new super resolution technique based on the\ninterpolation followed by registering them using iterative back projection\n(IBP). Low resolution images are being interpolated and then the interpolated\nimages are being registered in order to generate a sharper high resolution\nimage. The proposed technique has been tested on Lena, Elaine, Pepper, and\nBaboon. The quantitative peak signal-to-noise ratio (PSNR) and structural\nsimilarity index (SSIM) results as well as the visual results show the\nsuperiority of the proposed technique over the conventional and state-of-art\nimage super resolution techniques. For Lena's image, the PSNR is 6.52 dB higher\nthan the bicubic interpolation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 08:50:10 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Rasti", "Pejman", ""], ["Demirel", "Hasan", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1601.00311", "submitter": "Leonid Yaroslavsky", "authors": "Leonid Yaroslavsky", "title": "How can one sample images with sampling rates close to the theoretical\n  minimum?", "comments": "14 pages, 10 figures", "journal-ref": "Journal of Optics, 2017", "doi": "10.1088/2040-8986/aa65b7", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem is addressed of minimization of the number of measurements needed\nfor digital image acquisition and reconstruction with a given accuracy. A\nsampling theory based method of image sampling and reconstruction is suggested\nthat allows to draw near the minimal rate of image sampling defined by the\nsampling theory. Presented and discussed are also results of experimental\nverification of the method and its possible applicability extensions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 16:51:14 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 08:02:31 GMT"}, {"version": "v3", "created": "Tue, 17 May 2016 15:50:34 GMT"}, {"version": "v4", "created": "Wed, 19 Apr 2017 10:04:51 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Yaroslavsky", "Leonid", ""]]}, {"id": "1601.00396", "submitter": "Udaya Wijenayake", "authors": "Udaya Wijenayake, Sung-In Choi and Soon-Yong Park", "title": "Automatic Detection and Decoding of Photogrammetric Coded Targets", "comments": "3 pages, 4 figures, Electronics, Information and Communications\n  (ICEIC), 2014 International Conference on", "journal-ref": null, "doi": "10.1109/ELINFOCOM.2014.6914413", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Close-range Photogrammetry is widely used in many industries because of the\ncost effectiveness and efficiency of the technique. In this research, we\nintroduce an automated coded target detection method which can be used to\nenhance the efficiency of the Photogrammetry.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 07:28:53 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Wijenayake", "Udaya", ""], ["Choi", "Sung-In", ""], ["Park", "Soon-Yong", ""]]}, {"id": "1601.00400", "submitter": "Abrar Abdulnabi", "authors": "Abrar H. Abdulnabi, Gang Wang, Jiwen Lu, Kui Jia", "title": "Multi-task CNN Model for Attribute Prediction", "comments": "11 pages, 3 figures, ieee transaction paper", "journal-ref": "IEEE Transactions on Multimedia, Nov 2015, pp. 1949-1959", "doi": "10.1109/TMM.2015.2477680", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a joint multi-task learning algorithm to better predict\nattributes in images using deep convolutional neural networks (CNN). We\nconsider learning binary semantic attributes through a multi-task CNN model,\nwhere each CNN will predict one binary attribute. The multi-task learning\nallows CNN models to simultaneously share visual knowledge among different\nattribute categories. Each CNN will generate attribute-specific feature\nrepresentations, and then we apply multi-task learning on the features to\npredict their attributes. In our multi-task framework, we propose a method to\ndecompose the overall model's parameters into a latent task matrix and\ncombination matrix. Furthermore, under-sampled classifiers can leverage shared\nstatistics from other classifiers to improve their performance. Natural\ngrouping of attributes is applied such that attributes in the same group are\nencouraged to share more knowledge. Meanwhile, attributes in different groups\nwill generally compete with each other, and consequently share less knowledge.\nWe show the effectiveness of our method on two popular attribute datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 07:42:56 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Abdulnabi", "Abrar H.", ""], ["Wang", "Gang", ""], ["Lu", "Jiwen", ""], ["Jia", "Kui", ""]]}, {"id": "1601.00414", "submitter": "Ming Yin", "authors": "Ming Yin and Yi Guo and Junbin Gao and Zhaoshui He and Shengli Xie", "title": "Kernel Sparse Subspace Clustering on Symmetric Positive Definite\n  Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC), as one of the most successful subspace\nclustering methods, has achieved notable clustering accuracy in computer vision\ntasks. However, SSC applies only to vector data in Euclidean space. As such,\nthere is still no satisfactory approach to solve subspace clustering by ${\\it\nself-expressive}$ principle for symmetric positive definite (SPD) matrices\nwhich is very useful in computer vision. In this paper, by embedding the SPD\nmatrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspace\nclustering method is constructed on the SPD manifold through an appropriate\nLog-Euclidean kernel, termed as kernel sparse subspace clustering on the SPD\nRiemannian manifold (KSSCR). By exploiting the intrinsic Riemannian geometry\nwithin data, KSSCR can effectively characterize the geodesic distance between\nSPD matrices to uncover the underlying subspace structure. Experimental results\non two famous database demonstrate that the proposed method achieves better\nclustering results than the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 09:00:24 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Yin", "Ming", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""], ["He", "Zhaoshui", ""], ["Xie", "Shengli", ""]]}, {"id": "1601.00599", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer, Daniel Schopfhauser", "title": "Multimodal Classification of Events in Social Media", "comments": "Preprint of accepted manuscript for the Elsevier Image and Vision\n  Computing Journal (IMAVIS). The paper will be published by IMAVIS under DOI\n  10.1016/j.imavis.2015.12.004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of social media hosted on platforms like Flickr and Instagram\nis related to social events. The task of social event classification refers to\nthe distinction of event and non-event-related content as well as the\nclassification of event types (e.g. sports events, concerts, etc.). In this\npaper, we provide an extensive study of textual, visual, as well as multimodal\nrepresentations for social event classification. We investigate strengths and\nweaknesses of the modalities and study synergy effects between the modalities.\nExperimental results obtained with our multimodal representation outperform\nstate-of-the-art methods and provide a new baseline for future research.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 18:29:33 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Schopfhauser", "Daniel", ""]]}, {"id": "1601.00706", "submitter": "Jimei Yang", "authors": "Jimei Yang, Scott Reed, Ming-Hsuan Yang, Honglak Lee", "title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D\n  View Synthesis", "comments": "This was published in NIPS 2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem for both graphics and vision is to synthesize novel\nviews of a 3D object from a single image. This is particularly challenging due\nto the partial observability inherent in projecting a 3D object onto the image\nspace, and the ill-posedness of inferring object shape and pose. However, we\ncan train a neural network to address the problem if we restrict our attention\nto specific object categories (in our case faces and chairs) for which we can\ngather ample training data. In this paper, we propose a novel recurrent\nconvolutional encoder-decoder network that is trained end-to-end on the task of\nrendering rotated objects starting from a single image. The recurrent structure\nallows our model to capture long-term dependencies along a sequence of\ntransformations. We demonstrate the quality of its predictions for human faces\non the Multi-PIE dataset and for a dataset of 3D chair models, and also show\nits ability to disentangle latent factors of variation (e.g., identity and\npose) without using full supervision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 00:08:09 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Yang", "Jimei", ""], ["Reed", "Scott", ""], ["Yang", "Ming-Hsuan", ""], ["Lee", "Honglak", ""]]}, {"id": "1601.00722", "submitter": "Guanglei Qi", "authors": "Guanglei Qi, Yanfeng Sun, Junbin Gao, Yongli Hu and Jinghua Li", "title": "Matrix Variate RBM and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is an importan- t generative model\nmodeling vectorial data. While applying an RBM in practice to images, the data\nhave to be vec- torized. This results in high-dimensional data and valu- able\nspatial information has got lost in vectorization. In this paper, a\nMatrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed by\ngeneralizing the classic RBM to explicitly model matrix data. In the new RBM\nmodel, both input and hidden variables are in matrix forms which are connected\nby bilinear transforms. The MVRBM has much less model parameters, resulting in\na faster train- ing algorithm while retaining comparable performance as the\nclassic RBM. The advantages of the MVRBM have been demonstrated on two\nreal-world applications: Image super- resolution and handwritten digit\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 02:55:17 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Qi", "Guanglei", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hu", "Yongli", ""], ["Li", "Jinghua", ""]]}, {"id": "1601.00732", "submitter": "Junbin Gao Professor", "authors": "Stephen Tierney, Junbin Gao, Yi Guo and Zhengwu Zhang", "title": "Low-Rank Representation over the Manifold of Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning it is common to interpret each data point as a vector in\nEuclidean space. However the data may actually be functional i.e.\\ each data\npoint is a function of some variable such as time and the function is\ndiscretely sampled. The naive treatment of functional data as traditional\nmultivariate data can lead to poor performance since the algorithms are\nignoring the correlation in the curvature of each function. In this paper we\npropose a method to analyse subspace structure of the functional data by using\nthe state of the art Low-Rank Representation (LRR). Experimental evaluation on\nsynthetic and real data reveals that this method massively outperforms\nconventional LRR in tasks concerning functional data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 04:21:45 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 09:50:20 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Tierney", "Stephen", ""], ["Gao", "Junbin", ""], ["Guo", "Yi", ""], ["Zhang", "Zhengwu", ""]]}, {"id": "1601.00740", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Hema S Koppula, Shane Soh, Bharad Raghavan, Avi Singh,\n  Ashutosh Saxena", "title": "Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep\n  Learning Architecture", "comments": "Journal Version (ICCV and ICRA combination with more system details)\n  http://brain4cars.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Driver Assistance Systems (ADAS) have made driving safer over the\nlast decade. They prepare vehicles for unsafe road conditions and alert drivers\nif they perform a dangerous maneuver. However, many accidents are unavoidable\nbecause by the time drivers are alerted, it is already too late. Anticipating\nmaneuvers beforehand can alert drivers before they perform the maneuver and\nalso give ADAS more time to avoid or prepare for the danger.\n  In this work we propose a vehicular sensor-rich platform and learning\nalgorithms for maneuver anticipation. For this purpose we equip a car with\ncameras, Global Positioning System (GPS), and a computing device to capture the\ndriving context from both inside and outside of the car. In order to anticipate\nmaneuvers, we propose a sensory-fusion deep learning architecture which jointly\nlearns to anticipate and fuse multiple sensory streams. Our architecture\nconsists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory\n(LSTM) units to capture long temporal dependencies. We propose a novel training\nprocedure which allows the network to predict the future given only a partial\ntemporal context. We introduce a diverse data set with 1180 miles of natural\nfreeway and city driving, and show that we can anticipate maneuvers 3.5 seconds\nbefore they occur in real-time with a precision and recall of 90.5\\% and 87.4\\%\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 05:25:14 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Jain", "Ashesh", ""], ["Koppula", "Hema S", ""], ["Soh", "Shane", ""], ["Raghavan", "Bharad", ""], ["Singh", "Avi", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1601.00781", "submitter": "Grzegorz Kurzejamski", "authors": "Grzegorz Kurzejamski, Jacek Zawistowski and Grzegorz Sarwas", "title": "Robust Method of Vote Aggregation and Proposition Verification for\n  Invariant Local Features", "comments": "8 pages Short Paper, presented at VISAPP 2015 Conference in Berlin,\n  March. Proceedings of the 10th International Conference on Computer Vision\n  Theory and Applications, 252-259, 2015, Berlin, Germany, ISBN\n  978-989-758-090-1", "journal-ref": null, "doi": "10.5220/0005267002520259", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for analysis of the vote space created from the\nlocal features extraction process in a multi-detection system. The method is\nopposed to the classic clustering approach and gives a high level of control\nover the clusters composition for further verification steps. Proposed method\ncomprises of the graphical vote space presentation, the proposition generation,\nthe two-pass iterative vote aggregation and the cascade filters for\nverification of the propositions. Cascade filters contain all of the minor\nalgorithms needed for effective object detection verification. The new approach\ndoes not have the drawbacks of the classic clustering approaches and gives a\nsubstantial control over process of detection. Method exhibits an exceptionally\nhigh detection rate in conjunction with a low false detection chance in\ncomparison to alternative methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 10:18:13 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Kurzejamski", "Grzegorz", ""], ["Zawistowski", "Jacek", ""], ["Sarwas", "Grzegorz", ""]]}, {"id": "1601.00825", "submitter": "Concetto Spampinato Dr", "authors": "Simone Palazzo, Concetto Spampinato and Daniela Giordano", "title": "Gamifying Video Object Segmentation", "comments": "Submitted to PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation can be considered as one of the most challenging\ncomputer vision problems. Indeed, so far, no existing solution is able to\neffectively deal with the peculiarities of real-world videos, especially in\ncases of articulated motion and object occlusions; limitations that appear more\nevident when we compare their performance with the human one. However, manually\nsegmenting objects in videos is largely impractical as it requires a lot of\nhuman time and concentration. To address this problem, in this paper we propose\nan interactive video object segmentation method, which exploits, on one hand,\nthe capability of humans to identify correctly objects in visual scenes, and on\nthe other hand, the collective human brainpower to solve challenging tasks. In\nparticular, our method relies on a web game to collect human inputs on object\nlocations, followed by an accurate segmentation phase achieved by optimizing an\nenergy function encoding spatial and temporal constraints between object\nregions as well as human-provided input. Performance analysis carried out on\nchallenging video datasets with some users playing the game demonstrated that\nour method shows a better trade-off between annotation times and segmentation\naccuracy than interactive video annotation and automated video object\nsegmentation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 13:48:05 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Palazzo", "Simone", ""], ["Spampinato", "Concetto", ""], ["Giordano", "Daniela", ""]]}, {"id": "1601.00978", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Henry Z. Lo and Tingting Lu and Wei Ding", "title": "Crater Detection via Convolutional Neural Networks", "comments": "2 Pages. Submitted to 47th Lunar and Planetary Science Conference\n  (LPSC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Craters are among the most studied geomorphic features in the Solar System\nbecause they yield important information about the past and present geological\nprocesses and provide information about the relative ages of observed geologic\nformations. We present a method for automatic crater detection using advanced\nmachine learning to deal with the large amount of satellite imagery collected.\nThe challenge of automatically detecting craters comes from their is complex\nsurface because their shape erodes over time to blend into the surface.\nBandeira provided a seminal dataset that embodied this challenge that is still\nan unsolved pattern recognition problem to this day. There has been work to\nsolve this challenge based on extracting shape and contrast features and then\napplying classification models on those features. The limiting factor in this\nexisting work is the use of hand crafted filters on the image such as Gabor or\nSobel filters or Haar features. These hand crafted methods rely on domain\nknowledge to construct. We would like to learn the optimal filters and features\nbased on training examples. In order to dynamically learn filters and features\nwe look to Convolutional Neural Networks (CNNs) which have shown their\ndominance in computer vision. The power of CNNs is that they can learn image\nfilters which generate features for high accuracy classification.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 21:03:59 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Lo", "Henry Z.", ""], ["Lu", "Tingting", ""], ["Ding", "Wei", ""]]}, {"id": "1601.00998", "submitter": "Alexandre Robicquet Alexandre Robicquet", "authors": "Alexandre Robicquet, Alexandre Alahi, Amir Sadeghian, Bryan Anenberg,\n  John Doherty, Eli Wu, and Silvio Savarese", "title": "Forecasting Social Navigation in Crowded Complex Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans navigate a crowed space such as a university campus or the\nsidewalks of a busy street, they follow common sense rules based on social\netiquette. In this paper, we argue that in order to enable the design of new\nalgorithms that can take fully advantage of these rules to better solve tasks\nsuch as target tracking or trajectory forecasting, we need to have access to\nbetter data in the first place. To that end, we contribute the very first large\nscale dataset (to the best of our knowledge) that collects images and videos of\nvarious types of targets (not just pedestrians, but also bikers, skateboarders,\ncars, buses, golf carts) that navigate in a real-world outdoor environment such\nas a university campus. We present an extensive evaluation where different\nmethods for trajectory forecasting are evaluated and compared. Moreover, we\npresent a new algorithm for trajectory prediction that exploits the complexity\nof our new dataset and allows to: i) incorporate inter-class interactions into\ntrajectory prediction models (e.g, pedestrian vs bike) as opposed to just\nintra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree\nto which the social forces are regulating an interaction. We call the latter\n\"social sensitivity\"and it captures the sensitivity to which a target is\nresponding to a certain interaction. An extensive experimental evaluation\ndemonstrates the effectiveness of our novel approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 22:10:15 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Robicquet", "Alexandre", ""], ["Alahi", "Alexandre", ""], ["Sadeghian", "Amir", ""], ["Anenberg", "Bryan", ""], ["Doherty", "John", ""], ["Wu", "Eli", ""], ["Savarese", "Silvio", ""]]}, {"id": "1601.01006", "submitter": "Fei Han", "authors": "Fei Han, Brian Reily, William Hoff, Hao Zhang", "title": "Space-Time Representation of People Based on 3D Skeletal Data: A Review", "comments": "Our paper has been accepted by the journal Computer Vision and Image\n  Understanding, see\n  http://www.sciencedirect.com/science/article/pii/S1077314217300279, Computer\n  Vision and Image Understanding, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal human representation based on 3D visual perception data is a\nrapidly growing research area. Based on the information sources, these\nrepresentations can be broadly categorized into two groups based on RGB-D\ninformation or 3D skeleton data. Recently, skeleton-based human representations\nhave been intensively studied and kept attracting an increasing attention, due\nto their robustness to variations of viewpoint, human body scale and motion\nspeed as well as the realtime, online performance. This paper presents a\ncomprehensive survey of existing space-time representations of people based on\n3D skeletal data, and provides an informative categorization and analysis of\nthese methods from the perspectives, including information modality,\nrepresentation encoding, structure and transition, and feature engineering. We\nalso provide a brief overview of skeleton acquisition devices and construction\nmethods, enlist a number of public benchmark datasets with skeleton data, and\ndiscuss potential future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 22:38:36 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 06:00:39 GMT"}, {"version": "v3", "created": "Sat, 4 Feb 2017 01:08:55 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Han", "Fei", ""], ["Reily", "Brian", ""], ["Hoff", "William", ""], ["Zhang", "Hao", ""]]}, {"id": "1601.01060", "submitter": "Deyu Meng", "authors": "Xiangyong Cao, Qian Zhao, Deyu Meng, Yang Chen, Zongben Xu", "title": "Low-rank Matrix Factorization under General Mixture Noise Distributions", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2593343", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision problems can be posed as learning a low-dimensional\nsubspace from high dimensional data. The low rank matrix factorization (LRMF)\nrepresents a commonly utilized subspace learning strategy. Most of the current\nLRMF techniques are constructed on the optimization problems using L1-norm and\nL2-norm losses, which mainly deal with Laplacian and Gaussian noises,\nrespectively. To make LRMF capable of adapting more complex noise, this paper\nproposes a new LRMF model by assuming noise as Mixture of Exponential Power\n(MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combining\nthe penalized likelihood method with MoEP distributions. Such setting\nfacilitates the learned LRMF model capable of automatically fitting the real\nnoise through MoEP distributions. Each component in this mixture is adapted\nfrom a series of preliminary super- or sub-Gaussian candidates. Moreover, by\nfacilitating the local continuity of noise components, we embed Markov random\nfield into the PMoEP model and further propose the advanced PMoEP-MRF model. An\nExpectation Maximization (EM) algorithm and a variational EM (VEM) algorithm\nare also designed to infer the parameters involved in the proposed PMoEP and\nthe PMoEP-MRF model, respectively. The superseniority of our methods is\ndemonstrated by extensive experiments on synthetic data, face modeling,\nhyperspectral image restoration and background subtraction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 02:52:30 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Cao", "Xiangyong", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Chen", "Yang", ""], ["Xu", "Zongben", ""]]}, {"id": "1601.01100", "submitter": "Guo Qiang", "authors": "Guo Qiang, Tu Dan, Li Guohui, Lei Jun", "title": "Memory Matters: Convolutional Recurrent Neural Network for Scene Text\n  Recognition", "comments": "6 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text recognition in natural scene is a challenging problem due to the many\nfactors affecting text appearance. In this paper, we presents a method that\ndirectly transcribes scene text images to text without needing of sophisticated\ncharacter segmentation. We leverage recent advances of deep neural networks to\nmodel the appearance of scene text images with temporal dynamics. Specifically,\nwe integrates convolutional neural network (CNN) and recurrent neural network\n(RNN) which is motivated by observing the complementary modeling capabilities\nof the two models. The main contribution of this work is investigating how\ntemporal memory helps in an segmentation free fashion for this specific\nproblem. By using long short-term memory (LSTM) blocks as hidden units, our\nmodel can retain long-term memory compared with HMMs which only maintain\nshort-term state dependences. We conduct experiments on Street View House\nNumber dataset containing highly variable number images. The results\ndemonstrate the superiority of the proposed method over traditional HMM based\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 07:36:15 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Qiang", "Guo", ""], ["Dan", "Tu", ""], ["Guohui", "Li", ""], ["Jun", "Lei", ""]]}, {"id": "1601.01145", "submitter": "Yiren Zhou", "authors": "Yiren Zhou, Hossein Nejati, Thanh-Toan Do, Ngai-Man Cheung, Lynette\n  Cheah", "title": "Image-based Vehicle Analysis using Deep Neural Network: A Systematic\n  Study", "comments": "5 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the vehicle detection and classification problems using Deep\nNeural Networks (DNNs) approaches. Here we answer to questions that are\nspecific to our application including how to utilize DNN for vehicle detection,\nwhat features are useful for vehicle classification, and how to extend a model\ntrained on a limited size dataset, to the cases of extreme lighting condition.\nAnswering these questions we propose our approach that outperforms\nstate-of-the-art methods, and achieves promising results on image with extreme\nlighting conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 11:25:36 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 09:21:08 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Zhou", "Yiren", ""], ["Nejati", "Hossein", ""], ["Do", "Thanh-Toan", ""], ["Cheung", "Ngai-Man", ""], ["Cheah", "Lynette", ""]]}, {"id": "1601.01216", "submitter": "Ramin  Norousi", "authors": "Ramin Norousi, Volker J. Schmid", "title": "Automatic 3D object detection of Proteins in Fluorescent labeled\n  microscope images with spatial statistical analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since manual object detection is very inaccurate and time consuming, some\nautomatic object detection tools have been developed in recent years. At the\nmoment, there is no image analysis software available which provides an\nautomatic, objective assessment of 3D foci which is generally applicable.\nComplications arise from discrete foci which are very close or even come in\ncontact to other foci, moreover they are of variable sizes and show variable\nsignal-to-noise, and must be analyzed fully in 3D. Therefore we introduce the\n3D-OSCOS (3D-Object Segmentation and Colocalization Analysis based on Spatial\nstatistics) algorithm which is implemented as a user-friendly toolbox for\ninteractive detection of 3D objects and visualization of labeled images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 15:49:49 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Norousi", "Ramin", ""], ["Schmid", "Volker J.", ""]]}, {"id": "1601.01232", "submitter": "Edmond Boyer", "authors": "Benjamin Allain, Li Wang, Jean-Sebastien Franco, Franck Hetroy, and\n  Edmond Boyer", "title": "Shape Animation with Combined Captured and Simulated Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel volumetric animation generation framework to create new\ntypes of animations from raw 3D surface or point cloud sequence of captured\nreal performances. The framework considers as input time incoherent 3D\nobservations of a moving shape, and is thus particularly suitable for the\noutput of performance capture platforms. In our system, a suitable virtual\nrepresentation of the actor is built from real captures that allows seamless\ncombination and simulation with virtual external forces and objects, in which\nthe original captured actor can be reshaped, disassembled or reassembled from\nuser-specified virtual physics. Instead of using the dominant surface-based\ngeometric representation of the capture, which is less suitable for volumetric\neffects, our pipeline exploits Centroidal Voronoi tessellation decompositions\nas unified volumetric representation of the real captured actor, which we show\ncan be used seamlessly as a building block for all processing stages, from\ncapture and tracking to virtual physic simulation. The representation makes no\nhuman specific assumption and can be used to capture and re-simulate the actor\nwith props or other moving scenery elements. We demonstrate the potential of\nthis pipeline for virtual reanimation of a real captured event with various\nunprecedented volumetric visual effects, such as volumetric distortion,\nerosion, morphing, gravity pull, or collisions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 16:30:27 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Allain", "Benjamin", ""], ["Wang", "Li", ""], ["Franco", "Jean-Sebastien", ""], ["Hetroy", "Franck", ""], ["Boyer", "Edmond", ""]]}, {"id": "1601.01339", "submitter": "Xiao Shu", "authors": "Xiao Shu and Xiaolin Wu", "title": "Quality Adaptive Low-Rank Based JPEG Decoding with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small compression noises, despite being transparent to human eyes, can\nadversely affect the results of many image restoration processes, if left\nunaccounted for. Especially, compression noises are highly detrimental to\ninverse operators of high-boosting (sharpening) nature, such as deblurring and\nsuperresolution against a convolution kernel. By incorporating the non-linear\nDCT quantization mechanism into the formulation for image restoration, we\npropose a new sparsity-based convex programming approach for joint compression\nnoise removal and image restoration. Experimental results demonstrate\nsignificant performance gains of the new approach over existing image\nrestoration methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 22:06:58 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Shu", "Xiao", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1601.01422", "submitter": "Tsuyoshi Kato", "authors": "Tomoki Matsuzawa, Raissa Relator, Jun Sese, Tsuyoshi Kato", "title": "Stochastic Dykstra Algorithms for Metric Learning on Positive\n  Semi-Definite Cone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, covariance descriptors have received much attention as powerful\nrepresentations of set of points. In this research, we present a new metric\nlearning algorithm for covariance descriptors based on the Dykstra algorithm,\nin which the current solution is projected onto a half-space at each iteration,\nand runs at O(n^3) time. We empirically demonstrate that randomizing the order\nof half-spaces in our Dykstra-based algorithm significantly accelerates the\nconvergence to the optimal solution. Furthermore, we show that our approach\nyields promising experimental results on pattern recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 07:29:45 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Matsuzawa", "Tomoki", ""], ["Relator", "Raissa", ""], ["Sese", "Jun", ""], ["Kato", "Tsuyoshi", ""]]}, {"id": "1601.01431", "submitter": "Fujiao Ju", "authors": "Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu", "title": "Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal\n  Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic principal component analysis (PPCA) is built upon a global\nlinear mapping, with which it is insufficient to model complex data variation.\nThis paper proposes a mixture of bilateral-projection probabilistic principal\ncomponent analysis model (mixB2DPPCA) on 2D data. With multi-components in the\nmixture, this model can be seen as a soft cluster algorithm and has capability\nof modeling data with complex structures. A Bayesian inference scheme has been\nproposed based on the variational EM (Expectation-Maximization) approach for\nlearning model parameters. Experiments on some publicly available databases\nshow that the performance of mixB2DPPCA has been largely improved, resulting in\nmore accurate reconstruction errors and recognition rates than the existing\nPCA-based algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 07:48:41 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Ju", "Fujiao", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Liu", "Simeng", ""], ["Hu", "Yongli", ""]]}, {"id": "1601.01432", "submitter": "Xinglin Piao", "authors": "Xinglin Piao, Yongli Hu, Yanfeng Sun, Junbin Gao, Baocai Yin", "title": "Block-Diagonal Sparse Representation by Learning a Linear Combination\n  Dictionary for Recognition", "comments": "We want to withdraw this paper because we need more mathematical\n  derivation and experiments to support our method. Therefore, we think this\n  paper is not suitable to be published in this period", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sparse representation based recognition scheme, it is critical to learn\na desired dictionary, aiming both good representational power and\ndiscriminative performance. In this paper, we propose a new dictionary learning\nmodel for recognition applications, in which three strategies are adopted to\nachieve these two objectives simultaneously. First, a block-diagonal constraint\nis introduced into the model to eliminate the correlation between classes and\nenhance the discriminative performance. Second, a low-rank term is adopted to\nmodel the coherence within classes for refining the sparse representation of\neach class. Finally, instead of using the conventional over-complete\ndictionary, a specific dictionary constructed from the linear combination of\nthe training samples is proposed to enhance the representational power of the\ndictionary and to improve the robustness of the sparse representation model.\nThe proposed method is tested on several public datasets. The experimental\nresults show the method outperforms most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 08:01:56 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 00:31:37 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Piao", "Xinglin", ""], ["Hu", "Yongli", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Yin", "Baocai", ""]]}, {"id": "1601.01467", "submitter": "Evgeniy Martyushev", "authors": "Evgeniy Martyushev", "title": "On Some Properties of Calibrated Trifocal Tensors", "comments": "18 pages, 1 figure", "journal-ref": "Journal of Mathematical Imaging and Vision, Volume 58, Issue 2, pp\n  321-332, 2017", "doi": "10.1007/s10851-017-0712-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In two-view geometry, the essential matrix describes the relative position\nand orientation of two calibrated images. In three views, a similar role is\nassigned to the calibrated trifocal tensor. It is a particular case of the\n(uncalibrated) trifocal tensor and thus it inherits all its properties but, due\nto the smaller degrees of freedom, satisfies a number of additional algebraic\nconstraints. Some of them are described in this paper. More specifically, we\ndefine a new notion --- the trifocal essential matrix. On the one hand, it is a\ngeneralization of the ordinary (bifocal) essential matrix, and, on the other\nhand, it is closely related to the calibrated trifocal tensor. We prove the two\nnecessary and sufficient conditions that characterize the set of trifocal\nessential matrices. Based on these characterizations, we propose three\nnecessary conditions on a calibrated trifocal tensor. They have a form of 15\nquartic and 99 quintic polynomial equations. We show that in the practically\nsignificant real case the 15 quartic constraints are also sufficient.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 10:24:03 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 11:43:44 GMT"}, {"version": "v3", "created": "Sun, 15 May 2016 10:08:55 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Martyushev", "Evgeniy", ""]]}, {"id": "1601.01705", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein", "title": "Learning to Compose Neural Networks for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 21:21:59 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 18:20:37 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 01:44:25 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 23:25:51 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Klein", "Dan", ""]]}, {"id": "1601.01750", "submitter": "Kilho Son", "authors": "Kilho Son, Ming-Yu Liu, Yuichi Taguchi", "title": "Learning to Remove Multipath Distortions in Time-of-Flight Range Images\n  for a Robotic Arm Setup", "comments": "8 pages, 11 figures, will be presented to ICRA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range images captured by Time-of-Flight (ToF) cameras are corrupted with\nmultipath distortions due to interaction between modulated light signals and\nscenes. The interaction is often complicated, which makes a model-based\nsolution elusive. We propose a learning-based approach for removing the\nmultipath distortions for a ToF camera in a robotic arm setup. Our approach is\nbased on deep learning. We use the robotic arm to automatically collect a large\namount of ToF range images containing various multipath distortions. The\ntraining images are automatically labeled by leveraging a high precision\nstructured light sensor available only in the training time. In the test time,\nwe apply the learned model to remove the multipath distortions. This allows our\nrobotic arm setup to enjoy the speed and compact form of the ToF camera without\ncompromising with its range measurement errors. We conduct extensive\nexperimental validations and compare the proposed method to several baseline\nalgorithms. The experiment results show that our method achieves 55% error\nreduction in range estimation and largely outperforms the baseline algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 02:25:10 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 21:56:48 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 18:47:17 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Son", "Kilho", ""], ["Liu", "Ming-Yu", ""], ["Taguchi", "Yuichi", ""]]}, {"id": "1601.01876", "submitter": "Salah Eddine Bekhouche SE. Bekhouche", "authors": "Salah Eddine Bekhouche (1), Abdelkrim Ouafi (1), Abdelmalik\n  Taleb-Ahmed (2), Abdenour Hadid (3), Azeddine Benlamoudi (1) ((1) Laboratory\n  of LESIA, University of Biskra, Algeria, (2) LAMIH, University of\n  Valenciennes, France, (3) Center for Machine Vision Research, University of\n  Oulu, Finland)", "title": "Facial age estimation using BSIF and LBP", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": "10.13140/RG.2.1.1933.6483/1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face aging is irreversible process causing changes in human face\ncharacteristics such us hair whitening, muscles drop and wrinkles. Due to the\nimportance of human face aging in biometrics systems, age estimation became an\nattractive area for researchers. This paper presents a novel method to estimate\nthe age from face images, using binarized statistical image features (BSIF) and\nlocal binary patterns (LBP)histograms as features performed by support vector\nregression (SVR) and kernel ridge regression (KRR). We applied our method on\nFG-NET and PAL datasets. Our proposed method has shown superiority to that of\nthe state-of-the-art methods when using the whole PAL database.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:03:21 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Bekhouche", "Salah Eddine", ""], ["Ouafi", "Abdelkrim", ""], ["Taleb-Ahmed", "Abdelmalik", ""], ["Hadid", "Abdenour", ""], ["Benlamoudi", "Azeddine", ""]]}, {"id": "1601.01885", "submitter": "Anguelos Nicolaou", "authors": "Anguelos Nicolaou, Andrew Bagdanov, Lluis Gomez-Bigorda, Dimosthenis\n  Karatzas", "title": "Visual Script and Language Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a script identification method based on\nhand-crafted texture features and an artificial neural network. The proposed\npipeline achieves near state-of-the-art performance for script identification\nof video-text and state-of-the-art performance on visual language\nidentification of handwritten text. More than using the deep network as a\nclassifier, the use of its intermediary activations as a learned metric\ndemonstrates remarkable results and allows the use of discriminative models on\nunknown classes. Comparative experiments in video-text and text in the wild\ndatasets provide insights on the internals of the proposed deep network.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:25:20 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Nicolaou", "Anguelos", ""], ["Bagdanov", "Andrew", ""], ["Gomez-Bigorda", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1601.02088", "submitter": "Joerg Kappes", "authors": "J\\\"org Hendrik Kappes and Paul Swoboda and Bogdan Savchynskyy and\n  Tamir Hazan and Christoph Schn\\\"orr", "title": "Multicuts and Perturb & MAP for Probabilistic Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic graphical model formulation for the graph\nclustering problem. This enables to locally represent uncertainty of image\npartitions by approximate marginal distributions in a mathematically\nsubstantiated way, and to rectify local data term cues so as to close contours\nand to obtain valid partitions.\n  We exploit recent progress on globally optimal MAP inference by integer\nprogramming and on perturbation-based approximations of the log-partition\nfunction, in order to sample clusterings and to estimate marginal distributions\nof node-pairs both more accurately and more efficiently than state-of-the-art\nmethods. Our approach works for any graphically represented problem instance.\nThis is demonstrated for image segmentation and social network cluster\nanalysis. Our mathematical ansatz should be relevant also for other\ncombinatorial problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 09:29:31 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kappes", "J\u00f6rg Hendrik", ""], ["Swoboda", "Paul", ""], ["Savchynskyy", "Bogdan", ""], ["Hazan", "Tamir", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1601.02093", "submitter": "Olivier Mor\\`ere", "authors": "Olivier Mor\\`ere, Antoine Veillard, Jie Lin, Julie Petta, Vijay\n  Chandrasekhar, Tomaso Poggio", "title": "Group Invariant Deep Representations for Image Instance Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image instance retrieval pipelines are based on comparison of vectors\nknown as global image descriptors between a query image and the database\nimages. Due to their success in large scale image classification,\nrepresentations extracted from Convolutional Neural Networks (CNN) are quickly\ngaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors\nfor image instance retrieval. While CNN-based descriptors are generally\nremarked for good retrieval performance at lower bitrates, they nevertheless\npresent a number of drawbacks including the lack of robustness to common object\ntransformations such as rotations compared with their interest point based FV\ncounterparts.\n  In this paper, we propose a method for computing invariant global descriptors\nfrom CNNs. Our method implements a recently proposed mathematical theory for\ninvariance in a sensory cortex modeled as a feedforward neural network. The\nresulting global descriptors can be made invariant to multiple arbitrary\ntransformation groups while retaining good discriminativeness.\n  Based on a thorough empirical evaluation using several publicly available\ndatasets, we show that our method is able to significantly and consistently\nimprove retrieval results every time a new type of invariance is incorporated.\nWe also show that our method which has few parameters is not prone to\noverfitting: improvements generalize well across datasets with different\nproperties with regard to invariances. Finally, we show that our descriptors\nare able to compare favourably to other state-of-the-art compact descriptors in\nsimilar bitranges, exceeding the highest retrieval results reported in the\nliterature on some datasets. A dedicated dimensionality reduction step\n--quantization or hashing-- may be able to further improve the competitiveness\nof the descriptors.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 10:42:35 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 06:43:44 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Mor\u00e8re", "Olivier", ""], ["Veillard", "Antoine", ""], ["Lin", "Jie", ""], ["Petta", "Julie", ""], ["Chandrasekhar", "Vijay", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1601.02098", "submitter": "Qingjun Wang", "authors": "Qingjun Wang and Haiyan Lv and Jun Yue and Eugene Mitchell", "title": "Supervised multiview learning based on simultaneous learning of\n  multiview intact and single view classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview learning problem refers to the problem of learning a classifier\nfrom multiple view data. In this data set, each data points is presented by\nmultiple different views. In this paper, we propose a novel method for this\nproblem. This method is based on two assumptions. The first assumption is that\neach data point has an intact feature vector, and each view is obtained by a\nlinear transformation from the intact vector. The second assumption is that the\nintact vectors are discriminative, and in the intact space, we have a linear\nclassifier to separate the positive class from the negative class. We define an\nintact vector for each data point, and a view-conditional transformation matrix\nfor each view, and propose to reconstruct the multiple view feature vectors by\nthe product of the corresponding intact vectors and transformation matrices.\nMoreover, we also propose a linear classifier in the intact space, and learn it\njointly with the intact vectors. The learning problem is modeled by a\nminimization problem, and the objective function is composed of a Cauchy error\nestimator-based view-conditional reconstruction term over all data points and\nviews, and a classification error term measured by hinge loss over all the\nintact vectors of all the data points. Some regularization terms are also\nimposed to different variables in the objective function. The minimization\nproblem is solve by an iterative algorithm using alternate optimization\nstrategy and gradient descent algorithm. The proposed algorithm shows it\nadvantage in the compression to other multiview learning algorithms on\nbenchmark data sets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 11:50:46 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Wang", "Qingjun", ""], ["Lv", "Haiyan", ""], ["Yue", "Jun", ""], ["Mitchell", "Eugene", ""]]}, {"id": "1601.02124", "submitter": "Boyue Wang", "authors": "Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun and Baocai Yin", "title": "Kernelized LRR on Grassmann Manifolds for Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank representation (LRR) has recently attracted great interest due to\nits pleasing efficacy in exploring low-dimensional sub- space structures\nembedded in data. One of its successful applications is subspace clustering, by\nwhich data are clustered according to the subspaces they belong to. In this\npaper, at a higher level, we intend to cluster subspaces into classes of\nsubspaces. This is naturally described as a clustering problem on Grassmann\nmanifold. The novelty of this paper is to generalize LRR on Euclidean space\nonto an LRR model on Grassmann manifold in a uniform kernelized LRR framework.\nThe new method has many applications in data analysis in computer vision tasks.\nThe proposed models have been evaluated on a number of practical data analysis\napplications. The experimental results show that the proposed models outperform\na number of state-of-the-art subspace clustering methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 16:01:33 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Yin", "Baocai", ""]]}, {"id": "1601.02129", "submitter": "Zheng Shou", "authors": "Zheng Shou, Dongang Wang, Shih-Fu Chang", "title": "Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address temporal action localization in untrimmed long videos. This is\nimportant because videos in real applications are usually unconstrained and\ncontain multiple action instances plus video content of background scenes or\nother activities. To address this challenging issue, we exploit the\neffectiveness of deep networks in temporal action localization via three\nsegment-based 3D ConvNets: (1) a proposal network identifies candidate segments\nin a long video that may contain actions; (2) a classification network learns\none-vs-all action classification model to serve as initialization for the\nlocalization network; and (3) a localization network fine-tunes on the learned\nclassification network to localize each action instance. We propose a novel\nloss function for the localization network to explicitly consider temporal\noverlap and therefore achieve high temporal localization accuracy. Only the\nproposal network and the localization network are used during prediction. On\ntwo large-scale benchmarks, our approach achieves significantly superior\nperformances compared with other state-of-the-art systems: mAP increases from\n1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014,\nwhen the overlap threshold for evaluation is set to 0.5.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 16:41:21 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 22:15:22 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Shou", "Zheng", ""], ["Wang", "Dongang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1601.02220", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Michael Sapienza, Stuart Golodetz, Julien Valentin,\n  Ondrej Miksik, Shahram Izadi, Philip Torr", "title": "Joint Object-Material Category Segmentation from Audio-Visual Cues", "comments": "Published in British Machine Vision Conference (BMVC) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not always possible to recognise objects and infer material properties\nfor a scene from visual cues alone, since objects can look visually similar\nwhilst being made of very different materials. In this paper, we therefore\npresent an approach that augments the available dense visual cues with sparse\nauditory cues in order to estimate dense object and material labels. Since\nestimates of object class and material properties are mutually informative, we\noptimise our multi-output labelling jointly using a random-field framework. We\nevaluate our system on a new dataset with paired visual and auditory data that\nwe make publicly available. We demonstrate that this joint estimation of object\nand material labels significantly outperforms the estimation of either category\nin isolation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 14:14:53 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Arnab", "Anurag", ""], ["Sapienza", "Michael", ""], ["Golodetz", "Stuart", ""], ["Valentin", "Julien", ""], ["Miksik", "Ondrej", ""], ["Izadi", "Shahram", ""], ["Torr", "Philip", ""]]}, {"id": "1601.02225", "submitter": "Hamid Mansouri", "authors": "Hamid Mansouri (Machine Vision Lab., Computer Engineering Department,\n  Ferdowsi University of Mashhad, Mashhad, Iran) and Hamid-Reza Pourreza\n  (Machine Vision Lab., Computer Engineering Department, Ferdowsi University of\n  Mashhad, Mashhad, Iran)", "title": "Parallel Stroked Multi Line: a model-based method for compressing large\n  fingerprint databases", "comments": "26 pages, 10 figures, submitted to Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing usage of fingerprints as an important biometric data, the\nneed to compress the large fingerprint databases has become essential. The most\nrecommended compression algorithm, even by standards, is JPEG2K. But at high\ncompression rates, this algorithm is ineffective. In this paper, a model is\nproposed which is based on parallel lines with same orientations, arbitrary\nwidths and same gray level values located on rectangle with constant gray level\nvalue as background. We refer to this algorithm as Parallel Stroked Multi Line\n(PSML). By using Adaptive Geometrical Wavelet and employing PSML, a compression\nalgorithm is developed. This compression algorithm can preserve fingerprint\nstructure and minutiae. The exact algorithm of computing the PSML model take\nexponential time. However, we have proposed an alternative approximation\nalgorithm, which reduces the time complexity to $O(n^3)$. The proposed PSML\nalg. has significant advantage over Wedgelets Transform in PSNR value and\nvisual quality in compressed images. The proposed method, despite the lower\nPSNR values than JPEG2K algorithm in common range of compression rates, in all\ncompression rates have nearly equal or greater advantage over JPEG2K when used\nby Automatic Fingerprint Identification Systems (AFIS). At high compression\nrates, according to PSNR values, mean EER rate and visual quality, the encoded\nimages with JPEG2K can not be identified from each other after compression.\nBut, images encoded by the PSML alg. retained the sufficient information to\nmaintain fingerprint identification performances similar to the ones obtained\nby raw images without compression. One the U.are.U 400 database, the mean EER\nrate for uncompressed images is 4.54%, while at 267:1 compression ratio, this\nvalue becomes 49.41% and 6.22% for JPEG2K and PSML, respectively. This result\nshows a significant improvement over the standard JPEG2K algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 15:01:10 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mansouri", "Hamid", "", "Machine Vision Lab., Computer Engineering Department,\n  Ferdowsi University of Mashhad, Mashhad, Iran"], ["Pourreza", "Hamid-Reza", "", "Machine Vision Lab., Computer Engineering Department, Ferdowsi University of\n  Mashhad, Mashhad, Iran"]]}, {"id": "1601.02487", "submitter": "Ahmed Bassiouny", "authors": "Abubakrelsedik Karali, Ahmad Bassiouny and Motaz El-Saban", "title": "Facial Expression Recognition in the Wild using Rich Deep Features", "comments": "in International Conference in Image Processing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Expression Recognition is an active area of research in computer\nvision with a wide range of applications. Several approaches have been\ndeveloped to solve this problem for different benchmark datasets. However,\nFacial Expression Recognition in the wild remains an area where much work is\nstill needed to serve real-world applications. To this end, in this paper we\npresent a novel approach towards facial expression recognition. We fuse rich\ndeep features with domain knowledge through encoding discriminant facial\npatches. We conduct experiments on two of the most popular benchmark datasets;\nCK and TFE. Moreover, we present a novel dataset that, unlike its precedents,\nconsists of natural - not acted - expression images. Experimental results show\nthat our approach achieves state-of-the-art results over standard benchmarks\nand our own dataset\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 15:52:27 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Karali", "Abubakrelsedik", ""], ["Bassiouny", "Ahmad", ""], ["El-Saban", "Motaz", ""]]}, {"id": "1601.02644", "submitter": "Julian Steil", "authors": "Mohsen Mansouryar, Julian Steil, Yusuke Sugano, Andreas Bulling", "title": "3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye\n  Trackers", "comments": null, "journal-ref": null, "doi": "10.1145/2857491.2857530", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D gaze information is important for scene-centric attention analysis but\naccurate estimation and analysis of 3D gaze in real-world environments remains\nchallenging. We present a novel 3D gaze estimation method for monocular\nhead-mounted eye trackers. In contrast to previous work, our method does not\naim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gaze\ndirections in scene camera coordinate space. We first provide a detailed\ndiscussion of the 3D gaze estimation task and summarize different methods,\nincluding our own. We then evaluate the performance of different 3D gaze\nestimation approaches using both simulated and real data. Through experimental\nvalidation, we demonstrate the effectiveness of our method in reducing parallax\nerror, and we identify research challenges for the design of 3D calibration\nprocedures.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 21:11:31 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 14:27:41 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Mansouryar", "Mohsen", ""], ["Steil", "Julian", ""], ["Sugano", "Yusuke", ""], ["Bulling", "Andreas", ""]]}, {"id": "1601.02852", "submitter": "Tae-Hyun Oh", "authors": "Jinsoo Choi, Tae-Hyun Oh, In So Kweon", "title": "Human Attention Estimation for Natural Images: An Automatic Gaze\n  Refinement Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo collections and its applications today attempt to reflect user\ninteractions in various forms. Moreover, photo collections aim to capture the\nusers' intention with minimum effort through applications capturing user\nintentions. Human interest regions in an image carry powerful information about\nthe user's behavior and can be used in many photo applications. Research on\nhuman visual attention has been conducted in the form of gaze tracking and\ncomputational saliency models in the computer vision community, and has shown\nconsiderable progress. This paper presents an integration between implicit gaze\nestimation and computational saliency model to effectively estimate human\nattention regions in images on the fly. Furthermore, our method estimates human\nattention via implicit calibration and incremental model updating without any\nactive participation from the user. We also present extensive analysis and\npossible applications for personal photo collections.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 13:31:38 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1601.02913", "submitter": "Xinchao Li", "authors": "Xinchao Li, Peng Xu, Yue Shi, Martha Larson, Alan Hanjalic", "title": "Learning Subclass Representations for Visually-varied Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a subclass-representation approach that predicts\nthe probability of a social image belonging to one particular class. We explore\nthe co-occurrence of user-contributed tags to find subclasses with a strong\nconnection to the top level class. We then project each image on to the\nresulting subclass space to generate a subclass representation for the image.\nThe novelty of the approach is that subclass representations make use of not\nonly the content of the photos themselves, but also information on the\nco-occurrence of their tags, which determines membership in both subclasses and\ntop-level classes. The novelty is also that the images are classified into\nsmaller classes, which have a chance of being more visually stable and easier\nto model. These subclasses are used as a latent space and images are\nrepresented in this space by their probability of relatedness to all of the\nsubclasses. In contrast to approaches directly modeling each top-level class\nbased on the image content, the proposed method can exploit more information\nfor visually diverse classes. The approach is evaluated on a set of $2$ million\nphotos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale\nFlickr-tag Image Classification Grand Challenge. Experiments show that the\nproposed system delivers sound performance for visually diverse classes\ncompared with methods that directly model top classes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:30:58 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Li", "Xinchao", ""], ["Xu", "Peng", ""], ["Shi", "Yue", ""], ["Larson", "Martha", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1601.02919", "submitter": "Vincent Andrearczyk", "authors": "Vincent Andrearczyk and Paul F. Whelan", "title": "Using Filter Banks in Convolutional Neural Networks for Texture\n  Classification", "comments": "12 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has established many new state of the art solutions in the last\ndecade in areas such as object, scene and speech recognition. In particular\nConvolutional Neural Network (CNN) is a category of deep learning which obtains\nexcellent results in object detection and recognition tasks. Its architecture\nis indeed well suited to object analysis by learning and classifying complex\n(deep) features that represent parts of an object or the object itself.\nHowever, some of its features are very similar to texture analysis methods. CNN\nlayers can be thought of as filter banks of complexity increasing with the\ndepth. Filter banks are powerful tools to extract texture features and have\nbeen widely used in texture analysis. In this paper we develop a simple network\narchitecture named Texture CNN (T-CNN) which explores this observation. It is\nbuilt on the idea that the overall shape information extracted by the fully\nconnected layers of a classic CNN is of minor importance in texture analysis.\nTherefore, we pool an energy measure from the last convolution layer which we\nconnect to a fully connected layer. We show that our approach can improve the\nperformance of a network while greatly reducing the memory usage and\ncomputation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:38:41 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 10:43:24 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 09:32:52 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 10:24:00 GMT"}, {"version": "v5", "created": "Fri, 23 Sep 2016 09:20:56 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Andrearczyk", "Vincent", ""], ["Whelan", "Paul F.", ""]]}, {"id": "1601.02970", "submitter": "Radoslaw Cichy", "authors": "Radoslaw M. Cichy, Aditya Khosla, Dimitrios Pantazis, Antonio\n  Torralba, Aude Oliva", "title": "Deep Neural Networks predict Hierarchical Spatio-temporal Cortical\n  Dynamics of Human Visual Object Recognition", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex multi-stage architecture of cortical visual pathways provides the\nneural basis for efficient visual object recognition in humans. However, the\nstage-wise computations therein remain poorly understood. Here, we compared\ntemporal (magnetoencephalography) and spatial (functional MRI) visual brain\nrepresentations with representations in an artificial deep neural network (DNN)\ntuned to the statistics of real-world visual recognition. We showed that the\nDNN captured the stages of human visual processing in both time and space from\nearly visual areas towards the dorsal and ventral streams. Further\ninvestigation of crucial DNN parameters revealed that while model architecture\nwas important, training on real-world categorization was necessary to enforce\nspatio-temporal hierarchical relationships with the brain. Together our results\nprovide an algorithmically informed view on the spatio-temporal dynamics of\nvisual object recognition in the human visual brain.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 17:34:32 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Cichy", "Radoslaw M.", ""], ["Khosla", "Aditya", ""], ["Pantazis", "Dimitrios", ""], ["Torralba", "Antonio", ""], ["Oliva", "Aude", ""]]}, {"id": "1601.03055", "submitter": "Yuqing Hou", "authors": "Yuqing Hou, Zhouchen Lin, Jin-ge Yao", "title": "Subspace Clustering Based Tag Sharing for Inductive Tag Matrix\n  Refinement with Complex Errors", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating images with tags is useful for indexing and retrieving images.\nHowever, many available annotation data include missing or inaccurate\nannotations. In this paper, we propose an image annotation framework which\nsequentially performs tag completion and refinement. We utilize the subspace\nproperty of data via sparse subspace clustering for tag completion. Then we\npropose a novel matrix completion model for tag refinement, integrating visual\ncorrelation, semantic correlation and the novelly studied property of complex\nerrors. The proposed method outperforms the state-of-the-art approaches on\nmultiple benchmark datasets even when they contain certain levels of annotation\nnoise.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 21:03:43 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 04:41:53 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 15:48:06 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Hou", "Yuqing", ""], ["Lin", "Zhouchen", ""], ["Yao", "Jin-ge", ""]]}, {"id": "1601.03094", "submitter": "Jos\\'e Bento", "authors": "Jos\\'e Bento and Jia Jie Zhu", "title": "A metric for sets of trajectories that is practical and mathematically\n  consistent", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics on the space of sets of trajectories are important for scientists in\nthe field of computer vision, machine learning, robotics, and general\nartificial intelligence. However, existing notions of closeness between sets of\ntrajectories are either mathematically inconsistent or of limited practical\nuse. In this paper, we outline the limitations in the current\nmathematically-consistent metrics, which are based on OSPA (Schuhmacher et al.\n2008); and the inconsistencies in the heuristic notions of closeness used in\npractice, whose main ideas are common to the CLEAR MOT measures (Keni and\nRainer 2008) widely used in computer vision. In two steps, we then propose a\nnew intuitive metric between sets of trajectories and address these\nlimitations. First, we explain a solution that leads to a metric that is hard\nto compute. Then we modify this formulation to obtain a metric that is easy to\ncompute while keeping the useful properties of the previous metric. Our notion\nof closeness is the first demonstrating the following three features: the\nmetric 1) can be quickly computed, 2) incorporates confusion of trajectories'\nidentity in an optimal way, and 3) is a metric in the mathematical sense.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 22:44:16 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 17:53:04 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 22:59:20 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Zhu", "Jia Jie", ""]]}, {"id": "1601.03117", "submitter": "Guangyong Chen", "authors": "Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng", "title": "Blind Image Denoising via Dependent Dirichlet Process Tree", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing image denoising approaches assumed the noise to be homogeneous\nwhite Gaussian distributed with known intensity. However, in real noisy images,\nthe noise models are usually unknown beforehand and can be much more complex.\nThis paper addresses this problem and proposes a novel blind image denoising\nalgorithm to recover the clean image from noisy one with the unknown noise\nmodel. To model the empirical noise of an image, our method introduces the\nmixture of Gaussian distribution, which is flexible enough to approximate\ndifferent continuous distributions. The problem of blind image denoising is\nreformulated as a learning problem. The procedure is to first build a two-layer\nstructural model for noisy patches and consider the clean ones as latent\nvariable. To control the complexity of the noisy patch model, this work\nproposes a novel Bayesian nonparametric prior called \"Dependent Dirichlet\nProcess Tree\" to build the model. Then, this study derives a variational\ninference algorithm to estimate model parameters and recover clean patches. We\napply our method on synthesis and real noisy images with different noise\nmodels. Comparing with previous approaches, ours achieves better performance.\nThe experimental results indicate the efficiency of the proposed algorithm to\ncope with practical image denoising tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 02:44:36 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Zhu", "Fengyuan", ""], ["Chen", "Guangyong", ""], ["Hao", "Jianye", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1601.03128", "submitter": "Anand Mishra Mr.", "authors": "Anand Mishra and Karteek Alahari and C. V. Jawahar", "title": "Enhancing Energy Minimization Framework for Scene Text Recognition with\n  Top-Down Cues", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.01.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing scene text is a challenging problem, even more so than the\nrecognition of scanned documents. This problem has gained significant attention\nfrom the computer vision community in recent years, and several methods based\non energy minimization frameworks and deep learning approaches have been\nproposed. In this work, we focus on the energy minimization framework and\npropose a model that exploits both bottom-up and top-down cues for recognizing\ncropped words extracted from street images. The bottom-up cues are derived from\nindividual character detections from an image. We build a conditional random\nfield model on these detections to jointly model the strength of the detections\nand the interactions between them. These interactions are top-down cues\nobtained from a lexicon-based prior, i.e., language statistics. The optimal\nword represented by the text image is obtained by minimizing the energy\nfunction corresponding to the random field model. We evaluate our proposed\nalgorithm extensively on a number of cropped scene text benchmark datasets,\nnamely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word,\nand show better performance than comparable methods. We perform a rigorous\nanalysis of all the steps in our approach and analyze the results. We also show\nthat state-of-the-art convolutional neural network features can be integrated\nin our framework to further improve the recognition performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 04:47:28 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Mishra", "Anand", ""], ["Alahari", "Karteek", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1601.03239", "submitter": "Victor Schetinger", "authors": "Victor Schetinger, Massimo Iuliani, Alessandro Piva, Manuel M.\n  Oliveira", "title": "Digital Image Forensics vs. Image Composition: An Indirect Arms Race", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of image composition is constantly trying to improve the ways in\nwhich an image can be altered and enhanced. While this is usually done in the\nname of aesthetics and practicality, it also provides tools that can be used to\nmaliciously alter images. In this sense, the field of digital image forensics\nhas to be prepared to deal with the influx of new technology, in a constant\narms-race. In this paper, the current state of this arms-race is analyzed,\nsurveying the state-of-the-art and providing means to compare both sides. A\nnovel scale to classify image forensics assessments is proposed, and\nexperiments are performed to test composition techniques in regards to\ndifferent forensics traces. We show that even though research in forensics\nseems unaware of the advanced forms of image composition, it possesses the\nbasic tools to detect it.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 13:38:36 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Schetinger", "Victor", ""], ["Iuliani", "Massimo", ""], ["Piva", "Alessandro", ""], ["Oliveira", "Manuel M.", ""]]}, {"id": "1601.03295", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka", "title": "Document image classification, with a specific view on applications of\n  patent images", "comments": "Paper submitted in 2014 as book chapter of Current Challenges in\n  Patent Information Retrieval, Second edition by M. Lupu et al (eds.). To\n  appear in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main focus of this paper is document image classification and retrieval,\nwhere we analyze and compare different parameters for the RunLeght Histogram\n(RL) and Fisher Vector (FV) based image representations. We do an exhaustive\nexperimental study using different document image datasets, including the MARG\nbenchmarks, two datasets built on customer data and the images from the Patent\nImage Classification task of the Clef-IP 2011. The aim of the study is to give\nguidelines on how to best choose the parameters such that the same features\nperform well on different tasks. As an example of such need, we describe the\nImage-based Patent Retrieval task's of Clef-IP 2011, where we used the same\nimage representation to predict the image type and retrieve relevant patents.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 16:02:13 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Csurka", "Gabriela", ""]]}, {"id": "1601.03323", "submitter": "John McKay", "authors": "John McKay, Vishal Monga, Raghu Raj", "title": "Localized Dictionary design for Geometrically Robust Sonar ATR", "comments": "Submitted to IGARSS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in Sonar image capture have opened the door to powerful\nclassification schemes for automatic target recognition (ATR. Recent work has\nparticularly seen the application of sparse reconstruction-based classification\n(SRC) to sonar ATR, which provides compelling accuracy rates even in the\npresence of noise and blur. Existing sparsity based sonar ATR techniques\nhowever assume that the test images exhibit geometric pose that is consistent\nwith respect to the training set. This work addresses the outstanding open\nchallenge of handling inconsistently posed test sonar images relative to\ntraining. We develop a new localized block-based dictionary design that can\nenable geometric, i.e. pose robustness. Further, a dictionary learning method\nis incorporated to increase performance and efficiency. The proposed SRC with\nLocalized Pose Management (LPM), is shown to outperform the state of the art\nSIFT feature and SVM approach, due to its power to discern background clutter\nin Sonar images.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 17:35:19 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["McKay", "John", ""], ["Monga", "Vishal", ""], ["Raj", "Raghu", ""]]}, {"id": "1601.03333", "submitter": "Anjith George", "authors": "Anjith George and Aurobinda Routray", "title": "A Score-level Fusion Method for Eye Movement Biometrics", "comments": "11 pages, 6 figures, In press, Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2015.11.020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for the use of eye movement patterns\nfor biometric applications. Eye movements contain abundant information about\ncognitive brain functions, neural pathways, etc. In the proposed method, eye\nmovement data is classified into fixations and saccades. Features extracted\nfrom fixations and saccades are used by a Gaussian Radial Basis Function\nNetwork (GRBFN) based method for biometric authentication. A score fusion\napproach is adopted to classify the data in the output layer. In the evaluation\nstage, the algorithm has been tested using two types of stimuli: random dot\nfollowing on a screen and text reading. The results indicate the strength of\neye movement pattern as a biometric modality. The algorithm has been evaluated\non BioEye 2015 database and found to outperform all the other methods. Eye\nmovements are generated by a complex oculomotor plant which is very hard to\nspoof by mechanical replicas. Use of eye movement dynamics along with iris\nrecognition technology may lead to a robust counterfeit-resistant person\nidentification system.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 18:06:57 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1601.03375", "submitter": "Yinong Wang", "authors": "Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, and Ronald\n  M. Summers", "title": "Multi-Atlas Segmentation with Joint Label Fusion of Osteoporotic\n  Vertebral Compression Fractures on CT", "comments": "MICCAI 2015 Computational Methods and Clinical Applications for Spine\n  Imaging Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The precise and accurate segmentation of the vertebral column is essential in\nthe diagnosis and treatment of various orthopedic, neurological, and\noncological traumas and pathologies. Segmentation is especially challenging in\nthe presence of pathology such as vertebral compression fractures. In this\npaper, we propose a method to produce segmentations for osteoporotic\ncompression fractured vertebrae by applying a multi-atlas joint label fusion\ntechnique for clinical CT images. A total of 170 thoracic and lumbar vertebrae\nwere evaluated using atlases from five patients with varying degrees of spinal\ndegeneration. In an osteoporotic cohort of bundled atlases, registration\nprovided an average Dice coefficient and mean absolute surface distance of\n2.7$\\pm$4.5% and 0.32$\\pm$0.13mm for osteoporotic vertebrae, respectively, and\n90.9$\\pm$3.0% and 0.36$\\pm$0.11mm for compression fractured vertebrae.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 20:30:15 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Wang", "Yinong", ""], ["Yao", "Jianhua", ""], ["Roth", "Holger R.", ""], ["Burns", "Joseph E.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1601.03478", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri", "title": "Deep Learning Applied to Image and Text Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to describe images with natural language sentences is the\nhallmark for image and language understanding. Such a system has wide ranging\napplications such as annotating images and using natural sentences to search\nfor images.In this project we focus on the task of bidirectional image\nretrieval: such asystem is capable of retrieving an image based on a sentence\n(image search) andretrieve sentence based on an image query (image annotation).\nWe present asystem based on a global ranking objective function which uses a\ncombinationof convolutional neural networks (CNN) and multi layer perceptrons\n(MLP).It takes a pair of image and sentence and processes them in different\nchannels,finally embedding it into a common multimodal vector space. These\nembeddingsencode abstract semantic information about the two inputs and can be\ncomparedusing traditional information retrieval approaches. For each such pair,\nthe modelreturns a score which is interpretted as a similarity metric. If this\nscore is high,the image and sentence are likely to convey similar meaning, and\nif the score is low then they are likely not to.\n  The visual input is modeled via deep convolutional neural network. On\ntheother hand we explore three models for the textual module. The first one\nisbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a\ncombination of trigram & skip-grams) with an MLP. The third is morespecialized\ndeep network specific for modeling variable length sequences (SSE).We report\ncomparable performance to recent work in the field, even though ouroverall\nmodel is simpler. We also show that the training time choice of how wecan\ngenerate our negative samples has a significant impact on performance, and can\nbe used to specialize the bi-directional system in one particular task.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:19:33 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""]]}, {"id": "1601.03531", "submitter": "Omar Al-Kadi", "authors": "O. S. Al-Kadi, Daniel Y.F. Chung, Robert C. Carlisle, Constantin C.\n  Coussios, J. Alison Noble", "title": "Quantification of Ultrasonic Texture heterogeneity via Volumetric\n  Stochastic Modeling for Tissue Characterization", "comments": "Supplementary data associated with this article can be found, in the\n  online version, at http://dx.doi.org/10.1016/j.media.2014.12. 004", "journal-ref": "Medical Image Analysis, vol. 21(1), pp. 59-71, 2015", "doi": "10.1016/j.media.2014.12.004", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intensity variations in image texture can provide powerful quantitative\ninformation about physical properties of biological tissue. However, tissue\npatterns can vary according to the utilized imaging system and are\nintrinsically correlated to the scale of analysis. In the case of ultrasound,\nthe Nakagami distribution is a general model of the ultrasonic backscattering\nenvelope under various scattering conditions and densities where it can be\nemployed for characterizing image texture, but the subtle intra-heterogeneities\nwithin a given mass are difficult to capture via this model as it works at a\nsingle spatial scale. This paper proposes a locally adaptive 3D\nmulti-resolution Nakagami-based fractal feature descriptor that extends\nNakagami-based texture analysis to accommodate subtle speckle spatial frequency\ntissue intensity variability in volumetric scans. Local textural fractal\ndescriptors - which are invariant to affine intensity changes - are extracted\nfrom volumetric patches at different spatial resolutions from voxel\nlattice-based generated shape and scale Nakagami parameters. Using ultrasound\nradio-frequency datasets we found that after applying an adaptive fractal\ndecomposition label transfer approach on top of the generated Nakagami voxels,\ntissue characterization results were superior to the state of art. Experimental\nresults on real 3D ultrasonic pre-clinical and clinical datasets suggest that\ndescribing tumor intra-heterogeneity via this descriptor may facilitate\nimproved prediction of therapy response and disease characterization.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 09:51:37 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Al-Kadi", "O. S.", ""], ["Chung", "Daniel Y. F.", ""], ["Carlisle", "Robert C.", ""], ["Coussios", "Constantin C.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1601.03642", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "Creativity in Machine Learning", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent machine learning techniques can be modified to produce creative\nresults. Those results did not exist before; it is not a trivial combination of\nthe data which was fed into the machine learning system. The obtained results\ncome in multiple forms: As images, as text and as audio.\n  This paper gives a high level overview of how they are created and gives some\nexamples. It is meant to be a summary of the current work and give people who\nare new to machine learning some starting points.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 23:28:07 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1601.03679", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang and Yi Yang and Guodong Long and Chengqi Zhang and\n  Alexander G. Hauptmann", "title": "Dynamic Concept Composition for Zero-Example Event Detection", "comments": "7 pages, AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on automatically detecting events in unconstrained\nvideos without the use of any visual training exemplars. In principle,\nzero-shot learning makes it possible to train an event detection model based on\nthe assumption that events (e.g. \\emph{birthday party}) can be described by\nmultiple mid-level semantic concepts (e.g. \"blowing candle\", \"birthday cake\").\nTowards this goal, we first pre-train a bundle of concept classifiers using\ndata from other sources. Then we evaluate the semantic correlation of each\nconcept \\wrt the event of interest and pick up the relevant concept\nclassifiers, which are applied on all test videos to get multiple prediction\nscore vectors. While most existing systems combine the predictions of the\nconcept classifiers with fixed weights, we propose to learn the optimal weights\nof the concept classifiers for each testing video by exploring a set of online\navailable videos with free-form text descriptions of their content. To validate\nthe effectiveness of the proposed approach, we have conducted extensive\nexperiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset.\nThe experimental results confirm the superiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 17:40:09 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Chang", "Xiaojun", ""], ["Yang", "Yi", ""], ["Long", "Guodong", ""], ["Zhang", "Chengqi", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1601.03821", "submitter": "Guangcong Zhang Guangcong Zhang", "authors": "Guangcong Zhang, Mason J. Lilly, and Patricio A. Vela", "title": "Learning Binary Features Online from Motion Dynamics for Incremental\n  Loop-Closure Detection and Place Recognition", "comments": "Accepted to 2016 ICRA (IEEE International Conference on Robotics and\n  Automation)", "journal-ref": null, "doi": "10.1109/ICRA.2016.7487205", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple yet effective approach to learn visual features\nonline for improving loop-closure detection and place recognition, based on\nbag-of-words frameworks. The approach learns a codeword in bag-of-words model\nfrom a pair of matched features from two consecutive frames, such that the\ncodeword has temporally-derived perspective invariance to camera motion. The\nlearning algorithm is efficient: the binary descriptor is generated from the\nmean image patch, and the mask is learned based on discriminative projection by\nminimizing the intra-class distances among the learned feature and the two\noriginal features. A codeword for bag-of-words models is generated by packaging\nthe learned descriptor and mask, with a masked Hamming distance defined to\nmeasure the distance between two codewords. The geometric properties of the\nlearned codewords are then mathematically justified. In addition, hypothesis\nconstraints are imposed through temporal consistency in matched codewords,\nwhich improves precision. The approach, integrated in an incremental\nbag-of-words system, is validated on multiple benchmark data sets and compared\nto state-of-the-art methods. Experiments demonstrate improved precision/recall\noutperforming state of the art with little loss in runtime.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 05:40:13 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 06:44:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Guangcong", ""], ["Lilly", "Mason J.", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1601.03890", "submitter": "Hongyang Xue", "authors": "Hongyang Xue, Deng Cai", "title": "Stereo Matching by Joint Energy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [18], Mozerov et al. propose to perform stereo matching as a two-step\nenergy minimization problem. For the first step they solve a fully connected\nMRF model. And in the next step the marginal output is employed as the unary\ncost for a locally connected MRF model.\n  In this paper we intend to combine the two steps of energy minimization in\norder to improve stereo matching results. We observe that the fully connected\nMRF leads to smoother disparity maps, while the locally connected MRF achieves\nsuperior results in fine-structured regions. Thus we propose to jointly solve\nthe fully connected and locally connected models, taking both their advantages\ninto account. The joint model is solved by mean field approximations. While\nremaining efficient, our joint model outperforms the two-step energy\nminimization approach in both time and estimation error on the Middlebury\nstereo benchmark v3.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 12:15:14 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 23:42:45 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 10:01:54 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Xue", "Hongyang", ""], ["Cai", "Deng", ""]]}, {"id": "1601.03896", "submitter": "Frank Keller", "authors": "Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erkut\n  Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara Plank", "title": "Automatic Description Generation from Images: A Survey of Models,\n  Datasets, and Evaluation Measures", "comments": "Journal of Artificial Intelligence Research 55, 409-442, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic description generation from natural images is a challenging problem\nthat has recently received a large amount of interest from the computer vision\nand natural language processing communities. In this survey, we classify the\nexisting approaches based on how they conceptualize this problem, viz., models\nthat cast description as either generation problem or as a retrieval problem\nover a visual or multimodal representational space. We provide a detailed\nreview of existing models, highlighting their advantages and disadvantages.\nMoreover, we give an overview of the benchmark image datasets and the\nevaluation measures that have been developed to assess the quality of\nmachine-generated image descriptions. Finally we extrapolate future directions\nin the area of automatic image description generation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 12:50:32 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 09:47:20 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bernardi", "Raffaella", ""], ["Cakici", "Ruket", ""], ["Elliott", "Desmond", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""], ["Ikizler-Cinbis", "Nazli", ""], ["Keller", "Frank", ""], ["Muscat", "Adrian", ""], ["Plank", "Barbara", ""]]}, {"id": "1601.03945", "submitter": "Alberto N. Escalante-B.", "authors": "Alberto N. Escalante-B. and Laurenz Wiskott", "title": "Improved graph-based SFA: Information preservation complements the\n  slowness principle", "comments": "40 pages, 9 figures, 9 tables, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is an unsupervised-learning algorithm that\nextracts slowly varying features from a multi-dimensional time series. A\nsupervised extension to SFA for classification and regression is graph-based\nSFA (GSFA). GSFA is based on the preservation of similarities, which are\nspecified by a graph structure derived from the labels. It has been shown that\nhierarchical GSFA (HGSFA) allows learning from images and other\nhigh-dimensional data. The feature space spanned by HGSFA is complex due to the\ncomposition of the nonlinearities of the nodes in the network. However, we show\nthat the network discards useful information prematurely before it reaches\nhigher nodes, resulting in suboptimal global slowness and an under-exploited\nfeature space.\n  To counteract these problems, we propose an extension called hierarchical\ninformation-preserving GSFA (HiGSFA), where information preservation\ncomplements the slowness-maximization goal. We build a 10-layer HiGSFA network\nto estimate human age from facial photographs of the MORPH-II database,\nachieving a mean absolute error of 3.50 years, improving the state-of-the-art\nperformance. HiGSFA and HGSFA support multiple-labels and offer a rich feature\nspace, feed-forward training, and linear complexity in the number of samples\nand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature\nslowness, estimation accuracy and input reconstruction, giving rise to a\npromising hierarchical supervised-learning approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 15:00:20 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Escalante-B.", "Alberto N.", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1601.04115", "submitter": "Chuyang Ye", "authors": "Chuyang Ye, Jiachen Zhuo, Rao P. Gullapalli, Jerry L. Prince", "title": "Estimation of Fiber Orientations Using Neighborhood Information", "comments": "Journal paper accepted in Medical Image Analysis. 35 pages and 16\n  figures", "journal-ref": null, "doi": "10.1016/j.media.2016.05.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data from diffusion magnetic resonance imaging (dMRI) can be used to\nreconstruct fiber tracts, for example, in muscle and white matter. Estimation\nof fiber orientations (FOs) is a crucial step in the reconstruction process and\nthese estimates can be corrupted by noise. In this paper, a new method called\nFiber Orientation Reconstruction using Neighborhood Information (FORNI) is\ndescribed and shown to reduce the effects of noise and improve FO estimation\nperformance by incorporating spatial consistency. FORNI uses a fixed tensor\nbasis to model the diffusion weighted signals, which has the advantage of\nproviding an explicit relationship between the basis vectors and the FOs. FO\nspatial coherence is encouraged using weighted l1-norm regularization terms,\nwhich contain the interaction of directional information between neighbor\nvoxels. Data fidelity is encouraged using a squared error between the observed\nand reconstructed diffusion weighted signals. After appropriate weighting of\nthese competing objectives, the resulting objective function is minimized using\na block coordinate descent algorithm, and a straightforward parallelization\nstrategy is used to speed up processing. Experiments were performed on a\ndigital crossing phantom, ex vivo tongue dMRI data, and in vivo brain dMRI data\nfor both qualitative and quantitative evaluation. The results demonstrate that\nFORNI improves the quality of FO estimation over other state of the art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 02:49:28 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 09:59:51 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Ye", "Chuyang", ""], ["Zhuo", "Jiachen", ""], ["Gullapalli", "Rao P.", ""], ["Prince", "Jerry L.", ""]]}, {"id": "1601.04143", "submitter": "Chunhua Shen", "authors": "Lingqiao Liu, Peng Wang, Chunhua Shen, Lei Wang, Anton van den Hengel,\n  Chao Wang, Heng Tao Shen", "title": "Compositional Model based Fisher Vector Coding for Image Classification", "comments": "Fixed typos. 16 pages. Appearing in IEEE T. Pattern Analysis and\n  Machine Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving from the gradient vector of a generative model of local features,\nFisher vector coding (FVC) has been identified as an effective coding method\nfor image classification. Most, if not all, FVC implementations employ the\nGaussian mixture model (GMM) to depict the generation process of local\nfeatures. However, the representative power of the GMM could be limited because\nit essentially assumes that local features can be characterized by a fixed\nnumber of feature prototypes and the number of prototypes is usually small in\nFVC. To handle this limitation, in this paper we break the convention which\nassumes that a local feature is drawn from one of few Gaussian distributions.\nInstead, we adopt a compositional mechanism which assumes that a local feature\nis drawn from a Gaussian distribution whose mean vector is composed as the\nlinear combination of multiple key components and the combination weight is a\nlatent random variable. In this way, we can greatly enhance the representative\npower of the generative model of FVC. To implement our idea, we designed two\nparticular generative models with such a compositional mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 09:28:41 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 08:27:47 GMT"}, {"version": "v3", "created": "Sun, 8 Jan 2017 08:01:25 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Liu", "Lingqiao", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Wang", "Lei", ""], ["Hengel", "Anton van den", ""], ["Wang", "Chao", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1601.04149", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and\n  Thomas S. Huang", "title": "$\\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of\n  JPEG-Compressed Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a Deep Dual-Domain ($\\mathbf{D^3}$) based fast\nrestoration model to remove artifacts of JPEG compressed images. It leverages\nthe large learning capacity of deep networks, as well as the problem-specific\nexpertise that was hardly incorporated in the past design of deep\narchitectures. For the latter, we take into consideration both the prior\nknowledge of the JPEG compression scheme, and the successful practice of the\nsparsity-based dual-domain approach. We further design the One-Step Sparse\nInference (1-SI) module, as an efficient and light-weighted feed-forward\napproximation of sparse coding. Extensive experiments verify the superiority of\nthe proposed $D^3$ model over several state-of-the-art methods. Specifically,\nour best model is capable of outperforming the latest deep model for around 1\ndB in PSNR, and is 30 times faster.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:38:43 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 03:19:10 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 19:25:08 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wang", "Zhangyang", ""], ["Liu", "Ding", ""], ["Chang", "Shiyu", ""], ["Ling", "Qing", ""], ["Yang", "Yingzhen", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04153", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, and Thomas S.\n  Huang", "title": "Studying Very Low Resolution Recognition Using Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition research often assumes a sufficient resolution of the\nregion of interest (ROI). That is usually violated in practice, inspiring us to\nexplore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI\nin a VLRR problem can be smaller than $16 \\times 16$ pixels, and is challenging\nto be recognized even by human experts. We attempt to solve the VLRR problem\nusing deep learning methods. Taking advantage of techniques primarily in super\nresolution, domain adaptation and robust regression, we formulate a dedicated\ndeep learning method and demonstrate how these techniques are incorporated step\nby step. Any extra complexity, when introduced, is fully justified by both\nanalysis and simulation results. The resulting \\textit{Robust Partially Coupled\nNetworks} achieves feature enhancement and recognition simultaneously. It\nallows for both the flexibility to combat the LR-HR domain mismatch, and the\nrobustness to outliers. Finally, the effectiveness of the proposed models is\nevaluated on three different VLRR tasks, including face identification, digit\nrecognition and font recognition, all of which obtain very impressive\nperformances.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:54:33 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 03:21:40 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Yang", "Yingzhen", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04155", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, and\n  Thomas S. Huang", "title": "Brain-Inspired Deep Networks for Image Aesthetics Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetics assessment has been challenging due to its subjective\nnature. Inspired by the scientific advances in the human visual perception and\nneuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task.\nBDN first learns attributes through the parallel supervised pathways, on a\nvariety of selected feature dimensions. A high-level synthesis network is\ntrained to associate and transform those attributes into the overall aesthetics\nrating. We then extend BDN to predicting the distribution of human ratings,\nsince aesthetics ratings are often subjective. Another highlight is our\nfirst-of-its-kind study of label-preserving transformations in the context of\naesthetics assessment, which leads to an effective data augmentation approach.\nExperimental results on the AVA dataset show that our biological inspired and\ntask-specific BDN model gains significantly performance improvement, compared\nto other state-of-the-art models with the same or higher parameter capacity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:59:40 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 03:46:27 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Dolcos", "Florin", ""], ["Beck", "Diane", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04293", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld and Shimon Ullman", "title": "Face-space Action Recognition by Face-Object Interactions", "comments": "our more recent work on a related topic is described in a separate\n  paper : http://arxiv.org/abs/1511.03814", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition in still images has seen major improvement in recent years\ndue to advances in human pose estimation, object recognition and stronger\nfeature representations. However, there are still many cases in which\nperformance remains far from that of humans. In this paper, we approach the\nproblem by learning explicitly, and then integrating three components of\ntransitive actions: (1) the human body part relevant to the action (2) the\nobject being acted upon and (3) the specific form of interaction between the\nperson and the object. The process uses class-specific features and relations\nnot used in the past for action recognition and which use inherently two cycles\nin the process unlike most standard approaches. We focus on face-related\nactions (FRA), a subset of actions that includes several currently challenging\ncategories. We present an average relative improvement of 52% over state-of-the\nart. We also make a new benchmark publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 13:24:44 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Ullman", "Shimon", ""]]}, {"id": "1601.04386", "submitter": "Ying Huang", "authors": "Ying Huang, Hong Zheng, Haibin Ling, Erik Blasch, Hao Yang", "title": "A Comparative Study of Object Trackers for Infrared Flying Bird Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird strikes present a huge risk for aircraft, especially since traditional\nairport bird surveillance is mainly dependent on inefficient human observation.\nComputer vision based technology has been proposed to automatically detect\nbirds, determine bird flying trajectories, and predict aircraft takeoff delays.\nHowever, the characteristics of bird flight using imagery and the performance\nof existing methods applied to flying bird task are not well known. Therefore,\nwe perform infrared flying bird tracking experiments using 12 state-of-the-art\nalgorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommend\nfeature analysis. We also develop a Struck-scale method to demonstrate the\neffectiveness of multiple scale sampling adaption in handling the object of\nflying bird with varying shape and scale. The general analysis can be used to\ndevelop specialized bird tracking methods for airport safety, wildness and\nurban bird population studies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 02:08:18 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Huang", "Ying", ""], ["Zheng", "Hong", ""], ["Ling", "Haibin", ""], ["Blasch", "Erik", ""], ["Yang", "Hao", ""]]}, {"id": "1601.04406", "submitter": "Vinay Bettadapura", "authors": "Vinay Bettadapura, Daniel Castro, Irfan Essa", "title": "Discovering Picturesque Highlights from Egocentric Vacation Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for identifying picturesque highlights from large\namounts of egocentric video data. Given a set of egocentric videos captured\nover the course of a vacation, our method analyzes the videos and looks for\nimages that have good picturesque and artistic properties. We introduce novel\ntechniques to automatically determine aesthetic features such as composition,\nsymmetry and color vibrancy in egocentric videos and rank the video frames\nbased on their photographic qualities to generate highlights. Our approach also\nuses contextual information such as GPS, when available, to assess the relative\nimportance of each geographic location where the vacation videos were shot.\nFurthermore, we specifically leverage the properties of egocentric videos to\nimprove our highlight detection. We demonstrate results on a new egocentric\nvacation dataset which includes 26.5 hours of videos taken over a 14 day\nvacation that spans many famous tourist destinations and also provide results\nfrom a user-study to access our results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 06:23:14 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Bettadapura", "Vinay", ""], ["Castro", "Daniel", ""], ["Essa", "Irfan", ""]]}, {"id": "1601.04568", "submitter": "Rujie Yin", "authors": "Rujie Yin", "title": "Content Aware Neural Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a content-aware style transfer algorithm for paintings\nand photos of similar content using pre-trained neural network, obtaining\nbetter results than the previous work. In addition, the numerical experiments\nshow that the style pattern and the content information is not completely\nseparated by neural network.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 15:22:48 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Yin", "Rujie", ""]]}, {"id": "1601.04589", "submitter": "Chuan Li", "authors": "Chuan Li, Michael Wand", "title": "Combining Markov Random Fields and Convolutional Neural Networks for\n  Image Synthesis", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a combination of generative Markov random field (MRF)\nmodels and discriminatively trained deep convolutional neural networks (dCNNs)\nfor synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN\nfeature pyramid, controling the image layout at an abstract level. We apply the\nmethod to both photographic and non-photo-realistic (artwork) synthesis tasks.\nThe MRF regularizer prevents over-excitation artifacts and reduces implausible\nfeature mixtures common to previous dCNN inversion approaches, permitting\nsynthezing photographic content with increased visual plausibility. Unlike\nstandard MRF-based texture synthesis, the combined system can both match and\nadapt local features with considerable variability, yielding results far out of\nreach of classic generative MRF methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 16:31:37 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Li", "Chuan", ""], ["Wand", "Michael", ""]]}, {"id": "1601.04619", "submitter": "Daniel Weller", "authors": "Haoyi Liang and Daniel S. Weller", "title": "Comparison-based Image Quality Assessment for Parameter Selection", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2601783", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) is traditionally classified into\nfull-reference (FR) IQA and no-reference (NR) IQA according to whether the\noriginal image is required. Although NR-IQA is widely used in practical\napplications, room for improvement still remains because of the lack of the\nreference image. Inspired by the fact that in many applications, such as\nparameter selection, a series of distorted images are available, the authors\npropose a novel comparison-based image quality assessment (C-IQA) method. The\nnew comparison-based framework parallels FR-IQA by requiring two input images,\nand resembles NR-IQA by not using the original image. As a result, the new\ncomparison-based approach has more application scenarios than FR-IQA does, and\ntakes greater advantage of the accessible information than the traditional\nsingle-input NR-IQA does. Further, C-IQA is compared with other\nstate-of-the-art NR-IQA methods on two widely used IQA databases. Experimental\nresults show that C-IQA outperforms the other NR-IQA methods for parameter\nselection, and the parameter trimming framework combined with C-IQA saves the\ncomputation of iterative image reconstruction up to 80%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 17:38:26 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Liang", "Haoyi", ""], ["Weller", "Daniel S.", ""]]}, {"id": "1601.04667", "submitter": "Patrick Eschenfeldt", "authors": "Patrick Eschenfeldt, Dan Schmidt, Stark Draper, Jonathan Yedidia", "title": "Proactive Message Passing on Memory Factor Networks", "comments": "35 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of graphical model that we call a \"memory factor\nnetwork\" (MFN). We show how to use MFNs to model the structure inherent in many\ntypes of data sets. We also introduce an associated message-passing style\nalgorithm called \"proactive message passing\"' (PMP) that performs inference on\nMFNs. PMP comes with convergence guarantees and is efficient in comparison to\ncompeting algorithms such as variants of belief propagation. We specialize MFNs\nand PMP to a number of distinct types of data (discrete, continuous, labelled)\nand inference problems (interpolation, hypothesis testing), provide examples,\nand discuss approaches for efficient implementation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 19:38:51 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Eschenfeldt", "Patrick", ""], ["Schmidt", "Dan", ""], ["Draper", "Stark", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1601.04669", "submitter": "Cornelia Fermuller Cornelia Fermuller", "authors": "Morimichi Nishigaki and Cornelia Ferm\\\"uller", "title": "The Image Torque Operator for Contour Processing", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contours are salient features for image description, but the detection and\nlocalization of boundary contours is still considered a challenging problem.\nThis paper introduces a new tool for edge processing implementing the\nGestaltism idea of edge grouping. This tool is a mid-level image operator,\ncalled the Torque operator, that is designed to help detect closed contours in\nimages. The torque operator takes as input the raw image and creates an image\nmap by computing from the image gradients within regions of multiple sizes a\nmeasure of how well the edges are aligned to form closed convex contours.\nFundamental properties of the torque are explored and illustrated through\nexamples. Then it is applied in pure bottom-up processing in a variety of\napplications, including edge detection, visual attention and segmentation and\nexperimentally demonstrated a useful tool that can improve existing techniques.\nFinally, its extension as a more general grouping mechanism and application in\nobject recognition is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 19:40:50 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Nishigaki", "Morimichi", ""], ["Ferm\u00fcller", "Cornelia", ""]]}, {"id": "1601.04770", "submitter": "Enming Luo", "authors": "Enming Luo, Stanley H. Chan, Truong Q. Nguyen", "title": "Adaptive Image Denoising by Mixture Adaptation", "comments": "15 pages", "journal-ref": null, "doi": "10.1109/TIP.2016.2590318", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive learning procedure to learn patch-based image priors\nfor image denoising. The new algorithm, called the Expectation-Maximization\n(EM) adaptation, takes a generic prior learned from a generic external database\nand adapts it to the noisy image to generate a specific prior. Different from\nexisting methods that combine internal and external statistics in ad-hoc ways,\nthe proposed algorithm is rigorously derived from a Bayesian hyper-prior\nperspective. There are two contributions of this paper: First, we provide full\nderivation of the EM adaptation algorithm and demonstrate methods to improve\nthe computational complexity. Second, in the absence of the latent clean image,\nwe show how EM adaptation can be modified based on pre-filtering. Experimental\nresults show that the proposed adaptation algorithm yields consistently better\ndenoising results than the one without adaptation and is superior to several\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 01:41:36 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 19:27:36 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 02:37:45 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Luo", "Enming", ""], ["Chan", "Stanley H.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1601.04798", "submitter": "Zequn Jie", "authors": "Zequn Jie, Xiaodan Liang, Jiashi Feng, Wen Feng Lu, Eng Hock Francis\n  Tay and Shuicheng Yan", "title": "Scale-aware Pixel-wise Object Proposal Networks", "comments": "accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2593342", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposal is essential for current state-of-the-art object detection\npipelines. However, the existing proposal methods generally fail in producing\nresults with satisfying localization accuracy. The case is even worse for small\nobjects which however are quite common in practice. In this paper we propose a\nnovel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle the\nchallenges. The SPOP network can generate proposals with high recall rate and\naverage best overlap (ABO), even for small objects. In particular, in order to\nimprove the localization accuracy, a fully convolutional network is employed\nwhich predicts locations of object proposals for each pixel. The produced\nensemble of pixel-wise object proposals enhances the chance of hitting the\nobject significantly without incurring heavy extra computational cost. To solve\nthe challenge of localizing objects at small scale, two localization networks\nwhich are specialized for localizing objects with different scales are\nintroduced, following the divide-and-conquer philosophy. Location outputs of\nthese two networks are then adaptively combined to generate the final proposals\nby a large-/small-size weighting network. Extensive evaluations on PASCAL VOC\n2007 show the SPOP network is superior over the state-of-the-art models. The\nhigh-quality proposals from SPOP network also significantly improve the mean\naverage precision (mAP) of object detection with Fast-RCNN framework. Finally,\nthe SPOP network (trained on PASCAL VOC) shows great generalization performance\nwhen testing it on ILSVRC 2013 validation set.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 04:37:47 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 14:58:39 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 12:10:00 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Jie", "Zequn", ""], ["Liang", "Xiaodan", ""], ["Feng", "Jiashi", ""], ["Lu", "Wen Feng", ""], ["Tay", "Eng Hock Francis", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1601.04805", "submitter": "Anh Cat Le Ngo", "authors": "Anh Cat Le Ngo, John See, Raphael Chung-Wei Phan", "title": "Sparsity in Dynamics of Spontaneous Subtle Emotions: Analysis \\&\n  Application", "comments": "IEEE Transaction of Affective Computing (2016)", "journal-ref": null, "doi": "10.1109/TAFFC.2016.2523996", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous subtle emotions are expressed through micro-expressions, which\nare tiny, sudden and short-lived dynamics of facial muscles; thus poses a great\nchallenge for visual recognition. The abrupt but significant dynamics for the\nrecognition task are temporally sparse while the rest, irrelevant dynamics, are\ntemporally redundant. In this work, we analyze and enforce sparsity constrains\nto learn significant temporal and spectral structures while eliminate\nirrelevant facial dynamics of micro-expressions, which would ease the challenge\nin the visual recognition of spontaneous subtle emotions. The hypothesis is\nconfirmed through experimental results of automatic spontaneous subtle emotion\nrecognition with several sparsity levels on CASME II and SMIC, the only two\npublicly available spontaneous subtle emotion databases. The overall\nperformances of the automatic subtle emotion recognition are boosted when only\nsignificant dynamics are preserved from the original sequences.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 06:13:49 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 05:38:30 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Ngo", "Anh Cat Le", ""], ["See", "John", ""], ["Phan", "Raphael Chung-Wei", ""]]}, {"id": "1601.04871", "submitter": "Mitra Montazeri", "authors": "Mitra Montazeri, Mahdieh Montazeri, Saeid Saryazdi", "title": "Eye detection in digital images: challenges and solutions", "comments": "2th National Conference of Electrical Engineering (NEEC2011),2011,\n  Esfehan, Iran, in Persian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye Detection has an important role in the field of biometric identification\nand known as one method of person's identification. In recent years, many\nefforts have been done which can detect eye automatically and with different\nimage conditions. However, each method has its own drawbacks which can control\nsome of these conditions. In this paper, different methods of eye detection\nwill be categorized and explained. In each category, the advantages and\ndisadvantages of each method will be presented.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 11:02:26 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 08:01:37 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Montazeri", "Mitra", ""], ["Montazeri", "Mahdieh", ""], ["Saryazdi", "Saeid", ""]]}, {"id": "1601.04888", "submitter": "Chi Keung Tang", "authors": "Tai-Pang Wu and Sai-Kit Yeung and Jiaya Jia and Chi-Keung Tang and\n  Gerard Medioni", "title": "A Closed-Form Solution to Tensor Voting: Theory and Applications", "comments": "Addendum appended to the TPAMI paper", "journal-ref": "TPAMI 34(8): 1482-1495 (2012)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a closed-form solution to tensor voting (CFTV): given a point set in\nany dimensions, our closed-form solution provides an exact, continuous and\nefficient algorithm for computing a structure-aware tensor that simultaneously\nachieves salient structure detection and outlier attenuation. Using CFTV, we\nprove the convergence of tensor voting on a Markov random field (MRF), thus\ntermed as MRFTV, where the structure-aware tensor at each input site reaches a\nstationary state upon convergence in structure propagation. We then embed\nstructure-aware tensor into expectation maximization (EM) for optimizing a\nsingle linear structure to achieve efficient and robust parameter estimation.\nSpecifically, our EMTV algorithm optimizes both the tensor and fitting\nparameters and does not require random sampling consensus typically used in\nexisting robust statistical techniques. We performed quantitative evaluation on\nits accuracy and robustness, showing that EMTV performs better than the\noriginal TV and other state-of-the-art techniques in fundamental matrix\nestimation for multiview stereo matching. The extensions of CFTV and EMTV for\nextracting multiple and nonlinear structures are underway. An addendum is\nincluded in this arXiv version.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 12:19:45 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 03:30:37 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Wu", "Tai-Pang", ""], ["Yeung", "Sai-Kit", ""], ["Jia", "Jiaya", ""], ["Tang", "Chi-Keung", ""], ["Medioni", "Gerard", ""]]}, {"id": "1601.04902", "submitter": "Thiago Santini", "authors": "Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, Enkelejda Kasneci", "title": "PupilNet: Convolutional Neural Networks for Robust Pupil Detection", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time, accurate, and robust pupil detection is an essential prerequisite\nfor pervasive video-based eye-tracking. However, automated pupil detection in\nreal-world scenarios has proven to be an intricate challenge due to fast\nillumination changes, pupil occlusion, non centered and off-axis eye recording,\nand physiological eye characteristics. In this paper, we propose and evaluate a\nmethod based on a novel dual convolutional neural network pipeline. In its\nfirst stage the pipeline performs coarse pupil position identification using a\nconvolutional neural network and subregions from a downscaled input image to\ndecrease computational costs. Using subregions derived from a small window\naround the initial pupil position estimate, the second pipeline stage employs\nanother convolutional neural network to refine this position, resulting in an\nincreased pupil detection rate up to 25% in comparison with the best performing\nstate-of-the-art algorithm. Annotated data sets can be made available upon\nrequest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 13:06:16 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Santini", "Thiago", ""], ["Kasneci", "Gjergji", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1601.04920", "submitter": "St\\'ephane Mallat", "authors": "St\\'ephane Mallat", "title": "Understanding Deep Convolutional Networks", "comments": "17 pages, 4 Figures", "journal-ref": null, "doi": "10.1098/rsta.2015.0203", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks provide state of the art classifications and\nregressions results over many high-dimensional problems. We review their\narchitecture, which scatters data with a cascade of linear filter weights and\nnon-linearities. A mathematical framework is introduced to analyze their\nproperties. Computations of invariants involve multiscale contractions, the\nlinearization of hierarchical symmetries, and sparse separations. Applications\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 13:40:47 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Mallat", "St\u00e9phane", ""]]}, {"id": "1601.05030", "submitter": "Vassileios Balntas", "authors": "Vassileios Balntas and Edward Johns and Lilian Tang and Krystian\n  Mikolajczyk", "title": "PN-Net: Conjoined Triple Deep Network for Learning Local Image\n  Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach for learning local descriptors for\nmatching image patches. It has recently been demonstrated that descriptors\nbased on convolutional neural networks (CNN) can significantly improve the\nmatching performance. Unfortunately their computational complexity is\nprohibitive for any practical application. We address this problem and propose\na CNN based descriptor with improved matching performance, significantly\nreduced training and execution time, as well as low dimensionality.\n  We propose to train the network with triplets of patches that include a\npositive and negative pairs. To that end we introduce a new loss function that\nexploits the relations within the triplets. We compare our approach to recently\nintroduced MatchNet and DeepCompare and demonstrate the advantages of our\ndescriptor in terms of performance, memory footprint and speed i.e. when run in\nGPU, the extraction time of our 128 dimensional feature is comparable to the\nfastest available binary descriptors such as BRIEF and ORB.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 18:29:09 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Balntas", "Vassileios", ""], ["Johns", "Edward", ""], ["Tang", "Lilian", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1601.05116", "submitter": "Hossein Mobahi", "authors": "Hossein Mobahi, Stefano Soatto", "title": "A Theory of Local Matching: SIFT and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why has SIFT been so successful? Why its extension, DSP-SIFT, can further\nimprove SIFT? Is there a theory that can explain both? How can such theory\nbenefit real applications? Can it suggest new algorithms with reduced\ncomputational complexity or new descriptors with better accuracy for matching?\nWe construct a general theory of local descriptors for visual matching. Our\ntheory relies on concepts in energy minimization and heat diffusion. We show\nthat SIFT and DSP-SIFT approximate the solution the theory suggests. In\nparticular, DSP-SIFT gives a better approximation to the theoretical solution;\njustifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive\nnew descriptors that have fewer parameters and are potentially better in\nhandling affine deformations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 22:15:48 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Mobahi", "Hossein", ""], ["Soatto", "Stefano", ""]]}, {"id": "1601.05150", "submitter": "Wanli Ouyang", "authors": "Wanli Ouyang, Xiaogang Wang, Cong Zhang, Xiaokang Yang", "title": "Factors in Finetuning Deep Model for object detection", "comments": "CVPR2016 camera ready version. Our ImageNet large scale recognition\n  challenge (ILSVRC15) object detection results (rank 3rd for provided data and\n  2nd for external data) are based on this method. Code available later on\n  http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finetuning from a pretrained deep model is found to yield state-of-the-art\nperformance for many vision tasks. This paper investigates many factors that\ninfluence the performance in finetuning for object detection. There is a\nlong-tailed distribution of sample numbers for classes in object detection. Our\nanalysis and empirical results show that classes with more samples have higher\nimpact on the feature learning. And it is better to make the sample number more\nuniform across classes. Generic object detection can be considered as multiple\nequally important tasks. Detection of each class is a task. These classes/tasks\nhave their individuality in discriminative visual appearance representation.\nTaking this individuality into account, we cluster objects into visually\nsimilar class groups and learn deep representations for these groups\nseparately. A hierarchical feature learning scheme is proposed. In this scheme,\nthe knowledge from the group with large number of classes is transferred for\nlearning features in its sub-groups. Finetuned on the GoogLeNet model,\nexperimental results show 4.7% absolute mAP improvement of our approach on the\nImageNet object detection dataset without increasing much computational cost at\nthe testing stage.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 02:19:48 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 01:15:12 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Zhang", "Cong", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1601.05347", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz and Rainer Stiefelhagen", "title": "Deep Perceptual Mapping for Cross-Modal Face Recognition", "comments": "This is the extended version (invited IJCV submission) with new\n  results of our previous submission (arXiv:1507.02879)", "journal-ref": null, "doi": "10.1007/s11263-016-0933-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross modal face matching between the thermal and visible spectrum is a much\ndesired capability for night-time surveillance and security applications. Due\nto a very large modality gap, thermal-to-visible face recognition is one of the\nmost challenging face matching problem. In this paper, we present an approach\nto bridge this modality gap by a significant margin. Our approach captures the\nhighly non-linear relationship between the two modalities by using a deep\nneural network. Our model attempts to learn a non-linear mapping from visible\nto thermal spectrum while preserving the identity information. We show\nsubstantive performance improvement on three difficult thermal-visible face\ndatasets. The presented approach improves the state-of-the-art by more than\n10\\% on UND-X1 dataset and by more than 15-30\\% on NVESD dataset in terms of\nRank-1 identification. Our method bridges the drop in performance due to the\nmodality gap by more than 40\\%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 17:49:11 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 07:30:51 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1601.05350", "submitter": "Subit  Chakrabarti", "authors": "Subit Chakrabarti and Tara Bongiovanni and Jasmeet Judge and Anand\n  Rangarajan and Sanjay Ranka", "title": "Disaggregation of SMAP L3 Brightness Temperatures to 9km using Kernel\n  Machines", "comments": "14 Pages, 8 Figures, Submitted to IEEE Geoscience and Remote Sensing\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a machine learning algorithm is used for disaggregation of\nSMAP brightness temperatures (T$_{\\textrm{B}}$) from 36km to 9km. It uses image\nsegmentation to cluster the study region based on meteorological and land cover\nsimilarity, followed by a support vector machine based regression that computes\nthe value of the disaggregated T$_{\\textrm{B}}$ at all pixels. High resolution\nremote sensing products such as land surface temperature, normalized difference\nvegetation index, enhanced vegetation index, precipitation, soil texture, and\nland-cover were used for disaggregation. The algorithm was implemented in Iowa,\nUnited States, from April to July 2015, and compared with the SMAP L3_SM_AP\nT$_{\\textrm{B}}$ product at 9km. It was found that the disaggregated\nT$_{\\textrm{B}}$ were very similar to the SMAP-T$_{\\textrm{B}}$ product, even\nfor vegetated areas with a mean difference $\\leq$ 5K. However, the standard\ndeviation of the disaggregation was lower by 7K than that of the AP product.\nThe probability density functions of the disaggregated T$_{\\textrm{B}}$ were\nsimilar to the SMAP-T$_{\\textrm{B}}$. The results indicate that this algorithm\nmay be used for disaggregating T$_{\\textrm{B}}$ using complex non-linear\ncorrelations on a grid.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 17:55:30 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 00:24:46 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Chakrabarti", "Subit", ""], ["Bongiovanni", "Tara", ""], ["Judge", "Jasmeet", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1601.05409", "submitter": "Mitra Montazeri", "authors": "Mitra Montazeri, Mahdieh Soleymani Baghshah, Aliakbar Niknafs", "title": "Selecting Efficient Features via a Hyper-Heuristic Approach", "comments": "The Fifth Iran Data Mining Conference (IDMC 2011), Amirkabir\n  University of Technology, Tehran, Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By Emerging huge databases and the need to efficient learning algorithms on\nthese datasets, new problems have appeared and some methods have been proposed\nto solve these problems by selecting efficient features. Feature selection is a\nproblem of finding efficient features among all features in which the final\nfeature set can improve accuracy and reduce complexity. One way to solve this\nproblem is to evaluate all possible feature subsets. However, evaluating all\npossible feature subsets is an exhaustive search and thus it has high\ncomputational complexity. Until now many heuristic algorithms have been studied\nfor solving this problem. Hyper-heuristic is a new heuristic approach which can\nsearch the solution space effectively by applying local searches appropriately.\nEach local search is a neighborhood searching algorithm. Since each region of\nthe solution space can have its own characteristics, it should be chosen an\nappropriate local search and apply it to current solution. This task is tackled\nto a supervisor. The supervisor chooses a local search based on the functional\nhistory of local searches. By doing this task, it can trade of between\nexploitation and exploration. Since the existing heuristic cannot trade of\nbetween exploration and exploitation appropriately, the solution space has not\nbeen searched appropriately in these methods and thus they have low convergence\nrate. For the first time, in this paper use a hyper-heuristic approach to find\nan efficient feature subset. In the proposed method, genetic algorithm is used\nas a supervisor and 16 heuristic algorithms are used as local searches.\nEmpirical study of the proposed method on several commonly used data sets from\nUCI data sets indicates that it outperforms recent existing methods in the\nliterature for feature selection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 20:59:55 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Montazeri", "Mitra", ""], ["Baghshah", "Mahdieh Soleymani", ""], ["Niknafs", "Aliakbar", ""]]}, {"id": "1601.05447", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen", "title": "Detecting Temporally Consistent Objects in Videos through Object Class\n  Label Propagation", "comments": "Accepted for publication in WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposals for detecting moving or static video objects need to address\nissues such as speed, memory complexity and temporal consistency. We propose an\nefficient Video Object Proposal (VOP) generation method and show its efficacy\nin learning a better video object detector. A deep-learning based video object\ndetector learned using the proposed VOP achieves state-of-the-art detection\nperformance on the Youtube-Objects dataset. We further propose a clustering of\nVOPs which can efficiently be used for detecting objects in video in a\nstreaming fashion. As opposed to applying per-frame convolutional neural\nnetwork (CNN) based object detection, our proposed method called Objects in\nVideo Enabler thRough LAbel Propagation (OVERLAP) needs to classify only a\nsmall fraction of all candidate proposals in every video frame through\nstreaming clustering of object proposals and class-label propagation. Source\ncode will be made available soon.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:45:29 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Tripathi", "Subarna", ""], ["Belongie", "Serge", ""], ["Hwang", "Youngbae", ""], ["Nguyen", "Truong", ""]]}, {"id": "1601.05511", "submitter": "Pichao Wang", "authors": "Jing Zhang and Wanqing Li and Philip O. Ogunbona and Pichao Wang and\n  Chang Tang", "title": "RGB-D-based Action Recognition Datasets: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition from RGB-D (Red, Green, Blue and Depth) data has\nattracted increasing attention since the first work reported in 2010. Over this\nperiod, many benchmark datasets have been created to facilitate the development\nand evaluation of new algorithms. This raises the question of which dataset to\nselect and how to use it in providing a fair and objective comparative\nevaluation against state-of-the-art methods. To address this issue, this paper\nprovides a comprehensive review of the most commonly used action recognition\nrelated RGB-D video datasets, including 27 single-view datasets, 10 multi-view\ndatasets, and 7 multi-person datasets. The detailed information and analysis of\nthese datasets is a useful resource in guiding insightful selection of datasets\nfor future research. In addition, the issues with current algorithm evaluation\nvis-\\'{a}-vis limitations of the available datasets and evaluation protocols\nare also highlighted; resulting in a number of recommendations for collection\nof new datasets and use of evaluation protocols.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 04:58:04 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Zhang", "Jing", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip O.", ""], ["Wang", "Pichao", ""], ["Tang", "Chang", ""]]}, {"id": "1601.05535", "submitter": "Francois Goulette", "authors": "Pierre Charbonnier, Jean-Philippe Tarel (SYNTIM), Francois Goulette\n  (CAOR)", "title": "On the Diagnostic of Road Pathway Visibility", "comments": "in Transport Research Arena Europe, 2010, Bruxelles, France. 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visibility distance on the road pathway plays a significant role in road\nsafety and in particular, has a clear impact on the choice of speed limits.\nVisibility distance is thus of importance for road engineers and authorities.\nWhile visibility distance criteria are routinely taken into account in road\ndesign, only a few systems exist for estimating it on existing road networks.\nMost existing systems comprise a target vehicle followed at a constant distance\nby an observer vehicle, which only allows to check if a given, fixed visibility\ndistance is available. We propose two new approaches that allow estimating the\nmaximum available visibility distance, involving only one vehicle and based on\ndifferent sensor technologies, namely binocular stereovision and 3D range\nsensing (LIDAR). The first approach is based on the processing of two views\ntaken by digital cameras onboard the diagnostic vehicle. The main stages of the\nprocess are: road segmentation, edge registration between the two views, road\nprofile 3D reconstruction and finally, maximal road visibility distance\nestimation. The second approach involves the use of a Terrestrial LIDAR Mobile\nMapping System. The triangulated 3D model of the road and its surroundings\nprovided by the system is used to simulate targets at different distances,\nwhich allows estimating the maximum geometric visibility distance along the\npathway. These approaches were developed in the context of the SARI-VIZIR\nPREDIT project. Both approaches are described, evaluated and compared. Their\npros and cons with respect to vehicle following systems are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 07:50:42 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Charbonnier", "Pierre", "", "SYNTIM"], ["Tarel", "Jean-Philippe", "", "SYNTIM"], ["Goulette", "Francois", "", "CAOR"]]}, {"id": "1601.05585", "submitter": "Abu Sajana Rahmathullah", "authors": "Abu Sajana Rahmathullah, \\'Angel F. Garc\\'ia-Fern\\'andez, Lennart\n  Svensson", "title": "Generalized optimal sub-pattern assignment metric", "comments": "The paper received the Jean Pierre Le Cadre best paper award at the\n  20th International Conference on Information Fusion, July 2017. A Matlab\n  implementation of the proposed GOSPA metric is available in\n  https://github.com/abusajana/GOSPA Also visit https://youtu.be/M79GTTytvCM\n  for a 15-min presentation about the paper", "journal-ref": "Proceedings of the 20th International Conference on Information\n  Fusion (Fusion), 2017", "doi": "10.23919/ICIF.2017.8009645", "report-no": null, "categories": "cs.SY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the generalized optimal sub-pattern assignment (GOSPA)\nmetric on the space of finite sets of targets. Compared to the well-established\noptimal sub-pattern assignment (OSPA) metric, GOSPA is unnormalized as a\nfunction of the cardinality and it penalizes cardinality errors differently,\nwhich enables us to express it as an optimisation over assignments instead of\npermutations. An important consequence of this is that GOSPA allows us to\npenalize localization errors for detected targets and the errors due to missed\nand false targets, as indicated by traditional multiple target tracking (MTT)\nperformance measures, in a sound manner. In addition, we extend the GOSPA\nmetric to the space of random finite sets, which is important to evaluate MTT\nalgorithms via simulations in a rigorous way.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 10:48:58 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 12:09:23 GMT"}, {"version": "v3", "created": "Thu, 2 Feb 2017 20:01:11 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 18:44:13 GMT"}, {"version": "v5", "created": "Mon, 31 Jul 2017 19:50:49 GMT"}, {"version": "v6", "created": "Fri, 11 Aug 2017 09:59:09 GMT"}, {"version": "v7", "created": "Wed, 12 Sep 2018 14:54:49 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Rahmathullah", "Abu Sajana", ""], ["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""]]}, {"id": "1601.05593", "submitter": "Nick Pears", "authors": "Nick Pears and Christian Duncan", "title": "Automatic 3D modelling of craniofacial form", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional models of craniofacial variation over the general\npopulation are useful for assessing pre- and post-operative head shape when\ntreating various craniofacial conditions, such as craniosynostosis. We present\na new method of automatically building both sagittal profile models and full 3D\nsurface models of the human head using a range of techniques in 3D surface\nimage analysis; in particular, automatic facial landmarking using supervised\nmachine learning, global and local symmetry plane detection using a variant of\ntrimmed iterative closest points, locally-affine template warping (for full 3D\nmodels) and a novel pose normalisation using robust iterative ellipse fitting.\nThe PCA-based models built using the new pose normalisation are more compact\nthan those using Generalised Procrustes Analysis and we demonstrate their\nutility in a clinical case study.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 11:46:35 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Pears", "Nick", ""], ["Duncan", "Christian", ""]]}, {"id": "1601.05610", "submitter": "Chunhua Shen", "authors": "Hui Li, Chunhua Shen", "title": "Reading Car License Plates Using Deep Convolutional Neural Networks and\n  LSTMs", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of car license plate detection and\nrecognition in natural scene images. Inspired by the success of deep neural\nnetworks (DNNs) in various vision applications, here we leverage DNNs to learn\nhigh-level features in a cascade framework, which lead to improved performance\non both detection and recognition.\n  Firstly, we train a $37$-class convolutional neural network (CNN) to detect\nall characters in an image, which results in a high recall, compared with\nconventional approaches such as training a binary text/non-text classifier.\nFalse positives are then eliminated by the second plate/non-plate CNN\nclassifier. Bounding box refinement is then carried out based on the edge\ninformation of the license plates, in order to improve the\nintersection-over-union (IoU) ratio. The proposed cascade framework extracts\nlicense plates effectively with both high recall and precision. Last, we\npropose to recognize the license characters as a {sequence labelling} problem.\nA recurrent neural network (RNN) with long short-term memory (LSTM) is trained\nto recognize the sequential features extracted from the whole license plate via\nCNNs. The main advantage of this approach is that it is segmentation free. By\nexploring context information and avoiding errors caused by segmentation, the\nRNN method performs better than a baseline method of combining segmentation and\ndeep CNN classification; and achieves state-of-the-art recognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 12:42:19 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Li", "Hui", ""], ["Shen", "Chunhua", ""]]}, {"id": "1601.05613", "submitter": "Boyue Wang", "authors": "Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin", "title": "Partial Sum Minimization of Singular Values Representation on Grassmann\n  Manifolds", "comments": "Submitting to ACM Transactions on Knowledge Discovery from Data with\n  minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a significant subspace clustering method, low rank representation (LRR)\nhas attracted great attention in recent years. To further improve the\nperformance of LRR and extend its applications, there are several issues to be\nresolved. The nuclear norm in LRR does not sufficiently use the prior knowledge\nof the rank which is known in many practical problems. The LRR is designed for\nvectorial data from linear spaces, thus not suitable for high dimensional data\nwith intrinsic non-linear manifold structure. This paper proposes an extended\nLRR model for manifold-valued Grassmann data which incorporates prior knowledge\nby minimizing partial sum of singular values instead of the nuclear norm,\nnamely Partial Sum minimization of Singular Values Representation (GPSSVR). The\nnew model not only enforces the global structure of data in low rank, but also\nretains important information by minimizing only smaller singular values. To\nfurther maintain the local structures among Grassmann points, we also integrate\nthe Laplacian penalty with GPSSVR. An effective algorithm is proposed to solve\nthe optimization problem based on the GPSSVR model. The proposed model and\nalgorithms are assessed on some widely used human action video datasets and a\nreal scenery dataset. The experimental results show that the proposed methods\nobviously outperform other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 12:47:17 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2016 01:57:28 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 03:19:27 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Yin", "Baocai", ""]]}, {"id": "1601.05644", "submitter": "Weilong Peng", "authors": "Weilong Peng (1), Zhiyong Feng (1) and Chao Xu (2) ((1) School of\n  Computer Science, Tianjin University (2) School of Software, Tianjin\n  University)", "title": "B-spline Shape from Motion & Shading: An Automatic Free-form Surface\n  Modeling for Face Reconstruction", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, many methods have been proposed for face reconstruction from\nmultiple images, most of which involve fundamental principles of Shape from\nShading and Structure from motion. However, a majority of the methods just\ngenerate discrete surface model of face. In this paper, B-spline Shape from\nMotion and Shading (BsSfMS) is proposed to reconstruct continuous B-spline\nsurface for multi-view face images, according to an assumption that shading and\nmotion information in the images contain 1st- and 0th-order derivative of\nB-spline face respectively. Face surface is expressed as a B-spline surface\nthat can be reconstructed by optimizing B-spline control points. Therefore,\nnormals and 3D feature points computed from shading and motion of images\nrespectively are used as the 1st- and 0th- order derivative information, to be\njointly applied in optimizing the B-spline face. Additionally, an IMLS\n(iterative multi-least-square) algorithm is proposed to handle the difficult\ncontrol point optimization. Furthermore, synthetic samples and LFW dataset are\nintroduced and conducted to verify the proposed approach, and the experimental\nresults demonstrate the effectiveness with different poses, illuminations,\nexpressions etc., even with wild images.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 14:11:40 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Peng", "Weilong", ""], ["Feng", "Zhiyong", ""], ["Xu", "Chao", ""]]}, {"id": "1601.05767", "submitter": "Subit  Chakrabarti", "authors": "Subit Chakrabarti and Jasmeet Judge and Tara Bongiovanni and Anand\n  Rangarajan and Sanjay Ranka", "title": "Spatial Scaling of Satellite Soil Moisture using Temporal Correlations\n  and Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm is developed to downscale soil moisture (SM), obtained at\nsatellite scales of 10-40 km by utilizing its temporal correlations to\nhistorical auxiliary data at finer scales. Including such correlations\ndrastically reduces the size of the training set needed, accounts for\ntime-lagged relationships, and enables downscaling even in the presence of\nshort gaps in the auxiliary data. The algorithm is based upon bagged regression\ntrees (BRT) and uses correlations between high-resolution remote sensing\nproducts and SM observations. The algorithm trains multiple regression trees\nand automatically chooses the trees that generate the best downscaled\nestimates. The algorithm was evaluated using a multi-scale synthetic dataset in\nnorth central Florida for two years, including two growing seasons of corn and\none growing season of cotton per year. The time-averaged error across the\nregion was found to be 0.01 $\\mathrm{m}^3/\\mathrm{m}^3$, with a standard\ndeviation of 0.012 $\\mathrm{m}^3/\\mathrm{m}^3$ when 0.02% of the data were used\nfor training in addition to temporal correlations from the past seven days, and\nall available data from the past year. The maximum spatially averaged errors\nobtained using this algorithm in downscaled SM were 0.005\n$\\mathrm{m}^3/\\mathrm{m}^3$, for pixels with cotton land-cover. When land\nsurface temperature~(LST) on the day of downscaling was not included in the\nalgorithm to simulate \"data gaps\", the spatially averaged error increased\nminimally by 0.015 $\\mathrm{m}^3/\\mathrm{m}^3$ when LST is unavailable on the\nday of downscaling. The results indicate that the BRT-based algorithm provides\nhigh accuracy for downscaling SM using complex non-linear spatio-temporal\ncorrelations, under heterogeneous micro meteorological conditions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 20:19:19 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Chakrabarti", "Subit", ""], ["Judge", "Jasmeet", ""], ["Bongiovanni", "Tara", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1601.05861", "submitter": "Amr Bakry", "authors": "Amr Bakry and Ahmed Elgammal", "title": "Manifold-Kernels Comparison in MKPLS for Visual Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition is a challenging problem. Due to the acoustic limitations,\nusing visual information is essential for improving the recognition accuracy in\nreal-life unconstraint situations. One common approach is to model the visual\nrecognition as nonlinear optimization problem. Measuring the distances between\nvisual units is essential for solving this problem. Embedding the visual units\non a manifold and using manifold kernels is one way to measure these distances.\nThis work is intended to evaluate the performance of several manifold kernels\nfor solving the problem of visual speech recognition. We show the theory behind\neach kernel. We apply manifold kernel partial least squares framework to OuluVs\nand AvLetters databases, and show empirical comparison between all kernels.\nThis framework provides convenient way to explore different kernels.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 01:59:26 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Bakry", "Amr", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1601.05900", "submitter": "Jarrod Moore", "authors": "Margareta Ackerman and Jarrod Moore", "title": "When is Clustering Perturbation Robust?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental data mining tool that aims to divide data into\ngroups of similar items. Generally, intuition about clustering reflects the\nideal case -- exact data sets endowed with flawless dissimilarity between\nindividual instances.\n  In practice however, these cases are in the minority, and clustering\napplications are typically characterized by noisy data sets with approximate\npairwise dissimilarities. As such, the efficacy of clustering methods in\npractical applications necessitates robustness to perturbations.\n  In this paper, we perform a formal analysis of perturbation robustness,\nrevealing that the extent to which algorithms can exhibit this desirable\ncharacteristic is inherently limited, and identifying the types of structures\nthat allow popular clustering paradigms to discover meaningful clusters in\nspite of faulty data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 08:01:58 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Ackerman", "Margareta", ""], ["Moore", "Jarrod", ""]]}, {"id": "1601.05994", "submitter": "Wei Wang", "authors": "Wei Wang and Chuanjiang He", "title": "Depth and Reflection Total Variation for Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze removal has been a very challenging problem due to its ill-posedness,\nwhich is more ill-posed if the input data is only a single hazy image. In this\npaper, we present a new approach for removing haze from a single input image.\nThe proposed method combines the model widely used to describe the formation of\na haze image with the assumption in Retinex that an image is the product of the\nillumination and the reflection. We assume that the depth and reflection\nfunctions are spatially piecewise smooth in the model, where the total\nvariation is used for the regularization. The proposed model is defined as a\nconstrained optimization problem, which is solved by an alternating\nminimization scheme and the fast gradient projection algorithm. Some theoretic\nanalyses are given for the proposed model and algorithm. Finally, numerical\nexamples are presented to demonstrate that our method can restore vivid and\ncontrastive hazy images effectively.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 13:36:20 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Wang", "Wei", ""], ["He", "Chuanjiang", ""]]}, {"id": "1601.06032", "submitter": "Wangmeng Zuo", "authors": "Wangmeng Zuo, Xiaohe Wu, Liang Lin, Lei Zhang, and Ming-Hsuan Yang", "title": "Learning Support Correlation Filters for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling and budgeting training examples are two essential factors in\ntracking algorithms based on support vector machines (SVMs) as a trade-off\nbetween accuracy and efficiency. Recently, the circulant matrix formed by dense\nsampling of translated image patches has been utilized in correlation filters\nfor fast tracking. In this paper, we derive an equivalent formulation of a SVM\nmodel with circulant matrix expression and present an efficient alternating\noptimization method for visual tracking. We incorporate the discrete Fourier\ntransform with the proposed alternating optimization process, and pose the\ntracking problem as an iterative learning of support correlation filters (SCFs)\nwhich find the global optimal solution with real-time performance. For a given\ncirculant data matrix with n^2 samples of size n*n, the computational\ncomplexity of the proposed algorithm is O(n^2*logn) whereas that of the\nstandard SVM-based approaches is at least O(n^4). In addition, we extend the\nSCF-based tracking algorithm with multi-channel features, kernel functions, and\nscale-adaptive approaches to further improve the tracking performance.\nExperimental results on a large benchmark dataset show that the proposed\nSCF-based algorithms perform favorably against the state-of-the-art tracking\nmethods in terms of accuracy and speed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 15:02:50 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Zuo", "Wangmeng", ""], ["Wu", "Xiaohe", ""], ["Lin", "Liang", ""], ["Zhang", "Lei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1601.06041", "submitter": "Elias Alevizos", "authors": "Kostas Patroumpas and Elias Alevizos and Alexander Artikis and Marios\n  Vodas and Nikos Pelekis and Yannis Theodoridis", "title": "Online Event Recognition from Moving Vessel Trajectories", "comments": null, "journal-ref": null, "doi": "10.1007/s10707-016-0266-x", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for online monitoring of maritime activity over streaming\npositions from numerous vessels sailing at sea. It employs an online tracking\nmodule for detecting important changes in the evolving trajectory of each\nvessel across time, and thus can incrementally retain concise, yet reliable\nsummaries of its recent movement. In addition, thanks to its complex event\nrecognition module, this system can also offer instant notification to marine\nauthorities regarding emergency situations, such as risk of collisions,\nsuspicious moves in protected zones, or package picking at open sea. Not only\ndid our extensive tests validate the performance, efficiency, and robustness of\nthe system against scalable volumes of real-world and synthetically enlarged\ndatasets, but its deployment against online feeds from vessels has also\nconfirmed its capabilities for effective, real-time maritime surveillance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 15:24:41 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Patroumpas", "Kostas", ""], ["Alevizos", "Elias", ""], ["Artikis", "Alexander", ""], ["Vodas", "Marios", ""], ["Pelekis", "Nikos", ""], ["Theodoridis", "Yannis", ""]]}, {"id": "1601.06044", "submitter": "Wilder Bezerra Lopes", "authors": "Wilder B. Lopes, Anas Al-Nuaimi, Cassio G. Lopes", "title": "Geometric-Algebra LMS Adaptive Filter and its Application to Rotation\n  Estimation", "comments": "4 pages of content plus 1 of references; 4 figures. Supplementary\n  material (codes and datasets) available at www.lps.usp.br/wilder", "journal-ref": null, "doi": "10.1109/LSP.2016.2558461", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exploits Geometric (Clifford) Algebra (GA) theory in order to\ndevise and introduce a new adaptive filtering strategy. From a least-squares\ncost function, the gradient is calculated following results from Geometric\nCalculus (GC), the extension of GA to handle differential and integral\ncalculus. The novel GA least-mean-squares (GA-LMS) adaptive filter, which\ninherits properties from standard adaptive filters and from GA, is developed to\nrecursively estimate a rotor (multivector), a hypercomplex quantity able to\ndescribe rotations in any dimension. The adaptive filter (AF) performance is\nassessed via a 3D point-clouds registration problem, which contains a rotation\nestimation step. Calculating the AF computational complexity suggests that it\ncan contribute to reduce the cost of a full-blown 3D registration algorithm,\nespecially when the number of points to be processed grows. Moreover, the\nemployed GA/GC framework allows for easily applying the resulting filter to\nestimating rotors in higher dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 15:38:23 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Lopes", "Wilder B.", ""], ["Al-Nuaimi", "Anas", ""], ["Lopes", "Cassio G.", ""]]}, {"id": "1601.06057", "submitter": "Bartosz Zieli\\'nski", "authors": "Matthias Zeppelzauer, Bartosz Zieli\\'nski, Mateusz Juda and Markus\n  Seidl", "title": "Topological descriptors for 3D surface analysis", "comments": "12 pages, 3 figures, CTIC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate topological descriptors for 3D surface analysis, i.e. the\nclassification of surfaces according to their geometric fine structure. On a\ndataset of high-resolution 3D surface reconstructions we compute persistence\ndiagrams for a 2D cubical filtration. In the next step we investigate different\ntopological descriptors and measure their ability to discriminate structurally\ndifferent 3D surface patches. We evaluate their sensitivity to different\nparameters and compare the performance of the resulting topological descriptors\nto alternative (non-topological) descriptors. We present a comprehensive\nevaluation that shows that topological descriptors are (i) robust, (ii) yield\nstate-of-the-art performance for the task of 3D surface analysis and (iii)\nimprove classification performance when combined with non-topological\ndescriptors.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:10:54 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Zieli\u0144ski", "Bartosz", ""], ["Juda", "Mateusz", ""], ["Seidl", "Markus", ""]]}, {"id": "1601.06062", "submitter": "Matthias Hoffmann", "authors": "Matthias Hoffmann, Christopher Kowalewski, Andreas Maier, Klaus\n  Kurzidim, Norbert Strobel, Joachim Hornegger", "title": "3-D/2-D Registration of Cardiac Structures by 3-D Contrast Agent\n  Distribution Estimation", "comments": null, "journal-ref": null, "doi": "10.1155/2016/7690391", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For augmented fluoroscopy during cardiac catheter ablation procedures, a\npreoperatively acquired 3-D model of the left atrium of the patient can be\nregistered to X-ray images. Therefore the 3D-model is matched with the contrast\nagent based appearance of the left atrium. Commonly, only small amounts of\ncontrast agent (CA) are used to locate the left atrium. This is why we focus on\nrobust registration methods that work also if the structure of interest is only\npartially contrasted. In particular, we propose two similarity measures for\nCA-based registration: The first similarity measure, explicit apparent edges,\nfocuses on edges of the patient anatomy made visible by contrast agent and can\nbe computed quickly on the GPU. The second novel similarity measure computes a\ncontrast agent distribution estimate (CADE) inside the 3-D model and rates its\nconsistency with the CA seen in biplane fluoroscopic images. As the CADE\ncomputation involves a reconstruction of CA in 3-D using the CA within the\nfluoroscopic images, it is slower. Using a combination of both methods, our\nevaluation on 11 well-contrasted clinical datasets yielded an error of\n7.9+/-6.3 mm over all frames. For 10 datasets with little CA, we obtained an\nerror of 8.8+/-6.7 mm. Our new methods outperform a registration based on the\nprojected shadow significantly (p<0.05).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:23:25 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Hoffmann", "Matthias", ""], ["Kowalewski", "Christopher", ""], ["Maier", "Andreas", ""], ["Kurzidim", "Klaus", ""], ["Strobel", "Norbert", ""], ["Hornegger", "Joachim", ""]]}, {"id": "1601.06070", "submitter": "Zorah L\\\"ahner", "authors": "Zorah L\\\"ahner, Emanuele Rodol\\`a, Frank R. Schmidt, Michael M.\n  Bronstein, Daniel Cremers", "title": "Efficient Globally Optimal 2D-to-3D Deformable Shape Matching", "comments": "to appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first algorithm for non-rigid 2D-to-3D shape matching, where\nthe input is a 2D shape represented as a planar curve and a 3D shape\nrepresented as a surface; the output is a continuous curve on the surface. We\ncast the problem as finding the shortest circular path on the prod- uct\n3-manifold of the surface and the curve. We prove that the optimal matching can\nbe computed in polynomial time with a (worst-case) complexity of\n$O(mn^2\\log(n))$, where $m$ and $n$ denote the number of vertices on the\ntemplate curve and the 3D shape respectively. We also demonstrate that in\npractice the runtime is essentially linear in $m\\!\\cdot\\! n$ making it an\nefficient method for shape analysis and shape retrieval. Quantitative\nevaluation confirms that the method provides excellent results for sketch-based\ndeformable 3D shape re- trieval.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:58:32 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 09:55:22 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["L\u00e4hner", "Zorah", ""], ["Rodol\u00e0", "Emanuele", ""], ["Schmidt", "Frank R.", ""], ["Bronstein", "Michael M.", ""], ["Cremers", "Daniel", ""]]}, {"id": "1601.06087", "submitter": "Aria Ahmadi", "authors": "Aria Ahmadi and Ioannis Patras", "title": "Unsupervised convolutional neural networks for motion estimation", "comments": "Submitted to ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for motion estimation estimate the motion field F between\na pair of images as the one that minimizes a predesigned cost function. In this\npaper, we propose a direct method and train a Convolutional Neural Network\n(CNN) that when, at test time, is given a pair of images as input it produces a\ndense motion field F at its output layer. In the absence of large datasets with\nground truth motion that would allow classical supervised training, we propose\nto train the network in an unsupervised manner. The proposed cost function that\nis optimized during training, is based on the classical optical flow\nconstraint. The latter is differentiable with respect to the motion field and,\ntherefore, allows backpropagation of the error to previous layers of the\nnetwork. Our method is tested on both synthetic and real image sequences and\nperforms similarly to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 17:57:07 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Ahmadi", "Aria", ""], ["Patras", "Ioannis", ""]]}, {"id": "1601.06243", "submitter": "Yao Wang", "authors": "Shiying He, Haiwei Zhou, Yao Wang, Wenfei Cao and Zhi Han", "title": "Super-resolution reconstruction of hyperspectral images via low rank\n  tensor modeling and total variation regularization", "comments": "submitted to IGARSS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to hyperspectral image\nsuper-resolution by modeling the global spatial-and-spectral correlation and\nlocal smoothness properties over hyperspectral images. Specifically, we utilize\nthe tensor nuclear norm and tensor folded-concave penalty functions to describe\nthe global spatial-and-spectral correlation hidden in hyperspectral images, and\n3D total variation (TV) to characterize the local spatial-and-spectral\nsmoothness across all hyperspectral bands. Then, we develop an efficient\nalgorithm for solving the resulting optimization problem by combing the local\nlinear approximation (LLA) strategy and alternative direction method of\nmultipliers (ADMM). Experimental results on one hyperspectral image dataset\nillustrate the merits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 07:07:16 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["He", "Shiying", ""], ["Zhou", "Haiwei", ""], ["Wang", "Yao", ""], ["Cao", "Wenfei", ""], ["Han", "Zhi", ""]]}, {"id": "1601.06251", "submitter": "Homa Davoudi", "authors": "Homa Davoudi, Ehsanollah Kabir", "title": "Using compatible shape descriptor for lexicon reduction of printed Farsi\n  subwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This Paper presents a method for lexicon reduction of Printed Farsi subwords\nbased on their holistic shape features. Because of the large number of Persian\nsubwords variously shaped from a simple letter to a complex combination of\nseveral connected characters, it is not easy to find a fixed shape descriptor\nsuitable for all subwords. In this paper, we propose to select the descriptor\naccording to the input shape characteristics. To do this, a neural network is\ntrained to predict the appropriate descriptor of the input image. This network\nis implemented in the proposed lexicon reduction system to decide on the\ndescriptor used for comparison of the query image with the lexicon entries.\nEvaluating the proposed method on a dataset of Persian subwords allows one to\nattest the effectiveness of the proposed idea of dealing differently with\nvarious query shapes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 08:49:00 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Davoudi", "Homa", ""], ["Kabir", "Ehsanollah", ""]]}, {"id": "1601.06260", "submitter": "Xiatian Zhu", "authors": "Taiqing Wang and Shaogang Gong and Xiatian Zhu and Shengjin Wang", "title": "Person Re-Identification by Discriminative Selection in Video Ranking", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current person re-identification (ReID) methods typically rely on\nsingle-frame imagery features, whilst ignoring space-time information from\nimage sequences often available in the practical surveillance scenarios.\nSingle-frame (single-shot) based visual appearance matching is inherently\nlimited for person ReID in public spaces due to the challenging visual\nambiguity and uncertainty arising from non-overlapping camera views where\nviewing condition changes can cause significant people appearance variations.\nIn this work, we present a novel model to automatically select the most\ndiscriminative video fragments from noisy/incomplete image sequences of people\nfrom which reliable space-time and appearance features can be computed, whilst\nsimultaneously learning a video ranking function for person ReID. Using the\nPRID$2011$, iLIDS-VID, and HDA+ image sequence datasets, we extensively\nconducted comparative evaluations to demonstrate the advantages of the proposed\nmodel over contemporary gait recognition, holistic image sequence matching and\nstate-of-the-art single-/multi-shot ReID methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 10:33:45 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Wang", "Taiqing", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""], ["Wang", "Shengjin", ""]]}, {"id": "1601.06274", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov, Christian Reinbacher, Gottfried Graber and\n  Thomas Pock", "title": "Solving Dense Image Matching in Real-Time using Discrete-Continuous\n  Optimization", "comments": "21 st Computer Vision Winter Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense image matching is a fundamental low-level problem in Computer Vision,\nwhich has received tremendous attention from both discrete and continuous\noptimization communities. The goal of this paper is to combine the advantages\nof discrete and continuous optimization in a coherent framework. We devise a\nmodel based on energy minimization, to be optimized by both discrete and\ncontinuous algorithms in a consistent way. In the discrete setting, we propose\na novel optimization algorithm that can be massively parallelized. In the\ncontinuous setting we tackle the problem of non-convex regularizers by a\nformulation based on differences of convex functions. The resulting hybrid\ndiscrete-continuous algorithm can be efficiently accelerated by modern GPUs and\nwe demonstrate its real-time performance for the applications of dense stereo\nmatching and optical flow.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 14:59:08 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Reinbacher", "Christian", ""], ["Graber", "Gottfried", ""], ["Pock", "Thomas", ""]]}, {"id": "1601.06342", "submitter": "Sung-Hsien Hsieh", "authors": "Sung-Hsien Hsieh, Chun-Shien Lu, Soo-Chang Pei", "title": "Fast Binary Embedding via Circulant Downsampled Matrix -- A\n  Data-Independent Approach", "comments": "8 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary embedding of high-dimensional data aims to produce low-dimensional\nbinary codes while preserving discriminative power. State-of-the-art methods\noften suffer from high computation and storage costs. We present a simple and\nfast embedding scheme by first downsampling N-dimensional data into\nM-dimensional data and then multiplying the data with an MxM circulant matrix.\nOur method requires O(N +M log M) computation and O(N) storage costs. We prove\nif data have sparsity, our scheme can achieve similarity-preserving well.\nExperiments further demonstrate that though our method is cost-effective and\nfast, it still achieves comparable performance in image applications.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 03:32:22 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hsieh", "Sung-Hsien", ""], ["Lu", "Chun-Shien", ""], ["Pei", "Soo-Chang", ""]]}, {"id": "1601.06403", "submitter": "Ali Moharrer", "authors": "Ali Moharrer, Shuangqing Wei, George T. Amariucai, Jing Deng", "title": "Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An\n  Information Theoretic Approach", "comments": "14 pages, 9 figures, part of this work is submitted to Allerton 2016\n  conference, UIUC, IL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In latent Gaussian trees the pairwise correlation signs between the variables\nare intrinsically unrecoverable. Such information is vital since it completely\ndetermines the direction in which two variables are associated. In this work,\nwe resort to information theoretical approaches to achieve two fundamental\ngoals: First, we quantify the amount of information loss due to unrecoverable\nsign information. Second, we show the importance of such information in\ndetermining the maximum achievable rate region, in which the observed output\nvector can be synthesized, given its probability density function. In\nparticular, we model the graphical model as a communication channel and propose\na new layered encoding framework to synthesize observed data using upper layer\nGaussian inputs and independent Bernoulli correlation sign inputs from each\nlayer. We find the achievable rate region for the rate tuples of multi-layer\nlatent Gaussian messages to synthesize the desired observables.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 15:59:44 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 03:42:18 GMT"}, {"version": "v3", "created": "Sat, 23 Apr 2016 21:59:49 GMT"}, {"version": "v4", "created": "Mon, 4 Jul 2016 16:24:03 GMT"}, {"version": "v5", "created": "Thu, 7 Jul 2016 22:04:07 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Moharrer", "Ali", ""], ["Wei", "Shuangqing", ""], ["Amariucai", "George T.", ""], ["Deng", "Jing", ""]]}, {"id": "1601.06603", "submitter": "Sibo Song", "authors": "Sibo Song, Ngai-Man Cheung, Vijay Chandrasekhar, Bappaditya Mandal,\n  Jie Lin", "title": "Egocentric Activity Recognition with Multimodal Fisher Vector", "comments": "5 pages, 4 figures, ICASSP 2016 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of wearable devices, research on egocentric\nactivity recognition has received much attention recently. In this paper, we\nbuild a Multimodal Egocentric Activity dataset which includes egocentric videos\nand sensor data of 20 fine-grained and diverse activity categories. We present\na novel strategy to extract temporal trajectory-like features from sensor data.\nWe propose to apply the Fisher Kernel framework to fuse video and temporal\nenhanced sensor features. Experiment results show that with careful design of\nfeature extraction and fusion algorithm, sensor data can enhance\ninformation-rich video data. We make publicly available the Multimodal\nEgocentric Activity dataset to facilitate future research.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 13:57:07 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Song", "Sibo", ""], ["Cheung", "Ngai-Man", ""], ["Chandrasekhar", "Vijay", ""], ["Mandal", "Bappaditya", ""], ["Lin", "Jie", ""]]}, {"id": "1601.06608", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi and Samarendra Dandapat, and Rohit Sinha", "title": "An Unsupervised Method for Detection and Validation of The Optic Disc\n  and The Fovea", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have presented a novel method for detection of retinal image\nfeatures, the optic disc and the fovea, from colour fundus photographs of\ndilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map based\nmethod was used to detect the optic disc followed by an unsupervised\nprobabilistic Latent Semantic Analysis for detection validation. The validation\nconcept is based on distinct vessels structures in the optic disc. By using the\nclinical information of standard location of the fovea with respect to the\noptic disc, the macula region is estimated. Accuracy of 100\\% detection is\nachieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\\%\ndetection accuracy on STARE dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:05:36 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Haloi", "Mrinal", ""], ["Dandapat", "Samarendra", ""], ["Sinha", "Rohit", ""]]}, {"id": "1601.06615", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita\n  Prabhu, Srinivas S S Kruthiventi and R. Venkatesh Babu", "title": "A Taxonomy of Deep Convolutional Neural Nets for Computer Vision", "comments": "Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm)", "journal-ref": "Frontiers in Robotics and AI 2(36), January 2016", "doi": "10.3389/frobt.2015.00036", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional architectures for solving computer vision problems and the degree\nof success they enjoyed have been heavily reliant on hand-crafted features.\nHowever, of late, deep learning techniques have offered a compelling\nalternative -- that of automatically learning problem-specific features. With\nthis new paradigm, every problem in computer vision is now being re-examined\nfrom a deep learning perspective. Therefore, it has become important to\nunderstand what kind of deep networks are suitable for a given problem.\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\nexist, a survey specific to computer vision is missing. We specifically\nconsider one form of deep networks widely used in computer vision -\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\nand then examine the broad variations proposed over time to suit different\napplications. We hope that our recipe-style survey will serve as a guide,\nparticularly for novice practitioners intending to use deep-learning techniques\nfor computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:25:07 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Srinivas", "Suraj", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Mopuri", "Konda Reddy", ""], ["Prabhu", "Nikita", ""], ["Kruthiventi", "Srinivas S S", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1601.06719", "submitter": "Guiying Li", "authors": "Guiying Li, Junlong Liu, Chunhui Jiang, Liangpeng Zhang, Minlong Lin,\n  and Ke Tang", "title": "Relief R-CNN : Utilizing Convolutional Features for Fast Object\n  Detection", "comments": "9 pages, 2 figures, accepted by ISNN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R-CNN style methods are sorts of the state-of-the-art object detection\nmethods, which consist of region proposal generation and deep CNN\nclassification. However, the proposal generation phase in this paradigm is\nusually time consuming, which would slow down the whole detection time in\ntesting. This paper suggests that the value discrepancies among features in\ndeep convolutional feature maps contain plenty of useful spatial information,\nand proposes a simple approach to extract the information for fast region\nproposal generation in testing. The proposed method, namely Relief R-CNN\n(R2-CNN), adopts a novel region proposal generator in a trained R-CNN style\nmodel. The new generator directly generates proposals from convolutional\nfeatures by some simple rules, thus resulting in a much faster proposal\ngeneration speed and a lower demand of computation resources. Empirical studies\nshow that R2-CNN could achieve the fastest detection speed with comparable\naccuracy among all the compared algorithms in testing.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 18:53:14 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 13:10:55 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 11:52:59 GMT"}, {"version": "v4", "created": "Wed, 26 Apr 2017 07:12:14 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Li", "Guiying", ""], ["Liu", "Junlong", ""], ["Jiang", "Chunhui", ""], ["Zhang", "Liangpeng", ""], ["Lin", "Minlong", ""], ["Tang", "Ke", ""]]}, {"id": "1601.06759", "submitter": "A\\\"aron van den Oord", "authors": "Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu", "title": "Pixel Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:34:24 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 15:32:16 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 14:10:16 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Oord", "Aaron van den", ""], ["Kalchbrenner", "Nal", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1601.06823", "submitter": "Feng Wang", "authors": "Feng Wang, David M.J. Tax", "title": "Survey on the attention based RNN model and its applications in computer\n  vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recurrent neural networks (RNN) can be used to solve the sequence to\nsequence problem, where both the input and the output have sequential\nstructures. Usually there are some implicit relations between the structures.\nHowever, it is hard for the common RNN model to fully explore the relations\nbetween the sequences. In this survey, we introduce some attention based RNN\nmodels which can focus on different parts of the input for each output item, in\norder to explore and take advantage of the implicit relations between the input\nand the output items. The different attention mechanisms are described in\ndetail. We then introduce some applications in computer vision which apply the\nattention based RNN models. The superiority of the attention based RNN model is\nshown by the experimental results. At last some future research directions are\ngiven.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 21:54:02 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Wang", "Feng", ""], ["Tax", "David M. J.", ""]]}, {"id": "1601.06892", "submitter": "Kuldeep S Kulkarni Mr.", "authors": "Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, Amit\n  Ashok", "title": "ReconNet: Non-Iterative Reconstruction of Images from Compressively\n  Sensed Random Measurements", "comments": "Accepted at IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to present a non-iterative and more importantly an\nextremely fast algorithm to reconstruct images from compressively sensed (CS)\nrandom measurements. To this end, we propose a novel convolutional neural\nnetwork (CNN) architecture which takes in CS measurements of an image as input\nand outputs an intermediate reconstruction. We call this network, ReconNet. The\nintermediate reconstruction is fed into an off-the-shelf denoiser to obtain the\nfinal reconstructed image. On a standard dataset of images we show significant\nimprovements in reconstruction results (both in terms of PSNR and time\ncomplexity) over state-of-the-art iterative CS reconstruction algorithms at\nvarious measurement rates. Further, through qualitative experiments on real\ndata collected using our block single pixel camera (SPC), we show that our\nnetwork is highly robust to sensor noise and can recover visually better\nquality images than competitive algorithms at extremely low sensing rates of\n0.1 and 0.04. To demonstrate that our algorithm can recover semantically\ninformative images even at a low measurement rate of 0.01, we present a very\nrobust proof of concept real-time visual tracking application.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 05:17:14 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 23:31:08 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Kulkarni", "Kuldeep", ""], ["Lohit", "Suhas", ""], ["Turaga", "Pavan", ""], ["Kerviche", "Ronan", ""], ["Ashok", "Amit", ""]]}, {"id": "1601.06925", "submitter": "Alejandro Frery", "authors": "Osvaldo A. Rosso and Raydonal Ospina and Alejandro C. Frery", "title": "Classification and Verification of Online Handwritten Signatures with\n  Time Causal Information Theory Quantifiers", "comments": "Submitted to PLOS One", "journal-ref": null, "doi": "10.1371/journal.pone.0166868", "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for online handwritten signature classification and\nverification based on descriptors stemming from Information Theory. The\nproposal uses the Shannon Entropy, the Statistical Complexity, and the Fisher\nInformation evaluated over the Bandt and Pompe symbolization of the horizontal\nand vertical coordinates of signatures. These six features are easy and fast to\ncompute, and they are the input to an One-Class Support Vector Machine\nclassifier. The results produced surpass state-of-the-art techniques that\nemploy higher-dimensional feature spaces which often require specialized\nsoftware and hardware. We assess the consistency of our proposal with respect\nto the size of the training sample, and we also use it to classify the\nsignatures into meaningful groups.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 08:40:55 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Rosso", "Osvaldo A.", ""], ["Ospina", "Raydonal", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1601.06931", "submitter": "Manuel Marin-Jimenez", "authors": "F.M. Castro and M.J. Mar\\'in-Jim\\'enez and N. Guil and R.\n  Mu\\~noz-Salinas", "title": "Fisher Motion Descriptor for Multiview Gait Recognition", "comments": "This paper extends with new experiments the one published at\n  ICPR'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to identify individuals by analyzing their gait.\nInstead of using binary silhouettes as input data (as done in many previous\nworks) we propose and evaluate the use of motion descriptors based on densely\nsampled short-term trajectories. We take advantage of state-of-the-art people\ndetectors to define custom spatial configurations of the descriptors around the\ntarget person, obtaining a rich representation of the gait motion. The local\nmotion features (described by the Divergence-Curl-Shear descriptor) extracted\non the different spatial areas of the person are combined into a single\nhigh-level gait descriptor by using the Fisher Vector encoding. The proposed\napproach, coined Pyramidal Fisher Motion, is experimentally validated on\n`CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and the\nrecent `AVA Multiview Gait' dataset. The results show that this new approach\nachieves state-of-the-art results in the problem of gait recognition, allowing\nto recognize walking people from diverse viewpoints on single and multiple\ncamera setups, wearing different clothes, carrying bags, walking at diverse\nspeeds and not limited to straight walking paths.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 09:05:26 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Castro", "F. M.", ""], ["Mar\u00edn-Jim\u00e9nez", "M. J.", ""], ["Guil", "N.", ""], ["Mu\u00f1oz-Salinas", "R.", ""]]}, {"id": "1601.06950", "submitter": "Michael Waechter", "authors": "Michael Waechter, Mate Beljan, Simon Fuhrmann, Nils Moehrle, Johannes\n  Kopf, Michael Goesele", "title": "Virtual Rephotography: Novel View Prediction Error for 3D Reconstruction", "comments": "10 pages, 12 figures, paper was submitted to ACM Transactions on\n  Graphics for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of many image-based modeling systems is to render\nphoto-realistic novel views of a scene without visible artifacts. Existing\nevaluation metrics and benchmarks focus mainly on the geometric accuracy of the\nreconstructed model, which is, however, a poor predictor of visual accuracy.\nFurthermore, using only geometric accuracy by itself does not allow evaluating\nsystems that either lack a geometric scene representation or utilize coarse\nproxy geometry. Examples include light field or image-based rendering systems.\nWe propose a unified evaluation approach based on novel view prediction error\nthat is able to analyze the visual quality of any method that can render novel\nviews from input images. One of the key advantages of this approach is that it\ndoes not require ground truth geometry. This dramatically simplifies the\ncreation of test datasets and benchmarks. It also allows us to evaluate the\nquality of an unknown scene during the acquisition and reconstruction process,\nwhich is useful for acquisition planning. We evaluate our approach on a range\nof methods including standard geometry-plus-texture pipelines as well as\nimage-based rendering techniques, compare it to existing geometry-based\nbenchmarks, and demonstrate its utility for a range of use cases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 09:57:34 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Waechter", "Michael", ""], ["Beljan", "Mate", ""], ["Fuhrmann", "Simon", ""], ["Moehrle", "Nils", ""], ["Kopf", "Johannes", ""], ["Goesele", "Michael", ""]]}, {"id": "1601.07014", "submitter": "Fausto Milletari", "authors": "Fausto Milletari, Seyed-Ahmad Ahmadi, Christine Kroll, Annika Plate,\n  Verena Rozanski, Juliana Maiostre, Johannes Levin, Olaf Dietrich, Birgit\n  Ertl-Wagner, Kai B\\\"otzel, Nassir Navab", "title": "Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI\n  and Ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel approach to perform segmentation by\nleveraging the abstraction capabilities of convolutional neural networks\n(CNNs). Our method is based on Hough voting, a strategy that allows for fully\nautomatic localisation and segmentation of the anatomies of interest. This\napproach does not only use the CNN classification outcomes, but it also\nimplements voting by exploiting the features produced by the deepest portion of\nthe network. We show that this learning-based segmentation method is robust,\nmulti-region, flexible and can be easily adapted to different modalities. In\nthe attempt to show the capabilities and the behaviour of CNNs when they are\napplied to medical image analysis, we perform a systematic study of the\nperformances of six different network architectures, conceived according to\nstate-of-the-art criteria, in various situations. We evaluate the impact of\nboth different amount of training data and different data dimensionality (2D,\n2.5D and 3D) on the final results. We show results on both MRI and transcranial\nUS volumes depicting respectively 26 regions of the basal ganglia and the\nmidbrain.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 13:25:01 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 14:00:36 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2016 19:35:15 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Milletari", "Fausto", ""], ["Ahmadi", "Seyed-Ahmad", ""], ["Kroll", "Christine", ""], ["Plate", "Annika", ""], ["Rozanski", "Verena", ""], ["Maiostre", "Juliana", ""], ["Levin", "Johannes", ""], ["Dietrich", "Olaf", ""], ["Ertl-Wagner", "Birgit", ""], ["B\u00f6tzel", "Kai", ""], ["Navab", "Nassir", ""]]}, {"id": "1601.07021", "submitter": "Qingxiang Feng", "authors": "Qingxiang Feng, Jeng-Shyang Pan, Jar-Ferr Yang, and Yang-Ting Chou", "title": "Polyhedron Volume-Ratio-based Classification for Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method, called polyhedron volume ratio classification\n(PVRC) is proposed for image recognition\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 13:41:48 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Feng", "Qingxiang", ""], ["Pan", "Jeng-Shyang", ""], ["Yang", "Jar-Ferr", ""], ["Chou", "Yang-Ting", ""]]}, {"id": "1601.07140", "submitter": "Andreas Veit", "authors": "Andreas Veit and Tomas Matera and Lukas Neumann and Jiri Matas and\n  Serge Belongie", "title": "COCO-Text: Dataset and Benchmark for Text Detection and Recognition in\n  Natural Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the COCO-Text dataset. In recent years large-scale\ndatasets like SUN and Imagenet drove the advancement of scene understanding and\nobject recognition. The goal of COCO-Text is to advance state-of-the-art in\ntext detection and recognition in natural images. The dataset is based on the\nMS COCO dataset, which contains images of complex everyday scenes. The images\nwere not collected with text in mind and thus contain a broad variety of text\ninstances. To reflect the diversity of text in natural scenes, we annotate text\nwith (a) location in terms of a bounding box, (b) fine-grained classification\ninto machine printed text and handwritten text, (c) classification into legible\nand illegible text, (d) script of the text and (e) transcriptions of legible\ntext. The dataset contains over 173k text annotations in over 63k images. We\nprovide a statistical analysis of the accuracy of our annotations. In addition,\nwe present an analysis of three leading state-of-the-art photo Optical\nCharacter Recognition (OCR) approaches on our dataset. While scene text\ndetection and recognition enjoys strong advances in recent years, we identify\nsignificant shortcomings motivating future work.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 19:30:34 GMT"}, {"version": "v2", "created": "Sun, 19 Jun 2016 23:52:14 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Veit", "Andreas", ""], ["Matera", "Tomas", ""], ["Neumann", "Lukas", ""], ["Matas", "Jiri", ""], ["Belongie", "Serge", ""]]}, {"id": "1601.07252", "submitter": "Anshul Gupta", "authors": "Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard\n  Furuta, Laura Mandell", "title": "Font Identification in Historical Documents Using Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DL stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Identifying the type of font (e.g., Roman, Blackletter) used in historical\ndocuments can help optical character recognition (OCR) systems produce more\naccurate text transcriptions. Towards this end, we present an active-learning\nstrategy that can significantly reduce the number of labeled samples needed to\ntrain a font classifier. Our approach extracts image-based features that\nexploit geometric differences between fonts at the word level, and combines\nthem into a bag-of-word representation for each page in a document. We evaluate\nsix sampling strategies based on uncertainty, dissimilarity and diversity\ncriteria, and test them on a database containing over 3,000 historical\ndocuments with Blackletter, Roman and Mixed fonts. Our results show that a\ncombination of uncertainty and diversity achieves the highest predictive\naccuracy (89% of test cases correctly classified) while requiring only a small\nfraction of the data (17%) to be labeled. We discuss the implications of this\nresult for mass digitization projects of historical documents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 03:24:05 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Gupta", "Anshul", ""], ["Gutierrez-Osuna", "Ricardo", ""], ["Christy", "Matthew", ""], ["Furuta", "Richard", ""], ["Mandell", "Laura", ""]]}, {"id": "1601.07255", "submitter": "Chunhua Shen", "authors": "Lin Wu, Chunhua Shen, Anton van den Hengel", "title": "PersonNet: Person Re-identification with Deep Convolutional Neural\n  Networks", "comments": "7 pages. Fixed Figure 4 (a)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep end-to-end neu- ral network to\nsimultaneously learn high-level features and a corresponding similarity metric\nfor person re-identification. The network takes a pair of raw RGB images as\ninput, and outputs a similarity value indicating whether the two input images\ndepict the same person. A layer of computing neighborhood range differences\nacross two input images is employed to capture local relationship between\npatches. This operation is to seek a robust feature from input images. By\nincreasing the depth to 10 weight layers and using very small (3$\\times$3)\nconvolution filters, our architecture achieves a remarkable improvement on the\nprior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp)\ngradient decent algorithm is integrated into our architecture, which is\nbeneficial to deep nets. Our method consistently outperforms state-of-the-art\non two large datasets (CUHK03 and Market-1501), and a medium-sized data set\n(CUHK01).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 03:49:34 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 06:43:54 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wu", "Lin", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1601.07258", "submitter": "Kuldeep S Kulkarni Mr.", "authors": "Kuldeep Kulkarni and Pavan Turaga", "title": "Fast Integral Image Estimation at 1% measurement rate", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework called ReFInE to directly obtain integral image\nestimates from a very small number of spatially multiplexed measurements of the\nscene without iterative reconstruction of any auxiliary image, and demonstrate\ntheir practical utility in visual object tracking. Specifically, we design\nmeasurement matrices which are tailored to facilitate extremely fast estimation\nof the integral image, by using a single-shot linear operation on the measured\nvector. Leveraging a prior model for the images, we formulate a nuclear norm\nminimization problem with second order conic constraints to jointly obtain the\nmeasurement matrix and the linear operator. Through qualitative and\nquantitative experiments, we show that high quality integral image estimates\ncan be obtained using our framework at very low measurement rates. Further, on\na standard dataset of 50 videos, we present object tracking results which are\ncomparable to the state-of-the-art methods, even at an extremely low\nmeasurement rate of 1%.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 04:32:20 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Kulkarni", "Kuldeep", ""], ["Turaga", "Pavan", ""]]}, {"id": "1601.07265", "submitter": "Xi Li", "authors": "Siyu Huang, Xi Li, Zhongfei Zhang, Zhouzhou He, Fei Wu, Wei Liu,\n  Jinhui Tang, and Yueting Zhuang", "title": "Deep Learning Driven Visual Path Prediction from a Single Image", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 25, no. 12, pp.\n  5892-5904, Dec. 2016", "doi": "10.1109/TIP.2016.2613686", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capabilities of inference and prediction are significant components of visual\nsystems. In this paper, we address an important and challenging task of them:\nvisual path prediction. Its goal is to infer the future path for a visual\nobject in a static scene. This task is complicated as it needs high-level\nsemantic understandings of both the scenes and motion patterns underlying video\nsequences. In practice, cluttered situations have also raised higher demands on\nthe effectiveness and robustness of the considered models. Motivated by these\nobservations, we propose a deep learning framework which simultaneously\nperforms deep feature learning for visual representation in conjunction with\nspatio-temporal context modeling. After that, we propose a unified path\nplanning scheme to make accurate future path prediction based on the analytic\nresults of the context models. The highly effective visual representation and\ndeep context models ensure that our framework makes a deep semantic\nunderstanding of the scene and motion pattern, consequently improving the\nperformance of the visual path prediction task. In order to comprehensively\nevaluate the model's performance on the visual path prediction task, we\nconstruct two large benchmark datasets from the adaptation of video tracking\ndatasets. The qualitative and quantitative experimental results show that our\napproach outperforms the existing approaches and owns a better generalization\ncapability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 05:04:31 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Huang", "Siyu", ""], ["Li", "Xi", ""], ["Zhang", "Zhongfei", ""], ["He", "Zhouzhou", ""], ["Wu", "Fei", ""], ["Liu", "Wei", ""], ["Tang", "Jinhui", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1601.07270", "submitter": "Xiushan Nie", "authors": "Xiushan Nie, Yilong Yin, Jiande Sun", "title": "Comprehensive Feature-based Robust Video Fingerprinting Using Tensor\n  Model", "comments": "13pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based near-duplicate video detection (NDVD) is essential for\neffective search and retrieval, and robust video fingerprinting is a good\nsolution for NDVD. Most existing video fingerprinting methods use a single\nfeature or concatenating different features to generate video fingerprints, and\nshow a good performance under single-mode modifications such as noise addition\nand blurring. However, when they suffer combined modifications, the performance\nis degraded to a certain extent because such features cannot characterize the\nvideo content completely. By contrast, the assistance and consensus among\ndifferent features can improve the performance of video fingerprinting.\nTherefore, in the present study, we mine the assistance and consensus among\ndifferent features based on tensor model, and present a new comprehensive\nfeature to fully use them in the proposed video fingerprinting framework. We\nalso analyze what the comprehensive feature really is for representing the\noriginal video. In this framework, the video is initially set as a high-order\ntensor that consists of different features, and the video tensor is decomposed\nvia the Tucker model with a solution that determines the number of components.\nSubsequently, the comprehensive feature is generated by the low-order tensor\nobtained from tensor decomposition. Finally, the video fingerprint is computed\nusing this feature. A matching strategy used for narrowing the search is also\nproposed based on the core tensor. The robust video fingerprinting framework is\nresistant not only to single-mode modifications, but also to the combination of\nthem.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 06:02:59 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Nie", "Xiushan", ""], ["Yin", "Yilong", ""], ["Sun", "Jiande", ""]]}, {"id": "1601.07336", "submitter": "Ming Yin", "authors": "Ming Yin, Shengli Xie, Yi Guo, Junbin Gao and Yun Zhang", "title": "Neighborhood Preserved Sparse Representation for Robust Classification\n  on Symmetric Positive Definite Matrices", "comments": "arXiv admin note: text overlap with arXiv:1601.00414", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its promising classification performance, sparse representation based\nclassification(SRC) algorithm has attracted great attention in the past few\nyears. However, the existing SRC type methods apply only to vector data in\nEuclidean space. As such, there is still no satisfactory approach to conduct\nclassification task for symmetric positive definite (SPD) matrices which is\nvery useful in computer vision. To address this problem, in this paper, a\nneighborhood preserved kernel SRC method is proposed on SPD manifolds.\nSpecifically, by embedding the SPD matrices into a Reproducing Kernel Hilbert\nSpace (RKHS), the proposed method can perform classification on SPD manifolds\nthrough an appropriate Log-Euclidean kernel. Through exploiting the geodesic\ndistance between SPD matrices, our method can effectively characterize the\nintrinsic local Riemannian geometry within data so as to well unravel the\nunderlying sub-manifold structure. Despite its simplicity, experimental results\non several famous database demonstrate that the proposed method achieves better\nclassification results than the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 12:05:22 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Yin", "Ming", ""], ["Xie", "Shengli", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""], ["Zhang", "Yun", ""]]}, {"id": "1601.07471", "submitter": "Vinay Venkataraman", "authors": "Vinay Venkataraman, Pavan Turaga", "title": "Shape Distributions of Nonlinear Dynamical Systems for Video-based\n  Inference", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2533388", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a shape-theoretic framework for dynamical analysis of\nnonlinear dynamical systems which appear frequently in several video-based\ninference tasks. Traditional approaches to dynamical modeling have included\nlinear and nonlinear methods with their respective drawbacks. A novel approach\nwe propose is the use of descriptors of the shape of the dynamical attractor as\na feature representation of nature of dynamics. The proposed framework has two\nmain advantages over traditional approaches: a) representation of the dynamical\nsystem is derived directly from the observational data, without any inherent\nassumptions, and b) the proposed features show stability under different\ntime-series lengths where traditional dynamical invariants fail. We illustrate\nour idea using nonlinear dynamical models such as Lorenz and Rossler systems,\nwhere our feature representations (shape distribution) support our hypothesis\nthat the local shape of the reconstructed phase space can be used as a\ndiscriminative feature. Our experimental analyses on these models also indicate\nthat the proposed framework show stability for different time-series lengths,\nwhich is useful when the available number of samples are small/variable. The\nspecific applications of interest in this paper are: 1) activity recognition\nusing motion capture and RGBD sensors, 2) activity quality assessment for\napplications in stroke rehabilitation, and 3) dynamical scene classification.\nWe provide experimental validation through action and gesture recognition\nexperiments on motion capture and Kinect datasets. In all these scenarios, we\nshow experimental evidence of the favorable properties of the proposed\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 18:01:38 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Venkataraman", "Vinay", ""], ["Turaga", "Pavan", ""]]}, {"id": "1601.07532", "submitter": "Damien Teney", "authors": "Damien Teney, Martial Hebert", "title": "Learning to Extract Motion from Videos in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to extract dense optical flow from videos with a\nconvolutional neural network (CNN). The proposed model constitutes a potential\nbuilding block for deeper architectures to allow using motion without resorting\nto an external algorithm, \\eg for recognition in videos. We derive our network\narchitecture from signal processing principles to provide desired invariances\nto image contrast, phase and texture. We constrain weights within the network\nto enforce strict rotation invariance and substantially reduce the number of\nparameters to learn. We demonstrate end-to-end training on only 8 sequences of\nthe Middlebury dataset, orders of magnitude less than competing CNN-based\nmotion estimation methods, and obtain comparable performance to classical\nmethods on the Middlebury benchmark. Importantly, our method outputs a\ndistributed representation of motion that allows representing multiple,\ntransparent motions, and dynamic textures. Our contributions on network design\nand rotation invariance offer insights nonspecific to motion estimation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 20:19:14 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Teney", "Damien", ""], ["Hebert", "Martial", ""]]}, {"id": "1601.07533", "submitter": "Yinong Wang", "authors": "Yinong Wang, Jianhua Yao, Joseph E. Burns, Ronald M. Summers", "title": "Osteoporotic and Neoplastic Compression Fracture Classification on\n  Longitudinal CT", "comments": "Contributed 4-Page Paper to be presented at the 2016 IEEE\n  International Symposium on Biomedical Imaging (ISBI), April 13-16, 2016,\n  Prague, Czech Republic", "journal-ref": null, "doi": "10.1109/ISBI.2016.7493477", "report-no": null, "categories": "cs.CV q-bio.TO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Classification of vertebral compression fractures (VCF) having osteoporotic\nor neoplastic origin is fundamental to the planning of treatment. We developed\na fracture classification system by acquiring quantitative morphologic and bone\ndensity determinants of fracture progression through the use of automated\nmeasurements from longitudinal studies. A total of 250 CT studies were acquired\nfor the task, each having previously identified VCFs with osteoporosis or\nneoplasm. Thirty-six features or each identified VCF were computed and\nclassified using a committee of support vector machines. Ten-fold cross\nvalidation on 695 identified fractured vertebrae showed classification\naccuracies of 0.812, 0.665, and 0.820 for the measured, longitudinal, and\ncombined feature sets respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 20:20:05 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wang", "Yinong", ""], ["Yao", "Jianhua", ""], ["Burns", "Joseph E.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1601.07576", "submitter": "Weilin Huang", "authors": "Sheng Guo, Weilin Huang, Limin Wang, Yu Qiao", "title": "Locally-Supervised Deep Hybrid Model for Scene Recognition", "comments": "To appear in IEEE Trans. on Image Processing, 2017", "journal-ref": null, "doi": "10.1109/TIP.2016.2629443", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have recently achieved remarkable\nsuccesses in various image classification and understanding tasks. The deep\nfeatures obtained at the top fully-connected layer of the CNN (FC-features)\nexhibit rich global semantic information and are extremely effective in image\nclassification. On the other hand, the convolutional features in the middle\nlayers of the CNN also contain meaningful local information, but are not fully\nexplored for image representation. In this paper, we propose a novel\nLocally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and\nexplores the convolutional features for scene recognition. Firstly, we notice\nthat the convolutional features capture local objects and fine structures of\nscene images, which yield important cues for discriminating ambiguous scenes,\nwhereas these features are significantly eliminated in the highly-compressed FC\nrepresentation. Secondly, we propose a new Local Convolutional Supervision\n(LCS) layer to enhance the local structure of the image by directly propagating\nthe label information to the convolutional layers. Thirdly, we propose an\nefficient Fisher Convolutional Vector (FCV) that successfully rescues the\norderless mid-level semantic information (e.g. objects and textures) of scene\nimage. The FCV encodes the large-sized convolutional maps into a fixed-length\nmid-level representation, and is demonstrated to be strongly complementary to\nthe high-level FC-features. Finally, both the FCV and FC-features are\ncollaboratively employed in the LSDHM representation, which achieves\noutstanding performance in our experiments. It obtains 83.75% and 67.56%\naccuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397\ndatasets, advancing the stat-of-the-art substantially.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 21:32:15 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 21:30:09 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Wang", "Limin", ""], ["Qiao", "Yu", ""]]}, {"id": "1601.07630", "submitter": "Jiangye Yuan", "authors": "Jiangye Yuan and Anil M. Cheriyadat", "title": "Combining Maps and Street Level Images for Building Height and Facade\n  Estimation", "comments": "UrbanGIS '16 Proceedings of the 2nd ACM SIGSPATIAL Workshop on Smart\n  Cities and Urban Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that integrates two widely available data sources,\nbuilding footprints from 2D maps and street level images, to derive valuable\ninformation that is generally difficult to acquire -- building heights and\nbuilding facade masks in images. Building footprints are elevated in world\ncoordinates and projected onto images. Building heights are estimated by\nscoring projected footprints based on their alignment with building features in\nimages. Building footprints with estimated heights can be converted to simple\n3D building models, which are projected back to images to identify buildings.\nIn this procedure, accurate camera projections are critical. However, camera\nposition errors inherited from external sensors commonly exist, which adversely\naffect results. We derive a solution to precisely locate cameras on maps using\ncorrespondence between image features and building footprints. Experiments on\nreal-world datasets show the promise of our method.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 02:58:04 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 18:47:44 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Yuan", "Jiangye", ""], ["Cheriyadat", "Anil M.", ""]]}, {"id": "1601.07648", "submitter": "Mark Moyou", "authors": "Mark Moyou, John Corring, Adrian Peter, Anand Rangarajan", "title": "A Grassmannian Graph Approach to Affine Invariant Feature Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel and practical approach to address one of the\nlongstanding problems in computer vision: 2D and 3D affine invariant feature\nmatching. Our Grassmannian Graph (GrassGraph) framework employs a two stage\nprocedure that is capable of robustly recovering correspondences between two\nunorganized, affinely related feature (point) sets. The first stage maps the\nfeature sets to an affine invariant Grassmannian representation, where the\nfeatures are mapped into the same subspace. It turns out that coordinate\nrepresentations extracted from the Grassmannian differ by an arbitrary\northonormal matrix. In the second stage, by approximating the Laplace-Beltrami\noperator (LBO) on these coordinates, this extra orthonormal factor is\nnullified, providing true affine-invariant coordinates which we then utilize to\nrecover correspondences via simple nearest neighbor relations. The resulting\nGrassGraph algorithm is empirically shown to work well in non-ideal scenarios\nwith noise, outliers, and occlusions. Our validation benchmarks use an\nunprecedented 440,000+ experimental trials performed on 2D and 3D datasets,\nwith a variety of parameter settings and competing methods. State-of-the-art\nperformance in the majority of these extensive evaluations confirm the utility\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 05:17:17 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 05:18:52 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Moyou", "Mark", ""], ["Corring", "John", ""], ["Peter", "Adrian", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1601.07649", "submitter": "Chunhua Shen", "authors": "Fayao Liu, Guosheng Lin, Chunhua Shen", "title": "Discriminative Training of Deep Fully-connected Continuous CRF with\n  Task-specific Loss", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2675166", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on deep conditional random fields (CRF) have set new records on\nmany vision tasks involving structured predictions. Here we propose a\nfully-connected deep continuous CRF model for both discrete and continuous\nlabelling problems. We exemplify the usefulness of the proposed model on\nmulti-class semantic labelling (discrete) and the robust depth estimation\n(continuous) problems.\n  In our framework, we model both the unary and the pairwise potential\nfunctions as deep convolutional neural networks (CNN), which are jointly\nlearned in an end-to-end fashion. The proposed method possesses the main\nadvantage of continuously-valued CRF, which is a closed-form solution for the\nMaximum a posteriori (MAP) inference.\n  To better adapt to different tasks, instead of using the commonly employed\nmaximum likelihood CRF parameter learning protocol, we propose task-specific\nloss functions for learning the CRF parameters.\n  It enables direct optimization of the quality of the MAP estimates during the\ncourse of learning.\n  Specifically, we optimize the multi-class classification loss for the\nsemantic labelling task and the Turkey's biweight loss for the robust depth\nestimation problem.\n  Experimental results on the semantic labelling and robust depth estimation\ntasks demonstrate that the proposed method compare favorably against both\nbaseline and state-of-the-art methods.\n  In particular, we show that although the proposed deep CRF model is\ncontinuously valued, with the equipment of task-specific loss, it achieves\nimpressive results even on discrete labelling tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 05:30:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Liu", "Fayao", ""], ["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1601.07661", "submitter": "Bolun Cai", "authors": "Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing and Dacheng Tao", "title": "DehazeNet: An End-to-End System for Single Image Haze Removal", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2598681", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image haze removal is a challenging ill-posed problem. Existing\nmethods use various constraints/priors to get plausible dehazing solutions. The\nkey to achieve haze removal is to estimate a medium transmission map for an\ninput hazy image. In this paper, we propose a trainable end-to-end system\ncalled DehazeNet, for medium transmission estimation. DehazeNet takes a hazy\nimage as input, and outputs its medium transmission map that is subsequently\nused to recover a haze-free image via atmospheric scattering model. DehazeNet\nadopts Convolutional Neural Networks (CNN) based deep architecture, whose\nlayers are specially designed to embody the established assumptions/priors in\nimage dehazing. Specifically, layers of Maxout units are used for feature\nextraction, which can generate almost all haze-relevant features. We also\npropose a novel nonlinear activation function in DehazeNet, called Bilateral\nRectified Linear Unit (BReLU), which is able to improve the quality of\nrecovered haze-free image. We establish connections between components of the\nproposed DehazeNet and those used in existing methods. Experiments on benchmark\nimages show that DehazeNet achieves superior performance over existing methods,\nyet keeps efficient and easy to use.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 06:32:22 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 08:03:18 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Cai", "Bolun", ""], ["Xu", "Xiangmin", ""], ["Jia", "Kui", ""], ["Qing", "Chunmei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1601.07843", "submitter": "Nabin Mishra", "authors": "Nabin K. Mishra and M. Emre Celebi", "title": "An Overview of Melanoma Detection in Dermoscopy Images Using Image\n  Processing and Machine Learning", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incidence of malignant melanoma continues to increase worldwide. This\ncancer can strike at any age; it is one of the leading causes of loss of life\nin young persons. Since this cancer is visible on the skin, it is potentially\ndetectable at a very early stage when it is curable. New developments have\nconverged to make fully automatic early melanoma detection a real possibility.\nFirst, the advent of dermoscopy has enabled a dramatic boost in clinical\ndiagnostic ability to the point that melanoma can be detected in the clinic at\nthe very earliest stages. The global adoption of this technology has allowed\naccumulation of large collections of dermoscopy images of melanomas and benign\nlesions validated by histopathology. The development of advanced technologies\nin the areas of image processing and machine learning have given us the ability\nto allow distinction of malignant melanoma from the many benign mimics that\nrequire no biopsy. These new technologies should allow not only earlier\ndetection of melanoma, but also reduction of the large number of needless and\ncostly biopsy procedures. Although some of the new systems reported for these\ntechnologies have shown promise in preliminary trials, widespread\nimplementation must await further technical progress in accuracy and\nreproducibility. In this paper, we provide an overview of computerized\ndetection of melanoma in dermoscopy images. First, we discuss the various\naspects of lesion segmentation. Then, we provide a brief overview of clinical\nfeature segmentation. Finally, we discuss the classification stage where\nmachine learning algorithms are applied to the attributes generated from the\nsegmented features to predict the existence of melanoma.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 17:33:48 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Mishra", "Nabin K.", ""], ["Celebi", "M. Emre", ""]]}, {"id": "1601.07883", "submitter": "Jun-Cheng Chen", "authors": "Rama Chellappa, Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan,\n  Amit Kumar, Vishal M. Patel, Carlos D. Castillo", "title": "Towards the Design of an End-to-End Automated System for Image and\n  Video-based Recognition", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over many decades, researchers working in object recognition have longed for\nan end-to-end automated system that will simply accept 2D or 3D image or videos\nas inputs and output the labels of objects in the input data. Computer vision\nmethods that use representations derived based on geometric, radiometric and\nneural considerations and statistical and structural matchers and artificial\nneural network-based methods where a multi-layer network learns the mapping\nfrom inputs to class labels have provided competing approaches for image\nrecognition problems. Over the last four years, methods based on Deep\nConvolutional Neural Networks (DCNNs) have shown impressive performance\nimprovements on object detection/recognition challenge problems. This has been\nmade possible due to the availability of large annotated data, a better\nunderstanding of the non-linear mapping between image and class labels as well\nas the affordability of GPUs. In this paper, we present a brief history of\ndevelopments in computer vision and artificial neural networks over the last\nforty years for the problem of image-based recognition. We then present the\ndesign details of a deep learning system for end-to-end unconstrained face\nverification/recognition. Some open issues regarding DCNNs for object\nrecognition problems are then discussed. We caution the readers that the views\nexpressed in this paper are from the authors and authors only!\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 20:06:16 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Chellappa", "Rama", ""], ["Chen", "Jun-Cheng", ""], ["Ranjan", "Rajeev", ""], ["Sankaranarayanan", "Swami", ""], ["Kumar", "Amit", ""], ["Patel", "Vishal M.", ""], ["Castillo", "Carlos D.", ""]]}, {"id": "1601.07884", "submitter": "Xinchao Li", "authors": "Xinchao Li, Martha A. Larson, Alan Hanjalic", "title": "Geo-distinctive Visual Element Matching for Location Estimation of\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an image representation and matching approach that substantially\nimproves visual-based location estimation for images. The main novelty of the\napproach, called distinctive visual element matching (DVEM), is its use of\nrepresentations that are specific to the query image whose location is being\npredicted. These representations are based on visual element clouds, which\nrobustly capture the connection between the query and visual evidence from\ncandidate locations. We then maximize the influence of visual elements that are\ngeo-distinctive because they do not occur in images taken at many other\nlocations. We carry out experiments and analysis for both geo-constrained and\ngeo-unconstrained location estimation cases using two large-scale,\npublicly-available datasets: the San Francisco Landmark dataset with $1.06$\nmillion street-view images and the MediaEval '15 Placing Task dataset with\n$5.6$ million geo-tagged images from Flickr. We present examples that\nillustrate the highly-transparent mechanics of the approach, which are based on\ncommon sense observations about the visual patterns in image collections. Our\nresults show that the proposed method delivers a considerable performance\nimprovement compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 20:13:01 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Li", "Xinchao", ""], ["Larson", "Martha A.", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1601.07950", "submitter": "Amit Kumar", "authors": "Amit Kumar, Rajeev Ranjan, Vishal Patel, Rama Chellappa", "title": "Face Alignment by Local Deep Descriptor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for extracting key-point descriptors using deep\nconvolutional neural networks (CNN). Unlike many existing deep CNNs, our model\ncomputes local features around a given point in an image. We also present a\nface alignment algorithm based on regression using these local descriptors. The\nproposed method called Local Deep Descriptor Regression (LDDR) is able to\nlocalize face landmarks of varying sizes, poses and occlusions with high\naccuracy. Deep Descriptors presented in this paper are able to uniquely and\nefficiently describe every pixel in the image and therefore can potentially\nreplace traditional descriptors such as SIFT and HOG. Extensive evaluations on\nfive publicly available unconstrained face alignment datasets show that our\ndeep descriptor network is able to capture strong local features around a given\nlandmark and performs significantly better than many competitive and\nstate-of-the-art face alignment algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 00:00:16 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Kumar", "Amit", ""], ["Ranjan", "Rajeev", ""], ["Patel", "Vishal", ""], ["Chellappa", "Rama", ""]]}, {"id": "1601.07977", "submitter": "Guo-Sen Xie", "authors": "Guo-Sen Xie, Xu-Yao Zhang, Shuicheng Yan and Cheng-Lin Liu", "title": "Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain\n  Adaptation", "comments": "Accepted by TCSVT on Sep.2015", "journal-ref": null, "doi": "10.1109/TCSVT.2015.2511543", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) has achieved state-of-the-art performance\nin many different visual tasks. Learned from a large-scale training dataset,\nCNN features are much more discriminative and accurate than the hand-crafted\nfeatures. Moreover, CNN features are also transferable among different domains.\nOn the other hand, traditional dictionarybased features (such as BoW and SPM)\ncontain much more local discriminative and structural information, which is\nimplicitly embedded in the images. To further improve the performance, in this\npaper, we propose to combine CNN with dictionarybased models for scene\nrecognition and visual domain adaptation. Specifically, based on the well-tuned\nCNN models (e.g., AlexNet and VGG Net), two dictionary-based representations\nare further constructed, namely mid-level local representation (MLR) and\nconvolutional Fisher vector representation (CFV). In MLR, an efficient\ntwo-stage clustering method, i.e., weighted spatial and feature space spectral\nclustering on the parts of a single image followed by clustering all\nrepresentative parts of all images, is used to generate a class-mixture or a\nclassspecific part dictionary. After that, the part dictionary is used to\noperate with the multi-scale image inputs for generating midlevel\nrepresentation. In CFV, a multi-scale and scale-proportional GMM training\nstrategy is utilized to generate Fisher vectors based on the last convolutional\nlayer of CNN. By integrating the complementary information of MLR, CFV and the\nCNN features of the fully connected layer, the state-of-the-art performance can\nbe achieved on scene recognition and domain adaptation problems. An interested\nfinding is that our proposed hybrid representation (from VGG net trained on\nImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained on\nPlace205) greatly.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 05:32:52 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Xie", "Guo-Sen", ""], ["Zhang", "Xu-Yao", ""], ["Yan", "Shuicheng", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1601.08003", "submitter": "Michael Felsberg", "authors": "Erik Jonsson and Michael Felsberg", "title": "Efficient Robust Mean Value Calculation of 1D Features", "comments": "Presented at the SSBA Symposium 2005, Malm\\\"o, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust mean value is often a good alternative to the standard mean value\nwhen dealing with data containing many outliers. An efficient method for\nsamples of one-dimensional features and the truncated quadratic error norm is\npresented and compared to the method of channel averaging (soft histograms).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 08:54:22 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Jonsson", "Erik", ""], ["Felsberg", "Michael", ""]]}, {"id": "1601.08165", "submitter": "Emanuele Olivetti", "authors": "Thien Bao Nguyen, Emanuele Olivetti, Paolo Avesani", "title": "Mapping Tractography Across Subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) and tractography provide means to\nstudy the anatomical structures within the white matter of the brain. When\nstudying tractography data across subjects, it is usually necessary to align,\ni.e. to register, tractographies together. This registration step is most often\nperformed by applying the transformation resulting from the registration of\nother volumetric images (T1, FA). In contrast with registration methods that\n\"transform\" tractographies, in this work, we try to find which streamline in\none tractography correspond to which streamline in the other tractography,\nwithout any transformation. In other words, we try to find a \"mapping\" between\nthe tractographies. We propose a graph-based solution for the tractography\nmapping problem and we explain similarities and differences with the related\nwell-known graph matching problem. Specifically, we define a loss function\nbased on the pairwise streamline distance and reformulate the mapping problem\nas combinatorial optimization of that loss function. We show preliminary\npromising results where we compare the proposed method, implemented with\nsimulated annealing, against a standard registration techniques in a task of\nsegmentation of the corticospinal tract.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 15:50:20 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Nguyen", "Thien Bao", ""], ["Olivetti", "Emanuele", ""], ["Avesani", "Paolo", ""]]}, {"id": "1601.08188", "submitter": "Michael Wand", "authors": "Michael Wand and Jan Koutn\\'ik and J\\\"urgen Schmidhuber", "title": "Lipreading with Long Short-Term Memory", "comments": "Accepted for publication at ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipreading, i.e. speech recognition from visual-only recordings of a\nspeaker's face, can be achieved with a processing pipeline based solely on\nneural networks, yielding significantly better accuracy than conventional\nmethods. Feed-forward and recurrent neural network layers (namely Long\nShort-Term Memory; LSTM) are stacked to form a single structure which is\ntrained by back-propagating error gradients through all the layers. The\nperformance of such a stacked network was experimentally evaluated and compared\nto a standard Support Vector Machine classifier using conventional computer\nvision features (Eigenlips and Histograms of Oriented Gradients). The\nevaluation was performed on data from 19 speakers of the publicly available\nGRID corpus. With 51 different words to classify, we report a best word\naccuracy on held-out evaluation speakers of 79.6% using the end-to-end neural\nnetwork-based solution (11.6% improvement over the best feature-based solution\nevaluated).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:48:07 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Wand", "Michael", ""], ["Koutn\u00edk", "Jan", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1601.08201", "submitter": "Ikenna Odinaka", "authors": "Ikenna Odinaka, Yan Kaganovsky, Joel A. Greenberg, Mehadi Hassan,\n  David G. Politte, Joseph A. O'Sullivan, Lawrence Carin, David J. Brady", "title": "Spectrally Grouped Total Variation Reconstruction for Scatter Imaging\n  Using ADMM", "comments": "Presented at IEEE Nuclear Science Symposium and Medical Imaging\n  Conference (NSS/MIC) 2015. 4 pages, 2 figures", "journal-ref": null, "doi": "10.1109/NSSMIC.2015.7582220", "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider X-ray coherent scatter imaging, where the goal is to reconstruct\nmomentum transfer profiles (spectral distributions) at each spatial location\nfrom multiplexed measurements of scatter. Each material is characterized by a\nunique momentum transfer profile (MTP) which can be used to discriminate\nbetween different materials. We propose an iterative image reconstruction\nalgorithm based on a Poisson noise model that can account for photon-limited\nmeasurements as well as various second order statistics of the data. To improve\nimage quality, previous approaches use edge-preserving regularizers to promote\npiecewise constancy of the image in the spatial domain while treating each\nspectral bin separately. Instead, we propose spectrally grouped regularization\nthat promotes piecewise constant images along the spatial directions but also\nensures that the MTPs of neighboring spatial bins are similar, if they contain\nthe same material. We demonstrate that this group regularization results in\nimprovement of both spectral and spatial image quality. We pursue an\noptimization transfer approach where convex decompositions are used to lift the\nproblem such that all hyper-voxels can be updated in parallel and in\nclosed-form. The group penalty introduces a challenge since it is not directly\namendable to these decompositions. We use the alternating directions method of\nmultipliers (ADMM) to replace the original problem with an equivalent sequence\nof sub-problems that are amendable to convex decompositions, leading to a\nhighly parallel algorithm. We demonstrate the performance on real data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 17:27:37 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Odinaka", "Ikenna", ""], ["Kaganovsky", "Yan", ""], ["Greenberg", "Joel A.", ""], ["Hassan", "Mehadi", ""], ["Politte", "David G.", ""], ["O'Sullivan", "Joseph A.", ""], ["Carin", "Lawrence", ""], ["Brady", "David J.", ""]]}]