[{"id": "1805.00063", "submitter": "Igor Melnyk", "authors": "Pierre L. Dognin, Igor Melnyk, Youssef Mroueh, Jarret Ross, and Tom\n  Sercu (IBM Research, USA)", "title": "Adversarial Semantic Alignment for Improved Image Captions", "comments": "Authors Equal Contribution, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study image captioning as a conditional GAN training,\nproposing both a context-aware LSTM captioner and co-attentive discriminator,\nwhich enforces semantic alignment between images and captions. We empirically\nfocus on the viability of two training methods: Self-critical Sequence Training\n(SCST) and Gumbel Straight-Through (ST) and demonstrate that SCST shows more\nstable gradient behavior and improved results over Gumbel ST, even without\naccessing discriminator gradients directly. We also address the problem of\nautomatic evaluation for captioning models and introduce a new semantic score,\nand show its correlation to human judgement. As an evaluation paradigm, we\nargue that an important criterion for a captioner is the ability to generalize\nto compositions of objects that do not usually co-occur together. To this end,\nwe introduce a small captioned Out of Context (OOC) test set. The OOC set,\ncombined with our semantic score, are the proposed new diagnosis tools for the\ncaptioning community. When evaluated on OOC and MS-COCO benchmarks, we show\nthat SCST-based training has a strong performance in both semantic score and\nhuman evaluation, promising to be a valuable new approach for efficient\ndiscrete GAN training.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 19:10:43 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 17:43:25 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 18:41:03 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Dognin", "Pierre L.", "", "IBM Research, USA"], ["Melnyk", "Igor", "", "IBM Research, USA"], ["Mroueh", "Youssef", "", "IBM Research, USA"], ["Ross", "Jarret", "", "IBM Research, USA"], ["Sercu", "Tom", "", "IBM Research, USA"]]}, {"id": "1805.00107", "submitter": "Saeed Ranjbar Alvar", "authors": "Saeed Ranjbar Alvar and Ivan V. Baji\\'c", "title": "MV-YOLO: Motion Vector-aided Tracking by Semantic Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is the cornerstone of many visual analytics systems. While\nconsiderable progress has been made in this area in recent years, robust,\nefficient, and accurate tracking in real-world video remains a challenge. In\nthis paper, we present a hybrid tracker that leverages motion information from\nthe compressed video stream and a general-purpose semantic object detector\nacting on decoded frames to construct a fast and efficient tracking engine. The\nproposed approach is compared with several well-known recent trackers on the\nOTB tracking dataset. The results indicate advantages of the proposed method in\nterms of speed and/or accuracy.Other desirable features of the proposed method\nare its simplicity and deployment efficiency, which stems from the fact that it\nreuses the resources and information that may already exist in the system for\nother reasons.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 21:33:43 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 20:21:39 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Alvar", "Saeed Ranjbar", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "1805.00116", "submitter": "Jeffrey Uhlmann", "authors": "Jeffrey Uhlmann", "title": "A Canonical Image Set for Examining and Comparing Image Processing\n  Algorithms", "comments": null, "journal-ref": "Journal of Image and Graphics, Vol. 6, No. 2, pp. 137-144,\n  December 2018", "doi": "10.18178/joig.6.2.137-144", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce a set of four test images\ncontaining features and structures that can facilitate effective examination\nand comparison of image processing algorithms. More specifically, the images\nare designed to more explicitly expose the characteristic properties of\nalgorithms for image compression, virtual resolution adjustment, and\nenhancement. This set was developed at the Naval Research Laboratory (NRL) in\nthe late 1990s as a more rigorous alternative to Lena and other images that\nhave come into common use for purely ad hoc reasons with little or no rigorous\nconsideration of their suitability. The increasing number of test images\nappearing in the literature not only makes it more difficult to compare results\nfrom different papers, it also introduces the potential for cherry-picking to\ninfluence results. The key contribution of this paper is the proposal to\nestablish {\\em some} canonical set to ensure that published results can be\nanalyzed and compared in a rigorous way from one paper to another, and\nconsideration of the four NRL images is proposed for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 22:21:56 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Uhlmann", "Jeffrey", ""]]}, {"id": "1805.00123", "submitter": "Gang Yu", "authors": "Shuai Shao and Zijian Zhao and Boxun Li and Tete Xiao and Gang Yu and\n  Xiangyu Zhang and Jian Sun", "title": "CrowdHuman: A Benchmark for Detecting Human in a Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human detection has witnessed impressive progress in recent years. However,\nthe occlusion issue of detecting human in highly crowded environments is far\nfrom solved. To make matters worse, crowd scenarios are still under-represented\nin current human detection benchmarks. In this paper, we introduce a new\ndataset, called CrowdHuman, to better evaluate detectors in crowd scenarios.\nThe CrowdHuman dataset is large, rich-annotated and contains high diversity.\nThere are a total of $470K$ human instances from the train and validation\nsubsets, and $~22.6$ persons per image, with various kinds of occlusions in the\ndataset. Each human instance is annotated with a head bounding-box, human\nvisible-region bounding-box and human full-body bounding-box. Baseline\nperformance of state-of-the-art detection frameworks on CrowdHuman is\npresented. The cross-dataset generalization results of CrowdHuman dataset\ndemonstrate state-of-the-art performance on previous dataset including\nCaltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our\ndataset will serve as a solid baseline and help promote future research in\nhuman detection tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 22:49:54 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Shao", "Shuai", ""], ["Zhao", "Zijian", ""], ["Li", "Boxun", ""], ["Xiao", "Tete", ""], ["Yu", "Gang", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "1805.00138", "submitter": "Shubhra Aich", "authors": "Shubhra Aich, William van der Kamp, and Ian Stavness", "title": "Semantic Binary Segmentation using Convolutional Networks without\n  Decoders", "comments": "CVPR 2018 DeepGlobe Workshop; Code repository:\n  https://github.com/littleaich/deepglobe2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient architecture for semantic image\nsegmentation using the depth-to-space (D2S) operation. Our D2S model is\ncomprised of a standard CNN encoder followed by a depth-to-space reordering of\nthe final convolutional feature maps. Our approach eliminates the decoder\nportion of traditional encoder-decoder segmentation models and reduces the\namount of computation almost by half. As a participant of the DeepGlobe Road\nExtraction competition, we evaluate our models on the corresponding road\nsegmentation dataset. Our highly efficient D2S models exhibit comparable\nperformance to standard segmentation models with much lower computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 00:10:12 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 18:14:39 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Aich", "Shubhra", ""], ["van der Kamp", "William", ""], ["Stavness", "Ian", ""]]}, {"id": "1805.00145", "submitter": "Xiaoxiao Guo", "authors": "Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio\n  Schmidt Feris", "title": "Dialog-based Interactive Image Retrieval", "comments": "accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for interactive image retrieval have demonstrated the merit\nof integrating user feedback, improving retrieval results. However, most\ncurrent systems rely on restricted forms of user feedback, such as binary\nrelevance responses, or feedback based on a fixed set of relative attributes,\nwhich limits their impact. In this paper, we introduce a new approach to\ninteractive image search that enables users to provide feedback via natural\nlanguage, allowing for more natural and effective interaction. We formulate the\ntask of dialog-based interactive image retrieval as a reinforcement learning\nproblem, and reward the dialog system for improving the rank of the target\nimage during each dialog turn. To mitigate the cumbersome and costly process of\ncollecting human-machine conversations as the dialog system learns, we train\nour system with a user simulator, which is itself trained to describe the\ndifferences between target and candidate images. The efficacy of our approach\nis demonstrated in a footwear retrieval application. Experiments on both\nsimulated and real-world data show that 1) our proposed learning framework\nachieves better accuracy than other supervised and reinforcement learning\nbaselines and 2) user feedback based on natural language rather than\npre-specified attributes leads to more effective retrieval results, and a more\nnatural and expressive communication interface.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 01:13:01 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 17:56:08 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 22:13:05 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Guo", "Xiaoxiao", ""], ["Wu", "Hui", ""], ["Cheng", "Yu", ""], ["Rennie", "Steven", ""], ["Tesauro", "Gerald", ""], ["Feris", "Rogerio Schmidt", ""]]}, {"id": "1805.00192", "submitter": "Ashu Sharma", "authors": "Ashu Sharma, Jayanta Kumar Ghosh and Saptrarshi Kolay", "title": "Fixation Data Analysis for High Resolution Satellite Images", "comments": "Extended version is submitted to SPIE-2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented study is an eye tracking experiment for high-resolution\nsatellite (HRS) images. The reported experiment explores the Area Of Interest\n(AOI) based analysis of eye fixation data for complex HRS images. The study\nreflects the requisite of reference data for bottom-up saliency-based\nsegmentation and the struggle of eye tracking data analysis for complex\nsatellite images. The intended fixation data analysis aims towards the\nreference data creation for bottom-up saliency-based segmentation of\nhigh-resolution satellite images. The analytical outcome of this experimental\nstudy provides a solution for AOI-based analysis for fixation data in the\ncomplex environment of satellite images and recommendations for reference data\nconstruction which is already an ongoing effort.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 05:15:25 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Sharma", "Ashu", ""], ["Ghosh", "Jayanta Kumar", ""], ["Kolay", "Saptrarshi", ""]]}, {"id": "1805.00223", "submitter": "Deepak Mishra", "authors": "Deepak Mishra, Rajeev Ranjan, Santanu Chaudhury, Mukul Sarkar,\n  Arvinder Singh Soin", "title": "Localization: A Missing Link in the Pipeline of Object Matching and\n  Registration", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a process of aligning two or more images of same\nobjects using geometric transformation. Most of the existing approaches work on\nthe assumption of location invariance. These approaches require object-centric\nimages to perform matching. Further, in absence of intensity level symmetry\nbetween the corresponding points in two images, the learning based registration\napproaches rely on synthetic deformations, which often fail in real scenarios.\nTo address these issues, a combination of convolutional neural networks (CNNs)\nto perform the desired registration is developed in this work. The complete\nobjective is divided into three sub-objectives: object localization,\nsegmentation and matching transformation. Object localization step establishes\nan initial correspondence between the images. A modified version of single shot\nmulti-box detector is used for this purpose. The detected region is cropped to\nmake the images object-centric. Subsequently, the objects are segmented and\nmatched using a spatial transformer network employing thin plate spline\ndeformation. Initial experiments on MNIST and Caltech-101 datasets show that\nthe proposed model is able to produce accurate matching. Quantitative\nevaluation performed using dice coefficient (DC) and mean intersection over\nunion (mIoU) show that proposed method results in the values of 79% and 66%,\nrespectively for MNIST dataset and the values of 94% and 90%, respectively for\nCaltech-101 dataset. The proposed framework is extended to the registration of\nCT and US images, which is free from any data specific assumptions and has\nbetter generalization capability as compared to the existing\nrule-based/classical approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 07:50:25 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 12:01:54 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Mishra", "Deepak", ""], ["Ranjan", "Rajeev", ""], ["Chaudhury", "Santanu", ""], ["Sarkar", "Mukul", ""], ["Soin", "Arvinder Singh", ""]]}, {"id": "1805.00247", "submitter": "Jifei Song", "authors": "Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales", "title": "Learning to Sketch with Shortcut Cycle Consistency", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To see is to sketch -- free-hand sketching naturally builds ties between\nhuman and machine vision. In this paper, we present a novel approach for\ntranslating an object photo to a sketch, mimicking the human sketching process.\nThis is an extremely challenging task because the photo and sketch domains\ndiffer significantly. Furthermore, human sketches exhibit various levels of\nsophistication and abstraction even when depicting the same object instance in\na reference photo. This means that even if photo-sketch pairs are available,\nthey only provide weak supervision signal to learn a translation model.\nCompared with existing supervised approaches that solve the problem of\nD(E(photo)) -> sketch, where E($\\cdot$) and D($\\cdot$) denote encoder and\ndecoder respectively, we take advantage of the inverse problem (e.g.,\nD(E(sketch)) -> photo), and combine with the unsupervised learning tasks of\nwithin-domain reconstruction, all within a multi-task learning framework.\nCompared with existing unsupervised approaches based on cycle consistency\n(i.e., D(E(D(E(photo)))) -> photo), we introduce a shortcut consistency\nenforced at the encoder bottleneck (e.g., D(E(photo)) -> photo) to exploit the\nadditional self-supervision. Both qualitative and quantitative results show\nthat the proposed model is superior to a number of state-of-the-art\nalternatives. We also show that the synthetic sketches can be used to train a\nbetter fine-grained sketch-based image retrieval (FG-SBIR) model, effectively\nalleviating the problem of sketch data scarcity.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 09:13:14 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Song", "Jifei", ""], ["Pang", "Kaiyue", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1805.00251", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, Tie-Yan Liu", "title": "Conditional Image-to-Image Translation", "comments": "9 pages, 9 figures, IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation tasks have been widely investigated with\nGenerative Adversarial Networks (GANs) and dual learning. However, existing\nmodels lack the ability to control the translated results in the target domain\nand their results usually lack of diversity in the sense that a fixed image\nusually leads to (almost) deterministic translation result. In this paper, we\nstudy a new problem, conditional image-to-image translation, which is to\ntranslate an image from the source domain to the target domain conditioned on a\ngiven image in the target domain. It requires that the generated image should\ninherit some domain-specific features of the conditional image from the target\ndomain. Therefore, changing the conditional image in the target domain will\nlead to diverse translation results for a fixed input image from the source\ndomain, and therefore the conditional input image helps to control the\ntranslation results. We tackle this problem with unpaired data based on GANs\nand dual learning. We twist two conditional translation models (one translation\nfrom A domain to B domain, and the other one from B domain to A domain)\ntogether for inputs combination and reconstruction while preserving domain\nindependent features. We carry out experiments on men's faces from-to women's\nfaces translation and edges to shoes&bags translations. The results demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 09:23:07 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Lin", "Jianxin", ""], ["Xia", "Yingce", ""], ["Qin", "Tao", ""], ["Chen", "Zhibo", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1805.00258", "submitter": "Hui Feng Prof.", "authors": "Hui Feng, Shanshan Wang, Shuzhi Sam Ge", "title": "Object Activity Scene Description, Construction and Recognition", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TCYB.2019.2904901", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is a critical task for social robots to meaningfully\nengage with their environment. 3D human skeleton-based action recognition is an\nattractive research area in recent years. Although, the existing approaches are\ngood at action recognition, it is a great challenge to recognize a group of\nactions in an activity scene. To tackle this problem, at first, we partition\nthe scene into several primitive actions (PAs) based upon motion attention\nmechanism. Then, the primitive actions are described by the trajectory vectors\nof corresponding joints. After that, motivated by text classification based on\nword embedding, we employ convolution neural network (CNN) to recognize\nactivity scenes by considering motion of joints as \"word\" of activity. The\nexperimental results on the scenes of human activity dataset show the\nefficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 09:56:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Feng", "Hui", ""], ["Wang", "Shanshan", ""], ["Ge", "Shuzhi Sam", ""]]}, {"id": "1805.00264", "submitter": "Yuriy Anisimov", "authors": "Yuriy Anisimov and Didier Stricker", "title": "Fast and Efficient Depth Map Estimation from Light Fields", "comments": "International Conference on 3D Vision (3DV), Qingdao, China, October\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an algorithm for depth map estimation from the light field\nimages in relatively small amount of time, using only single thread on CPU. The\nproposed method improves existing principle of line fitting in 4-dimensional\nlight field space. Line fitting is based on color values comparison using\nkernel density estimation. Our method utilizes result of Semi-Global Matching\n(SGM) with Census transform-based matching cost as a border initialization for\nline fitting. It provides a significant reduction of computations needed to\nfind the best depth match. With the suggested evaluation metric we show that\nproposed method is applicable for efficient depth map estimation while\npreserving low computational time compared to others.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 10:20:35 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Anisimov", "Yuriy", ""], ["Stricker", "Didier", ""]]}, {"id": "1805.00309", "submitter": "Ning Ma", "authors": "Ning Ma, Alexey Volkov, Aleksandr Livshits, Pawel Pietrusinski,\n  Houdong Hu, Mark Bolin", "title": "An Universal Image Attractiveness Ranking Framework", "comments": "Accepted by 2019 Winter Conference on Application of Computer Vision\n  (WACV)", "journal-ref": "2019 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": "10.1109/WACV.2019.00075", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework to rank image attractiveness using a novel\npairwise deep network trained with a large set of side-by-side multi-labeled\nimage pairs from a web image index. The judges only provide relative ranking\nbetween two images without the need to directly assign an absolute score, or\nrate any predefined image attribute, thus making the rating more intuitive and\naccurate. We investigate a deep attractiveness rank net (DARN), a combination\nof deep convolutional neural network and rank net, to directly learn an\nattractiveness score mean and variance for each image and the underlying\ncriteria the judges use to label each pair. The extension of this model\n(DARN-V2) is able to adapt to individual judge's personal preference. We also\nshow the attractiveness of search results are significantly improved by using\nthis attractiveness information in a real commercial search engine. We evaluate\nour model against other state-of-the-art models on our side-by-side web test\ndata and another public aesthetic data set. With much less judgments (1M vs\n50M), our model outperforms on side-by-side labeled data, and is comparable on\ndata labeled by absolute score.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 21:10:37 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 06:27:01 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 06:34:48 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Ma", "Ning", ""], ["Volkov", "Alexey", ""], ["Livshits", "Aleksandr", ""], ["Pietrusinski", "Pawel", ""], ["Hu", "Houdong", ""], ["Bolin", "Mark", ""]]}, {"id": "1805.00310", "submitter": "Pin-Yu Chen", "authors": "Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, Chia-Mu Yu", "title": "On the Limitation of MagNet Defense against $L_1$-based Adversarial\n  Examples", "comments": "Accepted to IEEE/IFIP International Conference on Dependable and\n  Systems and Networks (DSN) 2018 Workshop on Dependable and Secure Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, defending adversarial perturbations to natural examples in\norder to build robust machine learning models trained by deep neural networks\n(DNNs) has become an emerging research field in the conjunction of deep\nlearning and security. In particular, MagNet consisting of an adversary\ndetector and a data reformer is by far one of the strongest defenses in the\nblack-box oblivious attack setting, where the attacker aims to craft\ntransferable adversarial examples from an undefended DNN model to bypass an\nunknown defense module deployed on the same DNN model. Under this setting,\nMagNet can successfully defend a variety of attacks in DNNs, including the\nhigh-confidence adversarial examples generated by the Carlini and Wagner's\nattack based on the $L_2$ distortion metric. However, in this paper, under the\nsame attack setting we show that adversarial examples crafted based on the\n$L_1$ distortion metric can easily bypass MagNet and mislead the target DNN\nimage classifiers on MNIST and CIFAR-10. We also provide explanations on why\nthe considered approach can yield adversarial examples with superior attack\nperformance and conduct extensive experiments on variants of MagNet to verify\nits lack of robustness to $L_1$ distortion based attacks. Notably, our results\nsubstantially weaken the assumption of effective threat models on MagNet that\nrequire knowing the deployed defense technique when attacking DNNs (i.e., the\ngray-box attack setting).\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 05:44:51 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 15:37:54 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Lu", "Pei-Hsuan", ""], ["Chen", "Pin-Yu", ""], ["Chen", "Kang-Cheng", ""], ["Yu", "Chia-Mu", ""]]}, {"id": "1805.00311", "submitter": "Yinheng Zhu", "authors": "Yinheng Zhu, Wanli Chen, Xun Zhan, Zonglin Guo, Hongjian Shi, Ian G.\n  Harris", "title": "Head Mounted Pupil Tracking Using Convolutional Neural Network", "comments": "It's out of date and not STOA any more", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pupil tracking is an important branch of object tracking which require high\nprecision. We investigate head mounted pupil tracking which is often more\nconvenient and precise than remote pupil tracking, but also more challenging.\nWhen pupil tracking suffers from noise like bad illumination, detection\nprecision dramatically decreases. Due to the appearance of head mounted\nrecording device and public benchmark image datasets, head mounted tracking\nalgorithms have become easier to design and evaluate. In this paper, we propose\na robust head mounted pupil detection algorithm which uses a Convolutional\nNeural Network (CNN) to combine different features of pupil. Here we consider\nthree features of pupil. Firstly, we use three pupil feature-based algorithms\nto find pupil center independently. Secondly, we use a CNN to evaluate the\nquality of each result. Finally, we select the best result as output. The\nexperimental results show that our proposed algorithm performs better than the\npresent state-of-art.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 04:48:16 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 01:44:01 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Zhu", "Yinheng", ""], ["Chen", "Wanli", ""], ["Zhan", "Xun", ""], ["Guo", "Zonglin", ""], ["Shi", "Hongjian", ""], ["Harris", "Ian G.", ""]]}, {"id": "1805.00312", "submitter": "Iman Sajedian", "authors": "Iman Sajedian, Jeonghyun Kim, Junsuk Rho", "title": "Predicting resonant properties of plasmonic structures by deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning can be used to extract meaningful results from images. In this\npaper, we used convolutional neural networks combined with recurrent neural\nnetworks on images of plasmonic structures and extract absorption data form\nthem. To provide the required data for the model we did 100,000 simulations\nwith similar setups and random structures. By designing a deep network we could\nfind a model that could predict the absorption of any structure with similar\nsetup. We used convolutional neural networks to get the spatial information\nfrom the images and we used recurrent neural networks to help the model find\nthe relationship between the spatial information obtained from convolutional\nneural network model. With this design we could reach a very low loss in\npredicting the absorption compared to the results obtained from numerical\nsimulation in a very short time.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 09:25:35 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Sajedian", "Iman", ""], ["Kim", "Jeonghyun", ""], ["Rho", "Junsuk", ""]]}, {"id": "1805.00313", "submitter": "Xuemeng Song", "authors": "Xuemeng Song, Fuli Feng, Xianjing Han, Xin Yang, Wei Liu, Liqiang Nie", "title": "Neural Compatibility Modeling with Attentive Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the booming fashion sector and its huge potential benefits have\nattracted tremendous attention from many research communities. In particular,\nincreasing research efforts have been dedicated to the complementary clothing\nmatching as matching clothes to make a suitable outfit has become a daily\nheadache for many people, especially those who do not have the sense of\naesthetics. Thanks to the remarkable success of neural networks in various\napplications such as image classification and speech recognition, the\nresearchers are enabled to adopt the data-driven learning methods to analyze\nfashion items. Nevertheless, existing studies overlook the rich valuable\nknowledge (rules) accumulated in fashion domain, especially the rules regarding\nclothing matching. Towards this end, in this work, we shed light on\ncomplementary clothing matching by integrating the advanced deep neural\nnetworks and the rich fashion domain knowledge. Considering that the rules can\nbe fuzzy and different rules may have different confidence levels to different\nsamples, we present a neural compatibility modeling scheme with attentive\nknowledge distillation based on the teacher-student network scheme. Extensive\nexperiments on the real-world dataset show the superiority of our model over\nseveral state-of-the-art baselines. Based upon the comparisons, we observe\ncertain fashion insights that add value to the fashion matching study. As a\nbyproduct, we released the codes, and involved parameters to benefit other\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 01:26:48 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Song", "Xuemeng", ""], ["Feng", "Fuli", ""], ["Han", "Xianjing", ""], ["Yang", "Xin", ""], ["Liu", "Wei", ""], ["Nie", "Liqiang", ""]]}, {"id": "1805.00314", "submitter": "Josiah Wang", "authors": "Josiah Wang, Pranava Madhyastha, Lucia Specia", "title": "Object Counts! Bringing Explicit Detections Back into Image Captioning", "comments": "Please cite: In Proceedings of 2018 Conference of the North American\n  Chapter of the Association for Computational Linguistics (NAACL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of explicit object detectors as an intermediate step to image\ncaptioning - which used to constitute an essential stage in early work - is\noften bypassed in the currently dominant end-to-end approaches, where the\nlanguage model is conditioned directly on a mid-level image embedding. We argue\nthat explicit detections provide rich semantic information, and can thus be\nused as an interpretable representation to better understand why end-to-end\nimage captioning systems work well. We provide an in-depth analysis of\nend-to-end image captioning by exploring a variety of cues that can be derived\nfrom such object detections. Our study reveals that end-to-end image captioning\nsystems rely on matching image representations to generate captions, and that\nencoding the frequency, size and position of objects are complementary and all\nplay a role in forming a good image representation. It also reveals that\ndifferent object categories contribute in different ways towards image\ncaptioning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:51:46 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Wang", "Josiah", ""], ["Madhyastha", "Pranava", ""], ["Specia", "Lucia", ""]]}, {"id": "1805.00316", "submitter": "Shabab Bazrafkan", "authors": "Shabab Bazrafkan, Hossein Javidnia, Peter Corcoran", "title": "Versatile Auxiliary Classifier with Generative Adversarial Network\n  (VAC+GAN)", "comments": "This paper will be uploaded as two separate manuscripts", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most interesting challenges in Artificial Intelligence is to train\nconditional generators which are able to provide labeled adversarial samples\ndrawn from a specific distribution. In this work, a new framework is presented\nto train a deep conditional generator by placing a classifier in parallel with\nthe discriminator and back propagate the classification error through the\ngenerator network. The method is versatile and is applicable to any variations\nof Generative Adversarial Network (GAN) implementation, and also gives superior\nresults compared to similar methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 13:17:39 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 17:26:41 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 23:47:57 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Bazrafkan", "Shabab", ""], ["Javidnia", "Hossein", ""], ["Corcoran", "Peter", ""]]}, {"id": "1805.00321", "submitter": "Ravi Lanka", "authors": "Ravi Lanka", "title": "PURE: Scalable Phase Unwrapping with Spatial Redundant Arcs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase unwrapping is a key problem in many coherent imaging systems, such as\nsynthetic aperture radar (SAR) interferometry. A general formulation for\nredundant integration of finite differences for phase unwrapping (Costantini et\nal., 2010) was shown to produce a more reliable solution by exploiting\nredundant differential estimates. However, this technique requires a commercial\nlinear programming solver for large-scale problems. For a linear cost function,\nwe propose a method based on Dual Decomposition that breaks the given problem\ndefined over a non-planar graph into tractable sub-problems over planar\nsubgraphs. We also propose a decomposition technique that exploits the\nunderlying graph structure for solving the sub-problems efficiently and\nguarantees asymptotic convergence to the globally optimal solution. The\nexperimental results demonstrate that the proposed approach is comparable to\nthe existing state-of-the-art methods in terms of the estimate with a better\nruntime and memory footprint.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:05:07 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 04:13:51 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Lanka", "Ravi", ""]]}, {"id": "1805.00322", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Thomas Lu, Edward Chow", "title": "Occluded object reconstruction for first responders with augmented\n  reality glasses using conditional generative adversarial networks", "comments": "SPIE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firefighters suffer a variety of life-threatening risks, including\nline-of-duty deaths, injuries, and exposures to hazardous substances. Support\nfor reducing these risks is important. We built a partially occluded object\nreconstruction method on augmented reality glasses for first responders. We\nused a deep learning based on conditional generative adversarial networks to\ntrain associations between the various images of flammable and hazardous\nobjects and their partially occluded counterparts. Our system then\nreconstructed an image of a new flammable object. Finally, the reconstructed\nimage was superimposed on the input image to provide \"transparency\". The system\nimitates human learning about the laws of physics through experience by\nlearning the shape of flammable objects and the flame characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 23:56:10 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Yun", "Kyongsik", ""], ["Lu", "Thomas", ""], ["Chow", "Edward", ""]]}, {"id": "1805.00324", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani, Nasser M. Nasrabadi, Jeremy Dawson", "title": "A Deep Face Identification Network Enhanced by Facial Attributes\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new deep framework which predicts facial\nattributes and leverage it as a soft modality to improve face identification\nperformance. Our model is an end to end framework which consists of a\nconvolutional neural network (CNN) whose output is fanned out into two separate\nbranches; the first branch predicts facial attributes while the second branch\nidentifies face images. Contrary to the existing multi-task methods which only\nuse a shared CNN feature space to train these two tasks jointly, we fuse the\npredicted attributes with the features from the face modality in order to\nimprove the face identification performance. Experimental results show that our\nmodel brings benefits to both face identification as well as facial attribute\nprediction performance, especially in the case of identity facial attributes\nsuch as gender prediction. We tested our model on two standard datasets\nannotated by identities and face attributes. Experimental results indicate that\nthe proposed model outperforms most of the current existing face identification\nand attribute prediction methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 20:43:52 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Nasrabadi", "Nasser M.", ""], ["Dawson", "Jeremy", ""]]}, {"id": "1805.00325", "submitter": "Mohammad Sadegh Ebrahimi", "authors": "Mohammad Sadegh Ebrahimi, Hossein Karkeh Abadi", "title": "Study of Residual Networks for Image Recognition", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks demonstrate to have a high performance on image\nclassification tasks while being more difficult to train. Due to the complexity\nand vanishing gradient problem, it normally takes a lot of time and more\ncomputational power to train deeper neural networks. Deep residual networks\n(ResNets) can make the training process faster and attain more accuracy\ncompared to their equivalent neural networks. ResNets achieve this improvement\nby adding a simple skip connection parallel to the layers of convolutional\nneural networks. In this project we first design a ResNet model that can\nperform the image classification task on the Tiny ImageNet dataset with a high\naccuracy, then we compare the performance of this ResNet model with its\nequivalent Convolutional Network (ConvNet). Our findings illustrate that\nResNets are more prone to overfitting despite their higher accuracy. Several\nmethods to prevent overfitting such as adding dropout layers and stochastic\naugmentation of the training dataset has been studied in this work.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 23:04:53 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Ebrahimi", "Mohammad Sadegh", ""], ["Abadi", "Hossein Karkeh", ""]]}, {"id": "1805.00326", "submitter": "Ivona Tautkute", "authors": "Ivona Tautkute, Tomasz Trzcinski, Adam Bielski", "title": "I Know How You Feel: Emotion Recognition with Facial Landmarks", "comments": "CVPRW 2018, The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) Workshops 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of human emotions remains an important and challenging task\nfor many computer vision algorithms, especially in the era of humanoid robots\nwhich coexist with humans in their everyday life. Currently proposed methods\nfor emotion recognition solve this task using multi-layered convolutional\nnetworks that do not explicitly infer any facial features in the classification\nphase. In this work, we postulate a fundamentally different approach to solve\nemotion recognition task that relies on incorporating facial landmarks as a\npart of the classification loss function. To that end, we extend a recently\nproposed Deep Alignment Network (DAN), that achieves state-of-the-art results\nin the recent facial landmark recognition challenge, with a term related to\nfacial features. Thanks to this simple modification, our model called\nEmotionalDAN is able to outperform state-of-the-art emotion classification\nmethods on two challenging benchmark dataset by up to 5%.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 19:06:50 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 18:29:02 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Tautkute", "Ivona", ""], ["Trzcinski", "Tomasz", ""], ["Bielski", "Adam", ""]]}, {"id": "1805.00328", "submitter": "Stefano Rosa", "authors": "Zhihua Wang and Stefano Rosa and Bo Yang and Sen Wang and Niki Trigoni\n  and Andrew Markham", "title": "3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object\n  Deformations", "comments": "in IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to interact and understand the environment is a fundamental\nprerequisite for a wide range of applications from robotics to augmented\nreality. In particular, predicting how deformable objects will react to applied\nforces in real time is a significant challenge. This is further confounded by\nthe fact that shape information about encountered objects in the real world is\noften impaired by occlusions, noise and missing regions e.g. a robot\nmanipulating an object will only be able to observe a partial view of the\nentire solid. In this work we present a framework, 3D-PhysNet, which is able to\npredict how a three-dimensional solid will deform under an applied force using\nintuitive physics modelling. In particular, we propose a new method to encode\nthe physical properties of the material and the applied force, enabling\ngeneralisation over materials. The key is to combine deep variational\nautoencoders with adversarial training, conditioned on the applied force and\nthe material properties. We further propose a cascaded architecture that takes\na single 2.5D depth view of the object and predicts its deformation. Training\ndata is provided by a physics simulator. The network is fast enough to be used\nin real-time applications from partial views. Experimental results show the\nviability and the generalisation properties of the proposed architecture.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 15:53:03 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 09:57:44 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Wang", "Zhihua", ""], ["Rosa", "Stefano", ""], ["Yang", "Bo", ""], ["Wang", "Sen", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "1805.00329", "submitter": "Michele Alberti", "authors": "Michele Alberti, Vinaychandran Pondenkandath, Marcel W\\\"ursch, Rolf\n  Ingold, Marcus Liwicki", "title": "DeepDIVA: A Highly-Functional Python Framework for Reproducible\n  Experiments", "comments": "Submitted at the 16th International Conference on Frontiers in\n  Handwriting Recognition (ICFHR), 6 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeepDIVA: an infrastructure designed to enable quick and\nintuitive setup of reproducible experiments with a large range of useful\nanalysis functionality. Reproducing scientific results can be a frustrating\nexperience, not only in document image analysis but in machine learning in\ngeneral. Using DeepDIVA a researcher can either reproduce a given experiment\nwith a very limited amount of information or share their own experiments with\nothers. Moreover, the framework offers a large range of functions, such as\nboilerplate code, keeping track of experiments, hyper-parameter optimization,\nand visualization of data and results. To demonstrate the effectiveness of this\nframework, this paper presents case studies in the area of handwritten document\nanalysis where researchers benefit from the integrated functionality. DeepDIVA\nis implemented in Python and uses the deep learning framework PyTorch. It is\ncompletely open source, and accessible as Web Service through DIVAServices.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:00:42 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Alberti", "Michele", ""], ["Pondenkandath", "Vinaychandran", ""], ["W\u00fcrsch", "Marcel", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1805.00330", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Sejun Song, Ronghua Xu, Baek-Young Choi,\n  Timothy R. Faughnan", "title": "Real-Time Human Detection as an Edge Service Enabled by a Lightweight\n  CNN", "comments": "to appear in the IEEE International Conference on Edge Computing\n  (IEEE EDGE 2018), San Francisco, CA, USA, July 2, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing allows more computing tasks to take place on the decentralized\nnodes at the edge of networks. Today many delay sensitive, mission-critical\napplications can leverage these edge devices to reduce the time delay or even\nto enable real time, online decision making thanks to their onsite presence.\nHuman objects detection, behavior recognition and prediction in smart\nsurveillance fall into that category, where a transition of a huge volume of\nvideo streaming data can take valuable time and place heavy pressure on\ncommunication networks. It is widely recognized that video processing and\nobject detection are computing intensive and too expensive to be handled by\nresource limited edge devices. Inspired by the depthwise separable convolution\nand Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural\nNetwork (LCNN) is introduced in this paper. By narrowing down the classifier's\nsearching space to focus on human objects in surveillance video frames, the\nproposed LCNN algorithm is able to detect pedestrians with an affordable\ncomputation workload to an edge device. A prototype has been implemented on an\nedge node (Raspberry PI 3) using openCV libraries, and satisfactory performance\nis achieved using real world surveillance video streams. The experimental study\nhas validated the design of LCNN and shown it is a promising approach to\ncomputing intensive applications at the edge.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 22:02:10 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Song", "Sejun", ""], ["Xu", "Ronghua", ""], ["Choi", "Baek-Young", ""], ["Faughnan", "Timothy R.", ""]]}, {"id": "1805.00331", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Sejun Song, Ronghua Xu, Baek-Young Choi,\n  Timothy R. Faughnan", "title": "Smart Surveillance as an Edge Network Service: from Harr-Cascade, SVM to\n  a Lightweight CNN", "comments": "10-page version, accepted by the 4th IEEE International Conference on\n  Collaboration and Internet Computing (IEEE CIC 2018), Oct 18 - 20, 2018.\n  Philadelphia, Pennsylvania, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing efficiently extends the realm of information technology beyond\nthe boundary defined by cloud computing paradigm. Performing computation near\nthe source and destination, edge computing is promising to address the\nchallenges in many delay-sensitive applications, like real-time human\nsurveillance. Leveraging the ubiquitously connected cameras and smart mobile\ndevices, it enables video analytics at the edge. In recent years, many smart\nvideo surveillance approaches are proposed for object detection and tracking by\nusing Artificial Intelligence (AI) and Machine Learning (ML) algorithms. This\nwork explores the feasibility of two popular human-objects detection schemes,\nHarr-Cascade and HOG feature extraction and SVM classifier, at the edge and\nintroduces a lightweight Convolutional Neural Network (L-CNN) leveraging the\ndepthwise separable convolution for less computation, for human detection.\nSingle Board computers (SBC) are used as edge devices for tests and algorithms\nare validated using real-world campus surveillance video streams and open data\nsets. The experimental results are promising that the final algorithm is able\nto track humans with a decent accuracy at a resource consumption affordable by\nedge devices in real-time manner.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 22:09:18 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 13:08:19 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Song", "Sejun", ""], ["Xu", "Ronghua", ""], ["Choi", "Baek-Young", ""], ["Faughnan", "Timothy R.", ""]]}, {"id": "1805.00334", "submitter": "Lei Tian", "authors": "Thanh Nguyen, Yujia Xue, Yunzhe Li, Lei Tian, George Nehmetallah", "title": "Deep learning approach to Fourier ptychographic microscopy", "comments": null, "journal-ref": "Opt. Express 26, 26470-26484 (2018)", "doi": "10.1364/OE.26.026470", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have gained tremendous success in\nsolving complex inverse problems. The aim of this work is to develop a novel\nCNN framework to reconstruct video sequence of dynamic live cells captured\nusing a computational microscopy technique, Fourier ptychographic microscopy\n(FPM). The unique feature of the FPM is its capability to reconstruct images\nwith both wide field-of-view (FOV) and high resolution, i.e. a large\nspace-bandwidth-product (SBP), by taking a series of low resolution intensity\nimages. For live cell imaging, a single FPM frame contains thousands of cell\nsamples with different morphological features. Our idea is to fully exploit the\nstatistical information provided by this large spatial ensemble so as to make\npredictions in a sequential measurement, without using any additional temporal\ndataset. Specifically, we show that it is possible to reconstruct high-SBP\ndynamic cell videos by a CNN trained only on the first FPM dataset captured at\nthe beginning of a time-series experiment. Our CNN approach reconstructs a\n12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared\nto the model-based FPM algorithm. In addition, the CNN further reduces the\nrequired number of images in each time frame by ~6X. Overall, this\nsignificantly improves the imaging throughput by reducing both the acquisition\nand computational times. The proposed CNN is based on the conditional\ngenerative adversarial network (cGAN) framework. Additionally, we also exploit\ntransfer learning so that our pre-trained CNN can be further optimized to image\nother cell types. Our technique demonstrates a promising deep learning approach\nto continuously monitor large live-cell populations over an extended time and\ngather useful spatial and temporal information with sub-cellular resolution.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 02:53:25 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 20:16:23 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 22:59:06 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Nguyen", "Thanh", ""], ["Xue", "Yujia", ""], ["Li", "Yunzhe", ""], ["Tian", "Lei", ""], ["Nehmetallah", "George", ""]]}, {"id": "1805.00348", "submitter": "Dongrui Wu", "authors": "Yuqi Cui, Xiao Zhang, Yang Wang, Chenfeng Guo, Dongrui Wu", "title": "OMG - Emotion Challenge Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper describes our solution to the 2018 IEEE World Congress on\nComputational Intelligence One-Minute Gradual-Emotional Behavior Challenge,\nwhose goal was to estimate continuous arousal and valence values from short\nvideos. We designed four base regression models using visual and audio\nfeatures, and then used a spectral approach to fuse them to obtain improved\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 10:50:30 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Cui", "Yuqi", ""], ["Zhang", "Xiao", ""], ["Wang", "Yang", ""], ["Guo", "Chenfeng", ""], ["Wu", "Dongrui", ""]]}, {"id": "1805.00355", "submitter": "Debasmit Das", "authors": "Debasmit Das, C.S. George Lee", "title": "Sample-to-Sample Correspondence for Unsupervised Domain Adaptation", "comments": "Final version appeared in Engineering Applications of Artificial\n  Intelligence. Mostly, the related work in this version is different from the\n  published version", "journal-ref": null, "doi": "10.1016/j.engappai.2018.05.001", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption that training and testing samples are generated from the same\ndistribution does not always hold for real-world machine-learning applications.\nThe procedure of tackling this discrepancy between the training (source) and\ntesting (target) domains is known as domain adaptation. We propose an\nunsupervised version of domain adaptation that considers the presence of only\nunlabelled data in the target domain. Our approach centers on finding\ncorrespondences between samples of each domain. The correspondences are\nobtained by treating the source and target samples as graphs and using a convex\ncriterion to match them. The criteria used are first-order and second-order\nsimilarities between the graphs as well as a class-based regularization. We\nhave also developed a computationally efficient routine for the convex\noptimization, thus allowing the proposed method to be used widely. To verify\nthe effectiveness of the proposed method, computer simulations were conducted\non synthetic, image classification and sentiment classification datasets.\nResults validated that the proposed local sample-to-sample matching method\nout-performs traditional moment-matching methods and is competitive with\nrespect to current local domain-adaptation methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 14:12:57 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:48:03 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 06:27:40 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Das", "Debasmit", ""], ["Lee", "C. S. George", ""]]}, {"id": "1805.00357", "submitter": "Markus Degel", "authors": "Markus A. Degel, Nassir Navab, Shadi Albarqouni", "title": "Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D\n  Ultrasound", "comments": null, "journal-ref": "Medical Image Computing and Computer Assisted Intervention (MICCAI\n  2018)", "doi": "10.1007/978-3-030-00937-3_72", "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the left atrium and deriving its size can help to predict and\ndetect various cardiovascular conditions. Automation of this process in 3D\nUltrasound image data is desirable, since manual delineations are\ntime-consuming, challenging and observer-dependent. Convolutional neural\nnetworks have made improvements in computer vision and in medical image\nanalysis. They have successfully been applied to segmentation tasks and were\nextended to work on volumetric data. In this paper we introduce a combined\ndeep-learning based approach on volumetric segmentation in Ultrasound\nacquisitions with incorporation of prior knowledge about left atrial shape and\nimaging device. The results show, that including a shape prior helps the domain\nadaptation and the accuracy of segmentation is further increased with\nadversarial learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 09:22:50 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Degel", "Markus A.", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1805.00361", "submitter": "Baohua Sun", "authors": "Baohua Sun, Lin Yang, Patrick Dong, Wenhan Zhang, Jason Dong, Charles\n  Young", "title": "Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt\n  for Mobile and Embedded Applications", "comments": "9 pages, 10 Figures. Accepted by CVPR 2018 Efficient Deep Learning\n  for Computer Vision workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision performances have been significantly improved in recent years\nby Convolutional Neural Networks(CNN). Currently, applications using CNN\nalgorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs\nor FPGAs. However, power consumption, speed, accuracy, memory footprint, and\ndie size should all be taken into consideration for mobile and embedded\napplications. Domain Specific Architecture (DSA) for CNN is the efficient and\npractical solution for CNN deployment and implementation. We designed and\nproduced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra\npower-efficient performance of 9.3TOPS/Watt and with all processing done in the\ninternal memory instead of outside DRAM. It classifies 224x224 RGB image inputs\nat more than 140fps with peak power consumption at less than 300mW and an\naccuracy comparable to the VGG benchmark. The CNN-DSA accelerator is\nreconfigurable to support CNN model coefficients of various layer sizes and\nlayer types, including convolution, depth-wise convolution, short-cut\nconnections, max pooling, and ReLU. Furthermore, in order to better support\nreal-world deployment for various application scenarios, especially with\nlow-end mobile and embedded platforms and MCUs (Microcontroller Units), we also\ndesigned algorithms to fully utilize the CNN-DSA accelerator efficiently by\nreducing the dependency on external accelerator computation resources,\nincluding implementation of Fully-Connected (FC) layers within the accelerator\nand compression of extracted features from the CNN-DSA accelerator. Live demos\nwith our CNN-DSA accelerator on mobile and embedded systems show its\ncapabilities to be widely and practically applied in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 17:36:14 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Sun", "Baohua", ""], ["Yang", "Lin", ""], ["Dong", "Patrick", ""], ["Zhang", "Wenhan", ""], ["Dong", "Jason", ""], ["Young", "Charles", ""]]}, {"id": "1805.00371", "submitter": "Baiqiang Xia Dr", "authors": "Baiqiang Xia", "title": "Which Facial Expressions Can Reveal Your Gender? A Study With 3D Faces", "comments": "20 pages, single column, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human exhibit rich gender cues in both appearance and behavior. In computer\nvision domain, gender recognition from facial appearance have been extensively\nstudied, while facial behavior based gender recognition studies remain rare. In\nthis work, we first demonstrate that facial expressions influence the gender\npatterns presented in 3D face, and gender recognition performance increases\nwhen training and testing within the same expression. In further, we design\nexperiments which directly extract the morphological changes resulted from\nfacial expressions as features, for expression-based gender recognition.\nExperimental results demonstrate that gender can be recognized with\nconsiderable accuracy in Happy and Disgust expressions, while Surprise and Sad\nexpressions do not convey much gender related information. This is the first\nwork in the literature which investigates expression-based gender\nclassification with 3D faces, and reveals the strength of gender patterns\nincorporated in different types of expressions, namely the Happy, the Disgust,\nthe Surprise and the Sad expressions.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 14:50:23 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Xia", "Baiqiang", ""]]}, {"id": "1805.00385", "submitter": "Mehdi Noroozi", "authors": "Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, Hamed Pirsiavash", "title": "Boosting Self-Supervised Learning via Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In self-supervised learning, one trains a model to solve a so-called pretext\ntask on a dataset without the need for human annotation. The main objective,\nhowever, is to transfer this model to a target domain and task. Currently, the\nmost effective transfer strategy is fine-tuning, which restricts one to use the\nsame model or parts thereof for both pretext and target tasks. In this paper,\nwe present a novel framework for self-supervised learning that overcomes\nlimitations in designing and comparing different tasks, models, and data\ndomains. In particular, our framework decouples the structure of the\nself-supervised model from the final task-specific fine-tuned model. This\nallows us to: 1) quantitatively assess previously incompatible models including\nhandcrafted features; 2) show that deeper neural network models can learn\nbetter representations from the same pretext task; 3) transfer knowledge\nlearned with a deep model to a shallower one and thus boost its learning. We\nuse this framework to design a novel self-supervised task, which achieves\nstate-of-the-art performance on the common benchmarks in PASCAL VOC 2007,\nILSVRC12 and Places by a significant margin. Our learned features shrink the\nmAP gap between models trained via self-supervised learning and supervised\nlearning from 5.9% to 2.6% in object detection on PASCAL VOC 2007.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 15:08:30 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Noroozi", "Mehdi", ""], ["Vinjimoor", "Ananth", ""], ["Favaro", "Paolo", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "1805.00406", "submitter": "Qijun Zhao", "authors": "Ziqing Feng and Qijun Zhao", "title": "Robust Face Recognition with Deeply Normalized Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information has been proven useful for face recognition. However,\nexisting depth-image-based face recognition methods still suffer from noisy\ndepth values and varying poses and expressions. In this paper, we propose a\nnovel method for normalizing facial depth images to frontal pose and neutral\nexpression and extracting robust features from the normalized depth images. The\nmethod is implemented via two deep convolutional neural networks (DCNN),\nnormalization network ($Net_{N}$) and feature extraction network ($Net_{F}$).\nGiven a facial depth image, $Net_{N}$ first converts it to an HHA image, from\nwhich the 3D face is reconstructed via a DCNN. $Net_{N}$ then generates a\npose-and-expression normalized (PEN) depth image from the reconstructed 3D\nface. The PEN depth image is finally passed to $Net_{F}$, which extracts a\nrobust feature representation via another DCNN for face recognition. Our\npreliminary evaluation results demonstrate the superiority of the proposed\nmethod in recognizing faces of arbitrary poses and expressions with depth\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 16:02:50 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Feng", "Ziqing", ""], ["Zhao", "Qijun", ""]]}, {"id": "1805.00460", "submitter": "Andrew Shin", "authors": "Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada", "title": "Customized Image Narrative Generation via Interactive Visual Question\n  Generation and Answering", "comments": "To Appear at CVPR 2018 as spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image description task has been invariably examined in a static manner with\nqualitative presumptions held to be universally applicable, regardless of the\nscope or target of the description. In practice, however, different viewers may\npay attention to different aspects of the image, and yield different\ndescriptions or interpretations under various contexts. Such diversity in\nperspectives is difficult to derive with conventional image description\ntechniques. In this paper, we propose a customized image narrative generation\ntask, in which the users are interactively engaged in the generation process by\nproviding answers to the questions. We further attempt to learn the user's\ninterest via repeating such interactive stages, and to automatically reflect\nthe interest in descriptions for new images. Experimental results demonstrate\nthat our model can generate a variety of descriptions from single image that\ncover a wider range of topics than conventional models, while being\ncustomizable to the target user of interaction.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:27:45 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Shin", "Andrew", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1805.00472", "submitter": "Muzammil Behzad", "authors": "Muzammil Behzad", "title": "Image Denoising via Collaborative Dual-Domain Patch Filtering", "comments": "14 pages, 14 figures, 4 tables, article pending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel image denoising algorithm exploiting\nfeatures from both spatial as well as transformed domain. We implement\nintensity-invariance based improved grouping for collaborative support-agnostic\nsparse reconstruction. For collaboration firstly, we stack similar-structured\npatches via intensity-invariant correlation measure. The grouped patches\ncollaborate to yield desirable sparse estimates for noise filtering. This is\nbecause similar patches share the same support in the transformed domain, such\nsimilar supports can be used as probabilities of active taps to refine the\nsparse estimates. This ultimately produces a very useful patch estimate thus\nincreasing the quality of recovered image by discarding the noise-causing\ncomponents. A region growing based spatially developed post-processor is then\napplied to further enhance the smooth regions by extracting the spatial domain\nfeatures. We also extend our proposed method for denoising of color images.\nComparison results with the state-of-the-art algorithms in terms of peak\nsignal-to-noise ratio (PNSR) and structural similarity (SSIM) index from\nextensive experimentations via a broad range of scenarios demonstrate the\nsuperiority of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 17:01:21 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Behzad", "Muzammil", ""]]}, {"id": "1805.00500", "submitter": "Jeremiah Johnson", "authors": "Jeremiah W. Johnson", "title": "Adapting Mask-RCNN for Automatic Nucleus Segmentation", "comments": "7 pages, 3 figures", "journal-ref": "Proceedings of the 2019 Computer Vision Conference, Vol. 2", "doi": "10.1007/978-3-030-17798-0", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of microscopy images is an important task in medical\nimage processing and analysis. Nucleus detection is an important example of\nthis task. Mask-RCNN is a recently proposed state-of-the-art algorithm for\nobject detection, object localization, and object instance segmentation of\nnatural images. In this paper we demonstrate that Mask-RCNN can be used to\nperform highly effective and efficient automatic segmentations of a wide range\nof microscopy images of cell nuclei, for a variety of cells acquired under a\nvariety of conditions.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 18:11:38 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Johnson", "Jeremiah W.", ""]]}, {"id": "1805.00503", "submitter": "Jessica Lee", "authors": "Xinyu Guan, Jessica Lee, Peter Wu, Yue Wu", "title": "Machine Learning for Exam Triage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we extend the state-of-the-art CheXNet (Rajpurkar et al.\n[2017]) by making use of the additional non-image features in the dataset. Our\nmodel produced better AUROC scores than the original CheXNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 03:49:22 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Guan", "Xinyu", ""], ["Lee", "Jessica", ""], ["Wu", "Peter", ""], ["Wu", "Yue", ""]]}, {"id": "1805.00506", "submitter": "Cheng Peng", "authors": "Cheng Peng and Volkan Isler", "title": "Adaptive View Planning for Aerial 3D Reconstruction", "comments": null, "journal-ref": "The 2019 International Conference on Robotics and Automation\n  (ICRA)", "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of small aerial vehicles, acquiring close up aerial\nimagery for high quality reconstruction of complex scenes is gaining\nimportance. We present an adaptive view planning method to collect such images\nin an automated fashion. We start by sampling a small set of views to build a\ncoarse proxy to the scene. We then present (i)~a method that builds a view\nmanifold for view selection, and (ii) an algorithm to select a sparse set of\nviews. The vehicle then visits these viewpoints to cover the scene, and the\nprocedure is repeated until reconstruction quality converges or a desired level\nof quality is achieved. The view manifold provides an effective\nefficiency/quality compromise between using the entire 6 degree of freedom pose\nspace and using a single view hemisphere to select the views.\n  Our results show that, in contrast to existing \"explore and exploit\" methods\nwhich collect only two sets of views, reconstruction quality can be drastically\nimproved by adding a third set. They also indicate that three rounds of data\ncollection is sufficient even for very complex scenes. We compare our algorithm\nto existing methods in three challenging scenes. We require each algorithm to\nselect the same number of views. Our algorithm generates views which produce\nthe least reconstruction error.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 18:28:23 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 18:28:48 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Peng", "Cheng", ""], ["Isler", "Volkan", ""]]}, {"id": "1805.00528", "submitter": "Yu Li", "authors": "Yu Li, Hu Wang, Kangjia Mo, Tao Zeng", "title": "Reconstruction of Simulation-Based Physical Field by Reconstruction\n  Neural Network Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of modeling techniques have been developed in the past decade to\nreduce the computational expense and improve the accuracy of modeling. In this\nstudy, a new framework of modeling is suggested. Compared with other popular\nmethods, a distinctive characteristic is \"from image based model to analysis\nbased model (e.g. stress, strain, and deformation)\". In such a framework, a\nreconstruction neural network (ReConNN) model designed for simulation-based\nphysical field's reconstruction is proposed. The ReConNN contains two submodels\nthat are convolutional neural network (CNN) and generative adversarial net-work\n(GAN). The CNN is employed to construct the mapping between contour images of\nphysical field and objective function. Subsequently, the GAN is utilized to\ngenerate more images which are similar to the existing contour images. Finally,\nLagrange polynomial is applied to complete the reconstruction. However, the\nexisting CNN models are commonly applied to the classification tasks, which\nseem to be difficult to handle with regression tasks of images. Meanwhile, the\nexisting GAN architectures are insufficient to generate high-accuracy \"pseudo\ncontour images\". Therefore, a ReConNN model based on a Convolution in\nConvolution (CIC) and a Convolutional AutoEncoder based on Wasserstein\nGenerative Adversarial Network (WGAN-CAE) is suggested. To evaluate the\nperformance of the proposed model representatively, a classical topology\noptimization procedure is considered. Then the ReConNN is utilized to the\nreconstruction of heat transfer process of a pin fin heat sink. It demonstrates\nthat the proposed ReConNN model is proved to be a potential capability to\nreconstruct physical field for multidisciplinary, such as structural\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 08:17:08 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 00:57:23 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 02:32:12 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Li", "Yu", ""], ["Wang", "Hu", ""], ["Mo", "Kangjia", ""], ["Zeng", "Tao", ""]]}, {"id": "1805.00545", "submitter": "Zhiyuan Fang", "authors": "Zhiyuan Fang, Shu Kong, Tianshu Yu, Yezhou Yang", "title": "Weakly Supervised Attention Learning for Textual Phrases Grounding", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding textual phrases in visual content is a meaningful yet challenging\nproblem with various potential applications such as image-text inference or\ntext-driven multimedia interaction. Most of the current existing methods adopt\nthe supervised learning mechanism which requires ground-truth at pixel level\nduring training. However, fine-grained level ground-truth annotation is quite\ntime-consuming and severely narrows the scope for more general applications. In\nthis extended abstract, we explore methods to localize flexibly image regions\nfrom the top-down signal (in a form of one-hot label or natural languages) with\na weakly supervised attention learning mechanism. In our model, two types of\nmodules are utilized: a backbone module for visual feature capturing, and an\nattentive module generating maps based on regularized bilinear pooling. We\nconstruct the model in an end-to-end fashion which is trained by encouraging\nthe spatial attentive map to shift and focus on the region that consists of the\nbest matched visual features with the top-down signal. We demonstrate the\npreliminary yet promising results on a testbed that is synthesized with\nmulti-label MNIST data.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 20:34:37 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Kong", "Shu", ""], ["Yu", "Tianshu", ""], ["Yang", "Yezhou", ""]]}, {"id": "1805.00553", "submitter": "Vivek Singh", "authors": "Brian Teixeira, Vivek Singh, Terrence Chen, Kai Ma, Birgi Tamersoy,\n  Yifan Wu, Elena Balashova, and Dorin Comaniciu", "title": "Generating Synthetic X-ray Images of a Person from the Surface Geometry", "comments": "accepted for spotlight presentation at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework that learns to predict human anatomy from body\nsurface. Specifically, our approach generates a synthetic X-ray image of a\nperson only from the person's surface geometry. Furthermore, the synthetic\nX-ray image is parametrized and can be manipulated by adjusting a set of body\nmarkers which are also generated during the X-ray image prediction. With the\nproposed framework, multiple synthetic X-ray images can easily be generated by\nvarying surface geometry. By perturbing the parameters, several additional\nsynthetic X-ray images can be generated from the same surface geometry. As a\nresult, our approach offers a potential to overcome the training data barrier\nin the medical domain. This capability is achieved by learning a pair of\nnetworks - one learns to generate the full image from the partial image and a\nset of parameters, and the other learns to estimate the parameters given the\nfull image. During training, the two networks are trained iteratively such that\nthey would converge to a solution where the predicted parameters and the full\nimage are consistent with each other. In addition to medical data enrichment,\nour framework can also be used for image completion as well as anomaly\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 21:07:30 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 14:21:32 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Teixeira", "Brian", ""], ["Singh", "Vivek", ""], ["Chen", "Terrence", ""], ["Ma", "Kai", ""], ["Tamersoy", "Birgi", ""], ["Wu", "Yifan", ""], ["Balashova", "Elena", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1805.00577", "submitter": "Vishnu Naresh Boddeti", "authors": "Vishnu Naresh Boddeti", "title": "Secure Face Matching Using Fully Homomorphic Encryption", "comments": "BTAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition technology has demonstrated tremendous progress over the\npast few years, primarily due to advances in representation learning. As we\nwitness the widespread adoption of these systems, it is imperative to consider\nthe security of face representations. In this paper, we explore the\npracticality of using a fully homomorphic encryption based framework to secure\na database of face templates. This framework is designed to preserve the\nprivacy of users and prevent information leakage from the templates, while\nmaintaining their utility through template matching directly in the encrypted\ndomain. Additionally, we also explore a batching and dimensionality reduction\nscheme to trade-off face matching accuracy and computational complexity.\nExperiments on benchmark face datasets (LFW, IJB-A, IJB-B, CASIA) indicate that\nsecure face matching can be practically feasible (16 KB template size and 0.01\nsec per match pair for 512-dimensional features from SphereFace) while\nexhibiting minimal loss in matching performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 23:46:41 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 19:19:00 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Boddeti", "Vishnu Naresh", ""]]}, {"id": "1805.00587", "submitter": "Chenyu You", "authors": "Chenyu You and Qingsong Yang and Hongming Shan and Lars Gjesteby and\n  Guang Li and Shenghong Ju and Zhuiyang Zhang and Zhen Zhao and Yi Zhang and\n  Wenxiang Cong and Ge Wang", "title": "Structure-sensitive Multi-scale Deep Neural Network for Low-Dose CT\n  Denoising", "comments": "IEEE Access 2018", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2858196", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) is a popular medical imaging modality in clinical\napplications. At the same time, the x-ray radiation dose associated with CT\nscans raises public concerns due to its potential risks to the patients. Over\nthe past years, major efforts have been dedicated to the development of\nLow-Dose CT (LDCT) methods. However, the radiation dose reduction compromises\nthe signal-to-noise ratio (SNR), leading to strong noise and artifacts that\ndown-grade CT image quality. In this paper, we propose a novel 3D noise\nreduction method, called Structure-sensitive Multi-scale Generative Adversarial\nNet (SMGAN), to improve the LDCT image quality. Specifically, we incorporate\nthree-dimensional (3D) volumetric information to improve the image quality.\nAlso, different loss functions for training denoising models are investigated.\nExperiments show that the proposed method can effectively preserve structural\nand texture information from normal-dose CT (NDCT) images, and significantly\nsuppress noise and artifacts. Qualitative visual assessments by three\nexperienced radiologists demonstrate that the proposed method retrieves more\ndetailed information, and outperforms competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 00:37:05 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 04:42:53 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 06:36:06 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["You", "Chenyu", ""], ["Yang", "Qingsong", ""], ["Shan", "Hongming", ""], ["Gjesteby", "Lars", ""], ["Li", "Guang", ""], ["Ju", "Shenghong", ""], ["Zhang", "Zhuiyang", ""], ["Zhao", "Zhen", ""], ["Zhang", "Yi", ""], ["Cong", "Wenxiang", ""], ["Wang", "Ge", ""]]}, {"id": "1805.00597", "submitter": "Wen Tang", "authors": "Wen Tang, Ashkan Panahi, Hamid Krim, Liyi Dai", "title": "Structured Analysis Dictionary Learning for Image Classification", "comments": "This is the final version accepted by ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient and high-performance classification\nalgorithm by incorporating class structural information in analysis dictionary\nlearning. To achieve more consistent classification, we associate a class\ncharacteristic structure of independent subspaces and impose it on the\nclassification error constrained analysis dictionary learning. Experiments\ndemonstrate that our method achieves a comparable or better performance than\nthe state-of-the-art algorithms in a variety of visual classification tasks. In\naddition, our method greatly reduces the training and testing computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 01:45:26 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Tang", "Wen", ""], ["Panahi", "Ashkan", ""], ["Krim", "Hamid", ""], ["Dai", "Liyi", ""]]}, {"id": "1805.00603", "submitter": "Ze Peng", "authors": "Jing Wang, Ze Peng, Pei Lv, Junyi Sun, Bing Zhou, Mingliang Xu", "title": "Bi-directional Graph Structure Information Model for Multi-Person Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-stage network architecture with two\nbranches in each stage to estimate multi-person poses in images. The first\nbranch predicts the confidence maps of joints and uses a geometrical transform\nkernel to propagate information between neighboring joints at the confidence\nlevel. The second branch proposes a bi-directional graph structure information\nmodel (BGSIM) to encode rich contextual information and to infer the occlusion\nrelationship among different joints. We dynamically determine the joint point\nwith highest response of the confidence maps as base point of passing message\nin BGSIM. Based on the proposed network structure, we achieve an average\nprecision of 62.9 on the COCO Keypoint Challenge dataset and 77.6 on the MPII\n(multi-person) dataset. Compared with other state-of-art methods, our method\ncan achieve highly promising results on our selected multi-person dataset\nwithout extra training.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 02:25:28 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 11:55:59 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Jing", ""], ["Peng", "Ze", ""], ["Lv", "Pei", ""], ["Sun", "Junyi", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "1805.00611", "submitter": "Bangjie Yin", "authors": "Bangjie Yin, Luan Tran, Haoxiang Li, Xiaohui Shen, Xiaoming Liu", "title": "Towards Interpretable Face Recognition", "comments": "10 pages, 9 figures, 6 tables, To appear in ICCV 2019 as an oral\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CNNs have been pushing the frontier of visual recognition over past\nyears. Besides recognition accuracy, strong demands in understanding deep CNNs\nin the research community motivate developments of tools to dissect pre-trained\nmodels to visualize how they make predictions. Recent works further push the\ninterpretability in the network learning stage to learn more meaningful\nrepresentations. In this work, focusing on a specific area of visual\nrecognition, we report our efforts towards interpretable face recognition. We\npropose a spatial activation diversity loss to learn more structured face\nrepresentations. By leveraging the structure, we further design a feature\nactivation diversity loss to push the interpretable representations to be\ndiscriminative and robust to occlusions. We demonstrate on three face\nrecognition benchmarks that our proposed method is able to improve face\nrecognition accuracy with easily interpretable face representations.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 03:46:47 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 17:02:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yin", "Bangjie", ""], ["Tran", "Luan", ""], ["Li", "Haoxiang", ""], ["Shen", "Xiaohui", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1805.00613", "submitter": "Seyed Hamid Rezatofighi", "authors": "S. Hamid Rezatofighi, Roman Kaskman, Farbod T. Motlagh, Qinfeng Shi,\n  Daniel Cremers, Laura Leal-Taix\\'e and Ian Reid", "title": "Deep Perm-Set Net: Learn to predict sets with unknown permutation and\n  cardinality using deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems, e.g. object detection, have outputs that are\nnaturally expressed as sets of entities. This creates a challenge for\ntraditional deep neural networks which naturally deal with structured outputs\nsuch as vectors, matrices or tensors. We present a novel approach for learning\nto predict sets with unknown permutation and cardinality using deep neural\nnetworks. Specifically, in our formulation we incorporate the permutation as\nunobservable variable and estimate its distribution during the learning process\nusing alternating optimization. We demonstrate the validity of this new\nformulation on two relevant vision problems: object detection, for which our\nformulation outperforms state-of-the-art detectors such as Faster R-CNN and\nYOLO, and a complex CAPTCHA test, where we observe that, surprisingly, our set\nbased network acquired the ability of mimicking arithmetics without any rules\nbeing coded.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 03:49:39 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 05:52:55 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 00:45:14 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2018 17:05:03 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Rezatofighi", "S. Hamid", ""], ["Kaskman", "Roman", ""], ["Motlagh", "Farbod T.", ""], ["Shi", "Qinfeng", ""], ["Cremers", "Daniel", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Reid", "Ian", ""]]}, {"id": "1805.00625", "submitter": "Didan Deng", "authors": "Didan Deng, Yuqian Zhou, Jimin Pi, Bertram E.Shi", "title": "Multimodal Utterance-level Affect Analysis using Visual, Audio and Text\n  Features", "comments": "5 pages, 1 figure, subject to the 2018 IJCNN challenge on One-Minute\n  Gradual-Emotion Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of information across multiple modalities and across time is\na promising way to enhance the emotion recognition performance of affective\nsystems. Much previous work has focused on instantaneous emotion recognition.\nThe 2018 One-Minute Gradual-Emotion Recognition (OMG-Emotion) challenge, which\nwas held in conjunction with the IEEE World Congress on Computational\nIntelligence, encouraged participants to address long-term emotion recognition\nby integrating cues from multiple modalities, including facial expression,\naudio and language. Intuitively, a multi-modal inference network should be able\nto leverage information from each modality and their correlations to improve\nrecognition over that achievable by a single modality network. We describe here\na multi-modal neural architecture that integrates visual information over time\nusing an LSTM, and combines it with utterance level audio and text cues to\nrecognize human sentiment from multimodal clips. Our model outperforms the\nunimodal baseline, achieving the concordance correlation coefficients (CCC) of\n0.400 on the arousal task, and 0.353 on the valence task.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 05:05:32 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 11:24:41 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Deng", "Didan", ""], ["Zhou", "Yuqian", ""], ["Pi", "Jimin", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1805.00632", "submitter": "Luis Carlos Garcia-Peraza-Herrera", "authors": "Luis C. Garcia-Peraza-Herrera, Martin Everson, Wenqi Li, Inmanol\n  Luengo, Lorenz Berger, Omer Ahmad, Laurence Lovat, Hsiu-Po Wang, Wen-Lun\n  Wang, Rehan Haidry, Danail Stoyanov, Tom Vercauteren, Sebastien Ourselin", "title": "Interpretable Fully Convolutional Classification of Intrapapillary\n  Capillary Loops for Real-Time Detection of Early Squamous Neoplasia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have concentrated our efforts on the interpretability of\nclassification results coming from a fully convolutional neural network.\nMotivated by the classification of oesophageal tissue for real-time detection\nof early squamous neoplasia, the most frequent kind of oesophageal cancer in\nAsia, we present a new dataset and a novel deep learning method that by means\nof deep supervision and a newly introduced concept, the embedded Class\nActivation Map (eCAM), focuses on the interpretability of results as a design\nconstraint of a convolutional network. We present a new approach to visualise\nattention that aims to give some insights on those areas of the oesophageal\ntissue that lead a network to conclude that the images belong to a particular\nclass and compare them with those visual features employed by clinicians to\nproduce a clinical diagnosis. In comparison to a baseline method which does not\nfeature deep supervision but provides attention by grafting Class Activation\nMaps, we improve the F1-score from 87.3% to 92.7% and provide more detailed\nattention maps.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 05:28:46 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Garcia-Peraza-Herrera", "Luis C.", ""], ["Everson", "Martin", ""], ["Li", "Wenqi", ""], ["Luengo", "Inmanol", ""], ["Berger", "Lorenz", ""], ["Ahmad", "Omer", ""], ["Lovat", "Laurence", ""], ["Wang", "Hsiu-Po", ""], ["Wang", "Wen-Lun", ""], ["Haidry", "Rehan", ""], ["Stoyanov", "Danail", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1805.00638", "submitter": "Songyou Peng", "authors": "Songyou Peng, Le Zhang, Yutong Ban, Meng Fang, Stefan Winkler", "title": "A Deep Network for Arousal-Valence Emotion Prediction with\n  Acoustic-Visual Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we comprehensively describe the methodology of our submissions\nto the One-Minute Gradual-Emotion Behavior Challenge 2018.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 06:03:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 05:11:09 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Peng", "Songyou", ""], ["Zhang", "Le", ""], ["Ban", "Yutong", ""], ["Fang", "Meng", ""], ["Winkler", "Stefan", ""]]}, {"id": "1805.00652", "submitter": "Irtiza Hasan", "authors": "Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Alessio Del Bue,\n  Fabio Galasso and Marco Cristani", "title": "MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories\n  and head poses", "comments": "10 pages, 3 figures to appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches on trajectory forecasting use tracklets to predict the\nfuture positions of pedestrians exploiting Long Short Term Memory (LSTM)\narchitectures. This paper shows that adding vislets, that is, short sequences\nof head pose estimations, allows to increase significantly the trajectory\nforecasting performance. We then propose to use vislets in a novel framework\ncalled MX-LSTM, capturing the interplay between tracklets and vislets thanks to\na joint unconstrained optimization of full covariance matrices during the LSTM\nbackpropagation. At the same time, MX-LSTM predicts the future head poses,\nincreasing the standard capabilities of the long-term trajectory forecasting\napproaches. With standard head pose estimators and an attentional-based social\npooling, MX-LSTM scores the new trajectory forecasting state-of-the-art in all\nthe considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic\nmargin when the pedestrians slow down, a case where most of the forecasting\napproaches struggle to provide an accurate solution.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:21:45 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Hasan", "Irtiza", ""], ["Setti", "Francesco", ""], ["Tsesmelis", "Theodore", ""], ["Del Bue", "Alessio", ""], ["Galasso", "Fabio", ""], ["Cristani", "Marco", ""]]}, {"id": "1805.00655", "submitter": "Chen Li", "authors": "Chen Li, Zhen Zhang, Wee Sun Lee, Gim Hee Lee", "title": "Convolutional Sequence to Sequence Model for Human Dynamics", "comments": "CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion modeling is a classic problem in computer vision and graphics.\nChallenges in modeling human motion include high dimensional prediction as well\nas extremely complicated dynamics.We present a novel approach to human motion\nmodeling based on convolutional neural networks (CNN). The hierarchical\nstructure of CNN makes it capable of capturing both spatial and temporal\ncorrelations effectively. In our proposed approach,a convolutional long-term\nencoder is used to encode the whole given motion sequence into a long-term\nhidden variable, which is used with a decoder to predict the remainder of the\nsequence. The decoder itself also has an encoder-decoder structure, in which\nthe short-term encoder encodes a shorter sequence to a short-term hidden\nvariable, and the spatial decoder maps the long and short-term hidden variable\nto motion predictions. By using such a model, we are able to capture both\ninvariant and dynamic information of human motion, which results in more\naccurate predictions. Experiments show that our algorithm outperforms the\nstate-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our\ncode is available at the project website.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:42:04 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Li", "Chen", ""], ["Zhang", "Zhen", ""], ["Lee", "Wee Sun", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1805.00676", "submitter": "Cristian Bodnar", "authors": "Cristian Bodnar", "title": "Text to Image Synthesis Using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.35817.39523", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images from natural language is one of the primary applications of\nrecent conditional generative models. Besides testing our ability to model\nconditional, highly dimensional distributions, text to image synthesis has many\nexciting and practical applications such as photo editing or computer-aided\ncontent creation. Recent progress has been made using Generative Adversarial\nNetworks (GANs). This material starts with a gentle introduction to these\ntopics and discusses the existent state of the art models. Moreover, I propose\nWasserstein GAN-CLS, a new model for conditional image generation based on the\nWasserstein distance which offers guarantees of stability. Then, I show how the\nnovel loss function of Wasserstein GAN-CLS can be used in a Conditional\nProgressive Growing GAN. In combination with the proposed loss, the model\nboosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the\nmodels which use only the sentence-level visual semantics. The only model which\nperforms better than the Conditional Wasserstein Progressive Growing GAN is the\nrecently proposed AttnGAN which uses word-level visual semantics as well.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 08:47:38 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Bodnar", "Cristian", ""]]}, {"id": "1805.00721", "submitter": "Duygu Sarikaya", "authors": "Duygu Sarikaya, Khurshid A. Guru, Jason J. Corso", "title": "Joint Surgical Gesture and Task Classification with Multi-Task and\n  Multimodal Learning", "comments": "Keywords Robot-Assisted Surgery, Surgical Gesture Classification,\n  Multi-task Learning, Multimodal Learning, Long Short-term Recurrent Neural\n  Networks, Convolutional Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-modal and multi-task architecture for simultaneous\nlow level gesture and surgical task classification in Robot Assisted Surgery\n(RAS) videos.Our end-to-end architecture is based on the principles of a long\nshort-term memory network (LSTM) that jointly learns temporal dynamics on rich\nrepresentations of visual and motion features, while simultaneously classifying\nactivities of low-level gestures and surgical tasks. Our experimental results\nshow that our approach is superior compared to an ar- chitecture that\nclassifies the gestures and surgical tasks separately on visual cues and motion\ncues respectively. We train our model on a fixed random set of 1200 gesture\nvideo segments and use the rest 422 for testing. This results in around 42,000\ngesture frames sampled for training and 14,500 for testing. For a 6 split\nexperimentation, while the conventional approach reaches an Average Precision\n(AP) of only 29% (29.13%), our architecture reaches an AP of 51% (50.83%) for 3\ntasks and 14 possible gesture labels, resulting in an improvement of 22%\n(21.7%). Our architecture learns temporal dynamics on rich representations of\nvisual and motion features that compliment each other for classification of\nlow-level gestures and surgical tasks. Its multi-task learning nature makes use\nof learned joint re- lationships and combinations of shared and task-specific\nrepresentations. While benchmark studies focus on recognizing gestures that\ntake place under specific tasks, we focus on recognizing common gestures that\nreoccur across different tasks and settings and significantly perform better\ncompared to conventional architectures.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 10:43:12 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Sarikaya", "Duygu", ""], ["Guru", "Khurshid A.", ""], ["Corso", "Jason J.", ""]]}, {"id": "1805.00780", "submitter": "Maren Awiszus", "authors": "Maren Awiszus, Stella Gra{\\ss}hof, Felix Kuhnke, J\\\"orn Ostermann", "title": "Unsupervised Features for Facial Expression Intensity Estimation over\n  Time", "comments": "Accepted for CVPR 2018 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of facial shapes and motions among persons is one of the\ngreatest challenges for automatic analysis of facial expressions. In this\npaper, we propose a feature describing expression intensity over time, while\nbeing invariant to person and the type of performed expression. Our feature is\na weighted combination of the dynamics of multiple points adapted to the\noverall expression trajectory. We evaluate our method on several tasks all\nrelated to temporal analysis of facial expression. The proposed feature is\ncompared to a state-of-the-art method for expression intensity estimation,\nwhich it outperforms. We use our proposed feature to temporally align multiple\nsequences of recorded 3D facial expressions. Furthermore, we show how our\nfeature can be used to reveal person-specific differences in performances of\nfacial expressions. Additionally, we apply our feature to identify the local\nchanges in face video sequences based on action unit labels. For all the\nexperiments our feature proves to be robust against noise and outliers, making\nit applicable to a variety of applications for analysis of facial movements.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 13:12:05 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 07:15:26 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Awiszus", "Maren", ""], ["Gra\u00dfhof", "Stella", ""], ["Kuhnke", "Felix", ""], ["Ostermann", "J\u00f6rn", ""]]}, {"id": "1805.00833", "submitter": "Arsha Nagrani", "authors": "Arsha Nagrani and Samuel Albanie and Andrew Zisserman", "title": "Learnable PINs: Cross-Modal Embeddings for Person Identity", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and investigate an identity sensitive joint embedding of face and\nvoice. Such an embedding enables cross-modal retrieval from voice to face and\nfrom face to voice. We make the following four contributions: first, we show\nthat the embedding can be learnt from videos of talking faces, without\nrequiring any identity labels, using a form of cross-modal self-supervision;\nsecond, we develop a curriculum learning schedule for hard negative mining\ntargeted to this task, that is essential for learning to proceed successfully;\nthird, we demonstrate and evaluate cross-modal retrieval for identities unseen\nand unheard during training over a number of scenarios and establish a\nbenchmark for this novel task; finally, we show an application of using the\njoint embedding for automatically retrieving and labelling characters in TV\ndramas.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 14:13:26 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 11:54:03 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Nagrani", "Arsha", ""], ["Albanie", "Samuel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1805.00862", "submitter": "Hadrien Van Lierde", "authors": "H. Van Lierde (1), T. W. S. Chow (1), J.-C. Delvenne (2) ((1) City\n  University of Hong Kong, (2) Universite Catholique de Louvain)", "title": "Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs", "comments": "This is the unrefereed Author's Original Version of the article. A\n  peer-reviewed version has been accepted for publication in the Journal of\n  Complex Networks published by Oxford University Press. The present version is\n  not the Accepted Manuscript", "journal-ref": "Journal of Complex Networks, cny011 (2018)", "doi": "10.1093/comnet/cny011", "report-no": null, "categories": "cs.DS cs.CV cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 15:19:49 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Van Lierde", "H.", ""], ["Chow", "T. W. S.", ""], ["Delvenne", "J. -C.", ""]]}, {"id": "1805.00900", "submitter": "Micael Carvalho", "authors": "Micael Carvalho, R\\'emi Cad\\`ene, David Picard, Laure Soulier,\n  Matthieu Cord", "title": "Images & Recipes: Retrieval in the cooking context", "comments": "Published at DECOR / ICDE 2018. Extended version accepted at SIGIR\n  2018, available here: arXiv:1804.11146", "journal-ref": null, "doi": "10.1109/ICDEW.2018.00035", "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the machine learning community allowed different use cases\nto emerge, as its association to domains like cooking which created the\ncomputational cuisine. In this paper, we tackle the picture-recipe alignment\nproblem, having as target application the large-scale retrieval task (finding a\nrecipe given a picture, and vice versa). Our approach is validated on the\nRecipe1M dataset, composed of one million image-recipe pairs and additional\nclass information, for which we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 16:34:01 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Carvalho", "Micael", ""], ["Cad\u00e8ne", "R\u00e9mi", ""], ["Picard", "David", ""], ["Soulier", "Laure", ""], ["Cord", "Matthieu", ""]]}, {"id": "1805.00911", "submitter": "Tarang Chugh", "authors": "Elham Tabassi, Tarang Chugh, Debayan Deb, and Anil K. Jain", "title": "Altered Fingerprints: Detection and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint alteration, also referred to as obfuscation presentation attack,\nis to intentionally tamper or damage the real friction ridge patterns to avoid\nidentification by an AFIS. This paper proposes a method for detection and\nlocalization of fingerprint alterations. Our main contributions are: (i) design\nand train CNN models on fingerprint images and minutiae-centered local patches\nin the image to detect and localize regions of fingerprint alterations, and\n(ii) train a Generative Adversarial Network (GAN) to synthesize altered\nfingerprints whose characteristics are similar to true altered fingerprints. A\nsuccessfully trained GAN can alleviate the limited availability of altered\nfingerprint images for research. A database of 4,815 altered fingerprints from\n270 subjects, and an equal number of rolled fingerprint images are used to\ntrain and test our models. The proposed approach achieves a True Detection Rate\n(TDR) of 99.24% at a False Detection Rate (FDR) of 2%, outperforming published\nresults. The synthetically generated altered fingerprint dataset will be\nopen-sourced.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 17:16:18 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 18:54:15 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Tabassi", "Elham", ""], ["Chugh", "Tarang", ""], ["Deb", "Debayan", ""], ["Jain", "Anil K.", ""]]}, {"id": "1805.00930", "submitter": "Xiaoxiao Du", "authors": "Xiaoxiao Du and Alina Zare", "title": "Multi-Resolution Multi-Modal Sensor Fusion For Remote Sensing Data With\n  Label Uncertainty", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2955320", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing, each sensor can provide complementary or reinforcing\ninformation. It is valuable to fuse outputs from multiple sensors to boost\noverall performance. Previous supervised fusion methods often require accurate\nlabels for each pixel in the training data. However, in many remote sensing\napplications, pixel-level labels are difficult or infeasible to obtain. In\naddition, outputs from multiple sensors often have different resolution or\nmodalities. For example, rasterized hyperspectral imagery presents data in a\npixel grid while airborne Light Detection and Ranging (LiDAR) generates dense\nthree-dimensional (3D) point clouds. It is often difficult to directly fuse\nsuch multi-modal, multi-resolution data. To address these challenges, we\npresent a novel Multiple Instance Multi-Resolution Fusion (MIMRF) framework\nthat can fuse multi-resolution and multi-modal sensor outputs while learning\nfrom automatically-generated, imprecisely-labeled data. Experiments were\nconducted on the MUUFL Gulfport hyperspectral and LiDAR data set and a\nremotely-sensed soybean and weed data set. Results show improved, consistent\nperformance on scene understanding and agricultural applications when compared\nto traditional fusion methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 17:51:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 19:00:17 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Du", "Xiaoxiao", ""], ["Zare", "Alina", ""]]}, {"id": "1805.00932", "submitter": "Ross Girshick", "authors": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar\n  Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten", "title": "Exploring the Limits of Weakly Supervised Pretraining", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art visual perception models for a wide range of tasks rely on\nsupervised pretraining. ImageNet classification is the de facto pretraining\ntask for these models. Yet, ImageNet is now nearly ten years old and is by\nmodern standards \"small\". Even so, relatively little is known about the\nbehavior of pretraining with datasets that are multiple orders of magnitude\nlarger. The reasons are obvious: such datasets are difficult to collect and\nannotate. In this paper, we present a unique study of transfer learning with\nlarge convolutional networks trained to predict hashtags on billions of social\nmedia images. Our experiments demonstrate that training for large-scale hashtag\nprediction leads to excellent results. We show improvements on several image\nclassification and object detection tasks, and report the highest ImageNet-1k\nsingle-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform\nextensive experiments that provide novel empirical data on the relationship\nbetween large-scale pretraining and transfer learning performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 17:57:16 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Mahajan", "Dhruv", ""], ["Girshick", "Ross", ""], ["Ramanathan", "Vignesh", ""], ["He", "Kaiming", ""], ["Paluri", "Manohar", ""], ["Li", "Yixuan", ""], ["Bharambe", "Ashwin", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1805.00980", "submitter": "Safa Cicek", "authors": "Safa Cicek, Alhussein Fawzi and Stefano Soatto", "title": "SaaS: Speed as a Supervisor for Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the SaaS Algorithm for semi-supervised learning, which uses\nlearning speed during stochastic gradient descent in a deep neural network to\nmeasure the quality of an iterative estimate of the posterior probability of\nunknown labels. Training speed in supervised learning correlates strongly with\nthe percentage of correct labels, so we use it as an inference criterion for\nthe unknown labels, without attempting to infer the model parameters at first.\nDespite its simplicity, SaaS achieves state-of-the-art results in\nsemi-supervised learning benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 18:52:18 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Cicek", "Safa", ""], ["Fawzi", "Alhussein", ""], ["Soatto", "Stefano", ""]]}, {"id": "1805.01006", "submitter": "Lukas F. Lang", "authors": "Lukas F. Lang", "title": "A Numerical Framework for Efficient Motion Estimation on Evolving\n  Sphere-Like Surfaces based on Brightness and Mass Conservation Laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider brightness and mass conservation laws for motion\nestimation on evolving Riemannian 2-manifolds that allow for a radial\nparametrisation from the 2-sphere. While conservation of brightness constitutes\nthe foundation for optical flow methods and has been generalised to said\nscenario, we formulate in this article the principle of mass conservation for\ntime-varying surfaces which are embedded in Euclidean 3-space and derive a\ngeneralised continuity equation. The main motivation for this work is efficient\ncell motion estimation in time-lapse (4D) volumetric fluorescence microscopy\nimages of a living zebrafish embryo. Increasing spatial and temporal resolution\nof modern microscopes require efficient analysis of such data. With this\napplication in mind we address this need and follow an emerging paradigm in\nthis field: dimensional reduction. In light of the ill-posedness of considered\nconservation laws we employ Tikhonov regularisation and propose the use of\nspatially varying regularisation functionals that recover motion only in\nregions with cells. For the efficient numerical solution we devise a Galerkin\nmethod based on compactly supported (tangent) vectorial basis functions.\nFurthermore, for the fast and accurate estimation of the evolving sphere-like\nsurface from scattered data we utilise surface interpolation with\nspatio-temporal regularisation. We present numerical results based on\naforementioned zebrafish microscopy data featuring fluorescently labelled\ncells.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 20:30:01 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Lang", "Lukas F.", ""]]}, {"id": "1805.01024", "submitter": "Shu Kong", "authors": "Feng Zhou, Shu Kong, Charless Fowlkes, Tao Chen, Baiying Lei", "title": "Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model", "comments": "code: http://www.ics.uci.edu/~skong2/DimensionalEmotionModel.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated facial expression analysis has a variety of applications in\nhuman-computer interaction. Traditional methods mainly analyze prototypical\nfacial expressions of no more than eight discrete emotions as a classification\ntask. However, in practice, spontaneous facial expressions in naturalistic\nenvironment can represent not only a wide range of emotions, but also different\nintensities within an emotion family. In such situation, these methods are not\nreliable or adequate. In this paper, we propose to train deep convolutional\nneural networks (CNNs) to analyze facial expressions explainable in a\ndimensional emotion model. The proposed method accommodates not only a set of\nbasic emotion expressions, but also a full range of other emotions and subtle\nemotion intensities that we both feel in ourselves and perceive in others in\nour daily life. Specifically, we first mapped facial expressions into\ndimensional measures so that we transformed facial expression analysis from a\nclassification problem to a regression one. We then tested our CNN-based\nmethods for facial expression regression and these methods demonstrated\npromising performance. Moreover, we improved our method by a bilinear pooling\nwhich encodes second-order statistics of features. We showed such bilinear-CNN\nmodels significantly outperformed their respective baselines.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 21:08:47 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhou", "Feng", ""], ["Kong", "Shu", ""], ["Fowlkes", "Charless", ""], ["Chen", "Tao", ""], ["Lei", "Baiying", ""]]}, {"id": "1805.01026", "submitter": "Benjamin Hou", "authors": "Benjamin Hou, Nina Miolane, Bishesh Khanal, Matthew C.H. Lee, Amir\n  Alansary, Steven McDonagh, Jo V. Hajnal, Daniel Rueckert, Ben Glocker,\n  Bernhard Kainz", "title": "Computing CNN Loss and Gradients for Pose Estimation with Riemannian\n  Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation, i.e. predicting a 3D rigid transformation with respect to a\nfixed co-ordinate frame in, SE(3), is an omnipresent problem in medical image\nanalysis with applications such as: image rigid registration, anatomical\nstandard plane detection, tracking and device/camera pose estimation. Deep\nlearning methods often parameterise a pose with a representation that separates\nrotation and translation. As commonly available frameworks do not provide means\nto calculate loss on a manifold, regression is usually performed using the\nL2-norm independently on the rotation's and the translation's\nparameterisations, which is a metric for linear spaces that does not take into\naccount the Lie group structure of SE(3). In this paper, we propose a general\nRiemannian formulation of the pose estimation problem. We propose to train the\nCNN directly on SE(3) equipped with a left-invariant Riemannian metric,\ncoupling the prediction of the translation and rotation defining the pose. At\neach training step, the ground truth and predicted pose are elements of the\nmanifold, where the loss is calculated as the Riemannian geodesic distance. We\nthen compute the optimisation direction by back-propagating the gradient with\nrespect to the predicted pose on the tangent space of the manifold SE(3) and\nupdate the network weights. We thoroughly evaluate the effectiveness of our\nloss function by comparing its performance with popular and most commonly used\nexisting methods, on tasks such as image-based localisation and intensity-based\n2D/3D registration. We also show that hyper-parameters, used in our loss\nfunction to weight the contribution between rotations and translations, can be\nintrinsically calculated from the dataset to achieve greater performance\nmargins.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 21:17:03 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 21:07:05 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 13:38:12 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Hou", "Benjamin", ""], ["Miolane", "Nina", ""], ["Khanal", "Bishesh", ""], ["Lee", "Matthew C. H.", ""], ["Alansary", "Amir", ""], ["McDonagh", "Steven", ""], ["Hajnal", "Jo V.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1805.01033", "submitter": "Qun Liu", "authors": "Qun Liu, Supratik Mukhopadhyay", "title": "Unsupervised Learning using Pretrained CNN and Associative Memory Bank", "comments": "Paper was accepted at the 2018 International Joint Conference on\n  Neural Networks (IJCNN 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional features extracted from a comprehensive labeled dataset,\ncontain substantial representations which could be effectively used in a new\ndomain. Despite the fact that generic features achieved good results in many\nvisual tasks, fine-tuning is required for pretrained deep CNN models to be more\neffective and provide state-of-the-art performance. Fine tuning using the\nbackpropagation algorithm in a supervised setting, is a time and resource\nconsuming process. In this paper, we present a new architecture and an approach\nfor unsupervised object recognition that addresses the above mentioned problem\nwith fine tuning associated with pretrained CNN-based supervised deep learning\napproaches while allowing automated feature extraction. Unlike existing works,\nour approach is applicable to general object recognition tasks. It uses a\npretrained (on a related domain) CNN model for automated feature extraction\npipelined with a Hopfield network based associative memory bank for storing\npatterns for classification purposes. The use of associative memory bank in our\nframework allows eliminating backpropagation while providing competitive\nperformance on an unseen dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 21:32:08 GMT"}], "update_date": "2018-05-06", "authors_parsed": [["Liu", "Qun", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1805.01047", "submitter": "Sen Jia", "authors": "Sen Jia and Neil D. B. Bruce", "title": "EML-NET:An Expandable Multi-Layer NETwork for Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency prediction can benefit from training that involves scene\nunderstanding that may be tangential to the central task; this may include\nunderstanding places, spatial layout, objects or involve different datasets and\ntheir bias. One can combine models, but to do this in a sophisticated manner\ncan be complex, and also result in unwieldy networks or produce competing\nobjectives that are hard to balance. In this paper, we propose a scalable\nsystem to leverage multiple powerful deep CNN models to better extract visual\nfeatures for saliency prediction. Our design differs from previous studies in\nthat the whole system is trained in an almost end-to-end piece-wise fashion.\nThe encoder and decoder components are separately trained to deal with\ncomplexity tied to the computational paradigm and required space. Furthermore,\nthe encoder can contain more than one CNN model to extract features, and models\ncan have different architectures or be pre-trained on different datasets. This\nparallel design yields a better computational paradigm overcoming limits to the\nvariety of information or inference that can be combined at the encoder stage\ntowards deeper networks and a more powerful encoding. Our network can be easily\nexpanded almost without any additional cost, and other pre-trained CNN models\ncan be incorporated availing a wider range of visual knowledge. We denote our\nexpandable multi-layer network as EML-NET and our method achieves the\nstate-of-the-art results on the public saliency benchmarks, SALICON, MIT300 and\nCAT2000.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 22:32:12 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 04:16:04 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Jia", "Sen", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1805.01055", "submitter": "Vedhus Hoskere", "authors": "Vedhus Hoskere, Yasutaka Narazaki, Tu Hoang, BillieF Spencer Jr", "title": "Vision-based Structural Inspection using Multiscale Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods of practice for inspection of civil infrastructure typically\ninvolve visual assessments conducted manually by trained inspectors. For\npost-earthquake structural inspections, the number of structures to be\ninspected often far exceeds the capability of the available inspectors. The\nlabor intensive and time consuming natures of manual inspection have engendered\nresearch into development of algorithms for automated damage identification\nusing computer vision techniques. In this paper, a novel damage localization\nand classification technique based on a state of the art computer vision\nalgorithm is presented to address several key limitations of current computer\nvision techniques. The proposed algorithm carries out a pixel-wise\nclassification of each image at multiple scales using a deep convolutional\nneural network and can recognize 6 different types of damage. The resulting\noutput is a segmented image where the portion of the image representing damage\nis outlined and classified as one of the trained damage categories. The\nproposed method is evaluated in terms of pixel accuracy and the application of\nthe method to real world images is shown.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 23:29:34 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Hoskere", "Vedhus", ""], ["Narazaki", "Yasutaka", ""], ["Hoang", "Tu", ""], ["Spencer", "BillieF", "Jr"]]}, {"id": "1805.01060", "submitter": "Ziqi Zheng", "authors": "Ziqi Zheng, Chenjie Cao, Xingwei Chen, Guoqiang Xu", "title": "Multimodal Emotion Recognition for One-Minute-Gradual Emotion Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous dimensional emotion modelled by arousal and valence can depict\ncomplex changes of emotions. In this paper, we present our works on arousal and\nvalence predictions for One-Minute-Gradual (OMG) Emotion Challenge. Multimodal\nrepresentations are first extracted from videos using a variety of acoustic,\nvideo and textual models and support vector machine (SVM) is then used for\nfusion of multimodal signals to make final predictions. Our solution achieves\nConcordant Correlation Coefficient (CCC) scores of 0.397 and 0.520 on arousal\nand valence respectively for the validation dataset, which outperforms the\nbaseline systems with the best CCC scores of 0.15 and 0.23 on arousal and\nvalence by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 00:10:10 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zheng", "Ziqi", ""], ["Cao", "Chenjie", ""], ["Chen", "Xingwei", ""], ["Xu", "Guoqiang", ""]]}, {"id": "1805.01084", "submitter": "Yixin Du", "authors": "Yixin Du and Xin Li", "title": "Perceptually Optimized Generative Adversarial Network for Single Image\n  Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches towards single image dehazing including both model-based\nand learning-based heavily rely on the estimation of so-called transmission\nmaps. Despite its conceptual simplicity, using transmission maps as an\nintermediate step often makes it more difficult to optimize the perceptual\nquality of reconstructed images. To overcome this weakness, we propose a direct\ndeep learning approach toward image dehazing bypassing the step of transmission\nmap estimation and facilitating end-to-end perceptual optimization. Our\ntechnical contributions are mainly three-fold. First, based on the analogy\nbetween dehazing and denoising, we propose to directly learn a nonlinear\nmapping from the space of degraded images to that of haze-free ones via\nrecursive deep residual learning; Second, inspired by the success of generative\nadversarial networks (GAN), we propose to optimize the perceptual quality of\ndehazed images by introducing a discriminator and a loss function adaptive to\nhazy conditions; Third, we propose to remove notorious halo-like artifacts at\nlarge scene depth discontinuities by a novel application of guided filtering.\nExtensive experimental results have shown that the subjective qualities of\ndehazed images by the proposed perceptually optimized GAN (POGAN) are often\nmore favorable than those by existing state-of-the-art approaches especially\nwhen hazy condition varies.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 02:00:33 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Du", "Yixin", ""], ["Li", "Xin", ""]]}, {"id": "1805.01090", "submitter": "Hung Vu", "authors": "Hung Vu and Tu Dinh Nguyen and Dinh Phung", "title": "Detection of Unknown Anomalies in Streaming Videos with Generative\n  Energy-based Boltzmann Models", "comments": "This manuscript is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal event detection is one of the important objectives in research and\npractical applications of video surveillance. However, there are still three\nchallenging problems for most anomaly detection systems in practical setting:\nlimited labeled data, ambiguous definition of \"abnormal\" and expensive feature\nengineering steps. This paper introduces a unified detection framework to\nhandle these challenges using energy-based models, which are powerful tools for\nunsupervised representation learning. Our proposed models are firstly trained\non unlabeled raw pixels of image frames from an input video rather than\nhand-crafted visual features; and then identify the locations of abnormal\nobjects based on the errors between the input video and its reconstruction\nproduced by the models. To handle video stream, we develop an online version of\nour framework, wherein the model parameters are updated incrementally with the\nimage frames arriving on the fly. Our experiments show that our detectors,\nusing Restricted Boltzmann Machines (RBMs) and Deep Boltzmann Machines (DBMs)\nas core modules, achieve superior anomaly detection performance to unsupervised\nbaselines and obtain accuracy comparable with the state-of-the-art approaches\nwhen evaluating at the pixel-level. More importantly, we discover that our\nsystem trained with DBMs is able to simultaneously perform scene clustering and\nscene reconstruction. This capacity not only distinguishes our method from\nother existing detectors but also offers a unique tool to investigate and\nunderstand how the model works.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 02:47:55 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 07:11:20 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Vu", "Hung", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1805.01091", "submitter": "Ze Peng", "authors": "Pei Lv (Zhengzhou University), Meng Wang (Zhengzhou University),\n  Yongbo Xu (Zhengzhou University), Ze Peng (Zhengzhou University), Junyi Sun\n  (Zhengzhou University), Shimei Su (Zhengzhou University), Bing Zhou\n  (Zhengzhou University), Mingliang Xu (Zhengzhou University)", "title": "USAR: an Interactive User-specific Aesthetic Ranking Framework for\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing whether an image is of high or low quality, it is\nindispensable to take personal preference into account. Existing aesthetic\nmodels lay emphasis on hand-crafted features or deep features commonly shared\nby high quality images, but with limited or no consideration for personal\npreference and user interaction. To that end, we propose a novel and\nuser-friendly aesthetic ranking framework via powerful deep neural network and\na small amount of user interaction, which can automatically estimate and rank\nthe aesthetic characteristics of images in accordance with users' preference.\nOur framework takes as input a series of photos that users prefer, and produces\nas output a reliable, user-specific aesthetic ranking model matching with\nusers' preference. Considering the subjectivity of personal preference and the\nuncertainty of user's single selection, a unique and exclusive dataset will be\nconstructed interactively to describe the preference of one individual by\nretrieving the most similar images with regard to those specified by users.\nBased on this unique user-specific dataset and sufficient well-designed\naesthetic attributes, a customized aesthetic distribution model can be learned,\nwhich concatenates both personalized preference and aesthetic rules. We conduct\nextensive experiments and user studies on two large-scale public datasets, and\ndemonstrate that our framework outperforms those work based on conventional\naesthetic assessment or ranking model.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 02:55:49 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 01:59:39 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Lv", "Pei", "", "Zhengzhou University"], ["Wang", "Meng", "", "Zhengzhou University"], ["Xu", "Yongbo", "", "Zhengzhou University"], ["Peng", "Ze", "", "Zhengzhou University"], ["Sun", "Junyi", "", "Zhengzhou University"], ["Su", "Shimei", "", "Zhengzhou University"], ["Zhou", "Bing", "", "Zhengzhou University"], ["Xu", "Mingliang", "", "Zhengzhou University"]]}, {"id": "1805.01093", "submitter": "Alexander Wong", "authors": "Jason L. Deglint, Chao Jin, Angela Chao, and Alexander Wong", "title": "The feasibility of automated identification of six algae types using\n  neural networks and fluorescence-based spectral-morphological features", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harmful algae blooms (HABs), which produce lethal toxins, are a growing\nglobal concern since they negatively affect the quality of drinking water and\nhave major negative impact on wildlife, the fishing industry, as well as\ntourism and recreational water use. In this study, we investigate the\nfeasibility of leveraging machine learning and fluorescence-based\nspectral-morphological features to enable the identification of six different\nalgae types in an automated fashion. More specifically, a custom multi-band\nfluorescence imaging microscope is used to capture fluorescence imaging data of\na water sample at six different excitation wavelengths ranging from 405 nm -\n530 nm. A number of morphological and spectral fluorescence features are then\nextracted from the isolated micro-organism imaging data, and used to train\nneural network classification models designed for the purpose of identification\nof the six algae types given an isolated micro-organism. Experimental results\nusing three different neural network classification models showed that the use\nof either fluorescence-based spectral features or fluorescence-based\nspectral-morphological features to train neural network classification models\nled to statistically significant improvements in identification accuracy when\ncompared to the use of morphological features (with average identification\naccuracies of 95.7%+/-3.5% and 96.1%+/-1.5%, respectively). These preliminary\nresults are quite promising, given that the identification accuracy of human\ntaxonomists are typically between the range of 67% and 83%, and thus\nillustrates the feasibility of leveraging machine learning and\nfluorescence-based spectral-morphological features as a viable method for\nautomated identification of different algae types.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 03:07:37 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Deglint", "Jason L.", ""], ["Jin", "Chao", ""], ["Chao", "Angela", ""], ["Wong", "Alexander", ""]]}, {"id": "1805.01123", "submitter": "Hyojin Park", "authors": "Hyojin Park, YoungJoon Yoo, Nojun Kwak", "title": "MC-GAN: Multi-conditional Generative Adversarial Network for Image\n  Synthesis", "comments": "BMVC 2018 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce a new method for generating an object image from\ntext attributes on a desired location, when the base image is given. One step\nfurther to the existing studies on text-to-image generation mainly focusing on\nthe object's appearance, the proposed method aims to generate an object image\npreserving the given background information, which is the first attempt in this\nfield. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which\ncontrols both the object and background information jointly. As a core\ncomponent of MC-GAN, we propose a synthesis block which disentangles the object\nand background information in the training stage. This block enables MC-GAN to\ngenerate a realistic object image with the desired background by controlling\nthe amount of the background information from the given base image using the\nforeground information from the text attributes. From the experiments with\nCaltech-200 bird and Oxford-102 flower datasets, we show that our model is able\nto generate photo-realistic images with a resolution of 128 x 128. The source\ncode of MC-GAN is released.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 05:47:22 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 15:02:34 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 08:44:11 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 07:31:10 GMT"}, {"version": "v5", "created": "Wed, 15 Aug 2018 08:35:43 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Park", "Hyojin", ""], ["Yoo", "YoungJoon", ""], ["Kwak", "Nojun", ""]]}, {"id": "1805.01146", "submitter": "George De Ath", "authors": "George De Ath, Richard Everson", "title": "Visual Object Tracking: The Initialisation Problem", "comments": "15th Conference on Computer and Robot Vision (CRV 2018). Source code\n  available at https://github.com/georgedeath/initialisation-problem", "journal-ref": null, "doi": "10.1109/CRV.2018.00029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model initialisation is an important component of object tracking. Tracking\nalgorithms are generally provided with the first frame of a sequence and a\nbounding box (BB) indicating the location of the object. This BB may contain a\nlarge number of background pixels in addition to the object and can lead to\nparts-based tracking algorithms initialising their object models in background\nregions of the BB. In this paper, we tackle this as a missing labels problem,\nmarking pixels sufficiently away from the BB as belonging to the background and\nlearning the labels of the unknown pixels. Three techniques, One-Class SVM\n(OC-SVM), Sampled-Based Background Model (SBBM) (a novel background model based\non pixel samples), and Learning Based Digital Matting (LBDM), are adapted to\nthe problem. These are evaluated with leave-one-video-out cross-validation on\nthe VOT2016 tracking benchmark. Our evaluation shows both OC-SVMs and SBBM are\ncapable of providing a good level of segmentation accuracy but are too\nparameter-dependent to be used in real-world scenarios. We show that LBDM\nachieves significantly increased performance with parameters selected by cross\nvalidation and we show that it is robust to parameter variation.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 07:43:00 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 12:35:05 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["De Ath", "George", ""], ["Everson", "Richard", ""]]}, {"id": "1805.01158", "submitter": "Xiao Guobao", "authors": "Guobao Xiao, Hanzi Wang, Yan Yan, David Suter", "title": "Superpixel-guided Two-view Deterministic Geometric Model Fitting", "comments": null, "journal-ref": "International Journal of Computer Vision (IJCV),2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric model fitting is a fundamental research topic in computer vision\nand it aims to fit and segment multiple-structure data. In this paper, we\npropose a novel superpixel-guided two-view geometric model fitting method\n(called SDF), which can obtain reliable and consistent results for real images.\nSpecifically, SDF includes three main parts: a deterministic sampling\nalgorithm, a model hypothesis updating strategy and a novel model selection\nalgorithm. The proposed deterministic sampling algorithm generates a set of\ninitial model hypotheses according to the prior information of superpixels.\nThen the proposed updating strategy further improves the quality of model\nhypotheses. After that, by analyzing the properties of the updated model\nhypotheses, the proposed model selection algorithm extends the conventional\n\"fit-and-remove\" framework to estimate model instances in multiple-structure\ndata. The three parts are tightly coupled to boost the performance of SDF in\nboth speed and accuracy, and SDF has the deterministic nature. Experimental\nresults show that the proposed SDF has significant advantages over several\nstate-of-the-art fitting methods when it is applied to real images with\nsingle-structure and multiple-structure data.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 08:16:27 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Xiao", "Guobao", ""], ["Wang", "Hanzi", ""], ["Yan", "Yan", ""], ["Suter", "David", ""]]}, {"id": "1805.01167", "submitter": "Qiangpeng Yang", "authors": "Qiangpeng Yang, Mengli Cheng, Wenmeng Zhou, Yan Chen, Minghui Qiu, Wei\n  Lin, Wei Chu", "title": "IncepText: A New Inception-Text Module with Deformable PSROI Pooling for\n  Multi-Oriented Scene Text Detection", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incidental scene text detection, especially for multi-oriented text regions,\nis one of the most challenging tasks in many computer vision applications.\nDifferent from the common object detection task, scene text often suffers from\na large variance of aspect ratio, scale, and orientation. To solve this\nproblem, we propose a novel end-to-end scene text detector IncepText from an\ninstance-aware segmentation perspective. We design a novel Inception-Text\nmodule and introduce deformable PSROI pooling to deal with multi-oriented text\ndetection. Extensive experiments on ICDAR2015, RCTW-17, and MSRA-TD500 datasets\ndemonstrate our method's superiority in terms of both effectiveness and\nefficiency. Our proposed method achieves 1st place result on ICDAR2015\nchallenge and the state-of-the-art performance on other datasets. Moreover, we\nhave released our implementation as an OCR product which is available for\npublic access.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 08:37:28 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 03:01:23 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Yang", "Qiangpeng", ""], ["Cheng", "Mengli", ""], ["Zhou", "Wenmeng", ""], ["Chen", "Yan", ""], ["Qiu", "Minghui", ""], ["Lin", "Wei", ""], ["Chu", "Wei", ""]]}, {"id": "1805.01195", "submitter": "Jorge Beltr\\'an", "authors": "Jorge Beltran, Carlos Guindel, Francisco Miguel Moreno, Daniel\n  Cruzado, Fernando Garcia, Arturo de la Escalera", "title": "BirdNet: a 3D Object Detection Framework from LiDAR information", "comments": "Submittied to IEEE International Conference on Intelligent\n  Transportation Systems 2018 (ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding driving situations regardless the conditions of the traffic\nscene is a cornerstone on the path towards autonomous vehicles; however,\ndespite common sensor setups already include complementary devices such as\nLiDAR or radar, most of the research on perception systems has traditionally\nfocused on computer vision. We present a LiDAR-based 3D object detection\npipeline entailing three stages. First, laser information is projected into a\nnovel cell encoding for bird's eye view projection. Later, both object location\non the plane and its heading are estimated through a convolutional neural\nnetwork originally designed for image processing. Finally, 3D oriented\ndetections are computed in a post-processing phase. Experiments on KITTI\ndataset show that the proposed framework achieves state-of-the-art results\namong comparable methods. Further tests with different LiDAR sensors in real\nscenarios assess the multi-device capabilities of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 09:59:45 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Beltran", "Jorge", ""], ["Guindel", "Carlos", ""], ["Moreno", "Francisco Miguel", ""], ["Cruzado", "Daniel", ""], ["Garcia", "Fernando", ""], ["de la Escalera", "Arturo", ""]]}, {"id": "1805.01199", "submitter": "Yaxin Shi", "authors": "Yaxin Shi, Donna Xu, Yuangang Pan, Ivor W. Tsang, Shirui Pan", "title": "Label Embedding with Partial Heterogeneous Contexts", "comments": "8 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label embedding plays an important role in many real-world applications. To\nenhance the label relatedness captured by the embeddings, multiple contexts can\nbe adopted. However, these contexts are heterogeneous and often partially\nobserved in practical tasks, imposing significant challenges to capture the\noverall relatedness among labels. In this paper, we propose a general Partial\nHeterogeneous Context Label Embedding (PHCLE) framework to address these\nchallenges. Categorizing heterogeneous contexts into two groups, relational\ncontext and descriptive context, we design tailor-made matrix factorization\nformula to effectively exploit the label relatedness in each context. With a\nshared embedding principle across heterogeneous contexts, the label relatedness\nis selectively aligned in a shared space. Due to our elegant formulation, PHCLE\novercomes the partial context problem and can nicely incorporate more contexts,\nwhich both cannot be tackled with existing multi-context label embedding\nmethods. An effective alternative optimization algorithm is further derived to\nsolve the sparse matrix factorization problem. Experimental results demonstrate\nthat the label embeddings obtained with PHCLE achieve superb performance in\nimage classification task and exhibit good interpretability in the downstream\nlabel similarity analysis and image understanding task.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 10:08:51 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 03:10:45 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Shi", "Yaxin", ""], ["Xu", "Donna", ""], ["Pan", "Yuangang", ""], ["Tsang", "Ivor W.", ""], ["Pan", "Shirui", ""]]}, {"id": "1805.01220", "submitter": "Esteban Pardo", "authors": "Esteban Pardo, Jos\\'e M\\'ario T Morgado, Norberto Malpica", "title": "Semantic segmentation of mFISH images using convolutional networks", "comments": null, "journal-ref": null, "doi": "10.1002/cyto.a.23375", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multicolor in situ hybridization (mFISH) is a karyotyping technique used to\ndetect major chromosomal alterations using fluorescent probes and imaging\ntechniques. Manual interpretation of mFISH images is a time consuming step that\ncan be automated using machine learning; in previous works, pixel or patch wise\nclassification was employed, overlooking spatial information which can help\nidentify chromosomes. In this work, we propose a fully convolutional semantic\nsegmentation network for the interpretation of mFISH images, which uses both\nspatial and spectral information to classify each pixel in an end-to-end\nfashion. The semantic segmentation network developed was tested on samples\nextracted from a public dataset using cross validation. Despite having no\nlabeling information of the image it was tested on our algorithm yielded an\naverage correct classification ratio (CCR) of 87.41%. Previously, this level of\naccuracy was only achieved with state of the art algorithms when classifying\npixels from the same image in which the classifier has been trained. These\nresults provide evidence that fully convolutional semantic segmentation\nnetworks may be employed in the computer aided diagnosis of genetic diseases\nwith improved performance over the current methods of image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 11:04:47 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Pardo", "Esteban", ""], ["Morgado", "Jos\u00e9 M\u00e1rio T", ""], ["Malpica", "Norberto", ""]]}, {"id": "1805.01222", "submitter": "Hesam Sagha", "authors": "Andreas Triantafyllopoulos, Hesam Sagha, Florian Eyben, Bj\\\"orn\n  Schuller", "title": "audEERING's approach to the One-Minute-Gradual Emotion Challenge", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes audEERING's submissions as well as additional\nevaluations for the One-Minute-Gradual (OMG) emotion recognition challenge. We\nprovide the results for audio and video processing on subject (in)dependent\nevaluations. On the provided Development set, we achieved 0.343 Concordance\nCorrelation Coefficient (CCC) for arousal (from audio) and .401 for valence\n(from video).\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 11:06:23 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Triantafyllopoulos", "Andreas", ""], ["Sagha", "Hesam", ""], ["Eyben", "Florian", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1805.01259", "submitter": "Siyang Song", "authors": "Siyang Song, Shuimei Zhang, Bj\\\"orn Schuller, Linlin Shen, Michel\n  Valstar", "title": "Noise Invariant Frame Selection: A Simple Method to Address the\n  Background Noise Problem for Text-independent Speaker Verification", "comments": "Paper accepted in IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of speaker-related systems usually degrades heavily in\npractical applications largely due to the presence of background noise. To\nimprove the robustness of such systems in unknown noisy environments, this\npaper proposes a simple pre-processing method called Noise Invariant Frame\nSelection (NIFS). Based on several noisy constraints, it selects noise\ninvariant frames from utterances to represent speakers. Experiments conducted\non the TIMIT database showed that the NIFS can significantly improve the\nperformance of Vector Quantization (VQ), Gaussian Mixture Model-Universal\nBackground Model (GMM-UBM) and i-vector-based speaker verification systems in\ndifferent unknown noisy environments with different SNRs, in comparison to\ntheir baselines. Meanwhile, the proposed NIFS-based speaker verification\nsystems achieves similar performance when we change the constraints\n(hyper-parameters) or features, which indicates that it is robust and easy to\nreproduce. Since NIFS is designed as a general algorithm, it could be further\napplied to other similar tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 12:35:06 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Song", "Siyang", ""], ["Zhang", "Shuimei", ""], ["Schuller", "Bj\u00f6rn", ""], ["Shen", "Linlin", ""], ["Valstar", "Michel", ""]]}, {"id": "1805.01266", "submitter": "Baran G\\\"ozc\\\"u", "authors": "Baran G\\\"ozc\\\"u, Rabeeh Karimi Mahabadi, Yen-Huan Li, Efe Il{\\i}cak,\n  Tolga \\c{C}ukur, Jonathan Scarlett, Volkan Cevher", "title": "Learning-Based Compressive MRI", "comments": "13 pages, 6 figures. IEEE TMI (Transactions of Medical Imaging)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of magnetic resonance imaging (MRI), an extensive range of\nnon-linear reconstruction algorithms have been proposed that can be used with\ngeneral Fourier subsampling patterns. However, the design of these subsampling\npatterns has typically been considered in isolation from the reconstruction\nrule and the anatomy under consideration. In this paper, we propose a\nlearning-based framework for optimizing MRI subsampling patterns for a specific\nreconstruction rule and anatomy, considering both the noiseless and noisy\nsettings. Our learning algorithm has access to a representative set of training\nsignals, and searches for a sampling pattern that performs well on average for\nthe signals in this set. We present a novel parameter-free greedy mask\nselection method, and show it to be effective for a variety of reconstruction\nrules and performance metrics. Moreover we also support our numerical findings\nby providing a rigorous justification of our framework via statistical learning\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 12:49:34 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["G\u00f6zc\u00fc", "Baran", ""], ["Mahabadi", "Rabeeh Karimi", ""], ["Li", "Yen-Huan", ""], ["Il\u0131cak", "Efe", ""], ["\u00c7ukur", "Tolga", ""], ["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1805.01282", "submitter": "Ni Zhuang", "authors": "Ni Zhuang, Yan Yan, Si Chen, Hanzi Wang and Chunhua Shen", "title": "Multi-label Learning Based Deep Transfer Neural Network for Facial\n  Attribute Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2018.03.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) has recently achieved outstanding performance in a\nvariety of computer vision tasks, including facial attribute classification.\nThe great success of classifying facial attributes with DNN often relies on a\nmassive amount of labelled data. However, in real-world applications, labelled\ndata are only provided for some commonly used attributes (such as age, gender);\nwhereas, unlabelled data are available for other attributes (such as\nattraction, hairline). To address the above problem, we propose a novel deep\ntransfer neural network method based on multi-label learning for facial\nattribute classification, termed FMTNet, which consists of three sub-networks:\nthe Face detection Network (FNet), the Multi-label learning Network (MNet) and\nthe Transfer learning Network (TNet). Firstly, based on the Faster Region-based\nConvolutional Neural Network (Faster R-CNN), FNet is fine-tuned for face\ndetection. Then, MNet is fine-tuned by FNet to predict multiple attributes with\nlabelled data, where an effective loss weight scheme is developed to explicitly\nexploit the correlation between facial attributes based on attribute grouping.\nFinally, based on MNet, TNet is trained by taking advantage of unsupervised\ndomain adaptation for unlabelled facial attribute classification. The three\nsub-networks are tightly coupled to perform effective facial attribute\nclassification. A distinguishing characteristic of the proposed FMTNet method\nis that the three sub-networks (FNet, MNet and TNet) are constructed in a\nsimilar network structure. Extensive experimental results on challenging face\ndatasets demonstrate the effectiveness of our proposed method compared with\nseveral state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:13:50 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhuang", "Ni", ""], ["Yan", "Yan", ""], ["Chen", "Si", ""], ["Wang", "Hanzi", ""], ["Shen", "Chunhua", ""]]}, {"id": "1805.01290", "submitter": "Ni Zhuang", "authors": "Ni Zhuang, Yan Yan, Si Chen, and Hanzi Wang", "title": "Multi-task Learning of Cascaded CNN for Facial Attribute Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, facial attribute classification (FAC) has attracted significant\nattention in the computer vision community. Great progress has been made along\nwith the availability of challenging FAC datasets. However, conventional FAC\nmethods usually firstly pre-process the input images (i.e., perform face\ndetection and alignment) and then predict facial attributes. These methods\nignore the inherent dependencies among these tasks (i.e., face detection,\nfacial landmark localization and FAC). Moreover, some methods using\nconvolutional neural network are trained based on the fixed loss weights\nwithout considering the differences between facial attributes. In order to\naddress the above problems, we propose a novel multi-task learning of cas-\ncaded convolutional neural network method, termed MCFA, for predicting multiple\nfacial attributes simultaneously. Specifically, the proposed method takes\nadvantage of three cascaded sub-networks (i.e., S_Net, M_Net and L_Net\ncorresponding to the neural networks under different scales) to jointly train\nmultiple tasks in a coarse-to-fine manner, which can achieve end-to-end\noptimization. Furthermore, the proposed method automatically assigns the loss\nweight to each facial attribute based on a novel dynamic weighting scheme, thus\nmaking the proposed method concentrate on predicting the more difficult facial\nattributes. Experimental results show that the proposed method outperforms\nseveral state-of-the-art FAC methods on the challenging CelebA and LFWA\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:23:23 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhuang", "Ni", ""], ["Yan", "Yan", ""], ["Chen", "Si", ""], ["Wang", "Hanzi", ""]]}, {"id": "1805.01317", "submitter": "Yunlong Ma", "authors": "Yunlong Ma and Chunyan Wang", "title": "SdcNet: A Computation-Efficient CNN for Object Recognition", "comments": "5 pages,3 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting features from a huge amount of data for object recognition is a\nchallenging task. Convolution neural network can be used to meet the challenge,\nbut it often requires a large number of computation resources. In this paper, a\ncomputation-efficient convolutional module, named SdcBlock, is proposed and\nbased on it, the convolution network SdcNet is introduced for object\nrecognition tasks. In the proposed module, optimized successive depthwise\nconvolutions supported by appropriate data management is applied in order to\ngenerate vectors containing high density and more varieties of feature\ninformation. The hyperparameters can be easily adjusted to suit varieties of\ntasks under different computation restrictions without significantly\njeopardizing the performance. The experiments have shown that SdcNet achieved\nan error rate of 5.60% in CIFAR-10 with only 55M Flops and also reduced further\nthe error rate to 5.24% using a moderate volume of 103M Flops. The expected\ncomputation efficiency of the SdcNet has been confirmed.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 14:16:25 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 00:36:26 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Ma", "Yunlong", ""], ["Wang", "Chunyan", ""]]}, {"id": "1805.01328", "submitter": "Tobias Koch", "authors": "Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, Marco K\\\"orner", "title": "Evaluation of CNN-based Single-Image Depth Estimation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While an increasing interest in deep models for single-image depth estimation\nmethods can be observed, established schemes for their evaluation are still\nlimited. We propose a set of novel quality criteria, allowing for a more\ndetailed analysis by focusing on specific characteristics of depth maps. In\nparticular, we address the preservation of edges and planar regions, depth\nconsistency, and absolute distance accuracy. In order to employ these metrics\nto evaluate and compare state-of-the-art single-image depth estimation\napproaches, we provide a new high-quality RGB-D dataset. We used a DSLR camera\ntogether with a laser scanner to acquire high-resolution images and highly\naccurate depth maps. Experimental results show the validity of our proposed\nevaluation protocol.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 14:34:34 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Koch", "Tobias", ""], ["Liebel", "Lukas", ""], ["Fraundorfer", "Friedrich", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1805.01352", "submitter": "Yangfan Hu", "authors": "Yangfan Hu, Huajin Tang, Gang Pan", "title": "Spiking Deep Residual Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) have received significant attention for their\nbiological plausibility. SNNs theoretically have at least the same\ncomputational power as traditional artificial neural networks (ANNs). They\npossess potential of achieving energy-efficiency while keeping comparable\nperformance to deep neural networks (DNNs). However, it is still a big\nchallenge to train a very deep SNN. In this paper, we propose an efficient\napproach to build a spiking version of deep residual network (ResNet). ResNet\nis considered as a kind of the state-of-the-art convolutional neural networks\n(CNNs). We employ the idea of converting a trained ResNet to a network of\nspiking neurons, named Spiking ResNet (S-ResNet). We propose a shortcut\nconversion model to appropriately scale continuous-valued activations to match\nfiring rates in SNN, and a compensation mechanism to reduce the error caused by\ndiscretisation. Experimental results demonstrate that, compared with the\nstate-of-the-art SNN approaches, the proposed Spiking ResNet achieves the best\nperformance on CIFAR-10, CIFAR-100, and ImageNet 2012. Our work is the first\ntime to build a SNN deeper than 40, with comparable performance to ANNs on a\nlarge-scale dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 06:44:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 16:55:37 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Hu", "Yangfan", ""], ["Tang", "Huajin", ""], ["Pan", "Gang", ""]]}, {"id": "1805.01358", "submitter": "Titus Cieslewski", "authors": "Titus Cieslewski, Konstantinos G. Derpanis, Davide Scaramuzza", "title": "SIPs: Succinct Interest Points from Unsupervised Inlierness Probability\n  Learning", "comments": "8 pages, 2p references, 1p supplementary material. Accepted for\n  publication at the IEEE International Conference on 3D Vision (3DV), Qu\\'ebec\n  City, 2019. v2 contains significant changes VS v1", "journal-ref": "IEEE International Conference on 3D Vision (3DV), Qu\\'ebec City,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of computer vision algorithms rely on identifying sparse\ninterest points in images and establishing correspondences between them.\nHowever, only a subset of the initially identified interest points results in\ntrue correspondences (inliers). In this paper, we seek a detector that finds\nthe minimum number of points that are likely to result in an\napplication-dependent \"sufficient\" number of inliers k. To quantify this goal,\nwe introduce the \"k-succinctness\" metric. Extracting a minimum number of\ninterest points is attractive for many applications, because it can reduce\ncomputational load, memory, and data transmission. Alongside succinctness, we\nintroduce an unsupervised training methodology for interest point detectors\nthat is based on predicting the probability of a given pixel being an inlier.\nIn comparison to previous learned detectors, our method requires the least\namount of data pre-processing. Our detector and other state-of-the-art\ndetectors are extensively evaluated with respect to succinctness on popular\npublic datasets covering both indoor and outdoor scenes, and both wide and\nnarrow baselines. In certain cases, our detector is able to obtain an\nequivalent amount of inliers with as little as 60% of the amount of points of\nother detectors. The code and trained networks are provided at\nhttps://github.com/uzh-rpg/sips2_open .\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 15:11:30 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 13:13:37 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Cieslewski", "Titus", ""], ["Derpanis", "Konstantinos G.", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1805.01361", "submitter": "Sina Keller", "authors": "Philipp M. Maier and Sina Keller", "title": "Machine learning regression on hyperspectral data to estimate multiple\n  water parameters", "comments": "This work has been accepted to the IEEE WHISPERS 2018 conference. (C)\n  2018 IEEE", "journal-ref": null, "doi": "10.1109/WHISPERS.2018.8747010", "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a regression framework involving several machine\nlearning models to estimate water parameters based on hyperspectral data.\nMeasurements from a multi-sensor field campaign, conducted on the River Elbe,\nGermany, represent the benchmark dataset. It contains hyperspectral data and\nthe five water parameters chlorophyll a, green algae, diatoms, CDOM and\nturbidity. We apply a PCA for the high-dimensional data as a possible\npreprocessing step. Then, we evaluate the performance of the regression\nframework with and without this preprocessing step. The regression results of\nthe framework clearly reveal the potential of estimating water parameters based\non hyperspectral data with machine learning. The proposed framework provides\nthe basis for further investigations, such as adapting the framework to\nestimate water parameters of different inland waters.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 15:13:02 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 08:32:58 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Maier", "Philipp M.", ""], ["Keller", "Sina", ""]]}, {"id": "1805.01369", "submitter": "Grigoriy Sterling", "authors": "Grigoriy Sterling, Andrey Belyaev, Maxim Ryabov", "title": "Framewise approach in multimodal emotion recognition in OMG challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we described our approach achieves $53\\%$ of unweighted\naccuracy over $7$ emotions and $0.05$ and $0.09$ mean squared errors for\narousal and valence in OMG emotion recognition challenge. Our results were\nobtained with ensemble of single modality models trained on voice and face data\nfrom video separately. We consider each stream as a sequence of frames. Next we\nestimated features from frames and handle it with recurrent neural network. As\naudio frame we mean short $0.4$ second spectrogram interval. For features\nestimation for face pictures we used own ResNet neural network pretrained on\nAffectNet database. Each short spectrogram was considered as a picture and\nprocessed by convolutional network too. As a base audio model we used ResNet\npretrained in speaker recognition task. Predictions from both modalities were\nfused on decision level and improve single-channel approaches by a few percent\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 15:21:44 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Sterling", "Grigoriy", ""], ["Belyaev", "Andrey", ""], ["Ryabov", "Maxim", ""]]}, {"id": "1805.01386", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bul\\`o, Barbara\n  Caputo, Elisa Ricci", "title": "Boosting Domain Adaptation by Discovering Latent Domains", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Domain Adaptation (DA) methods based on deep architectures assume\nthat the source samples arise from a single distribution. However, in practice,\nmost datasets can be regarded as mixtures of multiple domains. In these cases\nexploiting single-source DA methods for learning target classifiers may lead to\nsub-optimal, if not poor, results. In addition, in many applications it is\ndifficult to manually provide the domain labels for all source data points,\ni.e. latent domains should be automatically discovered. This paper introduces a\nnovel Convolutional Neural Network (CNN) architecture which (i) automatically\ndiscovers latent domains in visual datasets and (ii) exploits this information\nto learn robust target classifiers. Our approach is based on the introduction\nof two main components, which can be embedded into any existing CNN\narchitecture: (i) a side branch that automatically computes the assignment of a\nsource sample to a latent domain and (ii) novel layers that exploit domain\nmembership information to appropriately align the distribution of the CNN\ninternal feature representations to a reference distribution. We test our\napproach on publicly-available datasets, showing that it outperforms\nstate-of-the-art multi-source DA methods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 15:55:48 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Porzi", "Lorenzo", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "1805.01416", "submitter": "Pedro M. Ferreira", "authors": "Pedro M. Ferreira, Diogo Pernes, Kelwin Fernandes, Ana Rebelo and\n  Jaime S. Cardoso", "title": "Dimensional emotion recognition using visual and textual cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of automatic emotion recognition in the\nscope of the One-Minute Gradual-Emotional Behavior challenge (OMG-Emotion\nchallenge). The underlying objective of the challenge is the automatic\nestimation of emotion expressions in the two-dimensional emotion representation\nspace (i.e., arousal and valence). The adopted methodology is a weighted\nensemble of several models from both video and text modalities. For video-based\nrecognition, two different types of visual cues (i.e., face and facial\nlandmarks) were considered to feed a multi-input deep neural network. Regarding\nthe text modality, a sequential model based on a simple recurrent architecture\nwas implemented. In addition, we also introduce a model based on high-level\nfeatures in order to embed domain knowledge in the learning process.\nExperimental results on the OMG-Emotion validation set demonstrate the\neffectiveness of the implemented ensemble model as it clearly outperforms the\ncurrent baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 16:42:20 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Ferreira", "Pedro M.", ""], ["Pernes", "Diogo", ""], ["Fernandes", "Kelwin", ""], ["Rebelo", "Ana", ""], ["Cardoso", "Jaime S.", ""]]}, {"id": "1805.01442", "submitter": "Nafis Neehal", "authors": "Mohammad Shakirul Islam, Ferdouse Ahmed Foysal, Nafis Neehal, Enamul\n  Karim, Syed Akhter Hossain", "title": "InceptB: A CNN Based Classification Approach for Recognizing Traditional\n  Bengali Games", "comments": "8 pages, 8 sections (including reference), 5 images, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports activities are an integral part of our day to day life. Introducing\nautonomous decision making and predictive models to recognize and analyze\ndifferent sports events and activities has become an emerging trend in computer\nvision arena. Albeit the advances and vivid applications of artificial\nintelligence and computer vision in recognizing different popular western\ngames, there remains a very minimal amount of efforts in the application of\ncomputer vision in recognizing traditional Bangladeshi games. We, in this\npaper, have described a novel Deep Learning based approach for recognizing\ntraditional Bengali games. We have retrained the final layer of the renowned\nInception V3 architecture developed by Google for our classification approach.\nOur approach shows promising results with an average accuracy of 80%\napproximately in correctly recognizing among 5 traditional Bangladeshi sports\nevents.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 17:35:45 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 16:40:02 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Islam", "Mohammad Shakirul", ""], ["Foysal", "Ferdouse Ahmed", ""], ["Neehal", "Nafis", ""], ["Karim", "Enamul", ""], ["Hossain", "Syed Akhter", ""]]}, {"id": "1805.01452", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias, Stefanos Zafeiriou", "title": "A Multi-component CNN-RNN Approach for Dimensional Emotion Recognition\n  in-the-wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our approach to the One-Minute Gradual-Emotion\nRecognition (OMG-Emotion) Challenge, focusing on dimensional emotion\nrecognition through visual analysis of the provided emotion videos. The\napproach is based on a Convolutional and Recurrent (CNN-RNN) deep neural\narchitecture we have developed for the relevant large AffWild Emotion Database.\nWe extended and adapted this architecture, by letting a combination of multiple\nfeatures generated in the CNN component be explored by RNN subnets. Our target\nhas been to obtain best performance on the OMG-Emotion visual validation data\nset, while learning the respective visual training data set. Extended\nexperimentation has led to best architectures for the estimation of the values\nof the valence and arousal emotion dimensions over these data sets.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 17:54:44 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 01:01:00 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 09:43:17 GMT"}, {"version": "v4", "created": "Mon, 12 Nov 2018 23:17:42 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2019 23:32:41 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1805.01556", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Pixel-wise Attentional Gating for Parsimonious Pixel Labeling", "comments": "https://www.ics.uci.edu/~skong2/PAG.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve parsimonious inference in per-pixel labeling tasks with a limited\ncomputational budget, we propose a \\emph{Pixel-wise Attentional Gating} unit\n(\\emph{PAG}) that learns to selectively process a subset of spatial locations\nat each layer of a deep convolutional network. PAG is a generic,\narchitecture-independent, problem-agnostic mechanism that can be readily\n\"plugged in\" to an existing model with fine-tuning. We utilize PAG in two ways:\n1) learning spatially varying pooling fields that improve model performance\nwithout the extra computation cost associated with multi-scale pooling, and 2)\nlearning a dynamic computation policy for each pixel to decrease total\ncomputation while maintaining accuracy.\n  We extensively evaluate PAG on a variety of per-pixel labeling tasks,\nincluding semantic segmentation, boundary detection, monocular depth and\nsurface normal estimation. We demonstrate that PAG allows competitive or\nstate-of-the-art performance on these tasks. Our experiments show that PAG\nlearns dynamic spatial allocation of computation over the input image which\nprovides better performance trade-offs compared to related approaches (e.g.,\ntruncating deep models or dynamically skipping whole layers). Generally, we\nobserve PAG can reduce computation by $10\\%$ without noticeable loss in\naccuracy and performance degrades gracefully when imposing stronger\ncomputational constraints.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 22:05:57 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 22:46:32 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1805.01677", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel\n  Gonzalez-Garcia, Bogdan Raducanu", "title": "Transferring GANs: generating images from limited data", "comments": "ECCV2018-camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring the knowledge of pretrained networks to new domains by means of\nfinetuning is a widely used practice for applications based on discriminative\nmodels. To the best of our knowledge this practice has not been studied within\nthe context of generative deep networks. Therefore, we study domain adaptation\napplied to image generation with generative adversarial networks. We evaluate\nseveral aspects of domain adaptation, including the impact of target domain\nsize, the relative distance between source and target domain, and the\ninitialization of conditional GANs. Our results show that using knowledge from\npretrained networks can shorten the convergence time and can significantly\nimprove the quality of the generated images, especially when the target data is\nlimited. We show that these conclusions can also be drawn for conditional GANs\neven when the pretrained model was trained without conditioning. Our results\nalso suggest that density may be more important than diversity and a dataset\nwith one or few densely sampled classes may be a better source model than more\ndiverse datasets such as ImageNet or Places.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 09:23:20 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 09:55:10 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Wang", "Yaxing", ""], ["Wu", "Chenshen", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""], ["Gonzalez-Garcia", "Abel", ""], ["Raducanu", "Bogdan", ""]]}, {"id": "1805.01717", "submitter": "Zara Alaverdyan", "authors": "Z. Alaverdyan, C. Lartizien", "title": "Feature extraction with regularized siamese networks for outlier\n  detection: application to lesion screening in medical imaging", "comments": "Accepted to Conf\\'erence sur l'Apprentissage automatique (CAp) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer aided diagnosis (CAD) systems are designed to assist clinicians in\nvarious tasks, including highlighting abnormal regions in a medical image. A\ncommon approach consists in training a voxel-level binary classifier on a set\nof feature vectors extracted from normal and pathological areas in patients'\nscans. However, many pathologies (such as epilepsy) are characterized by\nlesions that may be located anywhere in the brain, have various shapes, sizes\nand texture. An adequate representation of such a heterogeneity requires a\nsignificant amount of annotated data which is a major issue in the medical\ndomain. Therefore, we built on a previously proposed approach that considers\nepilepsy lesion detection task as a voxel-level outlier detection problem. It\nconsists in building a oc-SVM classifier for each voxel in the brain volume\nusing a small number of clinically-guided features El Azami et al., 2016. Our\ngoal in this study is to make a step forward by replacing the handcrafted\nfeatures with automatically learnt representations using neural networks. We\npropose a novel version of siamese networks trained on patches extracted from\nhealthy patients' scans only. This network, composed of stacked autoencoders as\nsubnetworks, is regularized by the reconstruction error of the patches. It is\ndesigned to learn representations that bring patches centered at the same voxel\nlocalization 'closer' with respect to the chosen metric (i.e. cosine). Finally,\nthe middle layer representations of the subnetworks are fed to oc-SVM\nclassifiers at voxel-level. The method is validated on 3 patients' MRI scans\nwith confirmed epilepsy lesions and shows a promising performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 11:25:02 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Alaverdyan", "Z.", ""], ["Lartizien", "C.", ""]]}, {"id": "1805.01760", "submitter": "Rig Das", "authors": "Shahar Mahpod, Rig Das, Emanuele Maiorana, Yosi Keller, and Patrizio\n  Campisi", "title": "Facial Landmarks Localization using Cascaded Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2021.103171", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate localization of facial landmarks is at the core of face analysis\ntasks, such as face recognition and facial expression analysis, to name a few.\nIn this work, we propose a novel localization approach based on a deep learning\narchitecture that utilizes cascaded subnetworks with convolutional neural\nnetwork units. The cascaded units of the first subnetwork estimate\nheatmap-based encodings of the landmarks locations, while the cascaded units of\nthe second subnetwork receive as input the output of the corresponding heatmap\nestimation units, and refine them through regression. The proposed scheme is\nexperimentally shown to compare favorably with contemporary state-of-the-art\nschemes, especially when applied to images depicting challenging localization\nconditions.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:53:20 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 21:48:39 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 18:05:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mahpod", "Shahar", ""], ["Das", "Rig", ""], ["Maiorana", "Emanuele", ""], ["Keller", "Yosi", ""], ["Campisi", "Patrizio", ""]]}, {"id": "1805.01803", "submitter": "Eduardo Pinho", "authors": "Eduardo Pinho, Carlos Costa", "title": "Unsupervised learning for concept detection in medical images: a\n  comparative analysis", "comments": null, "journal-ref": null, "doi": "10.3390/app8081213", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As digital medical imaging becomes more prevalent and archives increase in\nsize, representation learning exposes an interesting opportunity for enhanced\nmedical decision support systems. On the other hand, medical imaging data is\noften scarce and short on annotations. In this paper, we present an assessment\nof unsupervised feature learning approaches for images in the biomedical\nliterature, which can be applied to automatic biomedical concept detection. Six\nunsupervised representation learning methods were built, including traditional\nbags of visual words, autoencoders, and generative adversarial networks. Each\nmodel was trained, and their respective feature space evaluated using images\nfrom the ImageCLEF 2017 concept detection task. We conclude that it is possible\nto obtain more powerful representations with modern deep learning approaches,\nin contrast with previously popular computer vision methods. Although\ngenerative adversarial networks can provide good results, they are harder to\nsucceed in highly varied data sets. The possibility of semi-supervised\nlearning, as well as their use in medical information retrieval problems, are\nthe next steps to be strongly considered.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 14:36:09 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Pinho", "Eduardo", ""], ["Costa", "Carlos", ""]]}, {"id": "1805.01811", "submitter": "Simon Hecker", "authors": "Simon Hecker, Dengxin Dai and Luc Van Gool", "title": "Failure Prediction for Autonomous Driving", "comments": "published in IEEE Intelligent Vehicle Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary focus of autonomous driving research is to improve driving\naccuracy. While great progress has been made, state-of-the-art algorithms still\nfail at times. Such failures may have catastrophic consequences. It therefore\nis important that automated cars foresee problems ahead as early as possible.\nThis is also of paramount importance if the driver will be asked to take over.\nWe conjecture that failures do not occur randomly. For instance, driving models\nmay fail more likely at places with heavy traffic, at complex intersections,\nand/or under adverse weather/illumination conditions. This work presents a\nmethod to learn to predict the occurrence of these failures, i.e. to assess how\ndifficult a scene is to a given driving model and to possibly give the human\ndriver an early headsup. A camera-based driving model is developed and trained\nover real driving datasets. The discrepancies between the model's predictions\nand the human `ground-truth' maneuvers were then recorded, to yield the\n`failure' scores. Experimental results show that the failure score can indeed\nbe learned and predicted. Thus, our prediction method is able to improve the\noverall safety of an automated driving model by alerting the human driver\ntimely, leading to better human-vehicle collaborative driving.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 14:56:00 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Hecker", "Simon", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1805.01818", "submitter": "Sungmin Eum", "authors": "Sungmin Eum, Christopher Reale, Heesung Kwon, Claire Bonial, Clare\n  Voss", "title": "Object and Text-guided Semantics for CNN-based Activity Recognition", "comments": "Submitted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous methods have demonstrated the importance of considering\nsemantically relevant objects for carrying out video-based human activity\nrecognition, yet none of the methods have harvested the power of large text\ncorpora to relate the objects and the activities to be transferred into\nlearning a unified deep convolutional neural network. We present a novel\nactivity recognition CNN which co-learns the object recognition task in an\nend-to-end multitask learning scheme to improve upon the baseline activity\nrecognition performance. We further improve upon the multitask learning\napproach by exploiting a text-guided semantic space to select the most relevant\nobjects with respect to the target activities. To the best of our knowledge, we\nare the first to investigate this approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 15:09:48 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Eum", "Sungmin", ""], ["Reale", "Christopher", ""], ["Kwon", "Heesung", ""], ["Bonial", "Claire", ""], ["Voss", "Clare", ""]]}, {"id": "1805.01837", "submitter": "Mathias Niepert", "authors": "Mathias Niepert and Alberto Garcia-Duran", "title": "Towards a Spectrum of Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our ongoing work on understanding the limitations of graph\nconvolutional networks (GCNs) as well as our work on generalizations of graph\nconvolutions for representing more complex node attribute dependencies. Based\non an analysis of GCNs with the help of the corresponding computation graphs,\nwe propose a generalization of existing GCNs where the aggregation operations\nare (a) determined by structural properties of the local neighborhood graphs\nand (b) not restricted to weighted averages. We show that the proposed approach\nis strictly more expressive while requiring only a modest increase in the\nnumber of parameters and computations. We also show that the proposed\ngeneralization is identical to standard convolutional layers when applied to\nregular grid graphs.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 16:13:36 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Niepert", "Mathias", ""], ["Garcia-Duran", "Alberto", ""]]}, {"id": "1805.01872", "submitter": "Matthias Bauer", "authors": "Matthias Bauer, Valentin Volchkov, Michael Hirsch, Bernhard\n  Sch\\\"olkopf", "title": "Automatic Estimation of Modulation Transfer Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modulation transfer function (MTF) is widely used to characterise the\nperformance of optical systems. Measuring it is costly and it is thus rarely\navailable for a given lens specimen. Instead, MTFs based on simulations or, at\nbest, MTFs measured on other specimens of the same lens are used. Fortunately,\nimages recorded through an optical system contain ample information about its\nMTF, only that it is confounded with the statistics of the images. This work\npresents a method to estimate the MTF of camera lens systems directly from\nphotographs, without the need for expensive equipment. We use a custom grid\ndisplay to accurately measure the point response of lenses to acquire ground\ntruth training data. We then use the same lenses to record natural images and\nemploy a data-driven supervised learning approach using a convolutional neural\nnetwork to estimate the MTF on small image patches, aggregating the information\ninto MTF charts over the entire field of view. It generalises to unseen lenses\nand can be applied for single photographs, with the performance improving if\nmultiple photographs are available.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:36:41 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Bauer", "Matthias", ""], ["Volchkov", "Valentin", ""], ["Hirsch", "Michael", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1805.01887", "submitter": "Christian Koch", "authors": "Christian Koch, Moritz Lode, Denny Stohr, Amr Rizk, Ralf Steinmetz", "title": "Collaborations on YouTube: From Unsupervised Detection to the Impact on\n  Video and Channel Popularity", "comments": "28 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  YouTube is one of the most popular platforms for streaming of user-generated\nvideo. Nowadays, professional YouTubers are organized in so called\nmulti-channel networks (MCNs). These networks offer services such as brand\ndeals, equipment, and strategic advice in exchange for a share of the\nYouTubers' revenue. A major strategy to gain more subscribers and, hence,\nrevenue is collaborating with other YouTubers. Yet, collaborations on YouTube\nhave not been studied in a detailed quantitative manner. This paper aims to\nclose this gap with the following contributions. First, we collect a YouTube\ndataset covering video statistics over three months for 7,942 channels. Second,\nwe design a framework for collaboration detection given a previously unknown\nnumber of persons featuring in YouTube videos. We denote this framework for the\nanalysis of collaborations in YouTube videos using a Deep Neural Network (DNN)\nbased approach as CATANA. Third, we analyze about 2.4 years of video content\nand use CATANA to answer research questions providing guidance for YouTubers\nand MCNs for efficient collaboration strategies. Thereby, we focus on (i)\ncollaboration frequency and partner selectivity, (ii) the influence of MCNs on\nchannel collaborations, (iii) collaborating channel types, and (iv) the impact\nof collaborations on video and channel popularity. Our results show that\ncollaborations are in many cases significantly beneficial in terms of viewers\nand newly attracted subscribers for both collaborating channels, showing often\nmore than 100% popularity growth compared with non-collaboration videos.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 14:38:18 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Koch", "Christian", ""], ["Lode", "Moritz", ""], ["Stohr", "Denny", ""], ["Rizk", "Amr", ""], ["Steinmetz", "Ralf", ""]]}, {"id": "1805.01890", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Mojtaba Heidarysafa, Donald E. Brown, Kiana Jafari\n  Meimandi, Laura E. Barnes", "title": "RMDL: Random Multimodel Deep Learning for Classification", "comments": "Best Paper award ACM ICISDM", "journal-ref": null, "doi": "10.1145/3206098.3206111", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification. Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 19:36:43 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 16:08:33 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Kowsari", "Kamran", ""], ["Heidarysafa", "Mojtaba", ""], ["Brown", "Donald E.", ""], ["Meimandi", "Kiana Jafari", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1805.01912", "submitter": "Denton Bobeldyk Mr", "authors": "Denton Bobeldyk and Arun Ross", "title": "Analyzing Covariate Influence on Gender and Race Prediction from\n  Near-Infrared Ocular Images", "comments": "15 pages, 20 tables, 11 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2886275", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has explored the possibility of automatically deducing\ninformation such as gender, age and race of an individual from their biometric\ndata. While the face modality has been extensively studied in this regard, the\niris modality less so. In this paper, we first review the medical literature to\nestablish a biological basis for extracting gender and race cues from the iris.\nThen, we demonstrate that it is possible to use simple texture descriptors,\nlike BSIF (Binarized Statistical Image Feature) and LBP (Local Binary\nPatterns), to extract gender and race attributes from an NIR ocular image used\nin a typical iris recognition system. The proposed method predicts gender and\nrace from a single eye image with an accuracy of 86% and 90%, respectively. In\naddition, the following analysis are conducted: (a) the role of different parts\nof the ocular region on attribute prediction; (b) the influence of gender on\nrace prediction, and vice-versa; (c) the impact of eye color on gender and race\nprediction; (d) the impact of image blur on gender and race prediction; (e) the\ngeneralizability of the method across different datasets; and (f) the\nconsistency of prediction performance across the left and right eyes.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 18:42:50 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 03:11:16 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 23:47:14 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 22:10:51 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Bobeldyk", "Denton", ""], ["Ross", "Arun", ""]]}, {"id": "1805.01934", "submitter": "Qifeng Chen", "authors": "Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun", "title": "Learning to See in the Dark", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging in low light is challenging due to low photon count and low SNR.\nShort-exposure images suffer from noise, while long exposure can induce blur\nand is often impractical. A variety of denoising, deblurring, and enhancement\ntechniques have been proposed, but their effectiveness is limited in extreme\nconditions, such as video-rate imaging at night. To support the development of\nlearning-based pipelines for low-light image processing, we introduce a dataset\nof raw short-exposure low-light images, with corresponding long-exposure\nreference images. Using the presented dataset, we develop a pipeline for\nprocessing low-light images, based on end-to-end training of a\nfully-convolutional network. The network operates directly on raw sensor data\nand replaces much of the traditional image processing pipeline, which tends to\nperform poorly on such data. We report promising results on the new dataset,\nanalyze factors that affect performance, and highlight opportunities for future\nwork. The results are shown in the supplementary video at\nhttps://youtu.be/qWKUFK7MWvg\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 21:03:12 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chen", "Chen", ""], ["Chen", "Qifeng", ""], ["Xu", "Jia", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1805.01946", "submitter": "David G\\\"uera", "authors": "David G\\\"uera, Sri Kalyan Yarlagadda, Paolo Bestagini, Fengqing Zhu,\n  Stefano Tubaro, Edward J. Delp", "title": "Reliability Map Estimation For CNN-Based Camera Model Attribution", "comments": "Presented at the IEEE Winter Conference on Applications of Computer\n  Vision (WACV18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the image forensic issues investigated in the last few years, great\nattention has been devoted to blind camera model attribution. This refers to\nthe problem of detecting which camera model has been used to acquire an image\nby only exploiting pixel information. Solving this problem has great impact on\nimage integrity assessment as well as on authenticity verification. Recent\nadvancements that use convolutional neural networks (CNNs) in the media\nforensic field have enabled camera model attribution methods to work well even\non small image patches. These improvements are also important for determining\nforgery localization. Some patches of an image may not contain enough\ninformation related to the camera model (e.g., saturated patches). In this\npaper, we propose a CNN-based solution to estimate the camera model attribution\nreliability of a given image patch. We show that we can estimate a\nreliability-map indicating which portions of the image contain reliable camera\ntraces. Testing using a well known dataset confirms that by using this\ninformation, it is possible to increase small patch camera model attribution\naccuracy by more than 8% on a single patch.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 22:07:20 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["G\u00fcera", "David", ""], ["Yarlagadda", "Sri Kalyan", ""], ["Bestagini", "Paolo", ""], ["Zhu", "Fengqing", ""], ["Tubaro", "Stefano", ""], ["Delp", "Edward J.", ""]]}, {"id": "1805.01951", "submitter": "Benjamin Allaert", "authors": "B. Allaert, IM. Bilasco, C. Djeraba", "title": "Advanced local motion patterns for macro and micro facial expression\n  recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TAFFC.2019.2949559", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new method that recognizes facial expressions, on\nthe basis of an innovative local motion patterns feature, with three main\ncontributions. The first one is the analysis of the face skin temporal\nelasticity and face deformations during expression. The second one is a unified\napproach for both macro and micro expression recognition. And, the third one is\nthe step forward towards in-the-wild expression recognition, dealing with\nchallenges such as various intensity and various expression activation\npatterns, illumination variation and small head pose variations. Our method\noutperforms state-of-the-art methods for micro expression recognition and\npositions itself among top-rank state-of-the-art methods for macro expression\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 22:19:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Allaert", "B.", ""], ["Bilasco", "IM.", ""], ["Djeraba", "C.", ""]]}, {"id": "1805.01963", "submitter": "Xin Liu Dr.", "authors": "Xin Liu, Zhikai Hu, Haibin Ling, and Yiu-ming Cheung", "title": "MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient\n  Cross-Modal Retrieval", "comments": "16 pages, accepted by IEEE T-PAMI", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine\n  Intelligence,2019", "doi": "10.1109/TPAMI.2019.2940446", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has recently sparked a great revolution in cross-modal retrieval\nbecause of its low storage cost and high query speed. Recent cross-modal\nhashing methods often learn unified or equal-length hash codes to represent the\nmulti-modal data and make them intuitively comparable. However, such unified or\nequal-length hash representations could inherently sacrifice their\nrepresentation scalability because the data from different modalities may not\nhave one-to-one correspondence and could be encoded more efficiently by\ndifferent hash codes of unequal lengths. To mitigate these problems, this paper\nexploits a related and relatively unexplored problem: encode the heterogeneous\ndata with varying hash lengths and generalize the cross-modal retrieval in\nvarious challenging scenarios. To this end, a generalized and flexible\ncross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH),\nis proposed to work seamlessly in various settings including paired or unpaired\nmulti-modal data, and equal or varying hash length encoding scenarios. More\nspecifically, MTFH exploits an efficient objective function to flexibly learn\nthe modality-specific hash codes with different length settings, while\nsynchronously learning two semantic correlation matrices to semantically\ncorrelate the different hash representations for heterogeneous data comparable.\nAs a result, the derived hash codes are more semantically meaningful for\nvarious challenging cross-modal retrieval tasks. Extensive experiments\nevaluated on public benchmark datasets highlight the superiority of MTFH under\nvarious retrieval scenarios and show its competitive performance with the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 23:21:15 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 00:30:51 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Liu", "Xin", ""], ["Hu", "Zhikai", ""], ["Ling", "Haibin", ""], ["Cheung", "Yiu-ming", ""]]}, {"id": "1805.01972", "submitter": "Chengcheng Li", "authors": "Chengcheng Li, Zi Wang, Hairong Qi", "title": "Fast-converging Conditional Generative Adversarial Networks for Image\n  Synthesis", "comments": "Accepted by ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on top of the success of generative adversarial networks (GANs),\nconditional GANs attempt to better direct the data generation process by\nconditioning with certain additional information. Inspired by the most recent\nAC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In\naddition to the real/fake classifier used in vanilla GANs, our discriminator\nhas an advanced auxiliary classifier which distinguishes each real class from\nan extra `fake' class. The `fake' class avoids mixing generated data with real\ndata, which can potentially confuse the classification of real data as AC-GAN\ndoes, and makes the advanced auxiliary classifier behave as another real/fake\nclassifier. As a result, FC-GAN can accelerate the process of differentiation\nof all classes, thus boost the convergence speed. Experimental results on image\nsynthesis demonstrate our model is competitive in the quality of images\ngenerated while achieving a faster convergence rate.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 00:18:19 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Li", "Chengcheng", ""], ["Wang", "Zi", ""], ["Qi", "Hairong", ""]]}, {"id": "1805.01978", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin", "title": "Unsupervised Feature Learning via Non-Parametric Instance-level\n  Discrimination", "comments": "CVPR 2018 spotlight paper. Code:\n  https://github.com/zhirongw/lemniscate.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural net classifiers trained on data with annotated class labels can also\ncapture apparent visual similarity among categories without being directed to\ndo so. We study whether this observation can be extended beyond the\nconventional domain of supervised learning: Can we learn a good feature\nrepresentation that captures apparent similarity among instances, instead of\nclasses, by merely asking the feature to be discriminative of individual\ninstances? We formulate this intuition as a non-parametric classification\nproblem at the instance-level, and use noise-contrastive estimation to tackle\nthe computational challenges imposed by the large number of instance classes.\nOur experimental results demonstrate that, under unsupervised learning\nsettings, our method surpasses the state-of-the-art on ImageNet classification\nby a large margin. Our method is also remarkable for consistently improving\ntest performance with more training data and better network architectures. By\nfine-tuning the learned feature, we further obtain competitive results for\nsemi-supervised learning and object detection tasks. Our non-parametric model\nis highly compact: With 128 features per image, our method requires only 600MB\nstorage for a million images, enabling fast nearest neighbour retrieval at the\nrun time.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 00:47:01 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wu", "Zhirong", ""], ["Xiong", "Yuanjun", ""], ["Yu", "Stella", ""], ["Lin", "Dahua", ""]]}, {"id": "1805.02020", "submitter": "Wu Yantong", "authors": "YanTong Wu, Yang Liu", "title": "Position Estimation of Camera Based on Unsupervised Learning", "comments": "6 pages,5 figures,1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an exciting task to recover the scene's 3d-structure and camera pose\nfrom the video sequence. Most of the current solutions divide it into two\nparts, monocular depth recovery and camera pose estimation. The monocular depth\nrecovery is often studied as an independent part, and a better depth estimation\nis used to solve the pose. While camera pose is still estimated by traditional\nSLAM (Simultaneous Localization And Mapping) methods in most cases. The use of\nunsupervised method for monocular depth recovery and pose estimation has\nbenefited from the study of [1] and achieved good results. In this paper, we\nimprove the method of [1]. Our emphasis is laid on the improvement of the idea\nand related theory, introducing a more reasonable inter frame constraints and\nfinally synthesize the camera trajectory with inter frame pose estimation in\nthe unified world coordinate system. And our results get better performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 08:35:23 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Wu", "YanTong", ""], ["Liu", "Yang", ""]]}, {"id": "1805.02031", "submitter": "Jen-Yu Liu", "authors": "Jen-Yu Liu, Yi-Hsuan Yang, Shyh-Kang Jeng", "title": "Weakly-supervised Visual Instrument-playing Action Detection in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrument playing is among the most common scenes in music-related videos,\nwhich represent nowadays one of the largest sources of online videos. In order\nto understand the instrument-playing scenes in the videos, it is important to\nknow what instruments are played, when they are played, and where the playing\nactions occur in the scene. While audio-based recognition of instruments has\nbeen widely studied, the visual aspect of the music instrument playing remains\nlargely unaddressed in the literature. One of the main obstacles is the\ndifficulty in collecting annotated data of the action locations for\ntraining-based methods. To address this issue, we propose a weakly-supervised\nframework to find when and where the instruments are played in the videos. We\npropose to use two auxiliary models, a sound model and an object model, to\nprovide supervisions for training the instrument-playing action model. The\nsound model provides temporal supervisions, while the object model provides\nspatial supervisions. They together can simultaneously provide temporal and\nspatial supervisions. The resulted model only needs to analyze the visual part\nof a music video to deduce which, when and where instruments are played. We\nfound that the proposed method significantly improves the localization\naccuracy. We evaluate the result of the proposed method temporally and\nspatially on a small dataset (totally 5,400 frames) that we manually annotated.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 09:52:36 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Liu", "Jen-Yu", ""], ["Yang", "Yi-Hsuan", ""], ["Jeng", "Shyh-Kang", ""]]}, {"id": "1805.02058", "submitter": "Cao Haichao", "authors": "Haichao Cao, Hong Liu and Enmin Song", "title": "Bone marrow cells detection: A technique for the microscopic image\n  analysis", "comments": "11 pages, 8 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the detection of myeloproliferative, the number of cells in each type of\nbone marrow cells (BMC) is an important parameter for the evaluation. In this\nstudy, we propose a new counting method, which also consists of three modules\nincluding localization, segmentation and classification. The localization of\nBMC is achieved from a color transformation enhanced BMC sample image and\nstepwise averaging method (SAM). In the nucleus segmentation, both SAM and\nOtsu's method will be applied to obtain a weighted threshold for segmenting the\npatch into nucleus and non-nucleus. In the cytoplasm segmentation, a color\nweakening transformation, an improved region growing method and the K-Means\nalgorithm are used. The connected cells with BMC will be separated by the\nmarker-controlled watershed algorithm. The features will be extracted for the\nclassification after the segmentation. In this study, the BMC are classified\nusing the SVM, Random Forest, Artificial Neural Networks, Adaboost and Bayesian\nNetworks into five classes including one outlier, namely, neutrophilic split\ngranulocyte, neutrophilic stab granulocyte, metarubricyte, mature lymphocytes\nand the outlier (all other cells not listed). Our experimental results show\nthat the best average recognition rate is 87.49% for the SVM.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 13:56:03 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Cao", "Haichao", ""], ["Liu", "Hong", ""], ["Song", "Enmin", ""]]}, {"id": "1805.02085", "submitter": "Yicun Liu", "authors": "Yicun Liu, Jimmy Ren, Jianbo Liu, Jiawei Zhang, Xiaohao Chen", "title": "Learning Selfie-Friendly Abstraction from Artistic Style Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic style transfer can be thought as a process to generate different\nversions of abstraction of the original image. However, most of the artistic\nstyle transfer operators are not optimized for human faces thus mainly suffers\nfrom two undesirable features when applying them to selfies. First, the edges\nof human faces may unpleasantly deviate from the ones in the original image.\nSecond, the skin color is far from faithful to the original one which is\nusually problematic in producing quality selfies. In this paper, we take a\ndifferent approach and formulate this abstraction process as a gradient domain\nlearning problem. We aim to learn a type of abstraction which not only achieves\nthe specified artistic style but also circumvents the two aforementioned\ndrawbacks thus highly applicable to selfie photography. We also show that our\nmethod can be directly generalized to videos with high inter-frame consistency.\nOur method is also robust to non-selfie images, and the generalization to\nvarious kinds of real-life scenes is discussed. We will make our code publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 17:06:30 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 18:01:12 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liu", "Yicun", ""], ["Ren", "Jimmy", ""], ["Liu", "Jianbo", ""], ["Zhang", "Jiawei", ""], ["Chen", "Xiaohao", ""]]}, {"id": "1805.02091", "submitter": "Lichao Mou", "authors": "Lichao Mou and Xiao Xiang Zhu", "title": "RiFCN: Recurrent Network in Fully Convolutional Network for Semantic\n  Segmentation of High Resolution Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation in high resolution remote sensing images is a\nfundamental and challenging task. Convolutional neural networks (CNNs), such as\nfully convolutional network (FCN) and SegNet, have shown outstanding\nperformance in many segmentation tasks. One key pillar of these successes is\nmining useful information from features in convolutional layers for producing\nhigh resolution segmentation maps. For example, FCN nonlinearly combines\nhigh-level features extracted from last convolutional layers; whereas SegNet\nutilizes a deconvolutional network which takes as input only coarse, high-level\nfeature maps of the last convolutional layer. However, how to better fuse\nmulti-level convolutional feature maps for semantic segmentation of remote\nsensing images is underexplored. In this work, we propose a novel bidirectional\nnetwork called recurrent network in fully convolutional network (RiFCN), which\nis end-to-end trainable. It has a forward stream and a backward stream. The\nformer is a classification CNN architecture for feature extraction, which takes\nan input image and produces multi-level convolutional feature maps from shallow\nto deep; while in the later, to achieve accurate boundary inference and\nsemantic segmentation, boundary-aware high resolution feature maps in shallower\nlayers and high-level but low-resolution features are recursively embedded into\nthe learning framework (from deep to shallow) to generate a fused feature\nrepresentation that draws a holistic picture of not only high-level semantic\ninformation but also low-level fine-grained details. Experimental results on\ntwo widely-used high resolution remote sensing data sets for semantic\nsegmentation tasks, ISPRS Potsdam and Inria Aerial Image Labeling Data Set,\ndemonstrate competitive performance obtained by the proposed methodology\ncompared to other studied approaches.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 18:00:43 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1805.02104", "submitter": "Jiyang Gao", "authors": "Jiyang Gao and Ram Nevatia", "title": "Revisiting Temporal Modeling for Video-based Person ReID", "comments": "codes available at https://github.com/jiyanggao/Video-Person-ReID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person reID is an important task, which has received much\nattention in recent years due to the increasing demand in surveillance and\ncamera networks. A typical video-based person reID system consists of three\nparts: an image-level feature extractor (e.g. CNN), a temporal modeling method\nto aggregate temporal features and a loss function. Although many methods on\ntemporal modeling have been proposed, it is hard to directly compare these\nmethods, because the choice of feature extractor and loss function also have a\nlarge impact on the final performance. We comprehensively study and compare\nfour different temporal modeling methods (temporal pooling, temporal attention,\nRNN and 3D convnets) for video-based person reID. We also propose a new\nattention generation network which adopts temporal convolution to extract\ntemporal information among frames. The evaluation is done on the MARS dataset,\nand our methods outperform state-of-the-art methods by a large margin. Our\nsource codes are released at https://github.com/jiyanggao/Video-Person-ReID.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 18:56:38 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 02:59:16 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Gao", "Jiyang", ""], ["Nevatia", "Ram", ""]]}, {"id": "1805.02125", "submitter": "Ebrahim Karami", "authors": "Ebrahim Karami, Mohamed Shehata, Andrew Smith", "title": "Estimation and Tracking of AP-diameter of the Inferior Vena Cava in\n  Ultrasound Images Using a Novel Active Circle Algorithm", "comments": "Published in Computers in Biology and Medicine", "journal-ref": "In Computers in Biology and Medicine, vol. 98, pp. 16-25, July\n  2018", "doi": "10.1016/j.compbiomed.2018.05.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical research suggests that the anterior-posterior (AP)-diameter of the\ninferior vena cava (IVC) and its associated temporal variation as imaged by\nbedside ultrasound is useful in guiding fluid resuscitation of the\ncritically-ill patient. Unfortunately, indistinct edges and gaps in vessel\nwalls are frequently present which impede accurate estimation of the IVC\nAP-diameter for both human operators and segmentation algorithms. The majority\nof research involving use of the IVC to guide fluid resuscitation involves\nmanual measurement of the maximum and minimum AP-diameter as it varies over\ntime. This effort proposes using a time-varying circle fitted inside the\ntypically ellipsoid IVC as an efficient, consistent and novel approach to\ntracking and approximating the AP-diameter even in the context of poor image\nquality. In this active-circle algorithm, a novel evolution functional is\nproposed and shown to be a useful tool for ultrasound image processing. The\nproposed algorithm is compared with an expert manual measurement, and\nstate-of-the-art relevant algorithms. It is shown that the algorithm\noutperforms other techniques and performs very close to manual measurement.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 23:47:24 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 05:16:29 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 18:41:43 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karami", "Ebrahim", ""], ["Shehata", "Mohamed", ""], ["Smith", "Andrew", ""]]}, {"id": "1805.02131", "submitter": "David G\\\"uera", "authors": "David G\\\"uera, Yu Wang, Luca Bondi, Paolo Bestagini, Stefano Tubaro,\n  Edward J. Delp", "title": "A Counter-Forensic Method for CNN-Based Camera Model Identification", "comments": "Presented at the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), Workshop on Media Forensics", "journal-ref": null, "doi": "10.1109/CVPRW.2017.230", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of digital images are being shared and accessed through\nwebsites, media, and social applications. Many of these images have been\nmodified and are not authentic. Recent advances in the use of deep\nconvolutional neural networks (CNNs) have facilitated the task of analyzing the\nveracity and authenticity of largely distributed image datasets. We examine in\nthis paper the problem of identifying the camera model or type that was used to\ntake an image and that can be spoofed. Due to the linear nature of CNNs and the\nhigh-dimensionality of images, neural networks are vulnerable to attacks with\nadversarial examples. These examples are imperceptibly different from correctly\nclassified images but are misclassified with high confidence by CNNs. In this\npaper, we describe a counter-forensic method capable of subtly altering images\nto change their estimated camera model when they are analyzed by any CNN-based\ncamera model detector. Our method can use both the Fast Gradient Sign Method\n(FGSM) or the Jacobian-based Saliency Map Attack (JSMA) to craft these\nadversarial images and does not require direct access to the CNN. Our results\nshow that even advanced deep learning architectures trained to analyze images\nand obtain camera model information are still vulnerable to our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 01:32:08 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["G\u00fcera", "David", ""], ["Wang", "Yu", ""], ["Bondi", "Luca", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""], ["Delp", "Edward J.", ""]]}, {"id": "1805.02142", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Yongbin Gao, Yujin Zhang", "title": "An Image dehazing approach based on the airlight field estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a scheme for single image haze removal based on the\nairlight field (ALF) estimation. Conventional image dehazing methods which are\nbased on a physical model generally take the global atmospheric light as a\nconstant. However, the constant-airlight assumption may be unsuitable for\nimages with large sky regions, which causes unacceptable brightness imbalance\nand color distortion in recovery images. This paper models the atmospheric\nlight as a field function, and presents a maximum a-priori (MAP) method for\njointly estimating the airlight field, the transmission rate and the haze free\nimage. We also introduce a valid haze-level prior for effective estimate of\ntransmission. Evaluation on real world images shows that the proposed approach\noutperforms existing methods in single image dehazing, especially when the\nlarge sky region is included.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 03:18:20 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zhang", "Lijun", ""], ["Gao", "Yongbin", ""], ["Zhang", "Yujin", ""]]}, {"id": "1805.02152", "submitter": "Yi Wei", "authors": "Yi Wei, Xinyu Pan, Hongwei Qin, Wanli Ouyang, Junjie Yan", "title": "Quantization Mimic: Towards Very Tiny CNN for Object Detection", "comments": "Accepted to ECCV 2018. Version for conference submission (polish and\n  use theoretical size in Table 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple and general framework for training very\ntiny CNNs for object detection. Due to limited representation ability, it is\nchallenging to train very tiny networks for complicated tasks like detection.\nTo the best of our knowledge, our method, called Quantization Mimic, is the\nfirst one focusing on very tiny networks. We utilize two types of acceleration\nmethods: mimic and quantization. Mimic improves the performance of a student\nnetwork by transfering knowledge from a teacher network. Quantization converts\na full-precision network to a quantized one without large degradation of\nperformance. If the teacher network is quantized, the search scope of the\nstudent network will be smaller. Using this feature of the quantization, we\npropose Quantization Mimic. It first quantizes the large network, then mimic a\nquantized small network. The quantization operation can help student network to\nbetter match the feature maps from teacher network. To evaluate our approach,\nwe carry out experiments on various popular CNNs including VGG and Resnet, as\nwell as different detection frameworks including Faster R-CNN and R-FCN.\nExperiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic\nalgorithm can be applied on various settings and outperforms state-of-the-art\nmodel acceleration methods given limited computing resouces.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 05:36:07 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 13:23:30 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 09:03:58 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wei", "Yi", ""], ["Pan", "Xinyu", ""], ["Qin", "Hongwei", ""], ["Ouyang", "Wanli", ""], ["Yan", "Junjie", ""]]}, {"id": "1805.02158", "submitter": "Tao Hong", "authors": "Tao Hong, Yaniv Romano and Michael Elad", "title": "Acceleration of RED via Vector Extrapolation", "comments": "5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models play an important role in inverse problems, serving as the prior for\nrepresenting the original signal to be recovered. REgularization by Denoising\n(RED) is a recently introduced general framework for constructing such priors\nusing state-of-the-art denoising algorithms. Using RED, solving inverse\nproblems is shown to amount to an iterated denoising process. However, as the\ncomplexity of denoising algorithms is generally high, this might lead to an\noverall slow algorithm. In this paper, we suggest an accelerated technique\nbased on vector extrapolation (VE) to speed-up existing RED solvers. Numerical\nexperiments validate the obtained gain by VE, leading to a substantial savings\nin computations compared with the original fixed-point method.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 06:43:02 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 21:00:07 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hong", "Tao", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1805.02164", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Tiankuang Zhou, Zhibo Chen", "title": "Multi-Scale Face Restoration with Sequential Gating Ensemble Network", "comments": "8 pages, 7 figures, Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restoring face images from distortions is important in face recognition\napplications and is challenged by multiple scale issues, which is still not\nwell-solved in research area. In this paper, we present a Sequential Gating\nEnsemble Network (SGEN) for multi-scale face restoration issue. We first employ\nthe principle of ensemble learning into SGEN architecture design to reinforce\npredictive performance of the network. The SGEN aggregates multi-level\nbase-encoders and base-decoders into the network, which enables the network to\ncontain multiple scales of receptive field. Instead of combining these\nbase-en/decoders directly with non-sequential operations, the SGEN takes\nbase-en/decoders from different levels as sequential data. Specifically, the\nSGEN learns to sequentially extract high level information from base-encoders\nin bottom-up manner and restore low level information from base-decoders in\ntop-down manner. Besides, we propose to realize bottom-up and top-down\ninformation combination and selection with Sequential Gating Unit (SGU). The\nSGU sequentially takes two inputs from different levels and decides the output\nbased on one active input. Experiment results demonstrate that our SGEN is more\neffective at multi-scale human face restoration with more image details and\nless noise than state-of-the-art image restoration models. By using adversarial\ntraining, SGEN also produces more visually preferred results than other models\nthrough subjective evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 07:38:42 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Lin", "Jianxin", ""], ["Zhou", "Tiankuang", ""], ["Chen", "Zhibo", ""]]}, {"id": "1805.02165", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding, John Paisley", "title": "Joint CS-MRI Reconstruction and Segmentation with a Unified Deep Network", "comments": "8 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for fast acquisition and automatic analysis of MRI data is growing\nin the age of big data. Although compressed sensing magnetic resonance imaging\n(CS-MRI) has been studied to accelerate MRI by reducing k-space measurements,\nin current CS-MRI techniques MRI applications such as segmentation are\noverlooked when doing image reconstruction. In this paper, we test the utility\nof CS-MRI methods in automatic segmentation models and propose a unified deep\nneural network architecture called SegNetMRI which we apply to the combined\nCS-MRI reconstruction and segmentation problem. SegNetMRI is built upon a MRI\nreconstruction network with multiple cascaded blocks each containing an\nencoder-decoder unit and a data fidelity unit, and MRI segmentation networks\nhaving the same encoder-decoder structure. The two subnetworks are pre-trained\nand fine-tuned with shared reconstruction encoders. The outputs are merged into\nthe final segmentation. Our experiments show that SegNetMRI can improve both\nthe reconstruction and segmentation performance when using compressive\nmeasurements.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 07:45:32 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Sun", "Liyan", ""], ["Fan", "Zhiwen", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Paisley", "John", ""]]}, {"id": "1805.02173", "submitter": "Vishal Agarwal", "authors": "Vishal Agarwal, Diwanshu Jain, A. Vamshi Krishna Reddy, Frank\n  Chung-Hoon Rhee", "title": "An Interval Type-2 Fuzzy Approach to Automatic PDF Generation for\n  Histogram Specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image enhancement plays an important role in several application in the field\nof computer vision and image processing. Histogram specification (HS) is one of\nthe most widely used techniques for contrast enhancement of an image, which\nrequires an appropriate probability density function for the transformation. In\nthis paper, we propose a fuzzy method to find a suitable PDF automatically for\nhistogram specification using interval type - 2 (IT2) fuzzy approach, based on\nthe fuzzy membership values obtained from the histogram of input image. The\nproposed algorithm works in 5 stages which includes - symmetric Gaussian\nfitting on the histogram, extraction of IT2 fuzzy membership functions (MFs)\nand therefore, footprint of uncertainty (FOU), obtaining membership value (MV),\ngenerating PDF and application of HS. We have proposed 4 different methods to\nfind membership values - point-wise method, center of weight method, area\nmethod, and karnik-mendel (KM) method. The framework is sensitive to local\nvariations in the histogram and chooses the best PDF so as to improve contrast\nenhancement. Experimental validity of the methods used is illustrated by\nqualitative and quantitative analysis on several images using the image quality\nindex - Average Information Content (AIC) or Entropy, and by comparison with\nthe commonly used algorithms such as Histogram Equalization (HE), Recursive\nMean-Separate Histogram Equalization (RMSHE) and Brightness Preserving Fuzzy\nHistogram Equalization (BPFHE). It has been found out that on an average, our\nalgorithm improves the AIC index by 11.5% as compared to the index obtained by\nhistogram equalisation.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 09:08:46 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Agarwal", "Vishal", ""], ["Jain", "Diwanshu", ""], ["Reddy", "A. Vamshi Krishna", ""], ["Rhee", "Frank Chung-Hoon", ""]]}, {"id": "1805.02242", "submitter": "Wenjie Ruan", "authors": "Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska", "title": "Reachability Analysis of Deep Neural Networks with Provable Guarantees", "comments": "This is the long version of the conference paper accepted in\n  IJCAI-2018. Github: https://github.com/TrustAI/DeepGO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying correctness of deep neural networks (DNNs) is challenging. We study\na generic reachability problem for feed-forward DNNs which, for a given set of\ninputs to the network and a Lipschitz-continuous function over its outputs,\ncomputes the lower and upper bound on the function values. Because the network\nand the function are Lipschitz continuous, all values in the interval between\nthe lower and upper bound are reachable. We show how to obtain the safety\nverification problem, the output range analysis problem and a robustness\nmeasure by instantiating the reachability problem. We present a novel algorithm\nbased on adaptive nested optimisation to solve the reachability problem. The\ntechnique has been implemented and evaluated on a range of DNNs, demonstrating\nits efficiency, scalability and ability to handle a broader class of networks\nthan state-of-the-art verification approaches.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 16:33:52 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Ruan", "Wenjie", ""], ["Huang", "Xiaowei", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "1805.02279", "submitter": "Naji Khosravan", "authors": "Naji Khosravan and Ulas Bagci", "title": "S4ND: Single-Shot Single-Scale Lung Nodule Detection", "comments": "Accepted for publication at MICCAI 2018 (21st International\n  Conference on Medical Image Computing and Computer Assisted Intervention)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art lung nodule detection studies rely on computationally\nexpensive multi-stage frameworks to detect nodules from CT scans. To address\nthis computational challenge and provide better performance, in this paper we\npropose S4ND, a new deep learning based method for lung nodule detection. Our\napproach uses a single feed forward pass of a single network for detection and\nprovides better performance when compared to the current literature. The whole\ndetection pipeline is designed as a single $3D$ Convolutional Neural Network\n(CNN) with dense connections, trained in an end-to-end manner. S4ND does not\nrequire any further post-processing or user guidance to refine detection\nresults. Experimentally, we compared our network with the current\nstate-of-the-art object detection network (SSD) in computer vision as well as\nthe state-of-the-art published method for lung nodule detection (3D DCNN). We\nused publically available $888$ CT scans from LUNA challenge dataset and showed\nthat the proposed method outperforms the current literature both in terms of\nefficiency and accuracy by achieving an average FROC-score of $0.897$. We also\nprovide an in-depth analysis of our proposed network to shed light on the\nunclear paradigms of tiny object detection.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 21:32:14 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 18:26:28 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Khosravan", "Naji", ""], ["Bagci", "Ulas", ""]]}, {"id": "1805.02283", "submitter": "Yichun Shi", "authors": "Yichun Shi and Anil K. Jain", "title": "DocFace: Matching ID Document Photos to Selfies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous activities in our daily life, including transactions, access to\nservices and transportation, require us to verify who we are by showing our ID\ndocuments containing face images, e.g. passports and driver licenses. An\nautomatic system for matching ID document photos to live face images in real\ntime with high accuracy would speedup the verification process and remove the\nburden on human operators. In this paper, by employing the transfer learning\ntechnique, we propose a new method, DocFace, to train a domain-specific network\nfor ID document photo matching without a large dataset. Compared with the\nbaseline of applying existing methods for general face recognition to this\nproblem, our method achieves considerable improvement. A cross validation on an\nID-Selfie dataset shows that DocFace improves the TAR from 61.14% to 92.77% at\nFAR=0.1%. Experimental results also indicate that given more training data, a\nviable system for automatic ID document photo matching can be developed and\ndeployed.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 22:17:35 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Shi", "Yichun", ""], ["Jain", "Anil K.", ""]]}, {"id": "1805.02335", "submitter": "Chenyang Si", "authors": "Chenyang Si, Ya Jing, Wei Wang, Liang Wang, Tieniu Tan", "title": "Skeleton-Based Action Recognition with Spatial Reasoning and Temporal\n  Stack Learning", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based action recognition has made great progress recently, but many\nproblems still remain unsolved. For example, most of the previous methods model\nthe representations of skeleton sequences without abundant spatial structure\ninformation and detailed temporal dynamics features. In this paper, we propose\na novel model with spatial reasoning and temporal stack learning (SR-TSL) for\nskeleton based action recognition, which consists of a spatial reasoning\nnetwork (SRN) and a temporal stack learning network (TSLN). The SRN can capture\nthe high-level spatial structural information within each frame by a residual\ngraph neural network, while the TSLN can model the detailed temporal dynamics\nof skeleton sequences by a composition of multiple skip-clip LSTMs. During\ntraining, we propose a clip-based incremental loss to optimize the model. We\nperform extensive experiments on the SYSU 3D Human-Object Interaction dataset\nand NTU RGB+D dataset and verify the effectiveness of each network of our\nmodel. The comparison results illustrate that our approach achieves much better\nresults than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 03:54:28 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 03:09:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Si", "Chenyang", ""], ["Jing", "Ya", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1805.02336", "submitter": "Chen Shen", "authors": "Chen Shen, Guo-Jun Qi, Rongxin Jiang, Zhongming Jin, Hongwei Yong,\n  Yaowu Chen, Xian-Sheng Hua", "title": "Sharp Attention Network via Adaptive Sampling for Person\n  Re-identification", "comments": "accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology(T-CSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present novel sharp attention networks by adaptively\nsampling feature maps from convolutional neural networks (CNNs) for person\nre-identification (re-ID) problem. Due to the introduction of sampling-based\nattention models, the proposed approach can adaptively generate sharper\nattention-aware feature masks. This greatly differs from the gating-based\nattention mechanism that relies soft gating functions to select the relevant\nfeatures for person re-ID. In contrast, the proposed sampling-based attention\nmechanism allows us to effectively trim irrelevant features by enforcing the\nresultant feature masks to focus on the most discriminative features. It can\nproduce sharper attentions that are more assertive in localizing subtle\nfeatures relevant to re-identifying people across cameras. For this purpose, a\ndifferentiable Gumbel-Softmax sampler is employed to approximate the Bernoulli\nsampling to train the sharp attention networks. Extensive experimental\nevaluations demonstrate the superiority of this new sharp attention model for\nperson re-ID over the other state-of-the-art methods on three challenging\nbenchmarks including CUHK03, Market-1501, and DukeMTMC-reID.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 03:54:40 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 11:27:49 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Shen", "Chen", ""], ["Qi", "Guo-Jun", ""], ["Jiang", "Rongxin", ""], ["Jin", "Zhongming", ""], ["Yong", "Hongwei", ""], ["Chen", "Yaowu", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1805.02339", "submitter": "Lingfeng Zhang", "authors": "Lingfeng Zhang, Ioannis A. Kakadiaris", "title": "A Hierarchical Matcher using Local Classifier Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on improving the performance of current convolutional\nneural networks in visual recognition without changing the network\narchitecture. A hierarchical matcher is proposed that builds chains of local\nbinary neural networks after one global neural network over all the class\nlabels, named as Local Classifier Chains based Convolutional Neural Network\n(LCC-CNN). The signature of each sample as two components: global component\nbased on the global network; local component based on local binary networks.\nThe local networks are built based on label pairs created by a similarity\nmatrix and confusion matrix. During matching, each sample travels through one\nglobal network and a chain of local networks to obtain its final matching to\navoid error propagation. The proposed matcher has been evaluated with image\nrecognition, character recognition and face recognition datasets. The\nexperimental results indicate that the proposed matcher achieves better\nperformance when compared with methods using only a global deep network.\nCompared with the UR2D system, the accuracy is improved significantly by 1% and\n0.17% on the UHDB31 dataset and the IJB-A dataset, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 04:29:19 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zhang", "Lingfeng", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1805.02352", "submitter": "Wojciech Chojnacki", "authors": "Wojciech Chojnacki and Zygmunt L. Szpak", "title": "Full explicit consistency constraints in uncalibrated multiple\n  homography estimation", "comments": "18 pages, 3 figures", "journal-ref": "Lecture Notes in Computer Science, vol. 11361, pp. 656-675,\n  Springer, Cham, 2019", "doi": "10.1007/978-3-030-20887-5_41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reveal a complete set of constraints that need to be imposed on a set of\n3-by-3 matrices to ensure that the matrices represent genuine homographies\nassociated with multiple planes between two views. We also show how to exploit\nthe constraints to obtain more accurate estimates of homography matrices\nbetween two views. Our study resolves a long-standing research question and\nprovides a fresh perspective and a more in-depth understanding of the multiple\nhomography estimation task.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 05:51:12 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 03:40:35 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 07:40:39 GMT"}, {"version": "v4", "created": "Fri, 24 Aug 2018 06:39:25 GMT"}, {"version": "v5", "created": "Tue, 27 Nov 2018 04:18:17 GMT"}, {"version": "v6", "created": "Tue, 19 Feb 2019 06:40:01 GMT"}, {"version": "v7", "created": "Wed, 29 May 2019 03:10:44 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Chojnacki", "Wojciech", ""], ["Szpak", "Zygmunt L.", ""]]}, {"id": "1805.02369", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra", "title": "GAN Based Medical Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to image registration consist of time consuming\niterative methods. Most current deep learning (DL) based registration methods\nextract deep features to use in an iterative setting. We propose an end-to-end\nDL method for registering multimodal images. Our approach uses generative\nadversarial networks (GANs) that eliminates the need for time consuming\niterative methods, and directly generates the registered image with the\ndeformation field. Appropriate constraints in the GAN cost function produce\naccurately registered images in less than a second. Experiments demonstrate\ntheir accuracy for multimodal retinal and cardiac MR image registration.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 07:03:04 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 06:43:41 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 00:23:31 GMT"}, {"version": "v4", "created": "Tue, 10 Sep 2019 07:59:40 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Mahapatra", "Dwarikanath", ""]]}, {"id": "1805.02397", "submitter": "Moi Hoon Yap", "authors": "Walied Merghani and Adrian K. Davison and Moi Hoon Yap", "title": "A Review on Facial Micro-Expressions Analysis: Datasets, Features and\n  Metrics", "comments": "Preprint submitted to IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial micro-expressions are very brief, spontaneous facial expressions that\nappear on the face of humans when they either deliberately or unconsciously\nconceal an emotion. Micro-expression has shorter duration than\nmacro-expression, which makes it more challenging for human and machine. Over\nthe past ten years, automatic micro-expressions recognition has attracted\nincreasing attention from researchers in psychology, computer science,\nsecurity, neuroscience and other related disciplines. The aim of this paper is\nto provide the insights of automatic micro-expressions and recommendations for\nfuture research. There has been a lot of datasets released over the last decade\nthat facilitated the rapid growth in this field. However, comparison across\ndifferent datasets is difficult due to the inconsistency in experiment\nprotocol, features used and evaluation methods. To address these issues, we\nreview the datasets, features and the performance metrics deployed in the\nliterature. Relevant challenges such as the spatial temporal settings during\ndata collection, emotional classes versus objective classes in data labelling,\nface regions in data analysis, standardisation of metrics and the requirements\nfor real-world implementation are discussed. We conclude by proposing some\npromising future directions to advancing micro-expressions research.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 08:29:24 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Merghani", "Walied", ""], ["Davison", "Adrian K.", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1805.02456", "submitter": "Xudong Mao", "authors": "Xudong Mao and Qing Li", "title": "Unpaired Multi-Domain Image Generation via Regularized Conditional GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of multi-domain image generation, the\ngoal of which is to generate pairs of corresponding images from different\ndomains. With the recent development in generative models, image generation has\nachieved great progress and has been applied to various computer vision tasks.\nHowever, multi-domain image generation may not achieve the desired performance\ndue to the difficulty of learning the correspondence of different domain\nimages, especially when the information of paired samples is not given. To\ntackle this problem, we propose Regularized Conditional GAN (RegCGAN) which is\ncapable of learning to generate corresponding images in the absence of paired\ntraining data. RegCGAN is based on the conditional GAN, and we introduce two\nregularizers to guide the model to learn the corresponding semantics of\ndifferent domains. We evaluate the proposed model on several tasks for which\npaired training data is not given, including the generation of edges and\nphotos, the generation of faces with different attributes, etc. The\nexperimental results show that our model can successfully generate\ncorresponding images for all these tasks, while outperforms the baseline\nmethods. We also introduce an approach of applying RegCGAN to unsupervised\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 11:52:28 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Mao", "Xudong", ""], ["Li", "Qing", ""]]}, {"id": "1805.02459", "submitter": "Xiangbo Shu", "authors": "Lu Jin, Xiangbo Shu, Kai Li, Zechao Li, Guo-Jun Qi, and Jinhui Tang", "title": "Deep Ordinal Hashing with Spatial Attention", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2883522", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has attracted increasing research attentions in recent years due to\nits high efficiency of computation and storage in image retrieval. Recent works\nhave demonstrated the superiority of simultaneous feature representations and\nhash functions learning with deep neural networks. However, most existing deep\nhashing methods directly learn the hash functions by encoding the global\nsemantic information, while ignoring the local spatial information of images.\nThe loss of local spatial structure makes the performance bottleneck of hash\nfunctions, therefore limiting its application for accurate similarity\nretrieval. In this work, we propose a novel Deep Ordinal Hashing (DOH) method,\nwhich learns ordinal representations by leveraging the ranking structure of\nfeature space from both local and global views. In particular, to effectively\nbuild the ranking structure, we propose to learn the rank correlation space by\nexploiting the local spatial information from Fully Convolutional Network (FCN)\nand the global semantic information from the Convolutional Neural Network (CNN)\nsimultaneously. More specifically, an effective spatial attention model is\ndesigned to capture the local spatial information by selectively learning\nwell-specified locations closely related to target objects. In such hashing\nframework,the local spatial and global semantic nature of images are captured\nin an end-to-end ranking-to-hashing manner. Experimental results conducted on\nthree widely-used datasets demonstrate that the proposed DOH method\nsignificantly outperforms the state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 11:59:55 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Jin", "Lu", ""], ["Shu", "Xiangbo", ""], ["Li", "Kai", ""], ["Li", "Zechao", ""], ["Qi", "Guo-Jun", ""], ["Tang", "Jinhui", ""]]}, {"id": "1805.02475", "submitter": "Sebastian Bodenstedt", "authors": "Sebastian Bodenstedt, Max Allan, Anthony Agustinos, Xiaofei Du, Luis\n  Garcia-Peraza-Herrera, Hannes Kenngott, Thomas Kurmann, Beat M\\\"uller-Stich,\n  Sebastien Ourselin, Daniil Pakhomov, Raphael Sznitman, Marvin Teichmann,\n  Martin Thoma, Tom Vercauteren, Sandrine Voros, Martin Wagner, Pamela Wochner,\n  Lena Maier-Hein, Danail Stoyanov, Stefanie Speidel", "title": "Comparative evaluation of instrument segmentation and tracking methods\n  in minimally invasive surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intraoperative segmentation and tracking of minimally invasive instruments is\na prerequisite for computer- and robotic-assisted surgery. Since additional\nhardware like tracking systems or the robot encoders are cumbersome and lack\naccuracy, surgical vision is evolving as promising techniques to segment and\ntrack the instruments using only the endoscopic images. However, what is\nmissing so far are common image data sets for consistent evaluation and\nbenchmarking of algorithms against each other. The paper presents a comparative\nvalidation study of different vision-based methods for instrument segmentation\nand tracking in the context of robotic as well as conventional laparoscopic\nsurgery. The contribution of the paper is twofold: we introduce a comprehensive\nvalidation data set that was provided to the study participants and present the\nresults of the comparative validation study. Based on the results of the\nvalidation study, we arrive at the conclusion that modern deep learning\napproaches outperform other methods in instrument segmentation tasks, but the\nresults are still not perfect. Furthermore, we show that merging results from\ndifferent methods actually significantly increases accuracy in comparison to\nthe best stand-alone method. On the other hand, the results of the instrument\ntracking task show that this is still an open challenge, especially during\nchallenging scenarios in conventional laparoscopic surgery.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 12:39:21 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Bodenstedt", "Sebastian", ""], ["Allan", "Max", ""], ["Agustinos", "Anthony", ""], ["Du", "Xiaofei", ""], ["Garcia-Peraza-Herrera", "Luis", ""], ["Kenngott", "Hannes", ""], ["Kurmann", "Thomas", ""], ["M\u00fcller-Stich", "Beat", ""], ["Ourselin", "Sebastien", ""], ["Pakhomov", "Daniil", ""], ["Sznitman", "Raphael", ""], ["Teichmann", "Marvin", ""], ["Thoma", "Martin", ""], ["Vercauteren", "Tom", ""], ["Voros", "Sandrine", ""], ["Wagner", "Martin", ""], ["Wochner", "Pamela", ""], ["Maier-Hein", "Lena", ""], ["Stoyanov", "Danail", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1805.02481", "submitter": "David Keetae Park", "authors": "David Keetae Park, Seungjoo Yoo, Hyojin Bahng, Jaegul Choo, Noseong\n  Park", "title": "MEGAN: Mixture of Experts of Generative Adversarial Networks for\n  Multimodal Image Generation", "comments": "27th International Joint Conference on Artificial Intelligence (IJCAI\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, generative adversarial networks (GANs) have shown promising\nperformance in generating realistic images. However, they often struggle in\nlearning complex underlying modalities in a given dataset, resulting in\npoor-quality generated images. To mitigate this problem, we present a novel\napproach called mixture of experts GAN (MEGAN), an ensemble approach of\nmultiple generator networks. Each generator network in MEGAN specializes in\ngenerating images with a particular subset of modalities, e.g., an image class.\nInstead of incorporating a separate step of handcrafted clustering of multiple\nmodalities, our proposed model is trained through an end-to-end learning of\nmultiple generators via gating networks, which is responsible for choosing the\nappropriate generator network for a given condition. We adopt the categorical\nreparameterization trick for a categorical decision to be made in selecting a\ngenerator while maintaining the flow of the gradients. We demonstrate that\nindividual generators learn different and salient subparts of the data and\nachieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA\nand a competitive unsupervised inception score of 8.33 in CIFAR-10.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 12:49:04 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 08:11:44 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Park", "David Keetae", ""], ["Yoo", "Seungjoo", ""], ["Bahng", "Hyojin", ""], ["Choo", "Jaegul", ""], ["Park", "Noseong", ""]]}, {"id": "1805.02505", "submitter": "Rudrasis Chakraborty Mr", "authors": "Rudrasis Chakraborty, Monami Banerjee and Baba C. Vemuri", "title": "Dictionary Learning and Sparse Coding on Statistical Manifolds", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.06939", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel information theoretic framework for\ndictionary learning (DL) and sparse coding (SC) on a statistical manifold (the\nmanifold of probability distributions). Unlike the traditional DL and SC\nframework, our new formulation does not explicitly incorporate any sparsity\ninducing norm in the cost function being optimized but yet yields sparse codes.\nOur algorithm approximates the data points on the statistical manifold (which\nare probability distributions) by the weighted Kullback-Leibeler center/mean\n(KL-center) of the dictionary atoms. The KL-center is defined as the minimizer\nof the maximum KL-divergence between itself and members of the set whose center\nis being sought. Further, we prove that the weighted KL-center is a sparse\ncombination of the dictionary atoms. This result also holds for the case when\nthe KL-divergence is replaced by the well known Hellinger distance. From an\napplications perspective, we present an extension of the aforementioned\nframework to the manifold of symmetric positive definite matrices (which can be\nidentified with the manifold of zero mean gaussian distributions),\n$\\mathcal{P}_n$. We present experiments involving a variety of dictionary-based\nreconstruction and classification problems in Computer Vision. Performance of\nthe proposed algorithm is demonstrated by comparing it to several\nstate-of-the-art methods in terms of reconstruction and classification accuracy\nas well as sparsity of the chosen representation.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 22:00:38 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Banerjee", "Monami", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1805.02513", "submitter": "Lin Ma", "authors": "Yongyi Tang, Lin Ma, Wei Liu, Weishi Zheng", "title": "Long-Term Human Motion Prediction by Modeling Motion Context and\n  Enhancing Motion Dynamic", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction aims at generating future frames of human motion\nbased on an observed sequence of skeletons. Recent methods employ the latest\nhidden states of a recurrent neural network (RNN) to encode the historical\nskeletons, which can only address short-term prediction. In this work, we\npropose a motion context modeling by summarizing the historical human motion\nwith respect to the current prediction. A modified highway unit (MHU) is\nproposed for efficiently eliminating motionless joints and estimating next pose\ngiven the motion context. Furthermore, we enhance the motion dynamic by\nminimizing the gram matrix loss for long-term motion prediction. Experimental\nresults show that the proposed model can promisingly forecast the human future\nmovements, which yields superior performances over related state-of-the-art\napproaches. Moreover, specifying the motion context with the activity labels\nenables our model to perform human motion transfer.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 13:23:17 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Tang", "Yongyi", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Zheng", "Weishi", ""]]}, {"id": "1805.02523", "submitter": "Julian M\\\"uller", "authors": "Julian M\\\"uller and Klaus Dietmayer", "title": "Detecting Traffic Lights by Single Shot Detection", "comments": "Submitted to International Conference on Intelligent Transportation\n  Systems (ITSC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements in object detection are driven by the success of\nconvolutional neural networks (CNN). They are able to learn rich features\noutperforming hand-crafted features. So far, research in traffic light\ndetection mainly focused on hand-crafted features, such as color, shape or\nbrightness of the traffic light bulb. This paper presents a deep learning\napproach for accurate traffic light detection in adapting a single shot\ndetection (SSD) approach. SSD performs object proposals creation and\nclassification using a single CNN. The original SSD struggles in detecting very\nsmall objects, which is essential for traffic light detection. By our\nadaptations it is possible to detect objects much smaller than ten pixels\nwithout increasing the input image size. We present an extensive evaluation on\nthe DriveU Traffic Light Dataset (DTLD). We reach both, high accuracy and low\nfalse positive rates. The trained model is real-time capable with ten frames\nper second on a Nvidia Titan Xp. Code has been made available at\nhttps://github.com/julimueller/tl_ssd.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 13:37:17 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 12:19:16 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 13:50:50 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["M\u00fcller", "Julian", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1805.02530", "submitter": "Bhaskaran David Prakash", "authors": "Bhaskaran David Prakash", "title": "Near-drowning Early Prediction Technique Using Novel Equations (NEPTUNE)\n  for Swimming Pools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is a critical aspect in all swimming pools. This paper describes a\nnear drowning early prediction technique using novel equations (NEPTUNE).\nNEPTUNE uses equations or rules that would be able to detect near drowning\nusing at least 1 but not more than 5 seconds of video sequence with no false\npositives. The backbone of NEPTUNE encompasses a mix of statistical image\nprocessing to merge images for a video sequence followed by K means clustering\nto extract segments in the merged image and finally a revisit to statistical\nimage processing to derive variables for every segment. These variables would\nbe used by the equations to identify near-drowning. NEPTUNE has the potential\nto be integrated into a swimming pool camera system that would send an alarm to\nthe lifeguards for early response so that the likelihood of recovery is high.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 13:51:54 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 14:56:37 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 11:53:37 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Prakash", "Bhaskaran David", ""]]}, {"id": "1805.02536", "submitter": "Francisco Cruz", "authors": "Francisco Cruz, Oriol Ramos Terrades", "title": "A probabilistic framework for handwritten text line segmentation", "comments": "47 pages, 23 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We successfully combine Expectation-Maximization algorithm and variational\napproaches for parameter learning and computing inference on Markov random\nfelds. This is a general method that can be applied to many computer vision\ntasks. In this paper, we apply it to handwritten text line segmentation. We\nconduct several experiments that demonstrate that our method deal with common\nissues of this task, such as complex document layout or non-latin scripts. The\nobtained results prove that our method achieve state-of-the-art performance on\ndifferent benchmark datasets without any particular fine tuning step.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 14:10:20 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 08:50:13 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Cruz", "Francisco", ""], ["Terrades", "Oriol Ramos", ""]]}, {"id": "1805.02543", "submitter": "Hannes Ovr\\'en", "authors": "Hannes Ovr\\'en and Per-Erik Forss\\'en", "title": "Trajectory Representation and Landmark Projection for Continuous-Time\n  Structure from Motion", "comments": "Submitted to IJRR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the problem of continuous-time structure from motion, and\nintroduces a number of extensions that improve convergence and efficiency. The\nformulation with a $\\mathcal{C}^2$-continuous spline for the trajectory\nnaturally incorporates inertial measurements, as derivatives of the sought\ntrajectory. We analyse the behaviour of split interpolation on $\\mathbb{SO}(3)$\nand on $\\mathbb{R}^3$, and a joint interpolation on $\\mathbb{SE}(3)$, and show\nthat the latter implicitly couples the direction of translation and rotation.\nSuch an assumption can make good sense for a camera mounted on a robot arm, but\nnot for hand-held or body-mounted cameras. Our experiments show that split\ninterpolation on $\\mathbb{SO}(3)$ and on $\\mathbb{R}^3$ is preferable over\n$\\mathbb{SE}(3)$ interpolation in all tested cases. Finally, we investigate the\nproblem of landmark reprojection on rolling shutter cameras, and show that the\ntested reprojection methods give similar quality, while their computational\nload varies by a factor of 2.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 14:29:30 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Ovr\u00e9n", "Hannes", ""], ["Forss\u00e9n", "Per-Erik", ""]]}, {"id": "1805.02556", "submitter": "Wu Zheng", "authors": "Wu Zheng, Lin Li, Zhaoxiang Zhang, Yan Huang, Liang Wang", "title": "Relational Network for Skeleton-Based Action Recognition", "comments": "Accepted by International Conference on Multimedia and Expo(ICME)\n  2019 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of effective and low-cost human skeleton capture\nsystems, skeleton-based action recognition has attracted much attention\nrecently. Most existing methods use Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN) to extract spatio-temporal information embedded\nin the skeleton sequences for action recognition. However, these approaches are\nlimited in the ability of relational modeling in a single skeleton, due to the\nloss of important structural information when converting the raw skeleton data\nto adapt to the input format of CNN or RNN. In this paper, we propose an\nAttentional Recurrent Relational Network-LSTM (ARRN-LSTM) to simultaneously\nmodel spatial configurations and temporal dynamics in skeletons for action\nrecognition. We introduce the Recurrent Relational Network to learn the spatial\nfeatures in a single skeleton, followed by a multi-layer LSTM to learn the\ntemporal features in the skeleton sequences. Between the two modules, we design\nan adaptive attentional module to focus attention on the most discriminative\nparts in the single skeleton. To exploit the complementarity from different\ngeometries in the skeleton for sufficient relational modeling, we design a\ntwo-stream architecture to learn the structural features among joints and lines\nsimultaneously. Extensive experiments are conducted on several popular skeleton\ndatasets and the results show that the proposed approach achieves better\nresults than most mainstream methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 14:59:54 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 07:01:34 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 06:34:04 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 12:36:13 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zheng", "Wu", ""], ["Li", "Lin", ""], ["Zhang", "Zhaoxiang", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""]]}, {"id": "1805.02579", "submitter": "Tengfei Long", "authors": "Tengfei Long, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang,\n  Bingfang Wu, Xiaomei Zhang, Guizhou Wang, Ranyu Yin", "title": "30m resolution Global Annual Burned Area Mapping based on Landsat images\n  and Google Earth Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heretofore, global burned area (BA) products are only available at coarse\nspatial resolution, since most of the current global BA products are produced\nwith the help of active fire detection or dense time-series change analysis,\nwhich requires very high temporal resolution. In this study, however, we focus\non automated global burned area mapping approach based on Landsat images. By\nutilizing the huge catalog of satellite imagery as well as the high-performance\ncomputing capacity of Google Earth Engine, we proposed an automated pipeline\nfor generating 30-meter resolution global-scale annual burned area map from\ntime-series of Landsat images, and a novel 30-meter resolution global annual\nburned area map of 2015 (GABAM 2015) is released. GABAM 2015 consists of\nspatial extent of fires that occurred during 2015 and not of fires that\noccurred in previous years. Cross-comparison with recent Fire_cci version 5.0\nBA product found a similar spatial distribution and a strong correlation\n($R^2=0.74$) between the burned areas from the two products, although\ndifferences were found in specific land cover categories (particularly in\nagriculture land). Preliminary global validation showed the commission and\nomission error of GABAM 2015 are 13.17% and 30.13%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 15:38:15 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Long", "Tengfei", ""], ["Zhang", "Zhaoming", ""], ["He", "Guojin", ""], ["Jiao", "Weili", ""], ["Tang", "Chao", ""], ["Wu", "Bingfang", ""], ["Zhang", "Xiaomei", ""], ["Wang", "Guizhou", ""], ["Yin", "Ranyu", ""]]}, {"id": "1805.02641", "submitter": "Hessam Bagherinezhad", "authors": "Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, Ali Farhadi", "title": "Label Refinery: Improving ImageNet Classification through Label\n  Progression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the three main components (data, labels, and models) of any supervised\nlearning system, data and models have been the main subjects of active\nresearch. However, studying labels and their properties has received very\nlittle attention. Current principles and paradigms of labeling impose several\nchallenges to machine learning algorithms. Labels are often incomplete,\nambiguous, and redundant. In this paper we study the effects of various\nproperties of labels and introduce the Label Refinery: an iterative procedure\nthat updates the ground truth labels after examining the entire dataset. We\nshow significant gain using refined labels across a wide range of models. Using\na Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet\nfrom 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from\n50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to\n74.47.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 17:52:42 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Bagherinezhad", "Hessam", ""], ["Horton", "Maxwell", ""], ["Rastegari", "Mohammad", ""], ["Farhadi", "Ali", ""]]}, {"id": "1805.02679", "submitter": "Subrahmanyam Murala", "authors": "Sonakshi Mathur, Mallika Chaudhary, Hemant Verma, Murari Mandal, S. K.\n  Vipparthi and Subrahmanyam Murala", "title": "Multichannel Distributed Local Pattern for Content Based Indexing and\n  Retrieval", "comments": "Accepted in INDICON-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel color feature descriptor, Multichannel Distributed Local Pattern\n(MDLP) is proposed in this manuscript. The MDLP combines the salient features\nof both local binary and local mesh patterns in the neighborhood. The\nmulti-distance information computed by the MDLP aids in robust extraction of\nthe texture arrangement. Further, MDLP features are extracted for each color\nchannel of an image. The retrieval performance of the MDLP is evaluated on the\nthree benchmark datasets for CBIR, namely Corel-5000, Corel-10000 and MIT-Color\nVistex respectively. The proposed technique attains substantial improvement as\ncompared to other state-of- the-art feature descriptors in terms of various\nevaluation parameters such as ARP and ARR on the respective databases.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 18:14:49 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Mathur", "Sonakshi", ""], ["Chaudhary", "Mallika", ""], ["Verma", "Hemant", ""], ["Mandal", "Murari", ""], ["Vipparthi", "S. K.", ""], ["Murala", "Subrahmanyam", ""]]}, {"id": "1805.02704", "submitter": "Wei Han", "authors": "Wei Han, Shiyu Chang, Ding Liu, Mo Yu, Michael Witbrock, Thomas S.\n  Huang", "title": "Image Super-Resolution via Dual-State Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in image super-resolution (SR) have recently benefited significantly\nfrom rapid developments in deep neural networks. Inspired by these recent\ndiscoveries, we note that many state-of-the-art deep SR architectures can be\nreformulated as a single-state recurrent neural network (RNN) with finite\nunfoldings. In this paper, we explore new structures for SR based on this\ncompact RNN view, leading us to a dual-state design, the Dual-State Recurrent\nNetwork (DSRN). Compared to its single state counterparts that operate at a\nfixed spatial resolution, DSRN exploits both low-resolution (LR) and\nhigh-resolution (HR) signals jointly. Recurrent signals are exchanged between\nthese states in both directions (both LR to HR and HR to LR) via delayed\nfeedback. Extensive quantitative and qualitative evaluations on benchmark\ndatasets and on a recent challenge demonstrate that the proposed DSRN performs\nfavorably against state-of-the-art algorithms in terms of both memory\nconsumption and predictive accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 19:23:17 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Han", "Wei", ""], ["Chang", "Shiyu", ""], ["Liu", "Ding", ""], ["Yu", "Mo", ""], ["Witbrock", "Michael", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1805.02718", "submitter": "Stephan Saalfeld", "authors": "Larissa Heinrich, Jan Funke, Constantin Pape, Juan Nunez-Iglesias,\n  Stephan Saalfeld", "title": "Synaptic Cleft Segmentation in Non-Isotropic Volume Electron Microscopy\n  of the Complete Drosophila Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural circuit reconstruction at single synapse resolution is increasingly\nrecognized as crucially important to decipher the function of biological\nnervous systems. Volume electron microscopy in serial transmission or scanning\nmode has been demonstrated to provide the necessary resolution to segment or\ntrace all neurites and to annotate all synaptic connections.\n  Automatic annotation of synaptic connections has been done successfully in\nnear isotropic electron microscopy of vertebrate model organisms. Results on\nnon-isotropic data in insect models, however, are not yet on par with human\nannotation.\n  We designed a new 3D-U-Net architecture to optimally represent isotropic\nfields of view in non-isotropic data. We used regression on a signed distance\ntransform of manually annotated synaptic clefts of the CREMI challenge dataset\nto train this model and observed significant improvement over the state of the\nart.\n  We developed open source software for optimized parallel prediction on very\nlarge volumetric datasets and applied our model to predict synaptic clefts in a\n50 tera-voxels dataset of the complete Drosophila brain. Our model generalizes\nwell to areas far away from where training data was available.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 19:48:19 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Heinrich", "Larissa", ""], ["Funke", "Jan", ""], ["Pape", "Constantin", ""], ["Nunez-Iglesias", "Juan", ""], ["Saalfeld", "Stephan", ""]]}, {"id": "1805.02730", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Alexandros Karargyris, Tanveer Syeda-Mahmood, Mehdi\n  Moradi", "title": "Building Disease Detection Algorithms with Very Small Numbers of\n  Positive Samples", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-66179-7_54", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning can provide promising results in medical image\nanalysis, the lack of very large annotated datasets confines its full\npotential. Furthermore, limited positive samples also create unbalanced\ndatasets which limit the true positive rates of trained models. As unbalanced\ndatasets are mostly unavoidable, it is greatly beneficial if we can extract\nuseful knowledge from negative samples to improve classification accuracy on\nlimited positive samples. To this end, we propose a new strategy for building\nmedical image analysis pipelines that target disease detection. We train a\ndiscriminative segmentation model only on normal images to provide a source of\nknowledge to be transferred to a disease detection classifier. We show that\nusing the feature maps of a trained segmentation network, deviations from\nnormal anatomy can be learned by a two-class classification network on an\nextremely unbalanced training dataset with as little as one positive for 17\nnegative samples. We demonstrate that even though the segmentation network is\nonly trained on normal cardiac computed tomography images, the resulting\nfeature maps can be used to detect pericardial effusion and cardiac septal\ndefects with two-class convolutional classification networks.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 20:26:14 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Karargyris", "Alexandros", ""], ["Syeda-Mahmood", "Tanveer", ""], ["Moradi", "Mehdi", ""]]}, {"id": "1805.02733", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Learning Optical Flow via Dilated Networks and Occlusion Reasoning", "comments": "Accepted at ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant progress that has been made on estimating optical\nflow recently, most estimation methods, including classical and deep learning\napproaches, still have difficulty with multi-scale estimation, real-time\ncomputation, and/or occlusion reasoning. In this paper, we introduce dilated\nconvolution and occlusion reasoning into unsupervised optical flow estimation\nto address these issues. The dilated convolution allows our network to avoid\nupsampling via deconvolution and the resulting gridding artifacts. Dilated\nconvolution also results in a smaller memory footprint which speeds up\ninterference. The occlusion reasoning prevents our network from learning\nincorrect deformations due to occluded image regions during training. Our\nproposed method outperforms state-of-the-art unsupervised approaches on the\nKITTI benchmark. We also demonstrate its generalization capability by applying\nit to action recognition in video.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 20:31:56 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1805.02792", "submitter": "Rameswar Panda", "authors": "Shuyue Lan, Rameswar Panda, Qi Zhu, Amit K. Roy-Chowdhury", "title": "FFNet: Video Fast-Forwarding via Reinforcement Learning", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications with limited computation, communication, storage and\nenergy resources, there is an imperative need of computer vision methods that\ncould select an informative subset of the input video for efficient processing\nat or near real time. In the literature, there are two relevant groups of\napproaches: generating a trailer for a video or fast-forwarding while\nwatching/processing the video. The first group is supported by video\nsummarization techniques, which require processing of the entire video to\nselect an important subset for showing to users. In the second group, current\nfast-forwarding methods depend on either manual control or automatic adaptation\nof playback speed, which often do not present an accurate representation and\nmay still require processing of every frame. In this paper, we introduce\nFastForwardNet (FFNet), a reinforcement learning agent that gets inspiration\nfrom video summarization and does fast-forwarding differently. It is an online\nframework that automatically fast-forwards a video and presents a\nrepresentative subset of frames to users on the fly. It does not require\nprocessing the entire video, but just the portion that is selected by the\nfast-forward agent, which makes the process very computationally efficient. The\nonline nature of our proposed method also enables the users to begin\nfast-forwarding at any point of the video. Experiments on two real-world\ndatasets demonstrate that our method can provide better representation of the\ninput video with much less processing requirement.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 01:04:19 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Lan", "Shuyue", ""], ["Panda", "Rameswar", ""], ["Zhu", "Qi", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1805.02798", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Yefeng Zheng, S. Kevin Zhou, Bogdan Georgescu,\n  Puneet Sharma, Daguang Xu, Dorin Comaniciu, Ghassan Hamarneh", "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous segmentation of multiple organs from different medical imaging\nmodalities is a crucial task as it can be utilized for computer-aided\ndiagnosis, computer-assisted surgery, and therapy planning. Thanks to the\nrecent advances in deep learning, several deep neural networks for medical\nimage segmentation have been introduced successfully for this purpose. In this\npaper, we focus on learning a deep multi-organ segmentation network that labels\nvoxels. In particular, we examine the critical choice of a loss function in\norder to handle the notorious imbalance problem that plagues both the input and\noutput of a learning model. The input imbalance refers to the class-imbalance\nin the input training samples (i.e., small foreground objects embedded in an\nabundance of background voxels, as well as organs of varying sizes). The output\nimbalance refers to the imbalance between the false positives and false\nnegatives of the inference model. In order to tackle both types of imbalance\nduring training and inference, we introduce a new curriculum learning based\nloss function. Specifically, we leverage Dice similarity coefficient to deter\nmodel parameters from being held at bad local minima and at the same time\ngradually learn better model parameters by penalizing for false\npositives/negatives using a cross entropy term. We evaluated the proposed loss\nfunction on three datasets: whole body positron emission tomography (PET) scans\nwith 5 target organs, magnetic resonance imaging (MRI) prostate scans, and\nultrasound echocardigraphy images with a single target organ i.e., left\nventricular. We show that a simple network architecture with the proposed\nintegrative loss function can outperform state-of-the-art methods and results\nof the competing methods can be improved when our proposed loss is used.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 01:39:59 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 01:27:59 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 19:13:11 GMT"}, {"version": "v4", "created": "Mon, 24 Sep 2018 22:04:14 GMT"}, {"version": "v5", "created": "Mon, 22 Oct 2018 07:11:38 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Zheng", "Yefeng", ""], ["Zhou", "S. Kevin", ""], ["Georgescu", "Bogdan", ""], ["Sharma", "Puneet", ""], ["Xu", "Daguang", ""], ["Comaniciu", "Dorin", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1805.02825", "submitter": "Yi Zhang", "authors": "Yi Zhang, Zhengfei Wang, Guoxiong Xu, Hongshi Huang and Wenxin Li", "title": "N2RPP: An Adversarial Network to Rebuild Plantar Pressure for ACLD\n  Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foot is a vital part of human, and lots of valuable information is embedded.\nPlantar pressure is one of which contains this information and it describes\nhuman walking features. It is proved that once one has trouble with lower limb,\nthe distribution of plantar pressure will change to some degree. Plantar\npressure can be converted into images according to some simple standards. In\nthis paper, we take full advantage of these plantar pressure images for medical\nusage. We present N2RPP, a generative adversarial network (GAN) based method to\nrebuild plantar pressure images of anterior cruciate ligament deficiency (ACLD)\npatients from low dimension features, which are extracted from an autoencoder.\nThrough the result of experiments, the extracted features are a useful\nrepresentation to describe and rebuild plantar pressure images. According to\nN2RPP's results, we find out that there are several noteworthy differences\nbetween normal people and patients. This can provide doctors a rough direction\nof adjusting plantar pressure to a better distribution to reduce patients' sore\nand pain during the rehabilitation treatment for ACLD.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 04:14:24 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhang", "Yi", ""], ["Wang", "Zhengfei", ""], ["Xu", "Guoxiong", ""], ["Huang", "Hongshi", ""], ["Li", "Wenxin", ""]]}, {"id": "1805.02834", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Nathan Louis, Jason J. Corso", "title": "Weakly-Supervised Video Object Grounding from Text by Loss Weighting and\n  Object Interaction", "comments": "16 pages including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study weakly-supervised video object grounding: given a video segment and\na corresponding descriptive sentence, the goal is to localize objects that are\nmentioned from the sentence in the video. During training, no object bounding\nboxes are available, but the set of possible objects to be grounded is known\nbeforehand. Existing approaches in the image domain use Multiple Instance\nLearning (MIL) to ground objects by enforcing matches between visual and\nsemantic features. A naive extension of this approach to the video domain is to\ntreat the entire segment as a bag of spatial object proposals. However, an\nobject existing sparsely across multiple frames might not be detected\ncompletely since successfully spotting it from one single frame would trigger a\nsatisfactory match. To this end, we propagate the weak supervisory signal from\nthe segment level to frames that likely contain the target object. For frames\nthat are unlikely to contain the target objects, we use an alternative penalty\nloss. We also leverage the interactions among objects as a textual guide for\nthe grounding. We evaluate our model on the newly-collected benchmark\nYouCook2-BoundingBox and show improvements over competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 05:05:56 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 15:53:44 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Zhou", "Luowei", ""], ["Louis", "Nathan", ""], ["Corso", "Jason J.", ""]]}, {"id": "1805.02838", "submitter": "Sangho Lee", "authors": "Sangho Lee, Jinyoung Sung, Youngjae Yu, Gunhee Kim", "title": "A Memory Network Approach for Story-based Temporal Summarization of\n  360{\\deg} Videos", "comments": "Accepted paper at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of story-based temporal summarization of long\n360{\\deg} videos. We propose a novel memory network model named Past-Future\nMemory Network (PFMN), in which we first compute the scores of 81 normal field\nof view (NFOV) region proposals cropped from the input 360{\\deg} video, and\nthen recover a latent, collective summary using the network with two external\nmemories that store the embeddings of previously selected subshots and future\ncandidate subshots. Our major contributions are two-fold. First, our work is\nthe first to address story-based temporal summarization of 360{\\deg} videos.\nSecond, our model is the first attempt to leverage memory networks for video\nsummarization tasks. For evaluation, we perform three sets of experiments.\nFirst, we investigate the view selection capability of our model on the\nPano2Vid dataset. Second, we evaluate the temporal summarization with a newly\ncollected 360{\\deg} video dataset. Finally, we experiment our model's\nperformance in another domain, with image-based storytelling VIST dataset. We\nverify that our model achieves state-of-the-art performance on all the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 05:22:18 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 00:46:03 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 15:05:21 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lee", "Sangho", ""], ["Sung", "Jinyoung", ""], ["Yu", "Youngjae", ""], ["Kim", "Gunhee", ""]]}, {"id": "1805.02850", "submitter": "Sundaresh Ram", "authors": "Sundaresh Ram and Vicky T. Nguyen and Kirsten H. Limesand and Mert R.\n  Sabuncu", "title": "Joint Cell Nuclei Detection and Segmentation in Microscopy Images Using\n  3D Convolutional Networks", "comments": "We were not able to reproduce the results", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 3D convolutional neural network to simultaneously segment and\ndetect cell nuclei in confocal microscopy images. Mirroring the co-dependency\nof these tasks, our proposed model consists of two serial components: the first\npart computes a segmentation of cell bodies, while the second module identifies\nthe centers of these cells. Our model is trained end-to-end from scratch on a\nmouse parotid salivary gland stem cell nuclei dataset comprising 107 image\nstacks from three independent cell preparations, each containing several\nhundred individual cell nuclei in 3D. In our experiments, we conduct a thorough\nevaluation of both detection accuracy and segmentation quality, on two\ndifferent datasets. The results show that the proposed method provides\nsignificantly improved detection and segmentation accuracy compared to\nstate-of-the-art and benchmark algorithms. Finally, we use a previously\ndescribed test-time drop-out strategy to obtain uncertainty estimates on our\npredictions and validate these estimates by demonstrating that they are\nstrongly correlated with accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 06:15:33 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 16:45:07 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Ram", "Sundaresh", ""], ["Nguyen", "Vicky T.", ""], ["Limesand", "Kirsten H.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1805.02855", "submitter": "Neal Jean", "authors": "Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell,\n  Stefano Ermon", "title": "Tile2Vec: Unsupervised representation learning for spatially distributed\n  data", "comments": "8 pages, 4 figures in main text; 9 pages, 11 figures in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial analysis lacks methods like the word vector representations and\npre-trained networks that significantly boost performance across a wide range\nof natural language and computer vision tasks. To fill this gap, we introduce\nTile2Vec, an unsupervised representation learning algorithm that extends the\ndistributional hypothesis from natural language -- words appearing in similar\ncontexts tend to have similar meanings -- to spatially distributed data. We\ndemonstrate empirically that Tile2Vec learns semantically meaningful\nrepresentations on three datasets. Our learned representations significantly\nimprove performance in downstream classification tasks and, similar to word\nvectors, visual analogies can be obtained via simple arithmetic in the latent\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 06:40:40 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 09:26:16 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Jean", "Neal", ""], ["Wang", "Sherrie", ""], ["Samar", "Anshul", ""], ["Azzari", "George", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1805.02860", "submitter": "Qilin Zhang", "authors": "Yunfeng Wang, Wengang Zhou, Qilin Zhang, Houqiang Li", "title": "Visual Attribute-augmented Three-dimensional Convolutional Neural\n  Network for Enhanced Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attributes in individual video frames, such as the presence of\ncharacteristic objects and scenes, offer substantial information for action\nrecognition in videos. With individual 2D video frame as input, visual\nattributes extraction could be achieved effectively and efficiently with more\nsophisticated convolutional neural network than current 3D CNNs with\nspatio-temporal filters, thanks to fewer parameters in 2D CNNs. In this paper,\nthe integration of visual attributes (including detection, encoding and\nclassification) into multi-stream 3D CNN is proposed for action recognition in\ntrimmed videos, with the proposed visual Attribute-augmented 3D CNN (A3D)\nframework. The visual attribute pipeline includes an object detection network,\nan attributes encoding network and a classification network. Our proposed A3D\nframework achieves state-of-the-art performance on both the HMDB51 and the\nUCF101 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 07:09:12 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Wang", "Yunfeng", ""], ["Zhou", "Wengang", ""], ["Zhang", "Qilin", ""], ["Li", "Houqiang", ""]]}, {"id": "1805.02877", "submitter": "Qilin Zhang", "authors": "Yunfeng Wang, Wengang Zhou, Qilin Zhang, Xiaotian Zhu, Houqiang Li", "title": "Low-Latency Human Action Recognition with Weighted Multi-Region\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal contexts are crucial in understanding human actions in\nvideos. Recent state-of-the-art Convolutional Neural Network (ConvNet) based\naction recognition systems frequently involve 3D spatio-temporal ConvNet\nfilters, chunking videos into fixed length clips and Long Short Term Memory\n(LSTM) networks. Such architectures are designed to take advantage of both\nshort term and long term temporal contexts, but also requires the accumulation\nof a predefined number of video frames (e.g., to construct video clips for 3D\nConvNet filters, to generate enough inputs for LSTMs). For applications that\nrequire low-latency online predictions of fast-changing action scenes, a new\naction recognition system is proposed in this paper. Termed \"Weighted\nMulti-Region Convolutional Neural Network\" (WMR ConvNet), the proposed system\nis LSTM-free, and is based on 2D ConvNet that does not require the accumulation\nof video frames for 3D ConvNet filtering. Unlike early 2D ConvNets that are\nbased purely on RGB frames and optical flow frames, the WMR ConvNet is designed\nto simultaneously capture multiple spatial and short term temporal cues (e.g.,\nhuman poses, occurrences of objects in the background) with both the primary\nregion (foreground) and secondary regions (mostly background). On both the\nUCF101 and HMDB51 datasets, the proposed WMR ConvNet achieves the\nstate-of-the-art performance among competing low-latency algorithms.\nFurthermore, WMR ConvNet even outperforms the 3D ConvNet based C3D algorithm\nthat requires video frame accumulation. In an ablation study with the optical\nflow ConvNet stream removed, the ablated WMR ConvNet nevertheless outperforms\ncompeting algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 07:57:54 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Wang", "Yunfeng", ""], ["Zhou", "Wengang", ""], ["Zhang", "Qilin", ""], ["Zhu", "Xiaotian", ""], ["Li", "Houqiang", ""]]}, {"id": "1805.02901", "submitter": "Xun Xu", "authors": "Chao Zhang, Ce Zhu, Jimin Xiao, Xun Xu, Yipeng Liu", "title": "Image Ordinal Classification and Understanding: Grid Dropout with\n  Masking Label", "comments": "IEEE International Conference on Multimedia Expo (ICME Oral\n  Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image ordinal classification refers to predicting a discrete target value\nwhich carries ordering correlation among image categories. The limited size of\nlabeled ordinal data renders modern deep learning approaches easy to overfit.\nTo tackle this issue, neuron dropout and data augmentation were proposed which,\nhowever, still suffer from over-parameterization and breaking spatial\nstructure, respectively. To address the issues, we first propose a grid dropout\nmethod that randomly dropout/blackout some areas of the raining image. Then we\ncombine the objective of predicting the blackout patches with classification to\ntake advantage of the spatial information. Finally we demonstrate the\neffectiveness of both approaches by visualizing the Class Activation Map (CAM)\nand discover that grid dropout is more aware of the whole facial areas and more\nrobust than neuron dropout for small training dataset. Experiments are\nconducted on a challenging age estimation dataset - Adience dataset with very\ncompetitive results compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 08:58:54 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhang", "Chao", ""], ["Zhu", "Ce", ""], ["Xiao", "Jimin", ""], ["Xu", "Xun", ""], ["Liu", "Yipeng", ""]]}, {"id": "1805.02919", "submitter": "Daniel O\\~noro-Rubio", "authors": "Daniel O\\~noro-Rubio, Mathias Niepert, Roberto J. L\\'opez-Sastre", "title": "Learning Short-Cut Connections for Object Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object counting is an important task in computer vision due to its growing\ndemand in applications such as traffic monitoring or surveillance. In this\npaper, we consider object counting as a learning problem of a joint feature\nextraction and pixel-wise object density estimation with\nConvolutional-Deconvolutional networks. We introduce a novel counting model,\nnamed Gated U-Net (GU-Net). Specifically, we propose to enrich the U-Net\narchitecture with the concept of learnable short-cut connections. Standard\nshort-cut connections are connections between layers in deep neural networks\nwhich skip at least one intermediate layer. Instead of simply setting short-cut\nconnections, we propose to learn these connections from data. Therefore, our\nshort-cuts can work as gating units, which optimize the flow of information\nbetween convolutional and deconvolutional layers in the U-Net architecture. We\nevaluate the introduced GU-Net architecture on three commonly used benchmark\ndata sets for object counting. GU-Nets consistently outperform the base U-Net\narchitecture, and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 09:31:51 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 11:58:05 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["O\u00f1oro-Rubio", "Daniel", ""], ["Niepert", "Mathias", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1805.02924", "submitter": "Helen L Bear", "authors": "Kwanchiva Thangthai, Helen L Bear and Richard Harvey", "title": "Comparing phonemes and visemes with DNN-based lipreading", "comments": null, "journal-ref": "BMVC Lipreading Workshop 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is debate if phoneme or viseme units are the most effective for a\nlipreading system. Some studies use phoneme units even though phonemes describe\nunique short sounds; other studies tried to improve lipreading accuracy by\nfocusing on visemes with varying results. We compare the performance of a\nlipreading system by modeling visual speech using either 13 viseme or 38\nphoneme units. We report the accuracy of our system at both word and unit\nlevels. The evaluation task is large vocabulary continuous speech using the\nTCD-TIMIT corpus. We complete our visual speech modeling via hybrid DNN-HMMs\nand our visual speech decoder is a Weighted Finite-State Transducer (WFST). We\nuse DCT and Eigenlips as a representation of mouth ROI image. The phoneme\nlipreading system word accuracy outperforms the viseme based system word\naccuracy. However, the phoneme system achieved lower accuracy at the unit level\nwhich shows the importance of the dictionary for decoding classification\noutputs into words.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 09:51:34 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Thangthai", "Kwanchiva", ""], ["Bear", "Helen L", ""], ["Harvey", "Richard", ""]]}, {"id": "1805.02934", "submitter": "Helen L Bear", "authors": "Helen L Bear and Richard Harvey", "title": "Phoneme-to-viseme mappings: the good, the bad, and the ugly", "comments": null, "journal-ref": "Speech Communication, Special Issue on AV expressive speech. 2017", "doi": "10.1016/j.specom.2017.07.001", "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visemes are the visual equivalent of phonemes. Although not precisely\ndefined, a working definition of a viseme is \"a set of phonemes which have\nidentical appearance on the lips\". Therefore a phoneme falls into one viseme\nclass but a viseme may represent many phonemes: a many to one mapping. This\nmapping introduces ambiguity between phonemes when using viseme classifiers.\nNot only is this ambiguity damaging to the performance of audio-visual\nclassifiers operating on real expressive speech, there is also considerable\nchoice between possible mappings. In this paper we explore the issue of this\nchoice of viseme-to-phoneme map. We show that there is definite difference in\nperformance between viseme-to-phoneme mappings and explore why some maps appear\nto work better than others. We also devise a new algorithm for constructing\nphoneme-to-viseme mappings from labeled speech data. These new visemes, `Bear'\nvisemes, are shown to perform better than previously known units.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 10:32:57 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Bear", "Helen L", ""], ["Harvey", "Richard", ""]]}, {"id": "1805.02948", "submitter": "Helen L Bear", "authors": "Helen L Bear and Richard Harvey", "title": "Comparing heterogeneous visual gestures for measuring the diversity of\n  visual speech signals", "comments": null, "journal-ref": "Computer Speech and Language, May 2018", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual lip gestures observed whilst lipreading have a few working\ndefinitions, the most common two are; `the visual equivalent of a phoneme' and\n`phonemes which are indistinguishable on the lips'. To date there is no formal\ndefinition, in part because to date we have not established a two-way\nrelationship or mapping between visemes and phonemes. Some evidence suggests\nthat visual speech is highly dependent upon the speaker. So here, we use a\nphoneme-clustering method to form new phoneme-to-viseme maps for both\nindividual and multiple speakers. We test these phoneme to viseme maps to\nexamine how similarly speakers talk visually and we use signed rank tests to\nmeasure the distance between individuals. We conclude that broadly speaking,\nspeakers have the same repertoire of mouth gestures, where they differ is in\nthe use of the gestures.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 11:17:52 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Bear", "Helen L", ""], ["Harvey", "Richard", ""]]}, {"id": "1805.02996", "submitter": "Yujing Sun", "authors": "Yujing Sun, Yizhou Yu, Wenping Wang", "title": "Moir\\'{e} Photo Restoration Using Multiresolution Convolutional Neural\n  Networks", "comments": "13 pages, 19 figures, accepted to appear in IEEE Transactions on\n  Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2834737", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital cameras and mobile phones enable us to conveniently record precious\nmoments. While digital image quality is constantly being improved, taking\nhigh-quality photos of digital screens still remains challenging because the\nphotos are often contaminated with moir\\'{e} patterns, a result of the\ninterference between the pixel grids of the camera sensor and the device\nscreen. Moir\\'{e} patterns can severely damage the visual quality of photos.\nHowever, few studies have aimed to solve this problem. In this paper, we\nintroduce a novel multiresolution fully convolutional network for automatically\nremoving moir\\'{e} patterns from photos. Since a moir\\'{e} pattern spans over a\nwide range of frequencies, our proposed network performs a nonlinear\nmultiresolution analysis of the input image before computing how to cancel\nmoir\\'{e} artefacts within every frequency band. We also create a large-scale\nbenchmark dataset with $100,000^+$ image pairs for investigating and evaluating\nmoir\\'{e} pattern removal algorithms. Our network achieves state-of-the-art\nperformance on this dataset in comparison to existing learning architectures\nfor image restoration problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 13:17:44 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Sun", "Yujing", ""], ["Yu", "Yizhou", ""], ["Wang", "Wenping", ""]]}, {"id": "1805.02997", "submitter": "Yi Yu", "authors": "Yi Yu, Suhua Tang, Kiyoharu Aizawa, Akiko Aizawa", "title": "Category-Based Deep CCA for Fine-Grained Venue Discovery from Multimodal\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, travel destination and business location are taken as venues.\nDiscovering a venue by a photo is very important for context-aware\napplications. Unfortunately, few efforts paid attention to complicated real\nimages such as venue photos generated by users. Our goal is fine-grained venue\ndiscovery from heterogeneous social multimodal data. To this end, we propose a\nnovel deep learning model, Category-based Deep Canonical Correlation Analysis\n(C-DCCA). Given a photo as input, this model performs (i) exact venue search\n(find the venue where the photo was taken), and (ii) group venue search (find\nrelevant venues with the same category as that of the photo), by the\ncross-modal correlation between the input photo and textual description of\nvenues. In this model, data in different modalities are projected to a same\nspace via deep networks. Pairwise correlation (between different modal data\nfrom the same venue) for exact venue search and category-based correlation\n(between different modal data from different venues with the same category) for\ngroup venue search are jointly optimized. Because a photo cannot fully reflect\nrich text description of a venue, the number of photos per venue in the\ntraining phase is increased to capture more aspects of a venue. We build a new\nvenue-aware multimodal dataset by integrating Wikipedia featured articles and\nFoursquare venue photos. Experimental results on this dataset confirm the\nfeasibility of the proposed method. Moreover, the evaluation over another\npublicly available dataset confirms that the proposed method outperforms\nstate-of-the-arts for cross-modal retrieval between image and text.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 13:17:57 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Yu", "Yi", ""], ["Tang", "Suhua", ""], ["Aizawa", "Kiyoharu", ""], ["Aizawa", "Akiko", ""]]}, {"id": "1805.03054", "submitter": "Fuqiang Liu", "authors": "Fuqiang Liu, C. Liu", "title": "Towards Accurate and High-Speed Spiking Neuromorphic Systems with Data\n  Quantization-Aware Deep Networks", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3195970.3196131", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have gained immense success in cognitive\napplications and greatly pushed today's artificial intelligence forward. The\nbiggest challenge in executing DNNs is their extremely data-extensive\ncomputations. The computing efficiency in speed and energy is constrained when\ntraditional computing platforms are employed in such computational hungry\nexecutions. Spiking neuromorphic computing (SNC) has been widely investigated\nin deep networks implementation own to their high efficiency in computation and\ncommunication. However, weights and signals of DNNs are required to be\nquantized when deploying the DNNs on the SNC, which results in unacceptable\naccuracy loss. %However, the system accuracy is limited by quantizing data\ndirectly in deep networks deployment. Previous works mainly focus on weights\ndiscretize while inter-layer signals are mainly neglected. In this work, we\npropose to represent DNNs with fixed integer inter-layer signals and\nfixed-point weights while holding good accuracy. We implement the proposed DNNs\non the memristor-based SNC system as a deployment example. With 4-bit data\nrepresentation, our results show that the accuracy loss can be controlled\nwithin 0.02% (2.3%) on MNIST (CIFAR-10). Compared with the 8-bit dynamic\nfixed-point DNNs, our system can achieve more than 9.8x speedup, 89.1% energy\nsaving, and 30% area saving.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 14:30:19 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 05:27:49 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 08:47:23 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Fuqiang", ""], ["Liu", "C.", ""]]}, {"id": "1805.03064", "submitter": "Cristina Palmero", "authors": "Cristina Palmero, Javier Selva, Mohammad Ali Bagheri, Sergio Escalera", "title": "Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues", "comments": "Proc. of British Machine Vision Conference (BMVC), BMVC 2018. Errata:\n  in pg.5 the camera matrices of the transformation matrix W should be\n  interchanged (correct version: W=C_n*M*(C_o)^-1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze behavior is an important non-verbal cue in social signal processing and\nhuman-computer interaction. In this paper, we tackle the problem of person- and\nhead pose-independent 3D gaze estimation from remote cameras, using a\nmulti-modal recurrent convolutional neural network (CNN). We propose to combine\nface, eyes region, and face landmarks as individual streams in a CNN to\nestimate gaze in still images. Then, we exploit the dynamic nature of gaze by\nfeeding the learned features of all the frames in a sequence to a many-to-one\nrecurrent module that predicts the 3D gaze vector of the last frame. Our\nmulti-modal static solution is evaluated on a wide range of head poses and gaze\ndirections, achieving a significant improvement of 14.6% over the state of the\nart on EYEDIAP dataset, further improved by 4% when the temporal modality is\nincluded.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 14:44:18 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 08:59:54 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 11:09:42 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Palmero", "Cristina", ""], ["Selva", "Javier", ""], ["Bagheri", "Mohammad Ali", ""], ["Escalera", "Sergio", ""]]}, {"id": "1805.03081", "submitter": "Yunabo Wang", "authors": "Xin Yang, Yuanbo Wang, Yaru Wang, Baocai Yin, Qiang Zhang, Xiaopeng\n  Wei, Hongbo Fu", "title": "Active Object Reconstruction Using a Guided View Planner", "comments": "7 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent advance of image-based object reconstruction using\ndeep learning, we present an active reconstruction model using a guided view\nplanner. We aim to reconstruct a 3D model using images observed from a planned\nsequence of informative and discriminative views. But where are such\ninformative and discriminative views around an object? To address this we\npropose a unified model for view planning and object reconstruction, which is\nutilized to learn a guided information acquisition model and to aggregate\ninformation from a sequence of images for reconstruction. Experiments show that\nour model (1) increases our reconstruction accuracy with an increasing number\nof views (2) and generally predicts a more informative sequence of views for\nobject reconstruction compared to other alternative methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 15:00:23 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Yang", "Xin", ""], ["Wang", "Yuanbo", ""], ["Wang", "Yaru", ""], ["Yin", "Baocai", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""], ["Fu", "Hongbo", ""]]}, {"id": "1805.03096", "submitter": "Tewodros Habtegebrial", "authors": "Christian Bailer, Tewodros Habtegebrial, Kiran varanasi, Didier\n  Stricker", "title": "Fast Feature Extraction with CNNs with Pooling Layers", "comments": "Accepted at BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, many publications showed that convolutional neural network\nbased features can have a superior performance to engineered features. However,\nnot much effort was taken so far to extract local features efficiently for a\nwhole image. In this paper, we present an approach to compute patch-based local\nfeature descriptors efficiently in presence of pooling and striding layers for\nwhole images at once. Our approach is generic and can be applied to nearly all\nexisting network architectures. This includes networks for all local feature\nextraction tasks like camera calibration, Patchmatching, optical flow\nestimation and stereo matching. In addition, our approach can be applied to\nother patch-based approaches like sliding window object detection and\nrecognition. We complete our paper with a speed benchmark of popular CNN based\nfeature extraction approaches applied on a whole image, with and without our\nspeedup, and example code (for Torch) that shows how an arbitrary CNN\narchitecture can be easily converted by our approach.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 15:14:20 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Bailer", "Christian", ""], ["Habtegebrial", "Tewodros", ""], ["varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1805.03106", "submitter": "Carlo Innamorati", "authors": "Carlo Innamorati, Tobias Ritschel, Tim Weyrich, Niloy J. Mitra", "title": "Learning on the Edge: Explicit Boundary Handling in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) handle the case where filters extend\nbeyond the image boundary using several heuristics, such as zero, repeat or\nmean padding. These schemes are applied in an ad-hoc fashion and, being weakly\nrelated to the image content and oblivious of the target task, result in low\noutput quality at the boundary. In this paper, we propose a simple and\neffective improvement that learns the boundary handling itself. At\ntraining-time, the network is provided with a separate set of explicit boundary\nfilters. At testing-time, we use these filters which have learned to\nextrapolate features at the boundary in an optimal way for the specific task.\nOur extensive evaluation, over a wide range of architectural changes\n(variations of layers, feature channels, or both), shows how the explicit\nfilters result in improved boundary handling. Consequently, we demonstrate an\nimprovement of 5% to 20% across the board of typical CNN applications\n(colorization, de-Bayering, optical flow, and disparity estimation).\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 15:29:17 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Innamorati", "Carlo", ""], ["Ritschel", "Tobias", ""], ["Weyrich", "Tim", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1805.03134", "submitter": "Nils Murrugarra-Llerena", "authors": "Nils Murrugarra-Llerena and Adriana Kovashka", "title": "Image Retrieval with Mixed Initiative and Multimodal Feedback", "comments": "In submission to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How would you search for a unique, fashionable shoe that a friend wore and\nyou want to buy, but you didn't take a picture? Existing approaches propose\ninteractive image search as a promising venue. However, they either entrust the\nuser with taking the initiative to provide informative feedback, or give all\ncontrol to the system which determines informative questions to ask. Instead,\nwe propose a mixed-initiative framework where both the user and system can be\nactive participants, depending on whose initiative will be more beneficial for\nobtaining high-quality search results. We develop a reinforcement learning\napproach which dynamically decides which of three interaction opportunities to\ngive to the user: drawing a sketch, providing free-form attribute feedback, or\nanswering attribute-based questions. By allowing these three options, our\nsystem optimizes both the informativeness and exploration capabilities allowing\nfaster image retrieval. We outperform three baselines on three datasets and\nextensive experimental settings.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:13:24 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Murrugarra-Llerena", "Nils", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1805.03144", "submitter": "Andrew Beers", "authors": "Andrew Beers, James Brown, Ken Chang, J. Peter Campbell, Susan Ostmo,\n  Michael F. Chiang, and Jayashree Kalpathy-Cramer", "title": "High-resolution medical image synthesis using progressively grown\n  generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a class of unsupervised machine\nlearning algorithms that can produce realistic images from randomly-sampled\nvectors in a multi-dimensional space. Until recently, it was not possible to\ngenerate realistic high-resolution images using GANs, which has limited their\napplicability to medical images that contain biomarkers only detectable at\nnative resolution. Progressive growing of GANs is an approach wherein an image\ngenerator is trained to initially synthesize low resolution synthetic images\n(8x8 pixels), which are then fed to a discriminator that distinguishes these\nsynthetic images from real downsampled images. Additional convolutional layers\nare then iteratively introduced to produce images at twice the previous\nresolution until the desired resolution is reached. In this work, we\ndemonstrate that this approach can produce realistic medical images in two\ndifferent domains; fundus photographs exhibiting vascular pathology associated\nwith retinopathy of prematurity (ROP), and multi-modal magnetic resonance\nimages of glioma. We also show that fine-grained details associated with\npathology, such as retinal vessels or tumor heterogeneity, can be preserved and\nenhanced by including segmentation maps as additional channels. We envisage\nseveral applications of the approach, including image augmentation and\nunsupervised classification of pathology.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:25:13 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 14:23:53 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Beers", "Andrew", ""], ["Brown", "James", ""], ["Chang", "Ken", ""], ["Campbell", "J. Peter", ""], ["Ostmo", "Susan", ""], ["Chiang", "Michael F.", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "1805.03146", "submitter": "Guanlong Zhao", "authors": "Yu Liu and Guanlong Zhao", "title": "PAD-Net: A Perception-Aided Single Image Dehazing Network", "comments": "8 pages, 4 figures; project page:\n  https://github.com/guanlongzhao/single-image-dehazing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the possibility of replacing the $\\ell_2$ loss\nwith perceptually derived loss functions (SSIM, MS-SSIM, etc.) in training an\nend-to-end dehazing neural network. Objective experimental results suggest that\nby merely changing the loss function we can obtain significantly higher PSNR\nand SSIM scores on the SOTS set in the RESIDE dataset, compared with a\nstate-of-the-art end-to-end dehazing neural network (AOD-Net) that uses the\n$\\ell_2$ loss. The best PSNR we obtained was 23.50 (4.2% relative improvement),\nand the best SSIM we obtained was 0.8747 (2.3% relative improvement.)\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:31:07 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Liu", "Yu", ""], ["Zhao", "Guanlong", ""]]}, {"id": "1805.03170", "submitter": "Sandra Martinez", "authors": "Sandra Mart\\'inez and Oscar E. Mart\\'inez", "title": "Superresolution method for data deconvolution by superposition of point\n  sources", "comments": "Title was changed to clarify that it is a single acquisition method\n  and the sources superposed are virtual. We add a new example of the two\n  straight segments synthesize an artificial image. We explain how we construct\n  the IRF and we explain how we implement the genetic algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math-ph math.MP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a new algorithm for data deconvolution that allows\nthe retrieval of the target function with super-resolution with a simple\napproach that after a precis e measurement of the instrument response function\n(IRF), the measured data are fit by a superposition of point sources (SUPPOSe)\nof equal intensity. In this manner only the positions of the sources need to be\ndetermined by an algorithm that minimizes the norm of the difference between\nthe measured data and the convolution of the superposed point sources with the\nIRF. An upper bound for the uncertainty in the position of the sources was\nderived and two very different experimental situations were used for the test\n(an optical spectrum and fluorescent microscopy images) showing excellent\nreconstructions and agreement with the predicted uncertainties, achieving\n{\\lambda}/10 resolution for the microscope and a fivefold improvement in the\nspectral resolution for the spectrometer. The method also provides a way to\ndetermine the optimum number of sources to be used for the fit.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:13:31 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 20:11:10 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Mart\u00ednez", "Sandra", ""], ["Mart\u00ednez", "Oscar E.", ""]]}, {"id": "1805.03183", "submitter": "Avelino Forechi", "authors": "Avelino Forechi, Thiago Oliveira-Santos, Claudine Badue, Alberto F. De\n  Souza", "title": "Visual Global Localization with a Hybrid WNN-CNN Approach", "comments": "Accepted by IEEE 2018 International Joint Conference on Neural\n  Networks (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, self-driving cars rely greatly on the Global Positioning System\n(GPS) infrastructure, albeit there is an increasing demand for alternative\nmethods for GPS-denied environments. One of them is known as place recognition,\nwhich associates images of places with their corresponding positions. We\npreviously proposed systems based on Weightless Neural Networks (WNN) to\naddress this problem as a classification task. This encompasses solely one part\nof the global localization, which is not precise enough for driverless cars.\nInstead of just recognizing past places and outputting their poses, it is\ndesired that a global localization system estimates the pose of current place\nimages. In this paper, we propose to tackle this problem as follows. Firstly,\ngiven a live image, the place recognition system returns the most similar image\nand its pose. Then, given live and recollected images, a visual localization\nsystem outputs the relative camera pose represented by those images. To\nestimate the relative camera pose between the recollected and the current\nimages, a Convolutional Neural Network (CNN) is trained with the two images as\ninput and a relative pose vector as output. Together, these systems solve the\nglobal localization problem using the topological and metric information to\napproximate the current vehicle pose. The full approach is compared to a Real-\nTime Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM)\nsystem. Experimental results show that the proposed approach correctly\nlocalizes a vehicle 90% of the time with a mean error of 1.20m compared to\n1.12m of the SLAM system and 0.37m of the GPS, 89% of the time.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:34:31 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 18:57:37 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Forechi", "Avelino", ""], ["Oliveira-Santos", "Thiago", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""]]}, {"id": "1805.03189", "submitter": "Soumya Tripathy", "authors": "Soumya Tripathy, Juho Kannala and Esa Rahtu", "title": "Learning image-to-image translation using paired and unpaired training\n  samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is a general name for a task where an image from\none domain is converted to a corresponding image in another domain, given\nsufficient training data. Traditionally different approaches have been proposed\ndepending on whether aligned image pairs or two sets of (unaligned) examples\nfrom both domains are available for training. While paired training samples\nmight be difficult to obtain, the unpaired setup leads to a highly\nunder-constrained problem and inferior results. In this paper, we propose a new\ngeneral purpose image-to-image translation model that is able to utilize both\npaired and unpaired training data simultaneously. We compare our method with\ntwo strong baselines and obtain both qualitatively and quantitatively improved\nresults. Our model outperforms the baselines also in the case of purely paired\nand unpaired training data. To our knowledge, this is the first work to\nconsider such hybrid setup in image-to-image translation.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:44:28 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Tripathy", "Soumya", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "1805.03225", "submitter": "Siddharth Mahendran", "authors": "Siddharth Mahendran, Haider Ali, Rene Vidal", "title": "A Mixed Classification-Regression Framework for 3D Pose Estimation from\n  2D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D pose estimation from a single 2D image is an important and challenging\ntask in computer vision with applications in autonomous driving, robot\nmanipulation and augmented reality. Since 3D pose is a continuous quantity, a\nnatural formulation for this task is to solve a pose regression problem.\nHowever, since pose regression methods return a single estimate of the pose,\nthey have difficulties handling multimodal pose distributions (e.g. in the case\nof symmetric objects). An alternative formulation, which can capture multimodal\npose distributions, is to discretize the pose space into bins and solve a pose\nclassification problem. However, pose classification methods can give large\npose estimation errors depending on the coarseness of the discretization. In\nthis paper, we propose a mixed classification-regression framework that uses a\nclassification network to produce a discrete multimodal pose estimate and a\nregression network to produce a continuous refinement of the discrete estimate.\nThe proposed framework can accommodate different architectures and loss\nfunctions, leading to multiple classification-regression models, some of which\nachieve state-of-the-art performance on the challenging Pascal3D+ dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 18:32:04 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Mahendran", "Siddharth", ""], ["Ali", "Haider", ""], ["Vidal", "Rene", ""]]}, {"id": "1805.03269", "submitter": "Hassan Mansour", "authors": "Hassan Mansour, Dehong Liu, Ulugbek S. Kamilov, Petros T. Boufounos", "title": "Sparse Blind Deconvolution for Distributed Radar Autofocus Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem that arises in radar imaging systems, especially those\nmounted on mobile platforms, is antenna position ambiguity. Approaches to\nresolve this ambiguity and correct position errors are generally known as radar\nautofocus. Common techniques that attempt to resolve the antenna ambiguity\ngenerally assume an unknown gain and phase error afflicting the radar\nmeasurements. However, ensuring identifiability and tractability of the unknown\nerror imposes strict restrictions on the allowable antenna perturbations.\nFurthermore, these techniques are often not applicable in near-field imaging,\nwhere mapping the position ambiguity to phase errors breaks down.\n  In this paper, we propose an alternate formulation where the position error\nof each antenna is mapped to a spatial shift operator in the image-domain.\nThus, the radar autofocus problem becomes a multichannel blind deconvolution\nproblem, in which the radar measurements correspond to observations of a static\nradar image that is convolved with the spatial shift kernel associated with\neach antenna. To solve the reformulated problem, we also develop a block\ncoordinate descent framework that leverages the sparsity and piece-wise\nsmoothness of the radar scene, as well as the one-sparse property of the two\ndimensional shift kernels. We evaluate the performance of our approach using\nboth simulated and experimental radar measurements, and demonstrate its\nsuperior performance compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 20:30:31 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Mansour", "Hassan", ""], ["Liu", "Dehong", ""], ["Kamilov", "Ulugbek S.", ""], ["Boufounos", "Petros T.", ""]]}, {"id": "1805.03278", "submitter": "Thomas Schlegl", "authors": "Thomas Schlegl (1 and 2), Hrvoje Bogunovic (2), Sophie Klimscha (2),\n  Philipp Seeb\\\"ock (1 and 2), Amir Sadeghipour (2), Bianca Gerendas (2),\n  Sebastian M. Waldstein (2), Georg Langs (1), Ursula Schmidt-Erfurth (2) ((1)\n  Department of Biomedical Imaging and Image-guided Therapy, Computational\n  Imaging Research Lab, Medical University Vienna, Austria, (2) Christian\n  Doppler Laboratory for Ophthalmic Image Analysis, Department of Ophthalmology\n  and Optometry, Medical University Vienna, Austria)", "title": "Fully Automated Segmentation of Hyperreflective Foci in Optical\n  Coherence Tomography Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of disease related entities in retinal imaging data\nis relevant for disease- and treatment monitoring. It enables the quantitative\nassessment of large amounts of data and the corresponding study of disease\ncharacteristics. The presence of hyperreflective foci (HRF) is related to\ndisease progression in various retinal diseases. Manual identification of HRF\nin spectral-domain optical coherence tomography (SD-OCT) scans is error-prone\nand tedious. We present a fully automated machine learning approach for\nsegmenting HRF in SD-OCT scans. Evaluation on annotated OCT images of the\nretina demonstrates that a residual U-Net allows to segment HRF with high\naccuracy. As our dataset comprised data from different retinal diseases\nincluding age-related macular degeneration, diabetic macular edema and retinal\nvein occlusion, the algorithm can safely be applied in all of them though\ndifferent pathophysiological origins are known.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 20:49:57 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Schlegl", "Thomas", "", "1 and 2"], ["Bogunovic", "Hrvoje", "", "1 and 2"], ["Klimscha", "Sophie", "", "1 and 2"], ["Seeb\u00f6ck", "Philipp", "", "1 and 2"], ["Sadeghipour", "Amir", ""], ["Gerendas", "Bianca", ""], ["Waldstein", "Sebastian M.", ""], ["Langs", "Georg", ""], ["Schmidt-Erfurth", "Ursula", ""]]}, {"id": "1805.03300", "submitter": "Joseph Cheng", "authors": "Joseph Y. Cheng, Feiyu Chen, Marcus T. Alley, John M. Pauly, Shreyas\n  S. Vasanawala", "title": "Highly Scalable Image Reconstruction using Deep Neural Networks with\n  Bandpass Filtering", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To increase the flexibility and scalability of deep neural networks for image\nreconstruction, a framework is proposed based on bandpass filtering. For many\napplications, sensing measurements are performed indirectly. For example, in\nmagnetic resonance imaging, data are sampled in the frequency domain. The\nintroduction of bandpass filtering enables leveraging known imaging physics\nwhile ensuring that the final reconstruction is consistent with actual\nmeasurements to maintain reconstruction accuracy. We demonstrate this flexible\narchitecture for reconstructing subsampled datasets of MRI scans. The resulting\nhigh subsampling rates increase the speed of MRI acquisitions and enable the\nvisualization rapid hemodynamics.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 21:42:53 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 18:47:24 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Cheng", "Joseph Y.", ""], ["Chen", "Feiyu", ""], ["Alley", "Marcus T.", ""], ["Pauly", "John M.", ""], ["Vasanawala", "Shreyas S.", ""]]}, {"id": "1805.03305", "submitter": "Zheng Xu", "authors": "Zheng Xu, Xitong Yang, Xue Li, Xiaoshuai Sun", "title": "The Effectiveness of Instance Normalization: a Strong Baseline for\n  Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep neural network architecture for the challenging\nproblem of single image dehazing, which aims to recover the clear image from a\ndegraded hazy image. Instead of relying on hand-crafted image priors or\nexplicitly estimating the components of the widely used atmospheric scattering\nmodel, our end-to-end system directly generates the clear image from an input\nhazy image. The proposed network has an encoder-decoder architecture with skip\nconnections and instance normalization. We adopt the convolutional layers of\nthe pre-trained VGG network as encoder to exploit the representation power of\ndeep features, and demonstrate the effectiveness of instance normalization for\nimage dehazing. Our simple yet effective network outperforms the\nstate-of-the-art methods by a large margin on the benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 22:08:35 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Xu", "Zheng", ""], ["Yang", "Xitong", ""], ["Li", "Xue", ""], ["Sun", "Xiaoshuai", ""]]}, {"id": "1805.03344", "submitter": "Jing Xu", "authors": "Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, Wanli Ouyang", "title": "Attention-Aware Compositional Network for Person Re-identification", "comments": "Accepted at CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is to identify pedestrians observed from\ndifferent camera views based on visual appearance. It is a challenging task due\nto large pose variations, complex background clutters and severe occlusions.\nRecently, human pose estimation by predicting joint locations was largely\nimproved in accuracy. It is reasonable to use pose estimation results for\nhandling pose variations and background clutters, and such attempts have\nobtained great improvement in ReID performance. However, we argue that the pose\ninformation was not well utilized and hasn't yet been fully exploited for\nperson ReID.\n  In this work, we introduce a novel framework called Attention-Aware\nCompositional Network (AACN) for person ReID. AACN consists of two main\ncomponents: Pose-guided Part Attention (PPA) and Attention-aware Feature\nComposition (AFC). PPA is learned and applied to mask out undesirable\nbackground features in pedestrian feature maps. Furthermore, pose-guided\nvisibility scores are estimated for body parts to deal with part occlusion in\nthe proposed AFC module. Extensive experiments with ablation analysis show the\neffectiveness of our method, and state-of-the-art results are achieved on\nseveral public datasets, including Market-1501, CUHK03, CUHK01, SenseReID,\nCUHK03-NP and DukeMTMC-reID.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 01:43:10 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 08:20:51 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Xu", "Jing", ""], ["Zhao", "Rui", ""], ["Zhu", "Feng", ""], ["Wang", "Huaming", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1805.03356", "submitter": "Yuhang Song", "authors": "Yuhang Song, Chao Yang, Yeji Shen, Peng Wang, Qin Huang, C.-C. Jay Kuo", "title": "SPG-Net: Segmentation Prediction and Guidance Network for Image\n  Inpainting", "comments": "BMVC 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on image inpainting task, aiming at recovering the\nmissing area of an incomplete image given the context information. Recent\ndevelopment in deep generative models enables an efficient end-to-end framework\nfor image synthesis and inpainting tasks, but existing methods based on\ngenerative models don't exploit the segmentation information to constrain the\nobject shapes, which usually lead to blurry results on the boundary. To tackle\nthis problem, we propose to introduce the semantic segmentation information,\nwhich disentangles the inter-class difference and intra-class variation for\nimage inpainting. This leads to much clearer recovered boundary between\nsemantically different regions and better texture within semantically\nconsistent segments. Our model factorizes the image inpainting process into\nsegmentation prediction (SP-Net) and segmentation guidance (SG-Net) as two\nsteps, which predict the segmentation labels in the missing area first, and\nthen generate segmentation guided inpainting results. Experiments on multiple\npublic datasets show that our approach outperforms existing methods in\noptimizing the image inpainting quality, and the interactive segmentation\nguidance provides possibilities for multi-modal predictions of image\ninpainting.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 03:02:06 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 21:46:27 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 23:32:34 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 19:53:18 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Song", "Yuhang", ""], ["Yang", "Chao", ""], ["Shen", "Yeji", ""], ["Wang", "Peng", ""], ["Huang", "Qin", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1805.03363", "submitter": "Dacheng Tao", "authors": "Baosheng Yu and Dacheng Tao", "title": "Anchor Cascade for Efficient Face Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2886790", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is essential to facial analysis tasks such as facial\nreenactment and face recognition. Both cascade face detectors and anchor-based\nface detectors have translated shining demos into practice and received\nintensive attention from the community. However, cascade face detectors often\nsuffer from a low detection accuracy, while anchor-based face detectors rely\nheavily on very large networks pre-trained on large scale image classification\ndatasets such as ImageNet [1], which is not computationally efficient for both\ntraining and deployment. In this paper, we devise an efficient anchor-based\ncascade framework called anchor cascade. To improve the detection accuracy by\nexploring contextual information, we further propose a context pyramid maxout\nmechanism for anchor cascade. As a result, anchor cascade can train very\nefficient face detection models with a high detection accuracy. Specifically,\ncomparing with a popular CNN-based cascade face detector MTCNN [2], our anchor\ncascade face detector greatly improves the detection accuracy, e.g., from\n0.9435 to 0.9704 at 1k false positives on FDDB, while it still runs in\ncomparable speed. Experimental results on two widely used face detection\nbenchmarks, FDDB and WIDER FACE, demonstrate the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 03:46:35 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Yu", "Baosheng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1805.03371", "submitter": "Qingjie Liu", "authors": "Qingjie Liu and Huanyu Zhou and Qizhi Xu and Xiangyu Liu and Yunhong\n  Wang", "title": "PSGAN: A Generative Adversarial Network for Remote Sensing Image\n  Pan-Sharpening", "comments": "Accepted to TGRS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of remote sensing image pan-sharpening from\nthe perspective of generative adversarial learning. We propose a novel deep\nneural network based method named PSGAN. To the best of our knowledge, this is\none of the first attempts at producing high-quality pan-sharpened images with\nGANs. The PSGAN consists of two components: a generative network (i.e.,\ngenerator) and a discriminative network (i.e., discriminator). The generator is\ndesigned to accept panchromatic (PAN) and multispectral (MS) images as inputs\nand maps them to the desired high-resolution (HR) MS images and the\ndiscriminator implements the adversarial training strategy for generating\nhigher fidelity pan-sharpened images. In this paper, we evaluate several\narchitectures and designs, namely two-stream input, stacking input, batch\nnormalization layer, and attention mechanism to find the optimal solution for\npan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2\nsatellite images demonstrate that the proposed PSGANs not only are effective in\ngenerating high-quality HR MS images and superior to state-of-the-art methods\nand also generalize well to full-scale images.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 04:58:57 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:20:29 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 01:49:29 GMT"}, {"version": "v4", "created": "Sun, 20 Dec 2020 15:57:25 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Liu", "Qingjie", ""], ["Zhou", "Huanyu", ""], ["Xu", "Qizhi", ""], ["Liu", "Xiangyu", ""], ["Wang", "Yunhong", ""]]}, {"id": "1805.03383", "submitter": "Yijie Bei", "authors": "Yijie Bei, Alex Damian, Shijia Hu, Sachit Menon, Nikhil Ravi, Cynthia\n  Rudin", "title": "New Techniques for Preserving Global Structure and Denoising with Low\n  Information Loss in Single-Image Super-Resolution", "comments": "8 pages, CVPR workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work identifies and addresses two important technical challenges in\nsingle-image super-resolution: (1) how to upsample an image without magnifying\nnoise and (2) how to preserve large scale structure when upsampling. We\nsummarize the techniques we developed for our second place entry in Track 1\n(Bicubic Downsampling), seventh place entry in Track 2 (Realistic Adverse\nConditions), and seventh place entry in Track 3 (Realistic difficult) in the\n2018 NTIRE Super-Resolution Challenge. Furthermore, we present new neural\nnetwork architectures that specifically address the two challenges listed\nabove: denoising and preservation of large-scale structure.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 05:59:10 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 00:19:44 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Bei", "Yijie", ""], ["Damian", "Alex", ""], ["Hu", "Shijia", ""], ["Menon", "Sachit", ""], ["Ravi", "Nikhil", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1805.03384", "submitter": "Fan Bai", "authors": "Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Shuigeng Zhou", "title": "Edit Probability for Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the scene text recognition problem under the attention-based\nencoder-decoder framework, which is the state of the art. The existing methods\nusually employ a frame-wise maximal likelihood loss to optimize the models.\nWhen we train the model, the misalignment between the ground truth strings and\nthe attention's output sequences of probability distribution, which is caused\nby missing or superfluous characters, will confuse and mislead the training\nprocess, and consequently make the training costly and degrade the recognition\naccuracy. To handle this problem, we propose a novel method called edit\nprobability (EP) for scene text recognition. EP tries to effectively estimate\nthe probability of generating a string from the output sequence of probability\ndistribution conditioned on the input image, while considering the possible\noccurrences of missing/superfluous characters. The advantage lies in that the\ntraining process can focus on the missing, superfluous and unrecognized\ncharacters, and thus the impact of the misalignment problem can be alleviated\nor even overcome. We conduct extensive experiments on standard benchmarks,\nincluding the IIIT-5K, Street View Text and ICDAR datasets. Experimental\nresults show that the EP can substantially boost scene text recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 06:14:51 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Bai", "Fan", ""], ["Cheng", "Zhanzhan", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1805.03430", "submitter": "Sergey Prokudin", "authors": "Sergey Prokudin, Peter Gehler, Sebastian Nowozin", "title": "Deep Directional Statistics: Pose Estimation with Uncertainty\n  Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning systems successfully solve many perception tasks such as\nobject pose estimation when the input image is of high quality. However, in\nchallenging imaging conditions such as on low-resolution images or when the\nimage is corrupted by imaging artifacts, current systems degrade considerably\nin accuracy. While a loss in performance is unavoidable, we would like our\nmodels to quantify their uncertainty in order to achieve robustness against\nimages of varying quality. Probabilistic deep learning models combine the\nexpressive power of deep learning with uncertainty quantification. In this\npaper, we propose a novel probabilistic deep learning model for the task of\nangular regression. Our model uses von Mises distributions to predict a\ndistribution over object pose angle. Whereas a single von Mises distribution is\nmaking strong assumptions about the shape of the distribution, we extend the\nbasic model to predict a mixture of von Mises distributions. We show how to\nlearn a mixture model using a finite and infinite number of mixture components.\nOur model allows for likelihood-based training and efficient inference at test\ntime. We demonstrate on a number of challenging pose estimation datasets that\nour model produces calibrated probability predictions and competitive or\nsuperior point estimates compared to the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 09:22:09 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Prokudin", "Sergey", ""], ["Gehler", "Peter", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1805.03438", "submitter": "Hong-Ming Yang", "authors": "Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, Cheng-Lin Liu", "title": "Robust Classification with Convolutional Prototype Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely used for image\nclassification. Despite its high accuracies, CNN has been shown to be easily\nfooled by some adversarial examples, indicating that CNN is not robust enough\nfor pattern classification. In this paper, we argue that the lack of robustness\nfor CNN is caused by the softmax layer, which is a totally discriminative model\nand based on the assumption of closed world (i.e., with a fixed number of\ncategories). To improve the robustness, we propose a novel learning framework\ncalled convolutional prototype learning (CPL). The advantage of using\nprototypes is that it can well handle the open world recognition problem and\ntherefore improve the robustness. Under the framework of CPL, we design\nmultiple classification criteria to train the network. Moreover, a prototype\nloss (PL) is proposed as a regularization to improve the intra-class\ncompactness of the feature representation, which can be viewed as a generative\nmodel based on the Gaussian assumption of different classes. Experiments on\nseveral datasets demonstrate that CPL can achieve comparable or even better\nresults than traditional CNN, and from the robustness perspective, CPL shows\ngreat advantages for both the rejection and incremental category learning\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 09:55:45 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Yang", "Hong-Ming", ""], ["Zhang", "Xu-Yao", ""], ["Yin", "Fei", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1805.03444", "submitter": "Woohyung Chun", "authors": "Woohyung Chun, Sung-Min Hong, Junho Huh and Inyup Kang", "title": "Controlling the privacy loss with the input feature maps of the layers\n  in convolutional neural networks", "comments": "9 pages (8 pages for contents) and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the method to sanitize the privacy of the IFM(Input Feature Map)s\nthat are fed into the layers of CNN(Convolutional Neural Network)s. The method\nintroduces the degree of the sanitization that makes the application using a\nCNN be able to control the privacy loss represented as the ratio of the\nprobabilistic accuracies for original IFM and sanitized IFM. For the\nsanitization of an IFM, the sample-and-hold based approximation scheme is\ndevised to satisfy an application-specific degree of the sanitization. The\nscheme approximates an IFM by replacing all the samples in a window with the\nnon-zero sample closest to the mean of the sampling window. It also removes the\ndependency on CNN configuration by unfolding multi-dimensional IFM tensors into\none-dimensional streams to be approximated.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 10:12:31 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 10:48:13 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 10:31:56 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 12:03:32 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chun", "Woohyung", ""], ["Hong", "Sung-Min", ""], ["Huh", "Junho", ""], ["Kang", "Inyup", ""]]}, {"id": "1805.03453", "submitter": "Hariharan Ramasangu", "authors": "Lasitha Mekkayil and Hariharan Ramasangu", "title": "Object Tracking with Correlation Filters using Selective Single\n  Background Patch", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter plays a major role in improved tracking performance\ncompared to existing trackers. The tracker uses the adaptive correlation\nresponse to predict the location of the target. Many varieties of correlation\ntrackers were proposed recently with high accuracy and frame rates. The paper\nproposes a method to select a single background patch to have a better tracking\nperformance. The paper also contributes a variant of correlation filter by\nmodifying the filter with image restoration filters. The approach is validated\nusing Object Tracking Benchmark sequences.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 10:49:14 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Mekkayil", "Lasitha", ""], ["Ramasangu", "Hariharan", ""]]}, {"id": "1805.03482", "submitter": "Bojian Wu", "authors": "Bojian Wu, Yang Zhou, Yiming Qian, Minglun Gong, Hui Huang", "title": "Full 3D Reconstruction of Transparent Objects", "comments": "Accepted to SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous techniques have been proposed for reconstructing 3D models for\nopaque objects in past decades. However, none of them can be directly applied\nto transparent objects. This paper presents a fully automatic approach for\nreconstructing complete 3D shapes of transparent objects. Through positioning\nan object on a turntable, its silhouettes and light refraction paths under\ndifferent viewing directions are captured. Then, starting from an initial rough\nmodel generated from space carving, our algorithm progressively optimizes the\nmodel under three constraints: surface and refraction normal consistency,\nsurface projection and silhouette consistency, and surface smoothness.\nExperimental results on both synthetic and real objects demonstrate that our\nmethod can successfully recover the complex shapes of transparent objects and\nfaithfully reproduce their light refraction properties.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 12:39:06 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 13:40:00 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Wu", "Bojian", ""], ["Zhou", "Yang", ""], ["Qian", "Yiming", ""], ["Gong", "Minglun", ""], ["Huang", "Hui", ""]]}, {"id": "1805.03487", "submitter": "Enrique Sanchez", "authors": "Enrique Sanchez-Lozano, Georgios Tzimiropoulos, Michel Valstar", "title": "Joint Action Unit localisation and intensity estimation through heatmap\n  regression", "comments": "BMVC 2018. Code and model will be available to download from\n  https://github.com/ESanchezLozano/Action-Units-Heatmaps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a supervised learning approach to jointly perform facial\nAction Unit (AU) localisation and intensity estimation. Contrary to previous\nworks that try to learn an unsupervised representation of the Action Unit\nregions, we propose to directly and jointly estimate all AU intensities through\nheatmap regression, along with the location in the face where they cause\nvisible changes. Our approach aims to learn a pixel-wise regression function\nreturning a score per AU, which indicates an AU intensity at a given spatial\nlocation. Heatmap regression then generates an image, or channel, per AU, in\nwhich each pixel indicates the corresponding AU intensity. To generate the\nground-truth heatmaps for a target AU, the facial landmarks are first\nestimated, and a 2D Gaussian is drawn around the points where the AU is known\nto cause changes. The amplitude and size of the Gaussian is determined by the\nintensity of the AU. We show that using a single Hourglass network suffices to\nattain new state of the art results, demonstrating the effectiveness of such a\nsimple approach. The use of heatmap regression allows learning of a shared\nrepresentation between AUs without the need to rely on latent representations,\nas these are implicitly learned from the data. We validate the proposed\napproach on the BP4D dataset, showing a modest improvement on recent, complex,\ntechniques, as well as robustness against misalignment errors. Code for testing\nand models will be available to download from\nhttps://github.com/ESanchezLozano/Action-Units-Heatmaps.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 12:49:20 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 14:19:16 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Sanchez-Lozano", "Enrique", ""], ["Tzimiropoulos", "Georgios", ""], ["Valstar", "Michel", ""]]}, {"id": "1805.03508", "submitter": "Zhou Yu", "authors": "Zhou Yu, Jun Yu, Chenchao Xiang, Zhou Zhao, Qi Tian, Dacheng Tao", "title": "Rethinking Diversified and Discriminative Proposal Generation for Visual\n  Grounding", "comments": "Accepted in IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual grounding aims to localize an object in an image referred to by a\ntextual query phrase. Various visual grounding approaches have been proposed,\nand the problem can be modularized into a general framework: proposal\ngeneration, multi-modal feature representation, and proposal ranking. Of these\nthree modules, most existing approaches focus on the latter two, with the\nimportance of proposal generation generally neglected. In this paper, we\nrethink the problem of what properties make a good proposal generator. We\nintroduce the diversity and discrimination simultaneously when generating\nproposals, and in doing so propose Diversified and Discriminative Proposal\nNetworks model (DDPN). Based on the proposals generated by DDPN, we propose a\nhigh performance baseline model for visual grounding and evaluate it on four\nbenchmark datasets. Experimental results demonstrate that our model delivers\nsignificant improvements on all the tested data-sets (e.g., 18.8\\% improvement\non ReferItGame and 8.2\\% improvement on Flickr30k Entities over the existing\nstate-of-the-arts respectively)\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 13:25:27 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Yu", "Zhou", ""], ["Yu", "Jun", ""], ["Xiang", "Chenchao", ""], ["Zhao", "Zhou", ""], ["Tian", "Qi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1805.03511", "submitter": "Georg Waltner", "authors": "Georg Waltner, Michael Maurer, Thomas Holzmann, Patrick Ruprecht,\n  Michael Opitz, Horst Possegger, Friedrich Fraundorfer and Horst Bischof", "title": "Deep 2.5D Vehicle Classification with Sparse SfM Depth Prior for\n  Automated Toll Systems", "comments": "Submitted to the IEEE International Conference on Intelligent\n  Transportation Systems 2018 (ITSC), 6 pages, 4 figures; changed format in\n  compliance with adapted IEEE template", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated toll systems rely on proper classification of the passing vehicles.\nThis is especially difficult when the images used for classification only cover\nparts of the vehicle. To obtain information about the whole vehicle. we\nreconstruct the vehicle as 3D object and exploit this additional information\nwithin a Convolutional Neural Network (CNN). However, when using deep networks\nfor 3D object classification, large amounts of dense 3D models are required for\ngood accuracy, which are often neither available nor feasible to process due to\nmemory requirements. Therefore, in our method we reproject the 3D object onto\nthe image plane using the reconstructed points, lines or both. We utilize this\nsparse depth prior within an auxiliary network branch that acts as a\nregularizer during training. We show that this auxiliary regularizer helps to\nimprove accuracy compared to 2D classification on a real-world dataset.\nFurthermore due to the design of the network, at test time only the 2D camera\nimages are required for classification which enables the usage in portable\ncomputer vision systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 13:28:52 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 06:24:33 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Waltner", "Georg", ""], ["Maurer", "Michael", ""], ["Holzmann", "Thomas", ""], ["Ruprecht", "Patrick", ""], ["Opitz", "Michael", ""], ["Possegger", "Horst", ""], ["Fraundorfer", "Friedrich", ""], ["Bischof", "Horst", ""]]}, {"id": "1805.03517", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Christian Bailer, Oliver Wasenm\\\"uller, Didier\n  Stricker", "title": "FlowFields++: Accurate Optical Flow Correspondences Meet Robust\n  Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Flow algorithms are of high importance for many applications.\nRecently, the Flow Field algorithm and its modifications have shown remarkable\nresults, as they have been evaluated with top accuracy on different data sets.\nIn our analysis of the algorithm we have found that it produces accurate sparse\nmatches, but there is room for improvement in the interpolation. Thus, we\npropose in this paper FlowFields++, where we combine the accurate matches of\nFlow Fields with a robust interpolation. In addition, we propose improved\nvariational optimization as post-processing. Our new algorithm is evaluated on\nthe challenging KITTI and MPI Sintel data sets with public top results on both\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 13:40:37 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Bailer", "Christian", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1805.03593", "submitter": "Lokesh Boominathan", "authors": "Lokesh Boominathan, Mayug Maniparambil, Honey Gupta, Rahul Baburajan\n  and Kaushik Mitra", "title": "Phase retrieval for Fourier Ptychography under varying amount of\n  measurements", "comments": "Supplementary material attached after Reference section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier Ptychography is a recently proposed imaging technique that yields\nhigh-resolution images by computationally transcending the diffraction blur of\nan optical system. At the crux of this method is the phase retrieval algorithm,\nwhich is used for computationally stitching together low-resolution images\ntaken under varying illumination angles of a coherent light source. However,\nthe traditional iterative phase retrieval technique relies heavily on the\ninitialization and also need a good amount of overlap in the Fourier domain for\nthe successively captured low-resolution images, thus increasing the\nacquisition time and data. We show that an auto-encoder based architecture can\nbe adaptively trained for phase retrieval under both low overlap, where\ntraditional techniques completely fail, and at higher levels of overlap. For\nthe low overlap case we show that a supervised deep learning technique using an\nautoencoder generator is a good choice for solving the Fourier ptychography\nproblem. And for the high overlap case, we show that optimizing the generator\nfor reducing the forward model error is an appropriate choice. Using\nsimulations for the challenging case of uncorrelated phase and amplitude, we\nshow that our method outperforms many of the previously proposed Fourier\nptychography phase retrieval techniques.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 15:42:44 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Boominathan", "Lokesh", ""], ["Maniparambil", "Mayug", ""], ["Gupta", "Honey", ""], ["Baburajan", "Rahul", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1805.03596", "submitter": "Xi Zhang", "authors": "Xi Zhang, Di Ma, Xu Ouyang, Shanshan Jiang, Lin Gan, Gady Agam", "title": "Layered Optical Flow Estimation Using a Deep Neural Network with a Soft\n  Mask", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a layered representation for motion estimation has the advantage of\nbeing able to cope with discontinuities and occlusions. In this paper, we learn\nto estimate optical flow by combining a layered motion representation with deep\nlearning. Instead of pre-segmenting the image to layers, the proposed approach\nautomatically generates a layered representation of optical flow using the\nproposed soft-mask module. The essential components of the soft-mask module are\nmaxout and fuse operations, which enable a disjoint layered representation of\noptical flow and more accurate flow estimation. We show that by using masks the\nmotion estimate results in a quadratic function of input features in the output\nlayer. The proposed soft-mask module can be added to any existing optical flow\nestimation networks by replacing their flow output layer. In this work, we use\nFlowNet as the base network to which we add the soft-mask module. The resulting\nnetwork is tested on three well-known benchmarks with both supervised and\nunsupervised flow estimation tasks. Evaluation results show that the proposed\nnetwork achieve better results compared with the original FlowNet.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 15:45:48 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Zhang", "Xi", ""], ["Ma", "Di", ""], ["Ouyang", "Xu", ""], ["Jiang", "Shanshan", ""], ["Gan", "Lin", ""], ["Agam", "Gady", ""]]}, {"id": "1805.03699", "submitter": "Talha Qaiser", "authors": "Talha Qaiser, Yee-Wah Tsang, Daiki Taniyama, Naoya Sakamoto, Kazuaki\n  Nakane, David Epstein, Nasir Rajpoot", "title": "Fast and Accurate Tumor Segmentation of Histology Images using\n  Persistent Homology and Deep Convolutional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor segmentation in whole-slide images of histology slides is an important\nstep towards computer-assisted diagnosis. In this work, we propose a tumor\nsegmentation framework based on the novel concept of persistent homology\nprofiles (PHPs). For a given image patch, the homology profiles are derived by\nefficient computation of persistent homology, which is an algebraic tool from\nhomology theory. We propose an efficient way of computing topological\npersistence of an image, alternative to simplicial homology. The PHPs are\ndevised to distinguish tumor regions from their normal counterparts by modeling\nthe atypical characteristics of tumor nuclei. We propose two variants of our\nmethod for tumor segmentation: one that targets speed without compromising\naccuracy and the other that targets higher accuracy. The fast version is based\non the selection of exemplar image patches from a convolution neural network\n(CNN) and patch classification by quantifying the divergence between the PHPs\nof exemplars and the input image patch. Detailed comparative evaluation shows\nthat the proposed algorithm is significantly faster than competing algorithms\nwhile achieving comparable results. The accurate version combines the PHPs and\nhigh-level CNN features and employs a multi-stage ensemble strategy for image\npatch labeling. Experimental results demonstrate that the combination of PHPs\nand CNN features outperforms competing algorithms. This study is performed on\ntwo independently collected colorectal datasets containing adenoma,\nadenocarcinoma, signet and healthy cases. Collectively, the accurate tumor\nsegmentation produces the highest average patch-level F1-score, as compared\nwith competing algorithms, on malignant and healthy cases from both the\ndatasets. Overall the proposed framework highlights the utility of persistent\nhomology for histopathology image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 19:02:29 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Qaiser", "Talha", ""], ["Tsang", "Yee-Wah", ""], ["Taniyama", "Daiki", ""], ["Sakamoto", "Naoya", ""], ["Nakane", "Kazuaki", ""], ["Epstein", "David", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1805.03707", "submitter": "Terrence Adams", "authors": "Terrence Adams", "title": "A Continuous, Full-scope, Spatio-temporal Tracking Metric based on\n  KL-divergence", "comments": "12 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified metric is given for the evaluation of object tracking systems. The\nmetric is inspired by KL-divergence or relative entropy, which is commonly used\nto evaluate clustering techniques. Since tracking problems are fundamentally\ndifferent from clustering, the components of KL-divergence are recast to handle\nvarious types of tracking errors (i.e., false alarms, missed detections,\nmerges, splits). Scoring results are given on a standard tracking dataset\n(Oxford Town Centre Dataset), as well as several simulated scenarios. Also,\nthis new metric is compared with several other metrics including the commonly\nused Multiple Object Tracking Accuracy metric. In the final section, advantages\nof this metric are given including the fact that it is continuous,\nparameter-less and comprehensive.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 19:47:03 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 17:37:58 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 17:49:33 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Adams", "Terrence", ""]]}, {"id": "1805.03779", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Leonard Sunwoo, and Jong Chul Ye", "title": "k-Space Deep Learning for Accelerated MRI", "comments": "Accepted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annihilating filter-based low-rank Hankel matrix approach (ALOHA) is one\nof the state-of-the-art compressed sensing approaches that directly\ninterpolates the missing k-space data using low-rank Hankel matrix completion.\nThe success of ALOHA is due to the concise signal representation in the k-space\ndomain thanks to the duality between structured low-rankness in the k-space\ndomain and the image domain sparsity. Inspired by the recent mathematical\ndiscovery that links convolutional neural networks to Hankel matrix\ndecomposition using data-driven framelet basis, here we propose a fully\ndata-driven deep learning algorithm for k-space interpolation. Our network can\nbe also easily applied to non-Cartesian k-space trajectories by simply adding\nan additional regridding layer. Extensive numerical experiments show that the\nproposed deep learning method consistently outperforms the existing\nimage-domain deep learning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 01:43:19 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 05:56:47 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 15:44:52 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Han", "Yoseob", ""], ["Sunwoo", "Leonard", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1805.03788", "submitter": "Guohui Li", "authors": "Zhiwen Luo, Guohui Li, Junfeng Du, and Jieping Wu", "title": "Dust concentration vision measurement based on moment of inertia in gray\n  level-rank co-occurrence matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the accuracy of existing dust concentration measurements, a dust\nconcentration measurement based on Moment of inertia in Gray level-Rank\nCo-occurrence Matrix (GRCM), which is from the dust image sample measured by a\nmachine vision system is proposed in this paper. Firstly, a Polynomial\ncomputational model between dust Concentration and Moment of inertia (PCM) is\nestablished by experimental methods and fitting methods. Then computing methods\nfor GRCM and its Moment of inertia are constructed by theoretical and\nmathematical analysis methods. And then developing an on-line dust\nconcentration vision measurement experimental system, the cement dust\nconcentration measurement in a cement production workshop is taken as a\npractice example with the system and the PCM measurement. The results show that\nmeasurement error is within 9%, and the measurement range is 0.5-1000 mg/m3.\nFinally, comparing with the filter membrane weighing measurement, light\nscattering measurement and laser measurement, the proposed PCM measurement has\nadvantages on error and cost, which can be provided a valuable reference for\nthe dust concentration vision measurements.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 02:20:48 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Luo", "Zhiwen", ""], ["Li", "Guohui", ""], ["Du", "Junfeng", ""], ["Wu", "Jieping", ""]]}, {"id": "1805.03857", "submitter": "Lu Sheng", "authors": "Lu Sheng and Ziyi Lin and Jing Shao and Xiaogang Wang", "title": "Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot artistic style transfer is an important image synthesis problem\naiming at transferring arbitrary style into content images. However, the\ntrade-off between the generalization and efficiency in existing methods impedes\na high quality zero-shot style transfer in real-time. In this paper, we resolve\nthis dilemma and propose an efficient yet effective Avatar-Net that enables\nvisually plausible multi-scale transfer for arbitrary style. The key ingredient\nof our method is a style decorator that makes up the content features by\nsemantically aligned style features from an arbitrary style image, which does\nnot only holistically match their feature distributions but also preserve\ndetailed style patterns in the decorated features. By embedding this module\ninto an image reconstruction network that fuses multi-scale style abstractions,\nthe Avatar-Net renders multi-scale stylization for any style image in one\nfeed-forward pass. We demonstrate the state-of-the-art effectiveness and\nefficiency of the proposed method in generating high-quality stylized images,\nwith a series of applications include multiple style integration, video\nstylization and etc.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 07:17:32 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 08:21:10 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Sheng", "Lu", ""], ["Lin", "Ziyi", ""], ["Shao", "Jing", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1805.03869", "submitter": "Mohammed Daoudi", "authors": "Naima Otberdout and Anis Kacem and Mohamed Daoudi and Lahoucine\n  Ballihi and Stefano Berretti", "title": "Deep Covariance Descriptors for Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, covariance matrices are exploited to encode the deep\nconvolutional neural networks (DCNN) features for facial expression\nrecognition. The space geometry of the covariance matrices is that of Symmetric\nPositive Definite (SPD) matrices. By performing the classification of the\nfacial expressions using Gaussian kernel on SPD manifold, we show that the\ncovariance descriptors computed on DCNN features are more efficient than the\nstandard classification with fully connected layers and softmax. By\nimplementing our approach using the VGG-face and ExpNet architectures with\nextensive experiments on the Oulu-CASIA and SFEW datasets, we show that the\nproposed approach achieves performance at the state of the art for facial\nexpression recognition.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 07:53:26 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Otberdout", "Naima", ""], ["Kacem", "Anis", ""], ["Daoudi", "Mohamed", ""], ["Ballihi", "Lahoucine", ""], ["Berretti", "Stefano", ""]]}, {"id": "1805.03879", "submitter": "Aji Resindra Widya", "authors": "Aji Resindra Widya, Akihiko Torii, Masatoshi Okutomi", "title": "Structure-from-Motion using Dense CNN Features with Keypoint\n  Relocalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure from Motion (SfM) using imagery that involves extreme appearance\nchanges is yet a challenging task due to a loss of feature repeatability. Using\nfeature correspondences obtained by matching densely extracted convolutional\nneural network (CNN) features significantly improves the SfM reconstruction\ncapability. However, the reconstruction accuracy is limited by the spatial\nresolution of the extracted CNN features which is not even pixel-level accuracy\nin the existing approach. Providing dense feature matches with precise keypoint\npositions is not trivial because of memory limitation and computational burden\nof dense features. To achieve accurate SfM reconstruction with highly\nrepeatable dense features, we propose an SfM pipeline that uses dense CNN\nfeatures with relocalization of keypoint position that can efficiently and\naccurately provide pixel-level feature correspondences. Then, we demonstrate on\nthe Aachen Day-Night dataset that the proposed SfM using dense CNN features\nwith the keypoint relocalization outperforms a state-of-the-art SfM (COLMAP\nusing RootSIFT) by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:35:06 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 01:43:32 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Widya", "Aji Resindra", ""], ["Torii", "Akihiko", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "1805.03897", "submitter": "Biel Moy\\`a", "authors": "Gabriel Moy\\`a, Antoni Jaume-i-Cap\\'o and Javier Varona", "title": "Dealing with sequences in the RGBDT space", "comments": "4 pages CVPR'18 Workshop: Brave New Ideas for Video Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most of the current research in computer vision is focused on working with\nsingle images without taking in account temporal information. We present a\nprobabilistic non-parametric model that mixes multiple information cues from\ndevices to segment regions that contain moving objects in image sequences. We\nprepared an experimental setup to show the importance of using previous\ninformation for obtaining an accurate segmentation result, using a novel\ndataset that provides sequences in the RGBDT space. We label the detected\nregions ts with a state-of-the-art human detector. Each one of the detected\nregions is at least marked as human once.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 09:02:30 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Moy\u00e0", "Gabriel", ""], ["Jaume-i-Cap\u00f3", "Antoni", ""], ["Varona", "Javier", ""]]}, {"id": "1805.03922", "submitter": "Xiaobo Wang", "authors": "Xiaobo Wang, Shifeng Zhang, Zhen Lei, Si Liu, Xiaojie Guo and Stan Z.\n  Li", "title": "Ensemble Soft-Margin Softmax Loss for Image Classification", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax loss is arguably one of the most popular losses to train CNN models\nfor image classification. However, recent works have exposed its limitation on\nfeature discriminability. This paper casts a new viewpoint on the weakness of\nsoftmax loss. On the one hand, the CNN features learned using the softmax loss\nare often inadequately discriminative. We hence introduce a soft-margin softmax\nfunction to explicitly encourage the discrimination between different classes.\nOn the other hand, the learned classifier of softmax loss is weak. We propose\nto assemble multiple these weak classifiers to a strong one, inspired by the\nrecognition that the diversity among weak classifiers is critical to a good\nensemble. To achieve the diversity, we adopt the Hilbert-Schmidt Independence\nCriterion (HSIC). Considering these two aspects in one framework, we design a\nnovel loss, named as Ensemble soft-Margin Softmax (EM-Softmax). Extensive\nexperiments on benchmark datasets are conducted to show the superiority of our\ndesign over the baseline softmax loss and several state-of-the-art\nalternatives.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 10:47:13 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Wang", "Xiaobo", ""], ["Zhang", "Shifeng", ""], ["Lei", "Zhen", ""], ["Liu", "Si", ""], ["Guo", "Xiaojie", ""], ["Li", "Stan Z.", ""]]}, {"id": "1805.03988", "submitter": "Min Liu", "authors": "Min Liu and Tobi Delbruck", "title": "ABMOF: A Novel Optical Flow Algorithm for Dynamic Vision Sensors", "comments": "11 pages, 10 figures, Video of result: https://youtu.be/Ss-MciioqTk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Vision Sensors (DVS), which output asynchronous log intensity change\nevents, have potential applications in high-speed robotics, autonomous cars and\ndrones. The precise event timing, sparse output, and wide dynamic range of the\nevents are well suited for optical flow, but conventional optical flow (OF)\nalgorithms are not well matched to the event stream data. This paper proposes\nan event-driven OF algorithm called adaptive block-matching optical flow\n(ABMOF). ABMOF uses time slices of accumulated DVS events. The time slices are\nadaptively rotated based on the input events and OF results. Compared with\nother methods such as gradient-based OF, ABMOF can efficiently be implemented\nin compact logic circuits. Results show that ABMOF achieves comparable accuracy\nto conventional standards such as Lucas-Kanade (LK). The main contributions of\nour paper are new adaptive time-slice rotation methods that ensure the\ngenerated slices have sufficient features for matching,including a feedback\nmechanism that controls the generated slices to have average slice displacement\nwithin the block search range. An LK method using our adapted slices is also\nimplemented. The ABMOF accuracy is compared with this LK method on natural\nscene data including sparse and dense texture, high dynamic range, and fast\nmotion exceeding 30,000 pixels per second.The paper dataset and source code are\navailable from http://sensors.ini.uzh.ch/databases.html.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:07:31 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Liu", "Min", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1805.03994", "submitter": "Bernhard Japes", "authors": "Bernhard Japes, Jennifer Mack, Florian Rist, Katja Herzog, Reinhard\n  T\\\"opfer, Volker Steinhage", "title": "Multi-View Semantic Labeling of 3D Point Clouds for Automated Plant\n  Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labeling of 3D point clouds is important for the derivation of 3D\nmodels from real world scenarios in several economic fields such as building\nindustry, facility management, town planning or heritage conservation. In\ncontrast to these most common applications, we describe in this study the\nsemantic labeling of 3D point clouds derived from plant organs by\nhigh-precision scanning. Our approach is optimized for the task of plant\nphenotyping with its very specific challenges and is employing a deep learning\nframework. Thereby, we report important experiences concerning detailed\nparameter initialization and optimization techniques. By evaluating our\napproach with challenging datasets we achieve state-of-the-art results without\ndifficult and time consuming feature engineering as being necessary in\ntraditional approaches to semantic labeling.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:19:50 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 16:28:18 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Japes", "Bernhard", ""], ["Mack", "Jennifer", ""], ["Rist", "Florian", ""], ["Herzog", "Katja", ""], ["T\u00f6pfer", "Reinhard", ""], ["Steinhage", "Volker", ""]]}, {"id": "1805.04001", "submitter": "Sai Samarth Rajesh Phaye", "authors": "Sai Samarth R Phaye, Apoorva Sikka, Abhinav Dhall, Deepti Bathula", "title": "Dense and Diverse Capsule Networks: Making the Capsules Learn Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past few years have witnessed exponential growth of interest in deep learning\nmethodologies with rapidly improving accuracies and reduced computational\ncomplexity. In particular, architectures using Convolutional Neural Networks\n(CNNs) have produced state-of-the-art performances for image classification and\nobject recognition tasks. Recently, Capsule Networks (CapsNet) achieved\nsignificant increase in performance by addressing an inherent limitation of\nCNNs in encoding pose and deformation. Inspired by such advancement, we asked\nourselves, can we do better? We propose Dense Capsule Networks (DCNet) and\nDiverse Capsule Networks (DCNet++). The two proposed frameworks customize the\nCapsNet by replacing the standard convolutional layers with densely connected\nconvolutions. This helps in incorporating feature maps learned by different\nlayers in forming the primary capsules. DCNet, essentially adds a deeper\nconvolution network, which leads to learning of discriminative feature maps.\nAdditionally, DCNet++ uses a hierarchical architecture to learn capsules that\nrepresent spatial information in a fine-to-coarser manner, which makes it more\nefficient for learning complex data. Experiments on image classification task\nusing benchmark datasets demonstrate the efficacy of the proposed\narchitectures. DCNet achieves state-of-the-art performance (99.75%) on MNIST\ndataset with twenty fold decrease in total training iterations, over the\nconventional CapsNet. Furthermore, DCNet++ performs better than CapsNet on SVHN\ndataset (96.90%), and outperforms the ensemble of seven CapsNet models on\nCIFAR-10 by 0.31% with seven fold decrease in number of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:29:17 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Phaye", "Sai Samarth R", ""], ["Sikka", "Apoorva", ""], ["Dhall", "Abhinav", ""], ["Bathula", "Deepti", ""]]}, {"id": "1805.04025", "submitter": "Chenxi Liu", "authors": "Alan L. Yuille, Chenxi Liu", "title": "Deep Nets: What have they ever done for Vision?", "comments": "To appear in IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an opinion paper about the strengths and weaknesses of Deep Nets for\nvision. They are at the heart of the enormous recent progress in artificial\nintelligence and are of growing importance in cognitive science and\nneuroscience. They have had many successes but also have several limitations\nand there is limited understanding of their inner workings. At present Deep\nNets perform very well on specific visual tasks with benchmark datasets but\nthey are much less general purpose, flexible, and adaptive than the human\nvisual system. We argue that Deep Nets in their current form are unlikely to be\nable to overcome the fundamental problem of computer vision, namely how to deal\nwith the combinatorial explosion, caused by the enormous complexity of natural\nimages, and obtain the rich understanding of visual scenes that the human\nvisual achieves. We argue that this combinatorial explosion takes us into a\nregime where \"big data is not enough\" and where we need to rethink our methods\nfor benchmarking performance and evaluating vision algorithms. We stress that,\nas vision algorithms are increasingly used in real world applications, that\nperformance evaluation is not merely an academic exercise but has important\nconsequences in the real world. It is impractical to review the entire Deep Net\nliterature so we restrict ourselves to a limited range of topics and references\nwhich are intended as entry points into the literature. The views expressed in\nthis paper are our own and do not necessarily represent those of anybody else\nin the computer vision community.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 15:43:44 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 01:47:35 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 19:52:59 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2020 15:34:56 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Yuille", "Alan L.", ""], ["Liu", "Chenxi", ""]]}, {"id": "1805.04026", "submitter": "Michael Wray", "authors": "Michael Wray, Davide Moltisanti and Dima Damen", "title": "Towards an Unequivocal Representation of Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces verb-only representations for actions and interactions;\nthe problem of describing similar motions (e.g. 'open door', 'open cupboard'),\nand distinguish differing ones (e.g. 'open door' vs 'open bottle') using\nverb-only labels. Current approaches for action recognition neglect legitimate\nsemantic ambiguities and class overlaps between verbs (Fig. 1), relying on the\nobjects to disambiguate interactions. We deviate from single-verb labels and\nintroduce a mapping between observations and multiple verb labels - in order to\ncreate an Unequivocal Representation of Actions. The new representation\nbenefits from increased vocabulary and a soft assignment to an enriched space\nof verb labels. We learn these representations as multi-output regression,\nusing a two-stream fusion CNN. The proposed approach outperforms conventional\nsingle-verb labels (also known as majority voting) on three egocentric datasets\nfor both recognition and retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 15:48:26 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Wray", "Michael", ""], ["Moltisanti", "Davide", ""], ["Damen", "Dima", ""]]}, {"id": "1805.04092", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, Kostas Daniilidis", "title": "Learning to Estimate 3D Human Pose and Shape from a Single Color Image", "comments": "CVPR 2018 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of estimating the full body 3D human pose and\nshape from a single color image. This is a task where iterative\noptimization-based solutions have typically prevailed, while Convolutional\nNetworks (ConvNets) have suffered because of the lack of training data and\ntheir low resolution 3D predictions. Our work aims to bridge this gap and\nproposes an efficient and effective direct prediction method based on ConvNets.\nCentral part to our approach is the incorporation of a parametric statistical\nbody shape model (SMPL) within our end-to-end framework. This allows us to get\nvery detailed 3D mesh results, while requiring estimation only of a small\nnumber of parameters, making it friendly for direct network prediction.\nInterestingly, we demonstrate that these parameters can be predicted reliably\nonly from 2D keypoints and masks. These are typical outputs of generic 2D human\nanalysis ConvNets, allowing us to relax the massive requirement that images\nwith 3D shape ground truth are available for training. Simultaneously, by\nmaintaining differentiability, at training time we generate the 3D mesh from\nthe estimated parameters and optimize explicitly for the surface using a 3D\nper-vertex loss. Finally, a differentiable renderer is employed to project the\n3D mesh to the image, which enables further refinement of the network, by\noptimizing for the consistency of the projection with 2D annotations (i.e., 2D\nkeypoints or masks). The proposed approach outperforms previous baselines on\nthis task and offers an attractive solution for direct prediction of 3D shape\nfrom a single color image.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:46:12 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Zhu", "Luyang", ""], ["Zhou", "Xiaowei", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1805.04095", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis", "title": "Ordinal Depth Supervision for 3D Human Pose Estimation", "comments": "CVPR 2018 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to train end-to-end systems for 3D human pose estimation from\nsingle images is currently constrained by the limited availability of 3D\nannotations for natural images. Most datasets are captured using Motion Capture\n(MoCap) systems in a studio setting and it is difficult to reach the\nvariability of 2D human pose datasets, like MPII or LSP. To alleviate the need\nfor accurate 3D ground truth, we propose to use a weaker supervision signal\nprovided by the ordinal depths of human joints. This information can be\nacquired by human annotators for a wide range of images and poses. We showcase\nthe effectiveness and flexibility of training Convolutional Networks (ConvNets)\nwith these ordinal relations in different settings, always achieving\ncompetitive performance with ConvNets trained with accurate 3D joint\ncoordinates. Additionally, to demonstrate the potential of the approach, we\naugment the popular LSP and MPII datasets with ordinal depth annotations. This\nextension allows us to present quantitative and qualitative evaluation in\nnon-studio conditions. Simultaneously, these ordinal annotations can be easily\nincorporated in the training procedure of typical ConvNets for 3D human pose.\nThrough this inclusion we achieve new state-of-the-art performance for the\nrelevant benchmarks and validate the effectiveness of ordinal depth supervision\nfor 3D human pose.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:47:26 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Zhou", "Xiaowei", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1805.04096", "submitter": "Andrew Liu", "authors": "Minyoung Huh, Andrew Liu, Andrew Owens, Alexei A. Efros", "title": "Fighting Fake News: Image Splice Detection via Learned Self-Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in photo editing and manipulation tools have made it significantly\neasier to create fake imagery. Learning to detect such manipulations, however,\nremains a challenging problem due to the lack of sufficient amounts of\nmanipulated training data. In this paper, we propose a learning algorithm for\ndetecting visual image manipulations that is trained only using a large dataset\nof real photographs. The algorithm uses the automatically recorded photo EXIF\nmetadata as supervisory signal for training a model to determine whether an\nimage is self-consistent -- that is, whether its content could have been\nproduced by a single imaging pipeline. We apply this self-consistency model to\nthe task of detecting and localizing image splices. The proposed method obtains\nstate-of-the-art performance on several image forensics benchmarks, despite\nnever seeing any manipulated images at training. That said, it is merely a step\nin the long quest for a truly general purpose visual forensics tool.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:49:03 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 09:25:48 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 18:16:41 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Huh", "Minyoung", ""], ["Liu", "Andrew", ""], ["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1805.04103", "submitter": "Shuyang Gu", "authors": "Shuyang Gu, Congliang Chen, Jing Liao, Lu Yuan", "title": "Arbitrary Style Transfer with Deep Feature Reshuffle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method by reshuffling deep features (i.e.,\npermuting the spacial locations of a feature map) of the style image for\narbitrary style transfer. We theoretically prove that our new style loss based\non reshuffle connects both global and local style losses respectively used by\nmost parametric and non-parametric neural style transfer methods. This simple\nidea can effectively address the challenging issues in existing style transfer\nmethods. On one hand, it can avoid distortions in local style patterns, and\nallow semantic-level transfer, compared with neural parametric methods. On the\nother hand, it can preserve globally similar appearance to the style image, and\navoid wash-out artifacts, compared with neural non-parametric methods. Based on\nthe proposed loss, we also present a progressive feature-domain optimization\napproach. The experiments show that our method is widely applicable to various\nstyles, and produces better quality than existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:58:11 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 07:50:11 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 04:13:16 GMT"}, {"version": "v4", "created": "Wed, 20 Jun 2018 14:26:42 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Gu", "Shuyang", ""], ["Chen", "Congliang", ""], ["Liao", "Jing", ""], ["Yuan", "Lu", ""]]}, {"id": "1805.04132", "submitter": "Pan He", "authors": "Xiaoyu Yue, Zhanghui Kuang, Zhaoyang Zhang, Zhenfang Chen, Pan He, Yu\n  Qiao, Wei Zhang", "title": "Boosting up Scene Text Detectors with Guided CNN", "comments": "Submitted to British Machine Vision Conference (BMVC), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CNNs have achieved great success in text detection. Most of existing\nmethods attempt to improve accuracy with sophisticated network design, while\npaying less attention on speed. In this paper, we propose a general framework\nfor text detection called Guided CNN to achieve the two goals simultaneously.\nThe proposed model consists of one guidance subnetwork, where a guidance mask\nis learned from the input image itself, and one primary text detector, where\nevery convolution and non-linear operation are conducted only in the guidance\nmask. On the one hand, the guidance subnetwork filters out non-text regions\ncoarsely, greatly reduces the computation complexity. On the other hand, the\nprimary text detector focuses on distinguishing between text and hard non-text\nregions and regressing text bounding boxes, achieves a better detection\naccuracy. A training strategy, called background-aware block-wise random\nsynthesis, is proposed to further boost up the performance. We demonstrate that\nthe proposed Guided CNN is not only effective but also efficient with two\nstate-of-the-art methods, CTPN and EAST, as backbones. On the challenging\nbenchmark ICDAR 2013, it speeds up CTPN by 2.9 times on average, while\nimproving the F-measure by 1.5%. On ICDAR 2015, it speeds up EAST by 2.0 times\nwhile improving the F-measure by 1.0%.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 18:51:19 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 03:38:25 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Yue", "Xiaoyu", ""], ["Kuang", "Zhanghui", ""], ["Zhang", "Zhaoyang", ""], ["Chen", "Zhenfang", ""], ["He", "Pan", ""], ["Qiao", "Yu", ""], ["Zhang", "Wei", ""]]}, {"id": "1805.04136", "submitter": "Leonhard Helminger", "authors": "Suman Saha, Rajitha Navarathna, Leonhard Helminger, Romann Weber", "title": "Unsupervised Deep Representations for Learning Audience Facial Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an unsupervised learning approach for analyzing\nfacial behavior based on a deep generative model combined with a convolutional\nneural network (CNN). We jointly train a variational auto-encoder (VAE) and a\ngenerative adversarial network (GAN) to learn a powerful latent representation\nfrom footage of audiences viewing feature-length movies. We show that the\nlearned latent representation successfully encodes meaningful signatures of\nbehaviors related to audience engagement (smiling & laughing) and disengagement\n(yawning). Our results provide a proof of concept for a more general\nmethodology for annotating hard-to-label multimedia data featuring sparse\nexamples of signals of interest.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 19:01:02 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Saha", "Suman", ""], ["Navarathna", "Rajitha", ""], ["Helminger", "Leonhard", ""], ["Weber", "Romann", ""]]}, {"id": "1805.04140", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Baoquan Chen,\n  Daniel Cohen-Or", "title": "Neural Best-Buddies: Sparse Cross-Domain Correspondence", "comments": "SIGGRAPH 2018", "journal-ref": null, "doi": "10.1145/3197517.3201332", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence between images is a fundamental problem in computer vision,\nwith a variety of graphics applications. This paper presents a novel method for\nsparse cross-domain correspondence. Our method is designed for pairs of images\nwhere the main objects of interest may belong to different semantic categories\nand differ drastically in shape and appearance, yet still contain semantically\nrelated or geometrically similar parts. Our approach operates on hierarchies of\ndeep features, extracted from the input images by a pre-trained CNN.\nSpecifically, starting from the coarsest layer in both hierarchies, we search\nfor Neural Best Buddies (NBB): pairs of neurons that are mutual nearest\nneighbors. The key idea is then to percolate NBBs through the hierarchy, while\nnarrowing down the search regions at each level and retaining only NBBs with\nsignificant activations. Furthermore, in order to overcome differences in\nappearance, each pair of search regions is transformed into a common\nappearance. We evaluate our method via a user study, in addition to comparisons\nwith alternative correspondence approaches. The usefulness of our method is\ndemonstrated using a variety of graphics applications, including cross-domain\nimage alignment, creation of hybrid images, automatic image morphing, and more.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 19:11:04 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 10:28:33 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Aberman", "Kfir", ""], ["Liao", "Jing", ""], ["Shi", "Mingyi", ""], ["Lischinski", "Dani", ""], ["Chen", "Baoquan", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1805.04141", "submitter": "Assia Benbihi", "authors": "Assia Benbihi, Matthieu Geist, C\\'edric Pradalier", "title": "Semi-Supervised Domain Adaptation with Representation Learning for\n  Semantic Segmentation across Time", "comments": null, "journal-ref": "Neural Information Processing - 26th International Conference,\n  {ICONIP} 2019, Sydney, Australia, December 12-15, 2019, Proceedings,", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning generates state-of-the-art semantic segmentation provided that\na large number of images together with pixel-wise annotations are available. To\nalleviate the expensive data collection process, we propose a semi-supervised\ndomain adaptation method for the specific case of images with similar semantic\ncontent but different pixel distributions. A network trained with supervision\non a past dataset is finetuned on the new dataset to conserve its features\nmaps. The domain adaptation becomes a simple regression between feature maps\nand does not require annotations on the new dataset. This method reaches\nperformances similar to classic transfer learning on the PASCAL VOC dataset\nwith synthetic transformations.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 19:14:06 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 19:34:31 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Benbihi", "Assia", ""], ["Geist", "Matthieu", ""], ["Pradalier", "C\u00e9dric", ""]]}, {"id": "1805.04176", "submitter": "Shiv Ram Dubey", "authors": "Chaitanya Nagpal, Shiv Ram Dubey", "title": "A Performance Evaluation of Convolutional Neural Networks for Face Anti\n  Spoofing", "comments": "Accepted in 2019 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era, biometric based access control is becoming more popular\ndue to its simplicity and ease to use by the users. It reduces the manual work\nof identity recognition and facilitates the automatic processing. The face is\none of the most important biometric visual information that can be easily\ncaptured without user cooperation in an uncontrolled environment. Precise\ndetection of spoofed faces should be on the high priority to make face based\nidentity recognition and access control robust against possible attacks. The\nrecently evolved Convolutional Neural Network (CNN) based deep learning\ntechnique has proven as one of the excellent method to deal with the visual\ninformation very effectively. The CNN learns the hierarchical features at\nintermediate layers automatically from the data. Several CNN based methods such\nas Inception and ResNet have shown outstanding performance for image\nclassification problem. This paper does a performance evaluation of CNNs for\nface anti-spoofing. The Inception and ResNet CNN architectures are used in this\nstudy. The results are computed over benchmark MSU Mobile Face Spoofing\nDatabase. The experiments are done by considering the different aspects such as\nthe depth of the model, random weight initialization vs weight transfer, fine\ntuning vs training from scratch and different learning rate. The favorable\nresults are obtained using these CNN architectures for face anti-spoofing in\ndifferent settings.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 01:34:59 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 12:40:19 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Nagpal", "Chaitanya", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1805.04224", "submitter": "Ning Tan", "authors": "Yun Jiang, Ning Tan", "title": "Retinal Vessel Segmentation Based on Conditional Deep Convolutional\n  Generative Adversarial Networks", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of retinal vessels is of significance for doctors to\ndiagnose the fundus diseases. However, existing methods have various problems\nin the segmentation of the retinal vessels, such as insufficient segmentation\nof retinal vessels, weak anti-noise interference ability, and sensitivity to\nlesions, etc. Aiming to the shortcomings of existed methods, this paper\nproposes the use of conditional deep convolutional generative adversarial\nnetworks to segment the retinal vessels. We mainly improve the network\nstructure of the generator. The introduction of the residual module at the\nconvolutional layer for residual learning makes the network structure sensitive\nto changes in the output, as to better adjust the weight of the generator. In\norder to reduce the number of parameters and calculations, using a small\nconvolution to halve the number of channels in the input signature before using\na large convolution kernel. By used skip connection to connect the output of\nthe convolutional layer with the output of the deconvolution layer to avoid\nlow-level information sharing. By verifying the method on the DRIVE and STARE\ndatasets, the segmentation accuracy rate is 96.08% and 97.71%, the sensitivity\nreaches 82.74% and 85.34% respectively, and the F-measure reaches 82.08% and\n85.02% respectively. The sensitivity is 4.82% and 2.4% higher than that of\nR2U-Net.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 02:17:04 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Jiang", "Yun", ""], ["Tan", "Ning", ""]]}, {"id": "1805.04239", "submitter": "Ravi Garg", "authors": "Chamara Saroj Weerasekera, Thanuja Dharmasiri, Ravi Garg, Tom Drummond\n  and Ian Reid", "title": "Just-in-Time Reconstruction: Inpainting Sparse Maps using Single View\n  Depth Predictors as Priors", "comments": "ICRA 2018", "journal-ref": null, "doi": "10.1109/ICRA.2018.8460549", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ``just-in-time reconstruction\" as real-time image-guided\ninpainting of a map with arbitrary scale and sparsity to generate a fully dense\ndepth map for the image. In particular, our goal is to inpaint a sparse map ---\nobtained from either a monocular visual SLAM system or a sparse sensor ---\nusing a single-view depth prediction network as a virtual depth sensor. We\nadopt a fairly standard approach to data fusion, to produce a fused depth map\nby performing inference over a novel fully-connected Conditional Random Field\n(CRF) which is parameterized by the input depth maps and their pixel-wise\nconfidence weights. Crucially, we obtain the confidence weights that\nparameterize the CRF model in a data-dependent manner via Convolutional Neural\nNetworks (CNNs) which are trained to model the conditional depth error\ndistributions given each source of input depth map and the associated RGB\nimage. Our CRF model penalises absolute depth error in its nodes and pairwise\nscale-invariant depth error in its edges, and the confidence-based fusion\nminimizes the impact of outlier input depth values on the fused result. We\ndemonstrate the flexibility of our method by real-time inpainting of ORB-SLAM,\nKinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary\nscale and varied amount of irregular sparsity.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 04:08:59 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Weerasekera", "Chamara Saroj", ""], ["Dharmasiri", "Thanuja", ""], ["Garg", "Ravi", ""], ["Drummond", "Tom", ""], ["Reid", "Ian", ""]]}, {"id": "1805.04247", "submitter": "Moshiur R Farazi", "authors": "Moshiur R Farazi, Salman H Khan", "title": "Reciprocal Attention Fusion for Visual Question Answering", "comments": "To appear in the British Machine Vision Conference (BMVC), September\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing attention mechanisms either attend to local image grid or object\nlevel features for Visual Question Answering (VQA). Motivated by the\nobservation that questions can relate to both object instances and their parts,\nwe propose a novel attention mechanism that jointly considers reciprocal\nrelationships between the two levels of visual details. The bottom-up attention\nthus generated is further coalesced with the top-down information to only focus\non the scene elements that are most relevant to a given question. Our design\nhierarchically fuses multi-modal information i.e., language, object- and\ngird-level features, through an efficient tensor decomposition scheme. The\nproposed model improves the state-of-the-art single model performances from\n67.9% to 68.2% on VQAv1 and from 65.7% to 67.4% on VQAv2, demonstrating a\nsignificant boost.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 06:13:56 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 06:16:54 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Farazi", "Moshiur R", ""], ["Khan", "Salman H", ""]]}, {"id": "1805.04252", "submitter": "Zheng Wang", "authors": "Ben Taylor, Vicent Sanz Marco, Willy Wolff, Yehia Elkhatib, Zheng Wang", "title": "Adaptive Selection of Deep Learning Models on Embedded Systems", "comments": "Accepted to be published at LCTES 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent ground-breaking advances in deep learning networks ( DNNs ) make\nthem attractive for embedded systems. However, it can take a long time for DNNs\nto make an inference on resource-limited embedded devices. Offloading the\ncomputation into the cloud is often infeasible due to privacy concerns, high\nlatency, or the lack of connectivity. As such, there is a critical need to find\na way to effectively execute the DNN models locally on the devices. This paper\npresents an adaptive scheme to determine which DNN model to use for a given\ninput, by considering the desired accuracy and inference time. Our approach\nemploys machine learning to develop a predictive model to quickly select a\npre-trained DNN to use for a given input and the optimization constraint. We\nachieve this by first training off-line a predictive model, and then use the\nlearnt model to select a DNN model to use for new, unseen inputs. We apply our\napproach to the image classification task and evaluate it on a Jetson TX2\nembedded deep learning platform using the ImageNet ILSVRC 2012 validation\ndataset. We consider a range of influential DNN models. Experimental results\nshow that our approach achieves a 7.52% improvement in inference accuracy, and\na 1.8x reduction in inference time over the most-capable single DNN model.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 06:53:59 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Taylor", "Ben", ""], ["Marco", "Vicent Sanz", ""], ["Wolff", "Willy", ""], ["Elkhatib", "Yehia", ""], ["Wang", "Zheng", ""]]}, {"id": "1805.04262", "submitter": "Yi-Min Chou", "authors": "Yi-Min Chou, Chien-Hung Chen, Keng-Hao Liu, and Chu-Song Chen", "title": "Stingray Detection of Aerial Images Using Augmented Training Images\n  Generated by A Conditional Generative Model", "comments": "to appear in CVPR 2018 Workshop (CVPR 2018 Workshop and Challenge:\n  Automated Analysis of Marine Video for Environmental Monitoring)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an object detection method that tackles the\nstingray detection problem based on aerial images. In this problem, the images\nare aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle\n(UAV), and the stingrays swimming under (but close to) the sea surface are the\ntarget we want to detect and locate. To this end, we use a deep object\ndetection method, faster RCNN, to train a stingray detector based on a limited\ntraining set of images. To boost the performance, we develop a new generative\napproach, conditional GLO, to increase the training samples of stingray, which\nis an extension of the Generative Latent Optimization (GLO) approach. Unlike\ntraditional data augmentation methods that generate new data only for image\nclassification, our proposed method that mixes foreground and background\ntogether can generate new data for an object detection task, and thus improve\nthe training efficacy of a CNN detector. Experimental results show that\nsatisfiable performance can be obtained by using our approach on stingray\ndetection in aerial images.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 07:29:23 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 03:24:23 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 06:44:41 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chou", "Yi-Min", ""], ["Chen", "Chien-Hung", ""], ["Liu", "Keng-Hao", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1805.04288", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Peng Wang, Lingqiao Liu, Chunhua Shen, Jianxin Wu", "title": "Piecewise classifier mappings: Learning fine-grained learners for novel\n  categories with few examples", "comments": "Accepted by IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2019.2924811", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of learning a new fine-grained concept with very little\nsupervision, \\emph{e.g.}, few exemplary images for a species of bird, yet our\nbest deep learning systems need hundreds or thousands of labeled examples. In\nthis paper, we try to reduce this gap by studying the fine-grained image\nrecognition problem in a challenging few-shot learning setting, termed few-shot\nfine-grained recognition (FSFG). The task of FSFG requires the learning systems\nto build classifiers for novel fine-grained categories from few examples (only\none or less than five). To solve this problem, we propose an end-to-end\ntrainable deep network which is inspired by the state-of-the-art fine-grained\nrecognition model and is tailored for the FSFG task.\n  Specifically, our network consists of a bilinear feature learning module and\na classifier mapping module: while the former encodes the discriminative\ninformation of an exemplar image into a feature vector, the latter maps the\nintermediate feature into the decision boundary of the novel category. The key\nnovelty of our model is a \"piecewise mappings\" function in the classifier\nmapping module, which generates the decision boundary via learning a set of\nmore attainable sub-classifiers in a more parameter-economic way. We learn the\nexemplar-to-classifier mapping based on an auxiliary dataset in a meta-learning\nfashion, which is expected to be able to generalize to novel categories. By\nconducting comprehensive experiments on three fine-grained datasets, we\ndemonstrate that the proposed method achieves superior performance over the\ncompeting baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 09:24:15 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 08:03:07 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Wu", "Jianxin", ""]]}, {"id": "1805.04310", "submitter": "Cewu Lu", "authors": "Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai,\n  Cewu Lu", "title": "Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided\n  Knowledge Transfer", "comments": "CVPR'18 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human body part parsing, or human semantic part segmentation, is fundamental\nto many computer vision tasks. In conventional semantic segmentation methods,\nthe ground truth segmentations are provided, and fully convolutional networks\n(FCN) are trained in an end-to-end scheme. Although these methods have\ndemonstrated impressive results, their performance highly depends on the\nquantity and quality of training data. In this paper, we present a novel method\nto generate synthetic human part segmentation data using easily-obtained human\nkeypoint annotations. Our key idea is to exploit the anatomical similarity\namong human to transfer the parsing results of a person to another person with\nsimilar pose. Using these estimated results as additional training data, our\nsemi-supervised model outperforms its strong-supervised counterpart by 6 mIOU\non the PASCAL-Person-Part dataset, and we achieve state-of-the-art human\nparsing results. Our approach is general and can be readily extended to other\nobject/animal parsing task assuming that their anatomical similarity can be\nannotated by keypoints. The proposed model and accompanying source code are\navailable at https://github.com/MVIG-SJTU/WSHP\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 10:30:56 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Fang", "Hao-Shu", ""], ["Lu", "Guansong", ""], ["Fang", "Xiaolin", ""], ["Xie", "Jianwen", ""], ["Tai", "Yu-Wing", ""], ["Lu", "Cewu", ""]]}, {"id": "1805.04384", "submitter": "Feiwu Yu", "authors": "Feiwu Yu, Xinxiao Wu, Yuchao Sun, Lixin Duan", "title": "Exploiting Images for Video Recognition with Hierarchical Generative\n  Adversarial Networks", "comments": "IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning methods of video recognition usually require a large\nnumber of labeled videos for training. But for a new task, videos are often\nunlabeled and it is also time-consuming and labor-intensive to annotate them.\nInstead of human annotation, we try to make use of existing fully labeled\nimages to help recognize those videos. However, due to the problem of domain\nshifts and heterogeneous feature representations, the performance of\nclassifiers trained on images may be dramatically degraded for video\nrecognition tasks. In this paper, we propose a novel method, called\nHierarchical Generative Adversarial Networks (HiGAN), to enhance recognition in\nvideos (i.e., target domain) by transferring knowledge from images (i.e.,\nsource domain). The HiGAN model consists of a \\emph{low-level} conditional GAN\nand a \\emph{high-level} conditional GAN. By taking advantage of these two-level\nadversarial learning, our method is capable of learning a domain-invariant\nfeature representation of source images and target videos. Comprehensive\nexperiments on two challenging video recognition datasets (i.e. UCF101 and\nHMDB51) demonstrate the effectiveness of the proposed method when compared with\nthe existing state-of-the-art domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 13:20:04 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Yu", "Feiwu", ""], ["Wu", "Xinxiao", ""], ["Sun", "Yuchao", ""], ["Duan", "Lixin", ""]]}, {"id": "1805.04385", "submitter": "Lu Yu", "authors": "Lu Yu, Yongmei Cheng, Joost van de Weijer", "title": "Weakly Supervised Domain-Specific Color Naming Based on Attention", "comments": "Accepted at ICPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing color naming methods focuses on the eleven basic\ncolor terms of the English language. However, in many applications, different\nsets of color names are used for the accurate description of objects. Labeling\ndata to learn these domain-specific color names is an expensive and laborious\ntask. Therefore, in this article we aim to learn color names from weakly\nlabeled data. For this purpose, we add an attention branch to the color naming\nnetwork. The attention branch is used to modulate the pixel-wise color naming\npredictions of the network. In experiments, we illustrate that the attention\nbranch correctly identifies the relevant regions. Furthermore, we show that our\nmethod obtains state-of-the-art results for pixel-wise and image-wise\nclassification on the EBAY dataset and is able to learn color names for various\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 13:21:37 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Yu", "Lu", ""], ["Cheng", "Yongmei", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1805.04398", "submitter": "Sabarinath Mahadevan", "authors": "Sabarinath Mahadevan, Paul Voigtlaender and Bastian Leibe", "title": "Iteratively Trained Interactive Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning requires large amounts of training data to be effective. For\nthe task of object segmentation, manually labeling data is very expensive, and\nhence interactive methods are needed. Following recent approaches, we develop\nan interactive object segmentation system which uses user input in the form of\nclicks as the input to a convolutional network. While previous methods use\nheuristic click sampling strategies to emulate user clicks during training, we\npropose a new iterative training strategy. During training, we iteratively add\nclicks based on the errors of the currently predicted segmentation. We show\nthat our iterative training strategy together with additional improvements to\nthe network architecture results in improved results over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 13:43:27 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Mahadevan", "Sabarinath", ""], ["Voigtlaender", "Paul", ""], ["Leibe", "Bastian", ""]]}, {"id": "1805.04409", "submitter": "Dan Xu", "authors": "Dan Xu, Wanli Ouyang, Xiaogang Wang, Nicu Sebe", "title": "PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for\n  Simultaneous Depth Estimation and Scene Parsing", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation and scene parsing are two particularly important tasks in\nvisual scene understanding. In this paper we tackle the problem of simultaneous\ndepth estimation and scene parsing in a joint CNN. The task can be typically\ntreated as a deep multi-task learning problem [42]. Different from previous\nmethods directly optimizing multiple tasks given the input training data, this\npaper proposes a novel multi-task guided prediction-and-distillation network\n(PAD-Net), which first predicts a set of intermediate auxiliary tasks ranging\nfrom low level to high level, and then the predictions from these intermediate\nauxiliary tasks are utilized as multi-modal input via our proposed multi-modal\ndistillation modules for the final tasks. During the joint learning, the\nintermediate tasks not only act as supervision for learning more robust deep\nrepresentations but also provide rich multi-modal information for improving the\nfinal tasks. Extensive experiments are conducted on two challenging datasets\n(i.e. NYUD-v2 and Cityscapes) for both the depth estimation and scene parsing\ntasks, demonstrating the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 14:12:17 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Xu", "Dan", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Sebe", "Nicu", ""]]}, {"id": "1805.04424", "submitter": "Dinesh Kumar Amara", "authors": "Amara Dinesh Kumar", "title": "Novel Deep Learning Model for Traffic Sign Detection Using Capsule\n  Networks", "comments": "5 pages,3 figures", "journal-ref": "International Journal of Pure and Applied Mathematics Volume 118\n  No. 20 2018, 4543-4548 ISSN: 1314-3395", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks are the most widely used deep learning\nalgorithms for traffic signal classification till date but they fail to capture\npose, view, orientation of the images because of the intrinsic inability of max\npooling layer.This paper proposes a novel method for Traffic sign detection\nusing deep learning architecture called capsule networks that achieves\noutstanding performance on the German traffic sign dataset.Capsule network\nconsists of capsules which are a group of neurons representing the\ninstantiating parameters of an object like the pose and orientation by using\nthe dynamic routing and route by agreement algorithms.unlike the previous\napproaches of manual feature extraction,multiple deep neural networks with many\nparameters,our method eliminates the manual effort and provides resistance to\nthe spatial variances.CNNs can be fooled easily using various adversary attacks\nand capsule networks can overcome such attacks from the intruders and can offer\nmore reliability in traffic sign detection for autonomous vehicles.Capsule\nnetwork have achieved the state-of-the-art accuracy of 97.6% on German Traffic\nSign Recognition Benchmark dataset (GTSRB).\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 14:34:15 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Kumar", "Amara Dinesh", ""]]}, {"id": "1805.04487", "submitter": "Yang Zhou", "authors": "Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, Hui\n  Huang", "title": "Non-Stationary Texture Synthesis by Adversarial Expansion", "comments": "Accepted to SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real world exhibits an abundance of non-stationary textures. Examples\ninclude textures with large-scale structures, as well as spatially variant and\ninhomogeneous textures. While existing example-based texture synthesis methods\ncan cope well with stationary textures, non-stationary textures still pose a\nconsiderable challenge, which remains unresolved. In this paper, we propose a\nnew approach for example-based non-stationary texture synthesis. Our approach\nuses a generative adversarial network (GAN), trained to double the spatial\nextent of texture blocks extracted from a specific texture exemplar. Once\ntrained, the fully convolutional generator is able to expand the size of the\nentire exemplar, as well as of any of its sub-blocks. We demonstrate that this\nconceptually simple approach is highly effective for capturing large-scale\nstructures, as well as other non-stationary attributes of the input exemplar.\nAs a result, it can cope with challenging textures, which, to our knowledge, no\nother existing method can handle.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 16:46:52 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Zhou", "Yang", ""], ["Zhu", "Zhen", ""], ["Bai", "Xiang", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "1805.04497", "submitter": "Seungryul Baek", "authors": "Seungryul Baek and Kwang In Kim and Tae-Kyun Kim", "title": "Augmented Skeleton Space Transfer for Depth-based Hand Pose Estimation", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crucial to the success of training a depth-based 3D hand pose estimator (HPE)\nis the availability of comprehensive datasets covering diverse camera\nperspectives, shapes, and pose variations. However, collecting such annotated\ndatasets is challenging. We propose to complete existing databases by\ngenerating new database entries. The key idea is to synthesize data in the\nskeleton space (instead of doing so in the depth-map space) which enables an\neasy and intuitive way of manipulating data entries. Since the skeleton entries\ngenerated in this way do not have the corresponding depth map entries, we\nexploit them by training a separate hand pose generator (HPG) which synthesizes\nthe depth map from the skeleton entries. By training the HPG and HPE in a\nsingle unified optimization framework enforcing that 1) the HPE agrees with the\npaired depth and skeleton entries; and 2) the HPG-HPE combination satisfies the\ncyclic consistency (both the input and the output of HPG-HPE are skeletons)\nobserved via the newly generated unpaired skeletons, our algorithm constructs a\nHPE which is robust to variations that go beyond the coverage of the existing\ndatabase. Our training algorithm adopts the generative adversarial networks\n(GAN) training process. As a by-product, we obtain a hand pose discriminator\n(HPD) that is capable of picking out realistic hand poses. Our algorithm\nexploits this capability to refine the initial skeleton estimates in testing,\nfurther improving the accuracy. We test our algorithm on four challenging\nbenchmark datasets (ICVL, MSRA, NYU and Big Hand 2.2M datasets) and demonstrate\nthat our approach outperforms or is on par with state-of-the-art methods\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 17:28:49 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Baek", "Seungryul", ""], ["Kim", "Kwang In", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1805.04537", "submitter": "George Stein", "authors": "Philippe Berger and George Stein", "title": "A volumetric deep Convolutional Neural Network for simulation of mock\n  dark matter halo catalogues", "comments": "12 pages, 8 figures, 1 table. Accepted to MNRAS", "journal-ref": "Monthly Notices of the Royal Astronomical Society, Volume 482,\n  Issue 3, p.2861-2871, 2019", "doi": "10.1093/mnras/sty2949", "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For modern large-scale structure survey techniques it has become standard\npractice to test data analysis pipelines on large suites of mock simulations, a\ntask which is currently prohibitively expensive for full N-body simulations.\nInstead of calculating this costly gravitational evolution, we have trained a\nthree-dimensional deep Convolutional Neural Network (CNN) to identify dark\nmatter protohalos directly from the cosmological initial conditions. Training\non halo catalogues from the Peak Patch semi-analytic code, we test various CNN\narchitectures and find they generically achieve a Dice coefficient of ~92% in\nonly 24 hours of training. We present a simple and fast geometric halo finding\nalgorithm to extract halos from this powerful pixel-wise binary classifier and\nfind that the predicted catalogues match the mass function and power spectra of\nthe ground truth simulations to within ~10%. We investigate the effect of\nlong-range tidal forces on an object-by-object basis and find that the\nnetwork's predictions are consistent with the non-linear ellipsoidal collapse\nequations used explicitly by the Peak Patch algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 18:05:50 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 15:43:53 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Berger", "Philippe", ""], ["Stein", "George", ""]]}, {"id": "1805.04554", "submitter": "Rudra P K Poudel", "authors": "Rudra P K Poudel, Ujwal Bonde, Stephan Liwicki, Christopher Zach", "title": "ContextNet: Exploring Context and Detail for Semantic Segmentation in\n  Real-time", "comments": "Published as a conference paper at British Machine Vision Conference\n  (BMVC), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning architectures produce highly accurate results on many\nchallenging semantic segmentation datasets. State-of-the-art methods are,\nhowever, not directly transferable to real-time applications or embedded\ndevices, since naive adaptation of such systems to reduce computational cost\n(speed, memory and energy) causes a significant drop in accuracy. We propose\nContextNet, a new deep neural network architecture which builds on factorized\nconvolution, network compression and pyramid representation to produce\ncompetitive semantic segmentation in real-time with low memory requirement.\nContextNet combines a deep network branch at low resolution that captures\nglobal context information efficiently with a shallow branch that focuses on\nhigh-resolution segmentation details. We analyse our network in a thorough\nablation study and present results on the Cityscapes dataset, achieving 66.1%\naccuracy at 18.3 frames per second at full (1024x2048) resolution (41.9 fps\nwith pipelined computations for streamed data).\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 18:52:45 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 14:21:56 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 21:25:51 GMT"}, {"version": "v4", "created": "Mon, 5 Nov 2018 12:41:28 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Poudel", "Rudra P K", ""], ["Bonde", "Ujwal", ""], ["Liwicki", "Stephan", ""], ["Zach", "Christopher", ""]]}, {"id": "1805.04563", "submitter": "Soheil Ghafurian", "authors": "Soheil Ghafurian, Peter Orth, Corey Strickland, Hua Su, Sangita Patel,\n  Steven Soisson, Belma Dogdas", "title": "Classification of Protein Crystallization X-Ray Images Using Major\n  Convolutional Neural Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of protein crystals is necessary for the study of protein\nmolecular function and structure. This is done empirically by processing large\nnumbers of crystallization trials and inspecting them regularly in search of\nthose with forming crystals. To avoid missing the hard-gained crystals, this\nvisual inspection of the trial X-ray images is done manually as opposed to the\nexisting less accurate machine learning methods. To achieve higher accuracy for\nautomation, we applied some of the most successful convolutional neural\nnetworks (ResNet, Inception, VGG, and AlexNet) for 10-way classification of the\nX-ray images. We showed that substantial classification accuracy is gained by\nusing such networks compared to two simpler ones previously proposed for this\npurpose. The best accuracy was obtained from ResNet (81.43%), which corresponds\nto a missed crystal rate of 5.9%. This rate could be lowered to less than 0.1%\nby using a top-3 classification strategy. Our dataset consisted of 486,000\ninternally annotated images, which was augmented to more than a million to\naddress class imbalance. We also provide a label-wise analysis of the results,\nidentifying the main sources of error and inaccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 19:15:21 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ghafurian", "Soheil", ""], ["Orth", "Peter", ""], ["Strickland", "Corey", ""], ["Su", "Hua", ""], ["Patel", "Sangita", ""], ["Soisson", "Steven", ""], ["Dogdas", "Belma", ""]]}, {"id": "1805.04574", "submitter": "Yunchao Wei", "authors": "Yunchao Wei and Huaxin Xiao and Honghui Shi and Zequn Jie and Jiashi\n  Feng and Thomas S. Huang", "title": "Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi-\n  Supervised Semantic Segmentation", "comments": "Accepted by CVPR 2018 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable progress, weakly supervised segmentation approaches\nare still inferior to their fully supervised counterparts. We obverse the\nperformance gap mainly comes from their limitation on learning to produce\nhigh-quality dense object localization maps from image-level supervision. To\nmitigate such a gap, we revisit the dilated convolution [1] and reveal how it\ncan be utilized in a novel way to effectively overcome this critical limitation\nof weakly supervised segmentation approaches. Specifically, we find that\nvarying dilation rates can effectively enlarge the receptive fields of\nconvolutional kernels and more importantly transfer the surrounding\ndiscriminative information to non-discriminative object regions, promoting the\nemergence of these regions in the object localization maps. Then, we design a\ngeneric classification network equipped with convolutional blocks of different\ndilated rates. It can produce dense and reliable object localization maps and\neffectively benefit both weakly- and semi- supervised semantic segmentation.\nDespite the apparent simplicity, our proposed approach obtains superior\nperformance over state-of-the-arts. In particular, it achieves 60.8% and 67.6%\nmIoU scores on Pascal VOC 2012 test set in weakly- (only image-level labels are\navailable) and semi- (1,464 segmentation masks are available) supervised\nsettings, which are the new state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 19:53:41 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 01:18:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wei", "Yunchao", ""], ["Xiao", "Huaxin", ""], ["Shi", "Honghui", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1805.04590", "submitter": "Akash Bapat", "authors": "Akash Bapat and Jan-Michael Frahm", "title": "The Domain Transform Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for edge-aware optimization that is an order of\nmagnitude faster than the state of the art while having comparable performance.\nOur key insight is that the optimization can be formulated by leveraging\nproperties of the domain transform, a method for edge-aware filtering that\ndefines a distance-preserving 1D mapping of the input space. This enables our\nmethod to improve performance for a variety of problems including stereo, depth\nsuper-resolution, and render from defocus, while keeping the computational\ncomplexity linear in the number of pixels. Our method is highly parallelizable\nand adaptable, and it has demonstrable scalability with respect to image\nresolution.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 20:57:46 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Bapat", "Akash", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1805.04596", "submitter": "Andreas Doering", "authors": "Andreas Doering, Umar Iqbal and Juergen Gall", "title": "Joint Flow: Temporal Flow Fields for Multi Person Tracking", "comments": "Accepted to BMVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an online multi person pose tracking approach which\nworks on two consecutive frames $I_{t-1}$ and $I_t$. The general formulation of\nour temporal network allows to rely on any multi person pose estimation\napproach as spatial network. From the spatial network we extract image features\nand pose features for both frames. These features serve as input for our\ntemporal model that predicts Temporal Flow Fields (TFF). These TFF are vector\nfields which indicate the direction in which each body joint is going to move\nfrom frame $I_{t-1}$ to frame $I_t$. This novel representation allows to\nformulate a similarity measure of detected joints. These similarities are used\nas binary potentials in a bipartite graph optimization problem in order to\nperform tracking of multiple poses. We show that these TFF can be learned by a\nrelative small CNN network whilst achieving state-of-the-art multi person pose\ntracking results.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 21:27:08 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 13:04:02 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Doering", "Andreas", ""], ["Iqbal", "Umar", ""], ["Gall", "Juergen", ""]]}, {"id": "1805.04605", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu", "title": "Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration", "comments": "MICCAI 2018 (Oral Presentation). Proceedings: LNCS 11070, pp 729-738", "journal-ref": "LNCS 11070, pp 729-738, Springer. 2018", "doi": "10.1007/978-3-030-00928-1_82", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deformable registration techniques achieve impressive results and\noffer a rigorous theoretical treatment, but are computationally intensive since\nthey solve an optimization problem for each image pair. Recently,\nlearning-based methods have facilitated fast registration by learning spatial\ndeformation functions. However, these approaches use restricted deformation\nmodels, require supervised labels, or do not guarantee a diffeomorphic\n(topology-preserving) registration. Furthermore, learning-based registration\ntools have not been derived from a probabilistic framework that can offer\nuncertainty estimates. In this paper, we present a probabilistic generative\nmodel and derive an unsupervised learning-based inference algorithm that makes\nuse of recent developments in convolutional neural networks (CNNs). We\ndemonstrate our method on a 3D brain registration task, and provide an\nempirical analysis of the algorithm. Our approach results in state of the art\naccuracy and very fast runtimes, while providing diffeomorphic guarantees and\nuncertainty estimates. Our implementation is available online at\nhttp://voxelmorph.csail.mit.edu .\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 22:12:01 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 13:28:36 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Balakrishnan", "Guha", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1805.04628", "submitter": "Hoel Kervadec", "authors": "Hoel Kervadec and Jose Dolz and Meng Tang and Eric Granger and Yuri\n  Boykov and Ismail Ben Ayed", "title": "Constrained-CNN losses for weakly supervised segmentation", "comments": "Extended work of the work presented at the 1st conference on Medical\n  Image with Deep Learning (MIDL). Currently under review at Medical Image\n  Analysis", "journal-ref": null, "doi": "10.1016/j.media.2019.02.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised learning based on, e.g., partially labelled images or\nimage-tags, is currently attracting significant attention in CNN segmentation\nas it can mitigate the need for full and laborious pixel/voxel annotations.\nEnforcing high-order (global) inequality constraints on the network output (for\ninstance, to constrain the size of the target region) can leverage unlabeled\ndata, guiding the training process with domain-specific knowledge. Inequality\nconstraints are very flexible because they do not assume exact prior knowledge.\nHowever, constrained Lagrangian dual optimization has been largely avoided in\ndeep networks, mainly for computational tractability reasons. To the best of\nour knowledge, the method of [Pathak et al., 2015] is the only prior work that\naddresses deep CNNs with linear constraints in weakly supervised segmentation.\nIt uses the constraints to synthesize fully-labeled training masks (proposals)\nfrom weak labels, mimicking full supervision and facilitating dual\noptimization. We propose to introduce a differentiable penalty, which enforces\ninequality constraints directly in the loss function, avoiding expensive\nLagrangian dual iterates and proposal generation. From constrained-optimization\nperspective, our simple penalty-based approach is not optimal as there is no\nguarantee that the constraints are satisfied. However, surprisingly, it yields\nsubstantially better results than the Lagrangian-based constrained CNNs in\n[Pathak et al., 2015], while reducing the computational demand for training. By\nannotating only a small fraction of the pixels, the proposed approach can reach\na level of segmentation performance that is comparable to full supervision on\nthree separate tasks. While our experiments focused on basic linear constraints\nsuch as the target-region size and image tags, our framework can be easily\nextended to other non-linear constraints.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 00:51:54 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 20:06:55 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kervadec", "Hoel", ""], ["Dolz", "Jose", ""], ["Tang", "Meng", ""], ["Granger", "Eric", ""], ["Boykov", "Yuri", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1805.04634", "submitter": "Min Xu", "authors": "Kai Wen Wang, Xiangrui Zeng, Xiaodan Liang, Zhiguang Huo, Eric P.\n  Xing, Min Xu", "title": "Image-derived generative modeling of pseudo-macromolecular structures -\n  towards the statistical assessment of Electron CryoTomography template\n  matching", "comments": null, "journal-ref": "British Machine Vision Conference (BMVC) 2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Electron CryoTomography (CECT) is a 3D imaging technique that\ncaptures information about the structure and spatial organization of\nmacromolecular complexes within single cells, in near-native state and at\nsub-molecular resolution. Although template matching is often used to locate\nmacromolecules in a CECT image, it is insufficient as it only measures the\nrelative structural similarity. Therefore, it is preferable to assess the\nstatistical credibility of the decision through hypothesis testing, requiring\nmany templates derived from a diverse population of macromolecular structures.\nDue to the very limited number of known structures, we need a generative model\nto efficiently and reliably sample pseudo-structures from the complex\ndistribution of macromolecular structures. To address this challenge, we\npropose a novel image-derived approach for performing hypothesis testing for\ntemplate matching by constructing generative models using the generative\nadversarial network. Finally, we conducted hypothesis testing experiments for\ntemplate matching on both simulated and experimental subtomograms, allowing us\nto conclude the identity of subtomograms with high statistical credibility and\nsignificantly reducing false positives.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 02:00:30 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Kai Wen", ""], ["Zeng", "Xiangrui", ""], ["Liang", "Xiaodan", ""], ["Huo", "Zhiguang", ""], ["Xing", "Eric P.", ""], ["Xu", "Min", ""]]}, {"id": "1805.04635", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin, and Pheng-Ann Heng", "title": "Direction-aware Spatial Context Features for Shadow Detection and\n  Removal", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). This is the journal version of arXiv:1712.04142, which\n  was accepted for oral presentation in CVPR 2018", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2919616", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow detection and shadow removal are fundamental and challenging tasks,\nrequiring an understanding of the global image semantics. This paper presents a\nnovel deep neural network design for shadow detection and removal by analyzing\nthe spatial image context in a direction-aware manner. To achieve this, we\nfirst formulate the direction-aware attention mechanism in a spatial recurrent\nneural network (RNN) by introducing attention weights when aggregating spatial\ncontext features in the RNN. By learning these weights through training, we can\nrecover direction-aware spatial context (DSC) for detecting and removing\nshadows. This design is developed into the DSC module and embedded in a\nconvolutional neural network (CNN) to learn the DSC features at different\nlevels. Moreover, we design a weighted cross entropy loss to make effective the\ntraining for shadow detection and further adopt the network for shadow removal\nby using a Euclidean loss function and formulating a color transfer function to\naddress the color and luminosity inconsistencies in the training pairs. We\nemployed two shadow detection benchmark datasets and two shadow removal\nbenchmark datasets, and performed various experiments to evaluate our method.\nExperimental results show that our method performs favorably against the\nstate-of-the-art methods for both shadow detection and shadow removal.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 02:15:42 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 05:38:18 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Hu", "Xiaowei", ""], ["Fu", "Chi-Wing", ""], ["Zhu", "Lei", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1805.04668", "submitter": "Shweta Bhardwaj", "authors": "Shweta Bhardwaj and Mitesh M. Khapra", "title": "I Have Seen Enough: A Teacher Student Network for Video Classification\n  Using Fewer Frames", "comments": "CVPR Workshop on Brave New Ideas for Video Understanding (BIVU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, various tasks involving videos such as\nclassification, description, summarization and question answering have received\na lot of attention. Current models for these tasks compute an encoding of the\nvideo by treating it as a sequence of images and going over every image in the\nsequence. However, for longer videos this is very time consuming. In this\npaper, we focus on the task of video classification and aim to reduce the\ncomputational time by using the idea of distillation. Specifically, we first\ntrain a teacher network which looks at all the frames in a video and computes a\nrepresentation for the video. We then train a student network whose objective\nis to process only a small fraction of the frames in the video and still\nproduce a representation which is very close to the representation computed by\nthe teacher network. This smaller student network involving fewer computations\ncan then be employed at inference time for video classification. We experiment\nwith the YouTube-8M dataset and show that the proposed student network can\nreduce the inference time by upto 30% with a very small drop in the performance\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 06:22:54 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Bhardwaj", "Shweta", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "1805.04687", "submitter": "Fisher Yu", "authors": "Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen\n  Liu, Vashisht Madhavan, Trevor Darrell", "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning", "comments": "Published at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets drive vision progress, yet existing driving datasets are\nimpoverished in terms of visual content and supported tasks to study multitask\nlearning for autonomous driving. Researchers are usually constrained to study a\nsmall set of problems on one dataset, while real-world computer vision\napplications require performing tasks of various complexities. We construct\nBDD100K, the largest driving video dataset with 100K videos and 10 tasks to\nevaluate the exciting progress of image recognition algorithms on autonomous\ndriving. The dataset possesses geographic, environmental, and weather\ndiversity, which is useful for training models that are less likely to be\nsurprised by new conditions. Based on this diverse dataset, we build a\nbenchmark for heterogeneous multitask learning and study how to solve the tasks\ntogether. Our experiments show that special training strategies are needed for\nexisting models to perform such heterogeneous tasks. BDD100K opens the door for\nfuture studies in this important venue.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 09:24:21 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 09:25:06 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Yu", "Fisher", ""], ["Chen", "Haofeng", ""], ["Wang", "Xin", ""], ["Xian", "Wenqi", ""], ["Chen", "Yingying", ""], ["Liu", "Fangchen", ""], ["Madhavan", "Vashisht", ""], ["Darrell", "Trevor", ""]]}, {"id": "1805.04714", "submitter": "Grigorios Kalliatakis M.A.", "authors": "Grigorios Kalliatakis, Shoaib Ehsan, Ales Leonardis and Klaus\n  McDonald-Maier", "title": "Exploring object-centric and scene-centric CNN features and their\n  complementarity for human rights violations recognition in images", "comments": "19 pages, 13 figures; Submitted to PLOS ONE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying potential abuses of human rights through imagery is a novel and\nchallenging task in the field of computer vision, that will enable to expose\nhuman rights violations over large-scale data that may otherwise be impossible.\nWhile standard databases for object and scene categorisation contain hundreds\nof different classes, the largest available dataset of human rights violations\ncontains only 4 classes. Here, we introduce the `Human Rights Archive Database'\n(HRA), a verified-by-experts repository of 3050 human rights violations\nphotographs, labelled with human rights semantic categories, comprising a list\nof the types of human rights abuses encountered at present. With the HRA\ndataset and a two-phase transfer learning scheme, we fine-tuned the\nstate-of-the-art deep convolutional neural networks (CNNs) to provide human\nrights violations classification CNNs (HRA-CNNs). We also present extensive\nexperiments refined to evaluate how well object-centric and scene-centric CNN\nfeatures can be combined for the task of recognising human rights abuses. With\nthis, we show that HRA database poses a challenge at a higher level for the\nwell studied representation learning methods, and provide a benchmark in the\ntask of human rights violations recognition in visual context. We expect this\ndataset can help to open up new horizons on creating systems able of\nrecognising rich information about human rights violations. Our dataset, codes\nand trained models are available online at\nhttps://github.com/GKalliatakis/Human-Rights-Archive-CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 12:50:03 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Ehsan", "Shoaib", ""], ["Leonardis", "Ales", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1805.04771", "submitter": "Seonwook Park", "authors": "Seonwook Park, Xucong Zhang, Andreas Bulling, Otmar Hilliges", "title": "Learning to Find Eye Region Landmarks for Remote Gaze Estimation in\n  Unconstrained Settings", "comments": null, "journal-ref": null, "doi": "10.1145/3204493.3204545", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional feature-based and model-based gaze estimation methods have\nproven to perform well in settings with controlled illumination and specialized\ncameras. In unconstrained real-world settings, however, such methods are\nsurpassed by recent appearance-based methods due to difficulties in modeling\nfactors such as illumination changes and other visual artifacts. We present a\nnovel learning-based method for eye region landmark localization that enables\nconventional methods to be competitive to latest appearance-based methods.\nDespite having been trained exclusively on synthetic data, our method exceeds\nthe state of the art for iris localization and eye shape registration on\nreal-world imagery. We then use the detected landmarks as input to iterative\nmodel-fitting and lightweight learning-based gaze estimation methods. Our\napproach outperforms existing model-fitting and appearance-based methods in the\ncontext of person-independent and personalized gaze estimation.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 19:50:32 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Park", "Seonwook", ""], ["Zhang", "Xucong", ""], ["Bulling", "Andreas", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1805.04777", "submitter": "Marvin T. T. Teichmann", "authors": "Marvin T. T. Teichmann and Roberto Cipolla", "title": "Convolutional CRFs for Semantic Segmentation", "comments": "8 Pages + Appendix, references. Code can be found under:\n  https://github.com/MarvinTeichmann/ConvCRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the challenging semantic image segmentation task the most efficient\nmodels have traditionally combined the structured modelling capabilities of\nConditional Random Fields (CRFs) with the feature extraction power of CNNs. In\nmore recent works however, CRF post-processing has fallen out of favour. We\nargue that this is mainly due to the slow training and inference speeds of\nCRFs, as well as the difficulty of learning the internal CRF parameters. To\novercome both issues we propose to add the assumption of conditional\nindependence to the framework of fully-connected CRFs. This allows us to\nreformulate the inference in terms of convolutions, which can be implemented\nhighly efficiently on GPUs. Doing so speeds up inference and training by a\nfactor of more then 100. All parameters of the convolutional CRFs can easily be\noptimized using backpropagation. To facilitating further CRF research we make\nour implementation publicly available. Please visit:\nhttps://github.com/MarvinTeichmann/ConvCRF\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 20:37:27 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 12:36:12 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Teichmann", "Marvin T. T.", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1805.04792", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li and Timothy R. Langlois and Changxi Zheng", "title": "Scene-Aware Audio for 360\\textdegree{} Videos", "comments": "SIGGRAPH 2018, Technical Papers, 12 pages, 17 figures,\n  http://www.cs.columbia.edu/cg/360audio/", "journal-ref": null, "doi": "10.1145/3197517.3201391", "report-no": null, "categories": "cs.GR cs.CV cs.ET cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although 360\\textdegree{} cameras ease the capture of panoramic footage, it\nremains challenging to add realistic 360\\textdegree{} audio that blends into\nthe captured scene and is synchronized with the camera motion. We present a\nmethod for adding scene-aware spatial audio to 360\\textdegree{} videos in\ntypical indoor scenes, using only a conventional mono-channel microphone and a\nspeaker. We observe that the late reverberation of a room's impulse response is\nusually diffuse spatially and directionally. Exploiting this fact, we propose a\nmethod that synthesizes the directional impulse response between any source and\nlistening locations by combining a synthesized early reverberation part and a\nmeasured late reverberation tail. The early reverberation is simulated using a\ngeometric acoustic simulation and then enhanced using a frequency modulation\nmethod to capture room resonances. The late reverberation is extracted from a\nrecorded impulse response, with a carefully chosen time duration that separates\nout the late reverberation from the early reverberation. In our validations, we\nshow that our synthesized spatial audio matches closely with recordings using\nambisonic microphones. Lastly, we demonstrate the strength of our method in\nseveral applications.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 22:06:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Li", "Dingzeyu", ""], ["Langlois", "Timothy R.", ""], ["Zheng", "Changxi", ""]]}, {"id": "1805.04828", "submitter": "Hojjat Seyed Mousavi", "authors": "Hojjat Seyed Mousavi", "title": "Enhanced Signal Recovery via Sparsity Inducing Image Priors", "comments": "PhD dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsimony in signal representation is a topic of active research. Sparse\nsignal processing and representation is the outcome of this line of research\nwhich has many applications in information processing and has shown significant\nimprovement in real-world applications such as recovery, classification,\nclustering, super resolution, etc. This vast influence of sparse signal\nprocessing in real-world problems raises a significant need in developing novel\nsparse signal representation algorithms to obtain more robust systems. In such\nalgorithms, a few open challenges remain in (a) efficiently posing sparsity on\nsignals that can capture the structure of underlying signal and (b) the design\nof tractable algorithms that can recover signals under aforementioned sparse\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 06:13:47 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Mousavi", "Hojjat Seyed", ""]]}, {"id": "1805.04855", "submitter": "Dinesh Acharya", "authors": "Dinesh Acharya, Zhiwu Huang, Danda Paudel, Luc Van Gool", "title": "Covariance Pooling For Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying facial expressions into different categories requires capturing\nregional distortions of facial landmarks. We believe that second-order\nstatistics such as covariance is better able to capture such distortions in\nregional facial fea- tures. In this work, we explore the benefits of using a\nman- ifold network structure for covariance pooling to improve facial\nexpression recognition. In particular, we first employ such kind of manifold\nnetworks in conjunction with tradi- tional convolutional networks for spatial\npooling within in- dividual image feature maps in an end-to-end deep learning\nmanner. By doing so, we are able to achieve a recognition accuracy of 58.14% on\nthe validation set of Static Facial Expressions in the Wild (SFEW 2.0) and\n87.0% on the vali- dation set of Real-World Affective Faces (RAF) Database.\nBoth of these results are the best results we are aware of. Besides, we\nleverage covariance pooling to capture the tem- poral evolution of per-frame\nfeatures for video-based facial expression recognition. Our reported results\ndemonstrate the advantage of pooling image-set features temporally by stacking\nthe designed manifold network of covariance pool-ing on top of convolutional\nnetwork layers.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 10:31:14 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Acharya", "Dinesh", ""], ["Huang", "Zhiwu", ""], ["Paudel", "Danda", ""], ["Van Gool", "Luc", ""]]}, {"id": "1805.04902", "submitter": "Kazuki Minemura Ph.D", "authors": "Kazuki Minemura, Hengfui Liau, Abraham Monrroy and Shinpei Kato", "title": "LMNet: Real-time Multiclass Object Detection on CPU using 3D LiDAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an optimized single-stage deep convolutional neural\nnetwork to detect objects in urban environments, using nothing more than point\ncloud data. This feature enables our method to work regardless the time of the\nday and the lighting conditions.The proposed network structure employs dilated\nconvolutions to gradually increase the perceptive field as depth increases,\nthis helps to reduce the computation time by about 30%. The network input\nconsists of five perspective representations of the unorganized point cloud\ndata. The network outputs an objectness map and the bounding box offset values\nfor each point. Our experiments showed that using reflection, range, and the\nposition on each of the three axes helped to improve the location and\norientation of the output bounding box. We carried out quantitative evaluations\nwith the help of the KITTI dataset evaluation server. It achieved the fastest\nprocessing speed among the other contenders, making it suitable for real-time\napplications. We implemented and tested it on a real vehicle with a Velodyne\nHDL-64 mounted on top of it. We achieved execution times as fast as 50 FPS\nusing desktop GPUs, and up to 10 FPS on a single Intel Core i5 CPU. The deploy\nimplementation is open-sourced and it can be found as a feature branch inside\nthe autonomous driving framework Autoware. Code is available at:\nhttps://github.com/CPFL/Autoware/tree/feature/cnn_lidar_detection\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 15:55:33 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 15:41:47 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Minemura", "Kazuki", ""], ["Liau", "Hengfui", ""], ["Monrroy", "Abraham", ""], ["Kato", "Shinpei", ""]]}, {"id": "1805.04925", "submitter": "Jiacheng Zhu", "authors": "Jiacheng Zhu, Wenshuo Wang, Ding Zhao", "title": "A Tempt to Unify Heterogeneous Driving Databases using Traffic\n  Primitives", "comments": "6 pages, 7 figures, 1 table, ITSC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multitude of publicly-available driving datasets and data platforms have\nbeen raised for autonomous vehicles (AV). However, the heterogeneities of\ndatabases in size, structure and driving context make existing datasets\npractically ineffective due to a lack of uniform frameworks and searchable\nindexes. In order to overcome these limitations on existing public datasets,\nthis paper proposes a data unification framework based on traffic primitives\nwith ability to automatically unify and label heterogeneous traffic data. This\nis achieved by two steps: 1) Carefully arrange raw multidimensional time series\ndriving data into a relational database and then 2) automatically extract\nlabeled and indexed traffic primitives from traffic data through a Bayesian\nnonparametric learning method. Finally, we evaluate the effectiveness of our\ndeveloped framework using the collected real vehicle data.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 18:42:59 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhu", "Jiacheng", ""], ["Wang", "Wenshuo", ""], ["Zhao", "Ding", ""]]}, {"id": "1805.04949", "submitter": "Peng Wang", "authors": "Peng Wang, Ruigang Yang, Binbin Cao, Wei Xu, Yuanqing Lin", "title": "DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map", "comments": "Accepted in CVPR 2018. arXiv admin note: substantial text overlap\n  with arXiv:1803.06184", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications such as autonomous driving, self-localization/camera pose\nestimation and scene parsing are crucial technologies. In this paper, we\npropose a unified framework to tackle these two problems simultaneously. The\nuniqueness of our design is a sensor fusion scheme which integrates camera\nvideos, motion sensors (GPS/IMU), and a 3D semantic map in order to achieve\nrobustness and efficiency of the system. Specifically, we first have an initial\ncoarse camera pose obtained from consumer-grade GPS/IMU, based on which a label\nmap can be rendered from the 3D semantic map. Then, the rendered label map and\nthe RGB image are jointly fed into a pose CNN, yielding a corrected camera\npose. In addition, to incorporate temporal information, a multi-layer recurrent\nneural network (RNN) is further deployed improve the pose accuracy. Finally,\nbased on the pose from RNN, we render a new label map, which is fed together\nwith the RGB image into a segment CNN which produces per-pixel semantic label.\nIn order to validate our approach, we build a dataset with registered 3D point\nclouds and video camera images. Both the point clouds and the images are\nsemantically-labeled. Each video frame has ground truth pose from highly\naccurate motion sensors. We show that practically, pose estimation solely\nrelying on images like PoseNet may fail due to street view confusion, and it is\nimportant to fuse multiple sensors. Finally, various ablation studies are\nperformed, which demonstrate the effectiveness of the proposed system. In\nparticular, we show that scene parsing and pose estimation are mutually\nbeneficial to achieve a more robust and accurate system.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 21:18:30 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Wang", "Peng", ""], ["Yang", "Ruigang", ""], ["Cao", "Binbin", ""], ["Xu", "Wei", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1805.04953", "submitter": "Peng Zhou", "authors": "Peng Zhou, Xintong Han, Vlad I. Morariu, Larry S. Davis", "title": "Learning Rich Features for Image Manipulation Detection", "comments": "CVPR 2018 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image manipulation detection is different from traditional semantic object\ndetection because it pays more attention to tampering artifacts than to image\ncontent, which suggests that richer features need to be learned. We propose a\ntwo-stream Faster R-CNN network and train it endto- end to detect the tampered\nregions given a manipulated image. One of the two streams is an RGB stream\nwhose purpose is to extract features from the RGB image input to find tampering\nartifacts like strong contrast difference, unnatural tampered boundaries, and\nso on. The other is a noise stream that leverages the noise features extracted\nfrom a steganalysis rich model filter layer to discover the noise inconsistency\nbetween authentic and tampered regions. We then fuse features from the two\nstreams through a bilinear pooling layer to further incorporate spatial\nco-occurrence of these two modalities. Experiments on four standard image\nmanipulation datasets demonstrate that our two-stream framework outperforms\neach individual stream, and also achieves state-of-the-art performance compared\nto alternative methods with robustness to resizing and compression.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 21:29:38 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhou", "Peng", ""], ["Han", "Xintong", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1805.04969", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Learning Temporal Strategic Relationships using Generative Adversarial\n  Imitation Learning", "comments": "International Foundation for Autonomous Agents and Multiagent\n  Systems, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for automatic learning of complex\nstrategies in human decision making. The task that we are interested in is to\nbetter facilitate long term planning for complex, multi-step events. We observe\ntemporal relationships at the subtask level of expert demonstrations, and\ndetermine the different strategies employed in order to successfully complete a\ntask. To capture the relationship between the subtasks and the overall goal, we\nutilise two external memory modules, one for capturing dependencies within a\nsingle expert demonstration, such as the sequential relationship among\ndifferent sub tasks, and a global memory module for modelling task level\ncharacteristics such as best practice employed by different humans based on\ntheir domain expertise. Furthermore, we demonstrate how the hidden state\nrepresentation of the memory can be used as a reward signal to smooth the state\ntransitions, eradicating subtle changes. We evaluate the effectiveness of the\nproposed model for an autonomous highway driving application, where we\ndemonstrate its capability to learn different expert policies and outperform\nstate-of-the-art methods. The scope in industrial applications extends to any\nrobotics and automation application which requires learning from complex\ndemonstrations containing series of subtasks.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 22:56:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1805.04980", "submitter": "Yi-Ming Chan", "authors": "Yi-Min Chou, Yi-Ming Chan, Jia-Hong Lee, Chih-Yi Chiu, Chu-Song Chen", "title": "Unifying and Merging Well-trained Deep Neural Networks for Inference\n  Stage", "comments": "To appear in the 27th International Joint Conference on Artificial\n  Intelligence and the 23rd European Conference on Artificial Intelligence,\n  2018. (IJCAI-ECAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to merge convolutional neural-nets for the\ninference stage. Given two well-trained networks that may have different\narchitectures that handle different tasks, our method aligns the layers of the\noriginal networks and merges them into a unified model by sharing the\nrepresentative codes of weights. The shared weights are further re-trained to\nfine-tune the performance of the merged model. The proposed method effectively\nproduces a compact model that may run original tasks simultaneously on\nresource-limited devices. As it preserves the general architectures and\nleverages the co-used weights of well-trained networks, a substantial training\noverhead can be reduced to shorten the system development time. Experimental\nresults demonstrate a satisfactory performance and validate the effectiveness\nof the method.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 01:32:02 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Chou", "Yi-Min", ""], ["Chan", "Yi-Ming", ""], ["Lee", "Jia-Hong", ""], ["Chiu", "Chih-Yi", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1805.05009", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Simon Denman", "title": "Deep Decision Trees for Discriminative Dictionary Learning with\n  Adversarial Multi-Agent Trajectories", "comments": "To appear in 4th International Workshop on Computer Vision in Sports\n  (CVsports) at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosion in the availability of spatio-temporal tracking data in\nmodern sports, there is an enormous opportunity to better analyse, learn and\npredict important events in adversarial group environments. In this paper, we\npropose a deep decision tree architecture for discriminative dictionary\nlearning from adversarial multi-agent trajectories. We first build up a\nhierarchy for the tree structure by adding each layer and performing feature\nweight based clustering in the forward pass. We then fine tune the player role\nweights using back propagation. The hierarchical architecture ensures the\ninterpretability and the integrity of the group representation. The resulting\narchitecture is a decision tree, with leaf-nodes capturing a dictionary of\nmulti-agent group interactions. Due to the ample volume of data available, we\nfocus on soccer tracking data, although our approach can be used in any\nadversarial multi-agent domain. We present applications of proposed method for\nsimulating soccer games as well as evaluating and quantifying team strategies.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 04:25:14 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Fernando", "Tharindu", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""], ["Denman", "Simon", ""]]}, {"id": "1805.05020", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Sifei Liu, Deqing Sun, Jiawei Zhang, Yang Liu, Jimmy Ren,\n  Zechao Li, Jinhui Tang, Huchuan Lu, Yu-Wing Tai, Ming-Hsuan Yang", "title": "Learning Dual Convolutional Neural Networks for Low-Level Vision", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general dual convolutional neural network\n(DualCNN) for low-level vision problems, e.g., super-resolution,\nedge-preserving filtering, deraining and dehazing. These problems usually\ninvolve the estimation of two components of the target signals: structures and\ndetails. Motivated by this, our proposed DualCNN consists of two parallel\nbranches, which respectively recovers the structures and details in an\nend-to-end manner. The recovered structures and details can generate the target\nsignals according to the formation model for each particular application. The\nDualCNN is a flexible framework for low-level vision tasks and can be easily\nincorporated into existing CNNs. Experimental results show that the DualCNN can\nbe effectively applied to numerous low-level vision tasks with favorable\nperformance against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 06:24:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Pan", "Jinshan", ""], ["Liu", "Sifei", ""], ["Sun", "Deqing", ""], ["Zhang", "Jiawei", ""], ["Liu", "Yang", ""], ["Ren", "Jimmy", ""], ["Li", "Zechao", ""], ["Tang", "Jinhui", ""], ["Lu", "Huchuan", ""], ["Tai", "Yu-Wing", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1805.05029", "submitter": "Jiamiao Xu", "authors": "Xinge You, Jiamiao Xu, Wei Yuan, Xiao-Yuan Jing, Dacheng Tao and\n  Taiping Zhang", "title": "Multi-view Common Component Discriminant Analysis for Cross-view\n  Classification", "comments": "This paper needs more revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view classification that means to classify samples from heterogeneous\nviews is a significant yet challenging problem in computer vision. A promising\napproach to handle this problem is the multi-view subspace learning (MvSL),\nwhich intends to find a common subspace for multi-view data. Despite the\nsatisfactory results achieved by existing methods, the performance of previous\nwork will be dramatically degraded when multi-view data lies on nonlinear\nmanifolds. To circumvent this drawback, we propose Multi-view Common Component\nDiscriminant Analysis (MvCCDA) to handle view discrepancy, discriminability and\nnonlinearity in a joint manner. Specifically, our MvCCDA incorporates\nsupervised information and local geometric information into the common\ncomponent extraction process to learn a discriminant common subspace and to\ndiscover the nonlinear structure embedded in multi-view data. We develop a\nkernel method of MvCCDA to further boost the performance of MvCCDA. Beyond\nkernel extension, optimization and complexity analysis of MvCCDA are also\npresented for completeness. Our MvCCDA is competitive with the state-of-the-art\nMvSL based methods on four benchmark datasets, demonstrating its superiority.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 07:09:33 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 03:56:44 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["You", "Xinge", ""], ["Xu", "Jiamiao", ""], ["Yuan", "Wei", ""], ["Jing", "Xiao-Yuan", ""], ["Tao", "Dacheng", ""], ["Zhang", "Taiping", ""]]}, {"id": "1805.05062", "submitter": "Maha Elbayad", "authors": "Maha Elbayad and Laurent Besacier and Jakob Verbeek", "title": "Token-level and sequence-level loss smoothing for RNN language models", "comments": "Accepted by ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the effectiveness of recurrent neural network language models, their\nmaximum likelihood estimation suffers from two limitations. It treats all\nsentences that do not match the ground truth as equally poor, ignoring the\nstructure of the output space. Second, it suffers from \"exposure bias\": during\ntraining tokens are predicted given ground-truth sequences, while at test time\nprediction is conditioned on generated output sequences. To overcome these\nlimitations we build upon the recent reward augmented maximum likelihood\napproach \\ie sequence-level smoothing that encourages the model to predict\nsentences close to the ground truth according to a given performance metric. We\nextend this approach to token-level loss smoothing, and propose improvements to\nthe sequence-level smoothing approach. Our experiments on two different tasks,\nimage captioning and machine translation, show that token-level and\nsequence-level loss smoothing are complementary, and significantly improve\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 08:37:50 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Elbayad", "Maha", ""], ["Besacier", "Laurent", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1805.05086", "submitter": "S\\'ebastien Ehrhardt", "authors": "Sebastien Ehrhardt and Aron Monszpart and Niloy Mitra and Andrea\n  Vedaldi", "title": "Unsupervised Intuitive Physics from Visual Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While learning models of intuitive physics is an increasingly active area of\nresearch, current approaches still fall short of natural intelligences in one\nimportant regard: they require external supervision, such as explicit access to\nphysical states, at training and sometimes even at test times. Some authors\nhave relaxed such requirements by supplementing the model with an handcrafted\nphysical simulator. Still, the resulting methods are unable to automatically\nlearn new complex environments and to understand physical interactions within\nthem. In this work, we demonstrated for the first time learning such predictors\ndirectly from raw visual observations and without relying on simulators. We do\nso in two steps: first, we learn to track mechanically-salient objects in\nvideos using causality and equivariance, two unsupervised learning principles\nthat do not require auto-encoding. Second, we demonstrate that the extracted\npositions are sufficient to successfully train visual motion predictors that\ncan take the underlying environment into account. We validate our predictors on\nsynthetic datasets; then, we introduce a new dataset, ROLL4REAL, consisting of\nreal objects rolling on complex terrains (pool table, elliptical bowl, and\nrandom height-field). We show that in all such cases it is possible to learn\nreliable extrapolators of the object trajectories from raw videos alone,\nwithout any form of external supervision and with no more prior knowledge than\nthe choice of a convolutional neural network architecture.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 09:41:23 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 11:15:22 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Ehrhardt", "Sebastien", ""], ["Monszpart", "Aron", ""], ["Mitra", "Niloy", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1805.05098", "submitter": "Wenshuo Li", "authors": "Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang,\n  Huazhong Yang", "title": "Hu-Fu: Hardware and Software Collaborative Attack Framework against\n  Neural Networks", "comments": "6 pages, 8 figures, 5 tables, accepted to ISVLSI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Learning (DL), especially Convolutional Neural Network (CNN),\ndevelops rapidly and is applied to many tasks, such as image classification,\nface recognition, image segmentation, and human detection. Due to its superior\nperformance, DL-based models have a wide range of application in many areas,\nsome of which are extremely safety-critical, e.g. intelligent surveillance and\nautonomous driving. Due to the latency and privacy problem of cloud computing,\nembedded accelerators are popular in these safety-critical areas. However, the\nrobustness of the embedded DL system might be harmed by inserting\nhardware/software Trojans into the accelerator and the neural network model,\nsince the accelerator and deploy tool (or neural network model) are usually\nprovided by third-party companies. Fortunately, inserting hardware Trojans can\nonly achieve inflexible attack, which means that hardware Trojans can easily\nbreak down the whole system or exchange two outputs, but can't make CNN\nrecognize unknown pictures as targets. Though inserting software Trojans has\nmore freedom of attack, it often requires tampering input images, which is not\neasy for attackers. So, in this paper, we propose a hardware-software\ncollaborative attack framework to inject hidden neural network Trojans, which\nworks as a back-door without requiring manipulating input images and is\nflexible for different scenarios. We test our attack framework for image\nclassification and face recognition tasks, and get attack success rate of 92.6%\nand 100% on CIFAR10 and YouTube Faces, respectively, while keeping almost the\nsame accuracy as the unattacked model in the normal mode. In addition, we show\na specific attack scenario in which a face recognition system is attacked and\ngives a specific wrong answer.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 10:15:29 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 06:33:45 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Li", "Wenshuo", ""], ["Yu", "Jincheng", ""], ["Ning", "Xuefei", ""], ["Wang", "Pengjun", ""], ["Wei", "Qi", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "1805.05132", "submitter": "Chunbiao Zhu", "authors": "Chunbiao Zhu, Wenhao Zhang, Thomas H. Li, Ge Li", "title": "Exploiting the Value of the Center-dark Channel Prior for Salient Object\n  Detection", "comments": "Project website: https://chunbiaozhu.github.io/ACVR2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection aims to detect the most attractive objects in images and\nis widely used as a foundation for various applications. In this paper, we\npropose a novel salient object detection algorithm for RGB-D images using\ncenter-dark channel priors. First, we generate an initial saliency map based on\na color saliency map and a depth saliency map of a given RGB-D image. Then, we\ngenerate a center-dark channel map based on center saliency and dark channel\npriors. Finally, we fuse the initial saliency map with the center dark channel\nmap to generate the final saliency map. Extensive evaluations over four\nbenchmark datasets demonstrate that our proposed method performs favorably\nagainst most of the state-of-the-art approaches. Besides, we further discuss\nthe application of the proposed algorithm in small target detection and\ndemonstrate the universal value of center-dark channel priors in the field of\nobject detection.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 12:02:20 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhu", "Chunbiao", ""], ["Zhang", "Wenhao", ""], ["Li", "Thomas H.", ""], ["Li", "Ge", ""]]}, {"id": "1805.05239", "submitter": "Hamid Tizhoosh", "authors": "Sara Ross-Howe, H.R. Tizhoosh", "title": "The Effects of Image Pre- and Post-Processing, Wavelet Decomposition,\n  and Local Binary Patterns on U-Nets for Skin Lesion Segmentation", "comments": "Accepted for publication in proceedings of the IEEE World Congress on\n  Computational Intelligence (IEEE WCCI), Rio de Janeiro, Brazil, 8-3 July,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is a widespread, global, and potentially deadly disease, which\nover the last three decades has afflicted more lives in the USA than all other\nforms of cancer combined. There have been a lot of promising recent works\nutilizing deep network architectures, such as FCNs, U-Nets, and ResNets, for\ndeveloping automated skin lesion segmentation. This paper investigates various\npre- and post-processing techniques for improving the performance of U-Nets as\nmeasured by the Jaccard Index. The dataset provided as part of the \"2017 ISBI\nChallenges on Skin Lesion Analysis Towards Melanoma Detection\" was used for\nthis evaluation and the performance of the finalist competitors was the\nstandard for comparison. The pre-processing techniques employed in the proposed\nsystem included contrast enhancement, artifact removal, and vignette\ncorrection. More advanced image transformations, such as local binary patterns\nand wavelet decomposition, were also employed to augment the raw grayscale\nimages used as network input features. While the performance of the proposed\nsystem fell short of the winners of the challenge, it was determined that using\nwavelet decomposition as an early transformation step improved the overall\nperformance of the system over pre- and post-processing steps alone.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 23:23:15 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ross-Howe", "Sara", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1805.05269", "submitter": "Jay Nandy", "authors": "Jay Nandy, Wynne Hsu, Mong Li Lee", "title": "Normal Similarity Network for Generative Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gaussian distributions are commonly used as a key building block in many\ngenerative models. However, their applicability has not been well explored in\ndeep networks. In this paper, we propose a novel deep generative model named as\nNormal Similarity Network (NSN) where the layers are constructed with\nGaussian-style filters. NSN is trained with a layer-wise non-parametric density\nestimation algorithm that iteratively down-samples the training images and\ncaptures the density of the down-sampled training images in the final layer.\nAdditionally, we propose NSN-Gen for generating new samples from noise vectors\nby iteratively reconstructing feature maps in the hidden layers of NSN. Our\nexperiments suggest encouraging results of the proposed model for a wide range\nof computer vision applications including image generation, styling and\nreconstruction from occluded images.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 16:34:20 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Nandy", "Jay", ""], ["Hsu", "Wynne", ""], ["Lee", "Mong Li", ""]]}, {"id": "1805.05308", "submitter": "Deniz Engin", "authors": "Deniz Engin, An{\\i}l Gen\\c{c}, Haz{\\i}m Kemal Ekenel", "title": "Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing", "comments": "Accepted at CVPRW: NTIRE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end network, called Cycle-Dehaze, for\nsingle image dehazing problem, which does not require pairs of hazy and\ncorresponding ground truth images for training. That is, we train the network\nby feeding clean and hazy images in an unpaired manner. Moreover, the proposed\napproach does not rely on estimation of the atmospheric scattering model\nparameters. Our method enhances CycleGAN formulation by combining\ncycle-consistency and perceptual losses in order to improve the quality of\ntextural information recovery and generate visually better haze-free images.\nTypically, deep learning models for dehazing take low resolution images as\ninput and produce low resolution outputs. However, in the NTIRE 2018 challenge\non single image dehazing, high resolution images were provided. Therefore, we\napply bicubic downscaling. After obtaining low-resolution outputs from the\nnetwork, we utilize the Laplacian pyramid to upscale the output images to the\noriginal resolution. We conduct experiments on NYU-Depth, I-HAZE, and O-HAZE\ndatasets. Extensive experiments demonstrate that the proposed approach improves\nCycleGAN method both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 17:34:21 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Engin", "Deniz", ""], ["Gen\u00e7", "An\u0131l", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "1805.05373", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Yeeleng S. Vang, Yufang Huang, Xiaohui Xie", "title": "DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule\n  Detection", "comments": "MICCAI2018 Early Accept, Code\n  https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning has been witnessing widespread adoption in various\nmedical image applications. However, training complex deep neural nets requires\nlarge-scale datasets labeled with ground truth, which are often unavailable in\nmany medical image domains. For instance, to train a deep neural net to detect\npulmonary nodules in lung computed tomography (CT) images, current practice is\nto manually label nodule locations and sizes in many CT images to construct a\nsufficiently large training dataset, which is costly and difficult to scale. On\nthe other hand, electronic medical records (EMR) contain plenty of partial\ninformation on the content of each medical image. In this work, we explore how\nto tap this vast, but currently unexplored data source to improve pulmonary\nnodule detection. We propose DeepEM, a novel deep 3D ConvNet framework\naugmented with expectation-maximization (EM), to mine weakly supervised labels\nin EMRs for pulmonary nodule detection. Experimental results show that DeepEM\ncan lead to 1.5\\% and 3.9\\% average improvement in free-response receiver\noperating characteristic (FROC) scores on LUNA16 and Tianchi datasets,\nrespectively, demonstrating the utility of incomplete information in EMRs for\nimproving deep learning\nalgorithms.\\footnote{https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git}\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 18:31:11 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 04:45:49 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 16:41:34 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Zhu", "Wentao", ""], ["Vang", "Yeeleng S.", ""], ["Huang", "Yufang", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1805.05389", "submitter": "Nakka Krishna Kanth", "authors": "Krishna Kanth Nakka, Mathieu Salzmann", "title": "Deep Attentional Structured Representation Learning for Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured representations, such as Bags of Words, VLAD and Fisher Vectors,\nhave proven highly effective to tackle complex visual recognition tasks. As\nsuch, they have recently been incorporated into deep architectures. However,\nwhile effective, the resulting deep structured representation learning\nstrategies typically aggregate local features from the entire image, ignoring\nthe fact that, in complex recognition tasks, some regions provide much more\ndiscriminative information than others.\n  In this paper, we introduce an attentional structured representation learning\nframework that incorporates an image-specific attention mechanism within the\naggregation process. Our framework learns to predict jointly the image class\nlabel and an attention map in an end-to-end fashion and without any other\nsupervision than the target label. As evidenced by our experiments, this\nconsistently outperforms attention-less structured representation learning and\nyields state-of-the-art results on standard scene recognition and fine-grained\ncategorization benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 19:13:16 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Nakka", "Krishna Kanth", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1805.05392", "submitter": "Caroline Quadros Cordeiro", "authors": "Caroline Q. Cordeiro and Sergio O. Ioshii and Jeovane H. Alves and\n  Lucas F. Oliveira", "title": "An Automatic Patch-based Approach for HER-2 Scoring in\n  Immunohistochemical Breast Cancer Images Using Color Features", "comments": "Accepted for presentation at the Brazilian Symposium of Applied\n  Computing in Health (SBCAS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer (BC) is the most common cancer among women world-wide,\napproximately 20-25% of BCs are HER-2 positive. Analysis of HER-2 is\nfundamental to defining the appropriate therapy for patients with breast\ncancer. Inter-pathologist variability in the test results can affect diagnostic\naccuracy. The present study intends to propose an automatic scoring HER-2\nalgorithm. Based on color features, the technique is fully-automated and avoids\nsegmentation, showing a concordance higher than 90% with a pathologist in the\nexperiments realized.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 19:17:32 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Cordeiro", "Caroline Q.", ""], ["Ioshii", "Sergio O.", ""], ["Alves", "Jeovane H.", ""], ["Oliveira", "Lucas F.", ""]]}, {"id": "1805.05421", "submitter": "Serdar Cakir", "authors": "T. Ceren Deveci and Serdar Cakir and A. Enis Cetin", "title": "Energy Efficient Hadamard Neural Networks", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning has made significant improvements at many image processing\ntasks in recent years, such as image classification, object recognition and\nobject detection. Convolutional neural networks (CNN), which is a popular deep\nlearning architecture designed to process data in multiple array form, show\ngreat success to almost all detection \\& recognition problems and computer\nvision tasks. However, the number of parameters in a CNN is too high such that\nthe computers require more energy and larger memory size. In order to solve\nthis problem, we propose a novel energy efficient model Binary Weight and\nHadamard-transformed Image Network (BWHIN), which is a combination of Binary\nWeight Network (BWN) and Hadamard-transformed Image Network (HIN). It is\nobserved that energy efficiency is achieved with a slight sacrifice at\nclassification accuracy. Among all energy efficient networks, our novel\nensemble model outperforms other energy efficient models.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 20:19:24 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Deveci", "T. Ceren", ""], ["Cakir", "Serdar", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1805.05487", "submitter": "Rudrasis Chakraborty Dr.", "authors": "Rudrasis Chakraborty, Monami Banerjee and Baba C. Vemuri", "title": "A CNN for homogneous Riemannian manifolds with applications to\n  Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are ubiquitous in Machine Learning applications\nfor solving a variety of problems. They however can not be used in their native\nform when the domain of the data is commonly encountered manifolds such as the\nsphere, the special orthogonal group, the Grassmanian, the manifold of\nsymmetric positive definite matrices and others. Most recently, generalization\nof CNNs to data domains such as the 2-sphere has been reported by some research\ngroups, which is referred to as the spherical CNNs (SCNNs). The key property of\nSCNNs distinct from CNNs is that they exhibit the rotational equivariance\nproperty that allows for sharing learned weights within a layer. In this paper,\nwe theoretically generalize the CNNs to Riemannian homogeneous manifolds, that\ninclude but are not limited to the aforementioned example manifolds. Our key\ncontributions in this work are: (i) A theorem stating that linear group\nequivariance systems are fully characterized by correlation of functions on the\ndomain manifold and vice-versa. This is fundamental to the characterization of\nall linear group equivariant systems and parallels the widely used result in\nlinear system theory for vector spaces. (ii) As a corrolary, we prove the\nequivariance of the correlation operation to group actions admitted by the\ninput domains which are Riemannian homogeneous manifolds. (iii) We present the\nfirst end-to-end deep network architecture for classification of diffusion\nmagnetic resonance image (dMRI) scans acquired from a cohort of 44 Parkinson\nDisease patients and 50 control/normal subjects. (iv) A proof of concept\nexperiment involving synthetic data generated on the manifold of symmetric\npositive definite matrices is presented to demonstrate the applicability of our\nnetwork to other types of domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 22:56:46 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 21:03:45 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 20:46:42 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Banerjee", "Monami", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1805.05499", "submitter": "Nachiket Deo", "authors": "Nachiket Deo and Mohan M. Trivedi", "title": "Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver\n  based LSTMs", "comments": "accepted for publication at IV 2018", "journal-ref": null, "doi": "10.1109/IVS.2018.8500493", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To safely and efficiently navigate through complex traffic scenarios,\nautonomous vehicles need to have the ability to predict the future motion of\nsurrounding vehicles. Multiple interacting agents, the multi-modal nature of\ndriver behavior, and the inherent uncertainty involved in the task make motion\nprediction of surrounding vehicles a challenging problem. In this paper, we\npresent an LSTM model for interaction aware motion prediction of surrounding\nvehicles on freeways. Our model assigns confidence values to maneuvers being\nperformed by vehicles and outputs a multi-modal distribution over future motion\nbased on them. We compare our approach with the prior art for vehicle motion\nprediction on the publicly available NGSIM US-101 and I-80 datasets. Our\nresults show an improvement in terms of RMS values of prediction error. We also\npresent an ablative analysis of the components of our proposed model and\nanalyze the predictions made by the model in complex traffic scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 00:10:45 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Deo", "Nachiket", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1805.05503", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Wenqi Ren, Zhe Hu, Ming-Hsuan Yang", "title": "Learning to Deblur Images with Exemplars", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human faces are one interesting object class with numerous applications.\nWhile significant progress has been made in the generic deblurring problem,\nexisting methods are less effective for blurry face images. The success of the\nstate-of-the-art image deblurring algorithms stems mainly from implicit or\nexplicit restoration of salient edges for kernel estimation. However, existing\nmethods are less effective as only few edges can be restored from blurry face\nimages for kernel estimation. In this paper, we address the problem of\ndeblurring face images by exploiting facial structures. We propose a deblurring\nalgorithm based on an exemplar dataset without using coarse-to-fine strategies\nor heuristic edge selections. In addition, we develop a convolutional neural\nnetwork to restore sharp edges from blurry images for deblurring. Extensive\nexperiments against the state-of-the-art methods demonstrate the effectiveness\nof the proposed algorithms for deblurring face images. In addition, we show the\nproposed algorithms can be applied to image deblurring for other object\nclasses.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 00:26:15 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Pan", "Jinshan", ""], ["Ren", "Wenqi", ""], ["Hu", "Zhe", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1805.05510", "submitter": "Wenbin Li", "authors": "Wenbin Li, Jing Huo, Yinghuan Shi, Yang Gao, Lei Wang and Jiebo Luo", "title": "Online Progressive Deep Metric Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning especially deep metric learning has been widely developed for\nlarge-scale image inputs data. However, in many real-world applications, we can\nonly have access to vectorized inputs data. Moreover, on one hand, well-labeled\ndata is usually limited due to the high annotation cost. On the other hand, the\nreal data is commonly streaming data, which requires to be processed online. In\nthese scenarios, the fashionable deep metric learning is not suitable anymore.\nTo this end, we reconsider the traditional shallow online metric learning and\nnewly develop an online progressive deep metric learning (ODML) framework to\nconstruct a metric-algorithm-based deep network. Specifically, we take an\nonline metric learning algorithm as a metric-algorithm-based layer (i.e.,\nmetric layer), followed by a nonlinear layer, and then stack these layers in a\nfashion similar to deep learning. Different from the shallow online metric\nlearning, which can only learn one metric space (feature transformation), the\nproposed ODML is able to learn multiple hierarchical metric spaces.\nFurthermore, in a progressively and nonlinearly learning way, ODML has a\nstronger learning ability than traditional shallow online metric learning in\nthe case of limited available training data. To make the learning process more\nexplainable and theoretically guaranteed, we also provide theoretical analysis.\nThe proposed ODML enjoys several nice properties and can indeed learn a metric\nprogressively and performs better on the benchmark datasets. Extensive\nexperiments with different settings have been conducted to verify these\nproperties of the proposed ODML.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 01:10:18 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 02:35:16 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Li", "Wenbin", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""], ["Wang", "Lei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1805.05532", "submitter": "Byeongho Heo", "authors": "Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi", "title": "Knowledge Distillation with Adversarial Samples Supporting Decision\n  Boundary", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works on knowledge distillation have provided ways to transfer\nthe knowledge of a trained network for improving the learning process of a new\none, but finding a good technique for knowledge distillation is still an open\nproblem. In this paper, we provide a new perspective based on a decision\nboundary, which is one of the most important component of a classifier. The\ngeneralization performance of a classifier is closely related to the adequacy\nof its decision boundary, so a good classifier bears a good decision boundary.\nTherefore, transferring information closely related to the decision boundary\ncan be a good attempt for knowledge distillation. To realize this goal, we\nutilize an adversarial attack to discover samples supporting a decision\nboundary. Based on this idea, to transfer more accurate information about the\ndecision boundary, the proposed algorithm trains a student classifier based on\nthe adversarial samples supporting the decision boundary. Experiments show that\nthe proposed method indeed improves knowledge distillation and achieves the\nstate-of-the-arts performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 02:42:40 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 05:19:06 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 03:53:54 GMT"}, {"version": "v4", "created": "Fri, 14 Dec 2018 15:20:19 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Heo", "Byeongho", ""], ["Lee", "Minsik", ""], ["Yun", "Sangdoo", ""], ["Choi", "Jin Young", ""]]}, {"id": "1805.05551", "submitter": "Lingxi Xie", "authors": "Chenglin Yang, Lingxi Xie, Siyuan Qiao, Alan Yuille", "title": "Knowledge Distillation in Generations: More Tolerant Teachers Educate\n  Better Students", "comments": "9 pages, 2 figures, major changes beyond v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of training a deep neural network in generations. The\nflowchart is that, in order to optimize the target network (student), another\nnetwork (teacher) with the same architecture is first trained, and used to\nprovide part of supervision signals in the next stage. While this strategy\nleads to a higher accuracy, many aspects (e.g., why teacher-student\noptimization helps) still need further explorations.\n  This paper studies this problem from a perspective of controlling the\nstrictness in training the teacher network. Existing approaches mostly used a\nhard distribution (e.g., one-hot vectors) in training, leading to a strict\nteacher which itself has a high accuracy, but we argue that the teacher needs\nto be more tolerant, although this often implies a lower accuracy. The\nimplementation is very easy, with merely an extra loss term added to the\nteacher network, facilitating a few secondary classes to emerge and complement\nto the primary class. Consequently, the teacher provides a milder supervision\nsignal (a less peaked distribution), and makes it possible for the student to\nlearn from inter-class similarity and potentially lower the risk of\nover-fitting. Experiments are performed on standard image classification tasks\n(CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful,\nthe students show a persistent ability growth and eventually achieve higher\nclassification accuracies than other competitors. Model ensemble and transfer\nfeature extraction also verify the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 03:51:03 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 17:37:25 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Yang", "Chenglin", ""], ["Xie", "Lingxi", ""], ["Qiao", "Siyuan", ""], ["Yuille", "Alan", ""]]}, {"id": "1805.05553", "submitter": "Changil Kim", "authors": "Changil Kim, Hijung Valentina Shin, Tae-Hyun Oh, Alexandre Kaspar,\n  Mohamed Elgharib, Wojciech Matusik", "title": "On Learning Associations of Faces and Voices", "comments": "27 pages including the supplementary material; Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the associations between human faces and voices.\nAudiovisual integration, specifically the integration of facial and vocal\ninformation is a well-researched area in neuroscience. It is shown that the\noverlapping information between the two modalities plays a significant role in\nperceptual tasks such as speaker identification. Through an online study on a\nnew dataset we created, we confirm previous findings that people can associate\nunseen faces with corresponding voices and vice versa with greater than chance\naccuracy. We computationally model the overlapping information between faces\nand voices and show that the learned cross-modal representation contains enough\ninformation to identify matching faces and voices with performance similar to\nthat of humans. Our representation exhibits correlations to certain demographic\nattributes and features obtained from either visual or aural modality alone. We\nrelease our dataset of audiovisual recordings and demographic annotations of\npeople reading out short text used in our studies.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 03:59:08 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 02:28:31 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 00:52:57 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kim", "Changil", ""], ["Shin", "Hijung Valentina", ""], ["Oh", "Tae-Hyun", ""], ["Kaspar", "Alexandre", ""], ["Elgharib", "Mohamed", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1805.05563", "submitter": "Yue Wu", "authors": "Yue Wu and Qiang Ji", "title": "Facial Landmark Detection: a Literature Survey", "comments": null, "journal-ref": "International Journal on Computer Vision, 2017", "doi": "10.1007/s11263-018-1097-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The locations of the fiducial facial landmark points around facial components\nand facial contour capture the rigid and non-rigid facial deformations due to\nhead movements and facial expressions. They are hence important for various\nfacial analysis tasks. Many facial landmark detection algorithms have been\ndeveloped to automatically detect those key points over the years, and in this\npaper, we perform an extensive review of them. We classify the facial landmark\ndetection algorithms into three major categories: holistic methods, Constrained\nLocal Model (CLM) methods, and the regression-based methods. They differ in the\nways to utilize the facial appearance and shape information. The holistic\nmethods explicitly build models to represent the global facial appearance and\nshape information. The CLMs explicitly leverage the global shape model but\nbuild the local appearance models. The regression-based methods implicitly\ncapture facial shape and appearance information. For algorithms within each\ncategory, we discuss their underlying theories as well as their differences. We\nalso compare their performances on both controlled and in the wild benchmark\ndatasets, under varying facial expressions, head poses, and occlusion. Based on\nthe evaluations, we point out their respective strengths and weaknesses. There\nis also a separate section to review the latest deep learning-based algorithms.\n  The survey also includes a listing of the benchmark databases and existing\nsoftware. Finally, we identify future research directions, including combining\nmethods in different categories to leverage their respective strengths to solve\nlandmark detection \"in-the-wild\".\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 05:22:16 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Wu", "Yue", ""], ["Ji", "Qiang", ""]]}, {"id": "1805.05569", "submitter": "Ryota Yoshihashi", "authors": "Seiichiro Fukuda, Ryota Yoshihashi, Rei Kawakami, Shaodi You, Makoto\n  Iida, Takeshi Naemura", "title": "Cross-connected Networks for Multi-task Learning of Detection and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning improves generalization performance by sharing knowledge\namong related tasks. Existing models are for task combinations annotated on the\nsame dataset, while there are cases where multiple datasets are available for\neach task. How to utilize knowledge of successful single-task CNNs that are\ntrained on each dataset has been explored less than multi-task learning with a\nsingle dataset. We propose a cross-connected CNN, a new architecture that\nconnects single-task CNNs through convolutional layers, which transfer useful\ninformation for the counterpart. We evaluated our proposed architecture on a\ncombination of detection and segmentation using two datasets. Experiments on\npedestrians show our CNN achieved a higher detection performance compared to\nbaseline CNNs, while maintaining high quality for segmentation. It is the first\nknown attempt to tackle multi-task learning with different training datasets\nbetween detection and segmentation. Experiments with wild birds demonstrate how\nour CNN learns general representations from limited datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 05:32:55 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Fukuda", "Seiichiro", ""], ["Yoshihashi", "Ryota", ""], ["Kawakami", "Rei", ""], ["You", "Shaodi", ""], ["Iida", "Makoto", ""], ["Naemura", "Takeshi", ""]]}, {"id": "1805.05610", "submitter": "Wei Teng", "authors": "Wei Teng and Yu Zhang and Xiaowu Chen and Jia Li and Zhiqiang He", "title": "Image Co-segmentation via Multi-scale Local Shape Transfer", "comments": "An extention of our previous study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image co-segmentation is a challenging task in computer vision that aims to\nsegment all pixels of the objects from a predefined semantic category. In\nreal-world cases, however, common foreground objects often vary greatly in\nappearance, making their global shapes highly inconsistent across images and\ndifficult to be segmented. To address this problem, this paper proposes a novel\nco-segmentation approach that transfers patch-level local object shapes which\nappear more consistent across different images. In our framework, a multi-scale\npatch neighbourhood system is first generated using proposal flow on arbitrary\nimage-pair, which is further refined by Locally Linear Embedding. Based on the\npatch relationships, we propose an efficient algorithm to jointly segment the\nobjects in each image while transferring their local shapes across different\nimages. Extensive experiments demonstrate that the proposed method can robustly\nand effectively segment common objects from an image set. On iCoseg, MSRC and\nCoseg-Rep dataset, the proposed approach performs comparable or better than the\nstate-of-thearts, while on a more challenging benchmark Fashionista dataset,\nour method achieves significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:48:52 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Teng", "Wei", ""], ["Zhang", "Yu", ""], ["Chen", "Xiaowu", ""], ["Li", "Jia", ""], ["He", "Zhiqiang", ""]]}, {"id": "1805.05612", "submitter": "Junwei Zhou", "authors": "Yiyun Pan, Junwei Zhou, Yongsheng Gao, Shengwu Xiong", "title": "Robust Facial Landmark Localization Based on Texture and Pose Correlated\n  Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust facial landmark localization remains a challenging task when faces are\npartially occluded. Recently, the cascaded pose regression has attracted\nincreasing attentions, due to it's superior performance in facial landmark\nlocalization and occlusion detection. However, such an approach is sensitive to\ninitialization, where an improper initialization can severly degrade the\nperformance. In this paper, we propose a Robust Initialization for Cascaded\nPose Regression (RICPR) by providing texture and pose correlated initial shapes\nfor the testing face. By examining the correlation of local binary patterns\nhistograms between the testing face and the training faces, the shapes of the\ntraining faces that are most correlated with the testing face are selected as\nthe texture correlated initialization. To make the initialization more robust\nto various poses, we estimate the rough pose of the testing face according to\nfive fiducial landmarks located by multitask cascaded convolutional networks.\nThen the pose correlated initial shapes are constructed by the mean face's\nshape and the rough testing face pose. Finally, the texture correlated and the\npose correlated initial shapes are joined together as the robust\ninitialization. We evaluate RICPR on the challenging dataset of COFW. The\nexperimental results demonstrate that the proposed scheme achieves better\nperformances than the state-of-the-art methods in facial landmark localization\nand occlusion detection.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:51:45 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Pan", "Yiyun", ""], ["Zhou", "Junwei", ""], ["Gao", "Yongsheng", ""], ["Xiong", "Shengwu", ""]]}, {"id": "1805.05622", "submitter": "Marko Smilevski", "authors": "Marko Smilevski, Ilija Lalkovski, Gjorgji Madjarov", "title": "Stories for Images-in-Sequence by using Visual and Narrative Components", "comments": "12 pages, 4 figures, ICT Innovations 2018", "journal-ref": "ICT Innovations 2018. Engineering and Life Sciences. ICT 2018.\n  Communications in Computer and Information Science, vol 940. Springer, Cham\n  (2018) pp. 148-159", "doi": "10.1007/978-3-030-00825-3_13", "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in AI is focusing towards generating narrative stories about\nvisual scenes. It has the potential to achieve more human-like understanding\nthan just basic description generation of images- in-sequence. In this work, we\npropose a solution for generating stories for images-in-sequence that is based\non the Sequence to Sequence model. As a novelty, our encoder model is composed\nof two separate encoders, one that models the behaviour of the image sequence\nand other that models the sentence-story generated for the previous image in\nthe sequence of images. By using the image sequence encoder we capture the\ntemporal dependencies between the image sequence and the sentence-story and by\nusing the previous sentence-story encoder we achieve a better story flow. Our\nsolution generates long human-like stories that not only describe the visual\ncontext of the image sequence but also contains narrative and evaluative\nlanguage. The obtained results were confirmed by manual human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 08:15:40 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 20:36:51 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2018 23:49:05 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Smilevski", "Marko", ""], ["Lalkovski", "Ilija", ""], ["Madjarov", "Gjorgji", ""]]}, {"id": "1805.05633", "submitter": "Fujin He", "authors": "Xinghao Ding, Zhirui Lin, Fujin He, Yu Wang, Yue Huang", "title": "A Deeply-Recursive Convolutional Network for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of crowd count in images has a wide range of applications such\nas video surveillance, traffic monitoring, public safety and urban planning.\nRecently, the convolutional neural network (CNN) based approaches have been\nshown to be more effective in crowd counting than traditional methods that use\nhandcrafted features. However, the existing CNN-based methods still suffer from\nlarge number of parameters and large storage space, which require high storage\nand computing resources and thus limit the real-world application.\nConsequently, we propose a deeply-recursive network (DR-ResNet) based on ResNet\nblocks for crowd counting. The recursive structure makes the network deeper\nwhile keeping the number of parameters unchanged, which enhances network\ncapability to capture statistical regularities in the context of the crowd.\nBesides, we generate a new dataset from the video-monitoring data of Beijing\nbus station. Experimental results have demonstrated that proposed method\noutperforms most state-of-the-art methods with far less number of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 08:24:53 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Ding", "Xinghao", ""], ["Lin", "Zhirui", ""], ["He", "Fujin", ""], ["Wang", "Yu", ""], ["Huang", "Yue", ""]]}, {"id": "1805.05638", "submitter": "Yixuan He", "authors": "Delu Zeng, Yixuan He, Li Liu, Zhihong Chen, Jiabin Huang, Jie Chen and\n  John Paisley", "title": "Ro-SOS: Metric Expression Network (MEnet) for Robust Salient Object\n  Segmentation", "comments": "This version: 11 pages (12 with reference), 12 figures, 5 table;\n  Version 1: 7 pages,7 figures, 4 tables; The paper for version 1 has been\n  accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI),2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep CNNs have brought significant improvement to image saliency\ndetection, most CNN based models are sensitive to distortion such as\ncompression and noise. In this paper, we propose an end-to-end generic salient\nobject segmentation model called Metric Expression Network (MEnet) to deal with\nsaliency detection with the tolerance of distortion. Within MEnet, a new\ntopological metric space is constructed, whose implicit metric is determined by\nthe deep network. As a result, we manage to group all the pixels in the\nobserved image semantically within this latent space into two regions: a\nsalient region and a non-salient region. With this architecture, all feature\nextractions are carried out at the pixel level, enabling fine granularity of\noutput boundaries of the salient objects. What's more, we try to give a general\nanalysis for the noise robustness of the network in the sense of Lipschitz and\nJacobian literature. Experiments demonstrate that robust salient maps\nfacilitating object segmentation can be generated by the proposed metric. Tests\non several public benchmarks show that MEnet has achieved desirable\nperformance. Furthermore, by direct computation and measuring the robustness,\nthe proposed method outperforms previous CNN-based methods on distorted inputs.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 08:32:42 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 21:45:46 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 20:41:52 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Zeng", "Delu", ""], ["He", "Yixuan", ""], ["Liu", "Li", ""], ["Chen", "Zhihong", ""], ["Huang", "Jiabin", ""], ["Chen", "Jie", ""], ["Paisley", "John", ""]]}, {"id": "1805.05687", "submitter": "Zongting Lyu", "authors": "Zongting Lyu, Yan Yan, and Fei Wu", "title": "Distribution-based Label Space Transformation for Multi-label Learning", "comments": "11 pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label learning problems have manifested themselves in various machine\nlearning applications. The key to successful multi-label learning algorithms\nlies in the exploration of inter-label correlations, which usually incur great\ncomputational cost. Another notable factor in multi-label learning is that the\nlabel vectors are usually extremely sparse, especially when the candidate label\nvocabulary is very large and only a few instances are assigned to each\ncategory. Recently, a label space transformation (LST) framework has been\nproposed targeting these challenges. However, current methods based on LST\nusually suffer from information loss in the label space dimension reduction\nprocess and fail to address the sparsity problem effectively. In this paper, we\npropose a distribution-based label space transformation (DLST) model. By\ndefining the distribution based on the similarity of label vectors, a more\ncomprehensive label structure can be captured. Then, by minimizing\nKL-divergence of two distributions, the information of the original label space\ncan be approximately preserved in the latent space. Consequently, multi-label\nclassifier trained using the dense latent codes yields better performance. The\nleverage of distribution enables DLST to fill out additional information about\nthe label correlations. This endows DLST the capability to handle label set\nsparsity and training data sparsity in multi-label learning problems. With the\noptimal latent code, a kernel logistic regression function is learned for the\nmapping from feature space to the latent space. Then ML-KNN is employed to\nrecover the original label vector from the transformed latent code. Extensive\nexperiments on several benchmark datasets demonstrate that DLST not only\nachieves high classification performance but also is computationally more\nefficient.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 10:29:01 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Lyu", "Zongting", ""], ["Yan", "Yan", ""], ["Wu", "Fei", ""]]}, {"id": "1805.05727", "submitter": "Tae Joon Jun", "authors": "Tae Joon Jun, Dohyeun Kim, Hoang Minh Nguyen, Daeyoung Kim, Youngsub\n  Eom", "title": "2sRanking-CNN: A 2-stage ranking-CNN for diagnosis of glaucoma from\n  fundus images using CAM-extracted ROI as an intermediate input", "comments": "Accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is a disease in which the optic nerve is chronically damaged by the\nelevation of the intra-ocular pressure, resulting in visual field defect.\nTherefore, it is important to monitor and treat suspected patients before they\nare confirmed with glaucoma. In this paper, we propose a 2-stage ranking-CNN\nthat classifies fundus images as normal, suspicious, and glaucoma. Furthermore,\nwe propose a method of using the class activation map as a mask filter and\ncombining it with the original fundus image as an intermediate input. Our\nresults have improved the average accuracy by about 10% over the existing\n3-class CNN and ranking-CNN, and especially improved the sensitivity of\nsuspicious class by more than 20% over 3-class CNN. In addition, the extracted\nROI was also found to overlap with the diagnostic criteria of the physician.\nThe method we propose is expected to be efficiently applied to any medical data\nwhere there is a suspicious condition between normal and disease.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 12:27:00 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 05:56:39 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Jun", "Tae Joon", ""], ["Kim", "Dohyeun", ""], ["Nguyen", "Hoang Minh", ""], ["Kim", "Daeyoung", ""], ["Eom", "Youngsub", ""]]}, {"id": "1805.05732", "submitter": "Adel Hafiane", "authors": "Mohammad Alkhatib and Adel Hafiane", "title": "Robust Adaptive Median Binary Pattern for noisy texture classification\n  and retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2916742", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is an important cue for different computer vision tasks and\napplications. Local Binary Pattern (LBP) is considered one of the best yet\nefficient texture descriptors. However, LBP has some notable limitations,\nmostly the sensitivity to noise. In this paper, we address these criteria by\nintroducing a novel texture descriptor, Robust Adaptive Median Binary Pattern\n(RAMBP). RAMBP based on classification process of noisy pixels, adaptive\nanalysis window, scale analysis and image regions median comparison. The\nproposed method handles images with high noisy textures, and increases the\ndiscriminative properties by capturing microstructure and macrostructure\ntexture information. The proposed method has been evaluated on popular texture\ndatasets for classification and retrieval tasks, and under different high noise\nconditions. Without any train or prior knowledge of noise type, RAMBP achieved\nthe best classification compared to state-of-the-art techniques. It scored more\nthan $90\\%$ under $50\\%$ impulse noise densities, more than $95\\%$ under\nGaussian noised textures with standard deviation $\\sigma = 5$, and more than\n$99\\%$ under Gaussian blurred textures with standard deviation $\\sigma = 1.25$.\nThe proposed method yielded competitive results and high performance as one of\nthe best descriptors in noise-free texture classification. Furthermore, RAMBP\nshowed also high performance for the problem of noisy texture retrieval\nproviding high scores of recall and precision measures for textures with high\nlevels of noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 12:41:48 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Alkhatib", "Mohammad", ""], ["Hafiane", "Adel", ""]]}, {"id": "1805.05760", "submitter": "Jonas Prellberg", "authors": "Jonas Prellberg, Oliver Kramer", "title": "Multi-label Classification of Surgical Tools with Convolutional Neural\n  Networks", "comments": "Accepted at IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic tool detection from surgical imagery has a multitude of useful\napplications, such as real-time computer assistance for the surgeon. Using the\nsuccessful residual network architecture, a system that can distinguish 21\ndifferent tools in cataract surgery videos is created. The videos are provided\nas part of the 2017 CATARACTS challenge and pose difficulties found in many\nreal-world datasets, for example a strong class imbalance. The construction of\nthe detection system is guided by a wide array of experiments that explore\ndifferent design decisions.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 13:39:54 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Prellberg", "Jonas", ""], ["Kramer", "Oliver", ""]]}, {"id": "1805.05809", "submitter": "Hyun Oh Song", "authors": "Yeonwoo Jeong, Hyun Oh Song", "title": "Efficient end-to-end learning for quantizable representations", "comments": "Accepted and to appear at ICML 2018. Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding representation learning via neural networks is at the core\nfoundation of modern similarity based search. While much effort has been put in\ndeveloping algorithms for learning binary hamming code representations for\nsearch efficiency, this still requires a linear scan of the entire dataset per\neach query and trades off the search accuracy through binarization. To this\nend, we consider the problem of directly learning a quantizable embedding\nrepresentation and the sparse binary hash code end-to-end which can be used to\nconstruct an efficient hash table not only providing significant search\nreduction in the number of data but also achieving the state of the art search\naccuracy outperforming previous state of the art deep metric learning methods.\nWe also show that finding the optimal sparse binary hash code in a mini-batch\ncan be computed exactly in polynomial time by solving a minimum cost flow\nproblem. Our results on Cifar-100 and on ImageNet datasets show the state of\nthe art search accuracy in precision@k and NMI metrics while providing up to\n98X and 478X search speedup respectively over exhaustive linear search. The\nsource code is available at\nhttps://github.com/maestrojeong/Deep-Hash-Table-ICML18\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 14:32:31 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 01:22:14 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 03:11:59 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Jeong", "Yeonwoo", ""], ["Song", "Hyun Oh", ""]]}, {"id": "1805.05838", "submitter": "Tribhuvanesh Orekondy", "authors": "Tribhuvanesh Orekondy, Seong Joon Oh, Yang Zhang, Bernt Schiele, Mario\n  Fritz", "title": "Gradient-Leaks: Understanding and Controlling Deanonymization in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated Learning (FL) systems are gaining popularity as a solution to\ntraining Machine Learning (ML) models from large-scale user data collected on\npersonal devices (e.g., smartphones) without their raw data leaving the device.\nAt the core of FL is a network of anonymous user devices sharing training\ninformation (model parameter updates) computed locally on personal data.\nHowever, the type and degree to which user-specific information is encoded in\nthe model updates is poorly understood. In this paper, we identify model\nupdates encode subtle variations in which users capture and generate data. The\nvariations provide a strong statistical signal, allowing an adversary to\neffectively deanonymize participating devices using a limited set of auxiliary\ndata. We analyze resulting deanonymization attacks on diverse tasks on\nreal-world (anonymized) user-generated data across a range of closed- and\nopen-world scenarios. We study various strategies to mitigate the risks of\ndeanonymization. As random perturbation methods do not offer convincing\noperating points, we propose data-augmentation strategies which introduces\nadversarial biases in device data and thereby, offer substantial protection\nagainst deanonymization threats with little effect on utility.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 15:12:45 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 12:02:07 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 15:56:36 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Orekondy", "Tribhuvanesh", ""], ["Oh", "Seong Joon", ""], ["Zhang", "Yang", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1805.05974", "submitter": "Nafis Neehal", "authors": "Md. Harun-Ur-Rashid, Shekina Khatun, Mehe Zabin Trisha, Nafis Neehal,\n  Md. Zahid Hasan", "title": "Crick-net: A Convolutional Neural Network based Classification Approach\n  for Detecting Waist High No Balls in Cricket", "comments": "Working on a different trajectory with this dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cricket is undoubtedly one of the most popular games in this modern era. As\nhuman beings are prone to error, there remains a constant need for automated\nanalysis and decision making of different events in this game. Simultaneously,\nwith advent and advances in Artificial Intelligence and Computer Vision,\napplication of these two in different domains has become an emerging trend.\nApplying several computer vision techniques in analyzing different Cricket\nevents and automatically coming into decisions has become popular in recent\ndays. In this paper, we have deployed a CNN based classification method with\nInception V3 in order to automatically detect and differentiate waist high no\nballs with fair balls. Our approach achieves an overall average accuracy of 88%\nwith a fairly low cross-entropy value.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 18:15:26 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 08:32:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Harun-Ur-Rashid", "Md.", ""], ["Khatun", "Shekina", ""], ["Trisha", "Mehe Zabin", ""], ["Neehal", "Nafis", ""], ["Hasan", "Md. Zahid", ""]]}, {"id": "1805.06038", "submitter": "Darryl D. Holm", "authors": "Alexis Arnaudon, Darryl Holm, Stefan Sommer", "title": "String Methods for Stochastic Image and Shape Matching", "comments": null, "journal-ref": "J Math Imaging Vis (2018) 60: 953--967", "doi": "10.1007/s10851-018-0823-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching of images and analysis of shape differences is traditionally pursued\nby energy minimization of paths of deformations acting to match the shape\nobjects. In the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework, iterative gradient descents on the matching functional lead to\nmatching algorithms informally known as Beg algorithms. When stochasticity is\nintroduced to model stochastic variability of shapes and to provide more\nrealistic models of observed shape data, the corresponding matching problem can\nbe solved with a stochastic Beg algorithm, similar to the finite temperature\nstring method used in rare event sampling. In this paper, we apply a stochastic\nmodel compatible with the geometry of the LDDMM framework to obtain a\nstochastic model of images and we derive the stochastic version of the Beg\nalgorithm which we compare with the string method and an\nexpectation-maximization optimization of posterior likelihoods. The algorithm\nand its use for statistical inference is tested on stochastic LDDMM landmarks\nand images.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 21:26:35 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 19:22:38 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 19:28:15 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Arnaudon", "Alexis", ""], ["Holm", "Darryl", ""], ["Sommer", "Stefan", ""]]}, {"id": "1805.06041", "submitter": "Yasutaka Narazaki", "authors": "Yasutaka Narazaki, Vedhus Hoskere, Tu A. Hoang, and Billie F. Spencer", "title": "Vision-based Automated Bridge Component Recognition Integrated With\n  High-level Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image data has a great potential of helping conventional visual inspections\nof civil engineering structures due to the ease of data acquisition and the\nadvantages in capturing visual information. A variety of techniques have been\nproposed to detect damages, such as cracks and spalling on a close-up image of\na single component (columns and road surfaces etc.). However, these techniques\ncommonly suffer from severe false-positives especially when the image includes\nmultiple components of different structures. To reduce the false-positives and\nextract reliable information about the structures' conditions, detection and\nlocalization of critical structural components are important first steps\npreceding the damage assessment. This study aims at recognizing bridge\nstructural and non-structural components from images of urban scenes. During\nthe bridge component recognition, every image pixel is classified into one of\nthe five classes (non-bridge, columns, beams and slabs, other structural, other\nnonstructural) by multi-scale convolutional neural networks (multi-scale CNNs).\nTo reduce false-positives and get consistent labels, the component\nclassifications are integrated with scene understanding by an additional\nclassifier with 10 higher-level scene classes (building, greenery, person,\npavement, signs and poles, vehicles, bridges, water, sky, and others). The\nbridge component recognition integrated with the scene understanding is\ncompared with the naive approach without scene classification in terms of\naccuracy, false-positives and consistencies to demonstrate the effectiveness of\nthe integrated approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 21:37:47 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Narazaki", "Yasutaka", ""], ["Hoskere", "Vedhus", ""], ["Hoang", "Tu A.", ""], ["Spencer", "Billie F.", ""]]}, {"id": "1805.06042", "submitter": "Yasutaka Narazaki", "authors": "Yasutaka Narazaki, Vedhus Hoskere, Tu A. Hoang, and Billie F. Spencer\n  Jr", "title": "Automated Vision-based Bridge Component Extraction Using Multiscale\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image data has a great potential of helping post-earthquake visual\ninspections of civil engineering structures due to the ease of data acquisition\nand the advantages in capturing visual information. A variety of techniques\nhave been applied to detect damages automatically from a close-up image of a\nstructural component. However, the application of the automatic damage\ndetection methods become increasingly difficult when the image includes\nmultiple components from different structures. To reduce the inaccurate false\npositive alarms, critical structural components need to be recognized first,\nand the damage alarms need to be cleaned using the component recognition\nresults. To achieve the goal, this study aims at recognizing and extracting\nbridge components from images of urban scenes. The bridge component recognition\nbegins with pixel-wise classifications of an image into 10 scene classes. Then,\nthe original image and the scene classification results are combined to\nclassify the image pixels into five component classes. The multi-scale\nconvolutional neural networks (multi-scale CNNs) are used to perform pixel-wise\nclassification, and the classification results are post-processed by averaging\nwithin superpixels and smoothing by conditional random fields (CRFs). The\nperformance of the bridge component extraction is tested in terms of accuracy\nand consistency.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 21:40:34 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Narazaki", "Yasutaka", ""], ["Hoskere", "Vedhus", ""], ["Hoang", "Tu A.", ""], ["Spencer", "Billie F.", "Jr"]]}, {"id": "1805.06066", "submitter": "Alexander Toshev", "authors": "Arsalan Mousavian, Alexander Toshev, Marek Fiser, Jana Kosecka, Ayzaan\n  Wahid, James Davidson", "title": "Visual Representations for Semantic Target Driven Navigation", "comments": "Accepted to ICRA 2019 and ECCV 2018 Workshop on Visual Learning and\n  Embodied Agents in Simulation Environments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is a good visual representation for autonomous agents? We address this\nquestion in the context of semantic visual navigation, which is the problem of\na robot finding its way through a complex environment to a target object, e.g.\ngo to the refrigerator. Instead of acquiring a metric semantic map of an\nenvironment and using planning for navigation, our approach learns navigation\npolicies on top of representations that capture spatial layout and semantic\ncontextual cues. We propose to using high level semantic and contextual\nfeatures including segmentation and detection masks obtained by off-the-shelf\nstate-of-the-art vision as observations and use deep network to learn the\nnavigation policy. This choice allows using additional data, from orthogonal\nsources, to better train different parts of the model the representation\nextraction is trained on large standard vision datasets while the navigation\ncomponent leverages large synthetic environments for training. This combination\nof real and synthetic is possible because equitable feature representations are\navailable in both (e.g., segmentation and detection masks), which alleviates\nthe need for domain adaptation. Both the representation and the navigation\npolicy can be readily applied to real non-synthetic environments as\ndemonstrated on the Active Vision Dataset [1]. Our approach gets successfully\nto the target in 54% of the cases in unexplored environments, compared to 46%\nfor non-learning based approach, and 28% for the learning-based baseline.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 23:26:52 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 00:49:41 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 18:05:37 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Mousavian", "Arsalan", ""], ["Toshev", "Alexander", ""], ["Fiser", "Marek", ""], ["Kosecka", "Jana", ""], ["Wahid", "Ayzaan", ""], ["Davidson", "James", ""]]}, {"id": "1805.06082", "submitter": "Grant Fennessy", "authors": "Grant Fennessy, Yevgeniy Vorobeychik", "title": "Optical Neural Networks", "comments": "Submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel optical neural network (ONN) framework which introduces a\ndegree of scalar invariance to image classification estima- tion. Taking a hint\nfrom the human eye, which has higher resolution near the center of the retina,\nimages are broken out into multiple levels of varying zoom based on a focal\npoint. Each level is passed through an identical convolutional neural network\n(CNN) in a Siamese fashion, and the results are recombined to produce a high\naccuracy estimate of the object class. ONNs act as a wrapper around existing\nCNNs, and can thus be applied to many existing algorithms to produce notable\naccuracy improvements without having to change the underlying architecture.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 01:12:11 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 12:36:27 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Fennessy", "Grant", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1805.06085", "submitter": "Jungwook Choi", "authors": "Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,\n  Vijayalakshmi Srinivasan, Kailash Gopalakrishnan", "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning algorithms achieve high classification accuracy at the expense\nof significant computation cost. To address this cost, a number of quantization\nschemes have been proposed - but most of these techniques focused on quantizing\nweights, which are relatively smaller in size compared to activations. This\npaper proposes a novel quantization scheme for activations during training -\nthat enables neural networks to work well with ultra low precision weights and\nactivations without any significant accuracy degradation. This technique,\nPArameterized Clipping acTivation (PACT), uses an activation clipping parameter\n$\\alpha$ that is optimized during training to find the right quantization\nscale. PACT allows quantizing activations to arbitrary bit precisions, while\nachieving much better accuracy relative to published state-of-the-art\nquantization schemes. We show, for the first time, that both weights and\nactivations can be quantized to 4-bits of precision while still achieving\naccuracy comparable to full precision networks across a range of popular models\nand datasets. We also show that exploiting these reduced-precision\ncomputational units in hardware can enable a super-linear improvement in\ninferencing performance due to a significant reduction in the area of\naccelerator compute engines coupled with the ability to retain the quantized\nmodel and activation data in on-chip memories.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 01:19:43 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 07:33:19 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Choi", "Jungwook", ""], ["Wang", "Zhuo", ""], ["Venkataramani", "Swagath", ""], ["Chuang", "Pierce I-Jen", ""], ["Srinivasan", "Vijayalakshmi", ""], ["Gopalakrishnan", "Kailash", ""]]}, {"id": "1805.06086", "submitter": "Parthipan Siva", "authors": "Paul Marchwica, Michael Jamieson, Parthipan Siva", "title": "An Evaluation of Deep CNN Baselines for Scene-Independent Person\n  Re-Identification", "comments": "To be published in 2018 15th Conference on Computer and Robot Vision\n  (CRV)", "journal-ref": null, "doi": "10.1109/CRV.2018.00049", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a variety of proposed methods based on deep convolutional\nneural networks (CNNs) have improved the state of the art for large-scale\nperson re-identification (ReID). While a large number of optimizations and\nnetwork improvements have been proposed, there has been relatively little\nevaluation of the influence of training data and baseline network architecture.\nIn particular, it is usually assumed either that networks are trained on\nlabeled data from the deployment location (scene-dependent), or else adapted\nwith unlabeled data, both of which complicate system deployment. In this paper,\nwe investigate the feasibility of achieving scene-independent person ReID by\nforming a large composite dataset for training. We present an in-depth\ncomparison of several CNN baseline architectures for both scene-dependent and\nscene-independent ReID, across a range of training dataset sizes. We show that\nscene-independent ReID can produce leading-edge results, competitive with\nunsupervised domain adaption techniques. Finally, we introduce a new dataset\nfor comparing within-camera and across-camera person ReID.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 01:23:56 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Marchwica", "Paul", ""], ["Jamieson", "Michael", ""], ["Siva", "Parthipan", ""]]}, {"id": "1805.06115", "submitter": "Di Kang", "authors": "Di Kang and Antoni Chan", "title": "Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid", "comments": "camera ready, accepted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the powerful learning capability of deep neural networks, counting\nperformance via density map estimation has improved significantly during the\npast several years. However, it is still very challenging due to severe\nocclusion, large scale variations, and perspective distortion. Scale variations\n(from image to image) coupled with perspective distortion (within one image)\nresult in huge scale changes of the object size. Earlier methods based on\nconvolutional neural networks (CNN) typically did not handle this scale\nvariation explicitly, until Hydra-CNN and MCNN. MCNN uses three columns, each\nwith different filter sizes, to extract features at different scales. In this\npaper, in contrast to using filters of different sizes, we utilize an image\npyramid to deal with scale variations. It is more effective and efficient to\nresize the input fed into the network, as compared to using larger filter\nsizes. Secondly, we adaptively fuse the predictions from different scales\n(using adaptively changing per-pixel weights), which makes our method adapt to\nscale changes within an image. The adaptive fusing is achieved by generating an\nacross-scale attention map, which softly selects a suitable scale for each\npixel, followed by a 1x1 convolution. Extensive experiments on three popular\ndatasets show very compelling results.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 03:27:02 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 07:57:59 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Kang", "Di", ""], ["Chan", "Antoni", ""]]}, {"id": "1805.06118", "submitter": "Guodong Ding", "authors": "Guodong Ding, Shanshan Zhang, Salman Khan, Zhenmin Tang, Jian Zhang\n  and Fatih Porikli", "title": "Feature Affinity based Pseudo Labeling for Semi-supervised Person\n  Re-identification", "comments": "12 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to match a person's identity across multiple\ncamera streams. Deep neural networks have been successfully applied to the\nchallenging person re-identification task. One remarkable bottleneck is that\nthe existing deep models are data hungry and require large amounts of labeled\ntraining data. Acquiring manual annotations for pedestrian identity matchings\nin large-scale surveillance camera installations is a highly cumbersome task.\nHere, we propose the first semi-supervised approach that performs\npseudo-labeling by considering complex relationships between unlabeled and\nlabeled training samples in the feature space. Our approach first approximates\nthe actual data manifold by learning a generative model via adversarial\ntraining. Given the trained model, data augmentation can be performed by\ngenerating new synthetic data samples which are unlabeled. An open research\nproblem is how to effectively use this additional data for improved feature\nlearning. To this end, this work proposes a novel Feature Affinity based\nPseudo-Labeling (FAPL) approach with two possible label encodings under a\nunified setting. Our approach measures the affinity of unlabeled samples with\nthe underlying clusters of labeled data samples using the intermediate feature\nrepresentations from deep networks. FAPL trains with the joint supervision of\ncross-entropy loss together with a center regularization term, which not only\nensures discriminative feature representation learning but also simultaneously\npredicts pseudo-labels for unlabeled data. Our extensive experiments on two\nstandard large-scale datasets, Market-1501 and DukeMTMC-reID, demonstrate\nsignificant performance boosts over closely related competitors and outperforms\nstate-of-the-art person re-identification techniques in most cases.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 03:42:36 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Ding", "Guodong", ""], ["Zhang", "Shanshan", ""], ["Khan", "Salman", ""], ["Tang", "Zhenmin", ""], ["Zhang", "Jian", ""], ["Porikli", "Fatih", ""]]}, {"id": "1805.06140", "submitter": "Prasan A Shedligeri Mr.", "authors": "Prasan A Shedligeri, Kaushik Mitra", "title": "Photorealistic Image Reconstruction from Hybrid Intensity and Event\n  based Sensor", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": "10.1117/1.JEI.28.6.063012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sensors output a stream of asynchronous brightness changes (called\n``events'') at a very high temporal rate. Previous works on recovering the lost\nintensity information from the event sensor data have heavily relied on the\nevent stream, which makes the reconstructed images non-photorealistic and also\nsusceptible to noise in the event stream. We propose to reconstruct\nphotorealistic intensity images from a hybrid sensor consisting of a low frame\nrate conventional camera, which has the scene texture information, along with\nthe event sensor. To accomplish our task, we warp the low frame rate intensity\nimages to temporally dense locations of the event data by estimating a\nspatially dense scene depth and temporally dense sensor ego-motion. The results\nobtained from our algorithm are more photorealistic compared to any of the\nprevious state-of-the-art algorithms. We also demonstrate our algorithm's\nrobustness to abrupt camera motion and noise in the event sensor data.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 05:49:25 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 05:37:20 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 03:22:37 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 07:30:42 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Shedligeri", "Prasan A", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1805.06148", "submitter": "Charmin Asirimath Pingamage Don", "authors": "Charmin Asirimath, Jayampathy Ratnayake, Chathuranga Weeraddana", "title": "Critical Points to Determine Persistence Homology", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of the simplicial complexes of a large point cloud often relies\non extracting a sample, to reduce the associated computational burden. The\nstudy considers sampling critical points of a Morse function associated to a\npoint cloud, to approximate the Vietoris-Rips complex or the witness complex\nand compute persistence homology. The effectiveness of the novel approach is\ncompared with the farthest point sampling, in a context of classifying human\nface images into ethnics groups using persistence homology.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:25:21 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Asirimath", "Charmin", ""], ["Ratnayake", "Jayampathy", ""], ["Weeraddana", "Chathuranga", ""]]}, {"id": "1805.06157", "submitter": "Ramazan Gokberk Cinbis", "authors": "Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis", "title": "Zero-Shot Object Detection by Hybrid Region Embedding", "comments": null, "journal-ref": "Published in British Machine Vision Conference 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is considered as one of the most challenging problems in\ncomputer vision, since it requires correct prediction of both classes and\nlocations of objects in images. In this study, we define a more difficult\nscenario, namely zero-shot object detection (ZSD) where no visual training data\nis available for some of the target object classes. We present a novel approach\nto tackle this ZSD problem, where a convex combination of embeddings are used\nin conjunction with a detection framework. For evaluation of ZSD methods, we\npropose a simple dataset constructed from Fashion-MNIST images and also a\ncustom zero-shot split for the Pascal VOC detection challenge. The experimental\nresults suggest that our method yields promising results for ZSD.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:01:27 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 07:59:37 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Demirel", "Berkan", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Ikizler-Cinbis", "Nazli", ""]]}, {"id": "1805.06173", "submitter": "Xueyang Fu", "authors": "Xueyang Fu, Borong Liang, Yue Huang, Xinghao Ding, John Paisley", "title": "Lightweight Pyramid Networks for Image Deraining", "comments": "submitted to IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep convolutional neural networks have found major success in image\nderaining, but at the expense of an enormous number of parameters. This limits\ntheir potential application, for example in mobile devices. In this paper, we\npropose a lightweight pyramid of networks (LPNet) for single image deraining.\nInstead of designing a complex network structures, we use domain-specific\nknowledge to simplify the learning process. Specifically, we find that by\nintroducing the mature Gaussian-Laplacian image pyramid decomposition\ntechnology to the neural network, the learning problem at each pyramid level is\ngreatly simplified and can be handled by a relatively shallow network with few\nparameters. We adopt recursive and residual network structures to build the\nproposed LPNet, which has less than 8K parameters while still achieving\nstate-of-the-art performance on rain removal. We also discuss the potential\nvalue of LPNet for other low- and high-level vision tasks.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:48:20 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Fu", "Xueyang", ""], ["Liang", "Borong", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Paisley", "John", ""]]}, {"id": "1805.06184", "submitter": "Dacheng Tao", "authors": "Xikun Zhang, Chang Xu, Xinmei Tian, Dacheng Tao", "title": "Graph Edge Convolutional Neural Networks for Skeleton Based Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates body bones from skeleton data for skeleton based\naction recognition. Body joints, as the direct result of mature pose estimation\ntechnologies, are always the key concerns of traditional action recognition\nmethods. However, instead of joints, we humans naturally identify how the human\nbody moves according to shapes, lengths and places of bones, which are more\nobvious and stable for observation. Hence given graphs generated from skeleton\ndata, we propose to develop convolutions over graph edges that correspond to\nbones in human skeleton. We describe an edge by integrating its spatial\nneighboring edges to explore the cooperation between different bones, as well\nas its temporal neighboring edges to address the consistency of movements in an\naction. A graph edge convolutional neural network is then designed for skeleton\nbased action recognition. Considering the complementarity between graph node\nconvolution and graph edge convolution, we additionally construct two hybrid\nneural networks to combine graph node convolutional neural network and graph\nedge convolutional neural network using shared intermediate layers.\nExperimental results on Kinetics and NTU-RGB+D datasets demonstrate that our\ngraph edge convolution is effective to capture characteristic of actions and\nour graph edge convolutional neural network significantly outperforms existing\nstate-of-art skeleton based action recognition methods. Additionally, more\nperformance improvements can be achieved by the hybrid networks.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 08:18:33 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 01:42:04 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Zhang", "Xikun", ""], ["Xu", "Chang", ""], ["Tian", "Xinmei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1805.06207", "submitter": "Luca Morreale", "authors": "Luca Morreale, Andrea Romanoni and Matteo Matteucci", "title": "Predicting the Next Best View for 3D Mesh Refinement", "comments": "13 pages, 5 figures, to be published in IAS-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction is a core task in many applications such as robot\nnavigation or sites inspections. Finding the best poses to capture part of the\nscene is one of the most challenging topic that goes under the name of Next\nBest View. Recently, many volumetric methods have been proposed; they choose\nthe Next Best View by reasoning over a 3D voxelized space and by finding which\npose minimizes the uncertainty decoded into the voxels. Such methods are\neffective, but they do not scale well since the underlaying representation\nrequires a huge amount of memory. In this paper we propose a novel mesh-based\napproach which focuses on the worst reconstructed region of the environment\nmesh. We define a photo-consistent index to evaluate the 3D mesh accuracy, and\nan energy function over the worst regions of the mesh which takes into account\nthe mutual parallax with respect to the previous cameras, the angle of\nincidence of the viewing ray to the surface and the visibility of the region.\nWe test our approach over a well known dataset and achieve state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 09:31:24 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Morreale", "Luca", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1805.06223", "submitter": "Nils Gessert", "authors": "Nils Gessert, Markus Heyder, Sarah Latus, David M. Leistner, Youssef\n  S. Abdelwahed, Matthias Lutz, Alexander Schlaefer", "title": "Adversarial Training for Patient-Independent Feature Learning with IVOCT\n  Data for Plaque Classification", "comments": "Presented at MIDL 2018 Conference\n  https://openreview.net/forum?id=SJWY1Ujsz", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown impressive results for a variety of medical\nproblems over the last few years. However, datasets tend to be small due to\ntime-consuming annotation. As datasets with different patients are often very\nheterogeneous generalization to new patients can be difficult. This is\ncomplicated further if large differences in image acquisition can occur, which\nis common during intravascular optical coherence tomography for coronary plaque\nimaging. We address this problem with an adversarial training strategy where we\nforce a part of a deep neural network to learn features that are independent of\npatient- or acquisitionspecific characteristics. We compare our regularization\nmethod to typical data augmentation strategies and show that our approach\nimproves performance for a small medical dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 10:08:15 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Gessert", "Nils", ""], ["Heyder", "Markus", ""], ["Latus", "Sarah", ""], ["Leistner", "David M.", ""], ["Abdelwahed", "Youssef S.", ""], ["Lutz", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1805.06260", "submitter": "Yijie Dang", "authors": "Yijie Dang (1, 2 and 3), Nan Jiang (1, 2 and 3), Hao Hu (1, 2 and 3),\n  Zhuoxiao Ji (1, 2 and 3) and Wenyin Zhang (4) ((1) Faculty of Information\n  Technology, Beijing University of Technology, Beijing, China (2) Beijing Key\n  Laboratory of Trusted Computing, Beijing, China (3) National Engineering\n  Laboratory for Critical Technologies of Information Security Classified\n  Protection, Beijing, China (4) School of Information Science and Technology,\n  Linyi University, Linyi, China)", "title": "Image Classification Based on Quantum KNN Algorithm", "comments": "19 pages, 9 Postscript figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is an important task in the field of machine learning\nand image processing. However, the usually used classification method --- the K\nNearest-Neighbor algorithm has high complexity, because its two main processes:\nsimilarity computing and searching are time-consuming. Especially in the era of\nbig data, the problem is prominent when the amount of images to be classified\nis large. In this paper, we try to use the powerful parallel computing ability\nof quantum computers to optimize the efficiency of image classification. The\nscheme is based on quantum K Nearest-Neighbor algorithm. Firstly, the feature\nvectors of images are extracted on classical computers. Then the feature\nvectors are inputted into a quantum superposition state, which is used to\nachieve parallel computing of similarity. Next, the quantum minimum search\nalgorithm is used to speed up searching process for similarity. Finally, the\nimage is classified by quantum measurement. The complexity of the quantum\nalgorithm is only O((kM)^(1/2)), which is superior to the classical algorithms.\nMoreover, the measurement step is executed only once to ensure the validity of\nthe scheme. The experimental results show that, the classification accuracy is\n83.1% on Graz-01 dataset and 78% on Caltech-101 dataset, which is close to\nexisting classical algorithms. Hence, our quantum scheme has a good\nclassification performance while greatly improving the efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 11:59:54 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Dang", "Yijie", "", "1, 2 and 3"], ["Jiang", "Nan", "", "1, 2 and 3"], ["Hu", "Hao", "", "1, 2 and 3"], ["Ji", "Zhuoxiao", "", "1, 2 and 3"], ["Zhang", "Wenyin", ""]]}, {"id": "1805.06298", "submitter": "Hidetoshi Furukawa", "authors": "Hidetoshi Furukawa", "title": "SAVERS: SAR ATR with Verification Support Based on Convolutional Neural\n  Network", "comments": "Technical Report, 6 pages, 8 figures, 5 tables, Copyright(C)2018\n  IEICE. arXiv admin note: substantial text overlap with arXiv:1801.08558", "journal-ref": "IEICE Technical Report, vol.118, no.28, SANE2018-5, pp.23-28, May\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new convolutional neural network (CNN) which performs coarse and\nfine segmentation for end-to-end synthetic aperture radar (SAR) automatic\ntarget recognition (ATR) system. In recent years, many CNNs for SAR ATR using\ndeep learning have been proposed, but most of them classify target classes from\nfixed size target chips extracted from SAR imagery. On the other hand, we\nproposed the CNN which outputs the score of the multiple target classes and a\nbackground class for each pixel from the SAR imagery of arbitrary size and\nmultiple targets as fine segmentation. However, it was necessary for humans to\njudge the CNN segmentation result. In this report, we propose a CNN called SAR\nATR with verification support (SAVERS), which performs region-wise (i.e.\ncoarse) segmentation and pixel-wise segmentation. SAVERS discriminates between\ntarget and non-target, and classifies multiple target classes and non-target\nclass by coarse segmentation. This report describes the evaluation results of\nSAVERS using the Moving and Stationary Target Acquisition and Recognition\n(MSTAR) dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 18:03:35 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Furukawa", "Hidetoshi", ""]]}, {"id": "1805.06306", "submitter": "Lingfeng Zhang", "authors": "Lingfeng Zhang, Ioannis A. Kakadiaris", "title": "Fully Associative Patch-based 1-to-N Matcher for Face Recognition", "comments": "Accepted in ICB2018. arXiv admin note: text overlap with\n  arXiv:1804.01417, arXiv:1805.02339 and substantial text overlap with\n  arXiv:1803.09359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on improving face recognition performance by a patch-based\n1-to-N signature matcher that learns correlations between different facial\npatches. A Fully Associative Patch-based Signature Matcher (FAPSM) is proposed\nso that the local matching identity of each patch contributes to the global\nmatching identities of all the patches. The proposed matcher consists of three\nsteps. First, based on the signature, the local matching identity and the\ncorresponding matching score of each patch are computed. Then, a fully\nassociative weight matrix is learned to obtain the global matching identities\nand scores of all the patches. At last, the l1-regularized weighting is applied\nto combine the global matching identity of each patch and obtain a final\nmatching identity. The proposed matcher has been integrated with the UR2D\nsystem for evaluation. The experimental results indicate that the proposed\nmatcher achieves better performance than the current UR2D system. The Rank-1\naccuracy is improved significantly by 3% and 0.55% on the UHDB31 dataset and\nthe IJB-A dataset, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 01:42:46 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Zhang", "Lingfeng", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1805.06323", "submitter": "Heng Fan", "authors": "Qin Zhou, Heng Fan, Hua Yang, Hang Su, Shibao Zheng, Shuang Wu, Haibin\n  Ling", "title": "Robust and Efficient Graph Correspondence Transfer for Person\n  Re-identification", "comments": "Tech. Report. The source code is available at\n  http://www.dabi.temple.edu/~hbling/code/gct.htm. arXiv admin note: text\n  overlap with arXiv:1804.00242", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial misalignment caused by variations in poses and viewpoints is one of\nthe most critical issues that hinders the performance improvement in existing\nperson re-identification (Re-ID) algorithms. To address this problem, in this\npaper, we present a robust and efficient graph correspondence transfer (REGCT)\napproach for explicit spatial alignment in Re-ID. Specifically, we propose to\nestablish the patch-wise correspondences of positive training pairs via graph\nmatching. By exploiting both spatial and visual contexts of human appearance in\ngraph matching, meaningful semantic correspondences can be obtained. To\ncircumvent the cumbersome \\emph{on-line} graph matching in testing phase, we\npropose to transfer the \\emph{off-line} learned patch-wise correspondences from\nthe positive training pairs to test pairs. In detail, for each test pair, the\ntraining pairs with similar pose-pair configurations are selected as\nreferences. The matching patterns (i.e., the correspondences) of the selected\nreferences are then utilized to calculate the patch-wise feature distances of\nthis test pair. To enhance the robustness of correspondence transfer, we design\na novel pose context descriptor to accurately model human body configurations,\nand present an approach to measure the similarity between a pair of pose\ncontext descriptors. Meanwhile, to improve testing efficiency, we propose a\ncorrespondence template ensemble method using the voting mechanism, which\nsignificantly reduces the amount of patch-wise matchings involved in distance\ncalculation. With aforementioned strategies, the REGCT model can effectively\nand efficiently handle the spatial misalignment problem in Re-ID. Extensive\nexperiments on five challenging benchmarks, including VIPeR, Road, PRID450S,\n3DPES and CUHK01, evidence the superior performance of REGCT over other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 13:52:01 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Zhou", "Qin", ""], ["Fan", "Heng", ""], ["Yang", "Hua", ""], ["Su", "Hang", ""], ["Zheng", "Shibao", ""], ["Wu", "Shuang", ""], ["Ling", "Haibin", ""]]}, {"id": "1805.06324", "submitter": "Chaitanya Mitash", "authors": "Chaitanya Mitash, Abdeslam Boularias, Kostas Bekris", "title": "Robust 6D Object Pose Estimation with Stochastic Congruent Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose estimation is frequently achieved by first segmenting an RGB\nimage and then, given depth data, registering the corresponding point cloud\nsegment against the object's 3D model. Despite the progress due to CNNs,\nsemantic segmentation output can be noisy, especially when the CNN is only\ntrained on synthetic data. This causes registration methods to fail in\nestimating a good object pose. This work proposes a novel stochastic\noptimization process that treats the segmentation output of CNNs as a\nconfidence probability. The algorithm, called Stochastic Congruent Sets\n(StoCS), samples pointsets on the point cloud according to the soft\nsegmentation distribution and so as to agree with the object's known geometry.\nThe pointsets are then matched to congruent sets on the 3D object model to\ngenerate pose estimates. StoCS is shown to be robust on an APC dataset, despite\nthe fact the CNN is trained only on synthetic data. In the YCB dataset, StoCS\noutperforms a recent network for 6D pose estimation and alternative pointset\nmatching techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 13:43:00 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Mitash", "Chaitanya", ""], ["Boularias", "Abdeslam", ""], ["Bekris", "Kostas", ""]]}, {"id": "1805.06332", "submitter": "Min Xu", "authors": "Chang Liu, Xiangrui Zeng, Kaiwen Wang, Qiang Guo, Min Xu", "title": "Multi-task Learning for Macromolecule Classification, Segmentation and\n  Coarse Structural Recovery in Cryo-Tomography", "comments": null, "journal-ref": "British Machine Vision Conference (BMVC) 2018", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Electron Cryo-Tomography (CECT) is a powerful 3D imaging tool for\nstudying the native structure and organization of macromolecules inside single\ncells. For systematic recognition and recovery of macromolecular structures\ncaptured by CECT, methods for several important tasks such as subtomogram\nclassification and semantic segmentation have been developed. However, the\nrecognition and recovery of macromolecular structures are still very difficult\ndue to high molecular structural diversity, crowding molecular environment, and\nthe imaging limitations of CECT. In this paper, we propose a novel multi-task\n3D convolutional neural network model for simultaneous classification,\nsegmentation, and coarse structural recovery of macromolecules of interest in\nsubtomograms. In our model, the learned image features of one task are shared\nand thereby mutually reinforce the learning of other tasks. Evaluated on\nrealistically simulated and experimental CECT data, our multi-task learning\nmodel outperformed all single-task learning methods for classification and\nsegmentation. In addition, we demonstrate that our model can generalize to\ndiscover, segment and recover novel structures that do not exist in the\ntraining data.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 13:58:09 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Liu", "Chang", ""], ["Zeng", "Xiangrui", ""], ["Wang", "Kaiwen", ""], ["Guo", "Qiang", ""], ["Xu", "Min", ""]]}, {"id": "1805.06334", "submitter": "Lukas Liebel", "authors": "Lukas Liebel and Marco K\\\"orner", "title": "Auxiliary Tasks in Multi-task Learning", "comments": "fixed minor typesetting issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task convolutional neural networks (CNNs) have shown impressive results\nfor certain combinations of tasks, such as single-image depth estimation (SIDE)\nand semantic segmentation. This is achieved by pushing the network towards\nlearning a robust representation that generalizes well to different atomic\ntasks. We extend this concept by adding auxiliary tasks, which are of minor\nrelevance for the application, to the set of learned tasks. As a kind of\nadditional regularization, they are expected to boost the performance of the\nultimately desired main tasks. To study the proposed approach, we picked\nvision-based road scene understanding (RSU) as an exemplary application. Since\nmulti-task learning requires specialized datasets, particularly when using\nextensive sets of tasks, we provide a multi-modal dataset for multi-task RSU,\ncalled synMT. More than 2.5 $\\cdot$ 10^5 synthetic images, annotated with 21\ndifferent labels, were acquired from the video game Grand Theft Auto V (GTA V).\nOur proposed deep multi-task CNN architecture was trained on various\ncombination of tasks using synMT. The experiments confirmed that auxiliary\ntasks can indeed boost network performance, both in terms of final results and\ntraining time.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 13:59:20 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 16:12:16 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liebel", "Lukas", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1805.06349", "submitter": "Charley Gros", "authors": "Charley Gros, Benjamin De Leener, Atef Badji, Josefina Maranzano,\n  Dominique Eden, Sara M. Dupont, Jason Talbott, Ren Zhuoquiong, Yaou Liu,\n  Tobias Granberg, Russell Ouellette, Yasuhiko Tachibana, Masaaki Hori, Kouhei\n  Kamiya, Lydia Chougar, Leszek Stawiarz, Jan Hillert, Elise Bannier, Anne\n  Kerbrat, Gilles Edan, Pierre Labauge, Virginie Callot, Jean Pelletier,\n  Bertrand Audoin, Henitsoa Rasoanandrianina, Jean-Christophe Brisset, Paola\n  Valsasina, Maria A. Rocca, Massimo Filippi, Rohit Bakshi, Shahamat Tauhid,\n  Ferran Prados, Marios Yiannakas, Hugh Kearney, Olga Ciccarelli, Seth Smith,\n  Constantina Andrada Treaba, Caterina Mainero, Jennifer Lefeuvre, Daniel S.\n  Reich, Govind Nair, Vincent Auclair, Donald G. McLaren, Allan R. Martin,\n  Michael G. Fehlings, Shahabeddin Vahdat, Ali Khatibi, Julien Doyon, Timothy\n  Shepherd, Erik Charlson, Sridar Narayanan, Julien Cohen-Adad", "title": "Automatic segmentation of the spinal cord and intramedullary multiple\n  sclerosis lesions with convolutional neural networks", "comments": "38 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spinal cord is frequently affected by atrophy and/or lesions in multiple\nsclerosis (MS) patients. Segmentation of the spinal cord and lesions from MRI\ndata provides measures of damage, which are key criteria for the diagnosis,\nprognosis, and longitudinal monitoring in MS. Automating this operation\neliminates inter-rater variability and increases the efficiency of\nlarge-throughput analysis pipelines. Robust and reliable segmentation across\nmulti-site spinal cord data is challenging because of the large variability\nrelated to acquisition parameters and image artifacts. The goal of this study\nwas to develop a fully-automatic framework, robust to variability in both image\nparameters and clinical condition, for segmentation of the spinal cord and\nintramedullary MS lesions from conventional MRI data. Scans of 1,042 subjects\n(459 healthy controls, 471 MS patients, and 112 with other spinal pathologies)\nwere included in this multi-site study (n=30). Data spanned three contrasts\n(T1-, T2-, and T2*-weighted) for a total of 1,943 volumes. The proposed cord\nand lesion automatic segmentation approach is based on a sequence of two\nConvolutional Neural Networks (CNNs). To deal with the very small proportion of\nspinal cord and/or lesion voxels compared to the rest of the volume, a first\nCNN with 2D dilated convolutions detects the spinal cord centerline, followed\nby a second CNN with 3D convolutions that segments the spinal cord and/or\nlesions. When compared against manual segmentation, our CNN-based approach\nshowed a median Dice of 95% vs. 88% for PropSeg, a state-of-the-art spinal cord\nsegmentation method. Regarding lesion segmentation on MS data, our framework\nprovided a Dice of 60%, a relative volume difference of -15%, and a lesion-wise\ndetection sensitivity and precision of 83% and 77%, respectively. The proposed\nframework is open-source and readily available in the Spinal Cord Toolbox.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 14:39:23 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 22:45:24 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Gros", "Charley", ""], ["De Leener", "Benjamin", ""], ["Badji", "Atef", ""], ["Maranzano", "Josefina", ""], ["Eden", "Dominique", ""], ["Dupont", "Sara M.", ""], ["Talbott", "Jason", ""], ["Zhuoquiong", "Ren", ""], ["Liu", "Yaou", ""], ["Granberg", "Tobias", ""], ["Ouellette", "Russell", ""], ["Tachibana", "Yasuhiko", ""], ["Hori", "Masaaki", ""], ["Kamiya", "Kouhei", ""], ["Chougar", "Lydia", ""], ["Stawiarz", "Leszek", ""], ["Hillert", "Jan", ""], ["Bannier", "Elise", ""], ["Kerbrat", "Anne", ""], ["Edan", "Gilles", ""], ["Labauge", "Pierre", ""], ["Callot", "Virginie", ""], ["Pelletier", "Jean", ""], ["Audoin", "Bertrand", ""], ["Rasoanandrianina", "Henitsoa", ""], ["Brisset", "Jean-Christophe", ""], ["Valsasina", "Paola", ""], ["Rocca", "Maria A.", ""], ["Filippi", "Massimo", ""], ["Bakshi", "Rohit", ""], ["Tauhid", "Shahamat", ""], ["Prados", "Ferran", ""], ["Yiannakas", "Marios", ""], ["Kearney", "Hugh", ""], ["Ciccarelli", "Olga", ""], ["Smith", "Seth", ""], ["Treaba", "Constantina Andrada", ""], ["Mainero", "Caterina", ""], ["Lefeuvre", "Jennifer", ""], ["Reich", "Daniel S.", ""], ["Nair", "Govind", ""], ["Auclair", "Vincent", ""], ["McLaren", "Donald G.", ""], ["Martin", "Allan R.", ""], ["Fehlings", "Michael G.", ""], ["Vahdat", "Shahabeddin", ""], ["Khatibi", "Ali", ""], ["Doyon", "Julien", ""], ["Shepherd", "Timothy", ""], ["Charlson", "Erik", ""], ["Narayanan", "Sridar", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "1805.06361", "submitter": "Rakesh Mehta", "authors": "Rakesh Mehta and Cemalettin Ozturk", "title": "Object detection at 200 Frames Per Second", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and fast object detector which can\nprocess hundreds of frames per second. To achieve this goal we investigate\nthree main aspects of the object detection framework: network architecture,\nloss function and training data (labeled and unlabeled). In order to obtain\ncompact network architecture, we introduce various improvements, based on\nrecent work, to develop an architecture which is computationally light-weight\nand achieves a reasonable performance. To further improve the performance,\nwhile keeping the complexity same, we utilize distillation loss function. Using\ndistillation loss we transfer the knowledge of a more accurate teacher network\nto proposed light-weight student network. We propose various innovations to\nmake distillation efficient for the proposed one stage detector pipeline:\nobjectness scaled distillation loss, feature map non-maximal suppression and a\nsingle unified distillation loss function for detection. Finally, building upon\nthe distillation loss, we explore how much can we push the performance by\nutilizing the unlabeled data. We train our model with unlabeled data using the\nsoft labels of the teacher network. Our final network consists of 10x fewer\nparameters than the VGG based object detection network and it achieves a speed\nof more than 200 FPS and proposed changes improve the detection accuracy by 14\nmAP over the baseline on Pascal dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:07:09 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Mehta", "Rakesh", ""], ["Ozturk", "Cemalettin", ""]]}, {"id": "1805.06374", "submitter": "Huaijin Chen", "authors": "Wanjia Liu, Huaijin Chen, Rishab Goel, Yuzhong Huang, Ashok\n  Veeraraghavan, Ankit Patel", "title": "Fast Retinomorphic Event Stream for Video Recognition and Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good temporal representations are crucial for video understanding, and the\nstate-of-the-art video recognition framework is based on two-stream networks.\nIn such framework, besides the regular ConvNets responsible for RGB frame\ninputs, a second network is introduced to handle the temporal representation,\nusually the optical flow (OF). However, OF or other task-oriented flow is\ncomputationally costly, and is thus typically pre-computed. Critically, this\nprevents the two-stream approach from being applied to reinforcement learning\n(RL) applications such as video game playing, where the next state depends on\ncurrent state and action choices. Inspired by the early vision systems of\nmammals and insects, we propose a fast event-driven representation (EDR) that\nmodels several major properties of early retinal circuits: (1) logarithmic\ninput response, (2) multi-timescale temporal smoothing to filter noise, and (3)\nbipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the\ndirectional information for fast speed (> 9000 fps), EDR en-ables fast\nreal-time inference/learning in video applications that require interaction\nbetween an agent and the world such as game-playing, virtual robotics, and\ndomain adaptation. In this vein, we use EDR to demonstrate performance\nimprovements over state-of-the-art reinforcement learning algorithms for Atari\ngames, something that has not been possible with pre-computed OF. Moreover,\nwith UCF-101 video action recognition experiments, we show that EDR performs\nnear state-of-the-art in accuracy while achieving a 1,500x speedup in input\nrepresentation processing, as compared to optical flow.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:42:37 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 15:10:37 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Liu", "Wanjia", ""], ["Chen", "Huaijin", ""], ["Goel", "Rishab", ""], ["Huang", "Yuzhong", ""], ["Veeraraghavan", "Ashok", ""], ["Patel", "Ankit", ""]]}, {"id": "1805.06386", "submitter": "Ken Nakanishi", "authors": "Ken Nakanishi, Shin-ichi Maeda, Takeru Miyato, Daisuke Okanohara", "title": "Neural Multi-scale Image Compression", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a new lossy image compression method that utilizes the\nmulti-scale features of natural images. Our model consists of two networks:\nmulti-scale lossy autoencoder and parallel multi-scale lossless coder. The\nmulti-scale lossy autoencoder extracts the multi-scale image features to\nquantized variables and the parallel multi-scale lossless coder enables rapid\nand accurate lossless coding of the quantized variables via encoding/decoding\nthe variables in parallel. Our proposed model achieves comparable performance\nto the state-of-the-art model on Kodak and RAISE-1k dataset images, and it\nencodes a PNG image of size $768 \\times 512$ in 70 ms with a single GPU and a\nsingle CPU process and decodes it into a high-fidelity image in approximately\n200 ms.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:00:01 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Nakanishi", "Ken", ""], ["Maeda", "Shin-ichi", ""], ["Miyato", "Takeru", ""], ["Okanohara", "Daisuke", ""]]}, {"id": "1805.06400", "submitter": "Mai Bui", "authors": "Mai Bui, Sergey Zakharov, Shadi Albarqouni, Slobodan Ilic and Nassir\n  Navab", "title": "When Regression Meets Manifold Learning for Object Recognition and Pose\n  Estimation", "comments": null, "journal-ref": "2018 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": "10.1109/ICRA.2018.8460654", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for object recognition and pose estimation\nfrom depth images using convolutional neural networks. Previous methods\naddressing this problem rely on manifold learning to learn low dimensional\nviewpoint descriptors and employ them in a nearest neighbor search on an\nestimated descriptor space. In comparison we create an efficient multi-task\nlearning framework combining manifold descriptor learning and pose regression.\nBy combining the strengths of manifold learning using triplet loss and pose\nregression, we could either estimate the pose directly reducing the complexity\ncompared to NN search, or use learned descriptor for the NN descriptor\nmatching. By in depth experimental evaluation of the novel loss function we\nobserved that the view descriptors learned by the network are much more\ndiscriminative resulting in almost 30% increase regarding relative pose\naccuracy compared to related works. On the other hand, regarding directly\nregressed poses we obtained important improvement compared to simple pose\nregression. By leveraging the advantages of both manifold learning and\nregression tasks, we are able to improve the current state-of-the-art for\nobject recognition and pose retrieval that we demonstrate through in depth\nexperimental evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:11:00 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Bui", "Mai", ""], ["Zakharov", "Sergey", ""], ["Albarqouni", "Shadi", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "1805.06406", "submitter": "Athanasios Vlontzos", "authors": "Athanasios Vlontzos and Krystian Mikolajczyk", "title": "Deep Segmentation and Registration in X-Ray Angiography Video", "comments": "To appear in BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In interventional radiology, short video sequences of vein structure in\nmotion are captured in order to help medical personnel identify vascular issues\nor plan intervention. Semantic segmentation can greatly improve the usefulness\nof these videos by indicating exact position of vessels and instruments, thus\nreducing the ambiguity. We propose a real-time segmentation method for these\ntasks, based on U-Net network trained in a Siamese architecture from\nautomatically generated annotations. We make use of noisy low level binary\nsegmentation and optical flow to generate multi class annotations that are\nsuccessively improved in a multistage segmentation approach. We significantly\nimprove the performance of a state of the art U-Net at the processing speeds of\n90fps.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:27:13 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 10:20:57 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Vlontzos", "Athanasios", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1805.06447", "submitter": "Yunhan Zhao", "authors": "Yunhan Zhao, Ye Tian, Charless Fowlkes, Wei Shen, Alan Yuille", "title": "Resisting Large Data Variations via Introspective Transformation Network", "comments": "camera-ready version, WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks that generalize to a wide range of variations in test\ndata is essential to building accurate and robust image classifiers. One\nstandard strategy is to apply data augmentation to synthetically enlarge the\ntraining set. However, data augmentation is essentially a brute-force method\nwhich generates uniform samples from some pre-defined set of transformations.\nIn this paper, we propose a principled approach to train networks with\nsignificantly improved resistance to large variations between training and\ntesting data. This is achieved by embedding a learnable transformation module\ninto the introspective network, which is a convolutional neural network (CNN)\nclassifier empowered with generative capabilities. Our approach alternates\nbetween synthesizing pseudo-negative samples and transformed positive examples\nbased on the current model, and optimizing model predictions on these\nsynthesized samples. Experimental results verify that our approach\nsignificantly improves the ability of deep networks to resist large variations\nbetween training and testing data and achieves classification accuracy\nimprovements on several benchmark datasets, including MNIST, affNIST, SVHN,\nCIFAR-10 and miniImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 17:53:21 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 22:41:49 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 06:13:13 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zhao", "Yunhan", ""], ["Tian", "Ye", ""], ["Fowlkes", "Charless", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "1805.06485", "submitter": "Dario Pavllo", "authors": "Dario Pavllo and David Grangier and Michael Auli", "title": "QuaterNet: A Quaternion-based Recurrent Model for Human Motion", "comments": "British Machine Vision Conference (BMVC), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for predicting or generating 3D human pose sequences is an\nactive research area. Previous work regresses either joint rotations or joint\npositions. The former strategy is prone to error accumulation along the\nkinematic chain, as well as discontinuities when using Euler angle or\nexponential map parameterizations. The latter requires re-projection onto\nskeleton constraints to avoid bone stretching and invalid configurations. This\nwork addresses both limitations. Our recurrent network, QuaterNet, represents\nrotations with quaternions and our loss function performs forward kinematics on\na skeleton to penalize absolute position errors instead of angle errors. On\nshort-term predictions, QuaterNet improves the state-of-the-art quantitatively.\nFor long-term generation, our approach is qualitatively judged as realistic as\nrecent neural strategies from the graphics literature.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 18:38:37 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 20:45:13 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Pavllo", "Dario", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1805.06549", "submitter": "Josiah Wang", "authors": "Pranava Madhyastha, Josiah Wang, Lucia Specia", "title": "Defoiling Foiled Image Captions", "comments": "In Proceedings of the 2018 Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of detecting foiled image captions, i.e. identifying\nwhether a caption contains a word that has been deliberately replaced by a\nsemantically similar word, thus rendering it inaccurate with respect to the\nimage being described. Solving this problem should in principle require a\nfine-grained understanding of images to detect linguistically valid\nperturbations in captions. In such contexts, encoding sufficiently descriptive\nimage information becomes a key challenge. In this paper, we demonstrate that\nit is possible to solve this task using simple, interpretable yet powerful\nrepresentations based on explicit object information. Our models achieve\nstate-of-the-art performance on a standard dataset, with scores exceeding those\nachieved by humans on the task. We also measure the upper-bound performance of\nour models using gold standard annotations. Our analysis reveals that the\nsimpler model performs well even without image information, suggesting that the\ndataset contains strong linguistic bias.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 22:50:17 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Madhyastha", "Pranava", ""], ["Wang", "Josiah", ""], ["Specia", "Lucia", ""]]}, {"id": "1805.06558", "submitter": "Rui Wang", "authors": "Rui Wang, Jan-Michael Frahm, Stephen M. Pizer", "title": "Recurrent Neural Network for Learning DenseDepth and Ego-Motion from\n  Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based, single-view depth estimation often generalizes poorly to\nunseen datasets. While learning-based, two-frame depth estimation solves this\nproblem to some extent by learning to match features across frames, it performs\npoorly at large depth where the uncertainty is high. There exists few\nlearning-based, multi-view depth estimation methods. In this paper, we present\na learning-based, multi-view dense depth map and ego-motion estimation method\nthat uses Recurrent Neural Networks (RNN). Our model is designed for 3D\nreconstruction from video where the input frames are temporally correlated. It\nis generalizable to single- or two-view dense depth estimation. Compared to\nrecent single- or two-view CNN-based depth estimation methods, our model\nleverages more views and achieves more accurate results, especially at large\ndistances. Our method produces superior results to the state-of-the-art\nlearning-based, single- or two-view depth estimation methods on both indoor and\noutdoor benchmark datasets. We also demonstrate that our method can even work\non extremely difficult sequences, such as endoscopic video, where none of the\nassumptions (static scene, constant lighting, Lambertian reflection, etc.) from\ntraditional 3D reconstruction methods hold.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 00:18:08 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Wang", "Rui", ""], ["Frahm", "Jan-Michael", ""], ["Pizer", "Stephen M.", ""]]}, {"id": "1805.06561", "submitter": "Ilke Demir", "authors": "Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing\n  Huang, Saikat Basu, Forest Hughes, Devis Tuia, Ramesh Raskar", "title": "DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images", "comments": "Dataset description for DeepGlobe 2018 Challenge at CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPRW.2018.00031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which\nincludes three public competitions for segmentation, detection, and\nclassification tasks on satellite images. Similar to other challenges in\ncomputer vision domain such as DAVIS and COCO, DeepGlobe proposes three\ndatasets and corresponding evaluation methodologies, coherently bundled in\nthree competitions with a dedicated workshop co-located with CVPR 2018.\n  We observed that satellite imagery is a rich and structured source of\ninformation, yet it is less investigated than everyday images by computer\nvision researchers. However, bridging modern computer vision with remote\nsensing data analysis could have critical impact to the way we understand our\nenvironment and lead to major breakthroughs in global urban planning or climate\nchange research. Keeping such bridging objective in mind, DeepGlobe aims to\nbring together researchers from different domains to raise awareness of remote\nsensing in the computer vision community and vice-versa. We aim to improve and\nevaluate state-of-the-art satellite image understanding approaches, which can\nhopefully serve as reference benchmarks for future research in the same topic.\nIn this paper, we analyze characteristics of each dataset, define the\nevaluation criteria of the competitions, and provide baselines for each task.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 00:45:37 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Demir", "Ilke", ""], ["Koperski", "Krzysztof", ""], ["Lindenbaum", "David", ""], ["Pang", "Guan", ""], ["Huang", "Jing", ""], ["Basu", "Saikat", ""], ["Hughes", "Forest", ""], ["Tuia", "Devis", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1805.06605", "submitter": "Maya Kabkab", "authors": "Pouya Samangouei, Maya Kabkab, Rama Chellappa", "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using\n  Generative Models", "comments": "Published as a conference paper at the Sixth International Conference\n  on Learning Representations (ICLR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural network approaches have been widely adopted for\nmachine learning tasks, including classification. However, they were shown to\nbe vulnerable to adversarial perturbations: carefully crafted small\nperturbations can cause misclassification of legitimate images. We propose\nDefense-GAN, a new framework leveraging the expressive capability of generative\nmodels to defend deep neural networks against such attacks. Defense-GAN is\ntrained to model the distribution of unperturbed images. At inference time, it\nfinds a close output to a given image which does not contain the adversarial\nchanges. This output is then fed to the classifier. Our proposed method can be\nused with any classification model and does not modify the classifier structure\nor training procedure. It can also be used as a defense against any attack as\nit does not assume knowledge of the process for generating the adversarial\nexamples. We empirically show that Defense-GAN is consistently effective\nagainst different attack methods and improves on existing defense strategies.\nOur code has been made publicly available at\nhttps://github.com/kabkabm/defensegan\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 05:38:55 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 00:20:52 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Samangouei", "Pouya", ""], ["Kabkab", "Maya", ""], ["Chellappa", "Rama", ""]]}, {"id": "1805.06618", "submitter": "Dhruv Rathi", "authors": "Dhruv Rathi", "title": "Optimization of Transfer Learning for Sign Language Recognition\n  Targeting Mobile Platform", "comments": "6 Pages, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The target of this research is to experiment, iterate and recommend a system\nthat is successful in recognition of American Sign Language (ASL). It is a\nchallenging as well as an interesting problem that if solved will bring a leap\nin social and technological aspects alike. In this paper, we propose a\nreal-time recognizer of ASL based on a mobile platform, so that it will have\nmore accessibility and provides an ease of use. The technique implemented is\nTransfer Learning of new data of Hand gestures for alphabets in ASL to be\nmodelled on various pre-trained high- end models and optimize the best model to\nrun on a mobile platform considering the various limitations of the same during\noptimization. The data used consists of 27,455 images of 24 alphabets of ASL.\nThe optimized model when ran over a memory-efficient mobile application,\nprovides an accuracy of 95.03% of accurate recognition with an average\nrecognition time of 2.42 seconds. This method ensures considerable\ndiscrimination in accuracy and recognition time than the previous research.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 06:50:57 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Rathi", "Dhruv", ""]]}, {"id": "1805.06625", "submitter": "Jun Cheng", "authors": "Jun Cheng, Zhengguo Li, Zaiwang Gu, Huazhu Fu, Damon Wing Kee Wong,\n  Jiang Liu", "title": "Structure-preserving Guided Retinal Image Filtering and Its Application\n  for Optic Disc Analysis", "comments": "Accepted for publication on IEEE Trans. on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging ( Volume: 37 , Issue: 11 ,\n  Pages 2536 - 2546, Nov. 2018 )", "doi": "10.1109/TMI.2018.2838550", "report-no": "https://ieeexplore.ieee.org/document/8361495", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal fundus photographs have been used in the diagnosis of many ocular\ndiseases such as glaucoma, pathological myopia, age-related macular\ndegeneration and diabetic retinopathy. With the development of computer\nscience, computer aided diagnosis has been developed to process and analyse the\nretinal images automatically. One of the challenges in the analysis is that the\nquality of the retinal image is often degraded. For example, a cataract in\nhuman lens will attenuate the retinal image, just as a cloudy camera lens which\nreduces the quality of a photograph. It often obscures the details in the\nretinal images and posts challenges in retinal image processing and analysing\ntasks. In this paper, we approximate the degradation of the retinal images as a\ncombination of human-lens attenuation and scattering. A novel\nstructure-preserving guided retinal image filtering (SGRIF) is then proposed to\nrestore images based on the attenuation and scattering model. The proposed\nSGRIF consists of a step of global structure transferring and a step of global\nedge-preserving smoothing. Our results show that the proposed SGRIF method is\nable to improve the contrast of retinal images, measured by histogram flatness\nmeasure, histogram spread and variability of local luminosity. In addition, we\nfurther explored the benefits of SGRIF for subsequent retinal image processing\nand analysing tasks. In the two applications of deep learning based optic cup\nsegmentation and sparse learning based cup-to-disc ratio (CDR) computation, our\nresults show that we are able to achieve more accurate optic cup segmentation\nand CDR measurements from images processed by SGRIF.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:17:51 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 09:11:19 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Cheng", "Jun", ""], ["Li", "Zhengguo", ""], ["Gu", "Zaiwang", ""], ["Fu", "Huazhu", ""], ["Wong", "Damon Wing Kee", ""], ["Liu", "Jiang", ""]]}, {"id": "1805.06641", "submitter": "Francisco Barranco", "authors": "Francisco Barranco, Cornelia Ferm\\\"uller, Yiannis Aloimonos, Eduardo\n  Ros", "title": "Joint direct estimation of 3D geometry and 3D motion using spatio\n  temporal gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conventional image motion based structure from motion methods first compute\noptical flow, then solve for the 3D motion parameters based on the epipolar\nconstraint, and finally recover the 3D geometry of the scene. However, errors\nin optical flow due to regularization can lead to large errors in 3D motion and\nstructure. This paper investigates whether performance and consistency can be\nimproved by avoiding optical flow estimation in the early stages of the\nstructure from motion pipeline, and it proposes a new direct method based on\nimage gradients (normal flow) only. The main idea lies in a reformulation of\nthe positive-depth constraint, which allows the use of well-known minimization\ntechniques to solve for 3D motion. The 3D motion estimate is then refined and\nstructure estimated adding a regularization based on depth. Experimental\ncomparisons on standard synthetic datasets and the real-world driving benchmark\ndataset KITTI using three different optic flow algorithms show that the method\nachieves better accuracy in all but one case. Furthermore, it outperforms\nexisting normal flow based 3D motion estimation techniques. Finally, the\nrecovered 3D geometry is shown to be also very accurate.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:54:24 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Barranco", "Francisco", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""], ["Ros", "Eduardo", ""]]}, {"id": "1805.06660", "submitter": "Yazhou Yang", "authors": "Yazhou Yang, Marco Loog", "title": "Single Shot Active Learning using Pseudo Annotators", "comments": "12 pages, 8 figure, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard myopic active learning assumes that human annotations are always\nobtainable whenever new samples are selected. This, however, is unrealistic in\nmany real-world applications where human experts are not readily available at\nall times. In this paper, we consider the single shot setting: all the required\nsamples should be chosen in a single shot and no human annotation can be\nexploited during the selection process. We propose a new method, Active\nLearning through Random Labeling (ALRL), which substitutes single human\nannotator for multiple, what we will refer to as, pseudo annotators. These\npseudo annotators always provide uniform and random labels whenever new\nunlabeled samples are queried. This random labeling enables standard active\nlearning algorithms to also exhibit the exploratory behavior needed for single\nshot active learning. The exploratory behavior is further enhanced by selecting\nthe most representative sample via minimizing nearest neighbor distance between\nunlabeled samples and queried samples. Experiments on real-world datasets\ndemonstrate that the proposed method outperforms several state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 09:05:28 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Yang", "Yazhou", ""], ["Loog", "Marco", ""]]}, {"id": "1805.06725", "submitter": "Samet Akcay", "authors": "Samet Akcay, Amir Atapour-Abarghouei and Toby P. Breckon", "title": "GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a classical problem in computer vision, namely the\ndetermination of the normal from the abnormal when datasets are highly biased\ntowards one class (normal) due to the insufficient sample size of the other\nclass (abnormal). While this can be addressed as a supervised learning problem,\na significantly more challenging problem is that of detecting the\nunknown/unseen anomaly case that takes us instead into the space of a\none-class, semi-supervised learning paradigm. We introduce such a novel anomaly\ndetection model, by using a conditional generative adversarial network that\njointly learns the generation of high-dimensional image space and the inference\nof latent space. Employing encoder-decoder-encoder sub-networks in the\ngenerator network enables the model to map the input image to a lower dimension\nvector, which is then used to reconstruct the generated output image. The use\nof the additional encoder network maps this generated image to its latent\nrepresentation. Minimizing the distance between these images and the latent\nvectors during training aids in learning the data distribution for the normal\nsamples. As a result, a larger distance metric from this learned data\ndistribution at inference time is indicative of an outlier from that\ndistribution - an anomaly. Experimentation over several benchmark datasets,\nfrom varying domains, shows the model efficacy and superiority over previous\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 12:36:02 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 09:01:39 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 15:56:32 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Akcay", "Samet", ""], ["Atapour-Abarghouei", "Amir", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1805.06737", "submitter": "Zhe Xu", "authors": "Zhe Xu, Biao Min, Ray C.C. Cheung", "title": "A Robust Background Initialization Algorithm with Superpixel Motion\n  Detection", "comments": "submitted to Elsevier Signal Processing: Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene background initialization allows the recovery of a clear image without\nforeground objects from a video sequence, which is generally the first step in\nmany computer vision and video processing applications. The process may be\nstrongly affected by some challenges such as illumination changes, foreground\ncluttering, intermittent movement, etc. In this paper, a robust background\ninitialization approach based on superpixel motion detection is proposed. Both\nspatial and temporal characteristics of frames are adopted to effectively\neliminate foreground objects. A subsequence with stable illumination condition\nis first selected for background estimation. Images are segmented into\nsuperpixels to preserve spatial texture information and foreground objects are\neliminated by superpixel motion filtering process. A low-complexity\ndensity-based clustering is then performed to generate reliable background\ncandidates for final background determination. The approach has been evaluated\non SBMnet dataset and it achieves a performance superior or comparable to other\nstate-of-the-art works with faster processing speed. Moreover, in those complex\nand dynamic categories, the algorithm produces the best results showing the\nrobustness against very challenging scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 12:52:57 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Xu", "Zhe", ""], ["Min", "Biao", ""], ["Cheung", "Ray C. C.", ""]]}, {"id": "1805.06741", "submitter": "Xin Wei", "authors": "Xin Wei, Hui Wang, Bryan Scotney and Huan Wan", "title": "Minimum Margin Loss for Deep Face Recognition", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2019.107012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has achieved great progress owing to the fast development of\nthe deep neural network in the past a few years. As an important part of deep\nneural networks, a number of the loss functions have been proposed which\nsignificantly improve the state-of-the-art methods. In this paper, we proposed\na new loss function called Minimum Margin Loss (MML) which aims at enlarging\nthe margin of those overclose class centre pairs so as to enhance the\ndiscriminative ability of the deep features. MML supervises the training\nprocess together with the Softmax Loss and the Centre Loss, and also makes up\nthe defect of Softmax + Centre Loss. The experimental results on MegaFace, LFW\nand YTF datasets show that the proposed method achieves the state-of-the-art\nperformance, which demonstrates the effectiveness of the proposed MML.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 13:02:23 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 09:46:43 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 09:28:28 GMT"}, {"version": "v4", "created": "Mon, 20 Aug 2018 15:27:51 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wei", "Xin", ""], ["Wang", "Hui", ""], ["Scotney", "Bryan", ""], ["Wan", "Huan", ""]]}, {"id": "1805.06749", "submitter": "Farnoosh Heidarivincheh", "authors": "Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen", "title": "Action Completion: A Temporal Model for Moment Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce completion moment detection for actions - the problem of\nlocating the moment of completion, when the action's goal is confidently\nconsidered achieved. The paper proposes a joint classification-regression\nrecurrent model that predicts completion from a given frame, and then\nintegrates frame-level contributions to detect sequence-level completion\nmoment. We introduce a recurrent voting node that predicts the frame's relative\nposition of the completion moment by either classification or regression. The\nmethod is also capable of detecting incompletion. For example, the method is\ncapable of detecting a missed ball-catch, as well as the moment at which the\nball is safely caught. We test the method on 16 actions from three public\ndatasets, covering sports as well as daily actions. Results show that when\ncombining contributions from frames prior to the completion moment as well as\nframes post completion, the completion moment is detected within one second in\n89% of all tested sequences.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 13:21:43 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 22:06:19 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Heidarivincheh", "Farnoosh", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""]]}, {"id": "1805.06771", "submitter": "Nachiket Deo", "authors": "Nachiket Deo and Mohan M. Trivedi", "title": "Convolutional Social Pooling for Vehicle Trajectory Prediction", "comments": "Accepted for publication at CVPR TrajNet Workshop, 2018. arXiv admin\n  note: text overlap with arXiv:1805.05499", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2018, pp. 1468-1476", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the motion of surrounding vehicles is a critical ability for an\nautonomous vehicle deployed in complex traffic. Motion of all vehicles in a\nscene is governed by the traffic context, i.e., the motion and relative spatial\nconfiguration of neighboring vehicles. In this paper we propose an LSTM\nencoder-decoder model that uses convolutional social pooling as an improvement\nto social pooling layers for robustly learning interdependencies in vehicle\nmotion. Additionally, our model outputs a multi-modal predictive distribution\nover future trajectories based on maneuver classes. We evaluate our model using\nthe publicly available NGSIM US-101 and I-80 datasets. Our results show\nimprovement over the state of the art in terms of RMS values of prediction\nerror and negative log-likelihoods of true future trajectories under the\nmodel's predictive distribution. We also present a qualitative analysis of the\nmodel's predicted distributions for various traffic scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 00:24:38 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Deo", "Nachiket", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1805.06776", "submitter": "Oliver Scheel", "authors": "Oliver Scheel, Loren Schwarz, Nassir Navab, Federico Tombari", "title": "Situation Assessment for Planning Lane Changes: Combining Recurrent\n  Models and Prediction", "comments": "ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest challenges towards fully autonomous cars is the\nunderstanding of complex and dynamic scenes. Such understanding is needed for\nplanning of maneuvers, especially those that are particularly frequent such as\nlane changes. While in recent years advanced driver-assistance systems have\nmade driving safer and more comfortable, these have mostly focused on car\nfollowing scenarios, and less on maneuvers involving lane changes. In this work\nwe propose a situation assessment algorithm for classifying driving situations\nwith respect to their suitability for lane changing. For this, we propose a\ndeep learning architecture based on a Bidirectional Recurrent Neural Network,\nwhich uses Long Short-Term Memory units, and integrates a prediction component\nin the form of the Intelligent Driver Model. We prove the feasibility of our\nalgorithm on the publicly available NGSIM datasets, where we outperform\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 13:52:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Scheel", "Oliver", ""], ["Schwarz", "Loren", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1805.06830", "submitter": "Julian M\\\"uller", "authors": "Julian M\\\"uller, Andreas Fregin and Klaus Dietmayer", "title": "Disparity Sliding Window: Object Proposals From Disparity Images", "comments": "Submitted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window approaches have been widely used for object recognition tasks\nin recent years. They guarantee an investigation of the entire input image for\nthe object to be detected and allow a localization of that object. Despite the\ncurrent trend towards deep neural networks, sliding window methods are still\nused in combination with convolutional neural networks. The risk of overlooking\nan object is clearly reduced compared to alternative detection approaches which\ndetect objects based on shape, edges or color. Nevertheless, the sliding window\ntechnique strongly increases the computational effort as the classifier has to\nverify a large number of object candidates. This paper proposes a sliding\nwindow approach which also uses depth information from a stereo camera. This\nleads to a greatly decreased number of object candidates without significantly\nreducing the detection accuracy. A theoretical investigation of the\nconventional sliding window approach is presented first. Other publications to\ndate only mentioned rough estimations of the computational cost. A mathematical\nderivation clarifies the number of object candidates with respect to parameters\nsuch as image and object size. Subsequently, the proposed disparity sliding\nwindow approach is presented in detail. The approach is evaluated on pedestrian\ndetection with annotations and images from the KITTI object detection\nbenchmark. Furthermore, a comparison with two state-of-the-art methods is made.\nCode is available in C++ and Python https://github.com/julimueller/\ndisparity-sliding-window.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 15:45:41 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 09:16:04 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["M\u00fcller", "Julian", ""], ["Fregin", "Andreas", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1805.06846", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Qiang Qiu, Robert Calderbank and Guillermo Sapiro", "title": "RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant\n  Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit encoding of group actions in deep features makes it possible for\nconvolutional neural networks (CNNs) to handle global deformations of images,\nwhich is critical to success in many vision tasks. This paper proposes to\ndecompose the convolutional filters over joint steerable bases across the space\nand the group geometry simultaneously, namely a rotation-equivariant CNN with\ndecomposed convolutional filters (RotDCF). This decomposition facilitates\ncomputing the joint convolution, which is proved to be necessary for the group\nequivariance. It significantly reduces the model size and computational\ncomplexity while preserving performance, and truncation of the bases expansion\nserves implicitly to regularize the filters. On datasets involving in-plane and\nout-of-plane object rotations, RotDCF deep features demonstrate greater\nrobustness and interpretability than regular CNNs. The stability of the\nequivariant representation to input variations is also proved theoretically\nunder generic assumptions on the filters in the decomposed form. The RotDCF\nframework can be extended to groups other than rotations, providing a general\napproach which achieves both group equivariance and representation stability at\na reduced model size.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:24:51 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1805.06875", "submitter": "Alexander Richard", "authors": "Alexander Richard, Hilde Kuehne, Ahsan Iqbal, Juergen Gall", "title": "NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video learning is an important task in computer vision and has experienced\nincreasing interest over the recent years. Since even a small amount of videos\neasily comprises several million frames, methods that do not rely on a\nframe-level annotation are of special importance. In this work, we propose a\nnovel learning algorithm with a Viterbi-based loss that allows for online and\nincremental learning of weakly annotated video data. We moreover show that\nexplicit context and length modeling leads to huge improvements in video\nsegmentation and labeling tasks andinclude these models into our framework. On\nseveral action segmentation benchmarks, we obtain an improvement of up to 10%\ncompared to current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 17:36:42 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Richard", "Alexander", ""], ["Kuehne", "Hilde", ""], ["Iqbal", "Ahsan", ""], ["Gall", "Juergen", ""]]}, {"id": "1805.06880", "submitter": "Oisin Mac Aodha", "authors": "Matteo Ruggero Ronchi, Oisin Mac Aodha, Robert Eng, Pietro Perona", "title": "It's all Relative: Monocular 3D Human Pose Estimation from Weakly\n  Supervised Data", "comments": "BMVC 2018. Project page available at\n  http://www.vision.caltech.edu/~mronchi/projects/RelativePose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of 3D human pose estimation from 2D input images using\nonly weakly supervised training data. Despite showing considerable success for\n2D pose estimation, the application of supervised machine learning to 3D pose\nestimation in real world images is currently hampered by the lack of varied\ntraining images with corresponding 3D poses. Most existing 3D pose estimation\nalgorithms train on data that has either been collected in carefully controlled\nstudio settings or has been generated synthetically. Instead, we take a\ndifferent approach, and propose a 3D human pose estimation algorithm that only\nrequires relative estimates of depth at training time. Such training signal,\nalthough noisy, can be easily collected from crowd annotators, and is of\nsufficient quality for enabling successful training and evaluation of 3D pose\nalgorithms. Our results are competitive with fully supervised regression based\napproaches on the Human3.6M dataset, despite using significantly weaker\ntraining data. Our proposed algorithm opens the door to using existing\nwidespread 2D datasets for 3D pose estimation by allowing fine-tuning with\nnoisy relative constraints, resulting in more accurate 3D poses.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 17:40:18 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 00:37:41 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ronchi", "Matteo Ruggero", ""], ["Mac Aodha", "Oisin", ""], ["Eng", "Robert", ""], ["Perona", "Pietro", ""]]}, {"id": "1805.06909", "submitter": "Aupendu Kar", "authors": "Aupendu Kar, Sri Phani Krishna Karri, Nirmalya Ghosh, Ramanathan\n  Sethuraman, Debdoot Sheet", "title": "Fully Convolutional Model for Variable Bit Length and Lossy High Density\n  Compression of Mammograms", "comments": "4 pages, 3 figures, To appear in Workshop on Learned Image\n  Compression, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early works on medical image compression date to the 1980's with the impetus\non deployment of teleradiology systems for high-resolution digital X-ray\ndetectors. Commercially deployed systems during the period could compress 4,096\nx 4,096 sized images at 12 bpp to 2 bpp using lossless arithmetic coding, and\nover the years JPEG and JPEG2000 were imbibed reaching upto 0.1 bpp. Inspired\nby the reprise of deep learning based compression for natural images over the\nlast two years, we propose a fully convolutional autoencoder for diagnostically\nrelevant feature preserving lossy compression. This is followed by leveraging\narithmetic coding for encapsulating high redundancy of features for further\nhigh-density code packing leading to variable bit length. We demonstrate\nperformance on two different publicly available digital mammography datasets\nusing peak signal-to-noise ratio (pSNR), structural similarity (SSIM) index and\ndomain adaptability tests between datasets. At high density compression factors\nof >300x (~0.04 bpp), our approach rivals JPEG and JPEG2000 as evaluated\nthrough a Radiologist's visual Turing test.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 18:01:39 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Kar", "Aupendu", ""], ["Karri", "Sri Phani Krishna", ""], ["Ghosh", "Nirmalya", ""], ["Sethuraman", "Ramanathan", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1805.06956", "submitter": "Ahmad Babaeian Jelodar", "authors": "Ahmad Babaeian Jelodar, Md Sirajus Salekin, Yu Sun", "title": "Identifying Object States in Cooking-Related Images", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding object states is as important as object recognition for robotic\ntask planning and manipulation. To our knowledge, this paper explicitly\nintroduces and addresses the state identification problem in cooking related\nimages for the first time. In this paper, objects and ingredients in cooking\nvideos are explored and the most frequent objects are analyzed. Eleven states\nfrom the most frequent cooking objects are examined and a dataset of images\ncontaining those objects and their states is created. As a solution to the\nstate identification problem, a Resnet based deep model is proposed. The model\nis initialized with Imagenet weights and trained on the dataset of eleven\nclasses. The trained state identification model is evaluated on a subset of the\nImagenet dataset and state labels are provided using a combination of the model\nwith manual checking. Moreover, an individual model is fine-tuned for each\nobject in the dataset using the weights from the initially trained model and\nobject-specific images, where significant improvement is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 20:18:56 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 21:06:21 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 14:37:19 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Jelodar", "Ahmad Babaeian", ""], ["Salekin", "Md Sirajus", ""], ["Sun", "Yu", ""]]}, {"id": "1805.06958", "submitter": "Amal Lahiani", "authors": "Amal Lahiani, Jacob Gildenblat, Irina Klaman, Nassir Navab, Eldad\n  Klaiman", "title": "Generalizing multistain immunohistochemistry tissue segmentation using\n  one-shot color deconvolution deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in cancer immunotherapy biomarker research is quantification\nof pattern changes in microscopic whole slide images of tumor biopsies.\nDifferent cell types tend to migrate into various tissue compartments and form\nvariable distribution patterns. Drug development requires correlative analysis\nof various biomarkers in and between the tissue compartments. To enable that,\ntissue slides are manually annotated by expert pathologists. Manual annotation\nof tissue slides is a labor intensive, tedious and error-prone task. Automation\nof this annotation process can improve accuracy and consistency while reducing\nworkload and cost in a way that will positively influence drug development\nefforts. In this paper we present a novel one-shot color deconvolution deep\nlearning method to automatically segment and annotate digitized slide images\nwith multiple stainings into compartments of tumor, healthy tissue, and\nnecrosis. We address the task in the context of drug development where multiple\nstains, tissue and tumor types exist and look into solutions for\ngeneralizations over these image populations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 20:23:00 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 14:19:16 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2018 20:41:54 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Lahiani", "Amal", ""], ["Gildenblat", "Jacob", ""], ["Klaman", "Irina", ""], ["Navab", "Nassir", ""], ["Klaiman", "Eldad", ""]]}, {"id": "1805.06960", "submitter": "Ravi Shekhar", "authors": "Ravi Shekhar, Tim Baumgartner, Aashish Venkatesh, Elia Bruni,\n  Raffaella Bernardi, Raquel Fernandez", "title": "Ask No More: Deciding when to guess in referential visual dialogue", "comments": "COLING 2018 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our goal is to explore how the abilities brought in by a dialogue manager can\nbe included in end-to-end visually grounded conversational agents. We make\ninitial steps towards this general goal by augmenting a task-oriented visual\ndialogue model with a decision-making component that decides whether to ask a\nfollow-up question to identify a target referent in an image, or to stop the\nconversation to make a guess. Our analyses show that adding a decision making\ncomponent produces dialogues that are less repetitive and that include fewer\nunnecessary questions, thus potentially leading to more efficient and less\nunnatural interactions.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 20:32:08 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 10:43:56 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Shekhar", "Ravi", ""], ["Baumgartner", "Tim", ""], ["Venkatesh", "Aashish", ""], ["Bruni", "Elia", ""], ["Bernardi", "Raffaella", ""], ["Fernandez", "Raquel", ""]]}, {"id": "1805.06983", "submitter": "Gabriele Campanella", "authors": "Gabriele Campanella, Vitor Werneck Krauss Silva, Thomas J. Fuchs", "title": "Terabyte-scale Deep Multiple Instance Learning for Classification and\n  Localization in Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of computational pathology, the use of decision support systems\npowered by state-of-the-art deep learning solutions has been hampered by the\nlack of large labeled datasets. Until recently, studies relied on datasets in\nthe order of few hundreds of slides which are not enough to train a model that\ncan work at scale in the clinic. Here, we have gathered a dataset consisting of\n12,160 slides, two orders of magnitude larger than previous datasets in\npathology and equivalent to 25 times the pixel count of the entire ImageNet\ndataset. Given the size of our dataset it is possible for us to train a deep\nlearning model under the Multiple Instance Learning (MIL) assumption where only\nthe overall slide diagnosis is necessary for training, avoiding all the\nexpensive pixel-wise annotations that are usually part of supervised learning\napproaches. We test our framework on a complex task, that of prostate cancer\ndiagnosis on needle biopsies. We performed a thorough evaluation of the\nperformance of our MIL pipeline under several conditions achieving an AUC of\n0.98 on a held-out test set of 1,824 slides. These results open the way for\ntraining accurate diagnosis prediction models at scale, laying the foundation\nfor decision support system deployment in the clinic.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 22:43:46 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 22:42:02 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Campanella", "Gabriele", ""], ["Silva", "Vitor Werneck Krauss", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "1805.07009", "submitter": "Lisha Cui", "authors": "Lisha Cui, Rui Ma, Pei Lv, Xiaoheng Jiang, Zhimin Gao, Bing Zhou and\n  Mingliang Xu", "title": "MDSSD: Multi-scale Deconvolutional Single Shot Detector for Small\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most of the object detectors based on multi-scale feature maps, the\nshallow layers are rich in fine spatial information and thus mainly responsible\nfor small object detection. The performance of small object detection, however,\nis still less than satisfactory because of the deficiency of semantic\ninformation on shallow feature maps. In this paper, we design a Multi-scale\nDeconvolutional Single Shot Detector (MDSSD), especially for small object\ndetection. In MDSSD, multiple high-level feature maps at different scales are\nupsampled simultaneously to increase the spatial resolution. Afterwards, we\nimplement the skip connections with low-level feature maps via Fusion Block.\nThe fusion feature maps, named Fusion Module, are of strong feature\nrepresentational power of small instances. It is noteworthy that these\nhigh-level feature maps utilized in Fusion Block preserve both strong semantic\ninformation and some fine details of small instances, rather than the top-most\nlayer where the representation of fine details for small objects are\npotentially wiped out. The proposed framework achieves 77.6% mAP for small\nobject detection on the challenging dataset TT100K with 512 x 512 input,\noutperforming other detectors with a large margin. Moreover, it can also\nachieve state-of-the-art results for general object detection on PASCAL VOC2007\ntest and MS COCO test-dev2015, especially achieving 2 to 5 points improvement\non small object categories.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 01:09:25 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 04:59:25 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 14:13:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Cui", "Lisha", ""], ["Ma", "Rui", ""], ["Lv", "Pei", ""], ["Jiang", "Xiaoheng", ""], ["Gao", "Zhimin", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "1805.07020", "submitter": "Li Xue", "authors": "Li Xue, Si Xiandong, Nie Lanshun, Li Jiazhen, Ding Renjie, Zhan\n  Dechen, Chu Dianhui", "title": "Understanding and Improving Deep Neural Network for Activity Recognition", "comments": "10 pages, 9 figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition has become a popular research branch in the field of\npervasive computing in recent years. A large number of experiments can be\nobtained that activity sensor-based data's characteristic in activity\nrecognition is variety, volume, and velocity. Deep learning technology,\ntogether with its various models, is one of the most effective ways of working\non activity data. Nevertheless, there is no clear understanding of why it\nperforms so well or how to make it more effective. In order to solve this\nproblem, first, we applied convolution neural network on Human Activity\nRecognition Using Smart phones Data Set. Second, we realized the visualization\nof the sensor-based activity's data features extracted from the neural network.\nThen we had in-depth analysis of the visualization of features, explored the\nrelationship between activity and features, and analyzed how Neural Networks\nidentify activity based on these features. After that, we extracted the\nsignificant features related to the activities and sent the features to the\nDNN-based fusion model, which improved the classification rate to 96.1%. This\nis the first work to our knowledge that visualizes abstract sensor-based\nactivity data features. Based on the results, the method proposed in the paper\npromises to realize the accurate classification of sensor- based activity\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 02:01:45 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Xue", "Li", ""], ["Xiandong", "Si", ""], ["Lanshun", "Nie", ""], ["Jiazhen", "Li", ""], ["Renjie", "Ding", ""], ["Dechen", "Zhan", ""], ["Dianhui", "Chu", ""]]}, {"id": "1805.07029", "submitter": "Jeongyeol Baek", "authors": "JeongYeol Baek, Ioana Veronica Chelu, Livia Iordache, Vlad Paunescu,\n  HyunJoo Ryu, Alexandru Ghiuta, Andrei Petreanu, YunSung Soh, Andrei Leica,\n  ByeongMoon Jeon", "title": "Scene Understanding Networks for Autonomous Driving based on Around View\n  Monitoring System", "comments": "Accepted by CVPR 2018 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern driver assistance systems rely on a wide range of sensors (RADAR,\nLIDAR, ultrasound and cameras) for scene understanding and prediction. These\nsensors are typically used for detecting traffic participants and scene\nelements required for navigation. In this paper we argue that relying on camera\nbased systems, specifically Around View Monitoring (AVM) system has great\npotential to achieve these goals in both parking and driving modes with\ndecreased costs. The contributions of this paper are as follows: we present a\nnew end-to-end solution for delimiting the safe drivable area for each frame by\nmeans of identifying the closest obstacle in each direction from the driving\nvehicle, we use this approach to calculate the distance to the nearest\nobstacles and we incorporate it into a unified end-to-end architecture capable\nof joint object detection, curb detection and safe drivable area detection.\nFurthermore, we describe the family of networks for both a high accuracy\nsolution and a low complexity solution. We also introduce further augmentation\nof the base architecture with 3D object detection.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 02:54:54 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Baek", "JeongYeol", ""], ["Chelu", "Ioana Veronica", ""], ["Iordache", "Livia", ""], ["Paunescu", "Vlad", ""], ["Ryu", "HyunJoo", ""], ["Ghiuta", "Alexandru", ""], ["Petreanu", "Andrei", ""], ["Soh", "YunSung", ""], ["Leica", "Andrei", ""], ["Jeon", "ByeongMoon", ""]]}, {"id": "1805.07030", "submitter": "Alexander Mathews", "authors": "Alexander Mathews, Lexing Xie, Xuming He", "title": "SemStyle: Learning to Generate Stylised Image Captions using Unaligned\n  Text", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic style is an essential part of written communication, with the\npower to affect both clarity and attractiveness. With recent advances in vision\nand language, we can start to tackle the problem of generating image captions\nthat are both visually grounded and appropriately styled. Existing approaches\neither require styled training captions aligned to images or generate captions\nwith low relevance. We develop a model that learns to generate visually\nrelevant styled captions from a large corpus of styled text without aligned\nimages. The core idea of this model, called SemStyle, is to separate semantics\nand style. One key component is a novel and concise semantic term\nrepresentation generated using natural language processing techniques and frame\nsemantics. In addition, we develop a unified language model that decodes\nsentences with diverse word choices and syntax for different styles.\nEvaluations, both automatic and manual, show captions from SemStyle preserve\nimage semantics, are descriptive, and are style shifted. More broadly, this\nwork provides possibilities to learn richer image descriptions from the\nplethora of linguistic data available on the web.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 03:01:45 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Mathews", "Alexander", ""], ["Xie", "Lexing", ""], ["He", "Xuming", ""]]}, {"id": "1805.07036", "submitter": "Tak-Wai Hui", "authors": "Tak-Wai Hui, Xiaoou Tang, Chen Change Loy", "title": "LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow\n  Estimation", "comments": "Accepted to CVPR 2018 (spotlight). Project page:\n  http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical\nflow estimation, requires over 160M parameters to achieve accurate flow\nestimation. In this paper we present an alternative network that outperforms\nFlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being\n30 times smaller in the model size and 1.36 times faster in the running speed.\nThis is made possible by drilling down to architectural details that might have\nbeen missed in the current frameworks: (1) We present a more effective flow\ninference approach at each pyramid level through a lightweight cascaded\nnetwork. It not only improves flow estimation accuracy through early\ncorrection, but also permits seamless incorporation of descriptor matching in\nour network. (2) We present a novel flow regularization layer to ameliorate the\nissue of outliers and vague flow boundaries by using a feature-driven local\nconvolution. (3) Our network owns an effective structure for pyramidal feature\nextraction and embraces feature warping rather than image warping as practiced\nin FlowNet2. Our code and trained models are available at\nhttps://github.com/twhui/LiteFlowNet .\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 03:29:54 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Hui", "Tak-Wai", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1805.07039", "submitter": "Weili Nie", "authors": "Weili Nie, Yang Zhang and Ankit Patel", "title": "A Theoretical Explanation for Perplexing Behaviors of\n  Backpropagation-based Visualizations", "comments": "21 pages, ICML 2018 (We revised the proofs of Theorem 1 and 2 in\n  Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation-based visualizations have been proposed to interpret\nconvolutional neural networks (CNNs), however a theory is missing to justify\ntheir behaviors: Guided backpropagation (GBP) and deconvolutional network\n(DeconvNet) generate more human-interpretable but less class-sensitive\nvisualizations than saliency map. Motivated by this, we develop a theoretical\nexplanation revealing that GBP and DeconvNet are essentially doing (partial)\nimage recovery which is unrelated to the network decisions. Specifically, our\nanalysis shows that the backward ReLU introduced by GBP and DeconvNet, and the\nlocal connections in CNNs are the two main causes of compelling visualizations.\nExtensive experiments are provided that support the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 03:45:06 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 21:54:22 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 03:48:33 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 16:23:39 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Nie", "Weili", ""], ["Zhang", "Yang", ""], ["Patel", "Ankit", ""]]}, {"id": "1805.07071", "submitter": "Pengju Liu", "authors": "Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo", "title": "Multi-level Wavelet-CNN for Image Restoration", "comments": "Accepted for publication at CVPR NTIRE Workshop, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tradeoff between receptive field size and efficiency is a crucial issue\nin low level vision. Plain convolutional networks (CNNs) generally enlarge the\nreceptive field at the expense of computational cost. Recently, dilated\nfiltering has been adopted to address this issue. But it suffers from gridding\neffect, and the resulting receptive field is only a sparse sampling of input\nimage with checkerboard patterns. In this paper, we present a novel multi-level\nwavelet CNN (MWCNN) model for better tradeoff between receptive field size and\ncomputational efficiency. With the modified U-Net architecture, wavelet\ntransform is introduced to reduce the size of feature maps in the contracting\nsubnetwork. Furthermore, another convolutional layer is further used to\ndecrease the channels of feature maps. In the expanding subnetwork, inverse\nwavelet transform is then deployed to reconstruct the high resolution feature\nmaps. Our MWCNN can also be explained as the generalization of dilated\nfiltering and subsampling, and can be applied to many image restoration tasks.\nThe experimental results clearly show the effectiveness of MWCNN for image\ndenoising, single image super-resolution, and JPEG image artifacts removal.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 06:59:00 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 14:02:13 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liu", "Pengju", ""], ["Zhang", "Hongzhi", ""], ["Zhang", "Kai", ""], ["Lin", "Liang", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1805.07103", "submitter": "Jakob Wasserthal", "authors": "Jakob Wasserthal, Peter Neher, Klaus H. Maier-Hein", "title": "TractSeg - Fast and accurate white matter tract segmentation", "comments": null, "journal-ref": "Wasserthal, J., Neher, P., Maier-Hein, K.H., 2018. TractSeg - Fast\n  and accurate white matter tract segmentation. NeuroImage", "doi": "10.1016/j.neuroimage.2018.07.070", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The individual course of white matter fiber tracts is an important key for\nanalysis of white matter characteristics in healthy and diseased brains.\nUniquely, diffusion-weighted MRI tractography in combination with region-based\nor clustering-based selection of streamlines allows for the in-vivo delineation\nand analysis of anatomically well known tracts. This, however, currently\nrequires complex, computationally intensive and tedious-to-set-up processing\npipelines. TractSeg is a novel convolutional neural network-based approach that\ndirectly segments tracts in the field of fiber orientation distribution\nfunction (fODF) peaks without requiring tractography, image registration or\nparcellation. We demonstrate in 105 subjects from the Human Connectome Project\nthat the proposed approach is much faster than existing methods while providing\nunprecedented accuracy. The code and data are openly available at\nhttps://github.com/MIC-DKFZ/TractSeg/ and\nhttps://doi.org/10.5281/zenodo.1088277, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 09:05:54 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 15:56:06 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wasserthal", "Jakob", ""], ["Neher", "Peter", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1805.07112", "submitter": "Chen Chen", "authors": "Chen Chen, Shuai Mu, Wanpeng Xiao, Zexiong Ye, Liesi Wu, Qi Ju", "title": "Improving Image Captioning with Conditional Generative Adversarial Nets", "comments": "12 pages; 33 figures; 36 refenences; Accepted by AAAI2019", "journal-ref": "AAAI2019", "doi": "10.1609/aaai.v33i01.33018142", "report-no": "Vol 33 No 01: AAAI-19, IAAI-19, EAAI-20", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel\nconditional-generative-adversarial-nets-based image captioning framework as an\nextension of traditional reinforcement-learning (RL)-based encoder-decoder\narchitecture. To deal with the inconsistent evaluation problem among different\nobjective language metrics, we are motivated to design some \"discriminator\"\nnetworks to automatically and progressively determine whether generated caption\nis human described or machine generated. Two kinds of discriminator\narchitectures (CNN and RNN-based structures) are introduced since each has its\nown advantages. The proposed algorithm is generic so that it can enhance any\nexisting RL-based image captioning framework and we show that the conventional\nRL training method is just a special case of our approach. Empirically, we show\nconsistent improvements over all language evaluation metrics for different\nstate-of-the-art image captioning models. In addition, the well-trained\ndiscriminators can also be viewed as objective image captioning evaluators\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 09:31:53 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 08:55:29 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 06:36:45 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 03:02:47 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Chen", "Chen", ""], ["Mu", "Shuai", ""], ["Xiao", "Wanpeng", ""], ["Ye", "Zexiong", ""], ["Wu", "Liesi", ""], ["Ju", "Qi", ""]]}, {"id": "1805.07170", "submitter": "Silvia-Laura Pintea", "authors": "Silvia L. Pintea, Yue Liu, Jan C. van Gemert", "title": "Recurrent knowledge distillation", "comments": "International Conference on Image Processing (ICIP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation compacts deep networks by letting a small student\nnetwork learn from a large teacher network. The accuracy of knowledge\ndistillation recently benefited from adding residual layers. We propose to\nreduce the size of the student network even further by recasting multiple\nresidual layers in the teacher network into a single recurrent student layer.\nWe propose three variants of adding recurrent connections into the student\nnetwork, and show experimentally on CIFAR-10, Scenes and MiniPlaces, that we\ncan reduce the number of parameters at little loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 12:32:16 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Pintea", "Silvia L.", ""], ["Liu", "Yue", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "1805.07193", "submitter": "Markus Braun", "authors": "Markus Braun, Sebastian Krebs, Fabian Flohr, and Dariu M. Gavrila", "title": "The EuroCity Persons Dataset: A Novel Benchmark for Object Detection", "comments": "Submitted to IEEE Trans. on Pattern Analysis and Machine Intelligence", "journal-ref": "Published in IEEE Trans. on Pattern Analysis and Machine\n  Intelligence, 2019", "doi": "10.1109/TPAMI.2019.2897684", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data has had a great share in the success of deep learning in computer\nvision. Recent works suggest that there is significant further potential to\nincrease object detection performance by utilizing even bigger datasets. In\nthis paper, we introduce the EuroCity Persons dataset, which provides a large\nnumber of highly diverse, accurate and detailed annotations of pedestrians,\ncyclists and other riders in urban traffic scenes. The images for this dataset\nwere collected on-board a moving vehicle in 31 cities of 12 European countries.\nWith over 238200 person instances manually labeled in over 47300 images,\nEuroCity Persons is nearly one order of magnitude larger than person datasets\nused previously for benchmarking. The dataset furthermore contains a large\nnumber of person orientation annotations (over 211200). We optimize four\nstate-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3)\nto serve as baselines for the new object detection benchmark. In experiments\nwith previous datasets we analyze the generalization capabilities of these\ndetectors when trained with the new dataset. We furthermore study the effect of\nthe training set size, the dataset diversity (day- vs. night-time, geographical\nregion), the dataset detail (i.e. availability of object orientation\ninformation) and the annotation quality on the detector performance. Finally,\nwe analyze error sources and discuss the road ahead.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 13:17:46 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 15:20:52 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Braun", "Markus", ""], ["Krebs", "Sebastian", ""], ["Flohr", "Fabian", ""], ["Gavrila", "Dariu M.", ""]]}, {"id": "1805.07253", "submitter": "Anjith George", "authors": "Anjith George, Aurobinda Routray", "title": "Recognition of Activities from Eye Gaze and Egocentric Video", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for recognition of human activity from\negocentric video and eye tracking data obtained from a head-mounted eye\ntracker. Three channels of information such as eye movement, ego-motion, and\nvisual features are combined for the classification of activities. Image\nfeatures were extracted using a pre-trained convolutional neural network. Eye\nand ego-motion are quantized, and the windowed histograms are used as the\nfeatures. The combination of features obtains better accuracy for activity\nclassification as compared to individual features.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 14:52:25 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1805.07258", "submitter": "Thorsten Laude", "authors": "Thorsten Laude, Yannick Richter and J\\\"orn Ostermann", "title": "Neural Network Compression using Transform Coding and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the deployment of neural networks on mobile devices and the necessity of\ntransmitting neural networks over limited or expensive channels, the file size\nof the trained model was identified as bottleneck. In this paper, we propose a\ncodec for the compression of neural networks which is based on transform coding\nfor convolutional and dense layers and on clustering for biases and\nnormalizations. By using this codec, we achieve average compression factors\nbetween 7.9-9.3 while the accuracy of the compressed networks for image\nclassification decreases only by 1%-2%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 14:57:52 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Laude", "Thorsten", ""], ["Richter", "Yannick", ""], ["Ostermann", "J\u00f6rn", ""]]}, {"id": "1805.07277", "submitter": "Yongqi Zhang", "authors": "Yongqi Zhang", "title": "XOGAN: One-to-Many Unsupervised Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation aims at learning the relationship\nbetween samples from two image domains without supervised pair information. The\nrelationship between two domain images can be one-to-one, one-to-many or\nmany-to-many. In this paper, we study the one-to-many unsupervised image\ntranslation problem in which an input sample from one domain can correspond to\nmultiple samples in the other domain. To learn the complex relationship between\nthe two domains, we introduce an additional variable to control the variations\nin our one-to-many mapping. A generative model with an XO-structure, called the\nXOGAN, is proposed to learn the cross domain relationship among the two domains\nand the ad- ditional variables. Not only can we learn to translate between the\ntwo image domains, we can also handle the translated images with additional\nvariations. Experiments are performed on unpaired image generation tasks,\nincluding edges-to-objects translation and facial image translation. We show\nthat the proposed XOGAN model can generate plausible images and control\nvariations, such as color and texture, of the generated images. Moreover, while\nstate-of-the-art unpaired image generation algorithms tend to generate images\nwith monotonous colors, XOGAN can generate more diverse results.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:19:22 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Zhang", "Yongqi", ""]]}, {"id": "1805.07281", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo\n  Bremer", "title": "An Unsupervised Approach to Solving Inverse Problems using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving inverse problems continues to be a challenge in a wide array of\napplications ranging from deblurring, image inpainting, source separation etc.\nMost existing techniques solve such inverse problems by either explicitly or\nimplicitly finding the inverse of the model. The former class of techniques\nrequire explicit knowledge of the measurement process which can be unrealistic,\nand rely on strong analytical regularizers to constrain the solution space,\nwhich often do not generalize well. The latter approaches have had remarkable\nsuccess in part due to deep learning, but require a large collection of\nsource-observation pairs, which can be prohibitively expensive. In this paper,\nwe propose an unsupervised technique to solve inverse problems with generative\nadversarial networks (GANs). Using a pre-trained GAN in the space of source\nsignals, we show that one can reliably recover solutions to under determined\nproblems in a `blind' fashion, i.e., without knowledge of the measurement\nprocess. We solve this by making successive estimates on the model and the\nsolution in an iterative fashion. We show promising results in three\nchallenging applications -- blind source separation, image deblurring, and\nrecovering an image from its edge map, and perform better than several\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:23:01 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:30:08 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Kailkhura", "Bhavya", ""], ["Bremer", "Timo", ""]]}, {"id": "1805.07290", "submitter": "David Stutz", "authors": "David Stutz, Andreas Geiger", "title": "Learning 3D Shape Completion under Weak Supervision", "comments": null, "journal-ref": "David Stutz, Andreas Geiger. Learning 3D Shape Completion under\n  Weak Supervision. International Journal of Computer Vision (2018)", "doi": "10.1007/s11263-018-1126-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of 3D shape completion from sparse and noisy point\nclouds, a fundamental problem in computer vision and robotics. Recent\napproaches are either data-driven or learning-based: Data-driven approaches\nrely on a shape model whose parameters are optimized to fit the observations;\nLearning-based approaches, in contrast, avoid the expensive optimization step\nby learning to directly predict complete shapes from incomplete observations in\na fully-supervised setting. However, full supervision is often not available in\npractice. In this work, we propose a weakly-supervised learning-based approach\nto 3D shape completion which neither requires slow optimization nor direct\nsupervision. While we also learn a shape prior on synthetic data, we amortize,\ni.e., learn, maximum likelihood fitting using deep neural networks resulting in\nefficient shape completion without sacrificing accuracy. On synthetic\nbenchmarks based on ShapeNet and ModelNet as well as on real robotics data from\nKITTI and Kinect, we demonstrate that the proposed amortized maximum likelihood\napproach is able to compete with recent fully supervised baselines and\noutperforms data-driven approaches, while requiring less supervision and being\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:37:28 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:03:06 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Stutz", "David", ""], ["Geiger", "Andreas", ""]]}, {"id": "1805.07291", "submitter": "Wei Zhu", "authors": "Wei Zhu, Qiang Qiu, Bao Wang, Jianfeng Lu, Guillermo Sapiro, Ingrid\n  Daubechies", "title": "Stop memorizing: A data-dependent regularization framework for intrinsic\n  pattern learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) typically have enough capacity to fit random data\nby brute force even when conventional data-dependent regularizations focusing\non the geometry of the features are imposed. We find out that the reason for\nthis is the inconsistency between the enforced geometry and the standard\nsoftmax cross entropy loss. To resolve this, we propose a new framework for\ndata-dependent DNN regularization, the\nGeometrically-Regularized-Self-Validating neural Networks (GRSVNet). During\ntraining, the geometry enforced on one batch of features is simultaneously\nvalidated on a separate batch using a validation loss consistent with the\ngeometry. We study a particular case of GRSVNet, the Orthogonal-Low-rank\nEmbedding (OLE)-GRSVNet, which is capable of producing highly discriminative\nfeatures residing in orthogonal low-rank subspaces. Numerical experiments show\nthat OLE-GRSVNet outperforms DNNs with conventional regularization when trained\non real data. More importantly, unlike conventional DNNs, OLE-GRSVNet refuses\nto memorize random data or random labels, suggesting it only learns intrinsic\npatterns by reducing the memorizing capacity of the baseline DNN.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:38:06 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 00:46:57 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Zhu", "Wei", ""], ["Qiu", "Qiang", ""], ["Wang", "Bao", ""], ["Lu", "Jianfeng", ""], ["Sapiro", "Guillermo", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1805.07295", "submitter": "Gaoyuan Liang", "authors": "Guohui Zhang, Gaoyuan Liang, Fang Su, Fanxin Qu, and Jing-Yan Wang", "title": "Cross-domain attribute representation based on convolutional neural\n  network", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.09733", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of domain transfer learning, we learn a model for the\npredic-tion in a target domain from the data of both some source domains and\nthe target domain, where the target domain is in lack of labels while the\nsource domain has sufficient labels. Besides the instances of the data,\nrecently the attributes of data shared across domains are also explored and\nproven to be very helpful to leverage the information of different domains. In\nthis paper, we propose a novel learning framework for domain-transfer learning\nbased on both instances and attributes. We proposed to embed the attributes of\ndif-ferent domains by a shared convolutional neural network (CNN), learn a\ndomain-independent CNN model to represent the information shared by dif-ferent\ndomains by matching across domains, and a domain-specific CNN model to\nrepresent the information of each domain. The concatenation of the three CNN\nmodel outputs is used to predict the class label. An iterative algo-rithm based\non gradient descent method is developed to learn the parameters of the model.\nThe experiments over benchmark datasets show the advantage of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 14:42:33 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Zhang", "Guohui", ""], ["Liang", "Gaoyuan", ""], ["Su", "Fang", ""], ["Qu", "Fanxin", ""], ["Wang", "Jing-Yan", ""]]}, {"id": "1805.07319", "submitter": "Kele Xu", "authors": "Kele Xu and Dawei Feng and Haibo Mi and Boqing Zhu and Dezhi Wang and\n  Lilun Zhang and Hengxing Cai and Shuwen Liu", "title": "Mixup-Based Acoustic Scene Classification Using Multi-Channel\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio scene classification, the problem of predicting class labels of audio\nscenes, has drawn lots of attention during the last several years. However, it\nremains challenging and falls short of accuracy and efficiency. Recently,\nConvolutional Neural Network (CNN)-based methods have achieved better\nperformance with comparison to the traditional methods. Nevertheless,\nconventional single channel CNN may fail to consider the fact that additional\ncues may be embedded in the multi-channel recordings. In this paper, we explore\nthe use of Multi-channel CNN for the classification task, which aims to extract\nfeatures from different channels in an end-to-end manner. We conduct the\nevaluation compared with the conventional CNN and traditional Gaussian Mixture\nModel-based methods. Moreover, to improve the classification accuracy further,\nthis paper explores the using of mixup method. In brief, mixup trains the\nneural network on linear combinations of pairs of the representation of audio\nscene examples and their labels. By employing the mixup approach for data\nargumentation, the novel model can provide higher prediction accuracy and\nrobustness in contrast with previous models, while the generalization error can\nalso be reduced on the evaluation data.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 16:36:19 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Xu", "Kele", ""], ["Feng", "Dawei", ""], ["Mi", "Haibo", ""], ["Zhu", "Boqing", ""], ["Wang", "Dezhi", ""], ["Zhang", "Lilun", ""], ["Cai", "Hengxing", ""], ["Liu", "Shuwen", ""]]}, {"id": "1805.07339", "submitter": "Alex Poms", "authors": "Alex Poms, Will Crichton, Pat Hanrahan, Kayvon Fatahalian", "title": "Scanner: Efficient Video Analysis at Scale", "comments": "14 pages, 14 figuers", "journal-ref": null, "doi": "10.1145/3197517.3201394", "report-no": null, "categories": "cs.CV cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of visual computing applications depend on the analysis of\nlarge video collections. The challenge is that scaling applications to operate\non these datasets requires efficient systems for pixel data access and parallel\nprocessing across large numbers of machines. Few programmers have the\ncapability to operate efficiently at these scales, limiting the field's ability\nto explore new applications that leverage big video data. In response, we have\ncreated Scanner, a system for productive and efficient video analysis at scale.\nScanner organizes video collections as tables in a data store optimized for\nsampling frames from compressed video, and executes pixel processing\ncomputations, expressed as dataflow graphs, on these frames. Scanner schedules\nvideo analysis applications expressed using these abstractions onto\nheterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and\nmedia processing ASICs, for high-throughput pixel processing. We demonstrate\nthe productivity of Scanner by authoring a variety of video processing\napplications including the synthesis of stereo VR video streams from\nmulti-camera rigs, markerless 3D human pose reconstruction from video, and\ndata-mining big video datasets such as hundreds of feature-length films or over\n70,000 hours of TV news. These applications achieve near-expert performance on\na single machine and scale efficiently to hundreds of machines, enabling\nformerly long-running big video data analysis tasks to be carried out in\nminutes to hours.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 17:43:55 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Poms", "Alex", ""], ["Crichton", "Will", ""], ["Hanrahan", "Pat", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1805.07389", "submitter": "Sean Mullery Mr.", "authors": "Sean Mullery and Paul F. Whelan", "title": "Batch Normalization in the final layer of generative networks", "comments": "8 pages, 3 figures, under IMVIP 2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Networks have shown great promise in generating photo-realistic\nimages. Despite this, the theory surrounding them is still an active research\narea. Much of the useful work with Generative networks rely on heuristics that\ntend to produce good results. One of these heuristics is the advice not to use\nBatch Normalization in the final layer of the generator network. Many of the\nstate-of-the-art generative network architectures use this heuristic, but the\nreasons for doing so are inconsistent. This paper will show that this is not\nnecessarily a good heuristic and that Batch Normalization can be beneficial in\nthe final layer of the generator network either by placing it before the final\nnon-linear activation, usually a $tanh$ or replacing the final $tanh$\nactivation altogether with Batch Normalization and clipping. We show that this\ncan lead to the faster training of Generator networks by matching the generator\nto the mean and standard deviation of the target distribution's image colour\nvalues.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 18:40:51 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Mullery", "Sean", ""], ["Whelan", "Paul F.", ""]]}, {"id": "1805.07426", "submitter": "Nafis Neehal", "authors": "Masum Shah Junayed, Afsana Ahsan Jeny, Nafis Neehal", "title": "Incept-N: A Convolutional Neural Network based Classification Approach\n  for Predicting Nationality from Facial Features", "comments": "5 Pages, 7 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nationality of a human being is a well-known identifying characteristic\nused for every major authentication purpose in every country. Albeit advances\nin the application of Artificial Intelligence and Computer Vision in different\naspects, its contribution to this specific security procedure is yet to be\ncultivated. With a goal to successfully applying computer vision techniques to\npredict the nationality of a person based on his facial features, we have\nproposed this novel method and have achieved an average of 93.6% accuracy with\nvery low misclassification rate.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 20:12:07 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Junayed", "Masum Shah", ""], ["Jeny", "Afsana Ahsan", ""], ["Neehal", "Nafis", ""]]}, {"id": "1805.07440", "submitter": "Linnan Wang", "authors": "Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca", "title": "Neural Architecture Search using Deep Neural Networks and Monte Carlo\n  Tree Search", "comments": "To appear in the Thirty-Fourth AAAI conference on Artificial\n  Intelligence (AAAI-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has shown great success in automating the\ndesign of neural networks, but the prohibitive amount of computations behind\ncurrent NAS methods requires further investigations in improving the sample\nefficiency and the network evaluation cost to get better results in a shorter\ntime. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS)\nbased NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the\nsearch efficiency by adaptively balancing the exploration and exploitation at\nthe state level, and by a Meta-Deep Neural Network (DNN) to predict network\naccuracies for biasing the search toward a promising region. To amortize the\nnetwork evaluation cost, AlphaX accelerates MCTS rollouts with a distributed\ndesign and reduces the number of epochs in evaluating a network by transfer\nlearning, which is guided with the tree structure in MCTS. In 12 GPU days and\n1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy\non CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods\nin both the accuracy and sampling efficiency. Particularly, we also evaluate\nAlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more\nsample efficient than Random Search and Regularized Evolution in finding the\nglobal optimum. Finally, we show the searched architecture improves a variety\nof vision applications from Neural Style Transfer, to Image Captioning and\nObject Detection.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 20:57:41 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 07:47:47 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 06:50:28 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 01:04:05 GMT"}, {"version": "v5", "created": "Thu, 21 Nov 2019 17:45:31 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Linnan", ""], ["Zhao", "Yiyang", ""], ["Jinnai", "Yuu", ""], ["Tian", "Yuandong", ""], ["Fonseca", "Rodrigo", ""]]}, {"id": "1805.07442", "submitter": "Nakka Krishna Kanth", "authors": "Sankaraganesh Jonna, Krishna Kanth Nakka, and Rajiv R. Sahay", "title": "My camera can see through fences: A deep learning approach for image\n  de-fencing", "comments": "ACPR 2015, Kuala Lumpur", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, the availability of inexpensive image capturing devices such\nas smartphones/tablets has led to an exponential increase in the number of\nimages/videos captured. However, sometimes the amateur photographer is hindered\nby fences in the scene which have to be removed after the image has been\ncaptured. Conventional approaches to image de-fencing suffer from inaccurate\nand non-robust fence detection apart from being limited to processing images of\nonly static occluded scenes. In this paper, we propose a semi-automated\nde-fencing algorithm using a video of the dynamic scene. We use convolutional\nneural networks for detecting fence pixels. We provide qualitative as well as\nquantitative comparison results with existing lattice detection algorithms on\nthe existing PSU NRT data set and a proposed challenging fenced image dataset.\nThe inverse problem of fence removal is solved using split Bregman technique\nassuming total variation of the de-fenced image as the regularization\nconstraint.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 21:02:04 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Jonna", "Sankaraganesh", ""], ["Nakka", "Krishna Kanth", ""], ["Sahay", "Rajiv R.", ""]]}, {"id": "1805.07457", "submitter": "Jyh-Jing Hwang", "authors": "Jyh-Jing Hwang, Tsung-Wei Ke, Jianbo Shi, Stella X. Yu", "title": "Adversarial Structure Matching for Structured Prediction Tasks", "comments": "In CVPR 2019. Webpage & Code:\n  https://jyhjinghwang.github.io/projects/asm.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-wise losses, e.g., cross-entropy or L2, have been widely used in\nstructured prediction tasks as a spatial extension of generic image\nclassification or regression. However, its i.i.d. assumption neglects the\nstructural regularity present in natural images. Various attempts have been\nmade to incorporate structural reasoning mostly through structure priors in a\ncooperative way where co-occurring patterns are encouraged.\n  We, on the other hand, approach this problem from an opposing angle and\npropose a new framework, Adversarial Structure Matching (ASM), for training\nsuch structured prediction networks via an adversarial process, in which we\ntrain a structure analyzer that provides the supervisory signals, the ASM loss.\nThe structure analyzer is trained to maximize the ASM loss, or to emphasize\nrecurring multi-scale hard negative structural mistakes among co-occurring\npatterns. On the contrary, the structured prediction network is trained to\nreduce those mistakes and is thus enabled to distinguish fine-grained\nstructures. As a result, training structured prediction networks using ASM\nreduces contextual confusion among objects and improves boundary localization.\nWe demonstrate that our ASM outperforms pixel-wise IID loss or structural prior\nGAN loss on three different structured prediction tasks: semantic segmentation,\nmonocular depth estimation, and surface normal prediction.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 22:03:58 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 16:46:07 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Ke", "Tsung-Wei", ""], ["Shi", "Jianbo", ""], ["Yu", "Stella X.", ""]]}, {"id": "1805.07468", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Yu Yang, Yuchen Liu, Ying Nian Wu, Song-Chun Zhu", "title": "Unsupervised Learning of Neural Networks to Explain Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised method to learn a neural network, namely\nan explainer, to interpret a pre-trained convolutional neural network (CNN),\ni.e., explaining knowledge representations hidden in middle conv-layers of the\nCNN. Given feature maps of a certain conv-layer of the CNN, the explainer\nperforms like an auto-encoder, which first disentangles the feature maps into\nobject-part features and then inverts object-part features back to features of\nhigher conv-layers of the CNN. More specifically, the explainer contains\ninterpretable conv-layers, where each filter disentangles the representation of\na specific object part from chaotic input feature maps. As a paraphrase of CNN\nfeatures, the disentangled representations of object parts help people\nunderstand the logic inside the CNN. We also learn the explainer to use\nobject-part features to reconstruct features of higher CNN layers, in order to\nminimize loss of information during the feature disentanglement. More\ncrucially, we learn the explainer via network distillation without using any\nannotations of sample labels, object parts, or textures for supervision. We\nhave applied our method to different types of CNNs for evaluation, and\nexplainers have significantly boosted the interpretability of CNN features.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 23:02:14 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Zhang", "Quanshi", ""], ["Yang", "Yu", ""], ["Liu", "Yuchen", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1805.07473", "submitter": "Yuhong Guo", "authors": "Meng Ye and Yuhong Guo", "title": "Progressive Ensemble Networks for Zero-Shot Recognition", "comments": "CVPR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advancement of supervised image recognition algorithms, their\ndependence on the availability of labeled data and the rapid expansion of image\ncategories raise the significant challenge of zero-shot learning. Zero-shot\nlearning (ZSL) aims to transfer knowledge from labeled classes into unlabeled\nclasses to reduce human labeling effort. In this paper, we propose a novel\nprogressive ensemble network model with multiple projected label embeddings to\naddress zero-shot image recognition. The ensemble network is built by learning\nmultiple image classification functions with a shared feature extraction\nnetwork but different label embedding representations, which enhance the\ndiversity of the classifiers and facilitate information transfer to unlabeled\nclasses. A progressive training framework is then deployed to gradually label\nthe most confident images in each unlabeled class with predicted pseudo-labels\nand update the ensemble network with the training data augmented by the\npseudo-labels. The proposed model performs training on both labeled and\nunlabeled data. It can naturally bridge the domain shift problem in visual\nappearances and be extended to the generalized zero-shot learning scenario. We\nconduct experiments on multiple ZSL datasets and the empirical results\ndemonstrate the efficacy of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 23:24:12 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 22:07:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ye", "Meng", ""], ["Guo", "Yuhong", ""]]}, {"id": "1805.07477", "submitter": "Alireza Zaeemzadeh", "authors": "Alireza Zaeemzadeh, Nazanin Rahnavard, Mubarak Shah", "title": "Norm-Preservation: Why Residual Networks Can Become Extremely Deep?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting neural networks with skip connections, as introduced in the\nso-called ResNet architecture, surprised the community by enabling the training\nof networks of more than 1,000 layers with significant performance gains. This\npaper deciphers ResNet by analyzing the effect of skip connections, and puts\nforward new theoretical results on the advantages of identity skip connections\nin neural networks. We prove that the skip connections in the residual blocks\nfacilitate preserving the norm of the gradient, and lead to stable\nback-propagation, which is desirable from optimization perspective. We also\nshow that, perhaps surprisingly, as more residual blocks are stacked, the\nnorm-preservation of the network is enhanced. Our theoretical arguments are\nsupported by extensive empirical evidence. Can we push for extra\nnorm-preservation? We answer this question by proposing an efficient method to\nregularize the singular values of the convolution operator and making the\nResNet's transition layers extra norm-preserving. Our numerical investigations\ndemonstrate that the learning dynamics and the classification performance of\nResNet can be improved by making it even more norm preserving. Our results and\nthe introduced modification for ResNet, referred to as Procrustes ResNets, can\nbe used as a guide for training deeper networks and can also inspire new deeper\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 23:37:17 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 22:05:37 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 17:53:58 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 01:11:14 GMT"}, {"version": "v5", "created": "Wed, 22 Apr 2020 19:05:09 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zaeemzadeh", "Alireza", ""], ["Rahnavard", "Nazanin", ""], ["Shah", "Mubarak", ""]]}, {"id": "1805.07499", "submitter": "Rowel Atienza", "authors": "Rowel Atienza", "title": "Fast Disparity Estimation using Dense Networks", "comments": "In Proc. International Conference on Robotics and Automation 2018\n  (ICRA2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparity estimation is a difficult problem in stereo vision because the\ncorrespondence technique fails in images with textureless and repetitive\nregions. Recent body of work using deep convolutional neural networks (CNN)\novercomes this problem with semantics. Most CNN implementations use an\nautoencoder method; stereo images are encoded, merged and finally decoded to\npredict the disparity map. In this paper, we present a CNN implementation\ninspired by dense networks to reduce the number of parameters. Furthermore, our\napproach takes into account semantic reasoning in disparity estimation. Our\nproposed network, called DenseMapNet, is compact, fast and can be trained\nend-to-end. DenseMapNet requires 290k parameters only and runs at 30Hz or\nfaster on color stereo images in full resolution. Experimental results show\nthat DenseMapNet accuracy is comparable with other significantly bigger\nCNN-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 03:15:12 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Atienza", "Rowel", ""]]}, {"id": "1805.07509", "submitter": "Jichao Zhang", "authors": "Jichao Zhang, Yezhi Shu, Songhua Xu, Gongze Cao, Fan Zhong, Meng Liu,\n  Xueying Qin", "title": "Sparsely Grouped Multi-task Generative Adversarial Networks for Facial\n  Attribute Manipulation", "comments": "Accepted by ACMMM2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Image-to-Image Translation algorithms have achieved significant\nprogress in neural style transfer and image attribute manipulation tasks.\nHowever, existing approaches require exhaustively labelling training data,\nwhich is labor demanding, difficult to scale up, and hard to migrate into new\ndomains. To overcome such a key limitation, we propose Sparsely Grouped\nGenerative Adversarial Networks (SG-GAN) as a novel approach that can translate\nimages on sparsely grouped datasets where only a few samples for training are\nlabelled. Using a novel one-input multi-output architecture, SG-GAN is\nwell-suited for tackling sparsely grouped learning and multi-task learning. The\nproposed model can translate images among multiple groups using only a single\ncommonly trained model. To experimentally validate advantages of the new model,\nwe apply the proposed method to tackle a series of attribute manipulation tasks\nfor facial images. Experimental results demonstrate that SG-GAN can generate\nimage translation results of comparable quality with baselines methods on\nadequately labelled datasets and results of superior quality on sparsely\ngrouped datasets. The official implementation is publicly\navailable:https://github.com/zhangqianhui/Sparsely-Grouped-GAN.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 04:02:57 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 02:52:55 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 14:40:33 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 08:14:20 GMT"}, {"version": "v5", "created": "Mon, 6 Aug 2018 08:34:07 GMT"}, {"version": "v6", "created": "Fri, 19 Oct 2018 09:34:14 GMT"}, {"version": "v7", "created": "Thu, 28 May 2020 18:26:40 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zhang", "Jichao", ""], ["Shu", "Yezhi", ""], ["Xu", "Songhua", ""], ["Cao", "Gongze", ""], ["Zhong", "Fan", ""], ["Liu", "Meng", ""], ["Qin", "Xueying", ""]]}, {"id": "1805.07526", "submitter": "Kuan Han", "authors": "Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello,\n  Zhongming Liu", "title": "Deep Predictive Coding Network with Local Recurrent Processing for\n  Object Recognition", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by \"predictive coding\" - a theory in neuroscience, we develop a\nbi-directional and dynamic neural network with local recurrent processing,\nnamely predictive coding network (PCN). Unlike feedforward-only convolutional\nneural networks, PCN includes both feedback connections, which carry top-down\npredictions, and feedforward connections, which carry bottom-up errors of\nprediction. Feedback and feedforward connections enable adjacent layers to\ninteract locally and recurrently to refine representations towards minimization\nof layer-wise prediction errors. When unfolded over time, the recurrent\nprocessing gives rise to an increasingly deeper hierarchy of non-linear\ntransformation, allowing a shallow network to dynamically extend itself into an\narbitrarily deep network. We train and test PCN for image classification with\nSVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters,\nPCN achieves competitive performance compared to classical and state-of-the-art\nmodels. Further analysis shows that the internal representations in PCN\nconverge over time and yield increasingly better accuracy in object\nrecognition. Errors of top-down prediction also reveal visual saliency or\nbottom-up attention.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 06:44:24 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 02:22:50 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Han", "Kuan", ""], ["Wen", "Haiguang", ""], ["Zhang", "Yizhen", ""], ["Fu", "Di", ""], ["Culurciello", "Eugenio", ""], ["Liu", "Zhongming", ""]]}, {"id": "1805.07527", "submitter": "Ram Prakash Sharma Mr.", "authors": "Ram Prakash Sharma, Somnath Dey", "title": "Two-stage quality adaptive fingerprint image enhancement using Fuzzy\n  c-means clustering based fingerprint quality analysis", "comments": "34 pages, 8 figures, Submitted to Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition techniques are immensely dependent on quality of the\nfingerprint images. To improve the performance of recognition algorithm for\npoor quality images an efficient enhancement algorithm should be designed.\nPerformance improvement of recognition algorithm will be more if enhancement\nprocess is adaptive to the fingerprint quality (wet, dry or normal). In this\npaper, a quality adaptive fingerprint enhancement algorithm is proposed. The\nproposed fingerprint quality assessment algorithm clusters the fingerprint\nimages in appropriate quality class of dry, wet, normal dry, normal wet and\ngood quality using fuzzy c-means technique. It considers seven features namely,\nmean, moisture, variance, uniformity, contrast, ridge valley area uniformity\nand ridge valley uniformity into account for clustering the fingerprint images\nin appropriate quality class. Fingerprint images of each quality class undergo\nthrough a two-stage fingerprint quality enhancement process. A quality adaptive\npreprocessing method is used as front-end before enhancing the fingerprint\nimages with Gabor, short term Fourier transform and oriented diffusion\nfiltering based enhancement techniques. Experimental results show improvement\nin the verification results for FVC2004 datasets. Significant improvement in\nequal error rate is observed while using quality adaptive preprocessing based\napproaches in comparison to the current state-of-the-art enhancement\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 06:49:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Sharma", "Ram Prakash", ""], ["Dey", "Somnath", ""]]}, {"id": "1805.07545", "submitter": "Qing Wang", "authors": "Qing Wang, Long Chen, Wei Tian", "title": "End-to-end driving simulation via angle branched network", "comments": "10 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning for end-to-end autonomous driving has drawn attention from\nacademic communities. Current methods either only use images as the input which\nis ambiguous when a car approaches an intersection, or use additional command\ninformation to navigate the vehicle but not automated enough. Focusing on\nmaking the vehicle drive along the given path, we propose a new navigation\ncommand that does not require human's participation and a novel model\narchitecture called angle branched network. Both the new navigation command and\nthe angle branched network are easy to understand and effective. Besides, we\nfind that not only segmentation information but also depth information can\nboost the performance of the driving model. We conduct experiments in a 3D\nurban simulator and both qualitative and quantitative evaluation results show\nthe effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 08:12:46 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Wang", "Qing", ""], ["Chen", "Long", ""], ["Tian", "Wei", ""]]}, {"id": "1805.07548", "submitter": "Yun Liu", "authors": "Yun Liu, Yujun Shi, JiaWang Bian, Le Zhang, Ming-Ming Cheng, Jiashi\n  Feng", "title": "Learning Pixel-wise Labeling from the Internet without Human Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning stands at the forefront in many computer vision tasks. However,\ndeep neural networks are usually data-hungry and require a huge amount of\nwell-annotated training samples. Collecting sufficient annotated data is very\nexpensive in many applications, especially for pixel-level prediction tasks\nsuch as semantic segmentation. To solve this fundamental issue, we consider a\nnew challenging vision task, Internetly supervised semantic segmentation, which\nonly uses Internet data with noisy image-level supervision of corresponding\nquery keywords for segmentation model training. We address this task by\nproposing the following solution. A class-specific attention model unifying\nmultiscale forward and backward convolutional features is proposed to provide\ninitial segmentation \"ground truth\". The model trained with such noisy\nannotations is then improved by an online fine-tuning procedure. It achieves\nstate-of-the-art performance under the weakly-supervised setting on PASCAL\nVOC2012 dataset. The proposed framework also paves a new way towards learning\nfrom the Internet without human interaction and could serve as a strong\nbaseline therein. Code and data will be released upon the paper acceptance.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 08:33:04 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Liu", "Yun", ""], ["Shi", "Yujun", ""], ["Bian", "JiaWang", ""], ["Zhang", "Le", ""], ["Cheng", "Ming-Ming", ""], ["Feng", "Jiashi", ""]]}, {"id": "1805.07549", "submitter": "Huazhu Fu", "authors": "Huazhu Fu, Jun Cheng, Yanwu Xu, Changqing Zhang, Damon Wing Kee Wong,\n  Jiang Liu, Xiaochun Cao", "title": "Disc-aware Ensemble Network for Glaucoma Screening from Fundus Image", "comments": "Project homepage: https://hzfu.github.io/proj_glaucoma_fundus.html ,\n  and Accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2837012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is a chronic eye disease that leads to irreversible vision loss.\nMost of the existing automatic screening methods firstly segment the main\nstructure, and subsequently calculate the clinical measurement for detection\nand screening of glaucoma. However, these measurement-based methods rely\nheavily on the segmentation accuracy, and ignore various visual features. In\nthis paper, we introduce a deep learning technique to gain additional\nimage-relevant information, and screen glaucoma from the fundus image directly.\nSpecifically, a novel Disc-aware Ensemble Network (DENet) for automatic\nglaucoma screening is proposed, which integrates the deep hierarchical context\nof the global fundus image and the local optic disc region. Four deep streams\non different levels and modules are respectively considered as global image\nstream, segmentation-guided network, local disc region stream, and disc polar\ntransformation stream. Finally, the output probabilities of different streams\nare fused as the final screening result. The experiments on two glaucoma\ndatasets (SCES and new SINDI datasets) show our method outperforms other\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 08:51:00 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Fu", "Huazhu", ""], ["Cheng", "Jun", ""], ["Xu", "Yanwu", ""], ["Zhang", "Changqing", ""], ["Wong", "Damon Wing Kee", ""], ["Liu", "Jiang", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1805.07550", "submitter": "Xiaokai Chen", "authors": "Xiaokai Chen and Ke Gao", "title": "DenseImage Network: Video Spatial-Temporal Evolution Encoding and\n  Understanding", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the leading approaches for video understanding are data-hungry and\ntime-consuming, failing to capture the gist of spatial-temporal evolution in an\nefficient manner. The latest research shows that CNN network can reason about\nstatic relation of entities in images. To further exploit its capacity in\ndynamic evolution reasoning, we introduce a novel network module called\nDenseImage Network(DIN) with two main contributions. 1) A novel compact\nrepresentation of video which distills its significant spatial-temporal\nevolution into a matrix called DenseImage, primed for efficient video encoding.\n2) A simple yet powerful learning strategy based on DenseImage and a\ntemporal-order-preserving CNN network is proposed for video understanding,\nwhich contains a local temporal correlation constraint capturing temporal\nevolution at multiple time scales with different filter widths. Extensive\nexperiments on two recent challenging benchmarks demonstrate that our\nDenseImage Network can accurately capture the common spatial-temporal evolution\nbetween similar actions, even with enormous visual variations or different time\nscales. Moreover, we obtain the state-of-the-art results in action and gesture\nrecognition with much less time-and-memory cost, indicating its immense\npotential in video representing and understanding.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 08:53:44 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chen", "Xiaokai", ""], ["Gao", "Ke", ""]]}, {"id": "1805.07566", "submitter": "Yunus Can Bilge", "authors": "Mehmet Kerim Yucel, Yunus Can Bilge, Oguzhan Oguz, Nazli\n  Ikizler-Cinbis, Pinar Duygulu, Ramazan Gokberk Cinbis", "title": "Wildest Faces: Face Detection and Recognition in Violent Settings", "comments": "Submitted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of large-scale datasets and deep learning models\ncapable of learning complex representations, impressive advances have emerged\nin face detection and recognition tasks. Despite such advances, existing\ndatasets do not capture the difficulty of face recognition in the wildest\nscenarios, such as hostile disputes or fights. Furthermore, existing datasets\ndo not represent completely unconstrained cases of low resolution, high blur\nand large pose/occlusion variances. To this end, we introduce the Wildest Faces\ndataset, which focuses on such adverse effects through violent scenes. The\ndataset consists of an extensive set of violent scenes of celebrities from\nmovies. Our experimental results demonstrate that state-of-the-art techniques\nare not well-suited for violent scenes, and therefore, Wildest Faces is likely\nto stir further interest in face detection and recognition research.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 10:46:24 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Yucel", "Mehmet Kerim", ""], ["Bilge", "Yunus Can", ""], ["Oguz", "Oguzhan", ""], ["Ikizler-Cinbis", "Nazli", ""], ["Duygulu", "Pinar", ""], ["Cinbis", "Ramazan Gokberk", ""]]}, {"id": "1805.07567", "submitter": "Kai Zhao", "authors": "Kai Zhao, Shanghua Gao, Wenguan Wang, Ming-Ming Cheng", "title": "Optimizing the F-measure for Threshold-free Salient Object Detection", "comments": "http://kaizhao.net/fmeasure", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current CNN-based solutions to salient object detection (SOD) mainly rely on\nthe optimization of cross-entropy loss (CELoss). Then the quality of detected\nsaliency maps is often evaluated in terms of F-measure. In this paper, we\ninvestigate an interesting issue: can we consistently use the F-measure\nformulation in both training and evaluation for SOD? By reformulating the\nstandard F-measure we propose the relaxed F-measure which is differentiable\nw.r.t the posterior and can be easily appended to the back of CNNs as the loss\nfunction. Compared to the conventional cross-entropy loss of which the\ngradients decrease dramatically in the saturated area, our loss function, named\nFLoss, holds considerable gradients even when the activation approaches the\ntarget. Consequently, the FLoss can continuously force the network to produce\npolarized activations. Comprehensive benchmarks on several popular datasets\nshow that FLoss outperforms the state-of-the-art with a considerable margin.\nMore specifically, due to the polarized predictions, our method is able to\nobtain high-quality saliency maps without carefully tuning the optimal\nthreshold, showing significant advantages in real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 10:54:43 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 10:06:43 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhao", "Kai", ""], ["Gao", "Shanghua", ""], ["Wang", "Wenguan", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1805.07582", "submitter": "Shuming Jiao", "authors": "Shuming Jiao", "title": "Fast Object Classification in Single-pixel Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single-pixel imaging (SPI), the target object is illuminated with varying\npatterns sequentially and an intensity sequence is recorded by a single-pixel\ndetector without spatial resolution. A high quality object image can only be\ncomputationally reconstructed after a large number of illuminations, with\ndisadvantages of long imaging time and high cost. Conventionally, object\nclassification is performed after a reconstructed object image with good\nfidelity is available. In this paper, we propose to classify the target object\nwith a small number of illuminations in a fast manner for Fourier SPI. A naive\nBayes classifier is employed to classify the target objects based on the\nsingle-pixel intensity sequence without any image reconstruction and each\nsequence element is regarded as an object feature in the classifier. Simulation\nresults demonstrate our proposed scheme can classify the number digit object\nimages with high accuracy (e.g. 80% accuracy using only 13 illuminations, at a\nsampling ratio of 0.3%).\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 12:41:19 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Jiao", "Shuming", ""]]}, {"id": "1805.07588", "submitter": "Qi Qian", "authors": "Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun and Hao Li", "title": "Robust Optimization over Multiple Domains", "comments": "accepted by AAAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of learning a single model for multiple\ndomains. Unlike the conventional machine learning scenario where each domain\ncan have the corresponding model, multiple domains (i.e., applications/users)\nmay share the same machine learning model due to maintenance loads in cloud\ncomputing services. For example, a digit-recognition model should be applicable\nto hand-written digits, house numbers, car plates, etc. Therefore, an ideal\nmodel for cloud computing has to perform well at each applicable domain. To\naddress this new challenge from cloud computing, we develop a framework of\nrobust optimization over multiple domains. In lieu of minimizing the empirical\nrisk, we aim to learn a model optimized to the adversarial distribution over\nmultiple domains. Hence, we propose to learn the model and the adversarial\ndistribution simultaneously with the stochastic algorithm for efficiency.\nTheoretically, we analyze the convergence rate for convex and non-convex\nmodels. To our best knowledge, we first study the convergence rate of learning\na robust non-convex model with a practical algorithm. Furthermore, we\ndemonstrate that the robustness of the framework and the convergence rate can\nbe further enhanced by appropriate regularizers over the adversarial\ndistribution. The empirical study on real-world fine-grained visual\ncategorization and digits recognition tasks verifies the effectiveness and\nefficiency of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 13:19:44 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 19:13:28 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Qian", "Qi", ""], ["Zhu", "Shenghuo", ""], ["Tang", "Jiasheng", ""], ["Jin", "Rong", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""]]}, {"id": "1805.07615", "submitter": "Simiao Yu", "authors": "Simiao Yu, Hao Dong, Pan Wang, Chao Wu, Yike Guo", "title": "Generative Creativity: Adversarial Learning for Bionic Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bionic design refers to an approach of generative creativity in which a\ntarget object (e.g. a floor lamp) is designed to contain features of biological\nsource objects (e.g. flowers), resulting in creative biologically-inspired\ndesign. In this work, we attempt to model the process of shape-oriented bionic\ndesign as follows: given an input image of a design target object, the model\ngenerates images that 1) maintain shape features of the input design target\nimage, 2) contain shape features of images from the specified biological source\ndomain, 3) are plausible and diverse. We propose DesignGAN, a novel\nunsupervised deep generative approach to realising bionic design. Specifically,\nwe employ a conditional Generative Adversarial Networks architecture with\nseveral designated losses (an adversarial loss, a regression loss, a cycle loss\nand a latent loss) that respectively constrict our model to meet the\ncorresponding aforementioned requirements of bionic design modelling. We\nperform qualitative and quantitative experiments to evaluate our method, and\ndemonstrate that our proposed approach successfully generates creative images\nof bionic design.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 15:44:29 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Yu", "Simiao", ""], ["Dong", "Hao", ""], ["Wang", "Pan", ""], ["Wu", "Chao", ""], ["Guo", "Yike", ""]]}, {"id": "1805.07616", "submitter": "Guillem Collell", "authors": "Guillem Collell and Marie-Francine Moens", "title": "Do Neural Network Cross-Modal Mappings Really Bridge Modalities?", "comments": "To appear at ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feed-forward networks are widely used in cross-modal applications to bridge\nmodalities by mapping distributed vectors of one modality to the other, or to a\nshared space. The predicted vectors are then used to perform e.g., retrieval or\nlabeling. Thus, the success of the whole system relies on the ability of the\nmapping to make the neighborhood structure (i.e., the pairwise similarities) of\nthe predicted vectors akin to that of the target vectors. However, whether this\nis achieved has not been investigated yet. Here, we propose a new similarity\nmeasure and two ad hoc experiments to shed light on this issue. In three\ncross-modal benchmarks we learn a large number of language-to-vision and\nvision-to-language neural network mappings (up to five layers) using a rich\ndiversity of image and text features and loss functions. Our results reveal\nthat, surprisingly, the neighborhood structure of the predicted vectors\nconsistently resembles more that of the input vectors than that of the target\nvectors. In a second experiment, we further show that untrained nets do not\nsignificantly disrupt the neighborhood (i.e., semantic) structure of the input\nvectors.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 15:51:43 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:16:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Collell", "Guillem", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1805.07621", "submitter": "Guo-Jun Qi", "authors": "Liheng Zhang, Marzieh Edraki, Guo-Jun Qi", "title": "CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule\n  Subspaces", "comments": "Liheng Zhang, Marzieh Edraki, Guo-Jun Qi. CapProNet: Deep Feature\n  Learning via Orthogonal Projections onto Capsule Subspaces, in Proccedings of\n  Thirty-second Conference on Neural Information Processing Systems (NIPS\n  2018), Palais des Congr\\`es de Montr\\'eal, Montr\\'eal, Canda, December 3-8,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formalize the idea behind capsule nets of using a capsule\nvector rather than a neuron activation to predict the label of samples. To this\nend, we propose to learn a group of capsule subspaces onto which an input\nfeature vector is projected. Then the lengths of resultant capsules are used to\nscore the probability of belonging to different classes. We train such a\nCapsule Projection Network (CapProNet) by learning an orthogonal projection\nmatrix for each capsule subspace, and show that each capsule subspace is\nupdated until it contains input feature vectors corresponding to the associated\nclass. We will also show that the capsule projection can be viewed as\nnormalizing the multiple columns of the weight matrix simultaneously to form an\northogonal basis, which makes it more effective in incorporating novel\ncomponents of input features to update capsule representations. In other words,\nthe capsule projection can be viewed as a multi-dimensional weight\nnormalization in capsule subspaces, where the conventional weight normalization\nis simply a special case of the capsule projection onto 1D lines. Only a small\nnegligible computing overhead is incurred to train the network in\nlow-dimensional capsule subspaces or through an alternative hyper-power\niteration to estimate the normalization matrix. Experiment results on image\ndatasets show the presented model can greatly improve the performance of the\nstate-of-the-art ResNet backbones by $10-20\\%$ and that of the Densenet by\n$5-7\\%$ respectively at the same level of computing and memory expenses. The\nCapProNet establishes the competitive state-of-the-art performance for the\nfamily of capsule nets by significantly reducing test errors on the benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 16:50:37 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 08:07:49 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Liheng", ""], ["Edraki", "Marzieh", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1805.07632", "submitter": "Stefan Sommer", "authors": "Line Kuhnel, Tom Fletcher, Sarang Joshi, Stefan Sommer", "title": "Latent Space Non-Linear Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data, deep generative models, such as variational autoencoders (VAE)\nand generative adversarial networks (GAN), train a lower dimensional latent\nrepresentation of the data space. The linear Euclidean geometry of data space\npulls back to a nonlinear Riemannian geometry on the latent space. The latent\nspace thus provides a low-dimensional nonlinear representation of data and\nclassical linear statistical techniques are no longer applicable. In this paper\nwe show how statistics of data in their latent space representation can be\nperformed using techniques from the field of nonlinear manifold statistics.\nNonlinear manifold statistics provide generalizations of Euclidean statistical\nnotions including means, principal component analysis, and maximum likelihood\nfits of parametric probability distributions. We develop new techniques for\nmaximum likelihood inference in latent space, and adress the computational\ncomplexity of using geometric algorithms with high-dimensional data by training\na separate neural network to approximate the Riemannian metric and cometric\ntensor capturing the shape of the learned data manifold.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 18:05:26 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Kuhnel", "Line", ""], ["Fletcher", "Tom", ""], ["Joshi", "Sarang", ""], ["Sommer", "Stefan", ""]]}, {"id": "1805.07641", "submitter": "Yash Patel", "authors": "Yash Patel, Kashyap Chitta, Bhavan Jasani", "title": "Learning Sampling Policies for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semi-supervised domain adaptation of classification\nalgorithms through deep Q-learning. The core idea is to consider the\npredictions of a source domain network on target domain data as noisy labels,\nand learn a policy to sample from this data so as to maximize classification\naccuracy on a small annotated reward partition of the target domain. Our\nexperiments show that learned sampling policies construct labeled sets that\nimprove accuracies of visual classifiers over baselines.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 19:09:18 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Patel", "Yash", ""], ["Chitta", "Kashyap", ""], ["Jasani", "Bhavan", ""]]}, {"id": "1805.07644", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Jordan W. Suchow, Krisha Aghi, Alexander Y. Ku,\n  Thomas L. Griffiths", "title": "Capturing human category representations by sampling in deep feature\n  spaces", "comments": "6 pages, 5 figures, 1 table. Accepted as a paper to the 40th Annual\n  Meeting of the Cognitive Science Society (CogSci 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how people represent categories is a core problem in cognitive\nscience. Decades of research have yielded a variety of formal theories of\ncategories, but validating them with naturalistic stimuli is difficult. The\nchallenge is that human category representations cannot be directly observed\nand running informative experiments with naturalistic stimuli such as images\nrequires a workable representation of these stimuli. Deep neural networks have\nrecently been successful in solving a range of computer vision tasks and\nprovide a way to compactly represent image features. Here, we introduce a\nmethod to estimate the structure of human categories that combines ideas from\ncognitive science and machine learning, blending human-based algorithms with\nstate-of-the-art deep image generators. We provide qualitative and quantitative\nresults as a proof-of-concept for the method's feasibility. Samples drawn from\nhuman distributions rival those from state-of-the-art generative models in\nquality and outperform alternative methods for estimating the structure of\nhuman categories.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 19:31:48 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Suchow", "Jordan W.", ""], ["Aghi", "Krisha", ""], ["Ku", "Alexander Y.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1805.07646", "submitter": "Elaheh Rashedi", "authors": "Kunlei Zhang, Elaheh Rashedi, Elaheh Barati, Xue-wen Chen", "title": "Long-term face tracking in the wild using deep learning", "comments": "KDD Workshop on Large-scale Deep Learning for Data Mining, August\n  2016, San Fransisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates long-term face tracking of a specific person given\nhis/her face image in a single frame as a query in a video stream. Through\ntaking advantage of pre-trained deep learning models on big data, a novel\nsystem is developed for accurate video face tracking in the unconstrained\nenvironments depicting various people and objects moving in and out of the\nframe. In the proposed system, we present a detection-verification-tracking\nmethod (dubbed as 'DVT') which accomplishes the long-term face tracking task\nthrough the collaboration of face detection, face verification, and\n(short-term) face tracking. An offline trained detector based on cascaded\nconvolutional neural networks localizes all faces appeared in the frames, and\nan offline trained face verifier based on deep convolutional neural networks\nand similarity metric learning decides if any face or which face corresponds to\nthe queried person. An online trained tracker follows the face from frame to\nframe. When validated on a sitcom episode and a TV show, the DVT method\noutperforms tracking-learning-detection (TLD) and face-TLD in terms of recall\nand precision. The proposed system is also tested on many other types of videos\nand shows very promising results.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 20:00:26 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Zhang", "Kunlei", ""], ["Rashedi", "Elaheh", ""], ["Barati", "Elaheh", ""], ["Chen", "Xue-wen", ""]]}, {"id": "1805.07647", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Paul Soulos, Aida Nematzadeh, Thomas L. Griffiths", "title": "Learning Hierarchical Visual Representations in Deep Neural Networks\n  Using Hierarchical Linguistic Labels", "comments": "6 pages, 4 figures, 1 table. Accepted as a paper to the 40th Annual\n  Meeting of the Cognitive Science Society (CogSci 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern convolutional neural networks (CNNs) are able to achieve human-level\nobject classification accuracy on specific tasks, and currently outperform\ncompeting models in explaining complex human visual representations. However,\nthe categorization problem is posed differently for these networks than for\nhumans: the accuracy of these networks is evaluated by their ability to\nidentify single labels assigned to each image. These labels often cut\narbitrarily across natural psychological taxonomies (e.g., dogs are separated\ninto breeds, but never jointly categorized as \"dogs\"), and bias the resulting\nrepresentations. By contrast, it is common for children to hear both \"dog\" and\n\"Dalmatian\" to describe the same stimulus, helping to group perceptually\ndisparate objects (e.g., breeds) into a common mental class. In this work, we\ntrain CNN classifiers with multiple labels for each image that correspond to\ndifferent levels of abstraction, and use this framework to reproduce classic\npatterns that appear in human generalization behavior.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 20:03:20 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Soulos", "Paul", ""], ["Nematzadeh", "Aida", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1805.07648", "submitter": "Vishvak Murahari", "authors": "Vishvak S Murahari, Thomas Ploetz", "title": "On Attention Models for Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most approaches that model time-series data in human activity recognition\nbased on body-worn sensing (HAR) use a fixed size temporal context to represent\ndifferent activities. This might, however, not be apt for sets of activities\nwith individ- ually varying durations. We introduce attention models into HAR\nresearch as a data driven approach for exploring relevant temporal context.\nAttention models learn a set of weights over input data, which we leverage to\nweight the temporal context being considered to model each sensor reading. We\nconstruct attention models for HAR by adding attention layers to a state-\nof-the-art deep learning HAR model (DeepConvLSTM) and evaluate our approach on\nbenchmark datasets achieving sig- nificant increase in performance. Finally, we\nvisualize the learned weights to better understand what constitutes relevant\ntemporal context.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 20:13:05 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Murahari", "Vishvak S", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1805.07653", "submitter": "Joshua Peterson", "authors": "Jordan W. Suchow, Joshua C. Peterson, Thomas L. Griffiths", "title": "Learning a face space for experiments on human identity", "comments": "10 figures. Accepted as a paper to the 40th Annual Meeting of the\n  Cognitive Science Society (CogSci 2018). *JWS and JCP contributed equally to\n  this submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models of human identity and appearance have broad applicability\nto behavioral science and technology, but the exquisite sensitivity of human\nface perception means that their utility hinges on the alignment of the model's\nrepresentation to human psychological representations and the photorealism of\nthe generated images. Meeting these requirements is an exacting task, and\nexisting models of human identity and appearance are often unworkably abstract,\nartificial, uncanny, or biased. Here, we use a variational autoencoder with an\nautoregressive decoder to learn a face space from a uniquely diverse dataset of\nportraits that control much of the variation irrelevant to human identity and\nappearance. Our method generates photorealistic portraits of fictive identities\nwith a smooth, navigable latent space. We validate our model's alignment with\nhuman sensitivities by introducing a psychophysical Turing test for images,\nwhich humans mostly fail. Lastly, we demonstrate an initial application of our\nmodel to the problem of fast search in mental space to obtain detailed \"police\nsketches\" in a small number of trials.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 20:51:49 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Suchow", "Jordan W.", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1805.07663", "submitter": "Stefan Becker", "authors": "Stefan Becker and Ronny Hug and Wolfgang H\\\"ubner and Michael Arens", "title": "An Evaluation of Trajectory Prediction Approaches and Notes on the\n  TrajNet Benchmark", "comments": "Accepted at ECCV Workshop on Anticipating Human Behavior under\n  adapted title. RED: A simple but effective Baseline Predictor for the TrajNet\n  Benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a shift from modeling the tracking problem based on\nBayesian formulation towards using deep neural networks. Towards this end, in\nthis paper the effectiveness of various deep neural networks for predicting\nfuture pedestrian paths are evaluated. The analyzed deep networks solely rely,\nlike in the traditional approaches, on observed tracklets without human-human\ninteraction information. The evaluation is done on the publicly available\nTrajNet benchmark dataset, which builds up a repository of considerable and\npopular datasets for trajectory-based activity forecasting. We show that a\nRecurrent-Encoder with a Dense layer stacked on top, referred to as\nRED-predictor, is able to achieve sophisticated results compared to elaborated\nmodels in such scenarios. Further, we investigate failure cases and give\nexplanations for observed phenomena and give some recommendations for\novercoming demonstrated shortcomings.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 22:08:54 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 21:04:59 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 13:00:37 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 11:40:45 GMT"}, {"version": "v5", "created": "Fri, 29 Jun 2018 08:49:24 GMT"}, {"version": "v6", "created": "Thu, 16 Aug 2018 09:21:13 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Becker", "Stefan", ""], ["Hug", "Ronny", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""]]}, {"id": "1805.07694", "submitter": "Lei Shi", "authors": "Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu", "title": "Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based\n  Action Recognition", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 12026-12035", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In skeleton-based action recognition, graph convolutional networks (GCNs),\nwhich model the human body skeletons as spatiotemporal graphs, have achieved\nremarkable performance. However, in existing GCN-based methods, the topology of\nthe graph is set manually, and it is fixed over all layers and input samples.\nThis may not be optimal for the hierarchical GCN and diverse samples in action\nrecognition tasks. In addition, the second-order information (the lengths and\ndirections of bones) of the skeleton data, which is naturally more informative\nand discriminative for action recognition, is rarely investigated in existing\nmethods. In this work, we propose a novel two-stream adaptive graph\nconvolutional network (2s-AGCN) for skeleton-based action recognition. The\ntopology of the graph in our model can be either uniformly or individually\nlearned by the BP algorithm in an end-to-end manner. This data-driven method\nincreases the flexibility of the model for graph construction and brings more\ngenerality to adapt to various data samples. Moreover, a two-stream framework\nis proposed to model both the first-order and the second-order information\nsimultaneously, which shows notable improvement for the recognition accuracy.\nExtensive experiments on the two large-scale datasets, NTU-RGBD and\nKinetics-Skeleton, demonstrate that the performance of our model exceeds the\nstate-of-the-art with a significant margin.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 02:48:38 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 00:50:14 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 03:09:49 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Shi", "Lei", ""], ["Zhang", "Yifan", ""], ["Cheng", "Jian", ""], ["Lu", "Hanqing", ""]]}, {"id": "1805.07696", "submitter": "Tarique Anwer", "authors": "Redhwan Jamiruddin, Ali Osman Sari, Jahanzaib Shabbir, and Tarique\n  Anwer", "title": "RGB-Depth SLAM Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous Localization and Mapping (SLAM) have made the real-time dense\nreconstruction possible increasing the prospects of navigation, tracking, and\naugmented reality problems. Some breakthroughs have been achieved in this\nregard during past few decades and more remarkable works are still going on.\nThis paper presents an overview of SLAM approaches that have been developed\ntill now. Kinect Fusion algorithm, its variants, and further developed\napproaches are discussed in detailed. The algorithms and approaches are\ncompared for their effectiveness in tracking and mapping based on Root Mean\nSquare error over online available datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 03:20:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Jamiruddin", "Redhwan", ""], ["Sari", "Ali Osman", ""], ["Shabbir", "Jahanzaib", ""], ["Anwer", "Tarique", ""]]}, {"id": "1805.07698", "submitter": "Chun-Guang Li", "authors": "Ruo-Pei Guo, Chun-Guang Li, Yonghua Li, Jiaru Lin, and Jun Guo", "title": "Density-Adaptive Kernel based Efficient Reranking Approaches for Person\n  Reidentification", "comments": "39 pages, 18 figures and 12 tables. This paper is an extended version\n  of our preliminary work on ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person reidentification (ReID) refers to the task of verifying the identity\nof a pedestrian observed from nonoverlapping views in a surveillance camera\nnetwork. It has recently been validated that reranking can achieve remarkable\nperformance improvements in person ReID systems. However, current reranking\napproaches either require feedback from users or suffer from burdensome\ncomputational costs. In this paper, we propose to exploit a density-adaptive\nsmooth kernel technique to achieve efficient and effective reranking.\nSpecifically, we adopt a smooth kernel function to formulate the neighbor\nrelationships among data samples with a density-adaptive parameter. Based on\nthis new formulation, we present two simple yet effective reranking methods,\ntermed \\emph{inverse} density-adaptive kernel based reranking (inv-DAKR) and\n\\emph{bidirectional} density-adaptive kernel based reranking (bi-DAKR), in\nwhich the local density information in the vicinity of each gallery sample is\nelegantly exploited. Moreover, we extend the proposed inv-DAKR and bi-DAKR\nmethods to incorporate the available extra probe samples and demonstrate that\nwhen and why these extra probe samples are able to improve the local\nneighborhood and thus further refine the ranking results. Extensive experiments\nare conducted on six benchmark datasets, including: PRID450s, VIPeR, CUHK03,\nGRID, Market-1501 and Mars. The experimental results demonstrate that our\nproposals are effective and efficient.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 03:44:09 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 04:28:00 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 12:43:54 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Guo", "Ruo-Pei", ""], ["Li", "Chun-Guang", ""], ["Li", "Yonghua", ""], ["Lin", "Jiaru", ""], ["Guo", "Jun", ""]]}, {"id": "1805.07706", "submitter": "Weitang Liu", "authors": "Weitang Liu, Emad Barsoum, John D. Owens", "title": "Object Localization with a Weakly Supervised CapsNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by CapsNet's routing-by-agreement mechanism with its ability to\nlearn object properties, we explore if those properties in turn can determine\nnew properties of the objects, such as the locations. We then propose a CapsNet\narchitecture with object coordinate atoms and a modified routing-by-agreement\nalgorithm with unevenly distributed initial routing probabilities. The model is\nbased on CapsNet but uses a routing algorithm to find the objects' approximate\npositions in the image coordinate system. We also discussed how to derive the\nproperty of translation through coordinate atoms and we show the importance of\nsparse representation. We train our model on the single moving MNIST dataset\nwith class labels. Our model can learn and derive the coordinates of the digits\nbetter than its convolution counterpart that lacks a routing-by-agreement\nalgorithm, and can also perform well when testing on the multi-digit moving\nMNIST and KTH datasets. The results show our method reaches the state-of-art\nperformance on object localization without any extra localization techniques\nand modules as in prior work.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 05:07:27 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 00:21:16 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 18:27:17 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Liu", "Weitang", ""], ["Barsoum", "Emad", ""], ["Owens", "John D.", ""]]}, {"id": "1805.07709", "submitter": "Yiping Lu", "authors": "Xiaoshuai Zhang, Yiping Lu, Jiaying Liu, Bin Dong", "title": "Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control\n  Method for Image Restoration", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new control framework called the moving endpoint\ncontrol to restore images corrupted by different degradation levels in one\nmodel. The proposed control problem contains a restoration dynamics which is\nmodeled by an RNN. The moving endpoint, which is essentially the terminal time\nof the associated dynamics, is determined by a policy network. We call the\nproposed model the dynamically unfolding recurrent restorer (DURR). Numerical\nexperiments show that DURR is able to achieve state-of-the-art performances on\nblind image denoising and JPEG image deblocking. Furthermore, DURR can well\ngeneralize to images with higher degradation levels that are not included in\nthe training stage.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 05:18:28 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 16:12:37 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Zhang", "Xiaoshuai", ""], ["Lu", "Yiping", ""], ["Liu", "Jiaying", ""], ["Dong", "Bin", ""]]}, {"id": "1805.07720", "submitter": "Dinesh Vishwakarma Dr", "authors": "Dinesh Kumar Vishwakarma, Sakshi Upadhyay", "title": "A Deep Structure of Person Re-Identification using Multi-Level Gaussian\n  Models", "comments": "9 pages", "journal-ref": "IEEE Transactions on Multi-Scale Computing Systems 4 (2018) 513 -\n  521", "doi": "10.1109/TMSCS.2018.2870592", "report-no": "8469037", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is being widely used in the forensic, and security\nand surveillance system, but person re-identification is a challenging task in\nreal life scenario. Hence, in this work, a new feature descriptor model has\nbeen proposed using a multilayer framework of Gaussian distribution model on\npixel features, which include color moments, color space values and Schmid\nfilter responses. An image of a person usually consists of distinct body\nregions, usually with differentiable clothing followed by local colors and\ntexture patterns. Thus, the image is evaluated locally by dividing the image\ninto overlapping regions. Each region is further fragmented into a set of local\nGaussians on small patches. A global Gaussian encodes, these local Gaussians\nfor each region creating a multi-level structure. Hence, the global picture of\na person is described by local level information present in it, which is often\nignored. Also, we have analyzed the efficiency of earlier metric learning\nmethods on this descriptor. The performance of the descriptor is evaluated on\nfour public available challenging datasets and the highest accuracy achieved on\nthese datasets are compared with similar state-of-the-arts, which demonstrate\nthe superior performance.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 07:21:52 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Vishwakarma", "Dinesh Kumar", ""], ["Upadhyay", "Sakshi", ""]]}, {"id": "1805.07740", "submitter": "Wenbo Li", "authors": "Shuchen Weng, Wenbo Li, Yi Zhang, Siwei Lyu", "title": "STS Classification with Dual-stream CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structured time series (STS) classification problem requires the modeling\nof interweaved spatiotemporal dependency. most previous STS classification\nmethods model the spatial and temporal dependencies independently. Due to the\ncomplexity of the STS data, we argue that a desirable STS classification method\nshould be a holistic framework that can be made as adaptive and flexible as\npossible. This motivates us to design a deep neural network with such merits.\nInspired by the dual-stream hypothesis in neural science, we propose a novel\ndual-stream framework for modeling the interweaved spatiotemporal dependency,\nand develop a convolutional neural network within this framework that aims to\nachieve high adaptability and flexibility in STS configurations from various\ndiagonals, i.e., sequential order, dependency range and features. The proposed\narchitecture is highly modularized and scalable, making it easy to be adapted\nto specific tasks. The effectiveness of our model is demonstrated through\nexperiments on synthetic data as well as benchmark datasets for skeleton based\nactivity recognition.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 08:59:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Weng", "Shuchen", ""], ["Li", "Wenbo", ""], ["Zhang", "Yi", ""], ["Lyu", "Siwei", ""]]}, {"id": "1805.07777", "submitter": "Yu Li", "authors": "Yu Li, Fan Xu, Fa Zhang, Pingyong Xu, Mingshu Zhang, Ming Fan, Lihua\n  Li, Xin Gao, Renmin Han", "title": "DLBI: Deep learning guided Bayesian inference for structure\n  reconstruction of super-resolution fluorescence microscopy", "comments": "Accepted by ISMB 2018", "journal-ref": "Bioinformatics, Volume 34, Issue 13, 1 July 2018", "doi": "10.1093/bioinformatics/bty241", "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution fluorescence microscopy, with a resolution beyond the\ndiffraction limit of light, has become an indispensable tool to directly\nvisualize biological structures in living cells at a nanometer-scale\nresolution. Despite advances in high-density super-resolution fluorescent\ntechniques, existing methods still have bottlenecks, including extremely long\nexecution time, artificial thinning and thickening of structures, and lack of\nability to capture latent structures. Here we propose a novel deep learning\nguided Bayesian inference approach, DLBI, for the time-series analysis of\nhigh-density fluorescent images. Our method combines the strength of deep\nlearning and statistical inference, where deep learning captures the underlying\ndistribution of the fluorophores that are consistent with the observed\ntime-series fluorescent images by exploring local features and correlation\nalong time-axis, and statistical inference further refines the ultrastructure\nextracted by deep learning and endues physical meaning to the final image.\nComprehensive experimental results on both real and simulated datasets\ndemonstrate that our method provides more accurate and realistic local patch\nand large-field reconstruction than the state-of-the-art method, the 3B\nanalysis, while our method is more than two orders of magnitude faster. The\nmain program is available at https://github.com/lykaust15/DLBI\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 15:28:56 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 05:32:56 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 20:07:42 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Yu", ""], ["Xu", "Fan", ""], ["Zhang", "Fa", ""], ["Xu", "Pingyong", ""], ["Zhang", "Mingshu", ""], ["Fan", "Ming", ""], ["Li", "Lihua", ""], ["Gao", "Xin", ""], ["Han", "Renmin", ""]]}, {"id": "1805.07780", "submitter": "Vik Goel", "authors": "Vik Goel, Jameson Weng, Pascal Poupart", "title": "Unsupervised Video Object Segmentation for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique for deep reinforcement learning that automatically\ndetects moving objects and uses the relevant information for action selection.\nThe detection of moving objects is done in an unsupervised way by exploiting\nstructure from motion. Instead of directly learning a policy from raw images,\nthe agent first learns to detect and segment moving objects by exploiting flow\ninformation in video sequences. The learned representation is then used to\nfocus the policy of the agent on the moving objects. Over time, the agent\nidentifies which objects are critical for decision making and gradually builds\na policy based on relevant moving objects. This approach, which we call\nMotion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of\nAtari games where the ability to detect moving objects reduces the amount of\ninteraction needed with the environment to obtain a good policy. Furthermore,\nthe resulting policy is more interpretable than policies that directly map\nimages to actions or values with a black box neural network. We can gain\ninsight into the policy by inspecting the segmentation and motion of each\nobject detected by the agent. This allows practitioners to confirm whether a\npolicy is making decisions based on sensible information.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 15:45:03 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Goel", "Vik", ""], ["Weng", "Jameson", ""], ["Poupart", "Pascal", ""]]}, {"id": "1805.07785", "submitter": "Ga Wu", "authors": "Ga Wu, Justin Domke, Scott Sanner", "title": "Conditional Inference in Pre-trained Variational Autoencoders via\n  Cross-coding", "comments": "8 pages main content, 4 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders (VAEs) are a popular generative model, but one in\nwhich conditional inference can be challenging. If the decomposition into query\nand evidence variables is fixed, conditional VAEs provide an attractive\nsolution. To support arbitrary queries, one is generally reduced to Markov\nChain Monte Carlo sampling methods that can suffer from long mixing times. In\nthis paper, we propose an idea we term cross-coding to approximate the\ndistribution over the latent variables after conditioning on an evidence\nassignment to some subset of the variables. This allows generating query\nsamples without retraining the full VAE. We experimentally evaluate three\nvariations of cross-coding showing that (i) they can be quickly optimized for\ndifferent decompositions of evidence and query and (ii) they quantitatively and\nqualitatively outperform Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 16:13:09 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 18:31:02 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Wu", "Ga", ""], ["Domke", "Justin", ""], ["Sanner", "Scott", ""]]}, {"id": "1805.07798", "submitter": "Simon Du", "authors": "Simon S. Du, Surbhi Goel", "title": "Improved Learning of One-hidden-layer Convolutional Neural Networks with\n  Overlaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to learn a one-hidden-layer convolutional neural\nnetwork where both the convolutional weights and the outputs weights are\nparameters to be learned. Our algorithm works for a general class of\n(potentially overlapping) patches, including commonly used structures for\ncomputer vision tasks. Our algorithm draws ideas from (1) isotonic regression\nfor learning neural networks and (2) landscape analysis of non-convex matrix\nfactorization problems. We believe these findings may inspire further\ndevelopment in designing provable algorithms for learning neural networks and\nother complex models.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 17:07:25 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 23:29:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Du", "Simon S.", ""], ["Goel", "Surbhi", ""]]}, {"id": "1805.07813", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Alan Wu, Michael S. Ryoo", "title": "Learning Real-World Robot Policies by Dreaming", "comments": null, "journal-ref": "IROS 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to control robots directly based on images is a primary challenge in\nrobotics. However, many existing reinforcement learning approaches require\niteratively obtaining millions of robot samples to learn a policy, which can\ntake significant time. In this paper, we focus on learning a realistic world\nmodel capturing the dynamics of scene changes conditioned on robot actions. Our\ndreaming model can emulate samples equivalent to a sequence of images from the\nactual environment, technically by learning an action-conditioned future\nrepresentation/scene regressor. This allows the agent to learn action policies\n(i.e., visuomotor policies) by interacting with the dreaming model rather than\nthe real-world. We experimentally confirm that our dreaming model enables robot\nlearning of policies that transfer to the real-world.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 19:18:32 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 00:12:16 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2018 12:35:18 GMT"}, {"version": "v4", "created": "Thu, 1 Aug 2019 17:37:41 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Wu", "Alan", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1805.07836", "submitter": "Zhilu Zhang", "authors": "Zhilu Zhang and Mert R. Sabuncu", "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with\n  Noisy Labels", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved tremendous success in a variety of\napplications across many disciplines. Yet, their superior performance comes\nwith the expensive cost of requiring correctly annotated large-scale datasets.\nMoreover, due to DNNs' rich capacity, errors in training labels can hamper\nperformance. To combat this problem, mean absolute error (MAE) has recently\nbeen proposed as a noise-robust alternative to the commonly-used categorical\ncross entropy (CCE) loss. However, as we show in this paper, MAE can perform\npoorly with DNNs and challenging datasets. Here, we present a theoretically\ngrounded set of noise-robust loss functions that can be seen as a\ngeneralization of MAE and CCE. Proposed loss functions can be readily applied\nwith any existing DNN architecture and algorithm, while yielding good\nperformance in a wide range of noisy label scenarios. We report results from\nexperiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and\nsynthetically generated noisy labels.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 23:01:49 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 01:37:13 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 14:44:20 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 22:41:40 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Zhang", "Zhilu", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1805.07857", "submitter": "Rongjie Lai", "authors": "Stefan C. Schonsheck, Bin Dong, Rongjie Lai", "title": "Parallel Transport Convolution: A New Tool for Convolutional Neural\n  Networks on Manifolds", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution has been playing a prominent role in various applications in\nscience and engineering for many years. It is the most important operation in\nconvolutional neural networks. There has been a recent growth of interests of\nresearch in generalizing convolutions on curved domains such as manifolds and\ngraphs. However, existing approaches cannot preserve all the desirable\nproperties of Euclidean convolutions, namely compactly supported filters,\ndirectionality, transferability across different manifolds. In this paper we\ndevelop a new generalization of the convolution operation, referred to as\nparallel transport convolution (PTC), on Riemannian manifolds and their\ndiscrete counterparts. PTC is designed based on the parallel transportation\nwhich is able to translate information along a manifold and to intrinsically\npreserve directionality. PTC allows for the construction of compactly supported\nfilters and is also robust to manifold deformations. This enables us to preform\nwavelet-like operations and to define deep convolutional neural networks on\ncurved domains.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 01:13:20 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 21:36:34 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Schonsheck", "Stefan C.", ""], ["Dong", "Bin", ""], ["Lai", "Rongjie", ""]]}, {"id": "1805.07862", "submitter": "Qingcan Wang", "authors": "Ruying Bao, Sihang Liang, Qingcan Wang", "title": "Featurized Bidirectional GAN: Adversarial Defense via Adversarially\n  Learned Semantic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been demonstrated to be vulnerable to adversarial\nattacks, where small perturbations intentionally added to the original inputs\ncan fool the classifier. In this paper, we propose a defense method, Featurized\nBidirectional Generative Adversarial Networks (FBGAN), to extract the semantic\nfeatures of the input and filter the non-semantic perturbation. FBGAN is\npre-trained on the clean dataset in an unsupervised manner, adversarially\nlearning a bidirectional mapping between the high-dimensional data space and\nthe low-dimensional semantic space; also mutual information is applied to\ndisentangle the semantically meaningful features. After the bidirectional\nmapping, the adversarial data can be reconstructed to denoised data, which\ncould be fed into any pre-trained classifier. We empirically show the quality\nof reconstruction images and the effectiveness of defense.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 01:49:18 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 20:30:06 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bao", "Ruying", ""], ["Liang", "Sihang", ""], ["Wang", "Qingcan", ""]]}, {"id": "1805.07872", "submitter": "Naveed Akhtar Dr.", "authors": "Huan Lei, Naveed Akhtar, Ajmal Mian", "title": "Spherical Convolutional Neural Network for 3D Point Clouds", "comments": "Submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network for 3D point cloud processing that exploits\n`spherical' convolution kernels and octree partitioning of space. The proposed\nmetric-based spherical kernels systematically quantize point neighborhoods to\nidentify local geometric structures in data, while maintaining the properties\nof translation-invariance and asymmetry. The network architecture itself is\nguided by octree data structuring that takes full advantage of the sparse\nnature of irregular point clouds. We specify spherical kernels with the help of\nneurons in each layer that in turn are associated with spatial locations. We\nexploit this association to avert dynamic kernel generation during network\ntraining, that enables efficient learning with high resolution point clouds. We\ndemonstrate the utility of the spherical convolutional neural network for 3D\nobject classification on standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 02:32:31 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 05:27:34 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Lei", "Huan", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1805.07883", "submitter": "Yining Wang", "authors": "Simon S. Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan\n  Salakhutdinov, Aarti Singh", "title": "How Many Samples are Needed to Estimate a Convolutional or Recurrent\n  Neural Network?", "comments": "Revised version, with new results on recurrent neural networks.\n  Preliminary version in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the practical success of Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs) owes to the fact that CNNs\nand RNNs use a more compact parametric representation than their\nFully-Connected Neural Network (FNN) counterparts, and consequently require\nfewer training examples to accurately estimate their parameters. We initiate\nthe study of rigorously characterizing the sample-complexity of estimating CNNs\nand RNNs. We show that the sample-complexity to learn CNNs and RNNs scales\nlinearly with their intrinsic dimension and this sample-complexity is much\nsmaller than for their FNN counterparts. For both CNNs and RNNs, we also\npresent lower bounds showing our sample complexities are tight up to\nlogarithmic factors. Our main technical tools for deriving these results are a\nlocalized empirical process analysis and a new technical lemma characterizing\nthe convolutional and recurrent structure. We believe that these tools may\ninspire further developments in understanding CNNs and RNNs.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 03:56:17 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 04:18:24 GMT"}, {"version": "v3", "created": "Sun, 30 Jun 2019 00:24:50 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Zhai", "Xiyu", ""], ["Balakrishnan", "Sivaraman", ""], ["Salakhutdinov", "Ruslan", ""], ["Singh", "Aarti", ""]]}, {"id": "1805.07888", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Daniel McDuff", "title": "DeepPhys: Video-Based Physiological Measurement Using Convolutional\n  Attention Networks", "comments": "Accepted paper at ECCV 2018. 16 pages, 3 figures, supplementary\n  materials in the ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-contact video-based physiological measurement has many applications in\nhealth care and human-computer interaction. Practical applications require\nmeasurements to be accurate even in the presence of large head rotations. We\npropose the first end-to-end system for video-based measurement of heart and\nbreathing rate using a deep convolutional network. The system features a new\nmotion representation based on a skin reflection model and a new attention\nmechanism using appearance information to guide motion estimation, both of\nwhich enable robust measurement under heterogeneous lighting and major motions.\nOur approach significantly outperforms all current state-of-the-art methods on\nboth RGB and infrared video datasets. Furthermore, it allows spatial-temporal\ndistributions of physiological signals to be visualized via the attention\nmechanism.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 04:42:42 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 19:25:30 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Chen", "Weixuan", ""], ["McDuff", "Daniel", ""]]}, {"id": "1805.07894", "submitter": "Yang Song", "authors": "Yang Song, Rui Shu, Nate Kushman, Stefano Ermon", "title": "Constructing Unrestricted Adversarial Examples with Generative Models", "comments": "Neural Information Processing Systems (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are typically constructed by perturbing an existing data\npoint within a small matrix norm, and current defense methods are focused on\nguarding against this type of attack. In this paper, we propose unrestricted\nadversarial examples, a new threat model where the attackers are not restricted\nto small norm-bounded perturbations. Different from perturbation-based attacks,\nwe propose to synthesize unrestricted adversarial examples entirely from\nscratch using conditional generative models. Specifically, we first train an\nAuxiliary Classifier Generative Adversarial Network (AC-GAN) to model the\nclass-conditional distribution over data samples. Then, conditioned on a\ndesired class, we search over the AC-GAN latent space to find images that are\nlikely under the generative model and are misclassified by a target classifier.\nWe demonstrate through human evaluation that unrestricted adversarial examples\ngenerated this way are legitimate and belong to the desired class. Our\nempirical results on the MNIST, SVHN, and CelebA datasets show that\nunrestricted adversarial examples can bypass strong adversarial training and\ncertified defense methods designed for traditional adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 05:19:08 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 03:55:54 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 05:46:09 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 22:18:56 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Song", "Yang", ""], ["Shu", "Rui", ""], ["Kushman", "Nate", ""], ["Ermon", "Stefano", ""]]}, {"id": "1805.07903", "submitter": "Maryam Sultana", "authors": "Maryam Sultana, Arif Mahmood, Sajid Javed and Soon Ki Jung", "title": "Unsupervised Deep Context Prediction for Background Foreground\n  Separation", "comments": "17 pages", "journal-ref": "Machine Vision and Applications 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many advanced video based applications background modeling is a\npre-processing step to eliminate redundant data, for instance in tracking or\nvideo surveillance applications. Over the past years background subtraction is\nusually based on low level or hand-crafted features such as raw color\ncomponents, gradients, or local binary patterns. The background subtraction\nalgorithms performance suffer in the presence of various challenges such as\ndynamic backgrounds, photometric variations, camera jitters, and shadows. To\nhandle these challenges for the purpose of accurate background modeling we\npropose a unified framework based on the algorithm of image inpainting. It is\nan unsupervised visual feature learning hybrid Generative Adversarial algorithm\nbased on context prediction. We have also presented the solution of random\nregion inpainting by the fusion of center region inpaiting and random region\ninpainting with the help of poisson blending technique. Furthermore we also\nevaluated foreground object detection with the fusion of our proposed method\nand morphological operations. The comparison of our proposed method with 12\nstate-of-the-art methods shows its stability in the application of background\nestimation and foreground detection.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 06:12:15 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Sultana", "Maryam", ""], ["Mahmood", "Arif", ""], ["Javed", "Sajid", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1805.07905", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa", "title": "Class Representative Autoencoder for Low Resolution Multi-Spectral\n  Gender Classification", "comments": "Published at IJCNN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender is one of the most common attributes used to describe an individual.\nIt is used in multiple domains such as human computer interaction, marketing,\nsecurity, and demographic reports. Research has been performed to automate the\ntask of gender recognition in constrained environment using face images,\nhowever, limited attention has been given to gender classification in\nunconstrained scenarios. This work attempts to address the challenging problem\nof gender classification in multi-spectral low resolution face images. We\npropose a robust Class Representative Autoencoder model, termed as AutoGen for\nthe same. The proposed model aims to minimize the intra-class variations while\nmaximizing the inter-class variations for the learned feature representations.\nResults on visible as well as near infrared spectrum data for different\nresolutions and multiple databases depict the efficacy of the proposed model.\nComparative results with existing approaches and two commercial off-the-shelf\nsystems further motivate the use of class representative features for\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 06:26:29 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1805.07920", "submitter": "Qingshan Xu", "authors": "Qingshan Xu, Wenbing Tao", "title": "Multi-View Stereo with Asymmetric Checkerboard Propagation and\n  Multi-Hypothesis Joint View Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision domain, how to fast and accurately perform multiview\nstereo (MVS) is still a challenging problem. In this paper we present a fast\nyet accurate method for 3D dense reconstruction, called AMHMVS, built on the\nPatchMatch based stereo algorithm. Different from the regular symmetric\npropagation scheme, our approach adopts an asymmetric checkerboard propagation\nstrategy, which can adaptively make effective hypotheses expand further\naccording to the confidence of current neighbor hypotheses. In order to\naggregate visual information from multiple images better, we propose the\nmulti-hypothesis joint view selection for each pixel, which leverages a cost\nmatrix based on the multiple propagated hypotheses to robustly infer an\nappropriate aggregation subset parallel. Combined with the above two steps, our\napproach not only has the capacity of massively parallel computation, but also\nobtains high accuracy and completeness. Experiments on extensive datasets show\nthat our method achieves more accurate and robust results, and runs faster than\nthe competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 07:10:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Xu", "Qingshan", ""], ["Tao", "Wenbing", ""]]}, {"id": "1805.07925", "submitter": "Hyeonseob Nam", "authors": "Hyeonseob Nam and Hyo-Eun Kim", "title": "Batch-Instance Normalization for Adaptively Style-Invariant Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world image recognition is often challenged by the variability of visual\nstyles including object textures, lighting conditions, filter effects, etc.\nAlthough these variations have been deemed to be implicitly handled by more\ntraining data and deeper networks, recent advances in image style transfer\nsuggest that it is also possible to explicitly manipulate the style\ninformation. Extending this idea to general visual recognition problems, we\npresent Batch-Instance Normalization (BIN) to explicitly normalize unnecessary\nstyles from images. Considering certain style features play an essential role\nin discriminative tasks, BIN learns to selectively normalize only disturbing\nstyles while preserving useful styles. The proposed normalization module is\neasily incorporated into existing network architectures such as Residual\nNetworks, and surprisingly improves the recognition performance in various\nscenarios. Furthermore, experiments verify that BIN effectively adapts to\ncompletely different tasks like object classification and style transfer, by\ncontrolling the trade-off between preserving and removing style variations. BIN\ncan be implemented with only a few lines of code using popular deep learning\nframeworks.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 07:30:26 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 02:50:00 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 08:02:35 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Nam", "Hyeonseob", ""], ["Kim", "Hyo-Eun", ""]]}, {"id": "1805.07931", "submitter": "Marco Cannici", "authors": "Marco Cannici, Marco Ciccone, Andrea Romanoni, Matteo Matteucci", "title": "Asynchronous Convolutional Networks for Object Detection in Neuromorphic\n  Cameras", "comments": "accepted at CVPR2019 Event-based Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras, also known as neuromorphic cameras, are bioinspired\nsensors able to perceive changes in the scene at high frequency with low power\nconsumption. Becoming available only very recently, a limited amount of work\naddresses object detection on these devices. In this paper we propose two\nneural networks architectures for object detection: YOLE, which integrates the\nevents into surfaces and uses a frame-based model to process them, and fcYOLE,\nan asynchronous event-based fully convolutional network which uses a novel and\ngeneral formalization of the convolutional and max pooling layers to exploit\nthe sparsity of camera events. We evaluate the algorithm with different\nextensions of publicly available datasets and on a novel synthetic dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 07:53:26 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 18:36:46 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 14:17:55 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Cannici", "Marco", ""], ["Ciccone", "Marco", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1805.07932", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang", "title": "Bilinear Attention Networks", "comments": "Accepted by NIPS 2018; Figure 1 was updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention networks in multimodal learning provide an efficient way to utilize\ngiven visual information selectively. However, the computational cost to learn\nattention distributions for every pair of multimodal input channels is\nprohibitively expensive. To solve this problem, co-attention builds two\nseparate attention distributions for each modality neglecting the interaction\nbetween multimodal inputs. In this paper, we propose bilinear attention\nnetworks (BAN) that find bilinear attention distributions to utilize given\nvision-language information seamlessly. BAN considers bilinear interactions\namong two groups of input channels, while low-rank bilinear pooling extracts\nthe joint representations for each pair of channels. Furthermore, we propose a\nvariant of multimodal residual networks to exploit eight-attention maps of the\nBAN efficiently. We quantitatively and qualitatively evaluate our model on\nvisual question answering (VQA 2.0) and Flickr30k Entities datasets, showing\nthat BAN significantly outperforms previous methods and achieves new\nstate-of-the-arts on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 07:58:31 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 11:29:49 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Jun", "Jaehyun", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1805.07935", "submitter": "Yuan Cheng Ph.D", "authors": "Yuan Cheng, Guangya Li, Hai-Bao Chen, Sheldon X.-D. Tan and Hao Yu", "title": "DEEPEYE: A Compact and Accurate Video Comprehension at Terminal Devices\n  Compressed with Quantization and Tensorization", "comments": "10 pages, 9 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As it requires a huge number of parameters when exposed to high dimensional\ninputs in video detection and classification, there is a grand challenge to\ndevelop a compact yet accurate video comprehension at terminal devices. Current\nworks focus on optimizations of video detection and classification in a\nseparated fashion. In this paper, we introduce a video comprehension (object\ndetection and action recognition) system for terminal devices, namely DEEPEYE.\nBased on You Only Look Once (YOLO), we have developed an 8-bit quantization\nmethod when training YOLO; and also developed a tensorized-compression method\nof Recurrent Neural Network (RNN) composed of features extracted from YOLO. The\ndeveloped quantization and tensorization can significantly compress the\noriginal network model yet with maintained accuracy. Using the challenging\nvideo datasets: MOMENTS and UCF11 as benchmarks, the results show that the\nproposed DEEPEYE achieves 3.994x model compression rate with only 0.47% mAP\ndecreased; and 15,047x parameter reduction and 2.87x speed-up with 16.58%\naccuracy improvement.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 08:08:07 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 16:17:33 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cheng", "Yuan", ""], ["Li", "Guangya", ""], ["Chen", "Hai-Bao", ""], ["Tan", "Sheldon X. -D.", ""], ["Yu", "Hao", ""]]}, {"id": "1805.07936", "submitter": "Qi Zheng", "authors": "Qi Zheng, Shujian Yu, Xinge You, Qinmu Peng", "title": "Coarse-to-Fine Salient Object Detection with Low-Rank Matrix Recovery", "comments": "Manuscript accepted by Neurocomputing, matlab code is available from\n  https://github.com/qizhust/HLRSaliency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low-Rank Matrix Recovery (LRMR) has recently been applied to saliency\ndetection by decomposing image features into a low-rank component associated\nwith background and a sparse component associated with visual salient regions.\nDespite its great potential, existing LRMR-based saliency detection methods\nseldom consider the inter-relationship among elements within these two\ncomponents, thus are prone to generating scattered or incomplete saliency maps.\nIn this paper, we introduce a novel and efficient LRMR-based saliency detection\nmodel under a coarse-to-fine framework to circumvent this limitation. First, we\nroughly measure the saliency of image regions with a baseline LRMR model that\nintegrates a $\\ell_1$-norm sparsity constraint and a Laplacian regularization\nsmooth term. Given samples from the coarse saliency map, we then learn a\nprojection that maps image features to refined saliency values, to\nsignificantly sharpen the object boundaries and to preserve the object\nentirety. We evaluate our framework against existing LRMR-based methods on\nthree benchmark datasets. Experimental results validate the superiority of our\nmethod as well as the effectiveness of our suggested coarse-to-fine framework,\nespecially for images containing multiple objects.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 08:08:55 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 03:17:19 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 05:39:33 GMT"}, {"version": "v4", "created": "Mon, 9 Sep 2019 01:50:50 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zheng", "Qi", ""], ["Yu", "Shujian", ""], ["You", "Xinge", ""], ["Peng", "Qinmu", ""]]}, {"id": "1805.07941", "submitter": "Sean O. Settle", "authors": "Sean O. Settle, Manasa Bollavaram, Paolo D'Alberto, Elliott Delaye,\n  Oscar Fernandez, Nicholas Fraser, Aaron Ng, Ashish Sirasao, Michael Wu", "title": "Quantizing Convolutional Neural Networks for Low-Power High-Throughput\n  Inference Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning as a means to inferencing has proliferated thanks to its\nversatility and ability to approach or exceed human-level accuracy. These\ncomputational models have seemingly insatiable appetites for computational\nresources not only while training, but also when deployed at scales ranging\nfrom data centers all the way down to embedded devices. As such, increasing\nconsideration is being made to maximize the computational efficiency given\nlimited hardware and energy resources and, as a result, inferencing with\nreduced precision has emerged as a viable alternative to the IEEE 754 Standard\nfor Floating-Point Arithmetic. We propose a quantization scheme that allows\ninferencing to be carried out using arithmetic that is fundamentally more\nefficient when compared to even half-precision floating-point. Our quantization\nprocedure is significant in that we determine our quantization scheme\nparameters by calibrating against its reference floating-point model using a\nsingle inference batch rather than (re)training and achieve end-to-end post\nquantization accuracies comparable to the reference model.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 08:31:46 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Settle", "Sean O.", ""], ["Bollavaram", "Manasa", ""], ["D'Alberto", "Paolo", ""], ["Delaye", "Elliott", ""], ["Fernandez", "Oscar", ""], ["Fraser", "Nicholas", ""], ["Ng", "Aaron", ""], ["Sirasao", "Ashish", ""], ["Wu", "Michael", ""]]}, {"id": "1805.07962", "submitter": "Aritra Dutta", "authors": "Aritra Dutta and Filip Hanzely and Peter Richt\\'arik", "title": "A Nonconvex Projection Method for Robust PCA", "comments": "In the proceedings of Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": "In the proceedings of Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19), 33(01), pp. 1468-1476, 2019", "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust principal component analysis (RPCA) is a well-studied problem with the\ngoal of decomposing a matrix into the sum of low-rank and sparse components. In\nthis paper, we propose a nonconvex feasibility reformulation of RPCA problem\nand apply an alternating projection method to solve it. To the best of our\nknowledge, we are the first to propose a method that solves RPCA problem\nwithout considering any objective function, convex relaxation, or surrogate\nconvex constraints. We demonstrate through extensive numerical experiments on a\nvariety of applications, including shadow removal, background estimation, face\ndetection, and galaxy evolution, that our approach matches and often\nsignificantly outperforms current state-of-the-art in various ways.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 09:49:06 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 16:20:55 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Dutta", "Aritra", ""], ["Hanzely", "Filip", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1805.07997", "submitter": "Sitao Xiang", "authors": "Sitao Xiang, Hao Li", "title": "Anime Style Space Exploration Using Metric Learning and Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based style transfer between images has recently become a\npopular area of research. A common way of encoding \"style\" is through a feature\nrepresentation based on the Gram matrix of features extracted by some\npre-trained neural network or some other form of feature statistics. Such a\ndefinition is based on an arbitrary human decision and may not best capture\nwhat a style really is. In trying to gain a better understanding of \"style\", we\npropose a metric learning-based method to explicitly encode the style of an\nartwork. In particular, our definition of style captures the differences\nbetween artists, as shown by classification performances, and such that the\nstyle representation can be interpreted, manipulated and visualized through\nstyle-conditioned image generation through a Generative Adversarial Network. We\nemploy this method to explore the style space of anime portrait illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 11:43:12 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Xiang", "Sitao", ""], ["Li", "Hao", ""]]}, {"id": "1805.08000", "submitter": "Zhonghui You", "authors": "Zhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, Ping Wang", "title": "Adversarial Noise Layer: Regularize Neural Network By Adding Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel regularization method called Adversarial\nNoise Layer (ANL) and its efficient version called Class Adversarial Noise\nLayer (CANL), which are able to significantly improve CNN's generalization\nability by adding carefully crafted noise into the intermediate layer\nactivations. ANL and CANL can be easily implemented and integrated with most of\nthe mainstream CNN-based models. We compared the effects of the different types\nof noise and visually demonstrate that our proposed adversarial noise instruct\nCNN models to learn to extract cleaner feature maps, which further reduce the\nrisk of over-fitting. We also conclude that models trained with ANL or CANL are\nmore robust to the adversarial examples generated by FGSM than the traditional\nadversarial training approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 11:50:59 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 03:02:45 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["You", "Zhonghui", ""], ["Ye", "Jinmian", ""], ["Li", "Kunming", ""], ["Xu", "Zenglin", ""], ["Wang", "Ping", ""]]}, {"id": "1805.08009", "submitter": "Wenyan Yang", "authors": "Wenyan Yang, Yanlin Qian, Francesco Cricri, Lixin Fan, Joni-Kristian\n  Kamarainen", "title": "Object Detection in Equirectangular Panorama", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduced a high-resolution equirectangular panorama (360-degree, virtual\nreality) dataset for object detection and propose a multi-projection variant of\nYOLO detector. The main challenge with equirectangular panorama image are i)\nthe lack of annotated training data, ii) high-resolution imagery and iii)\nsevere geometric distortions of objects near the panorama projection poles. In\nthis work, we solve the challenges by i) using training examples available in\nthe \"conventional datasets\" (ImageNet and COCO), ii) employing only\nlow-resolution images that require only moderate GPU computing power and\nmemory, and iii) our multi-projection YOLO handles projection distortions by\nmaking multiple stereographic sub-projections. In our experiments, YOLO\noutperforms the other state-of-art detector, Faster RCNN and our\nmulti-projection YOLO achieves the best accuracy with low-resolution input.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:11:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Yang", "Wenyan", ""], ["Qian", "Yanlin", ""], ["Cricri", "Francesco", ""], ["Fan", "Lixin", ""], ["Kamarainen", "Joni-Kristian", ""]]}, {"id": "1805.08015", "submitter": "Peng Jiang", "authors": "Peng Jiang and Fanglin Gu and Yunhai Wang and Changhe Tu and Baoquan\n  Chen", "title": "DifNet: Semantic Segmentation by Diffusion Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently shown state of the art performance\non semantic segmentation tasks, however, they still suffer from problems of\npoor boundary localization and spatial fragmented predictions. The difficulties\nlie in the requirement of making dense predictions from a long path model all\nat once since details are hard to keep when data goes through deeper layers.\nInstead, in this work, we decompose this difficult task into two relative\nsimple sub-tasks: seed detection which is required to predict initial\npredictions without the need of wholeness and preciseness, and similarity\nestimation which measures the possibility of any two nodes belong to the same\nclass without the need of knowing which class they are. We use one branch\nnetwork for one sub-task each, and apply a cascade of random walks base on\nhierarchical semantics to approximate a complex diffusion process which\npropagates seed information to the whole image according to the estimated\nsimilarities. The proposed DifNet consistently produces improvements over the\nbaseline models with the same depth and with the equivalent number of\nparameters, and also achieves promising performance on Pascal VOC and Pascal\nContext dataset. OurDifNet is trained end-to-end without complex loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:29:02 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 02:38:00 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 06:53:12 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 16:41:39 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Jiang", "Peng", ""], ["Gu", "Fanglin", ""], ["Wang", "Yunhai", ""], ["Tu", "Changhe", ""], ["Chen", "Baoquan", ""]]}, {"id": "1805.08019", "submitter": "Jinming Cao", "authors": "Jinming Cao, Oren Katzir, Peng Jiang, Dani Lischinski, Danny Cohen-Or,\n  Changhe Tu, Yangyan Li", "title": "DiDA: Disentangled Synthesis for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims at learning a shared model for two\nrelated, but not identical, domains by leveraging supervision from a source\ndomain to an unsupervised target domain. A number of effective domain\nadaptation approaches rely on the ability to extract discriminative, yet\ndomain-invariant, latent factors which are common to both domains. Extracting\nlatent commonality is also useful for disentanglement analysis, enabling\nseparation between the common and the domain-specific features of both domains.\nIn this paper, we present a method for boosting domain adaptation performance\nby leveraging disentanglement analysis. The key idea is that by learning to\nseparately extract both the common and the domain-specific features, one can\nsynthesize more target domain data with supervision, thereby boosting the\ndomain adaptation performance. Better common feature extraction, in turn, helps\nfurther improve the disentanglement analysis and disentangled synthesis. We\nshow that iterating between domain adaptation and disentanglement analysis can\nconsistently improve each other on several unsupervised domain adaptation\ntasks, for various domain adaptation backbone models.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:43:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Cao", "Jinming", ""], ["Katzir", "Oren", ""], ["Jiang", "Peng", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Danny", ""], ["Tu", "Changhe", ""], ["Li", "Yangyan", ""]]}, {"id": "1805.08090", "submitter": "Saurabh Verma", "authors": "Saurabh Verma, Zhi-Li Zhang", "title": "Graph Capsule Convolutional Neural Networks", "comments": "Accepted at Joint ICML and IJCAI Workshop on Computational Biology,\n  Stockholm, Sweden, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Convolutional Neural Networks (GCNNs) are the most recent exciting\nadvancement in deep learning field and their applications are quickly spreading\nin multi-cross-domains including bioinformatics, chemoinformatics, social\nnetworks, natural language processing and computer vision. In this paper, we\nexpose and tackle some of the basic weaknesses of a GCNN model with a capsule\nidea presented in \\cite{hinton2011transforming} and propose our Graph Capsule\nNetwork (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve\nespecially graph classification problem which current GCNN models find\nchallenging. Through extensive experiments, we show that our proposed Graph\nCapsule Network can significantly outperforms both the existing state-of-art\ndeep learning methods and graph kernels on graph classification benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 14:38:31 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 20:51:00 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 14:25:12 GMT"}, {"version": "v4", "created": "Sun, 26 Aug 2018 00:13:38 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Verma", "Saurabh", ""], ["Zhang", "Zhi-Li", ""]]}, {"id": "1805.08094", "submitter": "Jun Liu", "authors": "Faqiang Wang, Haiyang Huang, Jun Liu", "title": "Variational based Mixed Noise Removal with CNN Deep Learning\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the traditional model based variational method and learning\nbased algorithms are naturally integrated to address mixed noise removal\nproblem. To be different from single type noise (e.g. Gaussian) removal, it is\na challenge problem to accurately discriminate noise types and levels for each\npixel. We propose a variational method to iteratively estimate the noise\nparameters, and then the algorithm can automatically classify the noise\naccording to the different statistical parameters. The proposed variational\nproblem can be separated into regularization, synthesis, parameter estimation\nand noise classification four steps with the operator splitting scheme. Each\nstep is related to an optimization subproblem. To enforce the regularization,\nthe deep learning method is employed to learn the natural images priori.\nCompared with some model based regularizations, the CNN regularizer can\nsignificantly improve the quality of the restored images. Compared with some\nlearning based methods, the synthesis step can produce better reconstructions\nby analyzing the recognized noise types and levels. In our method, the\nconvolution neutral network (CNN) can be regarded as an operator which\nassociated to a variational functional. From this viewpoint, the proposed\nmethod can be extended to many image reconstruction and inverse problems.\nNumerical experiments in the paper show that our method can achieve some\nstate-of-the-art results for mixed noise removal.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 14:52:06 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Wang", "Faqiang", ""], ["Huang", "Haiyang", ""], ["Liu", "Jun", ""]]}, {"id": "1805.08095", "submitter": "Jo\\~ao F. Henriques", "authors": "Jo\\~ao F. Henriques, Sebastien Ehrhardt, Samuel Albanie, Andrea\n  Vedaldi", "title": "Small steps and giant leaps: Minimal Newton solvers for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast second-order method that can be used as a drop-in\nreplacement for current deep learning solvers. Compared to stochastic gradient\ndescent (SGD), it only requires two additional forward-mode automatic\ndifferentiation operations per iteration, which has a computational cost\ncomparable to two standard forward passes and is easy to implement. Our method\naddresses long-standing issues with current second-order solvers, which invert\nan approximate Hessian matrix every iteration exactly or by conjugate-gradient\nmethods, a procedure that is both costly and sensitive to noise. Instead, we\npropose to keep a single estimate of the gradient projected by the inverse\nHessian matrix, and update it once per iteration. This estimate has the same\nsize and is similar to the momentum variable that is commonly used in SGD. No\nestimate of the Hessian is maintained. We first validate our method, called\nCurveBall, on small problems with known closed-form solutions (noisy Rosenbrock\nfunction and degenerate 2-layer linear networks), where current deep learning\nsolvers seem to struggle. We then train several large models on CIFAR and\nImageNet, including ResNet and VGG-f networks, where we demonstrate faster\nconvergence with no hyperparameter tuning. Code is available.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 14:54:28 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Henriques", "Jo\u00e3o F.", ""], ["Ehrhardt", "Sebastien", ""], ["Albanie", "Samuel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1805.08105", "submitter": "Martin Cadik", "authors": "Touqeer Ahmad and Pavel Campr and Martin \\v{C}ad\\'ik and George Bebis", "title": "Comparison of Semantic Segmentation Approaches for Horizon/Sky Line\n  Detection", "comments": "Proceedings of the International Joint Conference on Neural Networks\n  (IJCNN) (oral presentation), IEEE Computational Intelligence Society, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Horizon or skyline detection plays a vital role towards mountainous visual\ngeo-localization, however most of the recently proposed visual geo-localization\napproaches rely on \\textbf{user-in-the-loop} skyline detection methods.\nDetecting such a segmenting boundary fully autonomously would definitely be a\nstep forward for these localization approaches. This paper provides a\nquantitative comparison of four such methods for autonomous horizon/sky line\ndetection on an extensive data set. Specifically, we provide the comparison\nbetween four recently proposed segmentation methods; one explicitly targeting\nthe problem of horizon detection\\cite{Ahmad15}, second focused on visual\ngeo-localization but relying on accurate detection of skyline \\cite{Saurer16}\nand other two proposed for general semantic segmentation -- Fully Convolutional\nNetworks (FCN) \\cite{Long15} and SegNet\\cite{Badrinarayanan15}. Each of the\nfirst two methods is trained on a common training set \\cite{Baatz12} comprised\nof about 200 images while models for the third and fourth method are fine tuned\nfor sky segmentation problem through transfer learning using the same data set.\nEach of the method is tested on an extensive test set (about 3K images)\ncovering various challenging geographical, weather, illumination and seasonal\nconditions. We report average accuracy and average absolute pixel error for\neach of the presented formulation.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:03:19 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Ahmad", "Touqeer", ""], ["Campr", "Pavel", ""], ["\u010cad\u00edk", "Martin", ""], ["Bebis", "George", ""]]}, {"id": "1805.08113", "submitter": "Yunlong Yu", "authors": "Yunlong Yu, Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei\n  Zhang", "title": "Stacked Semantic-Guided Attention Model for Fine-Grained Zero-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) is achieved via aligning the semantic relationships\nbetween the global image feature vector and the corresponding class semantic\ndescriptions. However, using the global features to represent fine-grained\nimages may lead to sub-optimal results since they neglect the discriminative\ndifferences of local regions. Besides, different regions contain distinct\ndiscriminative information. The important regions should contribute more to the\nprediction. To this end, we propose a novel stacked semantics-guided attention\n(S2GA) model to obtain semantic relevant features by using individual class\nsemantic features to progressively guide the visual features to generate an\nattention map for weighting the importance of different local regions. Feeding\nboth the integrated visual features and the class semantic features into a\nmulti-class classification architecture, the proposed framework can be trained\nend-to-end. Extensive experimental results on CUB and NABird datasets show that\nthe proposed approach has a consistent improvement on both fine-grained\nzero-shot classification and retrieval tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:12:19 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Yu", "Yunlong", ""], ["Ji", "Zhong", ""], ["Fu", "Yanwei", ""], ["Guo", "Jichang", ""], ["Pang", "Yanwei", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1805.08136", "submitter": "Luca Bertinetto", "authors": "Luca Bertinetto, Jo\\~ao F. Henriques, Philip H.S. Torr, Andrea Vedaldi", "title": "Meta-learning with differentiable closed-form solvers", "comments": "Published at ICLR'19. Code and data available at\n  http://www.robots.ox.ac.uk/~luca/r2d2.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting deep networks to new concepts from a few examples is challenging,\ndue to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques\nfor adaptation, such as nearest neighbours or gradient descent. Nonetheless,\nthe machine learning literature contains a wealth of methods that learn\nnon-deep models very efficiently. In this paper, we propose to use these fast\nconvergent methods as the main adaptation mechanism for few-shot learning. The\nmain idea is to teach a deep network to use standard machine learning tools,\nsuch as ridge regression, as part of its own internal model, enabling it to\nquickly adapt to novel data. This requires back-propagating errors through the\nsolver steps. While normally the cost of the matrix operations involved in such\na process would be significant, by using the Woodbury identity we can make the\nsmall number of examples work to our advantage. We propose both closed-form and\niterative solvers, based on ridge regression and logistic regression\ncomponents. Our methods constitute a simple and novel approach to the problem\nof few-shot learning and achieve performance competitive with or superior to\nthe state of the art on three benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:44:51 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 18:21:35 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 14:43:31 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Bertinetto", "Luca", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Torr", "Philip H. S.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1805.08162", "submitter": "Yogesh Rawat", "authors": "Kevin Duarte, Yogesh S Rawat, Mubarak Shah", "title": "VideoCapsuleNet: A Simplified Network for Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown\nextremely good results for video human action classification, however, action\ndetection is still a challenging problem. The current action detection\napproaches follow a complex pipeline which involves multiple tasks such as tube\nproposals, optical flow, and tube classification. In this work, we present a\nmore elegant solution for action detection based on the recently developed\ncapsule network. We propose a 3D capsule network for videos, called\nVideoCapsuleNet: a unified network for action detection which can jointly\nperform pixel-wise action segmentation along with action classification. The\nproposed network is a generalization of capsule network from 2D to 3D, which\ntakes a sequence of video frames as input. The 3D generalization drastically\nincreases the number of capsules in the network, making capsule routing\ncomputationally expensive. We introduce capsule-pooling in the convolutional\ncapsule layer to address this issue which makes the voting algorithm tractable.\nThe routing-by-agreement in the network inherently models the action\nrepresentations and various action characteristics are captured by the\npredicted capsules. This inspired us to utilize the capsules for action\nlocalization and the class-specific capsules predicted by the network are used\nto determine a pixel-wise localization of actions. The localization is further\nimproved by parameterized skip connections with the convolutional capsule\nlayers and the network is trained end-to-end with a classification as well as\nlocalization loss. The proposed network achieves sate-of-the-art performance on\nmultiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101\n(24 classes) with an impressive ~20% improvement on UCF-101 and ~15%\nimprovement on J-HMDB in terms of v-mAP scores.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 16:28:47 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Duarte", "Kevin", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "1805.08170", "submitter": "Qiuyuan Huang", "authors": "Qiuyuan Huang, Pengchuan Zhang, Dapeng Wu, Lei Zhang", "title": "Turbo Learning for Captionbot and Drawingbot", "comments": "in proceedings of NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the problems of both image captioning and\ntext-to-image generation, and present a novel turbo learning approach to\njointly training an image-to-text generator (a.k.a. CaptionBot) and a\ntext-to-image generator (a.k.a. DrawingBot). The key idea behind the joint\ntraining is that image-to-text generation and text-to-image generation as dual\nproblems can form a closed loop to provide informative feedback to each other.\nBased on such feedback, we introduce a new loss metric by comparing the\noriginal input with the output produced by the closed loop. In addition to the\nold loss metrics used in CaptionBot and DrawingBot, this extra loss metric\nmakes the jointly trained CaptionBot and DrawingBot better than the separately\ntrained CaptionBot and DrawingBot. Furthermore, the turbo-learning approach\nenables semi-supervised learning since the closed loop can provide\npseudo-labels for unlabeled samples. Experimental results on the COCO dataset\ndemonstrate that the proposed turbo learning can significantly improve the\nperformance of both CaptionBot and DrawingBot by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 16:43:47 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 17:04:51 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Huang", "Qiuyuan", ""], ["Zhang", "Pengchuan", ""], ["Wu", "Dapeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1805.08174", "submitter": "Shagun Sodhani", "authors": "Shagun Sodhani, Vardaan Pahuja", "title": "Reproducibility Report for \"Learning To Count Objects In Natural Images\n  For Visual Question Answering\"", "comments": "Submitted to Reproducibility in ML Workshop, ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is the reproducibility report for the paper \"Learning To Count Objects\nIn Natural Images For Visual QuestionAnswering\"\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 16:50:55 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Sodhani", "Shagun", ""], ["Pahuja", "Vardaan", ""]]}, {"id": "1805.08183", "submitter": "Chun-Guang Li", "authors": "Chun-Guang Li, Junjian Zhang, and Jun Guo", "title": "Constrained Sparse Subspace Clustering with Side-Information", "comments": "8 pages, 2 figures, and 3 tables. This work has been accepted by ICPR\n  2018 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of segmenting high dimensional data\ndrawn from a union of subspaces into the respective subspaces. In some\napplications, partial side-information to indicate \"must-link\" or \"cannot-link\"\nin clustering is available. This leads to the task of subspace clustering with\nside-information. However, in prior work the supervision value of the\nside-information for subspace clustering has not been fully exploited. To this\nend, in this paper, we present an enhanced approach for constrained subspace\nclustering with side-information, termed Constrained Sparse Subspace Clustering\nplus (CSSC+), in which the side-information is used not only in the stage of\nlearning an affinity matrix but also in the stage of spectral clustering.\nMoreover, we propose to estimate clustering accuracy based on the partial\nside-information and theoretically justify the connection to the ground-truth\nclustering accuracy in terms of the Rand index. We conduct experiments on three\ncancer gene expression datasets to validate the effectiveness of our proposals.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 17:06:17 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 01:10:51 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Li", "Chun-Guang", ""], ["Zhang", "Junjian", ""], ["Guo", "Jun", ""]]}, {"id": "1805.08191", "submitter": "Zhe Gan", "authors": "Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu, Jianfeng Wang,\n  Xiaodong He", "title": "Hierarchically Structured Reinforcement Learning for Topically Coherent\n  Visual Story Generation", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchically structured reinforcement learning approach to\naddress the challenges of planning for generating coherent multi-sentence\nstories for the visual storytelling task. Within our framework, the task of\ngenerating a story given a sequence of images is divided across a two-level\nhierarchical decoder. The high-level decoder constructs a plan by generating a\nsemantic concept (i.e., topic) for each image in sequence. The low-level\ndecoder generates a sentence for each image using a semantic compositional\nnetwork, which effectively grounds the sentence generation conditioned on the\ntopic. The two decoders are jointly trained end-to-end using reinforcement\nlearning. We evaluate our model on the visual storytelling (VIST) dataset.\nEmpirical results from both automatic and human evaluations demonstrate that\nthe proposed hierarchically structured reinforced training achieves\nsignificantly better performance compared to a strong flat deep reinforcement\nlearning baseline.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 17:23:31 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 20:11:56 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 07:58:43 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Huang", "Qiuyuan", ""], ["Gan", "Zhe", ""], ["Celikyilmaz", "Asli", ""], ["Wu", "Dapeng", ""], ["Wang", "Jianfeng", ""], ["He", "Xiaodong", ""]]}, {"id": "1805.08235", "submitter": "Milan Sulc", "authors": "Milan Sulc and Jiri Matas", "title": "Improving CNN classifiers by estimating test-time priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of different training and test set class priors is addressed in\nthe context of CNN classifiers. We compare two different approaches to\nestimating the new priors: an existing Maximum Likelihood Estimation approach\n(optimized by an EM algorithm or by projected gradient descend) and a proposed\nMaximum a Posteriori approach, which increases the stability of the estimate by\nintroducing a Dirichlet hyper-prior on the class prior probabilities.\nExperimental results show a significant improvement on the fine-grained\nclassification tasks using known evaluation-time priors, increasing the top-1\naccuracy by 4.0% on the FGVC iNaturalist 2018 validation set and by 3.9% on the\nFGVCx Fungi 2018 validation set. Estimation of the unknown test set priors\nnoticeably increases the accuracy on the PlantCLEF dataset, allowing a single\nCNN model to achieve state-of-the-art results and outperform the\ncompetition-winning ensemble of 12 CNNs. The proposed Maximum a Posteriori\nestimation increases the prediction accuracy by 2.8% on PlantCLEF 2017 and by\n1.8% on FGVCx Fungi, where the existing MLE method would lead to a decrease\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 18:06:53 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 08:36:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Sulc", "Milan", ""], ["Matas", "Jiri", ""]]}, {"id": "1805.08249", "submitter": "Konrad Zolna", "authors": "Konrad Zolna and Krzysztof J. Geras and Kyunghyun Cho", "title": "Classifier-agnostic saliency map extraction", "comments": null, "journal-ref": "Computer Vision and Image Understanding, Volume 196, 2020, 102969,\n  ISSN 1077-3142", "doi": "10.1016/j.cviu.2020.102969", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently available methods for extracting saliency maps identify parts of\nthe input which are the most important to a specific fixed classifier. We show\nthat this strong dependence on a given classifier hinders their performance. To\naddress this problem, we propose classifier-agnostic saliency map extraction,\nwhich finds all parts of the image that any classifier could use, not just one\ngiven in advance. We observe that the proposed approach extracts higher quality\nsaliency maps than prior work while being conceptually simple and easy to\nimplement. The method sets the new state of the art result for localization\ntask on the ImageNet data, outperforming all existing weakly-supervised\nlocalization techniques, despite not using the ground truth labels at the\ninference time. The code reproducing the results is available at\nhttps://github.com/kondiz/casme .\n  The final version of this manuscript is published in Computer Vision and\nImage Understanding and is available online at\nhttps://doi.org/10.1016/j.cviu.2020.102969 .\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 18:36:52 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 19:14:19 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 16:56:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zolna", "Konrad", ""], ["Geras", "Krzysztof J.", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1805.08298", "submitter": "Yuan Li", "authors": "Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing", "title": "Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating long and coherent reports to describe medical images poses\nchallenges to bridging visual patterns with informative human linguistic\ndescriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent\n(HRGR-Agent) which reconciles traditional retrieval-based approaches populated\nwith human prior knowledge, with modern learning-based approaches to achieve\nstructured, robust, and diverse report generation. HRGR-Agent employs a\nhierarchical decision-making procedure. For each sentence, a high-level\nretrieval policy module chooses to either retrieve a template sentence from an\noff-the-shelf template database, or invoke a low-level generation module to\ngenerate a new sentence. HRGR-Agent is updated via reinforcement learning,\nguided by sentence-level and word-level rewards. Experiments show that our\napproach achieves the state-of-the-art results on two medical report datasets,\ngenerating well-balanced structured sentences with robust coverage of\nheterogeneous medical report contents. In addition, our model achieves the\nhighest detection accuracy of medical terminologies, and improved human\nevaluation performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 21:40:02 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 15:11:11 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Christy Y.", ""], ["Liang", "Xiaodan", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1805.08303", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Compression of Deep Convolutional Neural Networks under Joint Sparsity\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization of deep convolutional neural networks (CNNs)\nsuch that they provide good performance while having reduced complexity if\ndeployed on either conventional systems utilizing spatial-domain convolution or\nlower complexity systems designed for Winograd convolution. Furthermore, we\nexplore the universal quantization and compression of these networks. In\nparticular, the proposed framework produces one compressed model whose\nconvolutional filters can be made sparse either in the spatial domain or in the\nWinograd domain. Hence, one compressed model can be deployed universally on any\nplatform, without need for re-training on the deployed platform, and the\nsparsity of its convolutional filters can be exploited for further complexity\nreduction in either domain. To get a better compression ratio, the sparse model\nis compressed in the spatial domain which has a less number of parameters. From\nour experiments, we obtain $24.2\\times$, $47.7\\times$ and $35.4\\times$\ncompressed models for ResNet-18, AlexNet and CT-SRCNN, while their\ncomputational cost is also reduced by $4.5\\times$, $5.1\\times$ and\n$23.5\\times$, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 22:00:21 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 02:18:12 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1805.08311", "submitter": "Mohammad Ghasemzadeh", "authors": "Mohammad Ghasemzadeh, Fang Lin, Bita Darvish Rouhani, Farinaz\n  Koushanfar, Ke Huang", "title": "AgileNet: Lightweight Dictionary-based Few-shot Learning", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning models is heavily tied to the use of massive\namount of labeled data and excessively long training time. With the emergence\nof intelligent edge applications that use these models, the critical challenge\nis to obtain the same inference capability on a resource-constrained device\nwhile providing adaptability to cope with the dynamic changes in the data. We\npropose AgileNet, a novel lightweight dictionary-based few-shot learning\nmethodology which provides reduced complexity deep neural network for efficient\nexecution at the edge while enabling low-cost updates to capture the dynamics\nof the new data. Evaluations of state-of-the-art few-shot learning benchmarks\ndemonstrate the superior accuracy of AgileNet compared to prior arts.\nAdditionally, AgileNet is the first few-shot learning approach that prevents\nmodel updates by eliminating the knowledge obtained from the primary training.\nThis property is ensured through the dictionaries learned by our novel\nend-to-end structured decomposition, which also reduces the memory footprint\nand computation complexity to match the edge device constraints.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 22:36:11 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ghasemzadeh", "Mohammad", ""], ["Lin", "Fang", ""], ["Rouhani", "Bita Darvish", ""], ["Koushanfar", "Farinaz", ""], ["Huang", "Ke", ""]]}, {"id": "1805.08315", "submitter": "Drew Linsley", "authors": "Drew Linsley and Junkyung Kim and Vijay Veerabadran and Thomas Serre", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent\n  units", "comments": "Published at NeurIPS 2018\n  https://papers.nips.cc/paper/7300-learning-long-range-spatial-dependencies-with-horizontal-gated-recurrent-units", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in deep learning has spawned great successes in many engineering\napplications. As a prime example, convolutional neural networks, a type of\nfeedforward neural networks, are now approaching -- and sometimes even\nsurpassing -- human accuracy on a variety of visual recognition tasks. Here,\nhowever, we show that these neural networks and their recent extensions\nstruggle in recognition tasks where co-dependent visual features must be\ndetected over long spatial ranges. We introduce the horizontal gated-recurrent\nunit (hGRU) to learn intrinsic horizontal connections -- both within and across\nfeature columns. We demonstrate that a single hGRU layer matches or outperforms\nall tested feedforward hierarchical baselines including state-of-the-art\narchitectures which have orders of magnitude more free parameters. We further\ndiscuss the biological plausibility of the hGRU in comparison to anatomical\ndata from the visual cortex as well as human behavioral data on a classic\ncontour detection task.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 22:52:25 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 13:31:43 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 15:40:51 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 13:00:56 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Linsley", "Drew", ""], ["Kim", "Junkyung", ""], ["Veerabadran", "Vijay", ""], ["Serre", "Thomas", ""]]}, {"id": "1805.08324", "submitter": "Michael Motro", "authors": "Michael Motro and Joydeep Ghosh", "title": "Measurement-wise Occlusion in Multi-object Tracking", "comments": "presenting at 21st International Conference on Information Fusion,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling object interaction is a fundamental challenge in practical\nmulti-object tracking, even for simple interactive effects such as one object\ntemporarily occluding another. We formalize the problem of occlusion in\ntracking with two different abstractions. In object-wise occlusion, objects\nthat are occluded by other objects do not generate measurements. In\nmeasurement-wise occlusion, a previously unstudied approach, all objects may\ngenerate measurements but some measurements may be occluded by others. While\nthe relative validity of each abstraction depends on the situation and sensor,\nmeasurement-wise occlusion fits into probabilistic multi-object tracking\nalgorithms with much looser assumptions on object interaction. Its value is\ndemonstrated by showing that it naturally derives a popular approximation for\nlidar tracking, and by an example of visual tracking in image space.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 23:49:40 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Motro", "Michael", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1805.08340", "submitter": "Tong Qin", "authors": "Tong Qin and Ling Zhou and Dongbin Xiu", "title": "Reducing Parameter Space for Neural Network Training", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For neural networks (NNs) with rectified linear unit (ReLU) or binary\nactivation functions, we show that their training can be accomplished in a\nreduced parameter space. Specifically, the weights in each neuron can be\ntrained on the unit sphere, as opposed to the entire space, and the threshold\ncan be trained in a bounded interval, as opposed to the real line. We show that\nthe NNs in the reduced parameter space are mathematically equivalent to the\nstandard NNs with parameters in the whole space. The reduced parameter space\nshall facilitate the optimization procedure for the network training, as the\nsearch space becomes (much) smaller. We demonstrate the improved training\nperformance using numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 01:08:40 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 17:13:01 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 18:10:43 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Qin", "Tong", ""], ["Zhou", "Ling", ""], ["Xiu", "Dongbin", ""]]}, {"id": "1805.08365", "submitter": "Zichuan Liu", "authors": "Zichuan Liu, Guosheng Lin, Sheng Yang, Jiashi Feng, Weisi Lin and Wang\n  Ling Goh", "title": "Learning Markov Clustering Networks for Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel framework named Markov Clustering Network (MCN) is proposed for fast\nand robust scene text detection. MCN predicts instance-level bounding boxes by\nfirstly converting an image into a Stochastic Flow Graph (SFG) and then\nperforming Markov Clustering on this graph. Our method can detect text objects\nwith arbitrary size and orientation without prior knowledge of object size. The\nstochastic flow graph encode objects' local correlation and semantic\ninformation. An object is modeled as strongly connected nodes, which allows\nflexible bottom-up detection for scale-varying and rotated objects. MCN\ngenerates bounding boxes without using Non-Maximum Suppression, and it can be\nfully parallelized on GPUs. The evaluation on public benchmarks shows that our\nmethod outperforms the existing methods by a large margin in detecting\nmultioriented text objects. MCN achieves new state-of-art performance on\nchallenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and\nF-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34\nFPS, which is $1.5\\times$ speedup when compared with the fastest scene text\ndetection algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 02:46:39 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liu", "Zichuan", ""], ["Lin", "Guosheng", ""], ["Yang", "Sheng", ""], ["Feng", "Jiashi", ""], ["Lin", "Weisi", ""], ["Goh", "Wang Ling", ""]]}, {"id": "1805.08389", "submitter": "Jialin Wu", "authors": "Jialin Wu, Zeyuan Hu, Raymond J. Mooney", "title": "Joint Image Captioning and Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering visual questions need acquire daily common knowledge and model the\nsemantic connection among different parts in images, which is too difficult for\nVQA systems to learn from images with the only supervision from answers.\nMeanwhile, image captioning systems with beam search strategy tend to generate\nsimilar captions and fail to diversely describe images. To address the\naforementioned issues, we present a system to have these two tasks compensate\nwith each other, which is capable of jointly producing image captions and\nanswering visual questions. In particular, we utilize question and image\nfeatures to generate question-related captions and use the generated captions\nas additional features to provide new knowledge to the VQA system. For image\ncaptioning, our system attains more informative results in term of the relative\nimprovements on VQA tasks as well as competitive results using automated\nmetrics. Applying our system to the VQA tasks, our results on VQA v2 dataset\nachieve 65.8% using generated captions and 69.1% using annotated captions in\nvalidation set and 68.4% in the test-standard set. Further, an ensemble of 10\nmodels results in 69.7% in the test-standard split.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 04:41:37 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wu", "Jialin", ""], ["Hu", "Zeyuan", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1805.08400", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, Richard Chen, Sandra Sudarsky, Daphne Yu, Nicholas J.\n  Durr", "title": "Deep Learning with Cinematic Rendering: Fine-Tuning Deep Neural Networks\n  Using Photorealistic Medical Images", "comments": "14 Pages, 4 Figures, 3 Tables, Physics in Medicine and Biology (2018)", "journal-ref": null, "doi": "10.1088/1361-6560/aada93", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has emerged as a powerful artificial intelligence tool to\ninterpret medical images for a growing variety of applications. However, the\npaucity of medical imaging data with high-quality annotations that is necessary\nfor training such methods ultimately limits their performance. Medical data is\nchallenging to acquire due to privacy issues, shortage of experts available for\nannotation, limited representation of rare conditions and cost. This problem\nhas previously been addressed by using synthetically generated data. However,\nnetworks trained on synthetic data often fail to generalize to real data.\nCinematic rendering simulates the propagation and interaction of light passing\nthrough tissue models reconstructed from CT data, enabling the generation of\nphotorealistic images. In this paper, we present one of the first applications\nof cinematic rendering in deep learning, in which we propose to fine-tune\nsynthetic data-driven networks using cinematically rendered CT data for the\ntask of monocular depth estimation in endoscopy. Our experiments demonstrate\nthat: (a) Convolutional Neural Networks (CNNs) trained on synthetic data and\nfine-tuned on photorealistic cinematically rendered data adapt better to real\nmedical images and demonstrate more robust performance when compared to\nnetworks with no fine-tuning, (b) these fine-tuned networks require less\ntraining data to converge to an optimal solution, and (c) fine-tuning with data\nfrom a variety of photorealistic rendering conditions of the same scene\nprevents the network from learning patient-specific information and aids in\ngeneralizability of the model. Our empirical evaluation demonstrates that\nnetworks fine-tuned with cinematically rendered data predict depth with 56.87%\nless error for rendered endoscopy images and 27.49% less error for real porcine\ncolon endoscopy images.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 05:24:41 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 01:41:37 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2018 19:09:43 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Mahmood", "Faisal", ""], ["Chen", "Richard", ""], ["Sudarsky", "Sandra", ""], ["Yu", "Daphne", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1805.08403", "submitter": "Yao Qin", "authors": "Yao Qin, Konstantinos Kamnitsas, Siddharth Ancha, Jay Nanavati,\n  Garrison Cottrell, Antonio Criminisi and Aditya Nori", "title": "Autofocus Layer for Semantic Segmentation", "comments": "Published on MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the autofocus convolutional layer for semantic segmentation with\nthe objective of enhancing the capabilities of neural networks for multi-scale\nprocessing. Autofocus layers adaptively change the size of the effective\nreceptive field based on the processed context to generate more powerful\nfeatures. This is achieved by parallelising multiple convolutional layers with\ndifferent dilation rates, combined by an attention mechanism that learns to\nfocus on the optimal scales driven by context. By sharing the weights of the\nparallel convolutions we make the network scale-invariant, with only a modest\nincrease in the number of parameters. The proposed autofocus layer can be\neasily integrated into existing networks to improve a model's representational\npower. We evaluate our models on the challenging tasks of multi-organ\nsegmentation in pelvic CT and brain tumor segmentation in MRI and achieve very\npromising performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 05:35:20 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 04:27:07 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 06:30:35 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Qin", "Yao", ""], ["Kamnitsas", "Konstantinos", ""], ["Ancha", "Siddharth", ""], ["Nanavati", "Jay", ""], ["Cottrell", "Garrison", ""], ["Criminisi", "Antonio", ""], ["Nori", "Aditya", ""]]}, {"id": "1805.08416", "submitter": "Nizar Massouh", "authors": "Nizar Massouh", "title": "Training Convolutional Networks with Web Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we investigate the effect of using web images to build a large\nscale database to be used along a deep learning method for a classification\ntask. We replicate the ImageNet large scale database (ILSVRC-2012) from images\ncollected from the web using 4 different download strategies varying: the\nsearch engine, the query and the image resolution. As a deep learning method,\nwe will choose the Convolutional Neural Network that was very successful with\nrecognition tasks; the AlexNet.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 06:11:00 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Massouh", "Nizar", ""]]}, {"id": "1805.08417", "submitter": "Huai Qian Khor Mr.", "authors": "Huai-Qian Khor, John See, Raphael C.W.Phan, Weiyao Lin", "title": "Enriched Long-term Recurrent Convolutional Network for Facial\n  Micro-Expression Recognition", "comments": "Published in Micro-Expression Grand Challenge 2018, Workshop of 13th\n  IEEE Facial & Gesture 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial micro-expression (ME) recognition has posed a huge challenge to\nresearchers for its subtlety in motion and limited databases. Recently,\nhandcrafted techniques have achieved superior performance in micro-expression\nrecognition but at the cost of domain specificity and cumbersome parametric\ntunings. In this paper, we propose an Enriched Long-term Recurrent\nConvolutional Network (ELRCN) that first encodes each micro-expression frame\ninto a feature vector through CNN module(s), then predicts the micro-expression\nby passing the feature vector through a Long Short-term Memory (LSTM) module.\nThe framework contains two different network variants: (1) Channel-wise\nstacking of input data for spatial enrichment, (2) Feature-wise stacking of\nfeatures for temporal enrichment. We demonstrate that the proposed approach is\nable to achieve reasonably good performance, without data augmentation. In\naddition, we also present ablation studies conducted on the framework and\nvisualizations of what CNN \"sees\" when predicting the micro-expression classes.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 06:18:30 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Khor", "Huai-Qian", ""], ["See", "John", ""], ["Phan", "Raphael C. W.", ""], ["Lin", "Weiyao", ""]]}, {"id": "1805.08440", "submitter": "Philipp Oberdiek", "authors": "Philipp Oberdiek, Matthias Rottmann, Hanno Gottschalk", "title": "Classification Uncertainty of Deep Neural Networks Based on Gradient\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quantification of uncertainty of Convolutional Neural Networks\n(CNNs) based on gradient metrics. Unlike the classical softmax entropy, such\nmetrics gather information from all layers of the CNN. We show for the EMNIST\ndigits data set that for several such metrics we achieve the same meta\nclassification accuracy -- i.e. the task of classifying predictions as correct\nor incorrect without knowing the actual label -- as for entropy thresholding.\nWe apply meta classification to unknown concepts (out-of-distribution samples)\n-- EMNIST/Omniglot letters, CIFAR10 and noise -- and demonstrate that meta\nclassification rates for unknown concepts can be increased when using entropy\ntogether with several gradient based metrics as input quantities for a meta\nclassifier. Meta classifiers only trained on the uncertainty metrics of known\nconcepts, i.e. EMNIST digits, usually do not perform equally well for all\nunknown concepts. If we however allow the meta classifier to be trained on\nuncertainty metrics for some out-of-distribution samples, meta classification\nfor concepts remote from EMNIST digits (then termed known unknowns) can be\nimproved considerably.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 08:07:14 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 10:37:11 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Oberdiek", "Philipp", ""], ["Rottmann", "Matthias", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1805.08443", "submitter": "Mai Bui", "authors": "Mai Bui, Shadi Albarqouni, Slobodan Ilic and Nassir Navab", "title": "Scene Coordinate and Correspondence Learning for Image-Based\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene coordinate regression has become an essential part of current camera\nre-localization methods. Different versions, such as regression forests and\ndeep learning methods, have been successfully applied to estimate the\ncorresponding camera pose given a single input image. In this work, we propose\nto regress the scene coordinates pixel-wise for a given RGB image by using deep\nlearning. Compared to the recent methods, which usually employ RANSAC to obtain\na robust pose estimate from the established point correspondences, we propose\nto regress confidences of these correspondences, which allows us to immediately\ndiscard erroneous predictions and improve the initial pose estimates. Finally,\nthe resulting confidences can be used to score initial pose hypothesis and aid\nin pose refinement, offering a generalized solution to solve this task.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 08:12:03 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 00:29:04 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 17:26:03 GMT"}, {"version": "v4", "created": "Thu, 26 Jul 2018 11:56:03 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Bui", "Mai", ""], ["Albarqouni", "Shadi", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "1805.08484", "submitter": "Chenyang Si", "authors": "Wei Wang, Jinjin Zhang, Chenyang Si, Liang Wang", "title": "Pose-Based Two-Stream Relational Networks for Action Recognition in\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pose-based action recognition has gained more and more attention\ndue to the better performance compared with traditional appearance-based\nmethods. However, there still exist two problems to be further solved. First,\nexisting pose-based methods generally recognize human actions with captured 3D\nhuman poses which are very difficult to obtain in real scenarios. Second, few\npose-based methods model the action-related objects in recognizing human-object\ninteraction actions in which objects play an important role. To solve the\nproblems above, we propose a pose-based two-stream relational network (PSRN)\nfor action recognition. In PSRN, one stream models the temporal dynamics of the\ntargeted 2D human pose sequences which are directly extracted from raw videos,\nand the other stream models the action-related objects from a randomly sampled\nvideo frame. Most importantly, instead of fusing two-streams in the class score\nlayer as before, we propose a pose-object relational network to model the\nrelationship between human poses and action-related objects. We evaluate the\nproposed PSRN on two challenging benchmarks, i.e., Sub-JHMDB and PennAction.\nExperimental results show that our PSRN obtains the state-the-of-art\nperformance on Sub-JHMDB (80.2%) and PennAction (98.1%). Our work opens a new\ndoor to action recognition by combining 2D human pose extracted from raw video\nand image appearance.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:30:20 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wang", "Wei", ""], ["Zhang", "Jinjin", ""], ["Si", "Chenyang", ""], ["Wang", "Liang", ""]]}, {"id": "1805.08492", "submitter": "Tao Yu", "authors": "Tao Yu, Yu Qiao and Huan Long", "title": "Knowledge-based Fully Convolutional Network and Its Application in\n  Segmentation of Lung CT Images", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of deep neural networks have been applied in medical image\nsegmentation and achieve good performance. Unlike natural images, medical\nimages of the same imaging modality are characterized by the same pattern,\nwhich indicates that same normal organs or tissues locate at similar positions\nin the images. Thus, in this paper we try to incorporate the prior knowledge of\nmedical images into the structure of neural networks such that the prior\nknowledge can be utilized for accurate segmentation. Based on this idea, we\npropose a novel deep network called knowledge-based fully convolutional network\n(KFCN) for medical image segmentation. The segmentation function and\ncorresponding error is analyzed. We show the existence of an asymptotically\nstable region for KFCN which traditional FCN doesn't possess. Experiments\nvalidate our knowledge assumption about the incorporation of prior knowledge\ninto the convolution kernels of KFCN and show that KFCN can achieve a\nreasonable segmentation and a satisfactory accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:43:17 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yu", "Tao", ""], ["Qiao", "Yu", ""], ["Long", "Huan", ""]]}, {"id": "1805.08493", "submitter": "Da Pan", "authors": "Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu, Yuan Zhang", "title": "Blind Predicting Similar Quality Map for Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in blind image quality assessment (BIQA) is how to effectively\nmodel the properties of human visual system in a data-driven manner. In this\npaper, we propose a simple and efficient BIQA model based on a novel framework\nwhich consists of a fully convolutional neural network (FCNN) and a pooling\nnetwork to solve this problem. In principle, FCNN is capable of predicting a\npixel-by-pixel similar quality map only from a distorted image by using the\nintermediate similarity maps derived from conventional full-reference image\nquality assessment methods. The predicted pixel-by-pixel quality maps have good\nconsistency with the distortion correlations between the reference and\ndistorted images. Finally, a deep pooling network regresses the quality map\ninto a score. Experiments have demonstrated that our predictions outperform\nmany state-of-the-art BIQA methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:49:01 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 07:09:16 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Pan", "Da", ""], ["Shi", "Ping", ""], ["Hou", "Ming", ""], ["Ying", "Zefeng", ""], ["Fu", "Sizhe", ""], ["Zhang", "Yuan", ""]]}, {"id": "1805.08503", "submitter": "Roman Seidel", "authors": "Roman Seidel and Andr\\'e Apitzsch and Gangolf Hirtz", "title": "Improved Person Detection on Omnidirectional Images with Non-maxima\n  Suppression", "comments": "8 pages, VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a person detector on omnidirectional images, an accurate method to\ngenerate minimal enclosing rectangles of persons. The basic idea is to adapt\nthe qualitative detection performance of a convolutional neural network based\nmethod, namely YOLOv2 to fish-eye images. The design of our approach picks up\nthe idea of a state-of-the-art object detector and highly overlapping areas of\nimages with their regions of interests. This overlap reduces the number of\nfalse negatives. Based on the raw bounding boxes of the detector we fine-tuned\noverlapping bounding boxes by three approaches: non-maximum suppression, soft\nnon-maximum suppression and soft non-maximum suppression with Gaussian\nsmoothing. The evaluation was done on the PIROPO database and an own annotated\nFlat dataset, supplemented with bounding boxes on omnidirectional images. We\nachieve an average precision of 64.4 % with YOLOv2 for the class person on\nPIROPO and 77.6 % on Flat. For this purpose we fine-tuned the soft non-maximum\nsuppression with Gaussian smoothing.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:14:14 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 12:50:15 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 14:05:09 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2019 09:34:10 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Seidel", "Roman", ""], ["Apitzsch", "Andr\u00e9", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "1805.08511", "submitter": "George De Ath", "authors": "George De Ath, Richard M. Everson", "title": "Part-based Tracking by Sampling", "comments": "Submitted to IEEE Winter Conference on Applications of Computer\n  Vision 2020 (WACV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel part-based method for tracking an arbitrary object in\nchallenging video sequences. The colour distribution of tracked image patches\non the target object are represented by pairs of RGB samples and counts of how\nmany pixels in the patch are similar to them. Patches are placed by segmenting\nthe object in the given bounding box and placing patches in homogeneous regions\nof the object. These are located in subsequent image frames by applying\nnon-shearing affine transformations to the patches' previous locations, locally\noptimising the best of these, and evaluating their quality using a modified\nBhattacharyya distance. In experiments carried out on VOT2018 and OTB100\nbenchmarks, the tracker achieves higher performance than all other part-based\ntrackers. An ablation study is used to reveal the effectiveness of each\ntracking component, with largest performance gains found when using the patch\nplacement scheme.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:38:04 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 10:14:48 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["De Ath", "George", ""], ["Everson", "Richard M.", ""]]}, {"id": "1805.08542", "submitter": "Janne Mustaniemi", "authors": "Janne Mustaniemi, Juho Kannala, Simo S\\\"arkk\\\"a, Jiri Matas and Janne\n  Heikkil\\\"a", "title": "Fast Motion Deblurring for Feature Detection and Matching Using Inertial\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision and image processing applications rely on local\nfeatures. It is well-known that motion blur decreases the performance of\ntraditional feature detectors and descriptors. We propose an inertial-based\ndeblurring method for improving the robustness of existing feature detectors\nand descriptors against the motion blur. Unlike most deblurring algorithms, the\nmethod can handle spatially-variant blur and rolling shutter distortion.\nFurthermore, it is capable of running in real-time contrary to state-of-the-art\nalgorithms. The limitations of inertial-based blur estimation are taken into\naccount by validating the blur estimates using image data. The evaluation shows\nthat when the method is used with traditional feature detector and descriptor,\nit increases the number of detected keypoints, provides higher repeatability\nand improves the localization accuracy. We also demonstrate that such features\nwill lead to more accurate and complete reconstructions when used in the\napplication of 3D visual reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:35:47 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Mustaniemi", "Janne", ""], ["Kannala", "Juho", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Matas", "Jiri", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1805.08545", "submitter": "Arturo Marban", "authors": "Arturo Marban, Vignesh Srinivasan, Wojciech Samek, Josep Fern\\'andez,\n  Alicia Casals", "title": "A Recurrent Convolutional Neural Network Approach for Sensorless Force\n  Estimation in Robotic Surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing force feedback as relevant information in current Robot-Assisted\nMinimally Invasive Surgery systems constitutes a technological challenge due to\nthe constraints imposed by the surgical environment. In this context,\nSensorless Force Estimation techniques represent a potential solution, enabling\nto sense the interaction forces between the surgical instruments and\nsoft-tissues. Specifically, if visual feedback is available for observing\nsoft-tissues' deformation, this feedback can be used to estimate the forces\napplied to these tissues. To this end, a force estimation model, based on\nConvolutional Neural Networks and Long-Short Term Memory networks, is proposed\nin this work. This model is designed to process both, the spatiotemporal\ninformation present in video sequences and the temporal structure of tool data\n(the surgical tool-tip trajectory and its grasping status). A series of\nanalyses are carried out to reveal the advantages of the proposal and the\nchallenges that remain for real applications. This research work focuses on two\nsurgical task scenarios, referred to as pushing and pulling tissue. For these\ntwo scenarios, different input data modalities and their effect on the force\nestimation quality are investigated. These input data modalities are tool data,\nvideo sequences and a combination of both. The results suggest that the force\nestimation quality is better when both, the tool data and video sequences, are\nprocessed by the neural network model. Moreover, this study reveals the need\nfor a loss function, designed to promote the modeling of smooth and sharp\ndetails found in force signals. Finally, the results show that the modeling of\nforces due to pulling tasks is more challenging than for the simplest pushing\nactions.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:39:24 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Marban", "Arturo", ""], ["Srinivasan", "Vignesh", ""], ["Samek", "Wojciech", ""], ["Fern\u00e1ndez", "Josep", ""], ["Casals", "Alicia", ""]]}, {"id": "1805.08564", "submitter": "Ihab Mohamed", "authors": "Ihab S. Mohamed, Alessio Capitanelli, Fulvio Mastrogiovanni, Stefano\n  Rovetta, Renato Zaccaria", "title": "A 2D laser rangefinder scans dataset of standard EUR pallets", "comments": "This paper has been accepted to be published in \"Data in Brief\n  (DiB)\". 10 pages, 4 figures, and 2 tables", "journal-ref": null, "doi": "10.1016/j.dib.2019.103837", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the technology of automated guided vehicles (AGVs) has\nnotably advanced. In particular, in the context of factory and warehouse\nautomation, different approaches have been presented for detecting and\nlocalizing pallets inside warehouses and shop-floor environments. In a related\nresearch paper [1], we show that an AGVs can detect, localize, and track\npallets using machine learning techniques based only on the data of an on-board\n2D laser rangefinder. Such sensor is very common in industrial scenarios due to\nits simplicity and robustness, but it can only provide a limited amount of\ndata. Therefore, it has been neglected in the past in favor of more complex\nsolutions. In this paper, we release to the community the data we collected in\n[1] for further research activities in the field of pallet localization and\ntracking. The dataset comprises a collection of 565 2D scans from real-world\nenvironments, which are divided into 340 samples where pallets are present, and\n225 samples where they are not. The data have been manually labelled and are\nprovided in different formats.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 13:16:54 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 21:48:59 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Mohamed", "Ihab S.", ""], ["Capitanelli", "Alessio", ""], ["Mastrogiovanni", "Fulvio", ""], ["Rovetta", "Stefano", ""], ["Zaccaria", "Renato", ""]]}, {"id": "1805.08569", "submitter": "Gaurav Yengera", "authors": "Gaurav Yengera, Didier Mutter, Jacques Marescaux, Nicolas Padoy", "title": "Less is More: Surgical Phase Recognition with Less Annotations through\n  Self-Supervised Pre-training of CNN-LSTM Networks", "comments": "15 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time algorithms for automatically recognizing surgical phases are needed\nto develop systems that can provide assistance to surgeons, enable better\nmanagement of operating room (OR) resources and consequently improve safety\nwithin the OR. State-of-the-art surgical phase recognition algorithms using\nlaparoscopic videos are based on fully supervised training. This limits their\npotential for widespread application, since creation of manual annotations is\nan expensive process considering the numerous types of existing surgeries and\nthe vast amount of laparoscopic videos available. In this work, we propose a\nnew self-supervised pre-training approach based on the prediction of remaining\nsurgery duration (RSD) from laparoscopic videos. The RSD prediction task is\nused to pre-train a convolutional neural network (CNN) and long short-term\nmemory (LSTM) network in an end-to-end manner. Our proposed approach utilizes\nall available data and reduces the reliance on annotated data, thereby\nfacilitating the scaling up of surgical phase recognition algorithms to\ndifferent kinds of surgeries. Additionally, we present EndoN2N, an end-to-end\ntrained CNN-LSTM model for surgical phase recognition and evaluate the\nperformance of our approach on a dataset of 120 Cholecystectomy laparoscopic\nvideos (Cholec120). This work also presents the first systematic study of\nself-supervised pre-training approaches to understand the amount of annotations\nrequired for surgical phase recognition. Interestingly, the proposed RSD\npre-training approach leads to performance improvement even when all the\ntraining data is manually annotated and outperforms the single pre-training\napproach for surgical phase recognition presently published in the literature.\nIt is also observed that end-to-end training of CNN-LSTM networks boosts\nsurgical phase recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 13:28:59 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yengera", "Gaurav", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1805.08587", "submitter": "Jihua Zhu", "authors": "Shanmin Pang and Jin Ma and Jianru Xue and Jihua Zhu and Vicente\n  Ordonez", "title": "Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for\n  Image Retrieval", "comments": "The paper has been accepted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval based on deep convolutional features has demonstrated\nstate-of-the-art performance in popular benchmarks. In this paper, we present a\nunified solution to address deep convolutional feature aggregation and image\nre-ranking by simulating the dynamics of heat diffusion. A distinctive problem\nin image retrieval is that repetitive or \\emph{bursty} features tend to\ndominate final image representations, resulting in representations less\ndistinguishable. We show that by considering each deep feature as a heat\nsource, our unsupervised aggregation method is able to avoid\nover-representation of \\emph{bursty} features. We additionally provide a\npractical solution for the proposed aggregation method and further show the\nefficiency of our method in experimental evaluation. Inspired by the\naforementioned deep feature aggregation method, we also propose a method to\nre-rank a number of top ranked images for a given query image by considering\nthe query as the heat source. Finally, we extensively evaluate the proposed\napproach with pre-trained and fine-tuned deep networks on common public\nbenchmarks and show superior performance compared to previous work.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:06:28 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 14:57:01 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 22:18:33 GMT"}, {"version": "v4", "created": "Sat, 2 Jun 2018 19:34:08 GMT"}, {"version": "v5", "created": "Tue, 9 Oct 2018 02:30:27 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Pang", "Shanmin", ""], ["Ma", "Jin", ""], ["Xue", "Jianru", ""], ["Zhu", "Jihua", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1805.08604", "submitter": "Jan Egger", "authors": "J\\\"urgen Wallner, Kerstin Hochegger, Xiaojun Chen, Irene Mischak, Knut\n  Reinbacher, Mauro Pau, Tomislav Zrnc, Katja Schwenzer-Zimmerer, Wolfgang\n  Zemann, Dieter Schmalstieg, Jan Egger", "title": "Clinical evaluation of semi-automatic opensource algorithmic software\n  segmentation of the mandibular bone: Practical feasibility and assessment of\n  a new course of action", "comments": "26 pages", "journal-ref": "Wallner J, et al. (2018) Clinical evaluation of semi-automatic\n  open-source algorithmic software segmentation of the mandibular bone:\n  Practical feasibility and assessment of a new course of action. PLoS ONE\n  13(5):e0196378", "doi": "10.1371/journal.pone.0196378", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer assisted technologies based on algorithmic software segmentation are\nan increasing topic of interest in complex surgical cases. However - due to\nfunctional instability, time consuming software processes, personnel resources\nor licensed-based financial costs many segmentation processes are often\noutsourced from clinical centers to third parties and the industry. Therefore,\nthe aim of this trial was to assess the practical feasibility of an easy\navailable, functional stable and licensed-free segmentation approach to be used\nin the clinical practice. In this retrospective, randomized, controlled trail\nthe accuracy and accordance of the open-source based segmentation algorithm\nGrowCut (GC) was assessed through the comparison to the manually generated\nground truth of the same anatomy using 10 CT lower jaw data-sets from the\nclinical routine. Assessment parameters were the segmentation time, the volume,\nthe voxel number, the Dice Score (DSC) and the Hausdorff distance (HD). Overall\nsegmentation times were about one minute. Mean DSC values of over 85% and HD\nbelow 33.5 voxel could be achieved. Statistical differences between the\nassessment parameters were not significant (p<0.05) and correlation\ncoefficients were close to the value one (r > 0.94). Complete functional stable\nand time saving segmentations with high accuracy and high positive correlation\ncould be performed by the presented interactive open-source based approach. In\nthe cranio-maxillofacial complex the used method could represent an algorithmic\nalternative for image-based segmentation in the clinical practice for e.g.\nsurgical treatment planning or visualization of postoperative results and\noffers several advantages. Systematic comparisons to other segmentation\napproaches or with a greater data amount are areas of future works.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 08:38:58 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wallner", "J\u00fcrgen", ""], ["Hochegger", "Kerstin", ""], ["Chen", "Xiaojun", ""], ["Mischak", "Irene", ""], ["Reinbacher", "Knut", ""], ["Pau", "Mauro", ""], ["Zrnc", "Tomislav", ""], ["Schwenzer-Zimmerer", "Katja", ""], ["Zemann", "Wolfgang", ""], ["Schmalstieg", "Dieter", ""], ["Egger", "Jan", ""]]}, {"id": "1805.08620", "submitter": "Shin Fujieda", "authors": "Shin Fujieda, Kohei Takayama, Toshiya Hachisuka", "title": "Wavelet Convolutional Neural Networks", "comments": "10 pages, 7 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1707.07394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and spectral approaches are two major approaches for image processing\ntasks such as image classification and object recognition. Among many such\nalgorithms, convolutional neural networks (CNNs) have recently achieved\nsignificant performance improvement in many challenging tasks. Since CNNs\nprocess images directly in the spatial domain, they are essentially spatial\napproaches. Given that spatial and spectral approaches are known to have\ndifferent characteristics, it will be interesting to incorporate a spectral\napproach into CNNs. We propose a novel CNN architecture, wavelet CNNs, which\ncombines a multiresolution analysis and CNNs into one model. Our insight is\nthat a CNN can be viewed as a limited form of a multiresolution analysis. Based\non this insight, we supplement missing parts of the multiresolution analysis\nvia wavelet transform and integrate them as additional components in the entire\narchitecture. Wavelet CNNs allow us to utilize spectral information which is\nmostly lost in conventional CNNs but useful in most image processing tasks. We\nevaluate the practical performance of wavelet CNNs on texture classification\nand image annotation. The experiments show that wavelet CNNs can achieve better\naccuracy in both tasks than existing models while having significantly fewer\nparameters than conventional CNNs.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 07:18:54 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Fujieda", "Shin", ""], ["Takayama", "Kohei", ""], ["Hachisuka", "Toshiya", ""]]}, {"id": "1805.08624", "submitter": "Hamed Fatemi Langroudi", "authors": "Seyed H. F. Langroudi, Tej Pandit, Dhireesha Kudithipudi", "title": "Deep Learning Inference on Embedded Devices: Fixed-Point vs Posit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing the inference step of deep learning in resource constrained\nenvironments, such as embedded devices, is challenging. Success requires\noptimization at both software and hardware levels. Low precision arithmetic and\nspecifically low precision fixed-point number systems have become the standard\nfor performing deep learning inference. However, representing non-uniform data\nand distributed parameters (e.g. weights) by using uniformly distributed\nfixed-point values is still a major drawback when using this number system.\nRecently, the posit number system was proposed, which represents numbers in a\nnon-uniform manner. Therefore, in this paper we are motivated to explore using\nthe posit number system to represent the weights of Deep Convolutional Neural\nNetworks. However, we do not apply any quantization techniques and hence the\nnetwork weights do not require re-training. The results of this exploration\nshow that using the posit number system outperformed the fixed point number\nsystem in terms of accuracy and memory utilization.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:31:27 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Langroudi", "Seyed H. F.", ""], ["Pandit", "Tej", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1805.08634", "submitter": "Wamiq Reyaz Para", "authors": "John Femiani, Wamiq Reyaz Para, Niloy Mitra, Peter Wonka", "title": "Facade Segmentation in the Wild", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban facade segmentation from automatically acquired imagery, in contrast to\ntraditional image segmentation, poses several unique challenges. 360-degree\nphotospheres captured from vehicles are an effective way to capture a large\nnumber of images, but this data presents difficult-to-model warping and\nstitching artifacts. In addition, each pixel can belong to multiple facade\nelements, and different facade elements (e.g., window, balcony, sill, etc.) are\ncorrelated and vary wildly in their characteristics. In this paper, we propose\nthree network architectures of varying complexity to achieve multilabel\nsemantic segmentation of facade images while exploiting their unique\ncharacteristics. Specifically, we propose a MULTIFACSEGNET architecture to\nassign multiple labels to each pixel, a SEPARABLE architecture as a low-rank\nformulation that encourages extraction of rectangular elements, and a\nCOMPATIBILITY network that simultaneously seeks segmentation across facade\nelement types allowing the network to 'see' intermediate output probabilities\nof the various facade element classes. Our results on benchmark datasets show\nsignificant improvements over existing facade segmentation approaches for the\ntypical facade elements. For example, on one commonly used dataset, the\naccuracy scores for window(the most important architectural element) increases\nfrom 0.91 to 0.97 percent compared to the best competing method, and comparable\nimprovements on other element types.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 16:21:31 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Femiani", "John", ""], ["Para", "Wamiq Reyaz", ""], ["Mitra", "Niloy", ""], ["Wonka", "Peter", ""]]}, {"id": "1805.08649", "submitter": "Vikram Ravindra", "authors": "Vikram Ravindra and Petros Drineas and Ananth Grama", "title": "Constructing Compact Brain Connectomes for Individual Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent neuroimaging studies have shown that functional connectomes are unique\nto individuals, i.e., two distinct fMRIs taken over different sessions of the\nsame subject are more similar in terms of their connectomes than those from two\ndifferent subjects. In this study, we present significant new results that\nidentify, for the first time, specific parts of resting-state and task-specific\nconnectomes that code the unique signatures. We show that a very small part of\nthe connectome codes the signatures. A network of these features is shown to\nachieve excellent training and test accuracy in matching imaging datasets. We\nshow that these features are statistically significant, robust to\nperturbations, invariant across populations, and are localized to a small\nnumber of structural regions of the brain. Furthermore, we show that for\ntask-specific connectomes, the regions identified by our method are consistent\nwith their known functional characterization. We present a new matrix sampling\ntechnique to derive computationally efficient and accurate methods for\nidentifying the discriminating sub-connectome and support all of our claims\nusing state-of-the-art statistical tests and computational techniques.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:58:52 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 19:56:36 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ravindra", "Vikram", ""], ["Drineas", "Petros", ""], ["Grama", "Ananth", ""]]}, {"id": "1805.08656", "submitter": "Ziming Zhang", "authors": "Ziming Zhang", "title": "LMKL-Net: A Fast Localized Multiple Kernel Learning Solver via Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose solving localized multiple kernel learning (LMKL)\nusing LMKL-Net, a feedforward deep neural network. In contrast to previous\nworks, as a learning principle we propose {\\em parameterizing} both the gating\nfunction for learning kernel combination weights and the multiclass classifier\nin LMKL using an attentional network (AN) and a multilayer perceptron (MLP),\nrespectively. In this way we can learn the (nonlinear) decision function in\nLMKL (approximately) by sequential applications of AN and MLP. Empirically on\nbenchmark datasets we demonstrate that overall LMKL-Net can not only outperform\nthe state-of-the-art MKL solvers in terms of accuracy, but also be trained\nabout {\\em two orders of magnitude} faster with much smaller memory footprint\nfor large-scale learning.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 15:12:38 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Zhang", "Ziming", ""]]}, {"id": "1805.08657", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Jean Kossaifi, Stefanos Zafeiriou", "title": "Robust Conditional Generative Adversarial Networks", "comments": "To appear in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative adversarial networks (cGAN) have led to large\nimprovements in the task of conditional image generation, which lies at the\nheart of computer vision. The major focus so far has been on performance\nimprovement, while there has been little effort in making cGAN more robust to\nnoise. The regression (of the generator) might lead to arbitrarily large errors\nin the output, which makes cGAN unreliable for real-world applications. In this\nwork, we introduce a novel conditional GAN model, called RoCGAN, which\nleverages structure in the target space of the model to address the issue. Our\nmodel augments the generator with an unsupervised pathway, which promotes the\noutputs of the generator to span the target manifold even in the presence of\nintense noise. We prove that RoCGAN share similar theoretical properties as GAN\nand experimentally verify that our model outperforms existing state-of-the-art\ncGAN architectures by a large margin in a variety of domains including images\nfrom natural scenes and faces.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 15:14:11 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 07:59:35 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Kossaifi", "Jean", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1805.08661", "submitter": "Xirong Li", "authors": "Xirong Li and Chaoxi Xu and Xiaoxu Wang and Weiyu Lan and Zhengxiong\n  Jia and Gang Yang and Jieping Xu", "title": "COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval", "comments": "accepted for publication as a regular paper in the IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to cross-lingual image annotation and retrieval in\nterms of data and baseline methods. We propose COCO-CN, a novel dataset\nenriching MS-COCO with manually written Chinese sentences and tags. For more\neffective annotation acquisition, we develop a recommendation-assisted\ncollective annotation system, automatically providing an annotator with several\ntags and sentences deemed to be relevant with respect to the pictorial content.\nHaving 20,342 images annotated with 27,218 Chinese sentences and 70,993 tags,\nCOCO-CN is currently the largest Chinese-English dataset that provides a\nunified and challenging platform for cross-lingual image tagging, captioning\nand retrieval. We develop conceptually simple yet effective methods per task\nfor learning from cross-lingual resources. Extensive experiments on the three\ntasks justify the viability of the proposed dataset and methods. Data and code\nare publicly available at https://github.com/li-xirong/coco-cn\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 15:26:15 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 00:24:08 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Li", "Xirong", ""], ["Xu", "Chaoxi", ""], ["Wang", "Xiaoxu", ""], ["Lan", "Weiyu", ""], ["Jia", "Zhengxiong", ""], ["Yang", "Gang", ""], ["Xu", "Jieping", ""]]}, {"id": "1805.08676", "submitter": "Jun Liu", "authors": "Shi Yan, Xue-cheng Tai, Jun Liu, Hai-yang Huang", "title": "Convexity Shape Prior for Level Set based Image Segmentation Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric convexity shape prior preservation method for\nvariational level set based image segmentation methods. Our method is built\nupon the fact that the level set of a convex signed distanced function must be\nconvex. This property enables us to transfer a complicated geometrical\nconvexity prior into a simple inequality constraint on the function. An active\nset based Gauss-Seidel iteration is used to handle this constrained\nminimization problem to get an efficient algorithm. We apply our method to\nregion and edge based level set segmentation models including Chan-Vese (CV)\nmodel with guarantee that the segmented region will be convex. Experimental\nresults show the effectiveness and quality of the proposed model and algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 15:47:41 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yan", "Shi", ""], ["Tai", "Xue-cheng", ""], ["Liu", "Jun", ""], ["Huang", "Hai-yang", ""]]}, {"id": "1805.08685", "submitter": "Luigi Celona", "authors": "Simone Bianco, Luigi Celona, Raimondo Schettini", "title": "Aesthetics Assessment of Images Containing Faces", "comments": "Accepted by ICIP 2018", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451368", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has widely explored the problem of aesthetics assessment of\nimages with generic content. However, few approaches have been specifically\ndesigned to predict the aesthetic quality of images containing human faces,\nwhich make up a massive portion of photos in the web. This paper introduces a\nmethod for aesthetic quality assessment of images with faces. We exploit three\ndifferent Convolutional Neural Networks to encode information regarding\nperceptual quality, global image aesthetics, and facial attributes; then, a\nmodel is trained to combine these features to explicitly predict the aesthetics\nof images containing faces. Experimental results show that our approach\noutperforms existing methods for both binary, i.e. low/high, and continuous\naesthetic score prediction on four different databases in the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 16:00:16 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bianco", "Simone", ""], ["Celona", "Luigi", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1805.08687", "submitter": "Alison O'Neil", "authors": "Alison Q O'Neil, Antanas Kascenas, Joseph Henry, Daniel Wyeth, Matthew\n  Shepherd, Erin Beveridge, Lauren Clunie, Carrie Sansom, Evelina\n  \\v{S}eduikyt\\.e, Keith Muir and Ian Poole", "title": "Attaining human-level performance with atlas location autocontext for\n  anatomical landmark detection in 3D CT data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient neural network method for locating anatomical\nlandmarks in 3D medical CT scans, using atlas location autocontext in order to\nlearn long-range spatial context. Location predictions are made by regression\nto Gaussian heatmaps, one heatmap per landmark. This system allows patchwise\napplication of a shallow network, thus enabling multiple volumetric heatmaps to\nbe predicted concurrently without prohibitive GPU memory requirements. Further,\nthe system allows inter-landmark spatial relationships to be exploited using a\nsimple overdetermined affine mapping that is robust to detection failures and\nocclusion or partial views. Evaluation is performed for 22 landmarks defined on\na range of structures in head CT scans. Models are trained and validated on 201\nscans. Over the final test set of 20 scans which was independently annotated by\n2 human annotators, the neural network reaches an accuracy which matches the\nannotator variability, with similar human and machine patterns of variability\nacross landmark classes.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 09:12:03 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 10:01:07 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["O'Neil", "Alison Q", ""], ["Kascenas", "Antanas", ""], ["Henry", "Joseph", ""], ["Wyeth", "Daniel", ""], ["Shepherd", "Matthew", ""], ["Beveridge", "Erin", ""], ["Clunie", "Lauren", ""], ["Sansom", "Carrie", ""], ["\u0160eduikyt\u0117", "Evelina", ""], ["Muir", "Keith", ""], ["Poole", "Ian", ""]]}, {"id": "1805.08688", "submitter": "Xianzhi Du", "authors": "Xianzhi Du, Mostafa El-Khamy, Vlad I. Morariu, Jungwon Lee, Larry\n  Davis", "title": "Fused Deep Neural Networks for Efficient Pedestrian Detection", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an efficient pedestrian detection system, designed\nby fusion of multiple deep neural network (DNN) systems. Pedestrian candidates\nare first generated by a single shot convolutional multi-box detector at\ndifferent locations with various scales and aspect ratios. The candidate\ngenerator is designed to provide the majority of ground truth pedestrian\nannotations at the cost of a large number of false positives. Then, a\nclassification system using the idea of ensemble learning is deployed to\nimprove the detection accuracy. The classification system further classifies\nthe generated candidates based on opinions of multiple deep verification\nnetworks and a fusion network which utilizes a novel soft-rejection fusion\nmethod to adjust the confidence in the detection results. To improve the\ntraining of the deep verification networks, a novel soft-label method is\ndevised to assign floating point labels to the generated pedestrian candidates.\nA deep context aggregation semantic segmentation network also provides\npixel-level classification of the scene and its results are softly fused with\nthe detection results by the single shot detector. Our pedestrian detector\ncompared favorably to state-of-art methods on all popular pedestrian detection\ndatasets. For example, our fused DNN has better detection accuracy on the\nCaltech Pedestrian dataset than all previous state of art methods, while also\nbeing the fastest. We significantly improved the log-average miss rate on the\nCaltech pedestrian dataset to 7.67% and achieved the new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 00:13:28 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Du", "Xianzhi", ""], ["El-Khamy", "Mostafa", ""], ["Morariu", "Vlad I.", ""], ["Lee", "Jungwon", ""], ["Davis", "Larry", ""]]}, {"id": "1805.08689", "submitter": "Sascha Wirges", "authors": "Sascha Wirges, Tom Fischer, Jesus Balado Frias and Christoph Stiller", "title": "Object Detection and Classification in Occupancy Grid Maps using Deep\n  Convolutional Networks", "comments": "6 pages, 4 tables, 4 figures", "journal-ref": "2018 IEEE Intelligent Transportation Systems Conference (ITSC)", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A detailed environment perception is a crucial component of automated\nvehicles. However, to deal with the amount of perceived information, we also\nrequire segmentation strategies. Based on a grid map environment\nrepresentation, well-suited for sensor fusion, free-space estimation and\nmachine learning, we detect and classify objects using deep convolutional\nneural networks. As input for our networks we use a multi-layer grid map\nefficiently encoding 3D range sensor information. The inference output consists\nof a list of rotated bounding boxes with associated semantic classes. We\nconduct extensive ablation studies, highlight important design considerations\nwhen using grid maps and evaluate our models on the KITTI Bird's Eye View\nbenchmark. Qualitative and quantitative benchmark results show that we achieve\nrobust detection and state of the art accuracy solely using top-view grid maps\nfrom range sensor data.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:31:03 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 17:28:10 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Wirges", "Sascha", ""], ["Fischer", "Tom", ""], ["Frias", "Jesus Balado", ""], ["Stiller", "Christoph", ""]]}, {"id": "1805.08690", "submitter": "Tianlin Liu", "authors": "Tianlin Liu and Arvid Kappas", "title": "Estimating Gradual-Emotional Behavior in One-Minute Videos with ESNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our approach for the OMG- Emotion Challenge 2018.\nThe goal is to produce utterance-level valence and arousal estimations for\nvideos of approximately 1 minute length. We tackle this problem by first\nextracting facial expressions features of videos as time series data, and then\nusing Recurrent Neural Networks of the Echo State Network type to model the\ncorrespondence between the time series data and valence-arousal values.\nExperimentally we show that the proposed approach surpasses the baseline\nmethods provided by the organizers.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 14:34:06 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liu", "Tianlin", ""], ["Kappas", "Arvid", ""]]}, {"id": "1805.08691", "submitter": "Haihao Shen", "authors": "Jiong Gong, Haihao Shen, Guoming Zhang, Xiaoli Liu, Shane Li, Ge Jin,\n  Niharika Maheshwari, Evarist Fomenko, Eden Segal", "title": "Highly Efficient 8-bit Low Precision Inference of Convolutional Neural\n  Networks with IntelCaffe", "comments": "1st Reproducible Tournament on Pareto-efficient Image Classification,\n  co-held with ASPLOS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput and low latency inference of deep neural networks are\ncritical for the deployment of deep learning applications. This paper presents\nthe efficient inference techniques of IntelCaffe, the first Intel optimized\ndeep learning framework that supports efficient 8-bit low precision inference\nand model optimization techniques of convolutional neural networks on Intel\nXeon Scalable Processors. The 8-bit optimized model is automatically generated\nwith a calibration process from FP32 model without the need of fine-tuning or\nretraining. We show that the inference throughput and latency with ResNet-50,\nInception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with\nneglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and\n26X-37X from BVLC Caffe. All these techniques have been open-sourced on\nIntelCaffe GitHub1, and the artifact is provided to reproduce the result on\nAmazon AWS Cloud.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 07:58:33 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Gong", "Jiong", ""], ["Shen", "Haihao", ""], ["Zhang", "Guoming", ""], ["Liu", "Xiaoli", ""], ["Li", "Shane", ""], ["Jin", "Ge", ""], ["Maheshwari", "Niharika", ""], ["Fomenko", "Evarist", ""], ["Segal", "Eden", ""]]}, {"id": "1805.08692", "submitter": "Amanda Ramcharan", "authors": "Amanda Ramcharan, Peter McCloskey, Kelsee Baranowski, Neema Mbilinyi,\n  Latifa Mrisho, Mathias Ndalahwa, James Legg, and David Hughes", "title": "Assessing a mobile-based deep learning model for plant disease\n  surveillance", "comments": "12 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural network models (CNNs) have made major advances in\ncomputer vision tasks in the last five years. Given the challenge in collecting\nreal world datasets, most studies report performance metrics based on available\nresearch datasets. In scenarios where CNNs are to be deployed on images or\nvideos from mobile devices, models are presented with new challenges due to\nlighting, angle, and camera specifications, which are not accounted for in\nresearch datasets. It is essential for assessment to also be conducted on real\nworld datasets if such models are to be reliably integrated with products and\nservices in society. Plant disease datasets can be used to test CNNs in real\ntime and gain insight into real world performance. We train a CNN object\ndetection model to identify foliar symptoms of diseases (or lack thereof) in\ncassava (Manihot esculenta Crantz). We then deploy the model on a mobile app\nand test its performance on mobile images and video of 720 diseased leaflets in\nan agricultural field in Tanzania. Within each disease category we test two\nlevels of severity of symptoms - mild and pronounced, to assess the model\nperformance for early detection of symptoms. In both severities we see a\ndecrease in the F-1 score for real world images and video. The F-1 score\ndropped by 32% for pronounced symptoms in real world images (the closest data\nto the training data) due to a drop in model recall. If the potential of\nsmartphone CNNs are to be realized our data suggest it is crucial to consider\ntuning precision and recall performance in order to achieve the desired\nperformance in real world settings. In addition, the varied performance related\nto different input data (image or video) is an important consideration for the\ndesign of CNNs in real world applications.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 15:07:29 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ramcharan", "Amanda", ""], ["McCloskey", "Peter", ""], ["Baranowski", "Kelsee", ""], ["Mbilinyi", "Neema", ""], ["Mrisho", "Latifa", ""], ["Ndalahwa", "Mathias", ""], ["Legg", "James", ""], ["Hughes", "David", ""]]}, {"id": "1805.08693", "submitter": "Brian DeCost", "authors": "Brian L. DeCost and Bo Lei and Toby Francis and Elizabeth A. Holm", "title": "High throughput quantitative metallography for complex microstructures\n  using deep learning: A case study in ultrahigh carbon steel", "comments": "Updated with minor revisions reflecting the review process at\n  Microscopy and Microanalysis. Full supplementary materials will be available\n  at https://holmgroup.github.io/publications/", "journal-ref": null, "doi": "10.1017/S1431927618015635", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We apply a deep convolutional neural network segmentation model to enable\nnovel automated microstructure segmentation applications for complex\nmicrostructures typically evaluated manually and subjectively. We explore two\nmicrostructure segmentation tasks in an openly-available ultrahigh carbon steel\nmicrostructure dataset: segmenting cementite particles in the spheroidized\nmatrix, and segmenting larger fields of view featuring grain boundary carbide,\nspheroidized particle matrix, particle-free grain boundary denuded zone, and\nWidmanst\\\"atten cementite. We also demonstrate how to combine these data-driven\nmicrostructure segmentation models to obtain empirical cementite particle size\nand denuded zone width distributions from more complex micrographs containing\nmultiple microconstituents. The full annotated dataset is available on\nmaterialsdata.nist.gov (https://materialsdata.nist.gov/handle/11256/964).\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:22:34 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 19:29:32 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["DeCost", "Brian L.", ""], ["Lei", "Bo", ""], ["Francis", "Toby", ""], ["Holm", "Elizabeth A.", ""]]}, {"id": "1805.08694", "submitter": "Markus Haltmeier", "authors": "Hessel Tuinhof, Clemens Pirker, Markus Haltmeier", "title": "Image Based Fashion Product Recommendation with Deep Learning", "comments": null, "journal-ref": "LOD: International Conference on Machine Learning, Optimization,\n  and Data Science Machine Learning, Optimization, and Data Science 4th\n  International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018,\n  Revised Selected Papers", "doi": "10.1007/978-3-030-13709-0_40", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a two-stage deep learning framework that recommends fashion images\nbased on other input images of similar style. For that purpose, a neural\nnetwork classifier is used as a data-driven, visually-aware feature extractor.\nThe latter then serves as input for similarity-based recommendations using a\nranking algorithm. Our approach is tested on the publicly available Fashion\ndataset. Initialization strategies using transfer learning from larger product\ndatabases are presented. Combined with more traditional content-based\nrecommendation systems, our framework can help to increase robustness and\nperformance, for example, by better matching a particular customer style.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 18:14:51 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 21:05:33 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Tuinhof", "Hessel", ""], ["Pirker", "Clemens", ""], ["Haltmeier", "Markus", ""]]}, {"id": "1805.08695", "submitter": "Panagiotis Mousouliotis", "authors": "Panagiotis G. Mousouliotis, Loukas P. Petrou", "title": "SqueezeJet: High-level Synthesis Accelerator Design for Deep\n  Convolutional Neural Networks", "comments": "The final publication is available at Springer via\n  https://doi.org/10.1007/978-3-319-78890-6_5", "journal-ref": null, "doi": "10.1007/978-3-319-78890-6_5", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have dominated the pattern recognition\nscene by providing much more accurate solutions in computer vision problems\nsuch as object recognition and object detection. Most of these solutions come\nat a huge computational cost, requiring billions of multiply-accumulate\noperations and, thus, making their use quite challenging in real-time\napplications that run on embedded mobile (resource-power constrained) hardware.\nThis work presents the architecture, the high-level synthesis design, and the\nimplementation of SqueezeJet, an FPGA accelerator for the inference phase of\nthe SqueezeNet DCNN architecture, which is designed specifically for use in\nembedded systems. Results show that SqueezeJet can achieve 15.16 times speed-up\ncompared to the software implementation of SqueezeNet running on an embedded\nmobile processor with less than 1% drop in top-5 accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 21:56:33 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mousouliotis", "Panagiotis G.", ""], ["Petrou", "Loukas P.", ""]]}, {"id": "1805.08698", "submitter": "Junwen Bai", "authors": "Junwen Bai, Zihang Lai, Runzhe Yang, Yexiang Xue, John Gregoire, Carla\n  Gomes", "title": "End-to-End Refinement Guided by Pre-trained Prototypical Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world tasks involve identifying patterns from data satisfying\nbackground or prior knowledge. In domains like materials discovery, due to the\nflaws and biases in raw experimental data, the identification of X-ray\ndiffraction patterns (XRD) often requires a huge amount of manual work in\nfinding refined phases that are similar to the ideal theoretical ones.\nAutomatically refining the raw XRDs utilizing the simulated theoretical data is\nthus desirable. We propose imitation refinement, a novel approach to refine\nimperfect input patterns, guided by a pre-trained classifier incorporating\nprior knowledge from simulated theoretical data, such that the refined patterns\nimitate the ideal data. The classifier is trained on the ideal simulated data\nto classify patterns and learns an embedding space where each class is\nrepresented by a prototype. The refiner learns to refine the imperfect patterns\nwith small modifications, such that their embeddings are closer to the\ncorresponding prototypes. We show that the refiner can be trained in both\nsupervised and unsupervised fashions. We further illustrate the effectiveness\nof the proposed approach both qualitatively and quantitatively in a digit\nrefinement task and an X-ray diffraction pattern refinement task in materials\ndiscovery.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 22:18:24 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 16:51:58 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Bai", "Junwen", ""], ["Lai", "Zihang", ""], ["Yang", "Runzhe", ""], ["Xue", "Yexiang", ""], ["Gregoire", "John", ""], ["Gomes", "Carla", ""]]}, {"id": "1805.08699", "submitter": "Sze Teng Liong", "authors": "Sze-Teng Liong, Y.S. Gan, Wei-Chuen Yau, Yen-Chang Huang, Tan Lit Ken", "title": "OFF-ApexNet on Micro-expression Recognition System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a person attempts to conceal an emotion, the genuine emotion is manifest\nas a micro-expression. Exploration of automatic facial micro-expression\nrecognition systems is relatively new in the computer vision domain. This is\ndue to the difficulty in implementing optimal feature extraction methods to\ncope with the subtlety and brief motion characteristics of the expression. Most\nof the existing approaches extract the subtle facial movements based on\nhand-crafted features. In this paper, we address the micro-expression\nrecognition task with a convolutional neural network (CNN) architecture, which\nwell integrates the features extracted from each video. A new feature\ndescriptor, Optical Flow Features from Apex frame Network (OFF-ApexNet) is\nintroduced. This feature descriptor combines the optical ow guided context with\nthe CNN. Firstly, we obtain the location of the apex frame from each video\nsequence as it portrays the highest intensity of facial motion among all\nframes. Then, the optical ow information are attained from the apex frame and a\nreference frame (i.e., onset frame). Finally, the optical flow features are fed\ninto a pre-designed CNN model for further feature enhancement as well as to\ncarry out the expression classification. To evaluate the effectiveness of\nOFF-ApexNet, comprehensive evaluations are conducted on three public\nspontaneous micro-expression datasets (i.e., SMIC, CASME II and SAMM). The\npromising recognition result suggests that the proposed method can optimally\ndescribe the significant micro-expression details. In particular, we report\nthat, in a multi-database with leave-one-subject-out cross-validation\nexperimental protocol, the recognition performance reaches 74.60% of\nrecognition accuracy and F-measure of 71.04%. We also note that this is the\nfirst work that performs cross-dataset validation on three databases in this\ndomain.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 02:22:47 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liong", "Sze-Teng", ""], ["Gan", "Y. S.", ""], ["Yau", "Wei-Chuen", ""], ["Huang", "Yen-Chang", ""], ["Ken", "Tan Lit", ""]]}, {"id": "1805.08700", "submitter": "Saifuddin Hitawala", "authors": "Saifuddin Hitawala", "title": "Evaluating ResNeXt Model Architecture for Image Classification", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods have been successfully applied to\nimage classification tasks. Many such deep neural networks exist today that can\neasily differentiate cats from dogs. One such model is the ResNeXt model that\nuses a homogeneous, multi-branch architecture for image classification. This\npaper aims at implementing and evaluating the ResNeXt model architecture on\nsubsets of the CIFAR-10 dataset. It also tweaks the original ResNeXt\nhyper-parameters such as cardinality, depth and base-width and compares the\nperformance of the modified model with the original. Analysis of the\nexperiments performed in this paper show that a slight decrease in depth or\nbase-width does not affect the performance of the model much leading to\ncomparable results.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 20:41:27 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hitawala", "Saifuddin", ""]]}, {"id": "1805.08702", "submitter": "Xavier-Lewis Palmer", "authors": "Darlington Ahiale Akogo and Xavier-Lewis Palmer", "title": "ScaffoldNet: Detecting and Classifying Biomedical Polymer-Based\n  Scaffolds via a Convolutional Neural Network", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We developed a Convolutional Neural Network model to identify and classify\nAirbrushed (alternatively known as Blow-spun), Electrospun and Steel Wire\nscaffolds. Our model ScaffoldNet is a 6-layer Convolutional Neural Network\ntrained and tested on 3,043 images of Airbrushed, Electrospun and Steel Wire\nscaffolds. The model takes in as input an imaged scaffold and then outputs the\nscaffold type (Airbrushed, Electrospun or Steel Wire) as predicted\nprobabilities for the 3 classes. Our model scored a 99.44% Accuracy,\ndemonstrating potential for adaptation to investigating and solving complex\nmachine learning problems aimed at abstract spatial contexts, or in screening\ncomplex, biological, fibrous structures seen in cortical bone and fibrous\nshells.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:19:40 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Akogo", "Darlington Ahiale", ""], ["Palmer", "Xavier-Lewis", ""]]}, {"id": "1805.08703", "submitter": "Jin Wu", "authors": "Jin Wu, Ming Liu, Zebo Zhou, Rui Li", "title": "Fast Symbolic 3D Registration Solution", "comments": "10 pages, 7 figures, 5 tables", "journal-ref": "IEEE Transactions on Automation Science and Engineering, 2019", "doi": "10.1109/TASE.2019.2942324", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D registration has always been performed invoking singular value\ndecomposition (SVD) or eigenvalue decomposition (EIG) in real engineering\npractices. However, these numerical algorithms suffer from uncertainty of\nconvergence in many cases. A novel fast symbolic solution is proposed in this\npaper by following our recent publication in this journal. The equivalence\nanalysis shows that our previous solver can be converted to deal with the 3D\nregistration problem. Rather, the computation procedure is studied for further\nsimplification of computing without complex-number support. Experimental\nresults show that the proposed solver does not loose accuracy and robustness\nbut improves the execution speed to a large extent by almost %50 to %80, on\nboth personal computer and embedded processor.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 05:05:06 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 04:07:28 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 07:47:24 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Wu", "Jin", ""], ["Liu", "Ming", ""], ["Zhou", "Zebo", ""], ["Li", "Rui", ""]]}, {"id": "1805.08704", "submitter": "Tian Han", "authors": "Tian Han, Jiawen Wu, and Ying Nian Wu", "title": "Replicating Active Appearance Model by Generator Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent Cell paper [Chang and Tsao, 2017] reports an interesting discovery.\nFor the face stimuli generated by a pre-trained active appearance model (AAM),\nthe responses of neurons in the areas of the primate brain that are responsible\nfor face recognition exhibit strong linear relationship with the shape\nvariables and appearance variables of the AAM that generates the face stimuli.\nIn this paper, we show that this behavior can be replicated by a deep\ngenerative model called the generator network, which assumes that the observed\nsignals are generated by latent random variables via a top-down convolutional\nneural network. Specifically, we learn the generator network from the face\nimages generated by a pre-trained AAM model using variational auto-encoder, and\nwe show that the inferred latent variables of the learned generator network\nhave strong linear relationship with the shape and appearance variables of the\nAAM model that generates the face images. Unlike the AAM model that has an\nexplicit shape model where the shape variables generate the control points or\nlandmarks, the generator network has no such shape model and shape variables.\nYet the generator network can learn the shape knowledge in the sense that some\nof the latent variables of the learned generator network capture the shape\nvariations in the face images generated by AAM.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 04:54:00 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Han", "Tian", ""], ["Wu", "Jiawen", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1805.08705", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Jianmin Li and Bo Zhang", "title": "Semantic Cluster Unary Loss for Efficient Deep Hashing", "comments": "13 pages", "journal-ref": "IEEE Transactions on Image Processing, 2019", "doi": "10.1109/TIP.2019.2891967", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, which has received a broad attention due to its low storage cost and\nfast retrieval speed. With the rapid development of deep learning, deep hashing\nmethods have achieved promising results in efficient information retrieval.\nMost of the existing deep hashing methods adopt pairwise or triplet losses to\ndeal with similarities underlying the data, but the training is difficult and\nless efficient because $O(n^2)$ data pairs and $O(n^3)$ triplets are involved.\nTo address these issues, we propose a novel deep hashing algorithm with unary\nloss which can be trained very efficiently. We first of all introduce a Unary\nUpper Bound of the traditional triplet loss, thus reducing the complexity to\n$O(n)$ and bridging the classification-based unary loss and the triplet loss.\nSecond, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by\nintroducing a modified Unary Upper Bound loss, named Semantic Cluster Unary\nLoss (SCUL). The resultant hashcodes form several compact clusters, which means\nhashcodes in the same cluster have similar semantic information. We also\ndemonstrate that the proposed SCDH is easy to be extended to semi-supervised\nsettings by incorporating the state-of-the-art semi-supervised learning\nalgorithms. Experiments on large-scale datasets show that the proposed method\nis superior to state-of-the-art hashing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 08:59:45 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 04:35:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhang", "Shifeng", ""], ["Li", "Jianmin", ""], ["Zhang", "Bo", ""]]}, {"id": "1805.08706", "submitter": "Jignesh Bhatt Shashikant", "authors": "Jignesh S. Bhatt and N. Padmanabhan", "title": "Automatic Data Registration of Geostationary Payloads for Meteorological\n  Applications at ISRO", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The launch of KALPANA-1 satellite in the year 2002 heralded the establishment\nof an indigenous operational payload for meteorological predictions. This was\nfurther enhanced in the year 2003 with the launching of INSAT-3A satellite. The\nsoftware for generating products from the data of these two satellites was\ntaken up subsequently in the year 2004 and the same was installed at the Indian\nMeteorological Department, New Delhi in January 2006. Registration has been one\nof the most fundamental operations to generate almost all the data products\nfrom the remotely sensed data. Registration is a challenging task due to\ninevitable radiometric and geometric distortions during the acquisition\nprocess. Besides the presence of clouds makes the problem more complicated. In\nthis paper, we present an algorithm for multitemporal and multiband\nregistration. In addition, India facing reference boundaries for the CCD data\nof INSAT-3A have also been generated. The complete implementation is made up of\nthe following steps: 1) automatic identification of the ground control points\n(GCPs) in the sensed data, 2) finding the optimal transformation model based on\nthe match-points, and 3) resampling the transformed imagery to the reference\ncoordinates. The proposed algorithm is demonstrated using the real datasets\nfrom KALPANA-1 and INSAT-3A. Both KALAPANA-1 and INSAT-3A have recently been\ndecommissioned due to lack of fuel, however, the experience gained from them\nhave given rise to a series of meteorological satellites and associated\nsoftware; like INSAT-3D series which give continuous weather forecasting for\nthe country. This paper is not so much focused on the theory (widely available\nin the literature) but concentrates on the implementation of operational\nsoftware.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:41:48 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Bhatt", "Jignesh S.", ""], ["Padmanabhan", "N.", ""]]}, {"id": "1805.08709", "submitter": "Emin Orhan", "authors": "A. Emin Orhan", "title": "A Simple Cache Model for Image Recognition", "comments": "Published as a conference paper at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large-scale image recognition models is computationally expensive.\nThis raises the question of whether there might be simple ways to improve the\ntest performance of an already trained model without having to re-train or\nfine-tune it with new data. Here, we show that, surprisingly, this is indeed\npossible. The key observation we make is that the layers of a deep network\nclose to the output layer contain independent, easily extractable\nclass-relevant information that is not contained in the output layer itself. We\npropose to extract this extra class-relevant information using a simple\nkey-value cache memory to improve the classification performance of the model\nat test time. Our cache memory is directly inspired by a similar cache model\npreviously proposed for language modeling (Grave et al., 2017). This cache\ncomponent does not require any training or fine-tuning; it can be applied to\nany pre-trained model and, by properly setting only two hyper-parameters, leads\nto significant improvements in its classification performance. Improvements are\nobserved across several architectures and datasets. In the cache component,\nusing features extracted from layers close to the output (but not from the\noutput layer itself) as keys leads to the largest improvements. Concatenating\nfeatures from multiple layers to form keys can further improve performance over\nusing single-layer features as keys. The cache component also has a\nregularizing effect, a simple consequence of which is that it substantially\nincreases the robustness of models against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 17:50:14 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 18:24:18 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Orhan", "A. Emin", ""]]}, {"id": "1805.08717", "submitter": "Minh Vo", "authors": "Minh Vo, Ersin Yumer, Kalyan Sunkavalli, Sunil Hadap, Yaser Sheikh,\n  and Srinivasa Narasimhan", "title": "Self-supervised Multi-view Person Association and Its Applications", "comments": "Accepted to IEEE TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2974726", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable markerless motion tracking of people participating in a complex\ngroup activity from multiple moving cameras is challenging due to frequent\nocclusions, strong viewpoint and appearance variations, and asynchronous video\nstreams. To solve this problem, reliable association of the same person across\ndistant viewpoints and temporal instances is essential. We present a\nself-supervised framework to adapt a generic person appearance descriptor to\nthe unlabeled videos by exploiting motion tracking, mutual exclusion\nconstraints, and multi-view geometry. The adapted discriminative descriptor is\nused in a tracking-by-clustering formulation. We validate the effectiveness of\nour descriptor learning on WILDTRACK [14] and three new complex social scenes\ncaptured by multiple cameras with up to 60 people \"in the wild\". We report\nsignificant improvement in association accuracy (up to 18%) and stable and\ncoherent 3D human skeleton tracking (5 to 10 times) over the baseline. Using\nthe reconstructed 3D skeletons, we cut the input videos into a multi-angle\nvideo where the image of a specified person is shown from the best visible\nfront-facing camera. Our algorithm detects inter-human occlusion to determine\nthe camera switching moment while still maintaining the flow of the action\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 16:25:26 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 21:39:20 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 06:16:40 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Vo", "Minh", ""], ["Yumer", "Ersin", ""], ["Sunkavalli", "Kalyan", ""], ["Hadap", "Sunil", ""], ["Sheikh", "Yaser", ""], ["Narasimhan", "Srinivasa", ""]]}, {"id": "1805.08743", "submitter": "Alexandros Kouris", "authors": "Alexandros Kouris, Stylianos I. Venieris, Christos-Savvas Bouganis", "title": "CascadeCNN: Pushing the performance limits of quantisation", "comments": "Accepted at SysML Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents CascadeCNN, an automated toolflow that pushes the\nquantisation limits of any given CNN model, to perform high-throughput\ninference by exploiting the computation time-accuracy trade-off. Without the\nneed for retraining, a two-stage architecture tailored for any given FPGA\ndevice is generated, consisting of a low- and a high-precision unit. A\nconfidence evaluation unit is employed between them to identify misclassified\ncases at run time and forward them to the high-precision unit or terminate\ncomputation. Experiments demonstrate that CascadeCNN achieves a performance\nboost of up to 55% for VGG-16 and 48% for AlexNet over the baseline design for\nthe same resource budget and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:06:02 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Kouris", "Alexandros", ""], ["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1805.08769", "submitter": "Jhilik Bhattacharya", "authors": "Baljit Kaur and Jhilik Bhattacharya", "title": "A Convolutional Feature Map based Deep Network targeted towards Traffic\n  Detection and Classification", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research mainly emphasizes on traffic detection thus essentially\ninvolving object detection and classification. The particular work discussed\nhere is motivated from unsatisfactory attempts of re-using well known\npre-trained object detection networks for domain specific data. In this course,\nsome trivial issues leading to prominent performance drop are identified and\nways to resolve them are discussed. For example, some simple yet relevant\ntricks regarding data collection and sampling prove to be very beneficial.\nAlso, introducing a blur net to deal with blurred real time data is another\nimportant factor promoting performance elevation. We further study the neural\nnetwork design issues for beneficial object classification and involve shared,\nregion-independent convolutional features. Adaptive learning rates to deal with\nsaddle points are also investigated and an average covariance matrix based\npre-conditioned approach is proposed. We also introduce the use of optical flow\nfeatures to accommodate orientation information. Experimental results\ndemonstrate that this results in a steady rise in the performance rate.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:56:02 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kaur", "Baljit", ""], ["Bhattacharya", "Jhilik", ""]]}, {"id": "1805.08798", "submitter": "Jhilik Bhattacharya", "authors": "Baljit Kaur, Jhilik Bhattacharya", "title": "A scene perception system for visually impaired based on object\n  detection and classification using multi-modal DCNN", "comments": "33pages", "journal-ref": null, "doi": "10.1117/1.JEI.28.1.013031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper represents a cost-effective scene perception system aimed towards\nvisually impaired individual. We use an odroid system integrated with an USB\ncamera and USB laser that can be attached on the chest. The system classifies\nthe detected objects along with its distance from the user and provides a voice\noutput. Experimental results provided in this paper use outdoor traffic scenes.\nThe object detection and classification framework exploits a multi-modal fusion\nbased faster RCNN using motion, sharpening and blurring filters for efficient\nfeature representation.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 18:02:08 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kaur", "Baljit", ""], ["Bhattacharya", "Jhilik", ""]]}, {"id": "1805.08801", "submitter": "Xi Zhang", "authors": "Xi Sheryl Zhang, Lifang He, Kun Chen, Yuan Luo, Jiayu Zhou, and Fei\n  Wang", "title": "Multi-View Graph Convolutional Network and Its Applications on\n  Neuroimage Analysis for Parkinson's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parkinson's Disease (PD) is one of the most prevalent neurodegenerative\ndiseases that affects tens of millions of Americans. PD is highly progressive\nand heterogeneous. Quite a few studies have been conducted in recent years on\npredictive or disease progression modeling of PD using clinical and biomarkers\ndata. Neuroimaging, as another important information source for\nneurodegenerative disease, has also arisen considerable interests from the PD\ncommunity. In this paper, we propose a deep learning method based on Graph\nConvolutional Networks (GCN) for fusing multiple modalities of brain images in\nrelationship prediction which is useful for distinguishing PD cases from\ncontrols. On Parkinson's Progression Markers Initiative (PPMI) cohort, our\napproach achieved $0.9537\\pm 0.0587$ AUC, compared with $0.6443\\pm 0.0223$ AUC\nachieved by traditional approaches such as PCA.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 18:11:04 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 04:59:08 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 20:59:51 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 03:55:16 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Zhang", "Xi Sheryl", ""], ["He", "Lifang", ""], ["Chen", "Kun", ""], ["Luo", "Yuan", ""], ["Zhou", "Jiayu", ""], ["Wang", "Fei", ""]]}, {"id": "1805.08805", "submitter": "Yan Wang", "authors": "Yan Wang, Lequn Wang, Yurong You, Xu Zou, Vincent Chen, Serena Li, Gao\n  Huang, Bharath Hariharan, Kilian Q. Weinberger", "title": "Resource Aware Person Re-identification across Multiple Resolutions", "comments": "8 pages, 8 figures, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all people are equally easy to identify: color statistics might be enough\nfor some cases while others might require careful reasoning about high- and\nlow-level details. However, prevailing person re-identification(re-ID) methods\nuse one-size-fits-all high-level embeddings from deep convolutional networks\nfor all cases. This might limit their accuracy on difficult examples or makes\nthem needlessly expensive for the easy ones. To remedy this, we present a new\nperson re-ID model that combines effective embeddings built on multiple\nconvolutional network layers, trained with deep-supervision. On traditional\nre-ID benchmarks, our method improves substantially over the previous\nstate-of-the-art results on all five datasets that we evaluate on. We then\npropose two new formulations of the person re-ID problem under\nresource-constraints, and show how our model can be used to effectively trade\noff accuracy and computation in the presence of resource constraints. Code and\npre-trained models are available at https://github.com/mileyan/DARENet.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 18:17:37 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 00:24:52 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 00:06:00 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Wang", "Yan", ""], ["Wang", "Lequn", ""], ["You", "Yurong", ""], ["Zou", "Xu", ""], ["Chen", "Vincent", ""], ["Li", "Serena", ""], ["Huang", "Gao", ""], ["Hariharan", "Bharath", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1805.08808", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Rongmei Lin, Alan Sullivan", "title": "Deformable Part Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose novel Deformable Part Networks (DPNs) to learn {\\em\npose-invariant} representations for 2D object recognition. In contrast to the\nstate-of-the-art pose-aware networks such as CapsNet \\cite{sabour2017dynamic}\nand STN \\cite{jaderberg2015spatial}, DPNs can be naturally {\\em interpreted} as\nan efficient solver for a challenging detection problem, namely Localized\nDeformable Part Models (LDPMs) where localization is introduced to DPMs as\nanother latent variable for searching for the best poses of objects over all\npixels and (predefined) scales. In particular we construct DPNs as sequences of\nsuch LDPM units to model the semantic and spatial relations among the\ndeformable parts as hierarchical composition and spatial parsing trees.\nEmpirically our 17-layer DPN can outperform both CapsNets and STNs\nsignificantly on affNIST \\cite{sabour2017dynamic}, for instance, by 19.19\\% and\n12.75\\%, respectively, with better generalization and better tolerance to\naffine transformations.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 18:35:28 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Zhang", "Ziming", ""], ["Lin", "Rongmei", ""], ["Sullivan", "Alan", ""]]}, {"id": "1805.08819", "submitter": "Drew Linsley", "authors": "Drew Linsley, Dan Shiebler, Sven Eberhardt and Thomas Serre", "title": "Learning what and where to attend", "comments": "Previously called Global-and-local attention networks for visual\n  recognition. Current version published in ICLR 2019:\n  https://openreview.net/forum?id=BJgLg3R9KQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent gains in visual recognition have originated from the inclusion of\nattention mechanisms in deep convolutional networks (DCNs). Because these\nnetworks are optimized for object recognition, they learn where to attend using\nonly a weak form of supervision derived from image class labels. Here, we\ndemonstrate the benefit of using stronger supervisory signals by teaching DCNs\nto attend to image regions that humans deem important for object recognition.\nWe first describe a large-scale online experiment (ClickMe) used to supplement\nImageNet with nearly half a million human-derived \"top-down\" attention maps.\nUsing human psychophysics, we confirm that the identified top-down features\nfrom ClickMe are more diagnostic than \"bottom-up\" saliency features for rapid\nimage categorization. As a proof of concept, we extend a state-of-the-art\nattention network and demonstrate that adding ClickMe supervision significantly\nimproves its accuracy and yields visual features that are more interpretable\nand more similar to those used by human observers.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:12:47 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 01:29:37 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 15:36:34 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 14:14:34 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Linsley", "Drew", ""], ["Shiebler", "Dan", ""], ["Eberhardt", "Sven", ""], ["Serre", "Thomas", ""]]}, {"id": "1805.08826", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Wouter Kimman, Jesper Dramsch, Steve Purves, Alfredo De\n  la Fuente, Graham Ganssle", "title": "Rapid seismic domain transfer: Seismic velocity inversion and modeling\n  using deep generative neural networks", "comments": "Extended abstract submitted to EAGE 2018, 5 pages, 3 figures", "journal-ref": null, "doi": "10.3997/2214-4609.201800734", "report-no": null, "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional physics-based approaches to infer sub-surface properties such as\nfull-waveform inversion or reflectivity inversion are time-consuming and\ncomputationally expensive. We present a deep-learning technique that eliminates\nthe need for these computationally complex methods by posing the problem as one\nof domain transfer. Our solution is based on a deep convolutional generative\nadversarial network and dramatically reduces computation time. Training based\non two different types of synthetic data produced a neural network that\ngenerates realistic velocity models when applied to a real dataset. The\nsystem's ability to generalize means it is robust against the inherent\noccurrence of velocity errors and artifacts in both training and test datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:32:56 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Mosser", "Lukas", ""], ["Kimman", "Wouter", ""], ["Dramsch", "Jesper", ""], ["Purves", "Steve", ""], ["De la Fuente", "Alfredo", ""], ["Ganssle", "Graham", ""]]}, {"id": "1805.08833", "submitter": "Hamid Tizhoosh", "authors": "Meghana Dinesh Kumar, Morteza Babaie, Hamid Tizhoosh", "title": "Deep Barcodes for Fast Retrieval of Histopathology Scans", "comments": "Accepted for publication in proceedings of the IEEE World Congress on\n  Computational Intelligence (IEEE WCCI), Rio de Janeiro, Brazil, 8-3 July,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the concept of deep barcodes and propose two methods to\ngenerate them in order to expedite the process of classification and retrieval\nof histopathology images. Since binary search is computationally less\nexpensive, in terms of both speed and storage, deep barcodes could be useful\nwhen dealing with big data retrieval. Our experiments use the dataset Kimia\nPath24 to test three pre-trained networks for image retrieval. The dataset\nconsists of 27,055 training images in 24 different classes with large\nvariability, and 1,325 test images for testing. Apart from the high-speed and\nefficiency, results show a surprising retrieval accuracy of 71.62% for deep\nbarcodes, as compared to 68.91% for deep features and 68.53% for compressed\ndeep features.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 23:27:29 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kumar", "Meghana Dinesh", ""], ["Babaie", "Morteza", ""], ["Tizhoosh", "Hamid", ""]]}, {"id": "1805.08841", "submitter": "Sina Honari", "authors": "Joseph Paul Cohen, Margaux Luck, Sina Honari", "title": "Distribution Matching Losses Can Hallucinate Features in Medical Image\n  Translation", "comments": "Published at Medical Image Computing & Computer Assisted Intervention\n  (MICCAI 2018). An abstract is published at the Medical Imaging with Deep\n  Learning Conference (MIDL 2018) as \"How to Cure Cancer (in images) with\n  Unpaired Image Translation\"", "journal-ref": "Medical Image Computing & Computer Assisted Intervention (MICCAI\n  2018 Oral)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses how distribution matching losses, such as those used in\nCycleGAN, when used to synthesize medical images can lead to mis-diagnosis of\nmedical conditions. It seems appealing to use these new image synthesis methods\nfor translating images from a source to a target domain because they can\nproduce high quality images and some even do not require paired data. However,\nthe basis of how these image translation models work is through matching the\ntranslation output to the distribution of the target domain. This can cause an\nissue when the data provided in the target domain has an over or under\nrepresentation of some classes (e.g. healthy or sick). When the output of an\nalgorithm is a transformed image there are uncertainties whether all known and\nunknown class labels have been preserved or changed. Therefore, we recommend\nthat these translated images should not be used for direct interpretation (e.g.\nby doctors) because they may lead to misdiagnosis of patients based on\nhallucinated image features by an algorithm that matches a distribution.\nHowever there are many recent papers that seem as though this is the goal.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 20:06:33 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 22:36:57 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 05:44:12 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Luck", "Margaux", ""], ["Honari", "Sina", ""]]}, {"id": "1805.08874", "submitter": "Debasmit Das", "authors": "Debasmit Das, C.S. George Lee", "title": "Unsupervised Domain Adaptation using Regularized Hyper-graph Matching", "comments": "Final version appeared in IEEE International Conference on Image\n  Processing 2018", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451152", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) addresses the real-world image classification problem\nof discrepancy between training (source) and testing (target) data\ndistributions. We propose an unsupervised DA method that considers the presence\nof only unlabelled data in the target domain. Our approach centers on finding\nmatches between samples of the source and target domains. The matches are\nobtained by treating the source and target domains as hyper-graphs and carrying\nout a class-regularized hyper-graph matching using first-, second- and\nthird-order similarities between the graphs. We have also developed a\ncomputationally efficient algorithm by initially selecting a subset of the\nsamples to construct a graph and then developing a customized optimization\nroutine for graph-matching based on Conditional Gradient and Alternating\nDirection Multiplier Method. This allows the proposed method to be used widely.\nWe also performed a set of experiments on standard object recognition datasets\nto validate the effectiveness of our framework over state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 21:38:38 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 17:18:53 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Das", "Debasmit", ""], ["Lee", "C. S. George", ""]]}, {"id": "1805.08897", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer, Patricia Goldberg, Kathleen St\\\"urmer, Tina Seidel,\n  Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci", "title": "Teacher's Perception in the Classroom", "comments": "Accepted by CVPRW 2018. The Second Workshop on Computational Models\n  Learning Systems and Educational Assessment (CMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability for a teacher to engage all students in active learning processes\nin classroom constitutes a crucial prerequisite for enhancing students\nachievement. Teachers' attentional processes provide important insights into\nteachers' ability to focus their attention on relevant information in the\ncomplexity of classroom interaction and distribute their attention across\nstudents in order to recognize the relevant needs for learning. In this\ncontext, mobile eye tracking is an innovative approach within teaching\neffectiveness research to capture teachers' attentional processes while\nteaching. However, analyzing mobile eye-tracking data by hand is time consuming\nand still limited. In this paper, we introduce a new approach to enhance the\nimpact of mobile eye tracking by connecting it with computer vision. In mobile\neye tracking videos from an educational study using a standardized small group\nsituation, we apply a state-ofthe-art face detector, create face tracklets, and\nintroduce a novel method to cluster faces into the number of identity.\nSubsequently, teachers' attentional focus is calculated per student during a\nteaching unit by associating eye tracking fixations and face tracklets. To the\nbest of our knowledge, this is the first work to combine computer vision and\nmobile eye tracking to model teachers' attention while instructing.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 22:48:47 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Goldberg", "Patricia", ""], ["St\u00fcrmer", "Kathleen", ""], ["Seidel", "Tina", ""], ["Gerjets", "Peter", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1805.08920", "submitter": "Tianyang Li", "authors": "Tianyang Li, Anastasios Kyrillidis, Liu Liu, Constantine Caramanis", "title": "Approximate Newton-based statistical inference using only stochastic\n  gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel statistical inference framework for convex empirical risk\nminimization, using approximate stochastic Newton steps. The proposed algorithm\nis based on the notion of finite differences and allows the approximation of a\nHessian-vector product from first-order information. In theory, our method\nefficiently computes the statistical error covariance in $M$-estimation, both\nfor unregularized convex learning problems and high-dimensional LASSO\nregression, without using exact second order information, or resampling the\nentire data set. We also present a stochastic gradient sampling scheme for\nstatistical inference in non-i.i.d. time series analysis, where we sample\ncontiguous blocks of indices. In practice, we demonstrate the effectiveness of\nour framework on large-scale machine learning problems, that go even beyond\nconvexity: as a highlight, our work can be used to detect certain adversarial\nattacks on neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:07:47 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:23:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Li", "Tianyang", ""], ["Kyrillidis", "Anastasios", ""], ["Liu", "Liu", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1805.08941", "submitter": "Jian-Hao Luo", "authors": "Jian-Hao Luo and Jianxin Wu", "title": "AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient\n  Deep Model Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning is an important family of methods to speed up deep model's\ninference. Previous filter pruning algorithms regard channel pruning and model\nfine-tuning as two independent steps. This paper argues that combining them\ninto a single end-to-end trainable system will lead to better results. We\npropose an efficient channel selection layer, namely AutoPruner, to find less\nimportant filters automatically in a joint training manner. Our AutoPruner\ntakes previous activation responses as an input and generates a true binary\nindex code for pruning. Hence, all the filters corresponding to zero index\nvalues can be removed safely after training. We empirically demonstrate that\nthe gradient information of this channel selection layer is also helpful for\nthe whole model training. By gradually erasing several weak filters, we can\nprevent an excessive drop in model accuracy. Compared with previous\nstate-of-the-art pruning algorithms (including training from scratch),\nAutoPruner achieves significantly better performance. Furthermore, ablation\nexperiments show that the proposed novel mini-batch pooling and binarization\noperations are vital for the success of filter pruning.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 03:05:48 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 02:28:29 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 06:11:48 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Luo", "Jian-Hao", ""], ["Wu", "Jianxin", ""]]}, {"id": "1805.08946", "submitter": "Hsiuhan Lexie Yang", "authors": "Hsiuhan Lexie Yang, Jiangye Yuan, Dalton Lunga, Melanie Laverdiere,\n  Amy Rose, Budhendra Bhaduri", "title": "Building Extraction at Scale using Convolutional Neural Network: Mapping\n  of the United States", "comments": "Accepted by IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing up-to-date large scale building maps is essential to understand\nurban dynamics, such as estimating population, urban planning and many other\napplications. Although many computer vision tasks has been successfully carried\nout with deep convolutional neural networks, there is a growing need to\nunderstand their large scale impact on building mapping with remote sensing\nimagery. Taking advantage of the scalability of CNNs and using only few areas\nwith the abundance of building footprints, for the first time we conduct a\ncomparative analysis of four state-of-the-art CNNs for extracting building\nfootprints across the entire continental United States. The four CNN\narchitectures namely: branch-out CNN, fully convolutional neural network (FCN),\nconditional random field as recurrent neural network (CRFasRNN), and SegNet,\nsupport semantic pixel-wise labeling and focus on capturing textural\ninformation at multi-scale. We use 1-meter resolution aerial images from\nNational Agriculture Imagery Program (NAIP) as the test-bed, and compare the\nextraction results across the four methods. In addition, we propose to combine\nsigned-distance labels with SegNet, the preferred CNN architecture identified\nby our extensive evaluations, to advance building extraction results to\ninstance level. We further demonstrate the usefulness of fusing additional near\nIR information into the building extraction framework. Large scale experimental\nevaluations are conducted and reported using metrics that include: precision,\nrecall rate, intersection over union, and the number of buildings extracted.\nWith the improved CNN model and no requirement of further post-processing, we\nhave generated building maps for the United States. The quality of extracted\nbuildings and processing time demonstrated the proposed CNN-based framework\nfits the need of building extraction at scale.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 03:28:05 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Yang", "Hsiuhan Lexie", ""], ["Yuan", "Jiangye", ""], ["Lunga", "Dalton", ""], ["Laverdiere", "Melanie", ""], ["Rose", "Amy", ""], ["Bhaduri", "Budhendra", ""]]}, {"id": "1805.08960", "submitter": "Yong Man Ro", "authors": "Seong Tae Kim, Hakmin Lee, Hak Gu Kim, Yong Man Ro", "title": "ICADx: Interpretable computer aided diagnosis of breast masses", "comments": "This paper was presented at SPIE Medical Imaging 2018, Houston, TX,\n  USA", "journal-ref": null, "doi": "10.1117/12.2293570", "report-no": "Proc. SPIE 10575, Medical Imaging 2018: Computer-Aided Diagnosis,\n  1057522 (27 February 2018)", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel computer aided diagnosis (CADx) framework is devised\nto investigate interpretability for classifying breast masses. Recently, a deep\nlearning technology has been successfully applied to medical image analysis\nincluding CADx. Existing deep learning based CADx approaches, however, have a\nlimitation in explaining the diagnostic decision. In real clinical practice,\nclinical decisions could be made with reasonable explanation. So current deep\nlearning approaches in CADx are limited in real world deployment. In this\npaper, we investigate interpretability in CADx with the proposed interpretable\nCADx (ICADx) framework. The proposed framework is devised with a generative\nadversarial network, which consists of interpretable diagnosis network and\nsynthetic lesion generative network to learn the relationship between\nmalignancy and a standardized description (BI-RADS). The lesion generative\nnetwork and the interpretable diagnosis network compete in an adversarial\nlearning so that the two networks are improved. The effectiveness of the\nproposed method was validated on public mammogram database. Experimental\nresults showed that the proposed ICADx framework could provide the\ninterpretability of mass as well as mass classification. It was mainly\nattributed to the fact that the proposed method was effectively trained to find\nthe relationship between malignancy and interpretations via the adversarial\nlearning. These results imply that the proposed ICADx framework could be a\npromising approach to develop the CADx system.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 04:52:06 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kim", "Seong Tae", ""], ["Lee", "Hakmin", ""], ["Kim", "Hak Gu", ""], ["Ro", "Yong Man", ""]]}, {"id": "1805.08961", "submitter": "Sungheon Park", "authors": "Sungheon Park and Nojun Kwak", "title": "3D Human Pose Estimation with Relational Networks", "comments": "BMVC 2018, source code: https://github.com/sungheonpark/3D_HPE_RN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel 3D human pose estimation algorithm from a\nsingle image based on neural networks. We adopted the structure of the\nrelational networks in order to capture the relations among different body\nparts. In our method, each pair of different body parts generates features, and\nthe average of the features from all the pairs are used for 3D pose estimation.\nIn addition, we propose a dropout method that can be used in relational\nmodules, which inherently imposes robustness to the occlusions. The proposed\nnetwork achieves state-of-the-art performance for 3D pose estimation in Human\n3.6M dataset, and it effectively produces plausible results even in the\nexistence of missing joints.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 05:12:36 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 13:49:22 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Park", "Sungheon", ""], ["Kwak", "Nojun", ""]]}, {"id": "1805.08969", "submitter": "Pei Guo", "authors": "Pei Guo, Connor Anderson, Kolten Pearson, Ryan Farrell", "title": "Neural Network Interpretation via Fine Grained Textual Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current visualization based network interpretation methodssuffer from lacking\nsemantic-level information. In this paper, we introduce the novel task of\ninterpreting classification models using fine grained textual summarization.\nAlong with the label prediction, the network will generate a sentence\nexplaining its decision. Constructing a fully annotated dataset of filter|text\npairs is unrealistic because of image to filter response function complexity.\nWe instead propose a weakly-supervised learning algorithm leveraging\noff-the-shelf image caption annotations. Central to our algorithm is the\nfilter-level attribute probability density function (p.d.f.), learned as a\nconditional probability through Bayesian inference with the input image and its\nfeature map as latent variables. We show our algorithm faithfully reflects the\nfeatures learned by the model using rigorous applications like attribute based\nimage retrieval and unsupervised text grounding. We further show that the\ntextual summarization process can help in understanding network failure\npatterns and can provide clues for further improvements.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 05:54:15 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 18:32:52 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Guo", "Pei", ""], ["Anderson", "Connor", ""], ["Pearson", "Kolten", ""], ["Farrell", "Ryan", ""]]}, {"id": "1805.08970", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson and Aydogan Ozcan", "title": "Toward a Thinking Microscope: Deep Learning in Optical Microscopy and\n  Image Reconstruction", "comments": null, "journal-ref": "OPN (2018)", "doi": "10.1364/OPN.29.7.000034", "report-no": null, "categories": "cs.LG cs.CV physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss recently emerging applications of the state-of-art deep learning\nmethods on optical microscopy and microscopic image reconstruction, which\nenable new transformations among different modes and modalities of microscopic\nimaging, driven entirely by image data. We believe that deep learning will\nfundamentally change both the hardware and image reconstruction methods used in\noptical microscopy in a holistic manner.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 05:54:51 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1805.08973", "submitter": "Min Wang", "authors": "Min Wang, Xipeng Chen, Wentao Liu, Chen Qian, Liang Lin, Lizhuang Ma", "title": "DRPose3D: Depth Ranking in 3D Human Pose Estimation", "comments": "Accepted by the 27th International Joint Conference on Artificial\n  Intelligence (IJCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-stage depth ranking based method (DRPose3D)\nto tackle the problem of 3D human pose estimation. Instead of accurate 3D\npositions, the depth ranking can be identified by human intuitively and learned\nusing the deep neural network more easily by solving classification problems.\nMoreover, depth ranking contains rich 3D information. It prevents the 2D-to-3D\npose regression in two-stage methods from being ill-posed. In our method,\nfirstly, we design a Pairwise Ranking Convolutional Neural Network (PRCNN) to\nextract depth rankings of human joints from images. Secondly, a coarse-to-fine\n3D Pose Network(DPNet) is proposed to estimate 3D poses from both depth\nrankings and 2D human joint locations. Additionally, to improve the generality\nof our model, we introduce a statistical method to augment depth rankings. Our\napproach outperforms the state-of-the-art methods in the Human3.6M benchmark\nfor all three testing protocols, indicating that depth ranking is an essential\ngeometric feature which can be learned to improve the 3D pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 06:05:48 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 09:15:32 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Wang", "Min", ""], ["Chen", "Xipeng", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Lin", "Liang", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1805.08974", "submitter": "Simon Kornblith", "authors": "Simon Kornblith, Jonathon Shlens, Quoc V. Le", "title": "Do Better ImageNet Models Transfer Better?", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a cornerstone of computer vision, yet little work has\nbeen done to evaluate the relationship between architecture and transfer. An\nimplicit hypothesis in modern computer vision research is that models that\nperform better on ImageNet necessarily perform better on other vision tasks.\nHowever, this hypothesis has never been systematically tested. Here, we compare\nthe performance of 16 classification networks on 12 image classification\ndatasets. We find that, when networks are used as fixed feature extractors or\nfine-tuned, there is a strong correlation between ImageNet accuracy and\ntransfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting,\nwe find that this relationship is very sensitive to the way in which networks\nare trained on ImageNet; many common forms of regularization slightly improve\nImageNet accuracy but yield penultimate layer features that are much worse for\ntransfer learning. Additionally, we find that, on two small fine-grained image\nclassification datasets, pretraining on ImageNet provides minimal benefits,\nindicating the learned features from ImageNet do not transfer well to\nfine-grained tasks. Together, our results show that ImageNet architectures\ngeneralize well across datasets, but ImageNet features are less general than\npreviously suggested.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 06:12:35 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 20:14:42 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 16:25:07 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kornblith", "Simon", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1805.08975", "submitter": "Peter Karkus", "authors": "Peter Karkus, David Hsu and Wee Sun Lee", "title": "Particle Filter Networks with Application to Visual Localization", "comments": "CoRL 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filtering is a powerful approach to sequential state estimation and\nfinds application in many domains, including robot localization, object\ntracking, etc. To apply particle filtering in practice, a critical challenge is\nto construct probabilistic system models, especially for systems with complex\ndynamics or rich sensory inputs such as camera images. This paper introduces\nthe Particle Filter Network (PFnet), which encodes both a system model and a\nparticle filter algorithm in a single neural network. The PF-net is fully\ndifferentiable and trained end-to-end from data. Instead of learning a generic\nsystem model, it learns a model optimized for the particle filter algorithm. We\napply the PF-net to a visual localization task, in which a robot must localize\nin a rich 3-D world, using only a schematic 2-D floor map. In simulation\nexperiments, PF-net consistently outperforms alternative learning\narchitectures, as well as a traditional model-based method, under a variety of\nsensor inputs. Further, PF-net generalizes well to new, unseen environments.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 06:21:08 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 17:44:03 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 17:23:11 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Karkus", "Peter", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1805.08982", "submitter": "Chenglong Li", "authors": "Chenglong Li, Xinyan Liang, Yijuan Lu, Nan Zhao, Jin Tang", "title": "RGB-T Object Tracking:Benchmark and Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Thermal (RGB-T) object tracking receives more and more attention due to\nthe strongly complementary benefits of thermal information to visible data.\nHowever, RGB-T research is limited by lacking a comprehensive evaluation\nplatform. In this paper, we propose a large-scale video benchmark dataset for\nRGB-T tracking.It has three major advantages over existing ones: 1) Its size is\nsufficiently large for large-scale performance evaluation (total frame number:\n234K, maximum frame per sequence: 8K). 2) The alignment between RGB-T sequence\npairs is highly accurate, which does not need pre- or post-processing. 3) The\nocclusion levels are annotated for occlusion-sensitive performance analysis of\ndifferent tracking algorithms.Moreover, we propose a novel graph-based approach\nto learn a robust object representation for RGB-T tracking. In particular, the\ntracked object is represented with a graph with image patches as nodes. This\ngraph including graph structure, node weights and edge weights is dynamically\nlearned in a unified ADMM (alternating direction method of multipliers)-based\noptimization framework, in which the modality weights are also incorporated for\nadaptive fusion of multiple source data.Extensive experiments on the\nlarge-scale dataset are executed to demonstrate the effectiveness of the\nproposed tracker against other state-of-the-art tracking methods. We also\nprovide new insights and potential research directions to the field of RGB-T\nobject tracking.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 07:13:39 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Li", "Chenglong", ""], ["Liang", "Xinyan", ""], ["Lu", "Yijuan", ""], ["Zhao", "Nan", ""], ["Tang", "Jin", ""]]}, {"id": "1805.08995", "submitter": "Tao Xu", "authors": "Tao Xu, Kun Sun, Wenbing Tao", "title": "GPU Accelerated Cascade Hashing Image Matching for Large Scale 3D\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature point matching is a key step in Structure from Motion(SFM).\nHowever, it is becoming more and more time consuming because the number of\nimages is getting larger and larger. In this paper, we proposed a GPU\naccelerated image matching method with improved Cascade Hashing. Firstly, we\npropose a Disk-Memory-GPU data exchange strategy and optimize the load order of\ndata, so that the proposed method can deal with big data. Next, we parallelize\nthe Cascade Hashing method on GPU. An improved parallel reduction and an\nimproved parallel hashing ranking are proposed to fulfill this task. Finally,\nextensive experiments show that our image matching is about 20 times faster\nthan SiftGPU on the same graphics card, nearly 100 times faster than the CPU\nCasHash method and hundreds of times faster than the CPU Kd-Tree based matching\nmethod. Further more, we introduce the epipolar constraint to the proposed\nmethod, and use the epipolar geometry to guide the feature matching procedure,\nwhich further reduces the matching cost.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 07:57:01 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Xu", "Tao", ""], ["Sun", "Kun", ""], ["Tao", "Wenbing", ""]]}, {"id": "1805.09019", "submitter": "Qingzhong Wang", "authors": "Qingzhong Wang and Antoni B. Chan", "title": "CNN+CNN: Convolutional Decoders for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a challenging task that combines the field of computer\nvision and natural language processing. A variety of approaches have been\nproposed to achieve the goal of automatically describing an image, and\nrecurrent neural network (RNN) or long-short term memory (LSTM) based models\ndominate this field. However, RNNs or LSTMs cannot be calculated in parallel\nand ignore the underlying hierarchical structure of a sentence. In this paper,\nwe propose a framework that only employs convolutional neural networks (CNNs)\nto generate captions. Owing to parallel computing, our basic model is around 3\ntimes faster than NIC (an LSTM-based model) during training time, while also\nproviding better results. We conduct extensive experiments on MSCOCO and\ninvestigate the influence of the model width and depth. Compared with\nLSTM-based models that apply similar attention mechanisms, our proposed models\nachieves comparable scores of BLEU-1,2,3,4 and METEOR, and higher scores of\nCIDEr. We also test our model on the paragraph annotation dataset, and get\nhigher CIDEr score compared with hierarchical LSTMs\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:16:59 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Wang", "Qingzhong", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1805.09028", "submitter": "Thomas Joy", "authors": "Thomas Joy, Alban Desmaison, Thalaiyasingam Ajanthan, Rudy Bunel,\n  Mathieu Salzmann, Pushmeet Kohli, Philip H.S. Torr and M. Pawan Kumar", "title": "Efficient Relaxations for Dense CRFs with Sparse Higher Order Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense conditional random fields (CRFs) have become a popular framework for\nmodelling several problems in computer vision such as stereo correspondence and\nmulti-class semantic segmentation. By modelling long-range interactions, dense\nCRFs provide a labelling that captures finer detail than their sparse\ncounterparts. Currently, the state-of-the-art algorithm performs mean-field\ninference using a filter-based method but fails to provide a strong theoretical\nguarantee on the quality of the solution. A question naturally arises as to\nwhether it is possible to obtain a maximum a posteriori (MAP) estimate of a\ndense CRF using a principled method. Within this paper, we show that this is\nindeed possible. We will show that, by using a filter-based method, continuous\nrelaxations of the MAP problem can be optimised efficiently using\nstate-of-the-art algorithms. Specifically, we will solve a quadratic\nprogramming (QP) relaxation using the Frank-Wolfe algorithm and a linear\nprogramming (LP) relaxation by developing a proximal minimisation framework. By\nexploiting labelling consistency in the higher-order potentials and utilising\nthe filter-based method, we are able to formulate the above algorithms such\nthat each iteration has a complexity linear in the number of classes and random\nvariables. The presented algorithms can be applied to any labelling problem\nusing a dense CRF with sparse higher-order potentials. In this paper, we use\nsemantic segmentation as an example application as it demonstrates the ability\nof the algorithm to scale to dense CRFs with large dimensions. We perform\nexperiments on the Pascal dataset to indicate that the presented algorithms are\nable to attain lower energies than the mean-field inference method.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:34:51 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 11:58:27 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Joy", "Thomas", ""], ["Desmaison", "Alban", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Bunel", "Rudy", ""], ["Salzmann", "Mathieu", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1805.09033", "submitter": "Xi Yang", "authors": "Xi Yang, Xinbo Gao, Bin Song, Nannan Wang, Dong Yang", "title": "Saliency deep embedding for aurora image search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable success in the field of image\nsearch. However, the state-of-the-art algorithms are trained and tested for\nnatural images captured with ordinary cameras. In this paper, we aim to explore\na new search method for images captured with circular fisheye lens, especially\nthe aurora images. To reduce the interference from uninformative regions and\nfocus on the most interested regions, we propose a saliency proposal network\n(SPN) to replace the region proposal network (RPN) in the recent Mask R-CNN. In\nour SPN, the centers of the anchors are not distributed in a rectangular\nmeshing manner, but exhibit spherical distortion. Additionally, the directions\nof the anchors are along the deformation lines perpendicular to the magnetic\nmeridian, which perfectly accords with the imaging principle of circular\nfisheye lens. Extensive experiments are performed on the big aurora data,\ndemonstrating the superiority of our method in both search accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 09:55:39 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Yang", "Xi", ""], ["Gao", "Xinbo", ""], ["Song", "Bin", ""], ["Wang", "Nannan", ""], ["Yang", "Dong", ""]]}, {"id": "1805.09092", "submitter": "Andrea Zunino", "authors": "Andrea Zunino, Sarah Adel Bargal, Pietro Morerio, Jianming Zhang, Stan\n  Sclaroff, Vittorio Murino", "title": "Excitation Dropout: Encouraging Plasticity in Deep Neural Networks", "comments": "This work is published in the International Journal of Computer\n  Vision (IJCV) in 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a guided dropout regularizer for deep networks based on the\nevidence of a network prediction defined as the firing of neurons in specific\npaths. In this work, we utilize the evidence at each neuron to determine the\nprobability of dropout, rather than dropping out neurons uniformly at random as\nin standard dropout. In essence, we dropout with higher probability those\nneurons which contribute more to decision making at training time. This\napproach penalizes high saliency neurons that are most relevant for model\nprediction, i.e. those having stronger evidence. By dropping such high-saliency\nneurons, the network is forced to learn alternative paths in order to maintain\nloss minimization, resulting in a plasticity-like behavior, a characteristic of\nhuman brains too. We demonstrate better generalization ability, an increased\nutilization of network neurons, and a higher resilience to network compression\nusing several metrics over four image/video recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 12:32:41 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 20:21:36 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 18:23:30 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zunino", "Andrea", ""], ["Bargal", "Sarah Adel", ""], ["Morerio", "Pietro", ""], ["Zhang", "Jianming", ""], ["Sclaroff", "Stan", ""], ["Murino", "Vittorio", ""]]}, {"id": "1805.09097", "submitter": "Jayoung Yoo", "authors": "Jaeyoung Yoo, Sang-ho Lee, Nojun Kwak", "title": "Image Restoration by Estimating Frequency Distribution of Local Patches", "comments": "9 pages, 5 figures, Accepted as a poster in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to solve the image restoration problem,\nwhich tries to restore the details of a corrupted image, especially due to the\nloss caused by JPEG compression. We have treated an image in the frequency\ndomain to explicitly restore the frequency components lost during image\ncompression. In doing so, the distribution in the frequency domain is learned\nusing the cross entropy loss. Unlike recent approaches, we have reconstructed\nthe details of an image without using the scheme of adversarial training.\nRather, the image restoration problem is treated as a classification problem to\ndetermine the frequency coefficient for each frequency band in an image patch.\nIn this paper, we show that the proposed method effectively restores a\nJPEG-compressed image with more detailed high frequency components, making the\nrestored image more vivid.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 12:50:08 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Yoo", "Jaeyoung", ""], ["Lee", "Sang-ho", ""], ["Kwak", "Nojun", ""]]}, {"id": "1805.09105", "submitter": "Xuan-Yu Wang", "authors": "Xuan-Yu Wang, Wen-Xuan Liao, Dong An, Yao-Guang Wei", "title": "Maize Haploid Identification via LSTM-CNN and Hyperspectral Imaging\n  Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and fast identification of seed cultivars is crucial to plant\nbreeding, with accelerating breeding of new products and increasing its\nquality. In our study, the first attempt to design a high-accurate\nidentification model of maize haploid seeds from diploid ones based on optimum\nwaveband selection of the LSTM-CNN algorithm is realized via deep learning and\nhyperspectral imaging technology, with accuracy reaching 97% in the determining\noptimum waveband of 1367.6-1526.4nm. The verification of testing another\ncultivar achieved an accuracy of 93% in the same waveband. The model collected\nimages of 256 wavebands of seeds in the spectral region of 862.9-1704.2nm. The\nhigh-noise waveband intervals were found and deleted by the LSTM. The\noptimum-data waveband intervals were determined by CNN's waveband-based\ndetection. The optimum sample set for network training only accounted for 1/5\nof total sample data. The accuracy was significantly higher than the\nfull-waveband modeling or modeling of any other wavebands. Our study\ndemonstrates that the proposed model has outstanding effect on maize haploid\nidentification and it could be generalized to some extent.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:01:15 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 08:17:39 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Wang", "Xuan-Yu", ""], ["Liao", "Wen-Xuan", ""], ["An", "Dong", ""], ["Wei", "Yao-Guang", ""]]}, {"id": "1805.09110", "submitter": "Joshua Levine", "authors": "Julien Tierny, Guillaume Favelier, Joshua A. Levine, Charles Gueunet,\n  and Michael Michaux", "title": "The Topology ToolKit", "comments": null, "journal-ref": "IEEE Trans. Vis. Comput. Graph. 24(1) (2018) 832-842", "doi": "10.1109/TVCG.2017.2743938", "report-no": null, "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This system paper presents the Topology ToolKit (TTK), a software platform\ndesigned for topological data analysis in scientific visualization. TTK\nprovides a unified, generic, efficient, and robust implementation of key\nalgorithms for the topological analysis of scalar data, including: critical\npoints, integral lines, persistence diagrams, persistence curves, merge trees,\ncontour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots,\nJacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due\nto a tight integration with ParaView. It is also easily accessible to\ndevelopers through a variety of bindings (Python, VTK/C++) for fast prototyping\nor through direct, dependence-free, C++, to ease integration into pre-existing\ncomplex systems. While developing TTK, we faced several algorithmic and\nsoftware engineering challenges, which we document in this paper. In\nparticular, we present an algorithm for the construction of a discrete gradient\nthat complies to the critical points extracted in the piecewise-linear setting.\nThis algorithm guarantees a combinatorial consistency across the topological\nabstractions supported by TTK, and importantly, a unified implementation of\ntopological data simplification for multi-scale exploration and analysis. We\nalso present a cached triangulation data structure, that supports time\nefficient and generic traversals, which self-adjusts its memory usage on demand\nfor input simplicial meshes and which implicitly emulates a triangulation for\nregular grids with no memory overhead. Finally, we describe an original\nsoftware architecture, which guarantees memory efficient and direct accesses to\nTTK features, while still allowing for researchers powerful and easy bindings\nand extensions. TTK is open source (BSD license) and its code, online\ndocumentation and video tutorials are available on TTK's website.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:27:24 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 22:53:53 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Tierny", "Julien", ""], ["Favelier", "Guillaume", ""], ["Levine", "Joshua A.", ""], ["Gueunet", "Charles", ""], ["Michaux", "Michael", ""]]}, {"id": "1805.09133", "submitter": "Supreeth Prajwal Shashikumar", "authors": "Supreeth P. Shashikumar, Amit J. Shah, Gari D. Clifford, and Shamim\n  Nemati", "title": "Detection of Paroxysmal Atrial Fibrillation using Attention-based\n  Bidirectional Recurrent Neural Networks", "comments": "Accepted to the 24th ACM SIGKDD International Conference on Knowledge\n  Discovery and Data Mining (KDD 2018), London, UK, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is\ndifficult since many cases of AF are usually clinically silent and undiagnosed.\nIn particular paroxysmal AF is a form of AF that occurs occasionally, and has a\nhigher probability of being undetected. In this work, we present an attention\nbased deep learning framework for detection of paroxysmal AF episodes from a\nsequence of windows. Time-frequency representation of 30 seconds recording\nwindows, over a 10 minute data segment, are fed sequentially into a deep\nconvolutional neural network for image-based feature extraction, which are then\npresented to a bidirectional recurrent neural network with an attention layer\nfor AF detection. To demonstrate the effectiveness of the proposed framework\nfor transient AF detection, we use a database of 24 hour Holter\nElectrocardiogram (ECG) recordings acquired from 2850 patients at the\nUniversity of Virginia heart station. The algorithm achieves an AUC of 0.94 on\nthe testing set, which exceeds the performance of baseline models. We also\ndemonstrate the cross-domain generalizablity of the approach by adapting the\nlearned model parameters from one recording modality (ECG) to another\n(photoplethysmogram) with improved AF detection performance. The proposed high\naccuracy, low false alarm algorithm for detecting paroxysmal AF has potential\napplications in long-term monitoring using wearable sensors.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 20:34:17 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Shashikumar", "Supreeth P.", ""], ["Shah", "Amit J.", ""], ["Clifford", "Gari D.", ""], ["Nemati", "Shamim", ""]]}, {"id": "1805.09137", "submitter": "Vikram Mullachery", "authors": "Vikram Mullachery, Vishal Motwani", "title": "Image Captioning", "comments": "arXiv admin note: text overlap with arXiv:1609.06647 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses and demonstrates the outcomes from our experimentation\non Image Captioning. Image captioning is a much more involved task than image\nrecognition or classification, because of the additional challenge of\nrecognizing the interdependence between the objects/concepts in the image and\nthe creation of a succinct sentential narration. Experiments on several labeled\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. As a toy application, we apply image\ncaptioning to create video captions, and we advance a few hypotheses on the\nchallenges we encountered.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 19:13:16 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Mullachery", "Vikram", ""], ["Motwani", "Vishal", ""]]}, {"id": "1805.09190", "submitter": "Wieland Brendel", "authors": "Lukas Schott, Jonas Rauber, Matthias Bethge, Wieland Brendel", "title": "Towards the first adversarially robust neural network model on MNIST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite much effort, deep neural networks remain highly susceptible to tiny\ninput perturbations and even for MNIST, one of the most common toy datasets in\ncomputer vision, no neural network model exists for which adversarial\nperturbations are large and make semantic sense to humans. We show that even\nthe widely recognized and by far most successful defense by Madry et al. (1)\noverfits on the L-infinity metric (it's highly susceptible to L2 and L0\nperturbations), (2) classifies unrecognizable images with high certainty, (3)\nperforms not much better than simple input binarization and (4) features\nadversarial perturbations that make little sense to humans. These results\nsuggest that MNIST is far from being solved in terms of adversarial robustness.\nWe present a novel robust classification model that performs analysis by\nsynthesis using learned class-conditional data distributions. We derive bounds\non the robustness and go to great length to empirically evaluate our model\nusing maximally effective adversarial attacks by (a) applying decision-based,\nscore-based, gradient-based and transfer-based attacks for several different Lp\nnorms, (b) by designing a new attack that exploits the structure of our\ndefended model and (c) by devising a novel decision-based attack that seeks to\nminimize the number of perturbed pixels (L0). The results suggest that our\napproach yields state-of-the-art robustness on MNIST against L0, L2 and\nL-infinity perturbations and we demonstrate that most adversarial examples are\nstrongly perturbed towards the perceptual boundary between the original and the\nadversarial class.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 14:16:22 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 17:18:50 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 17:49:14 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Schott", "Lukas", ""], ["Rauber", "Jonas", ""], ["Bethge", "Matthias", ""], ["Brendel", "Wieland", ""]]}, {"id": "1805.09203", "submitter": "Xudong Liu", "authors": "Xudong Liu and Guodong Guo", "title": "Attributes in Multiple Facial Images", "comments": "Accepted by 2018 13th IEEE International Conference on Automatic Face\n  & Gesture Recognition (FG 2018 Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute recognition is conventionally computed from a single image.\nIn practice, each subject may have multiple face images. Taking the eye size as\nan example, it should not change, but it may have different estimation in\nmultiple images, which would make a negative impact on face recognition. Thus,\nhow to compute these attributes corresponding to each subject rather than each\nsingle image is a profound work. To address this question, we deploy deep\ntraining for facial attributes prediction, and we explore the inconsistency\nissue among the attributes computed from each single image. Then, we develop\ntwo approaches to address the inconsistency issue. Experimental results show\nthat the proposed methods can handle facial attribute estimation on either\nmultiple still images or video frames, and can correct the incorrectly\nannotated labels. The experiments are conducted on two large public databases\nwith annotations of facial attributes.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 14:48:11 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Liu", "Xudong", ""], ["Guo", "Guodong", ""]]}, {"id": "1805.09232", "submitter": "Rajendra Nagar", "authors": "Rajendra Nagar, Shanmuganathan Raman", "title": "SymmSLIC: Symmetry Aware Superpixel Segmentation and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-segmentation of an image into superpixels has become a useful tool for\nsolving various problems in image processing and computer vision. Reflection\nsymmetry is quite prevalent in both natural and man-made objects and is an\nessential cue in understanding and grouping the objects in natural scenes.\nExisting algorithms for estimating superpixels do not preserve the reflection\nsymmetry of an object which leads to different sizes and shapes of superpixels\nacross the symmetry axis. In this work, we propose an algorithm to over-segment\nan image through the propagation of reflection symmetry evident at the pixel\nlevel to superpixel boundaries. In order to achieve this goal, we first find\nthe reflection symmetry in the image and represent it by a set of pairs of\npixels which are mirror reflections of each other. We partition the image into\nsuperpixels while preserving this reflection symmetry through an iterative\nalgorithm. We compare the proposed method with state-of-the-art superpixel\ngeneration methods and show the effectiveness in preserving the size and shape\nof superpixel boundaries across the reflection symmetry axes. We also present\ntwo applications, symmetry axes detection and unsupervised symmetric object\nsegmentation, to illustrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 15:43:53 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 12:35:55 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Nagar", "Rajendra", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1805.09233", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey, Aswin Vasan and A G Ramakrishnan", "title": "Segmentation of Liver Lesions with Reduced Complexity Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient architecture that learns to segment\nlesions from CT images of the liver. The proposed architecture uses bilinear\ninterpolation with sub-pixel convolution at the last layer to upscale the\ncourse feature in bottle neck architecture. Since bilinear interpolation and\nsub-pixel convolution do not have any learnable parameter, our overall model is\nfaster and occupies less memory footprint than the traditional U-net. We\nevaluate our proposed architecture on the highly competitive dataset of 2017\nLiver Tumor Segmentation (LiTS) Challenge. Our method achieves competitive\nresults while reducing the number of learnable parameters roughly by a factor\nof 13.8 compared to the original UNet model.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 15:45:16 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Vasan", "Aswin", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1805.09243", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jiashi Feng, Zhouchen Lin, Tao Mei, Shuicheng Yan", "title": "Subspace Clustering by Block Diagonal Representation", "comments": null, "journal-ref": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the subspace clustering problem. Given some data points\napproximately drawn from a union of subspaces, the goal is to group these data\npoints into their underlying subspaces. Many subspace clustering methods have\nbeen proposed and among which sparse subspace clustering and low-rank\nrepresentation are two representative ones. Despite the different motivations,\nwe observe that many existing methods own the common block diagonal property,\nwhich possibly leads to correct clustering, yet with their proofs given case by\ncase. In this work, we consider a general formulation and provide a unified\ntheoretical guarantee of the block diagonal property. The block diagonal\nproperty of many existing methods falls into our special case. Second, we\nobserve that many existing methods approximate the block diagonal\nrepresentation matrix by using different structure priors, e.g., sparsity and\nlow-rankness, which are indirect. We propose the first block diagonal matrix\ninduced regularizer for directly pursuing the block diagonal matrix. With this\nregularizer, we solve the subspace clustering problem by Block Diagonal\nRepresentation (BDR), which uses the block diagonal structure prior. The BDR\nmodel is nonconvex and we propose an alternating minimization solver and prove\nits convergence. Experiments on real datasets demonstrate the effectiveness of\nBDR.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 15:58:34 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Lu", "Canyi", ""], ["Feng", "Jiashi", ""], ["Lin", "Zhouchen", ""], ["Mei", "Tao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1805.09264", "submitter": "Marco Buzzelli", "authors": "Marco Buzzelli, Joost van de Weijer, Raimondo Schettini", "title": "Learning Illuminant Estimation from Object Recognition", "comments": "Accepted at ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deep learning method to estimate the illuminant of\nan image. Our model is not trained with illuminant annotations, but with the\nobjective of improving performance on an auxiliary task such as object\nrecognition. To the best of our knowledge, this is the first example of a deep\nlearning architecture for illuminant estimation that is trained without ground\ntruth illuminants. We evaluate our solution on standard datasets for color\nconstancy, and compare it with state of the art methods. Our proposal is shown\nto outperform most deep learning methods in a cross-dataset evaluation setup,\nand to present competitive results in a comparison with parametric solutions.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 16:25:26 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Buzzelli", "Marco", ""], ["van de Weijer", "Joost", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1805.09277", "submitter": "Sangha Lee", "authors": "Sang-Ha Lee, Soon-Chul Kwon, Jin-Wook Shim, Jeong-Eun Lim, Jisang Yoo", "title": "WisenetMD: Motion Detection Using Dynamic Background Region Analysis", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion detection algorithms that can be applied to surveillance cameras such\nas CCTV (Closed Circuit Television) have been studied extensively. Motion\ndetection algorithm is mostly based on background subtraction. One main issue\nin this technique is that false positives of dynamic backgrounds such as wind\nshaking trees and flowing rivers might occur. In this paper, we proposed a\nmethod to search for dynamic background region by analyzing the video and\nremoving false positives by re-checking false positives. The proposed method\nwas evaluated based on CDnet 2012/2014 dataset obtained at\n\"changedetection.net\" site. We also compared its processing speed with other\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 16:48:27 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Lee", "Sang-Ha", ""], ["Kwon", "Soon-Chul", ""], ["Shim", "Jin-Wook", ""], ["Lim", "Jeong-Eun", ""], ["Yoo", "Jisang", ""]]}, {"id": "1805.09298", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le\n  Song", "title": "Learning towards Minimum Hyperspherical Energy", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are a powerful class of nonlinear functions that can be\ntrained end-to-end on various applications. While the over-parametrization\nnature in many neural networks renders the ability to fit complex functions and\nthe strong representation power to handle challenging tasks, it also leads to\nhighly correlated neurons that can hurt the generalization ability and incur\nunnecessary computation cost. As a result, how to regularize the network to\navoid undesired representation redundancy becomes an important issue. To this\nend, we draw inspiration from a well-known problem in physics -- Thomson\nproblem, where one seeks to find a state that distributes N electrons on a unit\nsphere as evenly as possible with minimum potential energy. In light of this\nintuition, we reduce the redundancy regularization problem to generic energy\nminimization, and propose a minimum hyperspherical energy (MHE) objective as\ngeneric regularization for neural networks. We also propose a few novel\nvariants of MHE, and provide some insights from a theoretical point of view.\nFinally, we apply neural networks with MHE regularization to several\nchallenging tasks. Extensive experiments demonstrate the effectiveness of our\nintuition, by showing the superior performance with MHE regularization.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 17:34:47 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 21:50:09 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 22:44:57 GMT"}, {"version": "v4", "created": "Sat, 16 Jun 2018 07:47:21 GMT"}, {"version": "v5", "created": "Sat, 27 Oct 2018 07:17:57 GMT"}, {"version": "v6", "created": "Sat, 1 Dec 2018 09:28:53 GMT"}, {"version": "v7", "created": "Wed, 9 Jan 2019 09:16:13 GMT"}, {"version": "v8", "created": "Tue, 5 Mar 2019 03:07:32 GMT"}, {"version": "v9", "created": "Wed, 22 Jul 2020 15:23:29 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Liu", "Weiyang", ""], ["Lin", "Rongmei", ""], ["Liu", "Zhen", ""], ["Liu", "Lixin", ""], ["Yu", "Zhiding", ""], ["Dai", "Bo", ""], ["Song", "Le", ""]]}, {"id": "1805.09300", "submitter": "Mahyar Najibi", "authors": "Bharat Singh, Mahyar Najibi and Larry S. Davis", "title": "SNIPER: Efficient Multi-Scale Training", "comments": "Presented at the NIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SNIPER, an algorithm for performing efficient multi-scale training\nin instance level visual recognition tasks. Instead of processing every pixel\nin an image pyramid, SNIPER processes context regions around ground-truth\ninstances (referred to as chips) at the appropriate scale. For background\nsampling, these context-regions are generated using proposals extracted from a\nregion proposal network trained with a short learning schedule. Hence, the\nnumber of chips generated per image during training adaptively changes based on\nthe scene complexity. SNIPER only processes 30% more pixels compared to the\ncommonly used single scale training at 800x1333 pixels on the COCO dataset.\nBut, it also observes samples from extreme resolutions of the image pyramid,\nlike 1400x2000 pixels. As SNIPER operates on resampled low resolution chips\n(512x512 pixels), it can have a batch size as large as 20 on a single GPU even\nwith a ResNet-101 backbone. Therefore it can benefit from batch-normalization\nduring training without the need for synchronizing batch-normalization\nstatistics across GPUs. SNIPER brings training of instance level recognition\ntasks like object detection closer to the protocol for image classification and\nsuggests that the commonly accepted guideline that it is important to train on\nhigh resolution images for instance level visual recognition tasks might not be\ncorrect. Our implementation based on Faster-RCNN with a ResNet-101 backbone\nobtains an mAP of 47.6% on the COCO dataset for bounding box detection and can\nprocess 5 images per second during inference with a single GPU. Code is\navailable at https://github.com/MahyarNajibi/SNIPER/.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 17:38:27 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 08:37:13 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 18:56:14 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Singh", "Bharat", ""], ["Najibi", "Mahyar", ""], ["Davis", "Larry S.", ""]]}, {"id": "1805.09313", "submitter": "Konstantinos Vougioukas", "authors": "Konstantinos Vougioukas, Stavros Petridis, Maja Pantic", "title": "End-to-End Speech-Driven Facial Animation with Temporal GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-driven facial animation is the process which uses speech signals to\nautomatically synthesize a talking character. The majority of work in this\ndomain creates a mapping from audio features to visual features. This often\nrequires post-processing using computer graphics techniques to produce\nrealistic albeit subject dependent results. We present a system for generating\nvideos of a talking head, using a still image of a person and an audio clip\ncontaining speech, that doesn't rely on any handcrafted intermediate features.\nTo the best of our knowledge, this is the first method capable of generating\nsubject independent realistic videos directly from raw audio. Our method can\ngenerate videos which have (a) lip movements that are in sync with the audio\nand (b) natural facial expressions such as blinks and eyebrow movements. We\nachieve this by using a temporal GAN with 2 discriminators, which are capable\nof capturing different aspects of the video. The effect of each component in\nour system is quantified through an ablation study. The generated videos are\nevaluated based on their sharpness, reconstruction quality, and lip-reading\naccuracy. Finally, a user study is conducted, confirming that temporal GANs\nlead to more natural sequences than a static GAN-based approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 17:54:32 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 11:48:27 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 21:19:45 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 12:40:47 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Vougioukas", "Konstantinos", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1805.09380", "submitter": "Saheb Chhabra", "authors": "Saheb Chhabra, Richa Singh, Mayank Vatsa, Gaurav Gupta", "title": "Anonymizing k-Facial Attributes via Adversarial Perturbations", "comments": "Published in IJCAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A face image not only provides details about the identity of a subject but\nalso reveals several attributes such as gender, race, sexual orientation, and\nage. Advancements in machine learning algorithms and popularity of sharing\nimages on the World Wide Web, including social media websites, have increased\nthe scope of data analytics and information profiling from photo collections.\nThis poses a serious privacy threat for individuals who do not want to be\nprofiled. This research presents a novel algorithm for anonymizing selective\nattributes which an individual does not want to share without affecting the\nvisual quality of images. Using the proposed algorithm, a user can select\nsingle or multiple attributes to be surpassed while preserving identity\ninformation and visual content. The proposed adversarial perturbation based\nalgorithm embeds imperceptible noise in an image such that attribute prediction\nalgorithm for the selected attribute yields incorrect classification result,\nthereby preserving the information according to user's choice. Experiments on\nthree popular databases i.e. MUCT, LFWcrop, and CelebA show that the proposed\nalgorithm not only anonymizes k-attributes, but also preserves image quality\nand identity information.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 18:54:28 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 07:40:28 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Chhabra", "Saheb", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Gupta", "Gaurav", ""]]}, {"id": "1805.09391", "submitter": "Rahul Paul", "authors": "Rahul Paul", "title": "Classifying cooking object's state using a tuned VGG convolutional\n  neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotics, knowing the object states and recognizing the desired states are\nvery important. Objects at different states would require different grasping.\nTo achieve different states, different manipulations would be required, as well\nas different grasping. To analyze the objects at different states, a dataset of\ncooking objects was created. Cooking consists of various cutting techniques\nneeded for different dishes (e.g. diced, julienne etc.). Identifying each of\nthis state of cooking objects by the human can be difficult sometimes too. In\nthis paper, we have analyzed seven different cooking object states by tuning a\nconvolutional neural network (CNN). For this task, images were downloaded and\nannotated by students and they are divided into training and a completely\ndifferent test set. By tuning the vgg-16 CNN 77% accuracy was obtained. The\nwork presented in this paper focuses on classification between various object\nstates rather than task recognition or recipe prediction. This framework can be\neasily adapted in any other object state classification activity.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 19:22:03 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 15:58:50 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Paul", "Rahul", ""]]}, {"id": "1805.09400", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey and A G Ramakrishnan", "title": "A hybrid approach of interpolations and CNN to obtain super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": "TIP-19077-2018", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture that learns an end-to-end mapping function to\nimprove the spatial resolution of the input natural images. The model is unique\nin forming a nonlinear combination of three traditional interpolation\ntechniques using the convolutional neural network. Another proposed\narchitecture uses a skip connection with nearest neighbor interpolation,\nachieving almost similar results. The architectures have been carefully\ndesigned to ensure that the reconstructed images lie precisely in the manifold\nof high-resolution images, thereby preserving the high-frequency components\nwith fine details. We have compared with the state of the art and recent deep\nlearning based natural image super-resolution techniques and found that our\nmethods are able to preserve the sharp details in the image, while also\nobtaining comparable or better PSNR than them. Since our methods use only\ntraditional interpolations and a shallow CNN with less number of smaller\nfilters, the computational cost is kept low. We have reported the results of\ntwo proposed architectures on five standard datasets, for an upscale factor of\n2. Our methods generalize well in most cases, which is evident from the better\nresults obtained with increasingly complex datasets. For 4-times upscaling, we\nhave designed similar architectures for comparing with other methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 19:43:57 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1805.09408", "submitter": "Iv\\'an Ram\\'irez D\\'iaz", "authors": "Iv\\'an Ram\\'irez, Gonzalo Galiano and Emanuele Schiavi", "title": "Non-convex non-local flows for saliency detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose and numerically solve a new variational model for automatic\nsaliency detection in digital images. Using a non-local framework we consider a\nfamily of edge preserving functions combined with a new quadratic saliency\ndetection term. Such term defines a constrained bilateral obstacle problem for\nimage classification driven by p-Laplacian operators, including the so-called\nhyper-Laplacian case (0 < p < 1). The related non-convex non-local reactive\nflows are then considered and applied for glioblastoma segmentation in magnetic\nresonance fluid-attenuated inversion recovery (MRI-Flair) images. A fast\nconvolutional kernel based approximated solution is computed. The numerical\nexperiments show how the non-convexity related to the hyperLaplacian operators\nprovides monotonically better results in terms of the standard metrics.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 20:03:06 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Ram\u00edrez", "Iv\u00e1n", ""], ["Galiano", "Gonzalo", ""], ["Schiavi", "Emanuele", ""]]}, {"id": "1805.09421", "submitter": "Viacheslav Dudar", "authors": "Viacheslav Dudar and Vladimir Semenov", "title": "Use of symmetric kernels for convolutional neural networks", "comments": "ICDSIAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At this work we introduce horizontally symmetric convolutional kernels for\nCNNs which make the network output invariant to horizontal flips of the image.\nWe also study other types of symmetric kernels which lead to vertical flip\ninvariance, and approximate rotational invariance. We show that usage of such\nkernels acts as regularizer, and improves generalization of the convolutional\nneural networks at the cost of more complicated training process.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 20:57:31 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Dudar", "Viacheslav", ""], ["Semenov", "Vladimir", ""]]}, {"id": "1805.09430", "submitter": "Viacheslav Dudar", "authors": "Viacheslav Dudar, Giovanni Chierchia, Emilie Chouzenoux,\n  Jean-Christophe Pesquet, Vladimir Semenov", "title": "A Two-Stage Subspace Trust Region Approach for Deep Neural Network\n  Training", "comments": "EUSIPCO 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a novel second-order method for training\nfeed-forward neural nets. At each iteration, we construct a quadratic\napproximation to the cost function in a low-dimensional subspace. We minimize\nthis approximation inside a trust region through a two-stage procedure: first\ninside the embedded positive curvature subspace, followed by a gradient descent\nstep. This approach leads to a fast objective function decay, prevents\nconvergence to saddle points, and alleviates the need for manually tuning\nparameters. We show the good performance of the proposed algorithm on benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 21:12:48 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Dudar", "Viacheslav", ""], ["Chierchia", "Giovanni", ""], ["Chouzenoux", "Emilie", ""], ["Pesquet", "Jean-Christophe", ""], ["Semenov", "Vladimir", ""]]}, {"id": "1805.09441", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Stephen Rawls and Prem Natarajan", "title": "Implicit Language Model in LSTM for OCR", "comments": null, "journal-ref": "2017 14th IAPR International Conference on Document Analysis and\n  Recognition (ICDAR), vol. 7 (2017) pp. 27-31", "doi": "10.1109/ICDAR.2017.361", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become the technique of choice for OCR, but many aspects\nof how and why they deliver superior performance are still unknown. One key\ndifference between current neural network techniques using LSTMs and the\nprevious state-of-the-art HMM systems is that HMM systems have a strong\nindependence assumption. In comparison LSTMs have no explicit constraints on\nthe amount of context that can be considered during decoding. In this paper we\nshow that they learn an implicit LM and attempt to characterize the strength of\nthe LM in terms of equivalent n-gram context. We show that this implicitly\nlearned language model provides a 2.4\\% CER improvement on our synthetic test\nset when compared against a test set of random characters (i.e. not naturally\noccurring sequences), and that the LSTM learns to use up to 5 characters of\ncontext (which is roughly 88 frames in our configuration). We believe that this\nis the first ever attempt at characterizing the strength of the implicit LM in\nLSTM based OCR systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 22:01:38 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Sabir", "Ekraam", ""], ["Rawls", "Stephen", ""], ["Natarajan", "Prem", ""]]}, {"id": "1805.09460", "submitter": "Yotam Hechtlinger", "authors": "Yotam Hechtlinger, Barnab\\'as P\\'oczos and Larry Wasserman", "title": "Cautious Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classifiers operate by selecting the maximum of an estimate of the\nconditional distribution $p(y|x)$ where $x$ stands for the features of the\ninstance to be classified and $y$ denotes its label. This often results in a\n{\\em hubristic bias}: overconfidence in the assignment of a definite label.\nUsually, the observations are concentrated on a small volume but the classifier\nprovides definite predictions for the entire space. We propose constructing\nconformal prediction sets which contain a set of labels rather than a single\nlabel. These conformal prediction sets contain the true label with probability\n$1-\\alpha$. Our construction is based on $p(x|y)$ rather than $p(y|x)$ which\nresults in a classifier that is very cautious: it outputs the null set ---\nmeaning \"I don't know\" --- when the object does not resemble the training\nexamples. An important property of our approach is that adversarial attacks are\nlikely to be predicted as the null set or would also include the true label. We\ndemonstrate the performance on the ImageNet ILSVRC dataset and the CelebA and\nIMDB-Wiki facial datasets using high dimensional features obtained from state\nof the art convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 00:17:24 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 19:27:58 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Hechtlinger", "Yotam", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Wasserman", "Larry", ""]]}, {"id": "1805.09462", "submitter": "Cristina Mata", "authors": "Cristina Mata, Guy Ben-Yosef, Boris Katz", "title": "Complex Relations in a Deep Structured Prediction Model for Fine Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep learning architectures for semantic segmentation involve a Fully\nConvolutional Neural Network (FCN) followed by a Conditional Random Field (CRF)\nto carry out inference over an image. These models typically involve unary\npotentials based on local appearance features computed by FCNs, and binary\npotentials based on the displacement between pixels. We show that while current\nmethods succeed in segmenting whole objects, they perform poorly in situations\ninvolving a large number of object parts. We therefore suggest incorporating\ninto the inference algorithm additional higher-order potentials inspired by the\nway humans identify and localize parts. We incorporate two relations that were\nshown to be useful to human object identification - containment and attachment\n- into the energy term of the CRF and evaluate their performance on the Pascal\nVOC Parts dataset. Our experimental results show that the segmentation of fine\nparts is positively affected by the addition of these two relations, and that\nthe segmentation of fine parts can be further influenced by complex structural\nfeatures.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 00:21:40 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Mata", "Cristina", ""], ["Ben-Yosef", "Guy", ""], ["Katz", "Boris", ""]]}, {"id": "1805.09474", "submitter": "Anna Choromanska", "authors": "Devansh Bisla and Anna Choromanska", "title": "VisualBackProp for learning using privileged information with CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, from medical diagnostics to autonomous\ndriving, the availability of prior knowledge can be used to improve the\npredictive performance of learning algorithms and incorporate `physical,'\n`domain knowledge,' or `common sense' concepts into training of machine\nlearning systems as well as verify constraints/properties of the systems. We\nexplore the learning using privileged information paradigm and show how to\nincorporate the privileged information, such as segmentation mask available\nalong with the classification label of each example, into the training stage of\nconvolutional neural networks. This is done by augmenting the CNN model with an\narchitectural component that effectively focuses model's attention on the\ndesired region of the input image during the training process and that is\ntransparent to the network's label prediction mechanism at testing. This\ncomponent effectively corresponds to the visualization strategy for identifying\nthe parts of the input, often referred to as visualization mask, that most\ncontribute to the prediction, yet uses this strategy in reverse to the\nclassical setting in order to enforce the desired visualization mask instead.\nWe verify our proposed algorithms through exhaustive experiments on benchmark\nImageNet and PASCAL VOC data sets and achieve improvements in the performance\nof $2.4\\%$ and $2.7\\%$ over standard single-supervision model training.\nFinally, we confirm the effectiveness of our approach on skin lesion\nclassification problem.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 01:19:52 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Bisla", "Devansh", ""], ["Choromanska", "Anna", ""]]}, {"id": "1805.09501", "submitter": "Ekin Dogus Cubuk", "authors": "Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V.\n  Le", "title": "AutoAugment: Learning Augmentation Policies from Data", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an effective technique for improving the accuracy of\nmodern image classifiers. However, current data augmentation implementations\nare manually designed. In this paper, we describe a simple procedure called\nAutoAugment to automatically search for improved data augmentation policies. In\nour implementation, we have designed a search space where a policy consists of\nmany sub-policies, one of which is randomly chosen for each image in each\nmini-batch. A sub-policy consists of two operations, each operation being an\nimage processing function such as translation, rotation, or shearing, and the\nprobabilities and magnitudes with which the functions are applied. We use a\nsearch algorithm to find the best policy such that the neural network yields\nthe highest validation accuracy on a target dataset. Our method achieves\nstate-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without\nadditional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is\n0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error\nrate of 1.5%, which is 0.6% better than the previous state-of-the-art.\nAugmentation policies we find are transferable between datasets. The policy\nlearned on ImageNet transfers well to achieve significant improvements on other\ndatasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft,\nand Stanford Cars.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 04:05:42 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 20:27:22 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 22:39:27 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Cubuk", "Ekin D.", ""], ["Zoph", "Barret", ""], ["Mane", "Dandelion", ""], ["Vasudevan", "Vijay", ""], ["Le", "Quoc V.", ""]]}, {"id": "1805.09511", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Javier Hernandez, Rosalind W. Picard", "title": "Estimating Carotid Pulse and Breathing Rate from Near-infrared Video of\n  the Neck", "comments": "21 pages, 15 figures", "journal-ref": null, "doi": "10.1088/1361-6579/aae625", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Non-contact physiological measurement is a growing research area\nthat allows capturing vital signs such as heart rate (HR) and breathing rate\n(BR) comfortably and unobtrusively with remote devices. However, most of the\napproaches work only in bright environments in which subtle\nphotoplethysmographic and ballistocardiographic signals can be easily analyzed\nand/or require expensive and custom hardware to perform the measurements.\n  Approach: This work introduces a low-cost method to measure subtle motions\nassociated with the carotid pulse and breathing movement from the neck using\nnear-infrared (NIR) video imaging. A skin reflection model of the neck was\nestablished to provide a theoretical foundation for the method. In particular,\nthe method relies on template matching for neck detection, Principal Component\nAnalysis for feature extraction, and Hidden Markov Models for data smoothing.\n  Main Results: We compared the estimated HR and BR measures with ones provided\nby an FDA-cleared device in a 12-participant laboratory study: the estimates\nachieved a mean absolute error of 0.36 beats per minute and 0.24 breaths per\nminute under both bright and dark lighting.\n  Significance: This work advances the possibilities of non-contact\nphysiological measurement in real-life conditions in which environmental\nillumination is limited and in which the face of the person is not readily\navailable or needs to be protected. Due to the increasing availability of NIR\nimaging devices, the described methods are readily scalable.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 05:15:18 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chen", "Weixuan", ""], ["Hernandez", "Javier", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1805.09512", "submitter": "Adam Van Etten", "authors": "Adam Van Etten", "title": "You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite\n  Imagery", "comments": "8 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of small objects in large swaths of imagery is one of the primary\nproblems in satellite imagery analytics. While object detection in ground-based\nimagery has benefited from research into new deep learning approaches,\ntransitioning such technology to overhead imagery is nontrivial. Among the\nchallenges is the sheer number of pixels and geographic extent per image: a\nsingle DigitalGlobe satellite image encompasses >64 km2 and over 250 million\npixels. Another challenge is that objects of interest are minuscule (often only\n~10 pixels in extent), which complicates traditional computer vision\ntechniques. To address these issues, we propose a pipeline (You Only Look\nTwice, or YOLT) that evaluates satellite images of arbitrary size at a rate of\n>0.5 km2/s. The proposed approach can rapidly detect objects of vastly\ndifferent scales with relatively little training data over multiple sensors. We\nevaluate large test images at native resolution, and yield scores of F1 > 0.8\nfor vehicle localization. We further explore resolution and object size\nrequirements by systematically testing the pipeline at decreasing resolution,\nand conclude that objects only ~5 pixels in size can still be localized with\nhigh confidence. Code is available at https://github.com/CosmiQ/yolt.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 05:17:51 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Van Etten", "Adam", ""]]}, {"id": "1805.09521", "submitter": "Ehsan Adeli", "authors": "Mohammad Sabokrou, Masoud Pourreza, Mohsen Fayyaz, Rahim Entezari,\n  Mahmood Fathy, J\\\"urgen Gall, Ehsan Adeli", "title": "AVID: Adversarial Visual Irregularity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time detection of irregularities in visual data is very invaluable and\nuseful in many prospective applications including surveillance, patient\nmonitoring systems, etc. With the surge of deep learning methods in the recent\nyears, researchers have tried a wide spectrum of methods for different\napplications. However, for the case of irregularity or anomaly detection in\nvideos, training an end-to-end model is still an open challenge, since often\nirregularity is not well-defined and there are not enough irregular samples to\nuse during training. In this paper, inspired by the success of generative\nadversarial networks (GANs) for training deep models in unsupervised or\nself-supervised settings, we propose an end-to-end deep network for detection\nand fine localization of irregularities in videos (and images). Our proposed\narchitecture is composed of two networks, which are trained in competing with\neach other while collaborating to find the irregularity. One network works as a\npixel-level irregularity Inpainter, and the other works as a patch-level\nDetector. After an adversarial self-supervised training, in which I tries to\nfool D into accepting its inpainted output as regular (normal), the two\nnetworks collaborate to detect and fine-segment the irregularity in any given\ntesting video. Our results on three different datasets show that our method can\noutperform the state-of-the-art and fine-segment the irregularity.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 06:23:45 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 22:37:53 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Pourreza", "Masoud", ""], ["Fayyaz", "Mohsen", ""], ["Entezari", "Rahim", ""], ["Fathy", "Mahmood", ""], ["Gall", "J\u00fcrgen", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1805.09578", "submitter": "Tianli Liao", "authors": "Tianli Liao, Jing Chen and Yifang Xu", "title": "Coarse-to-fine Seam Estimation for Image Stitching", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seam-cutting and seam-driven techniques have been proven effective for\nhandling imperfect image series in image stitching. Generally, seam-driven is\nto utilize seam-cutting to find a best seam from one or finite alignment\nhypotheses based on a predefined seam quality metric. However, the quality\nmetrics in most methods are defined to measure the average performance of the\npixels on the seam without considering the relevance and variance among them.\nThis may cause that the seam with the minimal measure is not optimal\n(perception-inconsistent) in human perception. In this paper, we propose a\nnovel coarse-to-fine seam estimation method which applies the evaluation in a\ndifferent way. For pixels on the seam, we develop a patch-point evaluation\nalgorithm concentrating more on the correlation and variation of them. The\nevaluations are then used to recalculate the difference map of the overlapping\nregion and reestimate a stitching seam. This evaluation-reestimation procedure\niterates until the current seam changes negligibly comparing with the previous\nseams. Experiments show that our proposed method can finally find a nearly\nperception-consistent seam after several iterations, which outperforms the\nconventional seam-cutting and other seam-driven methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:49:06 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Liao", "Tianli", ""], ["Chen", "Jing", ""], ["Xu", "Yifang", ""]]}, {"id": "1805.09585", "submitter": "Francois Rousseau", "authors": "Francois Rousseau and Ronan Fablet", "title": "Residual Networks as Geodesic Flows of Diffeomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the understanding and characterization of residual\nnetworks (ResNet), which are among the state-of-the-art deep learning\narchitectures for a variety of supervised learning problems. We focus on the\nmapping component of ResNets, which map the embedding space towards a new\nunknown space where the prediction or classification can be stated according to\nlinear criteria. We show that this mapping component can be regarded as the\nnumerical implementation of continuous flows of diffeomorphisms governed by\nordinary differential equations. Especially, ResNets with shared weights are\nfully characterized as numerical approximation of exponential diffeomorphic\noperators. We stress both theoretically and numerically the relevance of the\nenforcement of diffeormorphic properties and the importance of numerical issues\nto make consistent the continuous formulation and the discretized ResNet\nimplementation. We further discuss the resulting theoretical and computational\ninsights on ResNet architectures.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 10:07:46 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 13:02:03 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Rousseau", "Francois", ""], ["Fablet", "Ronan", ""]]}, {"id": "1805.09591", "submitter": "Kele Xu", "authors": "Bo Li, Kele Xu, Xiaoyan Cui, Yiheng Wang, Xinbo Ai, Yanbo Wang", "title": "Multi-Scale DenseNet-Based Electricity Theft Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity theft detection issue has drawn lots of attention during last\ndecades. Timely identification of the electricity theft in the power system is\ncrucial for the safety and availability of the system. Although sustainable\nefforts have been made, the detection task remains challenging and falls short\nof accuracy and efficiency, especially with the increase of the data size.\nRecently, convolutional neural network-based methods have achieved better\nperformance in comparison with traditional methods, which employ handcrafted\nfeatures and shallow-architecture classifiers. In this paper, we present a\nnovel approach for automatic detection by using a multi-scale dense connected\nconvolution neural network (multi-scale DenseNet) in order to capture the\nlong-term and short-term periodic features within the sequential data. We\ncompare the proposed approaches with the classical algorithms, and the\nexperimental results demonstrate that the multiscale DenseNet approach can\nsignificantly improve the accuracy of the detection. Moreover, our method is\nscalable, enabling larger data processing while no handcrafted feature\nengineering is needed.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 10:25:16 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Li", "Bo", ""], ["Xu", "Kele", ""], ["Cui", "Xiaoyan", ""], ["Wang", "Yiheng", ""], ["Ai", "Xinbo", ""], ["Wang", "Yanbo", ""]]}, {"id": "1805.09621", "submitter": "Zhe Cheng Fan", "authors": "Zhe-Cheng Fan, Tak-Shing T. Chan, Yi-Hsuan Yang, and Jyh-Shing R. Jang", "title": "Backpropagation with N-D Vector-Valued Neurons Using Arbitrary Bilinear\n  Products", "comments": "14 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-valued neural learning has emerged as a promising direction in deep\nlearning recently. Traditionally, training data for neural networks (NNs) are\nformulated as a vector of scalars; however, its performance may not be optimal\nsince associations among adjacent scalars are not modeled. In this paper, we\npropose a new vector neural architecture called the Arbitrary BIlinear Product\nNeural Network (ABIPNN), which processes information as vectors in each neuron,\nand the feedforward projections are defined using arbitrary bilinear products.\nSuch bilinear products can include circular convolution, seven-dimensional\nvector product, skew circular convolution, reversed- time circular convolution,\nor other new products not seen in previous work. As a proof-of-concept, we\napply our proposed network to multispectral image denoising and singing voice\nsepa- ration. Experimental results show that ABIPNN gains substantial\nimprovements when compared to conventional NNs, suggesting that associations\nare learned during training.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 12:01:53 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Fan", "Zhe-Cheng", ""], ["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""], ["Jang", "Jyh-Shing R.", ""]]}, {"id": "1805.09622", "submitter": "Or Litany", "authors": "Or Litany and Daniel Freedman", "title": "SOSELETO: A Unified Approach to Transfer Learning and Training with\n  Noisy Labels", "comments": "ICLR workshop on Learning from Limited Labeled Data (LLD) -- Best\n  Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SOSELETO (SOurce SELEction for Target Optimization), a new method\nfor exploiting a source dataset to solve a classification problem on a target\ndataset. SOSELETO is based on the following simple intuition: some source\nexamples are more informative than others for the target problem. To capture\nthis intuition, source samples are each given weights; these weights are solved\nfor jointly with the source and target classification problems via a bilevel\noptimization scheme. The target therefore gets to choose the source samples\nwhich are most informative for its own classification task. Furthermore, the\nbilevel nature of the optimization acts as a kind of regularization on the\ntarget, mitigating overfitting. SOSELETO may be applied to both classic\ntransfer learning, as well as the problem of training on datasets with noisy\nlabels; we show state of the art results on both of these problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 12:03:58 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 15:41:44 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Litany", "Or", ""], ["Freedman", "Daniel", ""]]}, {"id": "1805.09662", "submitter": "Eduard Trulls", "authors": "Yuki Ono, Eduard Trulls, Pascal Fua, Kwang Moo Yi", "title": "LF-Net: Learning Local Features from Images", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep architecture and a training strategy to learn a local\nfeature pipeline from scratch, using collections of images without the need for\nhuman supervision. To do so we exploit depth and relative camera pose cues to\ncreate a virtual target that the network should achieve on one image, provided\nthe outputs of the network for the other image. While this process is\ninherently non-differentiable, we show that we can optimize the network in a\ntwo-branch setup by confining it to one branch, while preserving\ndifferentiability in the other. We train our method on both indoor and outdoor\ndatasets, with depth data from 3D sensors for the former, and depth estimates\nfrom an off-the-shelf Structure-from-Motion solution for the latter. Our models\noutperform the state of the art on sparse feature matching on both datasets,\nwhile running at 60+ fps for QVGA images.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 13:42:17 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 11:10:42 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Ono", "Yuki", ""], ["Trulls", "Eduard", ""], ["Fua", "Pascal", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1805.09687", "submitter": "Michele Dolfi", "authors": "Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas", "title": "Corpus Conversion Service: A machine learning platform to ingest\n  documents at scale [Poster abstract]", "comments": "Accepted in SysML 2018 (www.sysml.cc)", "journal-ref": null, "doi": "10.13140/RG.2.2.10858.82888", "report-no": null, "categories": "cs.DL cs.CL cs.CV cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make their\ncontent discoverable. Unfortunately, both the format of these documents (e.g.\nthe PDF format or bitmap images) as well as the presentation of the data (e.g.\ncomplex tables) make the extraction of qualitative and quantitive data\nextremely challenging. We present a platform to ingest documents at scale which\nis powered by Machine Learning techniques and allows the user to train custom\nmodels on document collections. We show precision/recall results greater than\n97% with regard to conversion to structured formats, as well as scaling\nevidence for each of the microservices constituting the platform.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:05:52 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Staar", "Peter W J", ""], ["Dolfi", "Michele", ""], ["Auer", "Christoph", ""], ["Bekas", "Costas", ""]]}, {"id": "1805.09701", "submitter": "Pan Lu", "authors": "Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang", "title": "R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual\n  Question Answering", "comments": "10 pages, 5 figures, accepted as an oral paper in SIGKDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3220036", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Visual Question Answering (VQA) has emerged as one of the most\nsignificant tasks in multimodal learning as it requires understanding both\nvisual and textual modalities. Existing methods mainly rely on extracting image\nand question features to learn their joint feature embedding via multimodal\nfusion or attention mechanism. Some recent studies utilize external\nVQA-independent models to detect candidate entities or attributes in images,\nwhich serve as semantic knowledge complementary to the VQA task. However, these\ncandidate entities or attributes might be unrelated to the VQA task and have\nlimited semantic capacities. To better utilize semantic knowledge in images, we\npropose a novel framework to learn visual relation facts for VQA. Specifically,\nwe build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset\nvia a semantic similarity module, in which each data consists of an image, a\ncorresponding question, a correct answer and a supporting relation fact. A\nwell-defined relation detector is then adopted to predict visual\nquestion-related relation facts. We further propose a multi-step attention\nmodel composed of visual attention and semantic attention sequentially to\nextract related visual knowledge and semantic knowledge. We conduct\ncomprehensive experiments on the two benchmark datasets, demonstrating that our\nmodel achieves state-of-the-art performance and verifying the benefit of\nconsidering visual relation facts.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:43:30 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 03:45:04 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Lu", "Pan", ""], ["Ji", "Lei", ""], ["Zhang", "Wei", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""], ["Wang", "Jianyong", ""]]}, {"id": "1805.09707", "submitter": "Xi Peng", "authors": "Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio Feris, Dimitris Metaxas", "title": "Jointly Optimize Data Augmentation and Network Training: Adversarial\n  Data Augmentation in Human Pose Estimation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random data augmentation is a critical technique to avoid overfitting in\ntraining deep neural network models. However, data augmentation and network\ntraining are usually treated as two isolated processes, limiting the\neffectiveness of network training. Why not jointly optimize the two? We propose\nadversarial data augmentation to address this limitation. The main idea is to\ndesign an augmentation network (generator) that competes against a target\nnetwork (discriminator) by generating `hard' augmentation operations online.\nThe augmentation network explores the weaknesses of the target network, while\nthe latter learns from `hard' augmentations to achieve better performance. We\nalso design a reward/penalty strategy for effective joint training. We\ndemonstrate our approach on the problem of human pose estimation and carry out\na comprehensive experimental analysis, showing that our method can\nsignificantly improve state-of-the-art models without additional data efforts.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:53:04 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Peng", "Xi", ""], ["Tang", "Zhiqiang", ""], ["Yang", "Fei", ""], ["Feris", "Rogerio", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1805.09730", "submitter": "Abel Gonzalez-Garcia", "authors": "Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio", "title": "Image-to-image translation for cross-domain disentanglement", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image translation methods have recently shown excellent results,\noutputting high-quality images covering multiple modes of the data\ndistribution. There has also been increased interest in disentangling the\ninternal representations learned by deep methods to further improve their\nperformance and achieve a finer control. In this paper, we bridge these two\nobjectives and introduce the concept of cross-domain disentanglement. We aim to\nseparate the internal representation into three parts. The shared part contains\ninformation for both domains. The exclusive parts, on the other hand, contain\nonly factors of variation that are particular to each domain. We achieve this\nthrough bidirectional image translation based on Generative Adversarial\nNetworks and cross-domain autoencoders, a novel network component. Our model\noffers multiple advantages. We can output diverse samples covering multiple\nmodes of the distributions of both domains, perform domain-specific image\ntransfer and interpolation, and cross-domain retrieval without the need of\nlabeled data, only paired images. We compare our model to the state-of-the-art\nin multi-modal image translation and achieve better results for translation on\nchallenging datasets as well as for cross-domain retrieval on realistic\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 15:30:23 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 14:58:32 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 17:27:04 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1805.09749", "submitter": "Yiming Lin", "authors": "Yiming Lin, Shiyang Cheng, Jie Shen, Maja Pantic", "title": "MobiFace: A Novel Dataset for Mobile Face Tracking in the Wild", "comments": "To appear on The 14th IEEE International Conference on Automatic Face\n  and Gesture Recognition (FG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face tracking serves as the crucial initial step in mobile applications\ntrying to analyse target faces over time in mobile settings. However, this\nproblem has received little attention, mainly due to the scarcity of dedicated\nface tracking benchmarks. In this work, we introduce MobiFace, the first\ndataset for single face tracking in mobile situations. It consists of 80\nunedited live-streaming mobile videos captured by 70 different smartphone users\nin fully unconstrained environments. Over $95K$ bounding boxes are manually\nlabelled. The videos are carefully selected to cover typical smartphone usage.\nThe videos are also annotated with 14 attributes, including 6 newly proposed\nattributes and 8 commonly seen in object tracking. 36 state-of-the-art\ntrackers, including facial landmark trackers, generic object trackers and\ntrackers that we have fine-tuned or improved, are evaluated. The results\nsuggest that mobile face tracking cannot be solved through existing approaches.\nIn addition, we show that fine-tuning on the MobiFace training data\nsignificantly boosts the performance of deep learning-based trackers,\nsuggesting that MobiFace captures the unique characteristics of mobile face\ntracking. Our goal is to offer the community a diverse dataset to enable the\ndesign and evaluation of mobile face trackers. The dataset, annotations and the\nevaluation server will be on \\url{https://mobiface.github.io/}.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 15:59:22 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 12:39:19 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Lin", "Yiming", ""], ["Cheng", "Shiyang", ""], ["Shen", "Jie", ""], ["Pantic", "Maja", ""]]}, {"id": "1805.09791", "submitter": "Xiaoxi He", "authors": "Xiaoxi He, Zimu Zhou, Lothar Thiele", "title": "Multi-Task Zipping via Layer-wise Neuron Sharing", "comments": "Published as a conference paper at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future mobile devices are anticipated to perceive, understand and react to\nthe world on their own by running multiple correlated deep neural networks\non-device. Yet the complexity of these neural networks needs to be trimmed down\nboth within-model and cross-model to fit in mobile storage and memory. Previous\nstudies focus on squeezing the redundancy within a single neural network. In\nthis work, we aim to reduce the redundancy across multiple models. We propose\nMulti-Task Zipping (MTZ), a framework to automatically merge correlated,\npre-trained deep neural networks for cross-model compression. Central in MTZ is\na layer-wise neuron sharing and incoming weight updating scheme that induces a\nminimal change in the error function. MTZ inherits information from each model\nand demands light retraining to re-boost the accuracy of individual tasks.\nEvaluations show that MTZ is able to fully merge the hidden layers of two\nVGG-16 networks with a 3.18% increase in the test error averaged on ImageNet\nand CelebA, or share 39.61% parameters between the two networks with <0.5%\nincrease in the test errors for both tasks. The number of iterations to retrain\nthe combined network is at least 17.8 times lower than that of training a\nsingle VGG-16 network. Moreover, experiments show that MTZ is also able to\neffectively merge multiple residual networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:33:38 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 09:05:31 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["He", "Xiaoxi", ""], ["Zhou", "Zimu", ""], ["Thiele", "Lothar", ""]]}, {"id": "1805.09799", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Daniel Yang, Archana Venkataraman, Pamela Ventola,\n  Lawrence H. Staib, Kevin A. Pelphrey, and James S. Duncan", "title": "Prediction of Autism Treatment Response from Baseline fMRI using Random\n  Forests and Tree Bagging", "comments": "Multimodal Learning for Clinical Decision Support (ML-CDS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treating children with autism spectrum disorders (ASD) with behavioral\ninterventions, such as Pivotal Response Treatment (PRT), has shown promise in\nrecent studies. However, deciding which therapy is best for a given patient is\nlargely by trial and error, and choosing an ineffective intervention results in\nloss of valuable treatment time. We propose predicting patient response to PRT\nfrom baseline task-based fMRI by the novel application of a random forest and\ntree bagging strategy. Our proposed learning pipeline uses random forest\nregression to determine candidate brain voxels that may be informative in\npredicting treatment response. The candidate voxels are then tested stepwise\nfor inclusion in a bagged tree ensemble. After the predictive model is\nconstructed, bias correction is performed to further increase prediction\naccuracy. Using data from 19 ASD children who underwent a 16 week trial of PRT\nand a leave-one-out cross-validation framework, the presented learning pipeline\nwas tested against several standard methods and variations of the pipeline and\nresulted in the highest prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:41:40 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Yang", "Daniel", ""], ["Venkataraman", "Archana", ""], ["Ventola", "Pamela", ""], ["Staib", "Lawrence H.", ""], ["Pelphrey", "Kevin A.", ""], ["Duncan", "James S.", ""]]}, {"id": "1805.09806", "submitter": "Anurag Ranjan", "authors": "Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun,\n  Jonas Wulff, Michael J. Black", "title": "Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera\n  Motion, Optical Flow and Motion Segmentation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the unsupervised learning of several interconnected problems in\nlow-level vision: single view depth prediction, camera motion estimation,\noptical flow, and segmentation of a video into the static scene and moving\nregions. Our key insight is that these four fundamental vision problems are\ncoupled through geometric constraints. Consequently, learning to solve them\ntogether simplifies the problem because the solutions can reinforce each other.\nWe go beyond previous work by exploiting geometry more explicitly and\nsegmenting the scene into static and moving regions. To that end, we introduce\nCompetitive Collaboration, a framework that facilitates the coordinated\ntraining of multiple specialized neural networks to solve complex problems.\nCompetitive Collaboration works much like expectation-maximization, but with\nneural networks that act as both competitors to explain pixels that correspond\nto static or moving regions, and as collaborators through a moderator that\nassigns pixels to be either static or independently moving. Our novel method\nintegrates all these problems in a common framework and simultaneously reasons\nabout the segmentation of the scene into moving objects and the static\nbackground, the camera motion, depth of the static scene structure, and the\noptical flow of moving objects. Our model is trained without any supervision\nand achieves state-of-the-art performance among joint unsupervised methods on\nall sub-problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:49:05 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 21:29:52 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 19:03:56 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Ranjan", "Anurag", ""], ["Jampani", "Varun", ""], ["Balles", "Lukas", ""], ["Kim", "Kihwan", ""], ["Sun", "Deqing", ""], ["Wulff", "Jonas", ""], ["Black", "Michael J.", ""]]}, {"id": "1805.09817", "submitter": "Tinghui Zhou", "authors": "Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely", "title": "Stereo Magnification: Learning View Synthesis using Multiplane Images", "comments": "Accepted to SIGGRAPH 2018. Project webpage:\n  https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The view synthesis problem--generating novel views of a scene from known\nimagery--has garnered recent attention due in part to compelling applications\nin virtual and augmented reality. In this paper, we explore an intriguing\nscenario for view synthesis: extrapolating views from imagery captured by\nnarrow-baseline stereo cameras, including VR cameras and now-widespread\ndual-lens camera phones. We call this problem stereo magnification, and propose\na learning framework that leverages a new layered representation that we call\nmultiplane images (MPIs). Our method also uses a massive new data source for\nlearning view extrapolation: online videos on YouTube. Using data mined from\nsuch videos, we train a deep network that predicts an MPI from an input stereo\nimage pair. This inferred MPI can then be used to synthesize a range of novel\nviews of the scene, including views that extrapolate significantly beyond the\ninput baseline. We show that our method compares favorably with several recent\nview synthesis methods, and demonstrate applications in magnifying\nnarrow-baseline stereo images.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:58:02 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Zhou", "Tinghui", ""], ["Tucker", "Richard", ""], ["Flynn", "John", ""], ["Fyffe", "Graham", ""], ["Snavely", "Noah", ""]]}, {"id": "1805.09940", "submitter": "Huihui Fang Miss", "authors": "Huihui Fang, Jian Yang, Jianjun Zhu, Danni Ai, Yong Huang, Yurong\n  Jiang, Hong Song, Yongtian Wang", "title": "Greedy Graph Searching for Vascular Tracking in Angiographic Image\n  Sequences", "comments": "Submitted to Medical Physics; 30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vascular tracking of angiographic image sequences is one of the most\nclinically important tasks in the diagnostic assessment and interventional\nguidance of cardiac disease. However, this task can be challenging to\naccomplish because of unsatisfactory angiography image quality and complex\nvascular structures. Thus, this study proposed a new greedy graph search-based\nmethod for vascular tracking. Each vascular branch is separated from the\nvasculature and is tracked independently. Then, all branches are combined using\ntopology optimization, thereby resulting in complete vasculature tracking. A\ngray-based image registration method was applied to determine the tracking\nrange, and the deformation field between two consecutive frames was calculated.\nThe vascular branch was described using a vascular centerline extraction method\nwith multi-probability fusion-based topology optimization. We introduce an\nundirected acyclic graph establishment technique. A greedy search method was\nproposed to acquire all possible paths in the graph that might match the\ntracked vascular branch. The final tracking result was selected by branch\nmatching using dynamic time warping with a DAISY descriptor. The solution to\nthe problem reflected both the spatial and textural information between\nsuccessive frames. Experimental results demonstrated that the proposed method\nwas effective and robust for vascular tracking, attaining a F1 score of 0.89 on\na single branch dataset and 0.88 on a vessel tree dataset. This approach\nprovided a universal solution to address the problem of filamentary structure\ntracking.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 00:53:57 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Fang", "Huihui", ""], ["Yang", "Jian", ""], ["Zhu", "Jianjun", ""], ["Ai", "Danni", ""], ["Huang", "Yong", ""], ["Jiang", "Yurong", ""], ["Song", "Hong", ""], ["Wang", "Yongtian", ""]]}, {"id": "1805.09946", "submitter": "Dung Nguyen", "authors": "Dung Nguyen, Kien Nguyen, Sridha Sridharan, Iman Abbasnejad, David\n  Dean and Clinton Fookes", "title": "Meta Transfer Learning for Facial Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning techniques for automatic facial expression\nrecognition has recently attracted great interest but developed models are\nstill unable to generalize well due to the lack of large emotion datasets for\ndeep learning. To overcome this problem, in this paper, we propose utilizing a\nnovel transfer learning approach relying on PathNet and investigate how\nknowledge can be accumulated within a given dataset and how the knowledge\ncaptured from one emotion dataset can be transferred into another in order to\nimprove the overall performance. To evaluate the robustness of our system, we\nhave conducted various sets of experiments on two emotion datasets: SAVEE and\neNTERFACE. The experimental results demonstrate that our proposed system leads\nto improvement in performance of emotion recognition and performs significantly\nbetter than the recent state-of-the-art schemes adopting fine-\\\ntuning/pre-trained approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 01:56:30 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Nguyen", "Dung", ""], ["Nguyen", "Kien", ""], ["Sridharan", "Sridha", ""], ["Abbasnejad", "Iman", ""], ["Dean", "David", ""], ["Fookes", "Clinton", ""]]}, {"id": "1805.09957", "submitter": "Minhyuk Sung", "authors": "Minhyuk Sung, Hao Su, Ronald Yu, and Leonidas Guibas", "title": "Deep Functional Dictionaries: Learning Consistent Semantic Structures on\n  3D Models from Functions", "comments": null, "journal-ref": "NeurIPS 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various 3D semantic attributes such as segmentation masks, geometric\nfeatures, keypoints, and materials can be encoded as per-point probe functions\non 3D geometries. Given a collection of related 3D shapes, we consider how to\njointly analyze such probe functions over different shapes, and how to discover\ncommon latent structures using a neural network --- even in the absence of any\ncorrespondence information. Our network is trained on point cloud\nrepresentations of shape geometry and associated semantic functions on that\npoint cloud. These functions express a shared semantic understanding of the\nshapes but are not coordinated in any way. For example, in a segmentation task,\nthe functions can be indicator functions of arbitrary sets of shape parts, with\nthe particular combination involved not known to the network. Our network is\nable to produce a small dictionary of basis functions for each shape, a\ndictionary whose span includes the semantic functions provided for that shape.\nEven though our shapes have independent discretizations and no functional\ncorrespondences are provided, the network is able to generate latent bases, in\na consistent order, that reflect the shared semantic structure among the\nshapes. We demonstrate the effectiveness of our technique in various\nsegmentation and keypoint selection applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 03:07:15 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 22:38:53 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 06:02:55 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Sung", "Minhyuk", ""], ["Su", "Hao", ""], ["Yu", "Ronald", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1805.09967", "submitter": "Md Sirajus Salekin", "authors": "Md Sirajus Salekin, Ahmad Babaeian Jelodar, Rafsanjany Kushol", "title": "Cooking State Recognition from Images Using Inception Architecture", "comments": "6 pages, 8 figures, 4 tables", "journal-ref": null, "doi": "10.1109/ICREST.2019.8644262", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kitchen robot properly needs to understand the cooking environment to\ncontinue any cooking activities. But object's state detection has not been\nresearched well so far as like object detection. In this paper, we propose a\ndeep learning approach to identify different cooking states from images for a\nkitchen robot. In our research, we investigate particularly the performance of\nInception architecture and propose a modified architecture based on Inception\nmodel to classify different cooking states. The model is analyzed robustly in\nterms of different layers, and optimizers. Experimental results on a cooking\ndatasets demonstrate that proposed model can be a potential solution to the\ncooking state recognition problem.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 03:42:58 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 01:42:00 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Salekin", "Md Sirajus", ""], ["Jelodar", "Ahmad Babaeian", ""], ["Kushol", "Rafsanjany", ""]]}, {"id": "1805.09971", "submitter": "Zhangjian Ji", "authors": "Zhangjian Ji, Kai Feng, Yuhua Qian", "title": "Part-based Visual Tracking via Structural Support Correlation Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, part-based and support vector machines (SVM) based trackers have\nshown favorable performance. Nonetheless, the time-consuming online training\nand updating process limit their real-time applications. In order to better\ndeal with the partial occlusion issue and improve their efficiency, we propose\na novel part-based structural support correlation filter tracking method, which\nabsorbs the strong discriminative ability from SVM and the excellent property\nof part-based tracking methods which is less sensitive to partial occlusion.\nThen, our proposed model can learn the support correlation filter of each part\njointly by a star structure model, which preserves the spatial layout structure\namong parts and tolerates outliers of parts. In addition, to mitigate the issue\nof drift away from object further, we introduce inter-frame consistencies of\nlocal parts into our model. Finally, in our model, we accurately estimate the\nscale changes of object by the relative distance change among reliable parts.\nThe extensive empirical evaluations on three benchmark datasets: OTB2015,\nTempleColor128 and VOT2015 demonstrate that the proposed method performs\nsuperiorly against several state-of-the-art trackers in terms of tracking\naccuracy, speed and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 04:04:00 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Ji", "Zhangjian", ""], ["Feng", "Kai", ""], ["Qian", "Yuhua", ""]]}, {"id": "1805.09987", "submitter": "Zheng Xu", "authors": "Zheng Xu, Michael Wilber, Chen Fang, Aaron Hertzmann, Hailin Jin", "title": "Learning from Multi-domain Artistic Images for Arbitrary Style Transfer", "comments": "Update code link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast feed-forward network for arbitrary style transfer, which\ncan generate stylized image for previously unseen content and style image\npairs. Besides the traditional content and style representation based on deep\nfeatures and statistics for textures, we use adversarial networks to regularize\nthe generation of stylized images. Our adversarial network learns the intrinsic\nproperty of image styles from large-scale multi-domain artistic images. The\nadversarial training is challenging because both the input and output of our\ngenerator are diverse multi-domain images. We use a conditional generator that\nstylized content by shifting the statistics of deep features, and a conditional\ndiscriminator based on the coarse category of styles. Moreover, we propose a\nmask module to spatially decide the stylization level and stabilize adversarial\ntraining by avoiding mode collapse. As a side effect, our trained discriminator\ncan be applied to rank and select representative stylized images. We\nqualitatively and quantitatively evaluate the proposed method, and compare with\nrecent style transfer methods. We release our code and model at\nhttps://github.com/nightldj/behance_release.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 05:54:39 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 06:22:51 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Zheng", ""], ["Wilber", "Michael", ""], ["Fang", "Chen", ""], ["Hertzmann", "Aaron", ""], ["Jin", "Hailin", ""]]}, {"id": "1805.10002", "submitter": "Yanbin Liu", "authors": "Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju\n  Hwang and Yi Yang", "title": "Learning to Propagate Labels: Transductive Propagation Network for\n  Few-shot Learning", "comments": "Accepted in ICLR 2019; code available at\n  https://github.com/csyanbin/TPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot learning is to learn a classifier that generalizes well\neven when trained with a limited number of training instances per class. The\nrecently introduced meta-learning approaches tackle this problem by learning a\ngeneric classifier across a large number of multiclass classification tasks and\ngeneralizing the model to a new task. Yet, even with such meta-learning, the\nlow-data problem in the novel classification task still remains. In this paper,\nwe propose Transductive Propagation Network (TPN), a novel meta-learning\nframework for transductive inference that classifies the entire test set at\nonce to alleviate the low-data problem. Specifically, we propose to learn to\npropagate labels from labeled instances to unlabeled test instances, by\nlearning a graph construction module that exploits the manifold structure in\nthe data. TPN jointly learns both the parameters of feature embedding and the\ngraph construction in an end-to-end manner. We validate TPN on multiple\nbenchmark datasets, on which it largely outperforms existing few-shot learning\napproaches and achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 07:00:31 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 06:31:47 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 04:31:05 GMT"}, {"version": "v4", "created": "Fri, 1 Feb 2019 04:36:54 GMT"}, {"version": "v5", "created": "Fri, 8 Feb 2019 09:30:53 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Liu", "Yanbin", ""], ["Lee", "Juho", ""], ["Park", "Minseop", ""], ["Kim", "Saehoon", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Yi", ""]]}, {"id": "1805.10017", "submitter": "Chen Chen", "authors": "Chen Chen, Min Cao, Xiyuan Hu, Silong Peng", "title": "Key Person Aided Re-identification in Partially Ordered Pedestrian Set", "comments": "12 pages,6 figures, BMVC conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideally person re-identification seeks for perfect feature representation and\nmetric model that re-identify all various pedestrians well in non-overlapping\nviews at different locations with different camera configurations, which is\nvery challenging. However, in most pedestrian sets, there always are some\noutstanding persons who are relatively easy to re-identify. Inspired by the\nexistence of such data division, we propose a novel key person aided person\nre-identification framework based on the re-defined partially ordered\npedestrian sets. The outstanding persons, namely \"key persons\", are selected by\nthe K-nearest neighbor based saliency measurement. The partial order defined by\npedestrian entering time in surveillance associates the key persons with the\nquery person temporally and helps to locate the possible candidates.\nExperiments conducted on two video datasets show that the proposed key person\naided framework outperforms the state-of-the-art methods and improves the\nmatching accuracy greatly at all ranks.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 07:46:36 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Chen", "Chen", ""], ["Cao", "Min", ""], ["Hu", "Xiyuan", ""], ["Peng", "Silong", ""]]}, {"id": "1805.10030", "submitter": "Devendra Pratap Yadav", "authors": "Vineet Mehta, Devendra Pratap Yadav, Sai Srinadhu Katta, Abhinav Dhall", "title": "DIF : Dataset of Perceived Intoxicated Faces for Drunk Person\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic accidents cause over a million deaths every year, of which a large\nfraction is attributed to drunk driving. An automated intoxicated driver\ndetection system in vehicles will be useful in reducing accidents and related\nfinancial costs. Existing solutions require special equipment such as\nelectrocardiogram, infrared cameras or breathalyzers. In this work, we propose\na new dataset called DIF (Dataset of perceived Intoxicated Faces) which\ncontains audio-visual data of intoxicated and sober people obtained from online\nsources. To the best of our knowledge, this is the first work for automatic\nbimodal non-invasive intoxication detection. Convolutional Neural Networks\n(CNN) and Deep Neural Networks (DNN) are trained for computing the video and\naudio baselines, respectively. 3D CNN is used to exploit the Spatio-temporal\nchanges in the video. A simple variation of the traditional 3D convolution\nblock is proposed based on inducing non-linearity between the spatial and\ntemporal channels. Extensive experiments are performed to validate the approach\nand baselines.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:25:26 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 09:21:45 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 23:25:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mehta", "Vineet", ""], ["Yadav", "Devendra Pratap", ""], ["Katta", "Sai Srinadhu", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1805.10059", "submitter": "Michael Gadermayr", "authors": "Michael Gadermayr, Laxmi Gupta, Barbara M. Klinkhammer, Peter Boor,\n  Dorit Merhof", "title": "Unsupervisedly Training GANs for Segmenting Digital Pathology with\n  Automatically Generated Annotations", "comments": "Submitted to ISBI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generative adversarial networks exhibited excellent performances in\nsemi-supervised image analysis scenarios. In this paper, we go even further by\nproposing a fully unsupervised approach for segmentation applications with\nprior knowledge of the objects' shapes. We propose and investigate different\nstrategies to generate simulated label data and perform image-to-image\ntranslation between the image and the label domain using an adversarial model.\nSpecifically, we assess the impact of the annotation model's accuracy as well\nas the effect of simulating additional low-level image features. For\nexperimental evaluation, we consider the segmentation of the glomeruli, an\napplication scenario from renal pathology. Experiments provide proof of concept\nand also confirm that the strategy for creating the simulated label data is of\nparticular relevance considering the stability of GAN trainings.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 09:42:59 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 09:01:27 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Gadermayr", "Michael", ""], ["Gupta", "Laxmi", ""], ["Klinkhammer", "Barbara M.", ""], ["Boor", "Peter", ""], ["Merhof", "Dorit", ""]]}, {"id": "1805.10078", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Mohammad A. Haque, Paulo Lobato Correia,\n  Kamal Nasrollahi, Thomas B. Moeslund, Fernando Pereira", "title": "A Double-Deep Spatio-Angular Learning Framework for Light Field based\n  Face Recognition", "comments": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has attracted increasing attention due to its wide range of\napplications, but it is still challenging when facing large variations in the\nbiometric data characteristics. Lenslet light field cameras have recently come\ninto prominence to capture rich spatio-angular information, thus offering new\npossibilities for advanced biometric recognition systems. This paper proposes a\ndouble-deep spatio-angular learning framework for light field based face\nrecognition, which is able to learn both texture and angular dynamics in\nsequence using convolutional representations; this is a novel recognition\nframework that has never been proposed before for either face recognition or\nany other visual recognition task. The proposed double-deep learning framework\nincludes a long short-term memory (LSTM) recurrent network whose inputs are\nVGG-Face descriptions that are computed using a VGG-Very-Deep-16 convolutional\nneural network (CNN). The VGG-16 network uses different face viewpoints\nrendered from a full light field image, which are organised as a pseudo-video\nsequence. A comprehensive set of experiments has been conducted with the\nIST-EURECOM light field face database, for varied and challenging recognition\ntasks. Results show that the proposed framework achieves superior face\nrecognition performance when compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 10:59:16 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 14:21:43 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 23:12:01 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Haque", "Mohammad A.", ""], ["Correia", "Paulo Lobato", ""], ["Nasrollahi", "Kamal", ""], ["Moeslund", "Thomas B.", ""], ["Pereira", "Fernando", ""]]}, {"id": "1805.10106", "submitter": "Dhruv Rathi", "authors": "Dhruv Rathi, Sushant Jain, Dr. S. Indu", "title": "Underwater Fish Species Classification using Convolutional Neural\n  Network and Deep Learning", "comments": "Pre-print of paper to be published in IEEEXplore, accepted under the\n  International Conference of Advances in Pattern Recognition 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The target of this paper is to recommend a way for Automated classification\nof Fish species. A high accuracy fish classification is required for greater\nunderstanding of fish behavior in Ichthyology and by marine biologists.\nMaintaining a ledger of the number of fishes per species and marking the\nendangered species in large and small water bodies is required by concerned\ninstitutions. Majority of available methods focus on classification of fishes\noutside of water because underwater classification poses challenges such as\nbackground noises, distortion of images, the presence of other water bodies in\nimages, image quality and occlusion. This method uses a novel technique based\non Convolutional Neural Networks, Deep Learning and Image Processing to achieve\nan accuracy of 96.29%. This method ensures considerably discrimination accuracy\nimprovements than the previously proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:34:10 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Rathi", "Dhruv", ""], ["Jain", "Sushant", ""], ["Indu", "Dr. S.", ""]]}, {"id": "1805.10108", "submitter": "Rudresh Dwivedi", "authors": "Rudresh Dwivedi, Somnath Dey", "title": "Generating protected fingerprint template utilizing coprime mapping\n  transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identity of a user is permanently lost if biometric data gets compromised\nsince the biometric information is irreplaceable and irrevocable. To revoke and\nreissue a new template in place of the compromised biometric template, the idea\nof cancelable biometrics has been introduced. The concept behind cancelable\nbiometric is to irreversibly transform the original biometric template and\nperform the comparison in the protected domain. In this paper, a coprime\ntransformation scheme has been proposed to derive a protected fingerprint\ntemplate. The method divides the fingerprint region into a number of sectors\nwith respect to each minutiae point and identifies the nearest-neighbor\nminutiae in each sector. Then, ridge features for all neighboring minutiae\npoints are computed and mapped onto co-prime positions of a random matrix to\ngenerate the cancelable template. The proposed approach achieves an EER of\n1.82, 1.39, 4.02 and 5.77 on DB1, DB2, DB3 and DB4 datasets of the FVC2002 and\nan EER of 8.70, 7.95, 5.23 and 4.87 on DB1, DB2, DB3 and DB4 datasets of\nFVC2004 databases, respectively. Experimental evaluations indicate that the\nmethod outperforms in comparison to the current state-of-the-art. Moreover, it\nhas been confirmed from the security analysis that the proposed method fulfills\nthe desired characteristics of diversity, revocability, and non-invertibility\nwith a minor performance degradation caused by the transformation.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:38:03 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Dwivedi", "Rudresh", ""], ["Dey", "Somnath", ""]]}, {"id": "1805.10123", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin and Pau Rodriguez and Alexandre Lacoste", "title": "TADAM: Task dependent adaptive metric for improved few-shot learning", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 31, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning has become essential for producing models that generalize\nfrom few examples. In this work, we identify that metric scaling and metric\ntask conditioning are important to improve the performance of few-shot\nalgorithms. Our analysis reveals that simple metric scaling completely changes\nthe nature of few-shot algorithm parameter updates. Metric scaling provides\nimprovements up to 14% in accuracy for certain metrics on the mini-Imagenet\n5-way 5-shot classification task. We further propose a simple and effective way\nof conditioning a learner on the task sample set, resulting in learning a\ntask-dependent metric space. Moreover, we propose and empirically test a\npractical end-to-end optimization procedure based on auxiliary task co-training\nto learn a task-dependent metric space. The resulting few-shot learning model\nbased on the task-dependent scaled metric achieves state of the art on\nmini-Imagenet. We confirm these results on another few-shot dataset that we\nintroduce in this paper based on CIFAR100. Our code is publicly available at\nhttps://github.com/ElementAI/TADAM.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 20:17:59 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 22:42:57 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 14:36:12 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 18:47:30 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Rodriguez", "Pau", ""], ["Lacoste", "Alexandre", ""]]}, {"id": "1805.10129", "submitter": "Ryan Faulkner", "authors": "Ryan Faulkner, Doina Precup", "title": "Dyna Planning using a Feature Based Generative Model", "comments": "8 pages, 7 figures", "journal-ref": "24th Annual Proceedings of the Advances in Neural Information\n  Processing Systems (2010) pp. 1-9", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyna-style reinforcement learning is a powerful approach for problems where\nnot much real data is available. The main idea is to supplement real\ntrajectories, or sequences of sampled states over time, with simulated ones\nsampled from a learned model of the environment. However, in large state\nspaces, the problem of learning a good generative model of the environment has\nbeen open so far. We propose to use deep belief networks to learn an\nenvironment model for use in Dyna. We present our approach and validate it\nempirically on problems where the state observations consist of images. Our\nresults demonstrate that using deep belief networks, which are full generative\nmodels, significantly outperforms the use of linear expectation models,\nproposed in Sutton et al. (2008)\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 23:23:34 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Faulkner", "Ryan", ""], ["Precup", "Doina", ""]]}, {"id": "1805.10130", "submitter": "Yingjing Lu", "authors": "Yingjing Lu", "title": "Cross Domain Image Generation through Latent Space Exploration with\n  Adversarial Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional domain generation is a good way to interactively control sample\ngeneration process of deep generative models. However, once a conditional\ngenerative model has been created, it is often expensive to allow it to adapt\nto new conditional controls, especially the network structure is relatively\ndeep. We propose a conditioned latent domain transfer framework across latent\nspaces of unconditional variational autoencoders(VAE). With this framework, we\ncan allow unconditionally trained VAEs to generate images in its domain with\nconditionals provided by a latent representation of another domain. This\nframework does not assume commonalities between two domains. We demonstrate\neffectiveness and robustness of our model under widely used image datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 04:02:26 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Lu", "Yingjing", ""]]}, {"id": "1805.10143", "submitter": "Tang Yongliang", "authors": "Yongliang Tang, Jiashui Huang, Faen Zhang, Weiguo Gong", "title": "Deep Residual Networks with a Fully Connected Recon-struction Layer for\n  Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have achieved impressive performance in terms\nof both reconstruction accuracy and efficiency for single image\nsuper-resolution (SISR). However, the network model of these methods is a fully\nconvolutional neural network, which is limit to exploit the differentiated\ncontextual information over the global region of the input image because of the\nweight sharing in convolution height and width extent. In this paper, we\ndiscuss a new SISR architecture where features are extracted in the\nlow-resolution (LR) space, and then we use a fully connected layer which learns\nan array of differentiated upsampling weights to reconstruct the desired\nhigh-resolution (HR) image from the final obtained LR features. By doing so, we\neffectively exploit the differentiated contextual information over the whole\ninput image region, whilst maintaining the low computational complexity for the\noverall SR operations. In addition, we introduce an edge difference constraint\ninto our loss function to preserve edges and texture structures. Extensive\nexperiments validate that our SISR method outperforms the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:11:19 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 16:48:09 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Tang", "Yongliang", ""], ["Huang", "Jiashui", ""], ["Zhang", "Faen", ""], ["Gong", "Weiguo", ""]]}, {"id": "1805.10174", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris and Christos-Savvas Bouganis", "title": "f-CNN$^{\\text{x}}$: A Toolflow for Mapping Multi-CNN Applications on\n  FPGAs", "comments": "Accepted at the 28th International Conference on Field Programmable\n  Logic & Applications (FPL) 2018", "journal-ref": null, "doi": "10.1109/FPL.2018.00072", "report-no": null, "categories": "cs.CV cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictive power of Convolutional Neural Networks (CNNs) has been an\nintegral factor for emerging latency-sensitive applications, such as autonomous\ndrones and vehicles. Such systems employ multiple CNNs, each one trained for a\nparticular task. The efficient mapping of multiple CNNs on a single FPGA device\nis a challenging task as the allocation of compute resources and external\nmemory bandwidth needs to be optimised at design time. This paper proposes\nf-CNN$^{\\text{x}}$, an automated toolflow for the optimised mapping of multiple\nCNNs on FPGAs, comprising a novel multi-CNN hardware architecture together with\nan automated design space exploration method that considers the user-specified\nperformance requirements for each model to allocate compute resources and\ngenerate a synthesisable accelerator. Moreover, f-CNN$^{\\text{x}}$ employs a\nnovel scheduling algorithm that alleviates the limitations of the memory\nbandwidth contention between CNNs and sustains the high utilisation of the\narchitecture. Experimental evaluation shows that f-CNN$^{\\text{x}}$'s designs\noutperform contention-unaware FPGA mappings by up to 50% and deliver up to 6.8x\nhigher performance-per-Watt over highly optimised GPU designs for multi-CNN\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 14:25:18 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 20:16:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1805.10180", "submitter": "Hanchao Li", "authors": "Hanchao Li, Pengfei Xiong, Jie An, Lingxue Wang", "title": "Pyramid Attention Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A Pyramid Attention Network(PAN) is proposed to exploit the impact of global\ncontextual information in semantic segmentation. Different from most existing\nworks, we combine attention mechanism and spatial pyramid to extract precise\ndense features for pixel labeling instead of complicated dilated convolution\nand artificially designed decoder networks. Specifically, we introduce a\nFeature Pyramid Attention module to perform spatial pyramid attention structure\non high-level output and combining global pooling to learn a better feature\nrepresentation, and a Global Attention Upsample module on each decoder layer to\nprovide global context as a guidance of low-level features to select category\nlocalization details. The proposed approach achieves state-of-the-art\nperformance on PASCAL VOC 2012 and Cityscapes benchmarks with a new record of\nmIoU accuracy 84.0% on PASCAL VOC 2012, while training without COCO dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 14:40:14 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 09:50:42 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 11:46:47 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Hanchao", ""], ["Xiong", "Pengfei", ""], ["An", "Jie", ""], ["Wang", "Lingxue", ""]]}, {"id": "1805.10201", "submitter": "Dhritiman Das", "authors": "Dhritiman Das, Eduardo Coello, Rolf F Schulte, Bjoern H Menze", "title": "Qunatification of Metabolites in MR Spectroscopic Imaging using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-66179-7_53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Spectroscopic Imaging (MRSI) is a clinical imaging\nmodality for measuring tissue metabolite levels in-vivo. An accurate estimation\nof spectral parameters allows for better assessment of spectral quality and\nmetabolite concentration levels. The current gold standard quantification\nmethod is the LCModel - a commercial fitting tool. However, this fails for\nspectra having poor signal-to-noise ratio (SNR) or a large number of artifacts.\nThis paper introduces a framework based on random forest regression for\naccurate estimation of the output parameters of a model based analysis of MR\nspectroscopy data. The goal of our proposed framework is to learn the spectral\nfeatures from a training set comprising of different variations of both\nsimulated and in-vivo brain spectra and then use this learning for the\nsubsequent metabolite quantification. Experiments involve training and testing\non simulated and in-vivo human brain spectra. We estimate parameters such as\nconcentration of metabolites and compare our results with that from the\nLCModel.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:34:22 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Das", "Dhritiman", ""], ["Coello", "Eduardo", ""], ["Schulte", "Rolf F", ""], ["Menze", "Bjoern H", ""]]}, {"id": "1805.10207", "submitter": "Vivek Kumar Singh", "authors": "Vivek Kumar Singh, Santiago Romani, Hatem A.Rashwan, Farhan Akram,\n  Nidhi Pandey, Md. Mostafa Kamal Sarker, Jordina Torrents Barrena, Saddam\n  Abdulwahab, Adel Saleh, Miguel Arquez, Meritxell Arenas, Domenec Puig", "title": "Conditional Generative Adversarial and Convolutional Networks for X-ray\n  Breast Mass Segmentation and Shape Classification", "comments": "8 pages, Accepted at Medical Image Computing and Computer Assisted\n  Intervention (MICCAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach based on conditional Generative\nAdversarial Networks (cGAN) for breast mass segmentation in mammography. We\nhypothesized that the cGAN structure is well-suited to accurately outline the\nmass area, especially when the training data is limited. The generative network\nlearns intrinsic features of tumors while the adversarial network enforces\nsegmentations to be similar to the ground truth. Experiments performed on\ndozens of malignant tumors extracted from the public DDSM dataset and from our\nin-house private dataset confirm our hypothesis with very high Dice coefficient\nand Jaccard index (>94% and >89%, respectively) outperforming the scores\nobtained by other state-of-the-art approaches. Furthermore, in order to detect\nportray significant morphological features of the segmented tumor, a specific\nConvolutional Neural Network (CNN) have also been designed for classifying the\nsegmented tumor areas into four types (irregular, lobular, oval and round),\nwhich provides an overall accuracy about 72% with the DDSM dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:44:20 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 20:21:52 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Singh", "Vivek Kumar", ""], ["Romani", "Santiago", ""], ["Rashwan", "Hatem A.", ""], ["Akram", "Farhan", ""], ["Pandey", "Nidhi", ""], ["Sarker", "Md. Mostafa Kamal", ""], ["Barrena", "Jordina Torrents", ""], ["Abdulwahab", "Saddam", ""], ["Saleh", "Adel", ""], ["Arquez", "Miguel", ""], ["Arenas", "Meritxell", ""], ["Puig", "Domenec", ""]]}, {"id": "1805.10210", "submitter": "Samy Blusseau", "authors": "Jos\\'e Lezama (CMLA), Samy Blusseau (CMLA), Jean-Michel Morel (CMLA),\n  Gregory Randall (IIE), Rafael Grompone von Gioi (CMLA)", "title": "Psychophysics, Gestalts and Games", "comments": null, "journal-ref": "Giovanna Citti, Alessandro Sarti. Neuromathematics of Vision,\n  Springer Berlin Heidelberg, pp.217-242, 2014, Lecture Notes in Morphogenesis", "doi": "10.1007/978-3-642-34444-2_6", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychophysical studies are dedicated to the evaluation of the human\ngestalt detection on dot or Gabor patterns, and to model its dependence on the\npattern and background parameters. Nevertheless, even for these constrained\npercepts, psychophysics have not yet reached the challenging prediction stage,\nwhere human detection would be quantitatively predicted by a (generic) model.\nOn the other hand, Computer Vision has attempted at defining automatic\ndetection thresholds. This chapter sketches a procedure to confront these two\nmethodologies inspired in gestaltism. Using a computational quantitative\nversion of the non-accidentalness principle, we raise the possibility that the\npsychophysical and the (older) gestaltist setups, both applicable on dot or\nGabor patterns, find a useful complement in a Turing test. In our perceptual\nTuring test, human performance is compared by the scientist to the detection\nresult given by a computer. This confrontation permits to revive the abandoned\nmethod of gestaltic games. We sketch the elaboration of such a game, where the\nsubjects of the experiment are confronted to an alignment detection algorithm,\nand are invited to draw examples that will fool it. We show that in that way a\nmore precise definition of the alignment gestalt and of its computational\nformulation seems to emerge. Detection algorithms might also be relevant to\nmore classic psychophysical setups, where they can again play the role of a\nTuring test. To a visual experiment where subjects were invited to detect\nalignments in Gabor patterns, we associated a single function measuring the\nalignment detectability in the form of a number of false alarms (NFA). The\nfirst results indicate that the values of the NFA, as a function of all\nsimulation parameters, are highly correlated to the human detection. This fact,\nthat we intend to support by further experiments , might end up confirming that\nhuman alignment detection is the result of a single mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:48:14 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Lezama", "Jos\u00e9", "", "CMLA"], ["Blusseau", "Samy", "", "CMLA"], ["Morel", "Jean-Michel", "", "CMLA"], ["Randall", "Gregory", "", "IIE"], ["von Gioi", "Rafael Grompone", "", "CMLA"]]}, {"id": "1805.10241", "submitter": "Md. Mostafa Kamal Sarker", "authors": "Md. Mostafa Kamal Sarker, Hatem A. Rashwan, Farhan Akram, Syeda\n  Furruka Banu, Adel Saleh, Vivek Kumar Singh, Forhad U H Chowdhury, Saddam\n  Abdulwahab, Santiago Romani, Petia Radeva, Domenec Puig", "title": "SLSDeep: Skin Lesion Segmentation Based on Dilated Residual and Pyramid\n  Pooling Networks", "comments": "Accepted in MICCAI 2018, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion segmentation (SLS) in dermoscopic images is a crucial task for\nautomated diagnosis of melanoma. In this paper, we present a robust deep\nlearning SLS model, so-called SLSDeep, which is represented as an\nencoder-decoder network. The encoder network is constructed by dilated residual\nlayers, in turn, a pyramid pooling network followed by three convolution layers\nis used for the decoder. Unlike the traditional methods employing a\ncross-entropy loss, we investigated a loss function by combining both Negative\nLog Likelihood (NLL) and End Point Error (EPE) to accurately segment the\nmelanoma regions with sharp boundaries. The robustness of the proposed model\nwas evaluated on two public databases: ISBI 2016 and 2017 for skin lesion\nanalysis towards melanoma detection challenge. The proposed model outperforms\nthe state-of-the-art methods in terms of segmentation accuracy. Moreover, it is\ncapable to segment more than $100$ images of size 384x384 per second on a\nrecent GPU.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 16:38:50 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 00:31:18 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Sarker", "Md. Mostafa Kamal", ""], ["Rashwan", "Hatem A.", ""], ["Akram", "Farhan", ""], ["Banu", "Syeda Furruka", ""], ["Saleh", "Adel", ""], ["Singh", "Vivek Kumar", ""], ["Chowdhury", "Forhad U H", ""], ["Abdulwahab", "Saddam", ""], ["Romani", "Santiago", ""], ["Radeva", "Petia", ""], ["Puig", "Domenec", ""]]}, {"id": "1805.10253", "submitter": "Lechao Cheng", "authors": "Lechao Cheng, Chengyi Zhang, Zicheng Liao", "title": "Intrinsic Image Transformation via Scale Space Decomposition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new network structure for decomposing an image into its\nintrinsic albedo and shading. We treat this as an image-to-image transformation\nproblem and explore the scale space of the input and output. By expanding the\noutput images (albedo and shading) into their Laplacian pyramid components, we\ndevelop a multi-channel network structure that learns the image-to-image\ntransformation function in successive frequency bands in parallel, within each\nchannel is a fully convolutional neural network with skip connections. This\nnetwork structure is general and extensible, and has demonstrated excellent\nperformance on the intrinsic image decomposition problem. We evaluate the\nnetwork on two benchmark datasets: the MPI-Sintel dataset and the MIT Intrinsic\nImages dataset. Both quantitative and qualitative results show our model\ndelivers a clear progression over state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:11:23 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Cheng", "Lechao", ""], ["Zhang", "Chengyi", ""], ["Liao", "Zicheng", ""]]}, {"id": "1805.10255", "submitter": "Manoj Kumar", "authors": "Manoj Kumar, George E. Dahl, Vijay Vasudevan, Mohammad Norouzi", "title": "Parallel Architecture and Hyperparameter Search via Successive Halving\n  and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and powerful algorithm for parallel black box\noptimization called Successive Halving and Classification (SHAC). The algorithm\noperates in $K$ stages of parallel function evaluations and trains a cascade of\nbinary classifiers to iteratively cull the undesirable regions of the search\nspace. SHAC is easy to implement, requires no tuning of its own configuration\nparameters, is invariant to the scale of the objective function and can be\nbuilt using any choice of binary classifier. We adopt tree-based classifiers\nwithin SHAC and achieve competitive performance against several strong\nbaselines for optimizing synthetic functions, hyperparameters and\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:12:38 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Kumar", "Manoj", ""], ["Dahl", "George E.", ""], ["Vasudevan", "Vijay", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "1805.10256", "submitter": "Hongkai Yu", "authors": "Hongkai Yu, Dazhou Guo, Zhipeng Yan, Wei Liu, Jeff Simmons, Craig P.\n  Przybyla, Song Wang", "title": "Unsupervised Learning for Large-Scale Fiber Detection and Tracking in\n  Microscopic Material Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing 3D structures from serial section data is a long standing\nproblem in microscopy. The structure of a fiber reinforced composite material\ncan be reconstructed using a tracking-by-detection model. Tracking-by-detection\nalgorithms rely heavily on detection accuracy, especially the recall\nperformance. The state-of-the-art fiber detection algorithms perform well under\nideal conditions, but are not accurate where there are local degradations of\nimage quality, due to contaminants on the material surface and/or defocus blur.\nConvolutional Neural Networks (CNN) could be used for this problem, but would\nrequire a large number of manual annotated fibers, which are not available. We\npropose an unsupervised learning method to accurately detect fibers on the\nlarge scale, that is robust against local degradations of image quality. The\nproposed method does not require manual annotations, but uses fiber shape/size\npriors and spatio-temporal consistency in tracking to simulate the supervision\nin the training of the CNN. Experiments show significant improvements over\nstate-of-the-art fiber detection algorithms together with advanced tracking\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:14:27 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Yu", "Hongkai", ""], ["Guo", "Dazhou", ""], ["Yan", "Zhipeng", ""], ["Liu", "Wei", ""], ["Simmons", "Jeff", ""], ["Przybyla", "Craig P.", ""], ["Wang", "Song", ""]]}, {"id": "1805.10273", "submitter": "Jos\\'e Ignacio Orlando Dr", "authors": "Jos\\'e Ignacio Orlando, Jo\\~ao Barbosa Breda, Karel van Keer, Matthew\n  B. Blaschko, Pablo J. Blanco, Carlos A. Bulant", "title": "Towards a glaucoma risk index based on simulated hemodynamics from\n  fundus images", "comments": "MICCAI 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Glaucoma is the leading cause of irreversible but preventable blindness in\nthe world. Its major treatable risk factor is the intra-ocular pressure,\nalthough other biomarkers are being explored to improve the understanding of\nthe pathophysiology of the disease. It has been recently observed that glaucoma\ninduces changes in the ocular hemodynamics. However, its effects on the\nfunctional behavior of the retinal arterioles have not been studied yet. In\nthis paper we propose a first approach for characterizing those changes using\ncomputational hemodynamics. The retinal blood flow is simulated using a 0D\nmodel for a steady, incompressible non Newtonian fluid in rigid domains. The\nsimulation is performed on patient-specific arterial trees extracted from\nfundus images. We also propose a novel feature representation technique to\ncomprise the outcomes of the simulation stage into a fixed length feature\nvector that can be used for classification studies. Our experiments on a new\ndatabase of fundus images show that our approach is able to capture\nrepresentative changes in the hemodynamics of glaucomatous patients. Code and\ndata are publicly available in https://ignaciorlando.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:46:49 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 11:50:34 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 08:43:20 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 08:22:33 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Orlando", "Jos\u00e9 Ignacio", ""], ["Breda", "Jo\u00e3o Barbosa", ""], ["van Keer", "Karel", ""], ["Blaschko", "Matthew B.", ""], ["Blanco", "Pablo J.", ""], ["Bulant", "Carlos A.", ""]]}, {"id": "1805.10344", "submitter": "Simon Andermatt", "authors": "Simon Andermatt, Antal Horv\\'ath, Simon Pezold, Philippe Cattin", "title": "Pathology Segmentation using Distributional Differences to Images of\n  Healthy Origin", "comments": null, "journal-ref": "In International MICCAI Brainlesion Workshop, pp. 228-238.\n  Springer, Cham, 2018", "doi": "10.1007/978-3-030-11723-8_23", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully supervised segmentation methods require a large training cohort of\nalready segmented images, providing information at the pixel level of each\nimage. We present a method to automatically segment and model pathologies in\nmedical images, trained solely on data labelled on the image level as either\nhealthy or containing a visual defect. We base our method on CycleGAN, an\nimage-to-image translation technique, to translate images between the domains\nof healthy and pathological images. We extend the core idea with two key\ncontributions. Implementing the generators as residual generators allows us to\nexplicitly model the segmentation of the pathology. Realizing the translation\nfrom the healthy to the pathological domain using a variational autoencoder\nallows us to specify one representation of the pathology, as this\ntransformation is otherwise not unique. Our model hence not only allows us to\ncreate pixelwise semantic segmentations, it is also able to create inpaintings\nfor the segmentations to render the pathological image healthy. Furthermore, we\ncan draw new unseen pathology samples from this model based on the distribution\nin the data. We show quantitatively, that our method is able to segment\npathologies with a surprising accuracy being only slightly inferior to a\nstate-of-the-art fully supervised method, although the latter has per-pixel\nrather than per-image training information. Moreover, we show qualitative\nresults of both the segmentations and inpaintings. Our findings motivate\nfurther research into weakly-supervised segmentation using image level\nannotations, allowing for faster and cheaper acquisition of training data\nwithout a large sacrifice in segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 19:42:47 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 08:28:24 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Andermatt", "Simon", ""], ["Horv\u00e1th", "Antal", ""], ["Pezold", "Simon", ""], ["Cattin", "Philippe", ""]]}, {"id": "1805.10355", "submitter": "Semih Gunel", "authors": "Semih G\\\"unel, Helge Rhodin, Pascal Fua", "title": "What Face and Body Shapes Can Tell About Height", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a person's height from a single image is important for virtual\ngarment fitting, autonomous driving and surveillance, however, it is also very\nchallenging due to the absence of absolute scale information. We tackle the\nrarely addressed case, where camera parameters and scene geometry is unknown.\nTo nevertheless resolve the inherent scale ambiguity, we infer height from\nstatistics that are intrinsic to human anatomy and can be estimated from images\ndirectly, such as articulated pose, bone length proportions, and facial\nfeatures. Our contribution is twofold. First, we experiment with different\nmachine learning models to capture the relation between image content and human\nheight. Second, we show that performance is predominantly limited by dataset\nsize and create a new dataset that is three magnitudes larger, by mining\nexplicit height labels and propagating them to additional images through face\nrecognition and assignment consistency. Our evaluation shows that monocular\nheight estimation is possible with a MAE of 5.56cm.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 20:25:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["G\u00fcnel", "Semih", ""], ["Rhodin", "Helge", ""], ["Fua", "Pascal", ""]]}, {"id": "1805.10368", "submitter": "Josh Fromm", "authors": "Josh Fromm and Shwetak Patel and Matthai Philipose", "title": "Heterogeneous Bitwidth Binarization in Convolutional Neural Networks", "comments": "NIPS 2018 camera ready update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that fast, compact low-bitwidth neural networks can be\nsurprisingly accurate. These networks use homogeneous binarization: all\nparameters in each layer or (more commonly) the whole model have the same low\nbitwidth (e.g., 2 bits). However, modern hardware allows efficient designs\nwhere each arithmetic instruction can have a custom bitwidth, motivating\nheterogeneous binarization, where every parameter in the network may have a\ndifferent bitwidth. In this paper, we show that it is feasible and useful to\nselect bitwidths at the parameter granularity during training. For instance a\nheterogeneously quantized version of modern networks such as AlexNet and\nMobileNet, with the right mix of 1-, 2- and 3-bit parameters that average to\njust 1.4 bits can equal the accuracy of homogeneous 2-bit versions of these\nnetworks. Further, we provide analyses to show that the heterogeneously\nbinarized systems yield FPGA- and ASIC-based implementations that are\ncorrespondingly more efficient in both circuit area and energy efficiency than\ntheir homogeneous counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 21:21:32 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 19:35:07 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Fromm", "Josh", ""], ["Patel", "Shwetak", ""], ["Philipose", "Matthai", ""]]}, {"id": "1805.10376", "submitter": "Zhoubing Xu", "authors": "Zhoubing Xu, Yuankai Huo, JinHyeong Park, Bennett Landman, Andy\n  Milkowski, Sasa Grbic, Shaohua Zhou", "title": "Less is More: Simultaneous View Classification and Landmark Detection\n  for Abdominal Ultrasound Images", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An abdominal ultrasound examination, which is the most common ultrasound\nexamination, requires substantial manual efforts to acquire standard abdominal\norgan views, annotate the views in texts, and record clinically relevant organ\nmeasurements. Hence, automatic view classification and landmark detection of\nthe organs can be instrumental to streamline the examination workflow. However,\nthis is a challenging problem given not only the inherent difficulties from the\nultrasound modality, e.g., low contrast and large variations, but also the\nheterogeneity across tasks, i.e., one classification task for all views, and\nthen one landmark detection task for each relevant view. While convolutional\nneural networks (CNN) have demonstrated more promising outcomes on ultrasound\nimage analytics than traditional machine learning approaches, it becomes\nimpractical to deploy multiple networks (one for each task) due to the limited\ncomputational and memory resources on most existing ultrasound scanners. To\novercome such limits, we propose a multi-task learning framework to handle all\nthe tasks by a single network. This network is integrated to perform view\nclassification and landmark detection simultaneously; it is also equipped with\nglobal convolutional kernels, coordinate constraints, and a conditional\nadversarial module to leverage the performances. In an experimental study based\non 187,219 ultrasound images, with the proposed simplified approach we achieve\n(1) view classification accuracy better than the agreement between two clinical\nexperts and (2) landmark-based measurement errors on par with inter-user\nvariability. The multi-task approach also benefits from sharing the feature\nextraction during the training process across all tasks and, as a result,\noutperforms the approaches that address each task individually.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 21:55:09 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 19:17:02 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Xu", "Zhoubing", ""], ["Huo", "Yuankai", ""], ["Park", "JinHyeong", ""], ["Landman", "Bennett", ""], ["Milkowski", "Andy", ""], ["Grbic", "Sasa", ""], ["Zhou", "Shaohua", ""]]}, {"id": "1805.10384", "submitter": "Qi Qian", "authors": "Qi Qian, Jiasheng Tang, Hao Li, Shenghuo Zhu and Rong Jin", "title": "Large-scale Distance Metric Learning with Uncertainty", "comments": "accepted by CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) has been studied extensively in the past\ndecades for its superior performance with distance-based algorithms. Most of\nthe existing methods propose to learn a distance metric with pairwise or\ntriplet constraints. However, the number of constraints is quadratic or even\ncubic in the number of the original examples, which makes it challenging for\nDML to handle the large-scale data set. Besides, the real-world data may\ncontain various uncertainty, especially for the image data. The uncertainty can\nmislead the learning procedure and cause the performance degradation. By\ninvestigating the image data, we find that the original data can be observed\nfrom a small set of clean latent examples with different distortions. In this\nwork, we propose the margin preserving metric learning framework to learn the\ndistance metric and latent examples simultaneously. By leveraging the ideal\nproperties of latent examples, the training efficiency can be improved\nsignificantly while the learned metric also becomes robust to the uncertainty\nin the original data. Furthermore, we can show that the metric is learned from\nlatent examples only, but it can preserve the large margin property even for\nthe original data. The empirical study on the benchmark image data sets\ndemonstrates the efficacy and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 22:44:59 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Qian", "Qi", ""], ["Tang", "Jiasheng", ""], ["Li", "Hao", ""], ["Zhu", "Shenghuo", ""], ["Jin", "Rong", ""]]}, {"id": "1805.10397", "submitter": "Dan Nguyen", "authors": "Dan Nguyen, Xun Jia, David Sher, Mu-Han Lin, Zohaib Iqbal, Hui Liu,\n  Steve Jiang", "title": "Three-Dimensional Radiotherapy Dose Prediction on Head and Neck Cancer\n  Patients with a Hierarchically Densely Connected U-net Deep Learning\n  Architecture", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/ab039b", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment planning process for patients with head and neck (H&N) cancer\nis regarded as one of the most complicated due to large target volume, multiple\nprescription dose levels, and many radiation-sensitive critical structures near\nthe target. Treatment planning for this site requires a high level of human\nexpertise and a tremendous amount of effort to produce personalized high\nquality plans, taking as long as a week, which deteriorates the chances of\ntumor control and patient survival. To solve this problem, we propose to\ninvestigate a deep learning-based dose prediction model, Hierarchically Densely\nConnected U-net, based on two highly popular network architectures: U-net and\nDenseNet. We find that this new architecture is able to accurately and\nefficiently predict the dose distribution, outperforming the other two models,\nthe Standard U-net and DenseNet, in homogeneity, dose conformity, and dose\ncoverage on the test data. Averaging across all organs at risk, our proposed\nmodel is capable of predicting the organ-at-risk max dose within 6.3% and mean\ndose within 5.1% of the prescription dose on the test data. The other models,\nthe Standard U-net and DenseNet, performed worse, having an averaged\norgan-at-risk max dose prediction error of 8.2% and 9.3%, respectively, and\naveraged mean dose prediction error of 6.4% and 6.8%, respectively. In\naddition, our proposed model used 12 times less trainable parameters than the\nStandard U-net, and predicted the patient dose 4 times faster than DenseNet.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 23:40:32 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 21:40:34 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 20:46:44 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Nguyen", "Dan", ""], ["Jia", "Xun", ""], ["Sher", "David", ""], ["Lin", "Mu-Han", ""], ["Iqbal", "Zohaib", ""], ["Liu", "Hui", ""], ["Jiang", "Steve", ""]]}, {"id": "1805.10416", "submitter": "Mohammad Ahangar Kiasari", "authors": "Mohammad Ahangar Kiasari, Dennis Singh Moirangthem, Minho Lee", "title": "Human Action Generation with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent advances in generative models, we introduce a human\naction generation model in order to generate a consecutive sequence of human\nmotions to formulate novel actions. We propose a framework of an autoencoder\nand a generative adversarial network (GAN) to produce multiple and consecutive\nhuman actions conditioned on the initial state and the given class label. The\nproposed model is trained in an end-to-end fashion, where the autoencoder is\njointly trained with the GAN. The model is trained on the NTU RGB+D dataset and\nwe show that the proposed model can generate different styles of actions.\nMoreover, the model can successfully generate a sequence of novel actions given\ndifferent action labels as conditions. The conventional human action prediction\nand generation models lack those features, which are essential for practical\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 02:57:34 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Kiasari", "Mohammad Ahangar", ""], ["Moirangthem", "Dennis Singh", ""], ["Lee", "Minho", ""]]}, {"id": "1805.10421", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, Ali\n  Borji", "title": "Enhanced-alignment Measure for Binary Foreground Map Evaluation", "comments": "8pages, 10 figures, IJCAI 2018 (oral)", "journal-ref": "IJCAI 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing binary foreground map (FM) measures to address various types of\nerrors in either pixel-wise or structural ways. These measures consider\npixel-level match or image-level information independently, while cognitive\nvision studies have shown that human vision is highly sensitive to both global\ninformation and local details in scenes. In this paper, we take a detailed look\nat current binary FM evaluation measures and propose a novel and effective\nE-measure (Enhanced-alignment measure). Our measure combines local pixel values\nwith the image-level mean value in one term, jointly capturing image-level\nstatistics and local pixel matching information. We demonstrate the superiority\nof our measure over the available measures on 4 popular datasets via 5\nmeta-measures, including ranking models for applications, demoting generic,\nrandom Gaussian noise maps, ground-truth switch, as well as human judgments. We\nfind large improvements in almost all the meta-measures. For instance, in terms\nof application ranking, we observe improvementrangingfrom9.08% to 19.65%\ncompared with other popular measures.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 03:29:38 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 03:04:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Gong", "Cheng", ""], ["Cao", "Yang", ""], ["Ren", "Bo", ""], ["Cheng", "Ming-Ming", ""], ["Borji", "Ali", ""]]}, {"id": "1805.10433", "submitter": "Rudresh Dwivedi", "authors": "Rudresh Dwivedi and Somnath Dey", "title": "A novel hybrid score level and decision level fusion scheme for\n  cancelable multi-biometric verification", "comments": null, "journal-ref": null, "doi": "10.1007/s10489-018-1311-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the benefits of biometric-based authentication systems, there are\nfew concerns raised because of the sensitivity of biometric data to outliers,\nlow performance caused due to intra-class variations and privacy invasion\ncaused by information leakage. To address these issues, we propose a hybrid\nfusion framework where only the protected modalities are combined to fulfill\nthe requirement of secrecy and performance improvement. This paper presents a\nmethod to integrate cancelable modalities utilizing mean-closure weighting\n(MCW) score level and Dempster-Shafer (DS) theory based decision level fusion\nfor iris and fingerprint to mitigate the limitations in the individual score or\ndecision fusion mechanisms. The proposed hybrid fusion scheme incorporates the\nsimilarity scores from different matchers corresponding to each protected\nmodality. The individual scores obtained from different matchers for each\nmodality are combined using MCW score fusion method. The MCW technique achieves\nthe optimal weight for each matcher involved in the score computation. Further,\nDS theory is applied to the induced scores to output the final decision. The\nrigorous experimental evaluations on three virtual databases indicate that the\nproposed hybrid fusion framework outperforms over the component level or\nindividual fusion methods (score level and decision level fusion). As a result,\nwe achieve (48%,66%), (72%,86%) and (49%,38%) of performance improvement over\nunimodal cancelable iris and unimodal cancelable fingerprint verification\nsystems for Virtual_A, Virtual_B and Virtual_C databases, respectively. Also,\nthe proposed method is robust enough to the variability of scores and outliers\nsatisfying the requirement of secure authentication.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 06:04:45 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Dwivedi", "Rudresh", ""], ["Dey", "Somnath", ""]]}, {"id": "1805.10437", "submitter": "Yanjun Li", "authors": "Yanjun Li and Yoram Bresler", "title": "Multichannel Sparse Blind Deconvolution on the Sphere", "comments": "50 pages, 10 figures. Some of the results in this paper were\n  presented at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multichannel blind deconvolution is the problem of recovering an unknown\nsignal $f$ and multiple unknown channels $x_i$ from their circular convolution\n$y_i=x_i \\circledast f$ ($i=1,2,\\dots,N$). We consider the case where the\n$x_i$'s are sparse, and convolution with $f$ is invertible. Our nonconvex\noptimization formulation solves for a filter $h$ on the unit sphere that\nproduces sparse output $y_i\\circledast h$. Under some technical assumptions, we\nshow that all local minima of the objective function correspond to the inverse\nfilter of $f$ up to an inherent sign and shift ambiguity, and all saddle points\nhave strictly negative curvatures. This geometric structure allows successful\nrecovery of $f$ and $x_i$ using a simple manifold gradient descent (MGD)\nalgorithm. Our theoretical findings are complemented by numerical experiments,\nwhich demonstrate superior performance of the proposed approach over the\nprevious methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 07:04:18 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 20:59:36 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Yanjun", ""], ["Bresler", "Yoram", ""]]}, {"id": "1805.10445", "submitter": "Ke Zhang", "authors": "Ke Zhang, Na Liu, Xingfang Yuan, Xinyao Guo, Ce Gao, Zhenbing Zhao,\n  Zhanyu Ma", "title": "Fine-Grained Age Estimation in the wild with Attention LSTM Networks", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation from a single face image has been an essential task in the\nfield of human-computer interaction and computer vision, which has a wide range\nof practical application values. Accuracy of age estimation of face images in\nthe wild is relatively low for existing methods, because they only take into\naccount the global features, while neglecting the fine-grained features of\nage-sensitive areas. We propose a novel method based on our attention long\nshort-term memory (AL) network for fine-grained age estimation in the wild,\ninspired by the fine-grained categories and the visual attention mechanism.\nThis method combines the residual networks (ResNets) or the residual network of\nresidual network (RoR) models with LSTM units to construct AL-ResNets or AL-RoR\nnetworks to extract local features of age-sensitive regions, which effectively\nimproves the age estimation accuracy. First, a ResNets or a RoR model\npretrained on ImageNet dataset is selected as the basic model, which is then\nfine-tuned on the IMDB-WIKI-101 dataset for age estimation. Then, we fine-tune\nthe ResNets or the RoR on the target age datasets to extract the global\nfeatures of face images. To extract the local features of age-sensitive\nregions, the LSTM unit is then presented to obtain the coordinates of the\nagesensitive region automatically. Finally, the age group classification is\nconducted directly on the Adience dataset, and age-regression experiments are\nperformed by the Deep EXpectation algorithm (DEX) on MORPH Album 2, FG-NET and\n15/16LAP datasets. By combining the global and the local features, we obtain\nour final prediction results. Experimental results illustrate the effectiveness\nand robustness of the proposed AL-ResNets or AL-RoR for age estimation in the\nwild, where it achieves better state-of-the-art performance than all other\nconvolutional neural network.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 08:27:01 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 11:47:05 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Zhang", "Ke", ""], ["Liu", "Na", ""], ["Yuan", "Xingfang", ""], ["Guo", "Xinyao", ""], ["Gao", "Ce", ""], ["Zhao", "Zhenbing", ""], ["Ma", "Zhanyu", ""]]}, {"id": "1805.10476", "submitter": "Li Yunkun", "authors": "YunKun Li and XiaoJun Wu and Josef Kittler", "title": "L1-(2D)2PCANet: A Deep Learning Network for Face Recognition", "comments": "8 pages and 5 figures", "journal-ref": null, "doi": "10.1117/1.JEI.28.2.023016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep learning network L1-(2D)2PCANet for\nface recognition, which is based on L1-norm-based two-directional\ntwo-dimensional principal component analysis (L1-(2D)2PCA). In our network, the\nrole of L1-(2D)2PCA is to learn the filters of multiple convolution layers.\nAfter the convolution layers, we deploy binary hashing and block-wise histogram\nfor pooling. We test our network on some benchmark facial datasets YALE, AR,\nExtended Yale B, LFW-a and FERET with CNN, PCANet, 2DPCANet and L1-PCANet as\ncomparison. The results show that the recognition performance of L1-(2D)2PCANet\nin all tests is better than baseline networks, especially when there are\noutliers in the test data. Owing to the L1-norm, L1-2D2PCANet is robust to\noutliers and changes of the training images.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 12:56:21 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Li", "YunKun", ""], ["Wu", "XiaoJun", ""], ["Kittler", "Josef", ""]]}, {"id": "1805.10483", "submitter": "Wayne Wu", "authors": "Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, Qiang Zhou", "title": "Look at Boundary: A Boundary-Aware Face Alignment Algorithm", "comments": "Accepted to CVPR 2018. Project page:\n  https://wywu.github.io/projects/LAB/LAB.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel boundary-aware face alignment algorithm by utilising\nboundary lines as the geometric structure of a human face to help facial\nlandmark localisation. Unlike the conventional heatmap based method and\nregression based method, our approach derives face landmarks from boundary\nlines which remove the ambiguities in the landmark definition. Three questions\nare explored and answered by this work: 1. Why using boundary? 2. How to use\nboundary? 3. What is the relationship between boundary estimation and landmarks\nlocalisation? Our boundary- aware face alignment algorithm achieves 3.49% mean\nerror on 300-W Fullset, which outperforms state-of-the-art methods by a large\nmargin. Our method can also easily integrate information from other datasets.\nBy utilising boundary information of 300-W dataset, our method achieves 3.92%\nmean error with 0.39% failure rate on COFW dataset, and 1.25% mean error on\nAFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training\nand testing across different factors, including poses, expressions,\nilluminations, makeups, occlusions, and blurriness. Dataset and model will be\npublicly available at https://wywu.github.io/projects/LAB/LAB.html\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 13:19:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["Yang", "Shuo", ""], ["Wang", "Quan", ""], ["Cai", "Yici", ""], ["Zhou", "Qiang", ""]]}, {"id": "1805.10485", "submitter": "Xiaoxiang Zhu", "authors": "Lichao Mou, and Xiao Xiang Zhu", "title": "Vehicle Instance Segmentation from Aerial Image and Video Using a\n  Multi-Task Learning Residual Fully Convolutional Network", "comments": "Preprint of a paper accepted by IEEE Transactions On Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2018.2841808", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and semantic segmentation are two main themes in object\nretrieval from high-resolution remote sensing images, which have recently\nachieved remarkable performance by surfing the wave of deep learning and, more\nnotably, convolutional neural networks (CNNs). In this paper, we are interested\nin a novel, more challenging problem of vehicle instance segmentation, which\nentails identifying, at a pixel-level, where the vehicles appear as well as\nassociating each pixel with a physical instance of a vehicle. In contrast,\nvehicle detection and semantic segmentation each only concern one of the two.\nWe propose to tackle this problem with a semantic boundary-aware multi-task\nlearning network. More specifically, we utilize the philosophy of residual\nlearning (ResNet) to construct a fully convolutional network that is capable of\nharnessing multi-level contextual feature representations learned from\ndifferent residual blocks. We theoretically analyze and discuss why residual\nnetworks can produce better probability maps for pixel-wise segmentation tasks.\nThen, based on this network architecture, we propose a unified multi-task\nlearning network that can simultaneously learn two complementary tasks, namely,\nsegmenting vehicle regions and detecting semantic boundaries. The latter\nsubproblem is helpful for differentiating closely spaced vehicles, which are\nusually not correctly separated into instances. Currently, datasets with\npixel-wise annotation for vehicle extraction are ISPRS dataset and IEEE GRSS\nDFC2015 dataset over Zeebrugge, which specializes in semantic segmentation.\nTherefore, we built a new, more challenging dataset for vehicle instance\nsegmentation, called the Busy Parking Lot UAV Video dataset, and we make our\ndataset available at http://www.sipeo.bgu.tum.de/download so that it can be\nused to benchmark future vehicle instance segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 13:56:47 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1805.10531", "submitter": "Christopher Metzler", "authors": "Christopher A. Metzler, Ali Mousavi, Reinhard Heckel, Richard G.\n  Baraniuk", "title": "Unsupervised Learning with Stein's Unbiased Risk Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from unlabeled and noisy data is one of the grand challenges of\nmachine learning. As such, it has seen a flurry of research with new ideas\nproposed continuously. In this work, we revisit a classical idea: Stein's\nUnbiased Risk Estimator (SURE). We show that, in the context of image recovery,\nSURE and its generalizations can be used to train convolutional neural networks\n(CNNs) for a range of image denoising and recovery problems without any ground\ntruth data.\n  Specifically, our goal is to reconstruct an image $x$ from a noisy linear\ntransformation (measurement) of the image. We consider two scenarios: one where\nno additional data is available and one where we have measurements of other\nimages that are drawn from the same noisy distribution as $x$, but have no\naccess to the clean images. Such is the case, for instance, in the context of\nmedical imaging, microscopy, and astronomy, where noise-less ground truth data\nis rarely available.\n  We show that in this situation, SURE can be used to estimate the\nmean-squared-error loss associated with an estimate of $x$. Using this estimate\nof the loss, we train networks to perform denoising and compressed sensing\nrecovery. In addition, we also use the SURE framework to partially explain and\nimprove upon an intriguing results presented by Ulyanov et al. in \"Deep Image\nPrior\": that a network initialized with random weights and fit to a single\nnoisy image can effectively denoise that image.\n  Public implementations of the networks and methods described in this paper\ncan be found at https://github.com/ricedsp/D-AMP_Toolbox.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 20:01:15 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 22:21:21 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 20:24:17 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Metzler", "Christopher A.", ""], ["Mousavi", "Ali", ""], ["Heckel", "Reinhard", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1805.10538", "submitter": "Mrigank Rochan", "authors": "Mrigank Rochan, Linwei Ye, Yang Wang", "title": "Video Summarization Using Fully Convolutional Sequence Networks", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of video summarization. Given an input\nvideo, the goal is to select a subset of the frames to create a summary video\nthat optimally captures the important information of the input video. With the\nlarge amount of videos available online, video summarization provides a useful\ntool that assists video search, retrieval, browsing, etc. In this paper, we\nformulate video summarization as a sequence labeling problem. Unlike existing\napproaches that use recurrent models, we propose fully convolutional sequence\nmodels to solve video summarization. We firstly establish a novel connection\nbetween semantic segmentation and video summarization, and then adapt popular\nsemantic segmentation networks for video summarization. Extensive experiments\nand analysis on two benchmark datasets demonstrate the effectiveness of our\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 20:58:38 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 21:10:08 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Rochan", "Mrigank", ""], ["Ye", "Linwei", ""], ["Wang", "Yang", ""]]}, {"id": "1805.10546", "submitter": "Ismail Elezi", "authors": "Ismail Elezi, Alessandro Torcinovich, Sebastiano Vascon and Marcello\n  Pelillo", "title": "Transductive Label Augmentation for Improved Deep Network Learning", "comments": "Accepted on IEEE International Conference on Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major impediment to the application of deep learning to real-world problems\nis the scarcity of labeled data. Small training sets are in fact of no use to\ndeep networks as, due to the large number of trainable parameters, they will\nvery likely be subject to overfitting phenomena. On the other hand, the\nincrement of the training set size through further manual or semi-automatic\nlabellings can be costly, if not possible at times. Thus, the standard\ntechniques to address this issue are transfer learning and data augmentation,\nwhich consists of applying some sort of \"transformation\" to existing labeled\ninstances to let the training set grow in size. Although this approach works\nwell in applications such as image classification, where it is relatively\nsimple to design suitable transformation operators, it is not obvious how to\napply it in more structured scenarios. Motivated by the observation that in\nvirtually all application domains it is easy to obtain unlabeled data, in this\npaper we take a different perspective and propose a \\emph{label augmentation}\napproach. We start from a small, curated labeled dataset and let the labels\npropagate through a larger set of unlabeled data using graph transduction\ntechniques. This allows us to naturally use (second-order) similarity\ninformation which resides in the data, a source of information which is\ntypically neglected by standard augmentation techniques. In particular, we show\nthat by using known game theoretic transductive processes we can create larger\nand accurate enough labeled datasets which use results in better trained neural\nnetworks. Preliminary experiments are reported which demonstrate a consistent\nimprovement over standard image classification datasets.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 21:59:09 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Elezi", "Ismail", ""], ["Torcinovich", "Alessandro", ""], ["Vascon", "Sebastiano", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1805.10547", "submitter": "Volkan Cirik", "authors": "Volkan Cirik, Taylor Berg-Kirkpatrick, Louis-Philippe Morency", "title": "Using Syntax to Ground Referring Expressions in Natural Images", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GroundNet, a neural network for referring expression recognition\n-- the task of localizing (or grounding) in an image the object referred to by\na natural language expression. Our approach to this task is the first to rely\non a syntactic analysis of the input referring expression in order to inform\nthe structure of the computation graph. Given a parse tree for an input\nexpression, we explicitly map the syntactic constituents and relationships\npresent in the tree to a composed graph of neural modules that defines our\narchitecture for performing localization. This syntax-based approach aids\nlocalization of \\textit{both} the target object and auxiliary supporting\nobjects mentioned in the expression. As a result, GroundNet is more\ninterpretable than previous methods: we can (1) determine which phrase of the\nreferring expression points to which object in the image and (2) track how the\nlocalization of the target object is determined by the network. We study this\nproperty empirically by introducing a new set of annotations on the GoogleRef\ndataset to evaluate localization of supporting objects. Our experiments show\nthat GroundNet achieves state-of-the-art accuracy in identifying supporting\nobjects, while maintaining comparable performance in the localization of target\nobjects.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 22:02:05 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Cirik", "Volkan", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1805.10548", "submitter": "Ismail Elezi", "authors": "Lukas Tuggener, Ismail Elezi, Jurgen Schmidhuber and Thilo Stadelmann", "title": "Deep Watershed Detector for Music Object Recognition", "comments": "Accepted on The 19th International Society for Music Information\n  Retrieval Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Music Recognition (OMR) is an important and challenging area within\nmusic information retrieval, the accurate detection of music symbols in digital\nimages is a core functionality of any OMR pipeline. In this paper, we introduce\na novel object detection method, based on synthetic energy maps and the\nwatershed transform, called Deep Watershed Detector (DWD). Our method is\nspecifically tailored to deal with high resolution images that contain a large\nnumber of very small objects and is therefore able to process full pages of\nwritten music. We present state-of-the-art detection results of common music\nsymbols and show DWD's ability to work with synthetic scores equally well as on\nhandwritten music.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 22:13:16 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Tuggener", "Lukas", ""], ["Elezi", "Ismail", ""], ["Schmidhuber", "Jurgen", ""], ["Stadelmann", "Thilo", ""]]}, {"id": "1805.10557", "submitter": "Naman Kohli", "authors": "Naman Kohli, Mayank Vatsa, Richa Singh, Afzel Noore, Angshul Majumdar", "title": "Hierarchical Representation Learning for Kinship Verification", "comments": null, "journal-ref": "IEEE Transactions on Image Processing ( Volume: 26, Issue: 1, Jan.\n  2017 )", "doi": "10.1109/TIP.2016.2609811", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Kinship verification has a number of applications such as organizing large\ncollections of images and recognizing resemblances among humans. In this\nresearch, first, a human study is conducted to understand the capabilities of\nhuman mind and to identify the discriminatory areas of a face that facilitate\nkinship-cues. Utilizing the information obtained from the human study, a\nhierarchical Kinship Verification via Representation Learning (KVRL) framework\nis utilized to learn the representation of different face regions in an\nunsupervised manner. We propose a novel approach for feature representation\ntermed as filtered contractive deep belief networks (fcDBN). The proposed\nfeature representation encodes relational information present in images using\nfilters and contractive regularization penalty. A compact representation of\nfacial images of kin is extracted as an output from the learned model and a\nmulti-layer neural network is utilized to verify the kin accurately. A new WVU\nKinship Database is created which consists of multiple images per subject to\nfacilitate kinship verification. The results show that the proposed deep\nlearning framework (KVRL-fcDBN) yields stateof-the-art kinship verification\naccuracy on the WVU Kinship database and on four existing benchmark datasets.\nFurther, kinship information is used as a soft biometric modality to boost the\nperformance of face verification via product of likelihood ratio and support\nvector machine based approaches. Using the proposed KVRL-fcDBN framework, an\nimprovement of over 20% is observed in the performance of face verification.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 00:18:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Kohli", "Naman", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1805.10558", "submitter": "Honggang Chen", "authors": "Honggang Chen and Xiaohai He and Linbo Qing and Shuhua Xiong and\n  Truong Q. Nguyen", "title": "DPW-SDNet: Dual Pixel-Wavelet Domain Deep CNNs for Soft Decoding of\n  JPEG-Compressed Images", "comments": "CVPRW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is one of the widely used lossy compression methods. JPEG-compressed\nimages usually suffer from compression artifacts including blocking and\nblurring, especially at low bit-rates. Soft decoding is an effective solution\nto improve the quality of compressed images without changing codec or\nintroducing extra coding bits. Inspired by the excellent performance of the\ndeep convolutional neural networks (CNNs) on both low-level and high-level\ncomputer vision problems, we develop a dual pixel-wavelet domain deep\nCNNs-based soft decoding network for JPEG-compressed images, namely DPW-SDNet.\nThe pixel domain deep network takes the four downsampled versions of the\ncompressed image to form a 4-channel input and outputs a pixel domain\nprediction, while the wavelet domain deep network uses the 1-level discrete\nwavelet transformation (DWT) coefficients to form a 4-channel input to produce\na DWT domain prediction. The pixel domain and wavelet domain estimates are\ncombined to generate the final soft decoded result. Experimental results\ndemonstrate the superiority of the proposed DPW-SDNet over several\nstate-of-the-art compression artifacts reduction algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 00:27:25 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Chen", "Honggang", ""], ["He", "Xiaohai", ""], ["Qing", "Linbo", ""], ["Xiong", "Shuhua", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1805.10561", "submitter": "Hongyu Ren", "authors": "Hongyu Ren, Russell Stewart, Jiaming Song, Volodymyr Kuleshov, Stefano\n  Ermon", "title": "Adversarial Constraint Learning for Structured Prediction", "comments": "To appear at IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based learning reduces the burden of collecting labels by having\nusers specify general properties of structured outputs, such as constraints\nimposed by physical laws. We propose a novel framework for simultaneously\nlearning these constraints and using them for supervision, bypassing the\ndifficulty of using domain expertise to manually specify constraints. Learning\nrequires a black-box simulator of structured outputs, which generates valid\nlabels, but need not model their corresponding inputs or the input-label\nrelationship. At training time, we constrain the model to produce outputs that\ncannot be distinguished from simulated labels by adversarial training.\nProviding our framework with a small number of labeled inputs gives rise to a\nnew semi-supervised structured prediction model; we evaluate this model on\nmultiple tasks --- tracking, pose estimation and time series prediction --- and\nfind that it achieves high accuracy with only a small number of labeled inputs.\nIn some cases, no labels are required at all.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 01:27:28 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 02:28:48 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Ren", "Hongyu", ""], ["Stewart", "Russell", ""], ["Song", "Jiaming", ""], ["Kuleshov", "Volodymyr", ""], ["Ermon", "Stefano", ""]]}, {"id": "1805.10583", "submitter": "Zunlei Feng", "authors": "Zunlei Feng, Xinchao Wang, Chenglong Ke, Anxiang Zeng, Dacheng Tao,\n  Mingli Song", "title": "Dual Swap Disentangling", "comments": "Accepted by NeurIPS 2018; Adding the theoretical proof for the\n  disentanglement of labeled pairs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning interpretable disentangled representations is a crucial yet\nchallenging task. In this paper, we propose a weakly semi-supervised method,\ntermed as Dual Swap Disentangling (DSD), for disentangling using both labeled\nand unlabeled data. Unlike conventional weakly supervised methods that rely on\nfull annotations on the group of samples, we require only limited annotations\non paired samples that indicate their shared attribute like the color. Our\nmodel takes the form of a dual autoencoder structure. To achieve disentangling\nusing the labeled pairs, we follow a \"encoding-swap-decoding\" process, where we\nfirst swap the parts of their encodings corresponding to the shared attribute\nand then decode the obtained hybrid codes to reconstruct the original input\npairs. For unlabeled pairs, we follow the \"encoding-swap-decoding\" process\ntwice on designated encoding parts and enforce the final outputs to approximate\nthe input pairs. By isolating parts of the encoding and swapping them back and\nforth, we impose the dimension-wise modularity and portability of the encodings\nof the unlabeled samples, which implicitly encourages disentangling under the\nguidance of labeled pairs. This dual swap mechanism, tailored for\nsemi-supervised setting, turns out to be very effective. Experiments on image\ndatasets from a wide domain show that our model yields state-of-the-art\ndisentangling performances.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 06:14:21 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 08:48:21 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 07:33:44 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Feng", "Zunlei", ""], ["Wang", "Xinchao", ""], ["Ke", "Chenglong", ""], ["Zeng", "Anxiang", ""], ["Tao", "Dacheng", ""], ["Song", "Mingli", ""]]}, {"id": "1805.10603", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Kaoru Hiramatsu, Kunio Kashino", "title": "Generative Adversarial Image Synthesis with Decision Tree Latent\n  Controller", "comments": "CVPR 2018. Project page:\n  http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/dtlc-gan/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the decision tree latent controller generative\nadversarial network (DTLC-GAN), an extension of a GAN that can learn\nhierarchically interpretable representations without relying on detailed\nsupervision. To impose a hierarchical inclusion structure on latent variables,\nwe incorporate a new architecture called the DTLC into the generator input. The\nDTLC has a multiple-layer tree structure in which the ON or OFF of the child\nnode codes is controlled by the parent node codes. By using this architecture\nhierarchically, we can obtain the latent space in which the lower layer codes\nare selectively used depending on the higher layer ones. To make the latent\ncodes capture salient semantic features of images in a hierarchically\ndisentangled manner in the DTLC, we also propose a hierarchical conditional\nmutual information regularization and optimize it with a newly defined\ncurriculum learning method that we propose as well. This makes it possible to\ndiscover hierarchically interpretable representations in a layer-by-layer\nmanner on the basis of information gain by only using a single DTLC-GAN model.\nWe evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny\nImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn\nhierarchically interpretable representations with either unsupervised or weakly\nsupervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval\ntasks and showed its effectiveness in representation learning.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 10:56:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Hiramatsu", "Kaoru", ""], ["Kashino", "Kunio", ""]]}, {"id": "1805.10604", "submitter": "Pratik Dubal", "authors": "Pratik Dubal, Rohan Mahadev, Suraj Kothawade, Kunal Dargan and Rishabh\n  Iyer", "title": "Deployment of Customized Deep Learning based Video Analytics On\n  Surveillance Cameras", "comments": "Added Equal Contribution footnote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the effectiveness of our customized deep learning\nbased video analytics system in various applications focused on security,\nsafety, customer analytics and process compliance. We describe our video\nanalytics system comprising of Search, Summarize, Statistics and real-time\nalerting, and outline its building blocks. These building blocks include object\ndetection, tracking, face detection and recognition, human and face\nsub-attribute analytics. In each case, we demonstrate how custom models trained\nusing data from the deployment scenarios provide considerably superior\naccuracies than off-the-shelf models. Towards this end, we describe our data\nprocessing and model training pipeline, which can train and fine-tune models\nfrom videos with a quick turnaround time. Finally, since most of these models\nare deployed on-site, it is important to have resource constrained models which\ndo not require GPUs. We demonstrate how we custom train resource constrained\nmodels and deploy them on embedded devices without significant loss in\naccuracy. To our knowledge, this is the first work which provides a\ncomprehensive evaluation of different deep learning models on various\nreal-world customer deployment scenarios of surveillance video analytics. By\nsharing our implementation details and the experiences learned from deploying\ncustomized deep learning models for various customers, we hope that customized\ndeep learning based video analytics is widely incorporated in commercial\nproducts around the world.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 11:01:30 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 14:04:07 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Dubal", "Pratik", ""], ["Mahadev", "Rohan", ""], ["Kothawade", "Suraj", ""], ["Dargan", "Kunal", ""], ["Iyer", "Rishabh", ""]]}, {"id": "1805.10620", "submitter": "Su Yang", "authors": "Xinfeng Zhang, Su Yang, Xinjian Zhang, Weishan Zhang, Jiulong Zhang", "title": "Anomaly Detection and Localization in Crowded Scenes by Motion-field\n  Shape Description and Similarity-based Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowded scenes, detection and localization of abnormal behaviors is\nchallenging in that high-density people make object segmentation and tracking\nextremely difficult. We associate the optical flows of multiple frames to\ncapture short-term trajectories and introduce the histogram-based shape\ndescriptor referred to as shape contexts to describe such short-term\ntrajectories. Furthermore, we propose a K-NN similarity-based statistical model\nto detect anomalies over time and space, which is an unsupervised one-class\nlearning algorithm requiring no clustering nor any prior assumption. Firstly,\nwe retrieve the K-NN samples from the training set in regard to the testing\nsample, and then use the similarities between every pair of the K-NN samples to\nconstruct a Gaussian model. Finally, the probabilities of the similarities from\nthe testing sample to the K-NN samples under the Gaussian model are calculated\nin the form of a joint probability. Abnormal events can be detected by judging\nwhether the joint probability is below predefined thresholds in terms of time\nand space, separately. Such a scheme can adapt to the whole scene, since the\nprobability computed as such is not affected by motion distortions arising from\nperspective distortion. We conduct experiments on real-world surveillance\nvideos, and the results demonstrate that the proposed method can reliably\ndetect and locate the abnormal events in the video sequences, outperforming the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 13:16:40 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Zhang", "Xinfeng", ""], ["Yang", "Su", ""], ["Zhang", "Xinjian", ""], ["Zhang", "Weishan", ""], ["Zhang", "Jiulong", ""]]}, {"id": "1805.10628", "submitter": "Wang Rui", "authors": "Rui Wang, Xiao-Jun Wu, and Josef Kittler", "title": "A Simple Riemannian Manifold Network for Image Set Classification", "comments": "There are some errors in the submitted paper. (1) Section III, part\n  B, the Equation (8) is formulated in wrong way. (2) Section III, part E, we\n  use S to represents the sum of eigenvalues in Equation (15) but it has been\n  used as the sactter matrix in Equation (17) and (18). As a result, we are\n  very sorry for this, and would like to withdraw this submitted paper to\n  carefully revise it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of image-set based classification, a considerable advance has\nbeen made by representing original image sets as covariance matrices which\ntypical lie in a Riemannian manifold. Specifically, it is a Symmetric Positive\nDefinite (SPD) manifold. Traditional manifold learning methods inevitably have\nthe property of high computational complexity or weak performance of the\nfeature representation. In order to overcome these limitations, we propose a\nvery simple Riemannian manifold network for image set classification. Inspired\nby deep learning architectures, we design a fully connected layer to generate\nmore novel, more powerful SPD matrices. However we exploit the rectifying layer\nto prevent the input SPD matrices from being singular. We also introduce a\nnon-linear learning of the proposed network with an innovative objective\nfunction. Furthermore we devise a pooling layer to further reduce the\nredundancy of the input SPD matrices, and the log-map layer to project the SPD\nmanifold to the Euclidean space. For learning the connection weights between\nthe input layer and the fully connected layer, we use Two-directional\ntwo-dimensional Principal Component Analysis ((2D)2PCA) algorithm. The proposed\nRiemannian manifold network (RieMNet) avoids complex computing and can be built\nand trained extremely easy and efficient. We have also developed a deep version\nof RieMNet, named as DRieMNet. The proposed RieMNet and DRieMNet are evaluated\non three tasks: video-based face recognition, set-based object categorization,\nand set-based cell identification. Extensive experimental results show the\nsuperiority of our method over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 14:05:47 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 13:12:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wang", "Rui", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1805.10664", "submitter": "Rick Chang", "authors": "Jen-Hao Rick Chang, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan", "title": "Towards Multifocal Displays with Dense Focal Stacks", "comments": null, "journal-ref": null, "doi": "10.1145/3272127.3275015", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a virtual reality display that is capable of generating a dense\ncollection of depth/focal planes. This is achieved by driving a focus-tunable\nlens to sweep a range of focal lengths at a high frequency and, subsequently,\ntracking the focal length precisely at microsecond time resolutions using an\noptical module. Precise tracking of the focal length, coupled with a high-speed\ndisplay, enables our lab prototype to generate 1600 focal planes per second.\nThis enables a novel first-of-its-kind virtual reality multifocal display that\nis capable of resolving the vergence-accommodation conflict endemic to today's\ndisplays.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 17:51:07 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 07:51:47 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2018 22:40:47 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Chang", "Jen-Hao Rick", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1805.10665", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Eli Gibson, Nooshin Ghavami, Ester Bonmati, Caroline M.\n  Moore, Mark Emberton, Tom Vercauteren, J. Alison Noble, Dean C. Barratt", "title": "Adversarial Deformation Regularization for Training Image Registration\n  Neural Networks", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00928-1_87", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an adversarial learning approach to constrain convolutional\nneural network training for image registration, replacing heuristic smoothness\nmeasures of displacement fields often used in these tasks. Using\nminimally-invasive prostate cancer intervention as an example application, we\ndemonstrate the feasibility of utilizing biomechanical simulations to\nregularize a weakly-supervised anatomical-label-driven registration network for\naligning pre-procedural magnetic resonance (MR) and 3D intra-procedural\ntransrectal ultrasound (TRUS) images. A discriminator network is optimized to\ndistinguish the registration-predicted displacement fields from the motion data\nsimulated by finite element analysis. During training, the registration network\nsimultaneously aims to maximize similarity between anatomical labels that\ndrives image alignment and to minimize an adversarial generator loss that\nmeasures divergence between the predicted- and simulated deformation. The\nend-to-end trained network enables efficient and fully-automated registration\nthat only requires an MR and TRUS image pair as input, without anatomical\nlabels or simulated data during inference. 108 pairs of labelled MR and TRUS\nimages from 76 prostate cancer patients and 71,500 nonlinear finite-element\nsimulations from 143 different patients were used for this study. We show that,\nwith only gland segmentation as training labels, the proposed method can help\npredict physically plausible deformation without any other smoothness penalty.\nBased on cross-validation experiments using 834 pairs of independent validation\nlandmarks, the proposed adversarial-regularized registration achieved a target\nregistration error of 6.3 mm that is significantly lower than those from\nseveral other regularization methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 17:56:51 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Hu", "Yipeng", ""], ["Gibson", "Eli", ""], ["Ghavami", "Nooshin", ""], ["Bonmati", "Ester", ""], ["Moore", "Caroline M.", ""], ["Emberton", "Mark", ""], ["Vercauteren", "Tom", ""], ["Noble", "J. Alison", ""], ["Barratt", "Dean C.", ""]]}, {"id": "1805.10704", "submitter": "Salman Ul Hassan Dar", "authors": "Salman Ul Hassan Dar, Mahmut Yurt, Mohammad Shahdloo, Muhammed\n  Emrullah Ild{\\i}z, Tolga \\c{C}ukur", "title": "Synergistic Reconstruction and Synthesis via Generative Adversarial\n  Networks for Accelerated Multi-Contrast MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-contrast MRI acquisitions of an anatomy enrich the magnitude of\ninformation available for diagnosis. Yet, excessive scan times associated with\nadditional contrasts may be a limiting factor. Two mainstream approaches for\nenhanced scan efficiency are reconstruction of undersampled acquisitions and\nsynthesis of missing acquisitions. In reconstruction, performance decreases\ntowards higher acceleration factors with diminished sampling density\nparticularly at high-spatial-frequencies. In synthesis, the absence of data\nsamples from the target contrast can lead to artefactual sensitivity or\ninsensitivity to image features. Here we propose a new approach for synergistic\nreconstruction-synthesis of multi-contrast MRI based on conditional generative\nadversarial networks. The proposed method preserves high-frequency details of\nthe target contrast by relying on the shared high-frequency information\navailable from the source contrast, and prevents feature leakage or loss by\nrelying on the undersampled acquisitions of the target contrast. Demonstrations\non brain MRI datasets from healthy subjects and patients indicate the superior\nperformance of the proposed method compared to previous state-of-the-art. The\nproposed method can help improve the quality and scan efficiency of\nmulti-contrast MRI exams.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 22:39:19 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Dar", "Salman Ul Hassan", ""], ["Yurt", "Mahmut", ""], ["Shahdloo", "Mohammad", ""], ["Ild\u0131z", "Muhammed Emrullah", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "1805.10705", "submitter": "Magnus Oskarsson", "authors": "Magnus Oskarsson", "title": "A fast minimal solver for absolute camera pose with unknown focal length\n  and radial distortion from four planar points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fast minimal solver for absolute camera pose\nestimation from four known points that lie in a plane. We assume a perspective\ncamera model with unknown focal length and unknown radial distortion. The\nradial distortion is modelled using the division model with one parameter. We\nshow that the solutions to this problem can be found from a univariate\nsix-degree polynomial. This results in a very fast and numerically stable\nsolver.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 22:53:21 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 20:18:09 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Oskarsson", "Magnus", ""]]}, {"id": "1805.10717", "submitter": "Hyunjung Shim Dr.", "authors": "Duhyeon Bang, Seoungyoon Kang, Hyunjung Shim", "title": "Discriminator Feature-based Inference by Recycling the Discriminator of\n  GANs", "comments": null, "journal-ref": "International Journal of Computer Vision 2020", "doi": "10.1007/s11263-020-01311-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs)successfully generate high quality data\nby learning amapping from a latent vector to the data. Various studies assert\nthat the latent space of a GAN is semanticallymeaningful and can be utilized\nfor advanced data analysis and manipulation. To analyze the real data in\nthelatent space of a GAN, it is necessary to build an inference mapping from\nthe data to the latent vector. Thispaper proposes an effective algorithm to\naccurately infer the latent vector by utilizing GAN discriminator features. Our\nprimary goal is to increase inference mappingaccuracy with minimal training\noverhead. Furthermore,using the proposed algorithm, we suggest a\nconditionalimage generation algorithm, namely a spatially conditioned GAN.\nExtensive evaluations confirmed that theproposed inference algorithm achieved\nmore semantically accurate inference mapping than existing methodsand can be\nsuccessfully applied to advanced conditionalimage generation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 00:22:18 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 09:16:37 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Bang", "Duhyeon", ""], ["Kang", "Seoungyoon", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1805.10720", "submitter": "Jose Dolz", "authors": "Jose Dolz, Xiaopan Xu, Jerome Rony, Jing Yuan, Yang Liu, Eric Granger,\n  Christian Desrosiers, Xi Zhang, Ismail Ben Ayed, Hongbing Lu", "title": "Multi-region segmentation of bladder cancer structures in MRI with\n  progressive dilated convolutional networks", "comments": "Published at the journal of Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13240", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise segmentation of bladder walls and tumor regions is an essential step\ntowards non-invasive identification of tumor stage and grade, which is critical\nfor treatment decision and prognosis of patients with bladder cancer (BC).\nHowever, the automatic delineation of bladder walls and tumor in magnetic\nresonance images (MRI) is a challenging task, due to important bladder shape\nvariations, strong intensity inhomogeneity in urine and very high variability\nacross population, particularly on tumors appearance. To tackle these issues,\nwe propose to use a deep fully convolutional neural network. The proposed\nnetwork includes dilated convolutions to increase the receptive field without\nincurring extra cost nor degrading its performance. Furthermore, we introduce\nprogressive dilations in each convolutional block, thereby enabling extensive\nreceptive fields without the need for large dilation rates. The proposed\nnetwork is evaluated on 3.0T T2-weighted MRI scans from 60 pathologically\nconfirmed patients with BC. Experiments shows the proposed model to achieve\nhigh accuracy, with a mean Dice similarity coefficient of 0.98, 0.84 and 0.69\nfor inner wall, outer wall and tumor region, respectively. These results\nrepresent a very good agreement with reference contours and an increase in\nperformance compared to existing methods. In addition, inference times are less\nthan a second for a whole 3D volume, which is between 2-3 orders of magnitude\nfaster than related state-of-the-art methods for this application. We showed\nthat a CNN can yield precise segmentation of bladder walls and tumors in\nbladder cancer patients on MRI. The whole segmentation process is\nfully-automatic and yields results in very good agreement with the reference\nstandard, demonstrating the viability of deep learning models for the automatic\nmulti-region segmentation of bladder cancer MRI images.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 00:28:56 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 14:33:46 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 15:30:00 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 19:54:30 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Dolz", "Jose", ""], ["Xu", "Xiaopan", ""], ["Rony", "Jerome", ""], ["Yuan", "Jing", ""], ["Liu", "Yang", ""], ["Granger", "Eric", ""], ["Desrosiers", "Christian", ""], ["Zhang", "Xi", ""], ["Ayed", "Ismail Ben", ""], ["Lu", "Hongbing", ""]]}, {"id": "1805.10726", "submitter": "Nathaniel Blanchard", "authors": "Nathaniel Blanchard, Jeffery Kinnison, Brandon RichardWebster, Pouya\n  Bashivan, Walter J. Scheirer", "title": "A Neurobiological Evaluation Metric for Neural Network Model Search", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience theory posits that the brain's visual system coarsely identifies\nbroad object categories via neural activation patterns, with similar objects\nproducing similar neural responses. Artificial neural networks also have\ninternal activation behavior in response to stimuli. We hypothesize that\nnetworks exhibiting brain-like activation behavior will demonstrate brain-like\ncharacteristics, e.g., stronger generalization capabilities. In this paper we\nintroduce a human-model similarity (HMS) metric, which quantifies the\nsimilarity of human fMRI and network activation behavior. To calculate HMS,\nrepresentational dissimilarity matrices (RDMs) are created as abstractions of\nactivation behavior, measured by the correlations of activations to stimulus\npairs. HMS is then the correlation between the fMRI RDM and the neural network\nRDM across all stimulus pairs. We test the metric on unsupervised predictive\ncoding networks, which specifically model visual perception, and assess the\nmetric for statistical significance over a large range of hyperparameters. Our\nexperiments show that networks with increased human-model similarity are\ncorrelated with better performance on two computer vision tasks: next frame\nprediction and object matching accuracy. Further, HMS identifies networks with\nhigh performance on both tasks. An unexpected secondary finding is that the\nmetric can be employed during training as an early-stopping mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 01:33:49 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 02:17:20 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 20:37:48 GMT"}, {"version": "v4", "created": "Tue, 27 Nov 2018 00:51:03 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Blanchard", "Nathaniel", ""], ["Kinnison", "Jeffery", ""], ["RichardWebster", "Brandon", ""], ["Bashivan", "Pouya", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1805.10734", "submitter": "William Lotter", "authors": "William Lotter, Gabriel Kreiman, David Cox", "title": "A neural network trained to predict future video frames mimics critical\n  properties of biological neuronal responses and perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks take loose inspiration from neuroscience, it is an\nopen question how seriously to take the analogies between artificial deep\nnetworks and biological neuronal systems. Interestingly, recent work has shown\nthat deep convolutional neural networks (CNNs) trained on large-scale image\nrecognition tasks can serve as strikingly good models for predicting the\nresponses of neurons in visual cortex to visual stimuli, suggesting that\nanalogies between artificial and biological neural networks may be more than\nsuperficial. However, while CNNs capture key properties of the average\nresponses of cortical neurons, they fail to explain other properties of these\nneurons. For one, CNNs typically require large quantities of labeled input data\nfor training. Our own brains, in contrast, rarely have access to this kind of\nsupervision, so to the extent that representations are similar between CNNs and\nbrains, this similarity must arise via different training paths. In addition,\nneurons in visual cortex produce complex time-varying responses even to static\ninputs, and they dynamically tune themselves to temporal regularities in the\nvisual environment. We argue that these differences are clues to fundamental\ndifferences between the computations performed in the brain and in deep\nnetworks. To begin to close the gap, here we study the emergent properties of a\npreviously-described recurrent generative network that is trained to predict\nfuture video frames in a self-supervised manner. Remarkably, the model is able\nto capture a wide variety of seemingly disparate phenomena observed in visual\ncortex, ranging from single unit response dynamics to complex perceptual motion\nillusions. These results suggest potentially deep connections between recurrent\npredictive neural network models and the brain, providing new leads that can\nenrich both fields.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 02:15:09 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 02:47:58 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Lotter", "William", ""], ["Kreiman", "Gabriel", ""], ["Cox", "David", ""]]}, {"id": "1805.10737", "submitter": "Ahmet Tuysuzoglu", "authors": "Ahmet Tuysuzoglu, Jeremy Tan, Kareem Eissa, Atilla P. Kiraly, Mamadou\n  Diallo, Ali Kamen", "title": "Deep Adversarial Context-Aware Landmark Detection for Ultrasound Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time localization of prostate gland in trans-rectal ultrasound images is\na key technology that is required to automate the ultrasound guided prostate\nbiopsy procedures. In this paper, we propose a new deep learning based approach\nwhich is aimed at localizing several prostate landmarks efficiently and\nrobustly. We propose a multitask learning approach primarily to make the\noverall algorithm more contextually aware. In this approach, we not only\nconsider the explicit learning of landmark locations, but also build-in a\nmechanism to learn the contour of the prostate. This multitask learning is\nfurther coupled with an adversarial arm to promote the generation of feasible\nstructures. We have trained this network using ~4000 labeled trans-rectal\nultrasound images and tested on an independent set of images with ground truth\nlandmark locations. We have achieved an overall Dice score of 92.6% for the\nadversarially trained multitask approach, which is significantly better than\nthe Dice score of 88.3% obtained by only learning of landmark locations. The\noverall mean distance error using the adversarial multitask approach has also\nimproved by 20% while reducing the standard deviation of the error compared to\nlearning landmark locations only. In terms of computational complexity both\napproaches can process the images in real-time using standard computer with a\nstandard CUDA enabled GPU.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 02:27:32 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Tuysuzoglu", "Ahmet", ""], ["Tan", "Jeremy", ""], ["Eissa", "Kareem", ""], ["Kiraly", "Atilla P.", ""], ["Diallo", "Mamadou", ""], ["Kamen", "Ali", ""]]}, {"id": "1805.10765", "submitter": "Nuri Kim", "authors": "Nuri Kim, Donghoon Lee, Songhwai Oh", "title": "Learning Instance-Aware Object Detection Using Determinantal Point\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent object detectors find instances while categorizing candidate regions.\nAs each region is evaluated independently, the number of candidate regions from\na detector is usually larger than the number of objects. Since the final goal\nof detection is to assign a single detection to each object, a heuristic\nalgorithm, such as non-maximum suppression (NMS), is used to select a single\nbounding box for an object. While simple heuristic algorithms are effective for\nstand-alone objects, they can fail to detect overlapped objects. In this paper,\nwe address this issue by training a network to distinguish different objects\nusing the relationship between candidate boxes. We propose an instance-aware\ndetection network (IDNet), which can learn to extract features from candidate\nregions and measure their similarities. Based on pairwise similarities and\ndetection qualities, the IDNet selects a subset of candidate bounding boxes\nusing instance-aware determinantal point process inference (IDPP). Extensive\nexperiments demonstrate that the proposed algorithm achieves significant\nimprovements for detecting overlapped objects compared to existing\nstate-of-the-art detection methods on the PASCAL VOC and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 04:25:33 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 08:18:19 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 02:14:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kim", "Nuri", ""], ["Lee", "Donghoon", ""], ["Oh", "Songhwai", ""]]}, {"id": "1805.10766", "submitter": "Shayan Sadigh", "authors": "Shayan Sadigh, Pradeep Sen", "title": "Improving the Resolution of CNN Feature Maps Efficiently with\n  Multisampling", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new class of subsampling techniques for CNNs, termed\nmultisampling, that significantly increases the amount of information kept by\nfeature maps through subsampling layers. One version of our method, which we\ncall checkered subsampling, significantly improves the accuracy of\nstate-of-the-art architectures such as DenseNet and ResNet without any\nadditional parameters and, remarkably, improves the accuracy of certain\npretrained ImageNet models without any training or fine-tuning. We glean new\ninsight into the nature of data augmentations and demonstrate, for the first\ntime, that coarse feature maps are significantly bottlenecking the performance\nof neural networks in image classification.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 04:29:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Sadigh", "Shayan", ""], ["Sen", "Pradeep", ""]]}, {"id": "1805.10777", "submitter": "Liangqu Long", "authors": "Liangqu Long, Wei Wang, Jun Wen, Meihui Zhang, Qian Lin, Beng Chin Ooi", "title": "Object-Level Representation Learning for Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning that trains image classifiers over few labeled examples per\ncategory is a challenging task. In this paper, we propose to exploit an\nadditional big dataset with different categories to improve the accuracy of\nfew-shot learning over our target dataset. Our approach is based on the\nobservation that images can be decomposed into objects, which may appear in\nimages from both the additional dataset and our target dataset. We use the\nobject-level relation learned from the additional dataset to infer the\nsimilarity of images in our target dataset with unseen categories. Nearest\nneighbor search is applied to do image classification, which is a\nnon-parametric model and thus does not need fine-tuning. We evaluate our\nalgorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain\n8.5\\% and 2.7\\% absolute improvements for 5-way 1-shot and 5-way 5-shot\nexperiments on MiniImagenet, respectively. Source code will be published upon\nacceptance.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 05:46:17 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Long", "Liangqu", ""], ["Wang", "Wei", ""], ["Wen", "Jun", ""], ["Zhang", "Meihui", ""], ["Lin", "Qian", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1805.10784", "submitter": "Hyo-Eun Kim", "authors": "Hyo-Eun Kim, Seungwook Kim, Jaehwan Lee", "title": "Keep and Learn: Continual Learning by Constraining the Latent Space for\n  Knowledge Preservation in Neural Networks", "comments": "accepted for presentation at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is one of the most important factors in machine learning. However, even\nif we have high-quality data, there is a situation in which access to the data\nis restricted. For example, access to the medical data from outside is strictly\nlimited due to the privacy issues. In this case, we have to learn a model\nsequentially only with the data accessible in the corresponding stage. In this\nwork, we propose a new method for preserving learned knowledge by modeling the\nhigh-level feature space and the output space to be mutually informative, and\nconstraining feature vectors to lie in the modeled space during training. The\nproposed method is easy to implement as it can be applied by simply adding a\nreconstruction loss to an objective function. We evaluate the proposed method\non CIFAR-10/100 and a chest X-ray dataset, and show benefits in terms of\nknowledge preservation compared to previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 06:26:58 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Kim", "Hyo-Eun", ""], ["Kim", "Seungwook", ""], ["Lee", "Jaehwan", ""]]}, {"id": "1805.10790", "submitter": "Cheng-Bin Jin", "authors": "Cheng-Bin Jin, Hakil Kim, Wonmo Jung, Seongsu Joo, Ensik Park, Ahn\n  Young Saem, In Ho Han, Jae Il Lee, and Xuenan Cui", "title": "Deep CT to MR Synthesis using Paired and Unpaired Data", "comments": "23 pages, 7 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MR imaging will play a very important role in radiotherapy treatment planning\nfor segmentation of tumor volumes and organs. However, the use of MR-based\nradiotherapy is limited because of the high cost and the increased use of metal\nimplants such as cardiac pacemakers and artificial joints in aging society. To\nimprove the accuracy of CT-based radiotherapy planning, we propose a synthetic\napproach that translates a CT image into an MR image using paired and unpaired\ntraining data. In contrast to the current synthetic methods for medical images,\nwhich depend on sparse pairwise-aligned data or plentiful unpaired data, the\nproposed approach alleviates the rigid registration challenge of paired\ntraining and overcomes the context-misalignment problem of the unpaired\ntraining. A generative adversarial network was trained to transform 2D brain CT\nimage slices into 2D brain MR image slices, combining adversarial loss, dual\ncycle-consistent loss, and voxel-wise loss. The experiments were analyzed using\nCT and MR images of 202 patients. Qualitative and quantitative comparisons\nagainst independent paired training and unpaired training methods demonstrate\nthe superiority of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:14:05 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 13:33:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jin", "Cheng-Bin", ""], ["Kim", "Hakil", ""], ["Jung", "Wonmo", ""], ["Joo", "Seongsu", ""], ["Park", "Ensik", ""], ["Saem", "Ahn Young", ""], ["Han", "In Ho", ""], ["Lee", "Jae Il", ""], ["Cui", "Xuenan", ""]]}, {"id": "1805.10795", "submitter": "Elad Tzoreff", "authors": "Elad Tzoreff, Olga Kogan and Yoni Choukroun", "title": "Deep Discriminative Latent Space for Clustering", "comments": "A version of this paper has been submitted to NIPS 2018. The paper\n  contains 9 pages including references, and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most fundamental tasks in data analysis and machine\nlearning. It is central to many data-driven applications that aim to separate\nthe data into groups with similar patterns. Moreover, clustering is a complex\nprocedure that is affected significantly by the choice of the data\nrepresentation method. Recent research has demonstrated encouraging clustering\nresults by learning effectively these representations. In most of these works a\ndeep auto-encoder is initially pre-trained to minimize a reconstruction loss,\nand then jointly optimized with clustering centroids in order to improve the\nclustering objective. Those works focus mainly on the clustering phase of the\nprocedure, while not utilizing the potential benefit out of the initial phase.\nIn this paper we propose to optimize an auto-encoder with respect to a\ndiscriminative pairwise loss function during the auto-encoder pre-training\nphase. We demonstrate the high accuracy obtained by the proposed method as well\nas its rapid convergence (e.g. reaching above 92% accuracy on MNIST during the\npre-training phase, in less than 50 epochs), even with small networks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:34:14 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Tzoreff", "Elad", ""], ["Kogan", "Olga", ""], ["Choukroun", "Yoni", ""]]}, {"id": "1805.10802", "submitter": "Fran\\c{c}ois Plesse", "authors": "Fran\\c{c}ois Plesse, Alexandru Ginsca, Bertrand Delezoide,\n  Fran\\c{c}oise Pr\\^eteux", "title": "Visual Relationship Detection Based on Guided Proposals and Semantic\n  Knowledge Distillation", "comments": "Accepted submission to ICME 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A thorough comprehension of image content demands a complex grasp of the\ninteractions that may occur in the natural world. One of the key issues is to\ndescribe the visual relationships between objects. When dealing with real world\ndata, capturing these very diverse interactions is a difficult problem. It can\nbe alleviated by incorporating common sense in a network. For this, we propose\na framework that makes use of semantic knowledge and estimates the relevance of\nobject pairs during both training and test phases. Extracted from precomputed\nmodels and training annotations, this information is distilled into the neural\nnetwork dedicated to this task. Using this approach, we observe a significant\nimprovement on all classes of Visual Genome, a challenging visual relationship\ndataset. A 68.5% relative gain on the recall at 100 is directly related to the\nrelevance estimate and a 32.7% gain to the knowledge distillation.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:59:28 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Plesse", "Fran\u00e7ois", ""], ["Ginsca", "Alexandru", ""], ["Delezoide", "Bertrand", ""], ["Pr\u00eateux", "Fran\u00e7oise", ""]]}, {"id": "1805.10807", "submitter": "Suofei Zhang", "authors": "Suofei Zhang, Wei Zhao, Xiaofu Wu, Quan Zhou", "title": "Fast Dynamic Routing Based on Weighted Kernel Density Estimation", "comments": "16 pages, 4 figures, submitted to eccv 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsules as well as dynamic routing between them are most recently proposed\nstructures for deep neural networks. A capsule groups data into vectors or\nmatrices as poses rather than conventional scalars to represent specific\nproperties of target instance. Besides of pose, a capsule should be attached\nwith a probability (often denoted as activation) for its presence. The dynamic\nrouting helps capsules achieve more generalization capacity with many fewer\nmodel parameters. However, the bottleneck that prevents widespread applications\nof capsule is the expense of computation during routing. To address this\nproblem, we generalize existing routing methods within the framework of\nweighted kernel density estimation, and propose two fast routing methods with\ndifferent optimization strategies. Our methods prompt the time efficiency of\nrouting by nearly 40\\% with negligible performance degradation. By stacking a\nhybrid of convolutional layers and capsule layers, we construct a network\narchitecture to handle inputs at a resolution of $64\\times{64}$ pixels. The\nproposed models achieve a parallel performance with other leading methods in\nmultiple benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 08:18:31 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 03:14:29 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zhang", "Suofei", ""], ["Zhao", "Wei", ""], ["Wu", "Xiaofu", ""], ["Zhou", "Quan", ""]]}, {"id": "1805.10853", "submitter": "Rudresh Dwivedi", "authors": "Rudresh Dwivedi and Somnath Dey", "title": "A non-invertible cancelable fingerprint template generation based on\n  ridge feature transformation", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.27.5.053031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a biometric verification system, leakage of biometric data leads to\npermanent identity loss since original biometric data is inherently linked to a\nuser. Further, various types of attacks on a biometric system may reveal the\noriginal template and utility in other applications. To address these security\nand privacy concerns cancelable biometric has been introduced. Cancelable\nbiometric constructs a protected template from the original biometric template\nusing transformation functions and performs the comparison between templates in\nthe transformed domain. Recent approaches towards cancelable fingerprint\ngeneration either rely on aligning minutiae points with respect to singular\npoints (core/delta) or utilize the absolute coordinate positions of minutiae\npoints. In this paper, we propose a novel non-invertible ridge feature\ntransformation method to protect the original fingerprint template information.\nThe proposed method partitions the fingerprint region into a number of sectors\nwith reference to each minutia point employing a ridge-based co-ordinate\nsystem. The nearest neighbor minutiae in each sector are identified, and\nridge-based features are computed. Further, a cancelable template is generated\nby applying the Cantor pairing function followed by random projection. We have\nevaluated our method with FVC2002, FVC2004 and FVC2006 databases. It is evident\nfrom the experimental results that the proposed method outperforms existing\nmethods in the literature. Moreover, the security analysis demonstrates that\nthe proposed method fulfills the necessary requirements of non-invertibility,\nrevocability, and diversity with a minor performance degradation caused due to\ncancelable transformation.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 10:27:47 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Dwivedi", "Rudresh", ""], ["Dey", "Somnath", ""]]}, {"id": "1805.10863", "submitter": "Patrick McClure", "authors": "Patrick McClure, Charles Y. Zheng, Jakub R. Kaczmarzyk, John A. Lee,\n  Satrajit S. Ghosh, Dylan Nielson, Peter Bandettini, and Francisco Pereira", "title": "Distributed Weight Consolidation: A Brain Segmentation Case Study", "comments": "Published in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting the large datasets needed to train deep neural networks can be\nvery difficult, particularly for the many applications for which sharing and\npooling data is complicated by practical, ethical, or legal concerns. However,\nit may be the case that derivative datasets or predictive models developed\nwithin individual sites can be shared and combined with fewer restrictions.\nTraining on distributed data and combining the resulting networks is often\nviewed as continual learning, but these methods require networks to be trained\nsequentially. In this paper, we introduce distributed weight consolidation\n(DWC), a continual learning method to consolidate the weights of separate\nneural networks, each trained on an independent dataset. We evaluated DWC with\na brain segmentation case study, where we consolidated dilated convolutional\nneural networks trained on independent structural magnetic resonance imaging\n(sMRI) datasets from different sites. We found that DWC led to increased\nperformance on test sets from the different sites, while maintaining\ngeneralization performance for a very large and completely independent\nmulti-site dataset, compared to an ensemble baseline.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 10:50:11 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 18:03:02 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 18:19:22 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 19:12:31 GMT"}, {"version": "v5", "created": "Mon, 26 Nov 2018 21:07:14 GMT"}, {"version": "v6", "created": "Fri, 7 Dec 2018 16:11:55 GMT"}, {"version": "v7", "created": "Mon, 17 Dec 2018 03:18:45 GMT"}, {"version": "v8", "created": "Tue, 18 Dec 2018 18:58:45 GMT"}, {"version": "v9", "created": "Wed, 16 Jan 2019 11:37:26 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["McClure", "Patrick", ""], ["Zheng", "Charles Y.", ""], ["Kaczmarzyk", "Jakub R.", ""], ["Lee", "John A.", ""], ["Ghosh", "Satrajit S.", ""], ["Nielson", "Dylan", ""], ["Bandettini", "Peter", ""], ["Pereira", "Francisco", ""]]}, {"id": "1805.10864", "submitter": "Shabab Bazrafkan", "authors": "Shabab Bazrafkan, Peter Corcoran", "title": "Versatile Auxiliary Regressor with Generative Adversarial network\n  (VAR+GAN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to generate constrained samples is one of the most appealing\napplications of the deep generators. Conditional generators are one of the\nsuccessful implementations of such models wherein the created samples are\nconstrained to a specific class. In this work, the application of these\nnetworks is extended to regression problems wherein the conditional generator\nis restrained to any continuous aspect of the data. A new loss function is\npresented for the regression network and also implementations for generating\nfaces with any particular set of landmarks is provided.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 10:55:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Bazrafkan", "Shabab", ""], ["Corcoran", "Peter", ""]]}, {"id": "1805.10871", "submitter": "Xiao Liu", "authors": "Xiao Liu, Shengchuan Zhang, Hong Liu, Xin Liu, Cheng Deng, Rongrong Ji", "title": "CerfGAN: A Compact, Effective, Robust, and Fast Model for Unsupervised\n  Multi-Domain Image-to-Image Translation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at solving the multi-domain image-to-image translation\nproblem with a unified model in an unsupervised manner. The most successful\nwork in this area refers to StarGAN, which works well in tasks like face\nattribute modulation. However, StarGAN is unable to match multiple translation\nmappings when encountering general translations with very diverse domain\nshifts. On the other hand, StarGAN adopts an Encoder-Decoder-Discriminator\n(EDD) architecture, where the model is time-consuming and unstable to train. To\nthis end, we propose a Compact, effective, robust, and fast GAN model, termed\nCerfGAN, to solve the above problem. In principle, CerfGAN contains a novel\ncomponent, i.e., a multi-class discriminator (MCD), which gives the model an\nextremely powerful ability to match multiple translation mappings. To stabilize\nthe training process, MCD also plays a role of the encoder in CerfGAN, which\nsaves a lot of computation and memory costs. We perform extensive experiments\nto verify the effectiveness of the proposed method. Quantitatively, CerfGAN is\ndemonstrated to handle a serial of image-to-image translation tasks including\nstyle transfer, season transfer, face hallucination, etc, where the input\nimages are sampled from diverse domains. The comparisons to several recently\nproposed approaches demonstrate the superiority and novelty of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 11:29:23 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 20:44:54 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Liu", "Xiao", ""], ["Zhang", "Shengchuan", ""], ["Liu", "Hong", ""], ["Liu", "Xin", ""], ["Deng", "Cheng", ""], ["Ji", "Rongrong", ""]]}, {"id": "1805.10881", "submitter": "Namhyuk Ahn", "authors": "Namhyuk Ahn and Byungkon Kang and Kyung-Ah Sohn", "title": "Image Distortion Detection using Convolutional Neural Network", "comments": "Accepted to ACPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image distortion classification and detection is an important task in many\napplications. For example when compressing images, if we know the exact\nlocation of the distortion, then it is possible to re-compress images by\nadjusting the local compression level dynamically. In this paper, we address\nthe problem of detecting the distortion region and classifying the distortion\ntype of a given image. We show that our model significantly outperforms the\nstate-of-the-art distortion classifier, and report accurate detection results\nfor the first time. We expect that such results prove the usefulness of our\napproach in many potential applications such as image compression or distortion\nrestoration.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 12:04:53 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ahn", "Namhyuk", ""], ["Kang", "Byungkon", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "1805.10884", "submitter": "Gabriel Maicas", "authors": "Gabriel Maicas, Andrew P. Bradley, Jacinto C. Nascimento, Ian Reid,\n  Gustavo Carneiro", "title": "Training Medical Image Analysis Systems like Radiologists", "comments": "Oral Presentation at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of medical image analysis systems using machine learning\napproaches follows a common script: collect and annotate a large dataset, train\nthe classifier on the training set, and test it on a hold-out test set. This\nprocess bears no direct resemblance with radiologist training, which is based\non solving a series of tasks of increasing difficulty, where each task involves\nthe use of significantly smaller datasets than those used in machine learning.\nIn this paper, we propose a novel training approach inspired by how\nradiologists are trained. In particular, we explore the use of meta-training\nthat models a classifier based on a series of tasks. Tasks are selected using\nteacher-student curriculum learning, where each task consists of simple\nclassification problems containing small training sets. We hypothesize that our\nproposed meta-training approach can be used to pre-train medical image analysis\nmodels. This hypothesis is tested on the automatic breast screening\nclassification from DCE-MRI trained with weakly labeled datasets. The\nclassification performance achieved by our approach is shown to be the best in\nthe field for that application, compared to state of art baseline approaches:\nDenseNet, multiple instance learning and multi-task learning.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 12:11:03 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 12:42:04 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 05:12:29 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Maicas", "Gabriel", ""], ["Bradley", "Andrew P.", ""], ["Nascimento", "Jacinto C.", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1805.10916", "submitter": "Young-chul Yoon", "authors": "Young-chul Yoon, Abhijeet Boragule, Young-min Song, Kwangjin Yoon,\n  Moongu Jeon", "title": "Online Multi-Object Tracking with Historical Appearance Matching and\n  Scene Adaptive Detection Filtering", "comments": "Accepted to IEEE International Conference on Advanced Video and\n  Signal-based Surveillance(AVSS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the methods to handle temporal errors during\nmulti-object tracking. Temporal error occurs when objects are occluded or noisy\ndetections appear near the object. In those situations, tracking may fail and\nvarious errors like drift or ID-switching occur. It is hard to overcome\ntemporal errors only by using motion and shape information. So, we propose the\nhistorical appearance matching method and joint-input siamese network which was\ntrained by 2-step process. It can prevent tracking failures although objects\nare temporally occluded or last matching information is unreliable. We also\nprovide useful technique to remove noisy detections effectively according to\nscene condition. Tracking performance, especially identity consistency, is\nhighly improved by attaching our methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 13:36:09 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 15:57:11 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 06:32:46 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 12:31:13 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Yoon", "Young-chul", ""], ["Boragule", "Abhijeet", ""], ["Song", "Young-min", ""], ["Yoon", "Kwangjin", ""], ["Jeon", "Moongu", ""]]}, {"id": "1805.10938", "submitter": "Klemen Grm", "authors": "Klemen Grm and Simon Dobri\\v{s}ek and Walter J. Scheirer and Vitomir\n  \\v{S}truc", "title": "Face hallucination using cascaded super-resolution and identity priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we address the problem of hallucinating high-resolution facial\nimages from unaligned low-resolution inputs at high magnification factors. We\napproach the problem with convolutional neural networks (CNNs) and propose a\nnovel (deep) face hallucination model that incorporates identity priors into\nthe learning procedure. The model consists of two main parts: i) a cascaded\nsuper-resolution network that upscales the low-resolution images, and ii) an\nensemble of face recognition models that act as identity priors for the\nsuper-resolution network during training. Different from competing\nsuper-resolution approaches that typically rely on a single model for upscaling\n(even with large magnification factors), our network uses a cascade of multiple\nSR models that progressively upscale the low-resolution images using steps of\n$2\\times$. This characteristic allows us to apply supervision signals (target\nappearances) at different resolutions and incorporate identity constraints at\nmultiple-scales. Our model is able to upscale (very) low-resolution images\ncaptured in unconstrained conditions and produce visually convincing results.\nWe rigorously evaluate the proposed model on a large datasets of facial images\nand report superior performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 14:25:31 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 07:59:44 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Grm", "Klemen", ""], ["Dobri\u0161ek", "Simon", ""], ["Scheirer", "Walter J.", ""], ["\u0160truc", "Vitomir", ""]]}, {"id": "1805.10949", "submitter": "Lucas Alexandre Ramos Mr", "authors": "Lucas Alexandre Ramos, Aparecido Nilceu Marana", "title": "Fusion of Methods Based on Minutiae, Ridges and Pores for Robust\n  Fingerprint Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of physical and behavioral characteristics for human identification\nis known as biometrics. Among the many biometrics traits available, the\nfingerprint is the most widely used. The fingerprint identification is based on\nthe impression patterns, as the pattern of ridges and minutiae, characteristics\nof first and second levels respectively. The current identification systems use\nthese two levels of fingerprint features due to the low cost of the sensors.\nHowever, due the recent advances in sensor technology, it is possible to use\nthird level features present within the ridges, such as the perspiration pores.\nRecent studies have shown that the use of third-level features can increase\nsecurity and fraud protection in biometric systems, since they are difficult to\nreproduce. In addition, recent researches have also focused on multibiometrics\nrecognition due to its many advantages. The goal of this work was to apply\nfusion techniques for fingerprint recognition in order to combine minutiae,\nridges and pore-based methods and, thus, provide more robust biometrics\nrecognition systems. We evaluated isotropic-based and adaptive-based automatic\npore extraction methods and the fusion of pore-based method with the\nidentification methods based on minutiae and ridges. The experiments were\nperformed on the public database PolyU HRF and showed a reduction of\napproximately 16% in the Equal Error Rate compared to the best results obtained\nby the methods individually.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 14:39:47 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ramos", "Lucas Alexandre", ""], ["Marana", "Aparecido Nilceu", ""]]}, {"id": "1805.10973", "submitter": "Taehyeong Kim", "authors": "Taehyeong Kim, Min-Oh Heo, Seonil Son, Kyoung-Wha Park, Byoung-Tak\n  Zhang", "title": "GLAC Net: GLocal Attention Cascading Networks for Multi-image Cued Story\n  Generation", "comments": "6 pages, 3 figures, paper for Visual Storytelling Challenge in\n  Storytelling Workshop co-located with NAACL 2018, source code and pre-trained\n  models are available at https://github.com/tkim-snu/GLACNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multi-image cued story generation, such as visual storytelling\ndataset (VIST) challenge, is to compose multiple coherent sentences from a\ngiven sequence of images. The main difficulty is how to generate image-specific\nsentences within the context of overall images. Here we propose a deep learning\nnetwork model, GLAC Net, that generates visual stories by combining\nglobal-local (glocal) attention and context cascading mechanisms. The model\nincorporates two levels of attention, i.e., overall encoding level and image\nfeature level, to construct image-dependent sentences. While standard attention\nconfiguration needs a large number of parameters, the GLAC Net implements them\nin a very simple way via hard connections from the outputs of encoders or image\nfeatures onto the sentence generators. The coherency of the generated story is\nfurther improved by conveying (cascading) the information of the previous\nsentence to the next sentence serially. We evaluate the performance of the GLAC\nNet on the visual storytelling dataset (VIST) and achieve very competitive\nresults compared to the state-of-the-art techniques. Our code and pre-trained\nmodels are available here.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 15:30:21 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 09:17:46 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 05:27:28 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Kim", "Taehyeong", ""], ["Heo", "Min-Oh", ""], ["Son", "Seonil", ""], ["Park", "Kyoung-Wha", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1805.10994", "submitter": "Marcin Dymczyk", "authors": "Marcin Dymczyk, Marius Fehr, Thomas Schneider and Roland Siegwart", "title": "Long-term Large-scale Mapping and Localization Using maplab", "comments": "Workshop on Long-term autonomy and deployment of intelligent robots\n  in the real-world, ICRA 2018, Brisbane, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a large-scale and long-term mapping and localization\nscenario using the maplab open-source framework. We present a brief overview of\nthe specific algorithms in the system that enable building a consistent map\nfrom multiple sessions. We then demonstrate that such a map can be reused even\na few months later for efficient 6-DoF localization and also new trajectories\ncan be registered within the existing 3D model. The datasets presented in this\npaper are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 15:57:32 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Dymczyk", "Marcin", ""], ["Fehr", "Marius", ""], ["Schneider", "Thomas", ""], ["Siegwart", "Roland", ""]]}, {"id": "1805.10997", "submitter": "I-Jeng Wang", "authors": "Wojciech Czaja, Neil Fendley, Michael Pekala, Christopher Ratto,\n  I-Jeng Wang", "title": "Adversarial Examples in Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers attacks against machine learning algorithms used in\nremote sensing applications, a domain that presents a suite of challenges that\nare not fully addressed by current research focused on natural image data such\nas ImageNet. In particular, we present a new study of adversarial examples in\nthe context of satellite image classification problems. Using a recently\ncurated data set and associated classifier, we provide a preliminary analysis\nof adversarial examples in settings where the targeted classifier is permitted\nmultiple observations of the same location over time. While our experiments to\ndate are purely digital, our problem setup explicitly incorporates a number of\npractical considerations that a real-world attacker would need to take into\naccount when mounting a physical attack. We hope this work provides a useful\nstarting point for future studies of potential vulnerabilities in this setting.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 16:01:05 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Czaja", "Wojciech", ""], ["Fendley", "Neil", ""], ["Pekala", "Michael", ""], ["Ratto", "Christopher", ""], ["Wang", "I-Jeng", ""]]}, {"id": "1805.11090", "submitter": "Moustafa Alzantot", "authors": "Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang,\n  Cho-Jui Hsieh, Mani Srivastava", "title": "GenAttack: Practical Black-box Attacks with Gradient-Free Optimization", "comments": "Accepted in The Genetic and Evolutionary Computation Conference\n  (GECCO) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, even in the\nblack-box setting, where the attacker is restricted solely to query access.\nExisting black-box approaches to generating adversarial examples typically\nrequire a significant number of queries, either for training a substitute\nnetwork or performing gradient estimation. We introduce GenAttack, a\ngradient-free optimization technique that uses genetic algorithms for\nsynthesizing adversarial examples in the black-box setting. Our experiments on\ndifferent datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can\nsuccessfully generate visually imperceptible adversarial examples against\nstate-of-the-art image recognition models with orders of magnitude fewer\nqueries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack\nrequired roughly 2,126 and 2,568 times fewer queries respectively, than ZOO,\nthe prior state-of-the-art black-box attack. In order to scale up the attack to\nlarge-scale high-dimensional ImageNet models, we perform a series of\noptimizations that further improve the query efficiency of our attack leading\nto 237 times fewer queries against the Inception-v3 model than ZOO.\nFurthermore, we show that GenAttack can successfully attack some\nstate-of-the-art ImageNet defenses, including ensemble adversarial training and\nnon-differentiable or randomized input transformations. Our results suggest\nthat evolutionary algorithms open up a promising area of research into\neffective black-box attacks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 06:40:55 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 08:08:31 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 00:32:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Alzantot", "Moustafa", ""], ["Sharma", "Yash", ""], ["Chakraborty", "Supriyo", ""], ["Zhang", "Huan", ""], ["Hsieh", "Cho-Jui", ""], ["Srivastava", "Mani", ""]]}, {"id": "1805.11091", "submitter": "Danial Maleki", "authors": "Danial Maleki, Soheila Nadalian, Mohammad Mahdi Derakhshani, Mohammad\n  Amin Sadeghi", "title": "BlockCNN: A Deep Network for Artifact Removal and Image Compression", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2018, pp. 2555-2558", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a general technique that performs both artifact removal and image\ncompression. For artifact removal, we input a JPEG image and try to remove its\ncompression artifacts. For compression, we input an image and process its 8 by\n8 blocks in a sequence. For each block, we first try to predict its intensities\nbased on previous blocks; then, we store a residual with respect to the input\nimage. Our technique reuses JPEG's legacy compression and decompression\nroutines. Both our artifact removal and our image compression techniques use\nthe same deep network, but with different training weights. Our technique is\nsimple and fast and it significantly improves the performance of artifact\nremoval and image compression.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 17:29:48 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Maleki", "Danial", ""], ["Nadalian", "Soheila", ""], ["Derakhshani", "Mohammad Mahdi", ""], ["Sadeghi", "Mohammad Amin", ""]]}, {"id": "1805.11119", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Elisa Ricci, Barbara Caputo, Samuel Rota Bul\\`o", "title": "Adding New Tasks to a Single Network with Weight Transformations using\n  Binary Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition algorithms are required today to exhibit adaptive\nabilities. Given a deep model trained on a specific, given task, it would be\nhighly desirable to be able to adapt incrementally to new tasks, preserving\nscalability as the number of new tasks increases, while at the same time\navoiding catastrophic forgetting issues. Recent work has shown that masking the\ninternal weights of a given original conv-net through learned binary variables\nis a promising strategy. We build upon this intuition and take into account\nmore elaborated affine transformations of the convolutional weights that\ninclude learned binary masks. We show that with our generalization it is\npossible to achieve significantly higher levels of adaptation to new tasks,\nenabling the approach to compete with fine tuning strategies by requiring\nslightly more than 1 bit per network parameter per additional task. Experiments\non two popular benchmarks showcase the power of our approach, that achieves the\nnew state of the art on the Visual Decathlon Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 18:22:42 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 17:26:08 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""], ["Bul\u00f2", "Samuel Rota", ""]]}, {"id": "1805.11123", "submitter": "Shubhra Aich", "authors": "Shubhra Aich and Ian Stavness", "title": "Global Sum Pooling: A Generalization Trick for Object Counting with\n  Small Datasets of Large Images", "comments": "CVPR 2019 Deep Vision Workshop\n  (https://sites.google.com/view/deepvision2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the problem of training one-look regression models\nfor counting objects in datasets comprising a small number of high-resolution,\nvariable-shaped images. We illustrate that conventional global average pooling\n(GAP) based models are unreliable due to the patchwise cancellation of true\noverestimates and underestimates for patchwise inference. To overcome this\nlimitation and reduce overfitting caused by the training on full-resolution\nimages, we propose to employ global sum pooling (GSP) instead of GAP or fully\nconnected (FC) layers at the backend of a convolutional network. Although\ncomputationally equivalent to GAP, we show through comprehensive\nexperimentation that GSP allows convolutional networks to learn the counting\ntask as a simple linear mapping problem generalized over the input shape and\nthe number of objects present. This generalization capability allows GSP to\navoid both patchwise cancellation and overfitting by training on small patches\nand inference on full-resolution images as a whole. We evaluate our approach on\nfour different aerial image datasets - two car counting datasets (CARPK and\nCOWC), one crowd counting dataset (ShanghaiTech; parts A and B) and one new\nchallenging dataset for wheat spike counting. Our GSP models improve upon the\nstate-of-the-art approaches on all four datasets with a simple architecture.\nAlso, GSP architectures trained with smaller-sized image patches exhibit better\nlocalization property due to their focus on learning from smaller regions while\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 18:33:37 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 04:06:21 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Aich", "Shubhra", ""], ["Stavness", "Ian", ""]]}, {"id": "1805.11145", "submitter": "Liqian Ma", "authors": "Liqian Ma, Xu Jia, Stamatios Georgoulis, Tinne Tuytelaars, Luc Van\n  Gool", "title": "Exemplar Guided Unsupervised Image-to-Image Translation with Semantic\n  Consistency", "comments": "To appear in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image-to-image translation has recently received significant attention due to\nadvances in deep learning. Most works focus on learning either a one-to-one\nmapping in an unsupervised way or a many-to-many mapping in a supervised way.\nHowever, a more practical setting is many-to-many mapping in an unsupervised\nway, which is harder due to the lack of supervision and the complex inner- and\ncross-domain variations. To alleviate these issues, we propose the Exemplar\nGuided & Semantically Consistent Image-to-image Translation (EGSC-IT) network\nwhich conditions the translation process on an exemplar image in the target\ndomain. We assume that an image comprises of a content component which is\nshared across domains, and a style component specific to each domain. Under the\nguidance of an exemplar from the target domain we apply Adaptive Instance\nNormalization to the shared content component, which allows us to transfer the\nstyle information of the target domain to the source domain. To avoid semantic\ninconsistencies during translation that naturally appear due to the large\ninner- and cross-domain variations, we introduce the concept of feature masks\nthat provide coarse semantic guidance without requiring the use of any semantic\nlabels. Experimental results on various datasets show that EGSC-IT does not\nonly translate the source image to diverse instances in the target domain, but\nalso preserves the semantic consistency during the process.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 19:47:07 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 08:48:33 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 10:37:27 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2019 18:30:30 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Ma", "Liqian", ""], ["Jia", "Xu", ""], ["Georgoulis", "Stamatios", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1805.11155", "submitter": "Daan Wynen", "authors": "Daan Wynen, Cordelia Schmid, Julien Mairal", "title": "Unsupervised Learning of Artistic Styles with Archetypal Style Analysis", "comments": "Accepted at NIPS 2018, Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an unsupervised learning approach to\nautomatically discover, summarize, and manipulate artistic styles from large\ncollections of paintings. Our method is based on archetypal analysis, which is\nan unsupervised learning technique akin to sparse coding with a geometric\ninterpretation. When applied to deep image representations from a collection of\nartworks, it learns a dictionary of archetypal styles, which can be easily\nvisualized. After training the model, the style of a new image, which is\ncharacterized by local statistics of deep visual features, is approximated by a\nsparse convex combination of archetypes. This enables us to interpret which\narchetypal styles are present in the input image, and in which proportion.\nFinally, our approach allows us to manipulate the coefficients of the latent\narchetypal decomposition, and achieve various special effects such as style\nenhancement, transfer, and interpolation between multiple archetypes.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 19:58:01 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:59:02 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Wynen", "Daan", ""], ["Schmid", "Cordelia", ""], ["Mairal", "Julien", ""]]}, {"id": "1805.11161", "submitter": "Noam Mor", "authors": "Noam Mor, Lior Wolf", "title": "Confidence Prediction for Lexicon-Free OCR", "comments": null, "journal-ref": null, "doi": "10.1109/WACV.2018.00030", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having a reliable accuracy score is crucial for real world applications of\nOCR, since such systems are judged by the number of false readings.\nLexicon-based OCR systems, which deal with what is essentially a multi-class\nclassification problem, often employ methods explicitly taking into account the\nlexicon, in order to improve accuracy. However, in lexicon-free scenarios,\nfiltering errors requires an explicit confidence calculation. In this work we\nshow two explicit confidence measurement techniques, and show that they are\nable to achieve a significant reduction in misreads on both standard benchmarks\nand a proprietary dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 20:24:04 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Mor", "Noam", ""], ["Wolf", "Lior", ""]]}, {"id": "1805.11178", "submitter": "Alexander Binder", "authors": "Alexander Binder, Michael Bockmayr, Miriam H\\\"agele, Stephan Wienert,\n  Daniel Heim, Katharina Hellweg, Albrecht Stenzinger, Laura Parlow, Jan\n  Budczies, Benjamin Goeppert, Denise Treue, Manato Kotani, Masaru Ishii,\n  Manfred Dietel, Andreas Hocke, Carsten Denkert, Klaus-Robert M\\\"uller,\n  Frederick Klauschen", "title": "Towards computational fluorescence microscopy: Machine learning-based\n  integrated prediction of morphological and molecular tumor profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cancer research largely rely on new developments in\nmicroscopic or molecular profiling techniques offering high level of detail\nwith respect to either spatial or molecular features, but usually not both.\nHere, we present a novel machine learning-based computational approach that\nallows for the identification of morphological tissue features and the\nprediction of molecular properties from breast cancer imaging data. This\nintegration of microanatomic information of tumors with complex molecular\nprofiling data, including protein or gene expression, copy number variation,\ngene methylation and somatic mutations, provides a novel means to\ncomputationally score molecular markers with respect to their relevance to\ncancer and their spatial associations within the tumor microenvironment.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 21:16:30 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Binder", "Alexander", ""], ["Bockmayr", "Michael", ""], ["H\u00e4gele", "Miriam", ""], ["Wienert", "Stephan", ""], ["Heim", "Daniel", ""], ["Hellweg", "Katharina", ""], ["Stenzinger", "Albrecht", ""], ["Parlow", "Laura", ""], ["Budczies", "Jan", ""], ["Goeppert", "Benjamin", ""], ["Treue", "Denise", ""], ["Kotani", "Manato", ""], ["Ishii", "Masaru", ""], ["Dietel", "Manfred", ""], ["Hocke", "Andreas", ""], ["Denkert", "Carsten", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Klauschen", "Frederick", ""]]}, {"id": "1805.11191", "submitter": "Anurag Sahoo", "authors": "Vishal Kaushal, Anurag Sahoo, Khoshrav Doctor, Narasimha Raju, Suyash\n  Shetty, Pankaj Singh, Rishabh Iyer, Ganesh Ramakrishnan", "title": "Learning From Less Data: Diversified Subset Selection and Active\n  Learning in Image Classification Tasks", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning based state-of-the-art computer vision techniques\nare in general data hungry and pose the challenges of not having adequate\ncomputing resources and of high costs involved in human labeling efforts.\nTraining data subset selection and active learning techniques have been\nproposed as possible solutions to these challenges respectively. A special\nclass of subset selection functions naturally model notions of diversity,\ncoverage and representation and they can be used to eliminate redundancy and\nthus lend themselves well for training data subset selection. They can also\nhelp improve the efficiency of active learning in further reducing human\nlabeling efforts by selecting a subset of the examples obtained using the\nconventional uncertainty sampling based techniques. In this work we empirically\ndemonstrate the effectiveness of two diversity models, namely the\nFacility-Location and Disparity-Min models for training-data subset selection\nand reducing labeling effort. We do this for a variety of computer vision tasks\nincluding Gender Recognition, Scene Recognition and Object Recognition. Our\nresults show that subset selection done in the right way can add 2-3% in\naccuracy on existing baselines, particularly in the case of less training data.\nThis allows the training of complex machine learning models (like Convolutional\nNeural Networks) with much less training data while incurring minimal\nperformance loss.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 22:27:29 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Kaushal", "Vishal", ""], ["Sahoo", "Anurag", ""], ["Doctor", "Khoshrav", ""], ["Raju", "Narasimha", ""], ["Shetty", "Suyash", ""], ["Singh", "Pankaj", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1805.11195", "submitter": "Rinat Mukhometzianov", "authors": "Rinat Mukhometzianov, Juan Carrillo", "title": "CapsNet comparative performance evaluation for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has become one of the main tasks in the field of\ncomputer vision technologies. In this context, a recent algorithm called\nCapsNet that implements an approach based on activity vectors and dynamic\nrouting between capsules may overcome some of the limitations of the current\nstate of the art artificial neural networks (ANN) classifiers, such as\nconvolutional neural networks (CNN). In this paper, we evaluated the\nperformance of the CapsNet algorithm in comparison with three well-known\nclassifiers (Fisher-faces, LeNet, and ResNet). We tested the classification\naccuracy on four datasets with a different number of instances and classes,\nincluding images of faces, traffic signs, and everyday objects. The evaluation\nresults show that even for simple architectures, training the CapsNet algorithm\nrequires significant computational resources and its classification performance\nfalls below the average accuracy values of the other three classifiers.\nHowever, we argue that CapsNet seems to be a promising new technique for image\nclassification, and further experiments using more robust computation resources\nand re-fined CapsNet architectures may produce better outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 22:54:17 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Mukhometzianov", "Rinat", ""], ["Carrillo", "Juan", ""]]}, {"id": "1805.11211", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita, Sayaka Shiota and Hitoshi Kiya", "title": "Automatic Exposure Compensation for Multi-Exposure Image Fusion", "comments": "To appear in Proc. ICIP2018 October 07-10, 2018, Athens, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel luminance adjustment method based on automatic\nexposure compensation for multi-exposure image fusion. Multi-exposure image\nfusion is a method to produce images without saturation regions, by using\nphotos with different exposures. In conventional works, it has been pointed out\nthat the quality of those multi-exposure images can be improved by adjusting\nthe luminance of them. However, how to determine the degree of adjustment has\nnever been discussed. This paper therefore proposes a way to automatically\ndetermines the degree on the basis of the luminance distribution of input\nmulti-exposure images. Moreover, new weights, called \"simple weights\", for\nimage fusion are also considered for the proposed luminance adjustment method.\nExperimental results show that the multi-exposure images adjusted by the\nproposed method have better quality than the input multi-exposure ones in terms\nof the well-exposedness. It is also confirmed that the proposed simple weights\nprovide the highest score of statistical naturalness and discrete entropy in\nall fusion methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 01:29:25 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1805.11219", "submitter": "Shafeeq Elanattil Mr", "authors": "Shafeeq Elanattil, Peyman Moghadam, Sridha Sridharan, Clinton Fookes,\n  Mark Cox", "title": "Non-rigid Reconstruction with a Single Moving RGB-D Camera", "comments": "Accepted in International Conference on Pattern Recognition (ICPR\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel non-rigid reconstruction method using a moving RGB-D\ncamera. Current approaches use only non-rigid part of the scene and completely\nignore the rigid background. Non-rigid parts often lack sufficient geometric\nand photometric information for tracking large frame-to-frame motion. Our\napproach uses camera pose estimated from the rigid background for foreground\ntracking. This enables robust foreground tracking in situations where large\nframe-to-frame motion occurs. Moreover, we are proposing a multi-scale\ndeformation graph which improves non-rigid tracking without compromising the\nquality of the reconstruction. We are also contributing a synthetic dataset\nwhich is made publically available for evaluating non-rigid reconstruction\nmethods. The dataset provides frame-by-frame ground truth geometry of the\nscene, the camera trajectory, and masks for background foreground. Experimental\nresults show that our approach is more robust in handling larger frame-to-frame\nmotions and provides better reconstruction compared to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 02:23:30 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 02:22:09 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Elanattil", "Shafeeq", ""], ["Moghadam", "Peyman", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""], ["Cox", "Mark", ""]]}, {"id": "1805.11223", "submitter": "Yaxiang Fan", "authors": "Yaxiang Fan, Gongjian Wen, Deren Li, Shaohua Qiu and Martin D. Levine", "title": "Video Anomaly Detection and Localization via Gaussian Mixture Fully\n  Convolutional Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end partially supervised deep learning approach for\nvideo anomaly detection and localization using only normal samples. The insight\nthat motivates this study is that the normal samples can be associated with at\nleast one Gaussian component of a Gaussian Mixture Model (GMM), while anomalies\neither do not belong to any Gaussian component. The method is based on Gaussian\nMixture Variational Autoencoder, which can learn feature representations of the\nnormal samples as a Gaussian Mixture Model trained using deep learning. A Fully\nConvolutional Network (FCN) that does not contain a fully-connected layer is\nemployed for the encoder-decoder structure to preserve relative spatial\ncoordinates between the input image and the output feature map. Based on the\njoint probabilities of each of the Gaussian mixture components, we introduce a\nsample energy based method to score the anomaly of image test patches. A\ntwo-stream network framework is employed to combine the appearance and motion\nanomalies, using RGB frames for the former and dynamic flow images, for the\nlatter. We test our approach on two popular benchmarks (UCSD Dataset and Avenue\nDataset). The experimental results verify the superiority of our method\ncompared to the state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 02:37:19 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Fan", "Yaxiang", ""], ["Wen", "Gongjian", ""], ["Li", "Deren", ""], ["Qiu", "Shaohua", ""], ["Levine", "Martin D.", ""]]}, {"id": "1805.11227", "submitter": "Chee Seng Chan", "authors": "Yuen Peng Loh, Chee Seng Chan", "title": "Getting to Know Low-light Images with The Exclusively Dark Dataset", "comments": "Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light\n  images from very low-light environments to twilight (i.e 10 different\n  conditions), and 12 object classes (as to PASCAL VOC) annotated on both image\n  class level and local object bounding boxes. 16 pages, 13 figures, submitted\n  to CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light is an inescapable element of our daily surroundings that greatly\naffects the efficiency of our vision. Research works on low-light has seen a\nsteady growth, particularly in the field of image enhancement, but there is\nstill a lack of a go-to database as benchmark. Besides, research fields that\nmay assist us in low-light environments, such as object detection, has glossed\nover this aspect even though breakthroughs-after-breakthroughs had been\nachieved in recent years, most noticeably from the lack of low-light data (less\nthan 2% of the total images) in successful public benchmark dataset such as\nPASCAL VOC, ImageNet, and Microsoft COCO. Thus, we propose the Exclusively Dark\ndataset to elevate this data drought, consisting exclusively of ten different\ntypes of low-light images (i.e. low, ambient, object, single, weak, strong,\nscreen, window, shadow and twilight) captured in visible light only with image\nand object level annotations. Moreover, we share insightful findings in regards\nto the effects of low-light on the object detection task by analyzing\nvisualizations of both hand-crafted and learned features. Most importantly, we\nfound that the effects of low-light reaches far deeper into the features than\ncan be solved by simple \"illumination invariance'\". It is our hope that this\nanalysis and the Exclusively Dark dataset can encourage the growth in low-light\ndomain researches on different fields. The Exclusively Dark dataset with its\nannotation is available at\nhttps://github.com/cs-chan/Exclusively-Dark-Image-Dataset\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 02:59:41 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Loh", "Yuen Peng", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1805.11247", "submitter": "Assaf Arbelle", "authors": "Assaf Arbelle and Tammy Riklin Raviv", "title": "Microscopy Cell Segmentation via Convolutional LSTM Networks", "comments": "Accepted to ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live cell microscopy sequences exhibit complex spatial structures and\ncomplicated temporal behaviour, making their analysis a challenging task.\nConsidering cell segmentation problem, which plays a significant role in the\nanalysis, the spatial properties of the data can be captured using\nConvolutional Neural Networks (CNNs). Recent approaches show promising\nsegmentation results using convolutional encoder-decoders such as the U-Net.\nNevertheless, these methods are limited by their inability to incorporate\ntemporal information, that can facilitate segmentation of individual touching\ncells or of cells that are partially visible. In order to exploit cell dynamics\nwe propose a novel segmentation architecture which integrates Convolutional\nLong Short Term Memory (C-LSTM) with the U-Net. The network's unique\narchitecture allows it to capture multi-scale, compact, spatio-temporal\nencoding in the C-LSTMs memory units. The method was evaluated on the Cell\nTracking Challenge and achieved state-of-the-art results (1st on Fluo-N2DH-SIM+\nand 2nd on DIC-C2DL-HeLa datasets) The code is freely available at:\nhttps://github.com/arbellea/LSTM-UNet.git\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 05:21:36 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 19:19:52 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Arbelle", "Assaf", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1805.11272", "submitter": "Cecilia Summers", "authors": "Cecilia Summers, Michael J. Dinneen", "title": "Improved Mixed-Example Data Augmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reduce overfitting, neural networks are typically trained with\ndata augmentation, the practice of artificially generating additional training\ndata via label-preserving transformations of existing training examples. While\nthese types of transformations make intuitive sense, recent work has\ndemonstrated that even non-label-preserving data augmentation can be\nsurprisingly effective, examining this type of data augmentation through linear\ncombinations of pairs of examples. Despite their effectiveness, little is known\nabout why such methods work. In this work, we aim to explore a new, more\ngeneralized form of this type of data augmentation in order to determine\nwhether such linearity is necessary. By considering this broader scope of\n\"mixed-example data augmentation\", we find a much larger space of practical\naugmentation techniques, including methods that improve upon previous\nstate-of-the-art. This generalization has benefits beyond the promise of\nimproved performance, revealing a number of types of mixed-example data\naugmentation that are radically different from those considered in prior work,\nwhich provides evidence that current theories for the effectiveness of such\nmethods are incomplete and suggests that any such theory must explain a much\nbroader phenomenon. Code is available at\nhttps://github.com/ceciliaresearch/MixedExample.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 07:06:58 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 06:50:22 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 06:10:23 GMT"}, {"version": "v4", "created": "Sat, 19 Jan 2019 07:04:35 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Summers", "Cecilia", ""], ["Dinneen", "Michael J.", ""]]}, {"id": "1805.11291", "submitter": "Chi Wing Mok", "authors": "Tony C.W Mok and Albert C.S Chung", "title": "Learning Data Augmentation for Brain Tumor Segmentation with\n  Coarse-to-Fine Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-11723-8_7", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is a common belief that the successful training of deep neural networks\nrequires many annotated training samples, which are often expensive and\ndifficult to obtain especially in the biomedical imaging field. While it is\noften easy for researchers to use data augmentation to expand the size of\ntraining sets, constructing and generating generic augmented data that is able\nto teach the network the desired invariance and robustness properties using\ntraditional data augmentation techniques is challenging in practice. In this\npaper, we propose a novel automatic data augmentation method that uses\ngenerative adversarial networks to learn augmentations that enable machine\nlearning based method to learn the available annotated samples more\nefficiently. The architecture consists of a coarse-to-fine generator to capture\nthe manifold of the training sets and generate generic augmented data. In our\nexperiments, we show the efficacy of our approach on a Magnetic Resonance\nImaging (MRI) image, achieving improvements of 3.5% Dice coefficient on the\nBRATS15 Challenge dataset as compared to traditional augmentation approaches.\nAlso, our proposed method successfully boosts a common segmentation network to\nreach the state-of-the-art performance on the BRATS15 Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 08:17:13 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 07:15:12 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mok", "Tony C. W", ""], ["Chung", "Albert C. S", ""]]}, {"id": "1805.11318", "submitter": "Benedetta Tondi", "authors": "Mauro Barni, Andrea Costanzo, Ehsan Nowroozi, Benedetta Tondi", "title": "CNN-Based Detection of Generic Constrast Adjustment with JPEG\n  Post-processing", "comments": "To be presented at the 25th IEEE International Conference on Image\n  Processing (ICIP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of contrast adjustments in the presence of JPEG postprocessing is\nknown to be a challenging task. JPEG post processing is often applied\ninnocently, as JPEG is the most common image format, or it may correspond to a\nlaundering attack, when it is purposely applied to erase the traces of\nmanipulation. In this paper, we propose a CNN-based detector for generic\ncontrast adjustment, which is robust to JPEG compression. The proposed system\nrelies on a patch-based Convolutional Neural Network (CNN), trained to\ndistinguish pristine images from contrast adjusted images, for some selected\nadjustment operators of different nature. Robustness to JPEG compression is\nachieved by training the CNN with JPEG examples, compressed over a range of\nQuality Factors (QFs). Experimental results show that the detector works very\nwell and scales well with respect to the adjustment type, yielding very good\nperformance under a large variety of unseen tonal adjustments.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 09:15:26 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Barni", "Mauro", ""], ["Costanzo", "Andrea", ""], ["Nowroozi", "Ehsan", ""], ["Tondi", "Benedetta", ""]]}, {"id": "1805.11327", "submitter": "Jochen Gast", "authors": "Jochen Gast and Stefan Roth", "title": "Lightweight Probabilistic Deep Networks", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though probabilistic treatments of neural networks have a long history,\nthey have not found widespread use in practice. Sampling approaches are often\ntoo slow already for simple networks. The size of the inputs and the depth of\ntypical CNN architectures in computer vision only compound this problem.\nUncertainty in neural networks has thus been largely ignored in practice,\ndespite the fact that it may provide important information about the\nreliability of predictions and the inner workings of the network. In this\npaper, we introduce two lightweight approaches to making supervised learning\nwith probabilistic deep networks practical: First, we suggest probabilistic\noutput layers for classification and regression that require only minimal\nchanges to existing networks. Second, we employ assumed density filtering and\nshow that activation uncertainties can be propagated in a practical fashion\nthrough the entire network, again with minor changes. Both probabilistic\nnetworks retain the predictive power of the deterministic counterpart, but\nyield uncertainties that correlate well with the empirical error induced by\ntheir predictions. Moreover, the robustness to adversarial examples is\nsignificantly increased.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 09:40:52 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Gast", "Jochen", ""], ["Roth", "Stefan", ""]]}, {"id": "1805.11333", "submitter": "Pascal Mettes", "authors": "Pascal Mettes and Cees G. M. Snoek", "title": "Pointly-Supervised Action Localization", "comments": "International Journal of Computer Vision, 2018", "journal-ref": null, "doi": "10.1007/s11263-018-1120-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives for spatio-temporal localization of human actions in\nvideos. In the literature, the consensus is to achieve localization by training\non bounding box annotations provided for each frame of each training video. As\nannotating boxes in video is expensive, cumbersome and error-prone, we propose\nto bypass box-supervision. Instead, we introduce action localization based on\npoint-supervision. We start from unsupervised spatio-temporal proposals, which\nprovide a set of candidate regions in videos. While normally used exclusively\nfor inference, we show spatio-temporal proposals can also be leveraged during\ntraining when guided by a sparse set of point annotations. We introduce an\noverlap measure between points and spatio-temporal proposals and incorporate\nthem all into a new objective of a Multiple Instance Learning optimization.\nDuring inference, we introduce pseudo-points, visual cues from videos, that\nautomatically guide the selection of spatio-temporal proposals. We outline five\nspatial and one temporal pseudo-point, as well as a measure to best leverage\npseudo-points at test time. Experimental evaluation on three action\nlocalization datasets shows our pointly-supervised approach (i) is as effective\nas traditional box-supervision at a fraction of the annotation cost, (ii) is\nrobust to sparse and noisy point annotations, (iii) benefits from pseudo-points\nduring inference, and (iv) outperforms recent weakly-supervised alternatives.\nThis leads us to conclude that points provide a viable alternative to boxes for\naction localization.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 09:52:37 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 12:38:33 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1805.11348", "submitter": "Guillem Pascual", "authors": "Guillem Pascual and Santi Segu\\'i and Jordi Vitri\\`a", "title": "Uncertainty Gated Network for Land Cover Segmentation", "comments": "Accepted in CVPR18 workshop: \"DeepGlobe: A Challenge for Parsing the\n  Earth through Satellite Images\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production of thematic maps depicting land cover is one of the most\ncommon applications of remote sensing. To this end, several semantic\nsegmentation approaches, based on deep learning, have been proposed in the\nliterature, but land cover segmentation is still considered an open problem due\nto some specific problems related to remote sensing imaging. In this paper we\npropose a novel approach to deal with the problem of modelling multiscale\ncontexts surrounding pixels of different land cover categories. The approach\nleverages the computation of a heteroscedastic measure of uncertainty when\nclassifying individual pixels in an image. This classification uncertainty\nmeasure is used to define a set of memory gates between layers that allow a\nprincipled method to select the optimal decision for each pixel.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 10:40:40 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Pascual", "Guillem", ""], ["Segu\u00ed", "Santi", ""], ["Vitri\u00e0", "Jordi", ""]]}, {"id": "1805.11357", "submitter": "Radu Tudor Ionescu", "authors": "Paul Andrei Bricman and Radu Tudor Ionescu", "title": "CocoNet: A deep neural network for mapping pixel coordinates to color\n  values", "comments": "Accepted at the International Conference on Neural Information\n  Processing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep neural network approach for mapping the 2D\npixel coordinates in an image to the corresponding Red-Green-Blue (RGB) color\nvalues. The neural network is termed CocoNet, i.e. coordinates-to-color\nnetwork. During the training process, the neural network learns to encode the\ninput image within its layers. More specifically, the network learns a\ncontinuous function that approximates the discrete RGB values sampled over the\ndiscrete 2D pixel locations. At test time, given a 2D pixel coordinate, the\nneural network will output the approximate RGB values of the corresponding\npixel. By considering every 2D pixel location, the network can actually\nreconstruct the entire learned image. It is important to note that we have to\ntrain an individual neural network for each input image, i.e. one network\nencodes a single image only. To the best of our knowledge, we are the first to\npropose a neural approach for encoding images individually, by learning a\nmapping from the 2D pixel coordinate space to the RGB color space. Our neural\nimage encoding approach has various low-level image processing applications\nranging from image encoding, image compression and image denoising to image\nresampling and image completion. We conduct experiments that include both\nquantitative and qualitative results, demonstrating the utility of our approach\nand its superiority over standard baselines, e.g. bilateral filtering or\nbicubic interpolation. Our code is available at\nhttps://github.com/paubric/python-fuse-coconet.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 11:19:20 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 13:08:28 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 12:39:33 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Bricman", "Paul Andrei", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1805.11372", "submitter": "Vishal Venkat Batchu", "authors": "Vishal Batchu, Varshit Battu, Murali Krishna Reddy, Radhika Mamidi", "title": "\"How to rate a video game?\" - A prediction system for video games based\n  on multimodal information", "comments": "ICPRAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video games have become an integral part of most people's lives in recent\ntimes. This led to an abundance of data related to video games being shared\nonline. However, this comes with issues such as incorrect ratings, reviews or\nanything that is being shared. Recommendation systems are powerful tools that\nhelp users by providing them with meaningful recommendations. A straightforward\napproach would be to predict the scores of video games based on other\ninformation related to the game. It could be used as a means to validate\nuser-submitted ratings as well as provide recommendations. This work provides a\nmethod to predict the G-Score, that defines how good a video game is, from its\ntrailer (video) and summary (text). We first propose models to predict the\nG-Score based on the trailer alone (unimodal). Later on, we show that\nconsidering information from multiple modalities helps the models perform\nbetter compared to using information from videos alone. Since we couldn't find\nany suitable multimodal video game dataset, we created our own dataset named\nVGD (Video Game Dataset) and provide it along with this work. The approach\nmentioned here can be generalized to other multimodal datasets such as movie\ntrailers and summaries etc. Towards the end, we talk about the shortcomings of\nthe work and some methods to overcome them.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:02:18 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Batchu", "Vishal", ""], ["Battu", "Varshit", ""], ["Reddy", "Murali Krishna", ""], ["Mamidi", "Radhika", ""]]}, {"id": "1805.11374", "submitter": "Yu Li", "authors": "Yu Li and Ya Zhang", "title": "Webpage Saliency Prediction with Two-stage Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web page saliency prediction is a challenge problem in image transformation\nand computer vision. In this paper, we propose a new model combined with web\npage outline information to prediction people's interest region in web page.\nFor each web page image, our model can generate the saliency map which\nindicates the region of interest for people. A two-stage generative adversarial\nnetworks are proposed and image outline information is introduced for better\ntransferring. Experiment results on FIWI dataset show that our model have\nbetter performance in terms of saliency prediction.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:03:42 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Li", "Yu", ""], ["Zhang", "Ya", ""]]}, {"id": "1805.11393", "submitter": "Sungmin Lee", "authors": "Sungmin Lee, Jangho Lee, Jungbeom Lee, Chul-Kee Park, and Sungroh Yoon", "title": "Robust Tumor Localization with Pyramid Grad-CAM", "comments": "10 pages, 3 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A meningioma is a type of brain tumor that requires tumor volume size follow\nups in order to reach appropriate clinical decisions. A fully automated tool\nfor meningioma detection is necessary for reliable and consistent tumor\nsurveillance. There have been various studies concerning automated lesion\ndetection. Studies on the application of convolutional neural network\n(CNN)-based methods, which have achieved a state-of-the-art level of\nperformance in various computer vision tasks, have been carried out. However,\nthe applicable diseases are limited, owing to a lack of strongly annotated data\nbeing present in medical image analysis. In order to resolve the above issue we\npropose pyramid gradient-based class activation mapping (PG-CAM) which is a\nnovel method for tumor localization that can be trained in weakly supervised\nmanner. PG-CAM uses a densely connected encoder-decoder-based feature pyramid\nnetwork (DC-FPN) as a backbone structure, and extracts a multi-scale Grad-CAM\nthat captures hierarchical features of a tumor. We tested our model using\nmeningioma brain magnetic resonance (MR) data collected from the collaborating\nhospital. In our experiments, PG-CAM outperformed Grad-CAM by delivering a 23\npercent higher localization accuracy for the validation set.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:36:24 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Lee", "Sungmin", ""], ["Lee", "Jangho", ""], ["Lee", "Jungbeom", ""], ["Park", "Chul-Kee", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1805.11394", "submitter": "Yiming Hu", "authors": "Yiming Hu, Siyang Sun, Jianquan Li, Xingang Wang, Qingyi Gu", "title": "A novel channel pruning method for deep neural network compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have achieved great success in the\nfield of computer vision. However, it is still a big challenge to deploy these\ndeep models on resource-constrained embedded devices such as mobile robots,\nsmart phones and so on. Therefore, network compression for such platforms is a\nreasonable solution to reduce memory consumption and computation complexity. In\nthis paper, a novel channel pruning method based on genetic algorithm is\nproposed to compress very deep Convolution Neural Networks (CNNs). Firstly, a\npre-trained CNN model is pruned layer by layer according to the sensitivity of\neach layer. After that, the pruned model is fine-tuned based on knowledge\ndistillation framework. These two improvements significantly decrease the model\nredundancy with less accuracy drop. Channel selection is a combinatorial\noptimization problem that has exponential solution space. In order to\naccelerate the selection process, the proposed method formulates it as a search\nproblem, which can be solved efficiently by genetic algorithm. Meanwhile, a\ntwo-step approximation fitness function is designed to further improve the\nefficiency of genetic process. The proposed method has been verified on three\nbenchmark datasets with two popular CNN models: VGGNet and ResNet. On the\nCIFAR-100 and ImageNet datasets, our approach outperforms several\nstate-of-the-art methods. On the CIFAR-10 and SVHN datasets, the pruned VGGNet\nachieves better performance than the original model with 8 times parameters\ncompression and 3 times FLOPs reduction.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:37:46 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Hu", "Yiming", ""], ["Sun", "Siyang", ""], ["Li", "Jianquan", ""], ["Wang", "Xingang", ""], ["Gu", "Qingyi", ""]]}, {"id": "1805.11491", "submitter": "Itthi Chatnuntawech", "authors": "Itthi Chatnuntawech, Kittipong Tantisantisom, Paisan Khanchaitit,\n  Thitikorn Boonkoom, Berkin Bilgic, Ekapol Chuangsuwanich", "title": "Rice Classification Using Spatio-Spectral Deep Convolutional Neural\n  Network", "comments": "22 pages, 10 figures, 6 tables; more methods and experiments included\n  with references; link to github included; article restructured for clarity;\n  typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rice has been one of the staple foods that contribute significantly to human\nfood supplies. Numerous rice varieties have been cultivated, imported, and\nexported worldwide. Different rice varieties could be mixed during rice\nproduction and trading. Rice impurities could damage the trust between rice\nimporters and exporters, calling for the need to develop a rice variety\ninspection system. In this work, we develop a non-destructive rice variety\nclassification system that benefits from the synergy between hyperspectral\nimaging and deep convolutional neural network (CNN). The proposed method uses a\nhyperspectral imaging system to simultaneously acquire complementary spatial\nand spectral information of rice seeds. The rice varieties are then determined\nfrom the acquired spatio-spectral data using a deep CNN. As opposed to several\nexisting rice variety classification methods that require hand-engineered\nfeatures, the proposed method automatically extracts spatio-spectral features\nfrom the raw sensor data. As demonstrated using two types of rice datasets, the\nproposed method achieved up to 11.9% absolute improvement in the mean\nclassification accuracy, compared to the commonly used classification methods\nbased on support vector machines.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:17:12 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 02:53:36 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 07:48:35 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Chatnuntawech", "Itthi", ""], ["Tantisantisom", "Kittipong", ""], ["Khanchaitit", "Paisan", ""], ["Boonkoom", "Thitikorn", ""], ["Bilgic", "Berkin", ""], ["Chuangsuwanich", "Ekapol", ""]]}, {"id": "1805.11504", "submitter": "Umair Javaid", "authors": "Umair Javaid and John A. Lee", "title": "Capturing Variabilities from Computed Tomography Images with Generative\n  Adversarial Networks", "comments": null, "journal-ref": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN) Proceedings, pages 403-408, 25-27th\n  April, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of Deep Learning (DL) techniques, especially Generative\nAdversarial Networks (GANs), data augmentation and generation are quickly\nevolving domains that have raised much interest recently. However, the DL\ntechniques are data demanding and since, medical data is not easily accessible,\nthey suffer from data insufficiency. To deal with this limitation, different\ndata augmentation techniques are used. Here, we propose a novel unsupervised\ndata-driven approach for data augmentation that can generate 2D Computed\nTomography (CT) images using a simple GAN. The generated CT images have good\nglobal and local features of a real CT image and can be used to augment the\ntraining datasets for effective learning. In this proof-of-concept study, we\nshow that our proposed solution using GANs is able to capture some of the\nglobal and local CT variabilities. Our network is able to generate visually\nrealistic CT images and we aim to further enhance its output by scaling it to a\nhigher resolution and potentially from 2D to 3D.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:34:56 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Javaid", "Umair", ""], ["Lee", "John A.", ""]]}, {"id": "1805.11519", "submitter": "Pei Li", "authors": "Pei Li, Loreto Prieto, Domingo Mery, Patrick Flynn", "title": "Face Recognition in Low Quality Images: A Survey", "comments": "There are some mistakes addressing in this paper which will be\n  misleading to the reader and we wont have a new version in short time. We\n  will resubmit once it is being corected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resolution face recognition (LRFR) has received increasing attention over\nthe past few years. Its applications lie widely in the real-world environment\nwhen high-resolution or high-quality images are hard to capture. One of the\nbiggest demands for LRFR technologies is video surveillance. As the the number\nof surveillance cameras in the city increases, the videos that captured will\nneed to be processed automatically. However, those videos or images are usually\ncaptured with large standoffs, arbitrary illumination condition, and diverse\nangles of view. Faces in these images are generally small in size. Several\nstudies addressed this problem employed techniques like super resolution,\ndeblurring, or learning a relationship between different resolution domains. In\nthis paper, we provide a comprehensive review of approaches to low-resolution\nface recognition in the past five years. First, a general problem definition is\ngiven. Later, systematically analysis of the works on this topic is presented\nby catogory. In addition to describing the methods, we also focus on datasets\nand experiment settings. We further address the related works on unconstrained\nlow-resolution face recognition and compare them with the result that use\nsynthetic low-resolution data. Finally, we summarized the general limitations\nand speculate a priorities for the future effort.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:50:39 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 13:16:07 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 23:34:23 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Li", "Pei", ""], ["Prieto", "Loreto", ""], ["Mery", "Domingo", ""], ["Flynn", "Patrick", ""]]}, {"id": "1805.11529", "submitter": "Pei Li", "authors": "Pei Li, Loreto Prieto, Domingo Mery, Patrick Flynn", "title": "On Low-Resolution Face Recognition in the Wild: Comparisons and New\n  Techniques", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2018.2890812", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although face recognition systems have achieved impressive performance in\nrecent years, the low-resolution face recognition (LRFR) task remains\nchallenging, especially when the LR faces are captured under non-ideal\nconditions, as is common in surveillance-based applications. Faces captured in\nsuch conditions are often contaminated by blur, nonuniform lighting, and\nnonfrontal face pose. In this paper, we analyze face recognition techniques\nusing data captured under low-quality conditions in the wild. We provide a\ncomprehensive analysis of experimental results for two of the most important\napplications in real surveillance applications, and demonstrate practical\napproaches to handle both cases that show promising performance. The following\nthree contributions are made: {\\em (i)} we conduct experiments to evaluate\nsuper-resolution methods for low-resolution face recognition; {\\em (ii)} we\nstudy face re-identification on various public face datasets including real\nsurveillance and low-resolution subsets of large-scale datasets, present a\nbaseline result for several deep learning based approaches, and improve them by\nintroducing a GAN pre-training approach and fully convolutional architecture;\nand {\\em (iii)} we explore low-resolution face identification by employing a\nstate-of-the-art supervised discriminative learning approach. Evaluations are\nconducted on challenging portions of the SCFace and UCCSface datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 15:04:19 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 22:48:59 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Li", "Pei", ""], ["Prieto", "Loreto", ""], ["Mery", "Domingo", ""], ["Flynn", "Patrick", ""]]}, {"id": "1805.11572", "submitter": "Sebastian Lunz", "authors": "Sebastian Lunz, Ozan \\\"Oktem, Carola-Bibiane Sch\\\"onlieb", "title": "Adversarial Regularizers in Inverse Problems", "comments": "published at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Problems in medical imaging and computer vision are traditionally\nsolved using purely model-based methods. Among those variational regularization\nmodels are one of the most popular approaches. We propose a new framework for\napplying data-driven approaches to inverse problems, using a neural network as\na regularization functional. The network learns to discriminate between the\ndistribution of ground truth images and the distribution of unregularized\nreconstructions. Once trained, the network is applied to the inverse problem by\nsolving the corresponding variational problem. Unlike other data-based\napproaches for inverse problems, the algorithm can be applied even if only\nunsupervised training data is available. Experiments demonstrate the potential\nof the framework for denoising on the BSDS dataset and for computed tomography\nreconstruction on the LIDC dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 16:40:37 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 17:24:06 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Lunz", "Sebastian", ""], ["\u00d6ktem", "Ozan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1805.11589", "submitter": "Angelica I. Aviles-Rivero", "authors": "Daniel Heydecker, Georg Maierhofer, Angelica I. Aviles-Rivero, Qingnan\n  Fan, Dongdong Chen, Carola-Bibiane Sch\\\"onlieb and Sabine S\\\"usstrunk", "title": "Mirror, Mirror, on the Wall, Who's Got the Clearest Image of Them All? -\n  A Tailored Approach to Single Image Reflection Removal", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2923559", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing reflection artefacts from a single image is a problem of both\ntheoretical and practical interest, which still presents challenges because of\nthe massively ill-posed nature of the problem. In this work, we propose a\ntechnique based on a novel optimisation problem. Firstly, we introduce a simple\nuser interaction scheme, which helps minimise information loss in\nreflection-free regions. Secondly, we introduce an $H^2$ fidelity term, which\npreserves fine detail while enforcing global colour similarity. We show that\nthis combination allows us to mitigate some major drawbacks of the existing\nmethods for reflection removal. We demonstrate, through numerical and visual\nexperiments, that our method is able to outperform the state-of-the-art methods\nand compete with recent deep-learning approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 17:12:33 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 20:49:29 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Heydecker", "Daniel", ""], ["Maierhofer", "Georg", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Fan", "Qingnan", ""], ["Chen", "Dongdong", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1805.11592", "submitter": "Yusuf Aytar", "authors": "Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang,\n  Nando de Freitas", "title": "Playing hard exploration games by watching YouTube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning methods traditionally struggle with tasks where\nenvironment rewards are particularly sparse. One successful method of guiding\nexploration in these domains is to imitate trajectories provided by a human\ndemonstrator. However, these demonstrations are typically collected under\nartificial conditions, i.e. with access to the agent's exact environment setup\nand the demonstrator's action and reward trajectories. Here we propose a\ntwo-stage method that overcomes these limitations by relying on noisy,\nunaligned footage without access to such data. First, we learn to map unaligned\nvideos from multiple sources to a common representation using self-supervised\nobjectives constructed over both time and modality (i.e. vision and sound).\nSecond, we embed a single YouTube video in this representation to construct a\nreward function that encourages an agent to imitate human gameplay. This method\nof one-shot imitation allows our agent to convincingly exceed human-level\nperformance on the infamously hard exploration games Montezuma's Revenge,\nPitfall! and Private Eye for the first time, even if the agent is not presented\nwith any environment rewards.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 17:19:36 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:59:27 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Aytar", "Yusuf", ""], ["Pfaff", "Tobias", ""], ["Budden", "David", ""], ["Paine", "Tom Le", ""], ["Wang", "Ziyu", ""], ["de Freitas", "Nando", ""]]}, {"id": "1805.11601", "submitter": "Alon Hazan", "authors": "Alon Hazan, Yoel Shoshan, Daniel Khapun, Roy Aladjem, Vadim Ratner", "title": "AdapterNet - learning input transformation for domain adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated impressive performance in various\nmachine learning tasks. However, they are notoriously sensitive to changes in\ndata distribution. Often, even a slight change in the distribution can lead to\ndrastic performance reduction. Artificially augmenting the data may help to\nsome extent, but in most cases, fails to achieve model invariance to the data\ndistribution. Some examples where this sub-class of domain adaptation can be\nvaluable are various imaging modalities such as thermal imaging, X-ray,\nultrasound, and MRI, where changes in acquisition parameters or acquisition\ndevice manufacturer will result in a different representation of the same\ninput. Our work shows that standard fine-tuning fails to adapt the model in\ncertain important cases. We propose a novel method of adapting to a new data\nsource, and demonstrate near perfect adaptation on a customized ImageNet\nbenchmark. Moreover, our method does not require any samples from the original\ndata set, it is completely explainable and can be tailored to the task.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 17:38:38 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 12:56:58 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Hazan", "Alon", ""], ["Shoshan", "Yoel", ""], ["Khapun", "Daniel", ""], ["Aladjem", "Roy", ""], ["Ratner", "Vadim", ""]]}, {"id": "1805.11685", "submitter": "George Sterpu", "authors": "George Sterpu, Christian Saam, Naomi Harte", "title": "Can DNNs Learn to Lipread Full Sentences?", "comments": "Accepted at the 2018 IEEE International Conference on Image\n  Processing (ICIP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding visual features and suitable models for lipreading tasks that are\nmore complex than a well-constrained vocabulary has proven challenging. This\npaper explores state-of-the-art Deep Neural Network architectures for\nlipreading based on a Sequence to Sequence Recurrent Neural Network. We report\nresults for both hand-crafted and 2D/3D Convolutional Neural Network visual\nfront-ends, online monotonic attention, and a joint Connectionist Temporal\nClassification-Sequence-to-Sequence loss. The system is evaluated on the\npublicly available TCD-TIMIT dataset, with 59 speakers and a vocabulary of over\n6000 words. Results show a major improvement on a Hidden Markov Model\nframework. A fuller analysis of performance across visemes demonstrates that\nthe network is not only learning the language model, but actually learning to\nlipread.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 19:54:19 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Sterpu", "George", ""], ["Saam", "Christian", ""], ["Harte", "Naomi", ""]]}, {"id": "1805.11712", "submitter": "Elaheh Rashedi", "authors": "Elaheh Rashedi, Abdolreza Mirzaei", "title": "A Novel Multi-clustering Method for Hierarchical Clusterings, Based on\n  Boosting", "comments": "19th Iranian Conference on Electrical Engineering (ICEE 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bagging and boosting are proved to be the best methods of building multiple\nclassifiers in classification combination problems. In the area of \"flat\nclustering\" problems, it is also recognized that multi-clustering methods based\non boosting provide clusterings of an improved quality. In this paper, we\nintroduce a novel multi-clustering method for \"hierarchical clusterings\" based\non boosting theory, which creates a more stable hierarchical clustering of a\ndataset. The proposed algorithm includes a boosting iteration in which a\nbootstrap of samples is created by weighted random sampling of elements from\nthe original dataset. A hierarchical clustering algorithm is then applied to\nselected subsample to build a dendrogram which describes the hierarchy.\nFinally, dissimilarity description matrices of multiple dendrogram results are\ncombined to a consensus one, using a hierarchical-clustering-combination\napproach. Experiments on real popular datasets show that boosted method\nprovides superior quality solutions compared to standard hierarchical\nclustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 21:22:53 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Rashedi", "Elaheh", ""], ["Mirzaei", "Abdolreza", ""]]}, {"id": "1805.11714", "submitter": "Michael Zollh\\\"ofer", "authors": "Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies,\n  Matthias Nie{\\ss}ner, Patrick P\\'erez, Christian Richardt, Michael\n  Zollh\\\"ofer, Christian Theobalt", "title": "Deep Video Portraits", "comments": "SIGGRAPH 2018, Video: https://www.youtube.com/watch?v=qc5P2bvfl44", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach that enables photo-realistic re-animation of\nportrait videos using only an input video. In contrast to existing approaches\nthat are restricted to manipulations of facial expressions only, we are the\nfirst to transfer the full 3D head position, head rotation, face expression,\neye gaze, and eye blinking from a source actor to a portrait video of a target\nactor. The core of our approach is a generative neural network with a novel\nspace-time architecture. The network takes as input synthetic renderings of a\nparametric face model, based on which it predicts photo-realistic video frames\nfor a given target actor. The realism in this rendering-to-video transfer is\nachieved by careful adversarial training, and as a result, we can create\nmodified target videos that mimic the behavior of the synthetically-created\ninput. In order to enable source-to-target video re-animation, we render a\nsynthetic target video with the reconstructed head animation parameters from a\nsource video, and feed it into the trained network -- thus taking full control\nof the target. With the ability to freely recombine source and target\nparameters, we are able to demonstrate a large variety of video rewrite\napplications without explicitly modeling hair, body or background. For\ninstance, we can reenact the full head using interactive user-controlled\nediting, and realize high-fidelity visual dubbing. To demonstrate the high\nquality of our output, we conduct an extensive series of experiments and\nevaluations, where for instance a user study shows that our video edits are\nhard to detect.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 21:31:14 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kim", "Hyeongwoo", ""], ["Garrido", "Pablo", ""], ["Tewari", "Ayush", ""], ["Xu", "Weipeng", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["P\u00e9rez", "Patrick", ""], ["Richardt", "Christian", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "1805.11718", "submitter": "Sidharth Gupta", "authors": "Sidharth Gupta, Konik Kothari, Maarten V. de Hoop and Ivan Dokmani\\'c", "title": "Random mesh projectors for inverse problems", "comments": "S. Gupta and K. Kothari contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new learning-based approach to solve ill-posed inverse problems\nin imaging. We address the case where ground truth training samples are rare\nand the problem is severely ill-posed - both because of the underlying physics\nand because we can only get few measurements. This setting is common in\ngeophysical imaging and remote sensing. We show that in this case the common\napproach to directly learn the mapping from the measured data to the\nreconstruction becomes unstable. Instead, we propose to first learn an ensemble\nof simpler mappings from the data to projections of the unknown image into\nrandom piecewise-constant subspaces. We then combine the projections to form a\nfinal reconstruction by solving a deconvolution-like problem. We show\nexperimentally that the proposed method is more robust to measurement noise and\ncorruptions not seen during training than a directly learned inverse.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 21:36:05 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 15:52:04 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 04:31:08 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Gupta", "Sidharth", ""], ["Kothari", "Konik", ""], ["de Hoop", "Maarten V.", ""], ["Dokmani\u0107", "Ivan", ""]]}, {"id": "1805.11724", "submitter": "Michael Kampffmeyer", "authors": "Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang\n  and Eric P. Xing", "title": "Rethinking Knowledge Graph Propagation for Zero-Shot Learning", "comments": "The first two authors contributed equally. Code at\n  https://github.com/cyvius96/adgpm. To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural networks have recently shown great potential for\nthe task of zero-shot learning. These models are highly sample efficient as\nrelated concepts in the graph structure share statistical strength allowing\ngeneralization to new classes when faced with a lack of data. However,\nmulti-layer architectures, which are required to propagate knowledge to distant\nnodes in the graph, dilute the knowledge by performing extensive Laplacian\nsmoothing at each layer and thereby consequently decrease performance. In order\nto still enjoy the benefit brought by the graph structure while preventing\ndilution of knowledge from distant nodes, we propose a Dense Graph Propagation\n(DGP) module with carefully designed direct links among distant nodes. DGP\nallows us to exploit the hierarchical graph structure of the knowledge graph\nthrough additional connections. These connections are added based on a node's\nrelationship to its ancestors and descendants. A weighting scheme is further\nused to weigh their contribution depending on the distance to the node to\nimprove information propagation in the graph. Combined with finetuning of the\nrepresentations in a two-stage training approach our method outperforms\nstate-of-the-art zero-shot learning approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 21:55:46 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 20:14:38 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 17:26:38 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Kampffmeyer", "Michael", ""], ["Chen", "Yinbo", ""], ["Liang", "Xiaodan", ""], ["Wang", "Hao", ""], ["Zhang", "Yujia", ""], ["Xing", "Eric P.", ""]]}, {"id": "1805.11729", "submitter": "Justus Thies", "authors": "Justus Thies, Michael Zollh\\\"ofer, Christian Theobalt, Marc\n  Stamminger, Matthias Nie{\\ss}ner", "title": "HeadOn: Real-time Reenactment of Human Portrait Videos", "comments": "Video: https://www.youtube.com/watch?v=7Dg49wv2c_g Presented at\n  Siggraph'18", "journal-ref": null, "doi": "10.1145/3197517.3201350", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HeadOn, the first real-time source-to-target reenactment approach\nfor complete human portrait videos that enables transfer of torso and head\nmotion, face expression, and eye gaze. Given a short RGB-D video of the target\nactor, we automatically construct a personalized geometry proxy that embeds a\nparametric head, eye, and kinematic torso model. A novel real-time reenactment\nalgorithm employs this proxy to photo-realistically map the captured motion\nfrom the source actor to the target actor. On top of the coarse geometric\nproxy, we propose a video-based rendering technique that composites the\nmodified target portrait video via view- and pose-dependent texturing, and\ncreates photo-realistic imagery of the target actor under novel torso and head\nposes, facial expressions, and gaze directions. To this end, we propose a\nrobust tracking of the face and torso of the source actor. We extensively\nevaluate our approach and show significant improvements in enabling much\ngreater flexibility in creating realistic reenacted output videos.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 22:24:13 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""], ["Stamminger", "Marc", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1805.11737", "submitter": "Li Sulimowicz Ms.", "authors": "Li Sulimowicz, Ishfaq Ahmad, Alexander Aved", "title": "Superpixel-enhanced Pairwise Conditional Random Field for Semantic\n  Segmentation", "comments": "5 pages", "journal-ref": "ICIP 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel-based Higher-order Conditional Random Fields (CRFs) are effective\nin enforcing long-range consistency in pixel-wise labeling problems, such as\nsemantic segmentation. However, their major short coming is considerably longer\ntime to learn higher-order potentials and extra hyperparameters and/or weights\ncompared with pairwise models. This paper proposes a superpixel-enhanced\npairwise CRF framework that consists of the conventional pairwise as well as\nour proposed superpixel-enhanced pairwise (SP-Pairwise) potentials. SP-Pairwise\npotentials incorporate the superpixel-based higher-order cues by conditioning\non a segment filtered image and share the same set of parameters as the\nconventional pairwise potentials. Therefore, the proposed superpixel-enhanced\npairwise CRF has a lower time complexity in parameter learning and at the same\ntime it outperforms higher-order CRF in terms of inference accuracy. Moreover,\nthe new scheme takes advantage of the pre-trained pairwise models by reusing\ntheir parameters and/or weights, which provides a significant accuracy boost on\nthe basis of CRF-RNN even without training. Experiments on MSRC-21 and PASCAL\nVOC 2012 dataset confirm the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 22:59:32 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Sulimowicz", "Li", ""], ["Ahmad", "Ishfaq", ""], ["Aved", "Alexander", ""]]}, {"id": "1805.11746", "submitter": "Federico Becattini", "authors": "Lorenzo Berlincioni, Federico Becattini, Leonardo Galteri, Lorenzo\n  Seidenari, Alberto Del Bimbo", "title": "Semantic Road Layout Understanding by Generative Adversarial Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is becoming a reality, yet vehicles still need to rely on\ncomplex sensor fusion to understand the scene they act in. The ability to\ndiscern static environment and dynamic entities provides a comprehension of the\nroad layout that poses constraints to the reasoning process about moving\nobjects. We pursue this through a GAN-based semantic segmentation inpainting\nmodel to remove all dynamic objects from the scene and focus on understanding\nits static components such as streets, sidewalks and buildings. We evaluate\nthis task on the Cityscapes dataset and on a novel synthetically generated\ndataset obtained with the CARLA simulator and specifically designed to\nquantitatively evaluate semantic segmentation inpaintings. We compare our\nmethods with a variety of baselines working both in the RGB and segmentation\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 23:51:41 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 12:24:37 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Berlincioni", "Lorenzo", ""], ["Becattini", "Federico", ""], ["Galteri", "Leonardo", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1805.11761", "submitter": "Guocong Song", "authors": "Guocong Song, Wei Chai", "title": "Collaborative Learning for Deep Neural Networks", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce collaborative learning in which multiple classifier heads of the\nsame network are simultaneously trained on the same training data to improve\ngeneralization and robustness to label noise with no extra inference cost. It\nacquires the strengths from auxiliary training, multi-task learning and\nknowledge distillation. There are two important mechanisms involved in\ncollaborative learning. First, the consensus of multiple views from different\nclassifier heads on the same example provides supplementary information as well\nas regularization to each classifier, thereby improving generalization. Second,\nintermediate-level representation (ILR) sharing with backpropagation rescaling\naggregates the gradient flows from all heads, which not only reduces training\ncomputational complexity, but also facilitates supervision to the shared\nlayers. The empirical results on CIFAR and ImageNet datasets demonstrate that\ndeep neural networks learned as a group in a collaborative way significantly\nreduce the generalization error and increase the robustness to label noise.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 00:46:33 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 00:06:03 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Song", "Guocong", ""], ["Chai", "Wei", ""]]}, {"id": "1805.11770", "submitter": "Pin-Yu Chen", "authors": "Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang,\n  Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng", "title": "AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for\n  Attacking Black-box Neural Networks", "comments": "Chun-Chen Tu, Paishun Ting and Pin-Yu Chen contribute equally to this\n  work; Paper accepted to AAAI 2019; updated model information in Table S2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that adversarial examples in state-of-the-art image\nclassifiers trained by deep neural networks (DNN) can be easily generated when\nthe target model is transparent to an attacker, known as the white-box setting.\nHowever, when attacking a deployed machine learning service, one can only\nacquire the input-output correspondences of the target model; this is the\nso-called black-box attack setting. The major drawback of existing black-box\nattacks is the need for excessive model queries, which may give a false sense\nof model robustness due to inefficient query designs. To bridge this gap, we\npropose a generic framework for query-efficient black-box attacks. Our\nframework, AutoZOOM, which is short for Autoencoder-based Zeroth Order\nOptimization Method, has two novel building blocks towards efficient black-box\nattacks: (i) an adaptive random gradient estimation strategy to balance query\ncounts and distortion, and (ii) an autoencoder that is either trained offline\nwith unlabeled data or a bilinear resizing operation for attack acceleration.\nExperimental results suggest that, by applying AutoZOOM to a state-of-the-art\nblack-box attack (ZOO), a significant reduction in model queries can be\nachieved without sacrificing the attack success rate and the visual quality of\nthe resulting adversarial examples. In particular, when compared to the\nstandard ZOO method, AutoZOOM can consistently reduce the mean query counts in\nfinding successful adversarial examples (or reaching the same distortion level)\nby at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel\ninsights on adversarial robustness.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 01:39:34 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 21:03:43 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 22:26:12 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2019 00:41:40 GMT"}, {"version": "v5", "created": "Fri, 31 Jan 2020 11:46:26 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Tu", "Chun-Chen", ""], ["Ting", "Paishun", ""], ["Chen", "Pin-Yu", ""], ["Liu", "Sijia", ""], ["Zhang", "Huan", ""], ["Yi", "Jinfeng", ""], ["Hsieh", "Cho-Jui", ""], ["Cheng", "Shin-Ming", ""]]}, {"id": "1805.11773", "submitter": "Amir Rasouli", "authors": "Amir Rasouli and John K. Tsotsos", "title": "Autonomous Vehicles that Interact with Pedestrians: A Survey of Theory\n  and Practice", "comments": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges that autonomous cars are facing today is driving\nin urban environments. To make it a reality, autonomous vehicles require the\nability to communicate with other road users and understand their intentions.\nSuch interactions are essential between the vehicles and pedestrians as the\nmost vulnerable road users. Understanding pedestrian behavior, however, is not\nintuitive and depends on various factors such as demographics of the\npedestrians, traffic dynamics, environmental conditions, etc. In this paper, we\nidentify these factors by surveying pedestrian behavior studies, both the\nclassical works on pedestrian-driver interaction and the modern ones that\ninvolve autonomous vehicles. To this end, we will discuss various methods of\nstudying pedestrian behavior, and analyze how the factors identified in the\nliterature are interrelated. We will also review the practical applications\naimed at solving the interaction problem including design approaches for\nautonomous vehicles that communicate with pedestrians and visual perception and\nreasoning algorithms tailored to understanding pedestrian intention. Based on\nour findings, we will discuss the open problems and propose future research\ndirections.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 02:01:17 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Rasouli", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1805.11778", "submitter": "Sakyasingha Dasgupta", "authors": "Fernando Camaro Nogues, Andrew Huie, Sakyasingha Dasgupta", "title": "Object Detection using Domain Randomization and Generative Adversarial\n  Refinement of Synthetic Images", "comments": "CVPR 2018 Deep Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an application of domain randomization and\ngenerative adversarial networks (GAN) to train a near real-time object detector\nfor industrial electric parts, entirely in a simulated environment. Large scale\navailability of labelled real world data is typically rare and difficult to\nobtain in many industrial settings. As such here, only a few hundred of\nunlabelled real images are used to train a Cyclic-GAN network, in combination\nwith various degree of domain randomization procedures. We demonstrate that\nthis enables robust translation of synthetic images to the real world domain.\nWe show that a combination of the original synthetic (simulation) and GAN\ntranslated images, when used for training a Mask-RCNN object detection network\nachieves greater than 0.95 mean average precision in detecting and classifying\na collection of industrial electric parts. We evaluate the performance across\ndifferent combinations of training data.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 02:27:10 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 09:50:33 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Nogues", "Fernando Camaro", ""], ["Huie", "Andrew", ""], ["Dasgupta", "Sakyasingha", ""]]}, {"id": "1805.11784", "submitter": "Wen-Xuan Liao", "authors": "Wen-Xuan Liao, Xuan-Yu Wang, Dong An, Yao-Guang Wei", "title": "Hyperspectral Imaging Technology and Transfer Learning Utilized in\n  Identification Haploid Maize Seeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is extremely important to correctly identify the cultivars of maize seeds\nin the breeding process of maize. In this paper, the transfer learning as a\nmethod of deep learning is adopted to establish a model by combining with the\nhyperspectral imaging technology. The haploid seeds can be recognized from\nlarge amount of diploid maize ones with great accuracy through the model.\nFirst, the information of maize seeds on each wave band is collected using the\nhyperspectral imaging technology, and then the recognition model is built on\nVGG-19 network, which is pre-trained by large-scale computer vision database\n(Image-Net). The correct identification rate of model utilizing seed spectral\nimages containing 256 wave bands (862.5-1704.2nm) reaches 96.32%, and the\ncorrect identification rate of the model utilizing the seed spectral images\ncontaining single-band reaches 95.75%. The experimental results show that, CNN\nmodel which is pre-trained by visible light image database can be applied to\nthe near-infrared hyperspectral imaging-based identification of maize seeds,\nand high accurate identification rate can be achieved. Meanwhile, when there is\nsmall amount of data samples, it can still realize high recognition by using\ntransfer learning. The model not only meets the requirements of breeding\nrecognition, but also greatly reduce the cost occurred in sample collection.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 02:55:14 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Liao", "Wen-Xuan", ""], ["Wang", "Xuan-Yu", ""], ["An", "Dong", ""], ["Wei", "Yao-Guang", ""]]}, {"id": "1805.11788", "submitter": "Luna Zhang", "authors": "Luna M. Zhang", "title": "Multi-function Convolutional Neural Networks for Improving Image\n  Classification Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Convolutional Neural Networks (CNNs) typically use the same\nactivation function (usually ReLU) for all neurons with non-linear mapping\noperations. For example, the deep convolutional architecture Inception-v4 uses\nReLU. To improve the classification performance of traditional CNNs, a new\n\"Multi-function Convolutional Neural Network\" (MCNN) is created by using\ndifferent activation functions for different neurons. For $n$ neurons and $m$\ndifferent activation functions, there are a total of $m^n-m$ MCNNs and only $m$\ntraditional CNNs. Therefore, the best model is very likely to be chosen from\nMCNNs because there are $m^n-2m$ more MCNNs than traditional CNNs. For\nperformance analysis, two different datasets for two applications (classifying\nhandwritten digits from the MNIST database and classifying brain MRI images\ninto one of the four stages of Alzheimer's disease (AD)) are used. For both\napplications, an activation function is randomly selected for each layer of a\nMCNN. For the AD diagnosis application, MCNNs using a newly created\nmulti-function Inception-v4 architecture are constructed. Overall, simulations\nshow that MCNNs can outperform traditional CNNs in terms of multi-class\nclassification accuracy for both applications. An important future research\nwork will be to efficiently select the best MCNN from $m^n-m$ candidate MCNNs.\nCurrent CNN software only provides users with partial functionality of MCNNs\nsince different layers can use different activation functions but not\nindividual neurons in the same layer. Thus, modifying current CNN software\nsystems such as ResNets, DenseNets, and Dual Path Networks by using multiple\nactivation functions and developing more effective and faster MCNN software\nsystems and tools would be very useful to solve difficult practical image\nclassification problems.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 03:14:03 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zhang", "Luna M.", ""]]}, {"id": "1805.11790", "submitter": "Thao Le", "authors": "Thao Minh Le, Nakamasa Inoue, Koichi Shinoda", "title": "A Fine-to-Coarse Convolutional Neural Network for 3D Human Action\n  Recognition", "comments": "Camera-ready manuscript for BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for human action recognition from a 3D\nskeleton sequence. Previous studies do not fully utilize the temporal\nrelationships between video segments in a human action. Some studies\nsuccessfully used very deep Convolutional Neural Network (CNN) models but often\nsuffer from the data insufficiency problem. In this study, we first segment a\nskeleton sequence into distinct temporal segments in order to exploit the\ncorrelations between them. The temporal and spatial features of a skeleton\nsequence are then extracted simultaneously by utilizing a fine-to-coarse (F2C)\nCNN architecture optimized for human skeleton sequences. We evaluate our\nproposed method on NTU RGB+D and SBU Kinect Interaction dataset. It achieves\n79.6% and 84.6% of accuracies on NTU RGB+D with cross-object and cross-view\nprotocol, respectively, which are almost identical with the state-of-the-art\nperformance. In addition, our method significantly improves the accuracy of the\nactions in two-person interactions.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 03:25:14 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 08:20:52 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Le", "Thao Minh", ""], ["Inoue", "Nakamasa", ""], ["Shinoda", "Koichi", ""]]}, {"id": "1805.11797", "submitter": "Xiaoliang Dai", "authors": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "title": "Grow and Prune Compact, Fast, and Accurate LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory (LSTM) has been widely used for sequential data\nmodeling. Researchers have increased LSTM depth by stacking LSTM cells to\nimprove performance. This incurs model redundancy, increases run-time delay,\nand makes the LSTMs more prone to overfitting. To address these problems, we\npropose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original\none level non-linear control gates. H-LSTM increases accuracy while employing\nfewer external stacked layers, thus reducing the number of parameters and\nrun-time latency significantly. We employ grow-and-prune (GP) training to\niteratively adjust the hidden layers through gradient-based growth and\nmagnitude-based pruning of connections. This learns both the weights and the\ncompact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for\nimage captioning and speech recognition applications. For the NeuralTalk\narchitecture on the MSCOCO dataset, our three models reduce the number of\nparameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time\nlatency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2\narchitecture on the AN4 dataset, our two models reduce the number of parameters\nby 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate\nfrom 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast,\nand accurate.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:15:58 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 03:49:25 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Dai", "Xiaoliang", ""], ["Yin", "Hongxu", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1805.11802", "submitter": "Patrick Wan", "authors": "Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, Alex C. Kot", "title": "CRRN: Multi-Scale Guided Concurrent Reflection Removal Network", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Removing the undesired reflections from images taken through the glass is of\nbroad application to various computer vision tasks. Non-learning based methods\nutilize different handcrafted priors such as the separable sparse gradients\ncaused by different levels of blurs, which often fail due to their limited\ndescription capability to the properties of real-world reflections. In this\npaper, we propose the Concurrent Reflection Removal Network (CRRN) to tackle\nthis problem in a unified framework. Our proposed network integrates image\nappearance information and multi-scale gradient information with human\nperception inspired loss function, and is trained on a new dataset with 3250\nreflection images taken under diverse real-world scenes. Extensive experiments\non a public benchmark dataset show that the proposed method performs favorably\nagainst state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:29:42 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Wan", "Renjie", ""], ["Shi", "Boxin", ""], ["Duan", "Ling-Yu", ""], ["Tan", "Ah-Hwee", ""], ["Kot", "Alex C.", ""]]}, {"id": "1805.11815", "submitter": "Puneet Kohli", "authors": "Puneet Kohli and Anjali Chadha", "title": "Enabling Pedestrian Safety using Computer Vision Techniques: A Case\n  Study of the 2018 Uber Inc. Self-driving Car Crash", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": "Arai K., Bhatia R. (eds) Advances in Information and\n  Communication. FICC 2019. Lecture Notes in Networks and Systems, vol 69.\n  Springer, Cham", "doi": "10.1007/978-3-030-12388-8_19", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human lives are important. The decision to allow self-driving vehicles\noperate on our roads carries great weight. This has been a hot topic of debate\nbetween policy-makers, technologists and public safety institutions. The recent\nUber Inc. self-driving car crash, resulting in the death of a pedestrian, has\nstrengthened the argument that autonomous vehicle technology is still not ready\nfor deployment on public roads. In this work, we analyze the Uber car crash and\nshed light on the question, \"Could the Uber Car Crash have been avoided?\". We\napply state-of-the-art Computer Vision models to this highly practical\nscenario. More generally, our experimental results are an evaluation of various\nimage enhancement and object recognition techniques for enabling pedestrian\nsafety in low-lighting conditions using the Uber crash as a case study.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 05:38:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kohli", "Puneet", ""], ["Chadha", "Anjali", ""]]}, {"id": "1805.11818", "submitter": "Volkan Cirik", "authors": "Volkan Cirik, Louis-Philippe Morency, Taylor Berg-Kirkpatrick", "title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?", "comments": "NAACL2018 short", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an empirical analysis of the state-of-the-art systems for\nreferring expression recognition -- the task of identifying the object in an\nimage referred to by a natural language expression -- with the goal of gaining\ninsight into how these systems reason about language and vision. Surprisingly,\nwe find strong evidence that even sophisticated and linguistically-motivated\nmodels for this task may ignore the linguistic structure, instead relying on\nshallow correlations introduced by unintended biases in the data selection and\nannotation process. For example, we show that a system trained and tested on\nthe input image $\\textit{without the input referring expression}$ can achieve a\nprecision of 71.2% in top-2 predictions. Furthermore, a system that predicts\nonly the object category given the input can achieve a precision of 84.2% in\ntop-2 predictions. These surprisingly positive results for what should be\ndeficient prediction scenarios suggest that careful analysis of what our models\nare learning -- and further, how our data is constructed -- is critical as we\nseek to make substantive progress on grounded language tasks.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 06:03:21 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Cirik", "Volkan", ""], ["Morency", "Louis-Philippe", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1805.11837", "submitter": "Vadim Ratner", "authors": "Vadim Ratner, Yoel Shoshan, Tal Kachman", "title": "Learning multiple non-mutually-exclusive tasks for improved\n  classification of inherently ordered labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical image classification involves thresholding of labels that represent\nmalignancy risk levels. Usually, a task defines a single threshold, and when\ndeveloping computer-aided diagnosis tools, a single network is trained per such\nthreshold, e.g. as screening out healthy (very low risk) patients to leave\npossibly sick ones for further analysis (low threshold), or trying to find\nmalignant cases among those marked as non-risk by the radiologist (\"second\nreading\", high threshold). We propose a way to rephrase the classification\nproblem in a manner that yields several problems (corresponding to different\nthresholds) to be solved simultaneously. This allows the use of Multiple Task\nLearning (MTL) methods, significantly improving the performance of the original\nclassifier, by facilitating effective extraction of information from existing\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 07:25:54 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 07:06:08 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Ratner", "Vadim", ""], ["Shoshan", "Yoel", ""], ["Kachman", "Tal", ""]]}, {"id": "1805.11850", "submitter": "Kota Yoshida", "authors": "Kota Yoshida, Munetaka Minoguchi, Kenichiro Wani, Akio Nakamura and\n  Hirokatsu Kataoka", "title": "Neural Joking Machine : Humorous image captioning", "comments": "Accepted to CVPR 2018 Language & Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is an effective expression that draws laughter from human beings? In the\npresent paper, in order to consider this question from an academic standpoint,\nwe generate an image caption that draws a \"laugh\" by a computer. A system that\noutputs funny captions based on the image caption proposed in the computer\nvision field is constructed. Moreover, we also propose the Funny Score, which\nflexibly gives weights according to an evaluation database. The Funny Score\nmore effectively brings out \"laughter\" to optimize a model. In addition, we\nbuild a self-collected BoketeDB, which contains a theme (image) and funny\ncaption (text) posted on \"Bokete\", which is an image Ogiri website. In an\nexperiment, we use BoketeDB to verify the effectiveness of the proposed method\nby comparing the results obtained using the proposed method and those obtained\nusing MS COCO Pre-trained CNN+LSTM, which is the baseline and idiot created by\nhumans. We refer to the proposed method, which uses the BoketeDB pre-trained\nmodel, as the Neural Joking Machine (NJM).\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 08:20:55 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Yoshida", "Kota", ""], ["Minoguchi", "Munetaka", ""], ["Wani", "Kenichiro", ""], ["Nakamura", "Akio", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1805.11856", "submitter": "Yuanyuan Li", "authors": "Tian Lan, Yuanyuan Li, Jonah Kimani Murugi, Yi Ding, Zhiguang Qin", "title": "RUN:Residual U-Net for Computer-Aided Detection of Pulmonary Nodules\n  without Candidate Selection", "comments": "15 pages, 5 figures, manuscript for Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early detection and early diagnosis of lung cancer are crucial to improve\nthe survival rate of lung cancer patients. Pulmonary nodules detection results\nhave a significant impact on the later diagnosis. In this work, we propose a\nnew network named RUN to complete nodule detection in a single step by\nbypassing the candidate selection. The system introduces the shortcut of the\nresidual network to improve the traditional U-Net, thereby solving the\ndisadvantage of poor results due to its lack of depth. Furthermore, we compare\nthe experimental results with the traditional U-Net. We validate our method in\nLUng Nodule Analysis 2016 (LUNA16) Nodule Detection Challenge. We acquire a\nsensitivity of 90.90% at 2 false positives per scan and therefore achieve\nbetter performance than the current state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 08:42:20 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Lan", "Tian", ""], ["Li", "Yuanyuan", ""], ["Murugi", "Jonah Kimani", ""], ["Ding", "Yi", ""], ["Qin", "Zhiguang", ""]]}, {"id": "1805.11911", "submitter": "Nils Gessert", "authors": "Nils Gessert, Torben Priegnitz, Thore Saathoff, Sven-Thomas Antoni,\n  David Meyer, Moritz Franz Hamann, Klaus-Peter J\\\"unemann, Christoph Otte,\n  Alexander Schlaefer", "title": "Needle Tip Force Estimation using an OCT Fiber and a Fused convGRU-CNN\n  Architecture", "comments": "Accepted for Publication at MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00937-3_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Needle insertion is common during minimally invasive interventions such as\nbiopsy or brachytherapy. During soft tissue needle insertion, forces acting at\nthe needle tip cause tissue deformation and needle deflection. Accurate needle\ntip force measurement provides information on needle-tissue interaction and\nhelps detecting and compensating potential misplacement. For this purpose we\nintroduce an image-based needle tip force estimation method using an optical\nfiber imaging the deformation of an epoxy layer below the needle tip over time.\nFor calibration and force estimation, we introduce a novel deep learning-based\nfused convolutional GRU-CNN model which effectively exploits the\nspatio-temporal data structure. The needle is easy to manufacture and our model\nachieves a mean absolute error of 1.76 +- 1.5 mN with a cross-correlation\ncoefficient of 0.9996, clearly outperforming other methods. We test needles\nwith different materials to demonstrate that the approach can be adapted for\ndifferent sensitivities and force ranges. Furthermore, we validate our approach\nin an ex-vivo prostate needle insertion scenario.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 11:53:17 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Gessert", "Nils", ""], ["Priegnitz", "Torben", ""], ["Saathoff", "Thore", ""], ["Antoni", "Sven-Thomas", ""], ["Meyer", "David", ""], ["Hamann", "Moritz Franz", ""], ["J\u00fcnemann", "Klaus-Peter", ""], ["Otte", "Christoph", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1805.11913", "submitter": "Abdelrahman Eldesokey", "authors": "Abdelrahman Eldesokey, Michael Felsberg and Fahad Shahbaz Khan", "title": "Propagating Confidences through CNNs for Sparse Data Regression", "comments": "To appear in the British Machine Vision Conference (BMVC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most computer vision applications, convolutional neural networks (CNNs)\noperate on dense image data generated by ordinary cameras. Designing CNNs for\nsparse and irregularly spaced input data is still an open problem with numerous\napplications in autonomous driving, robotics, and surveillance. To tackle this\nchallenging problem, we introduce an algebraically-constrained convolution\nlayer for CNNs with sparse input and demonstrate its capabilities for the scene\ndepth completion task. We propose novel strategies for determining the\nconfidence from the convolution operation and propagating it to consecutive\nlayers. Furthermore, we propose an objective function that simultaneously\nminimizes the data error while maximizing the output confidence. Comprehensive\nexperiments are performed on the KITTI depth benchmark and the results clearly\ndemonstrate that the proposed approach achieves superior performance while\nrequiring three times fewer parameters than the state-of-the-art methods.\nMoreover, our approach produces a continuous pixel-wise confidence map enabling\ninformation fusion, state inference, and decision support.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 12:09:51 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 13:53:05 GMT"}, {"version": "v3", "created": "Fri, 3 Aug 2018 09:05:49 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Eldesokey", "Abdelrahman", ""], ["Felsberg", "Michael", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "1805.11918", "submitter": "Wang Rui", "authors": "Rui Wang, Xiao-Jun Wu, Kai-Xuan Chen, Josef Kittler", "title": "Multiple Manifolds Metric Learning with Application to Image Set\n  Classification", "comments": "6 pages, 4 figures,ICPR 2018(accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image set classification, a considerable advance has been made by modeling\nthe original image sets by second order statistics or linear subspace, which\ntypically lie on the Riemannian manifold. Specifically, they are Symmetric\nPositive Definite (SPD) manifold and Grassmann manifold respectively, and some\nalgorithms have been developed on them for classification tasks. Motivated by\nthe inability of existing methods to extract discriminatory features for data\non Riemannian manifolds, we propose a novel algorithm which combines multiple\nmanifolds as the features of the original image sets. In order to fuse these\nmanifolds, the well-studied Riemannian kernels have been utilized to map the\noriginal Riemannian spaces into high dimensional Hilbert spaces. A metric\nLearning method has been devised to embed these kernel spaces into a lower\ndimensional common subspace for classification. The state-of-the-art results\nachieved on three datasets corresponding to two different classification tasks,\nnamely face recognition and object categorization, demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 12:23:52 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Wang", "Rui", ""], ["Wu", "Xiao-Jun", ""], ["Chen", "Kai-Xuan", ""], ["Kittler", "Josef", ""]]}, {"id": "1805.11927", "submitter": "Guido Borghi", "authors": "Stefano Pini, Filippo Grazioli, Guido Borghi, Roberto Vezzani and Rita\n  Cucchiara", "title": "Learning to Generate Facial Depth Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an adversarial architecture for facial depth map estimation\nfrom monocular intensity images is presented. By following an image-to-image\napproach, we combine the advantages of supervised learning and adversarial\ntraining, proposing a conditional Generative Adversarial Network that\neffectively learns to translate intensity face images into the corresponding\ndepth maps. Two public datasets, namely Biwi database and Pandora dataset, are\nexploited to demonstrate that the proposed model generates high-quality\nsynthetic depth images, both in terms of visual appearance and informative\ncontent. Furthermore, we show that the model is capable of predicting\ndistinctive facial details by testing the generated depth maps through a deep\nmodel trained on authentic depth maps for the face verification task.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 13:00:42 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Pini", "Stefano", ""], ["Grazioli", "Filippo", ""], ["Borghi", "Guido", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1805.11970", "submitter": "Rodrigo Berriel", "authors": "Rodrigo F. Berriel, Franco Schmidt Rossi, Alberto F. de Souza, Thiago\n  Oliveira-Santos", "title": "Automatic Large-Scale Data Acquisition via Crowdsourcing for Crosswalk\n  Classification: A Deep Learning Approach", "comments": "13 pages, 13 figures, 3 videos, and GitHub with models", "journal-ref": "Computers & Graphics, 2017, vol. 68, pp. 32-42", "doi": "10.1016/J.CAG.2017.08.004", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly identifying crosswalks is an essential task for the driving\nactivity and mobility autonomy. Many crosswalk classification, detection and\nlocalization systems have been proposed in the literature over the years. These\nsystems use different perspectives to tackle the crosswalk classification\nproblem: satellite imagery, cockpit view (from the top of a car or behind the\nwindshield), and pedestrian perspective. Most of the works in the literature\nare designed and evaluated using small and local datasets, i.e. datasets that\npresent low diversity. Scaling to large datasets imposes a challenge for the\nannotation procedure. Moreover, there is still need for cross-database\nexperiments in the literature because it is usually hard to collect the data in\nthe same place and conditions of the final application. In this paper, we\npresent a crosswalk classification system based on deep learning. For that,\ncrowdsourcing platforms, such as OpenStreetMap and Google Street View, are\nexploited to enable automatic training via automatic acquisition and annotation\nof a large-scale database. Additionally, this work proposes a comparison study\nof models trained using fully-automatic data acquisition and annotation against\nmodels that were partially annotated. Cross-database experiments were also\nincluded in the experimentation to show that the proposed methods enable use\nwith real world applications. Our results show that the model trained on the\nfully-automatic database achieved high overall accuracy (94.12%), and that a\nstatistically significant improvement (to 96.30%) can be achieved by manually\nannotating a specific part of the database. Finally, the results of the\ncross-database experiments show that both models are robust to the many\nvariations of image and scenarios, presenting a consistent behavior.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 13:55:14 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Berriel", "Rodrigo F.", ""], ["Rossi", "Franco Schmidt", ""], ["de Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1805.11984", "submitter": "Mihai Andries", "authors": "Mihai Andries, Atabak Dehban, Jos\\'e Santos-Victor", "title": "Automatic generation of object shapes with desired functionalities", "comments": "12 pages, 9 figures, 28 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D objects (artefacts) are made to fulfill functions. Designing an object\noften starts with defining a list of functionalities that it should provide,\nalso known as functional requirements. Today, the design of 3D object models is\nstill a slow and largely artisanal activity, with few Computer-Aided Design\n(CAD) tools existing to aid the exploration of the design solution space. To\naccelerate the design process, we introduce an algorithm for generating object\nshapes with desired functionalities. Following the concept of form follows\nfunction, we assume that existing object shapes were rationally chosen to\nprovide desired functionalities. First, we use an artificial neural network to\nlearn a function-to-form mapping by analysing a dataset of objects labeled with\ntheir functionalities. Then, we combine forms providing one or more desired\nfunctions, generating an object shape that is expected to provide all of them.\nFinally, we verify in simulation whether the generated object possesses the\ndesired functionalities, by defining and executing functionality tests on it.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 14:00:40 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 20:11:52 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Andries", "Mihai", ""], ["Dehban", "Atabak", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1805.12018", "submitter": "Riccardo Volpi", "authors": "Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio\n  Murino, Silvio Savarese", "title": "Generalizing to Unseen Domains via Adversarial Data Augmentation", "comments": "Accepted to NIPS 2018 (camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with learning models that generalize well to different\n\\emph{unseen} domains. We consider a worst-case formulation over data\ndistributions that are near the source domain in the feature space. Only using\ntraining data from a single source distribution, we propose an iterative\nprocedure that augments the dataset with examples from a fictitious target\ndomain that is \"hard\" under the current model. We show that our iterative\nscheme is an adaptive data augmentation method where we append adversarial\nexamples at each iteration. For softmax losses, we show that our method is a\ndata-dependent regularization scheme that behaves differently from classical\nregularizers that regularize towards zero (e.g., ridge or lasso). On digit\nrecognition and semantic segmentation tasks, our method learns models improve\nperformance across a range of a priori unknown target domains.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 15:03:27 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 16:51:32 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Volpi", "Riccardo", ""], ["Namkoong", "Hongseok", ""], ["Sener", "Ozan", ""], ["Duchi", "John", ""], ["Murino", "Vittorio", ""], ["Savarese", "Silvio", ""]]}, {"id": "1805.12024", "submitter": "Sam Leroux", "authors": "Sam Leroux, Tim Verbelen, Pieter Simoens, Bart Dhoedt", "title": "Privacy Aware Offloading of Deep Neural Networks", "comments": "ICML 2018 Privacy in Machine Learning and Artificial Intelligence\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks require large amounts of resources which makes them hard\nto use on resource constrained devices such as Internet-of-things devices.\nOffloading the computations to the cloud can circumvent these constraints but\nintroduces a privacy risk since the operator of the cloud is not necessarily\ntrustworthy. We propose a technique that obfuscates the data before sending it\nto the remote computation node. The obfuscated data is unintelligible for a\nhuman eavesdropper but can still be classified with a high accuracy by a neural\nnetwork trained on unobfuscated images.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 15:10:20 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Leroux", "Sam", ""], ["Verbelen", "Tim", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1805.12048", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Samuel Rota Bul\\`o, Barbara Caputo, Elisa Ricci", "title": "Robust Place Categorization with Deep Domain Generalization", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2018.2809700", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional place categorization approaches in robot vision assume that\ntraining and test images have similar visual appearance. Therefore, any\nseasonal, illumination and environmental changes typically lead to severe\ndegradation in performance. To cope with this problem, recent works have\nproposed to adopt domain adaptation techniques. While effective, these methods\nassume that some prior information about the scenario where the robot will\noperate is available at training time. Unfortunately, in many cases this\nassumption does not hold, as we often do not know where a robot will be\ndeployed. To overcome this issue, in this paper we present an approach which\naims at learning classification models able to generalize to unseen scenarios.\nSpecifically, we propose a novel deep learning framework for domain\ngeneralization. Our method develops from the intuition that, given a set of\ndifferent classification models associated to known domains (e.g. corresponding\nto multiple environments, robots), the best model for a new sample in the novel\ndomain can be computed directly at test time by optimally combining the known\nmodels. To implement our idea, we exploit recent advances in deep domain\nadaptation and design a Convolutional Neural Network architecture with novel\nlayers performing a weighted version of Batch Normalization. Our experiments,\nconducted on three common datasets for robot place categorization, confirm the\nvalidity of our contribution.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:00:34 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "1805.12064", "submitter": "Jo Schlemper", "authors": "Jo Schlemper, Guang Yang, Pedro Ferreira, Andrew Scott, Laura-Ann\n  McGill, Zohya Khalique, Margarita Gorodezky, Malte Roehl, Jennifer Keegan,\n  Dudley Pennell, David Firmin, Daniel Rueckert", "title": "Stochastic Deep Compressive Sensing for the Reconstruction of Diffusion\n  Tensor Cardiac MRI", "comments": "Accepted for MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of the heart at the microscopic scale of\ncardiomyocytes and their aggregates provides new insights into the mechanisms\nof heart disease and enables the investigation of effective therapeutics.\nDiffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is a unique non-invasive\ntechnique that can resolve the microscopic structure, organisation, and\nintegrity of the myocardium without the need for exogenous contrast agents.\nHowever, this technique suffers from relatively low signal-to-noise ratio (SNR)\nand frequent signal loss due to respiratory and cardiac motion. Current DT-CMR\ntechniques rely on acquiring and averaging multiple signal acquisitions to\nimprove the SNR. Moreover, in order to mitigate the influence of respiratory\nmovement, patients are required to perform many breath holds which results in\nprolonged acquisition durations (e.g., ~30 mins using the existing technology).\nIn this study, we propose a novel cascaded Convolutional Neural Networks (CNN)\nbased compressive sensing (CS) technique and explore its applicability to\nimprove DT-CMR acquisitions. Our simulation based studies have achieved high\nreconstruction fidelity and good agreement between DT-CMR parameters obtained\nwith the proposed reconstruction and fully sampled ground truth. When compared\nto other state-of-the-art methods, our proposed deep cascaded CNN method and\nits stochastic variation demonstrated significant improvements. To the best of\nour knowledge, this is the first study using deep CNN based CS for the DT-CMR\nreconstruction. In addition, with relatively straightforward modifications to\nthe acquisition scheme, our method can easily be translated into a method for\nonline, at-the-scanner reconstruction enabling the deployment of accelerated\nDT-CMR in various clinical applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:31:04 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Schlemper", "Jo", ""], ["Yang", "Guang", ""], ["Ferreira", "Pedro", ""], ["Scott", "Andrew", ""], ["McGill", "Laura-Ann", ""], ["Khalique", "Zohya", ""], ["Gorodezky", "Margarita", ""], ["Roehl", "Malte", ""], ["Keegan", "Jennifer", ""], ["Pennell", "Dudley", ""], ["Firmin", "David", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1805.12067", "submitter": "Byungjae Lee", "authors": "Byungjae Lee and Kyunghyun Paeng", "title": "A Robust and Effective Approach Towards Accurate Metastasis Detection\n  and pN-stage Classification in Breast Cancer", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting TNM stage is the major determinant of breast cancer prognosis and\ntreatment. The essential part of TNM stage classification is whether the cancer\nhas metastasized to the regional lymph nodes (N-stage). Pathologic N-stage\n(pN-stage) is commonly performed by pathologists detecting metastasis in\nhistological slides. However, this diagnostic procedure is prone to\nmisinterpretation and would normally require extensive time by pathologists\nbecause of the sheer volume of data that needs a thorough review. Automated\ndetection of lymph node metastasis and pN-stage prediction has a great\npotential to reduce their workload and help the pathologist. Recent advances in\nconvolutional neural networks (CNN) have shown significant improvements in\nhistological slide analysis, but accuracy is not optimized because of the\ndifficulty in the handling of gigapixel images. In this paper, we propose a\nrobust method for metastasis detection and pN-stage classification in breast\ncancer from multiple gigapixel pathology images in an effective way. pN-stage\nis predicted by combining patch-level CNN based metastasis detector and\nslide-level lymph node classifier. The proposed framework achieves a\nstate-of-the-art quadratic weighted kappa score of 0.9203 on the Camelyon17\ndataset, outperforming the previous winning method of the Camelyon17 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:33:37 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Lee", "Byungjae", ""], ["Paeng", "Kyunghyun", ""]]}, {"id": "1805.12071", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean, Alberto De Luca, Max A. Viergever, Alexander Leemans", "title": "Automatic, fast and robust characterization of noise distributions for\n  diffusion MRI", "comments": "v2: added publisher DOI statement, fixed text typo in appendix A2", "journal-ref": "St-Jean S. et al. (2018) Automatic, Fast and Robust\n  Characterization of Noise Distributions for Diffusion MRI. In: Medical Image\n  Computing and Computer Assisted Intervention - MICCAI 2018. LNCS, vol 11070.\n  Springer, Cham", "doi": "10.1007/978-3-030-00928-1_35", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the noise distribution in magnitude diffusion MRI images is the\ncenterpiece to quantify uncertainties arising from the acquisition process. The\nuse of parallel imaging methods, the number of receiver coils and imaging\nfilters applied by the scanner, amongst other factors, dictate the resulting\nsignal distribution. Accurate estimation beyond textbook Rician or noncentral\nchi distributions often requires information about the acquisition process\n(e.g. coils sensitivity maps or reconstruction coefficients), which is not\nusually available. We introduce a new method where a change of variable\nnaturally gives rise to a particular form of the gamma distribution for\nbackground signals. The first moments and maximum likelihood estimators of this\ngamma distribution explicitly depend on the number of coils, making it possible\nto estimate all unknown parameters using only the magnitude data. A rejection\nstep is used to make the method automatic and robust to artifacts. Experiments\non synthetic datasets show that the proposed method can reliably estimate both\nthe degrees of freedom and the standard deviation. The worst case errors range\nfrom below 2% (spatially uniform noise) to approximately 10% (spatially\nvariable noise). Repeated acquisitions of in vivo datasets show that the\nestimated parameters are stable and have lower variances than compared methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:38:25 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 10:10:48 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["St-Jean", "Samuel", ""], ["De Luca", "Alberto", ""], ["Viergever", "Max A.", ""], ["Leemans", "Alexander", ""]]}, {"id": "1805.12081", "submitter": "Md. Mostafa Kamal Sarker", "authors": "Md. Mostafa Kamal Sarker, Mohammed Jabreel, Hatem A. Rashwan, Syeda\n  Furruka Banu, Antonio Moreno, Petia Radeva, Domenec Puig", "title": "CuisineNet: Food Attributes Classification using Multi-scale Convolution\n  Network", "comments": "8 pages, Submitted in CCIA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity of food and its attributes represents the culinary habits of\npeoples from different countries. Thus, this paper addresses the problem of\nidentifying food culture of people around the world and its flavor by\nclassifying two main food attributes, cuisine and flavor. A deep learning model\nbased on multi-scale convotuional networks is proposed for extracting more\naccurate features from input images. The aggregation of multi-scale convolution\nlayers with different kernel size is also used for weighting the features\nresults from different scales. In addition, a joint loss function based on\nNegative Log Likelihood (NLL) is used to fit the model probability to multi\nlabeled classes for multi-modal classification task. Furthermore, this work\nprovides a new dataset for food attributes, so-called Yummly48K, extracted from\nthe popular food website, Yummly. Our model is assessed on the constructed\nYummly48K dataset. The experimental results show that our proposed method\nyields 65% and 62% average F1 score on validation and test set which\noutperforming the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:56:32 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 16:44:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sarker", "Md. Mostafa Kamal", ""], ["Jabreel", "Mohammed", ""], ["Rashwan", "Hatem A.", ""], ["Banu", "Syeda Furruka", ""], ["Moreno", "Antonio", ""], ["Radeva", "Petia", ""], ["Puig", "Domenec", ""]]}, {"id": "1805.12098", "submitter": "Man-Chin Sun", "authors": "Man-Chin Sun, Shih-Huan Hsu, Min-Chun Yang, Jen-Hsien Chien", "title": "Context-aware Cascade Attention-based RNN for Video Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition can provide crucial information about the user in many\napplications when building human-computer interaction (HCI) systems. Most of\ncurrent researches on visual emotion recognition are focusing on exploring\nfacial features. However, context information including surrounding environment\nand human body can also provide extra clues to recognize emotion more\naccurately. Inspired by \"sequence to sequence model\" for neural machine\ntranslation, which models input and output sequences by an encoder and a\ndecoder in recurrent neural network (RNN) architecture respectively, a novel\narchitecture, \"CACA-RNN\", is proposed in this work. The proposed network\nconsists of two RNNs in a cascaded architecture to process both context and\nfacial information to perform video emotion classification. Results of the\nmodel were submitted to video emotion recognition sub-challenge in Multimodal\nEmotion Recognition Challenge (MEC2017). CACA-RNN outperforms the MEC2017\nbaseline (mAP of 21.7%): it achieved mAP of 45.51% on the testing set in the\nvideo only challenge.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 17:31:46 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Sun", "Man-Chin", ""], ["Hsu", "Shih-Huan", ""], ["Yang", "Min-Chun", ""], ["Chien", "Jen-Hsien", ""]]}, {"id": "1805.12120", "submitter": "Aditya Balu", "authors": "Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar", "title": "On Consensus-Optimality Trade-offs in Collaborative Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed machine learning, where agents collaboratively learn from\ndiverse private data sets, there is a fundamental tension between consensus and\noptimality. In this paper, we build on recent algorithmic progresses in\ndistributed deep learning to explore various consensus-optimality trade-offs\nover a fixed communication topology. First, we propose the incremental\nconsensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple\nconsensus steps (where each agent communicates information with its neighbors)\nwithin each SGD iteration. Second, we propose the generalized consensus-based\ndistributed SGD (g-CDSGD) algorithm that enables us to navigate the full\nspectrum from complete consensus (all agents agree) to complete disagreement\n(each agent converges to individual model parameters). We analytically\nestablish convergence of the proposed algorithms for strongly convex and\nnonconvex objective functions; we also analyze the momentum variants of the\nalgorithms for the strongly convex case. We support our algorithms via\nnumerical experiments, and demonstrate significant improvements over existing\nmethods for collaborative deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 17:59:24 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Jiang", "Zhanhong", ""], ["Balu", "Aditya", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1805.12152", "submitter": "Dimitris Tsipras", "authors": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner,\n  Aleksander Madry", "title": "Robustness May Be at Odds with Accuracy", "comments": "ICLR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there may exist an inherent tension between the goal of\nadversarial robustness and that of standard generalization. Specifically,\ntraining robust models may not only be more resource-consuming, but also lead\nto a reduction of standard accuracy. We demonstrate that this trade-off between\nthe standard accuracy of a model and its robustness to adversarial\nperturbations provably exists in a fairly simple and natural setting. These\nfindings also corroborate a similar phenomenon observed empirically in more\ncomplex settings. Further, we argue that this phenomenon is a consequence of\nrobust classifiers learning fundamentally different feature representations\nthan standard classifiers. These differences, in particular, seem to result in\nunexpected benefits: the representations learned by robust models tend to align\nbetter with salient data characteristics and human perception.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 18:00:32 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 03:35:11 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 05:09:19 GMT"}, {"version": "v4", "created": "Fri, 30 Aug 2019 22:57:22 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 08:09:25 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tsipras", "Dimitris", ""], ["Santurkar", "Shibani", ""], ["Engstrom", "Logan", ""], ["Turner", "Alexander", ""], ["Madry", "Aleksander", ""]]}, {"id": "1805.12167", "submitter": "Naman Kohli", "authors": "Naman Kohli, Daksha Yadav, Mayank Vatsa, Richa Singh, Afzel Noore", "title": "Supervised Mixed Norm Autoencoder for Kinship Verification in\n  Unconstrained Videos", "comments": "Accepted for publication in Transactions in Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2840880", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying kinship relations has garnered interest due to several\napplications such as organizing and tagging the enormous amount of videos being\nuploaded on the Internet. Existing research in kinship verification primarily\nfocuses on kinship prediction with image pairs. In this research, we propose a\nnew deep learning framework for kinship verification in unconstrained videos\nusing a novel Supervised Mixed Norm regularization Autoencoder (SMNAE). This\nnew autoencoder formulation introduces class-specific sparsity in the weight\nmatrix. The proposed three-stage SMNAE based kinship verification framework\nutilizes the learned spatio-temporal representation in the video frames for\nverifying kinship in a pair of videos. A new kinship video (KIVI) database of\nmore than 500 individuals with variations due to illumination, pose, occlusion,\nethnicity, and expression is collected for this research. It comprises a total\nof 355 true kin video pairs with over 250,000 still frames. The effectiveness\nof the proposed framework is demonstrated on the KIVI database and six existing\nkinship databases. On the KIVI database, SMNAE yields video-based kinship\nverification accuracy of 83.18% which is at least 3.2% better than existing\nalgorithms. The algorithm is also evaluated on six publicly available kinship\ndatabases and compared with best-reported results. It is observed that the\nproposed SMNAE consistently yields best results on all the databases\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 18:27:05 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Kohli", "Naman", ""], ["Yadav", "Daksha", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""]]}, {"id": "1805.12174", "submitter": "Mrigank Rochan", "authors": "Mrigank Rochan, Yang Wang", "title": "Video Summarization by Learning from Unpaired Data", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of video summarization. Given an input raw video, the\ngoal is to select a small subset of key frames from the input video to create a\nshorter summary video that best describes the content of the original video.\nMost of the current state-of-the-art video summarization approaches use\nsupervised learning and require labeled training data. Each training instance\nconsists of a raw input video and its ground truth summary video curated by\nhuman annotators. However, it is very expensive and difficult to create such\nlabeled training examples. To address this limitation, we propose a novel\nformulation to learn video summarization from unpaired data. We present an\napproach that learns to generate optimal video summaries using a set of raw\nvideos ($V$) and a set of summary videos ($S$), where there exists no\ncorrespondence between $V$ and $S$. We argue that this type of data is much\neasier to collect. Our model aims to learn a mapping function $F : V\n\\rightarrow S$ such that the distribution of resultant summary videos from\n$F(V)$ is similar to the distribution of $S$ with the help of an adversarial\nobjective. In addition, we enforce a diversity constraint on $F(V)$ to ensure\nthat the generated video summaries are visually diverse. Experimental results\non two benchmark datasets indicate that our proposed approach significantly\noutperforms other alternative methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 18:48:25 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:50:01 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Rochan", "Mrigank", ""], ["Wang", "Yang", ""]]}, {"id": "1805.12177", "submitter": "Aharon Azulay", "authors": "Aharon Azulay, Yair Weiss", "title": "Why do deep convolutional networks generalize so poorly to small image\n  transformations?", "comments": null, "journal-ref": "JMLR 20(184) 1-25 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to\nsmall image transformations: either because of the convolutional architecture\nor because they were trained using data augmentation. Recently, several authors\nhave shown that this is not the case: small translations or rescalings of the\ninput image can drastically change the network's prediction. In this paper, we\nquantify this phenomena and ask why neither the convolutional architecture nor\ndata augmentation are sufficient to achieve the desired invariance.\nSpecifically, we show that the convolutional architecture does not give\ninvariance since architectures ignore the classical sampling theorem, and data\naugmentation does not give invariance because the CNNs learn to be invariant to\ntransformations only for images that are very similar to typical images from\nthe training set. We discuss two possible solutions to this problem: (1)\nantialiasing the intermediate representations and (2) increasing data\naugmentation and show that they provide only a partial solution at best. Taken\ntogether, our results indicate that the problem of insuring invariance to small\nimage transformations in neural networks while preserving high accuracy remains\nunsolved.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 18:56:33 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 11:33:27 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 07:52:53 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 13:40:12 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Azulay", "Aharon", ""], ["Weiss", "Yair", ""]]}, {"id": "1805.12183", "submitter": "Christopher George", "authors": "Christopher A. George, Pranab Banerjee, Kendra E. Moore", "title": "Context Exploitation using Hierarchical Bayesian Models", "comments": "4 pages; 3 figures; 5 tables", "journal-ref": "Proceedings of the National Fire Control Symposium, February 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how to improve automatic target recognition by\nfusing the naive sensor-level classification decisions with \"intuition,\" or\ncontext, in a mathematically principled way. This is a general approach that is\ncompatible with many definitions of context, but for specificity, we consider\ncontext as co-occurrence in imagery. In particular, we consider images that\ncontain multiple objects identified at various confidence levels. We learn the\npatterns of co-occurrence in each context, then use these patterns as\nhyper-parameters for a Hierarchical Bayesian Model. The result is that\nlow-confidence sensor classification decisions can be dramatically improved by\nfusing those readings with context. We further use hyperpriors to address the\ncase where multiple contexts may be appropriate. We also consider the Bayesian\nNetwork, an alternative to the Hierarchical Bayesian Model, which is\ncomputationally more efficient but assumes that context and sensor readings are\nuncorrelated.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 19:09:11 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["George", "Christopher A.", ""], ["Banerjee", "Pranab", ""], ["Moore", "Kendra E.", ""]]}, {"id": "1805.12219", "submitter": "Bohao Huang", "authors": "Bohao Huang, Daniel Reichman, Leslie M. Collins, Kyle Bradbury and\n  Jordan M. Malof", "title": "Tiling and Stitching Segmentation Output for Remote Sensing: Basic\n  Challenges and Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the application of convolutional neural networks\n(CNNs) for pixel-wise labeling (a.k.a., semantic segmentation) of remote\nsensing imagery (e.g., aerial color or hyperspectral imagery). Remote sensing\nimagery is usually stored in the form of very large images, referred to as\n\"tiles\", which are too large to be segmented directly using most CNNs and their\nassociated hardware. As a result, during label inference, smaller sub-images,\ncalled \"patches\", are processed individually and then \"stitched\" (concatenated)\nback together to create a tile-sized label map. This approach suffers from\ncomputational ineffiency and can result in discontinuities at output\nboundaries. We propose a simple alternative approach in which the input size of\nthe CNN is dramatically increased only during label inference. This does not\navoid stitching altogether, but substantially mitigates its limitations. We\nevaluate the performance of the proposed approach against a vonventional\nstitching approach using two popular segmentation CNN models and two\nlarge-scale remote sensing imagery datasets. The results suggest that the\nproposed approach substantially reduces label inference time, while also\nyielding modest overall label accuracy increases. This approach contributed to\nour wining entry (overall performance) in the INRIA building labeling\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 20:34:07 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 02:10:05 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 14:44:09 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Huang", "Bohao", ""], ["Reichman", "Daniel", ""], ["Collins", "Leslie M.", ""], ["Bradbury", "Kyle", ""], ["Malof", "Jordan M.", ""]]}, {"id": "1805.12234", "submitter": "Noel Codella", "authors": "Noel C. F. Codella, Chung-Ching Lin, Allan Halpern, Michael Hind,\n  Rogerio Feris, and John R. Smith", "title": "Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma\n  Classification in Dermoscopic Images", "comments": "Presented at MICCAI 2018, Workshop on Interpretability of Machine\n  Intelligence in Medical Image Computing (IMIMIC): https://imimic.bitbucket.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated dermoscopic image analysis has witnessed rapid growth in diagnostic\nperformance. Yet adoption faces resistance, in part, because no evidence is\nprovided to support decisions. In this work, an approach for evidence-based\nclassification is presented. A feature embedding is learned with CNNs,\ntriplet-loss, and global average pooling, and used to classify via kNN search.\nEvidence is provided as both the discovered neighbors, as well as localized\nimage regions most relevant to measuring distance between query and neighbors.\nTo ensure that results are relevant in terms of both label accuracy and human\nvisual similarity for any skill level, a novel hierarchical triplet logic is\nimplemented to jointly learn an embedding according to disease labels and\nnon-expert similarity. Results are improved over baselines trained on disease\nlabels alone, as well as standard multiclass loss. Quantitative relevance of\nresults, according to non-expert similarity, as well as localized image\nregions, are also significantly improved.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 21:28:39 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 20:16:53 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 18:01:48 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Codella", "Noel C. F.", ""], ["Lin", "Chung-Ching", ""], ["Halpern", "Allan", ""], ["Hind", "Michael", ""], ["Feris", "Rogerio", ""], ["Smith", "John R.", ""]]}, {"id": "1805.12243", "submitter": "Henglai Wei", "authors": "Henglai Wei, Xiaochuan Yin, Penghong Lin", "title": "Novel Video Prediction for Large-scale Scene using Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making predictions of future frames is a critical challenge in autonomous\ndriving research. Most of the existing methods for video prediction attempt to\ngenerate future frames in simple and fixed scenes. In this paper, we propose a\nnovel and effective optical flow conditioned method for the task of video\nprediction with an application to complex urban scenes. In contrast with\nprevious work, the prediction model only requires video sequences and optical\nflow sequences for training and testing. Our method uses the rich\nspatial-temporal features in video sequences. The method takes advantage of the\nmotion information extracting from optical flow maps between neighbor images as\nwell as previous images. Empirical evaluations on the KITTI dataset and the\nCityscapes dataset demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 22:11:54 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Wei", "Henglai", ""], ["Yin", "Xiaochuan", ""], ["Lin", "Penghong", ""]]}, {"id": "1805.12254", "submitter": "Aditya Balu", "authors": "Sambit Ghadai, Xian Lee, Aditya Balu, Soumik Sarkar and Adarsh\n  Krishnamurthy", "title": "Multi-level 3D CNN for Learning Multi-scale Spatial Features", "comments": "CVPR 2019 workshop on Deep Learning for Geometric Shape Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object recognition accuracy can be improved by learning the multi-scale\nspatial features from 3D spatial geometric representations of objects such as\npoint clouds, 3D models, surfaces, and RGB-D data. Current deep learning\napproaches learn such features either using structured data representations\n(voxel grids and octrees) or from unstructured representations (graphs and\npoint clouds). Learning features from such structured representations is\nlimited by the restriction on resolution and tree depth while unstructured\nrepresentations creates a challenge due to non-uniformity among data samples.\nIn this paper, we propose an end-to-end multi-level learning approach on a\nmulti-level voxel grid to overcome these drawbacks. To demonstrate the utility\nof the proposed multi-level learning, we use a multi-level voxel representation\nof 3D objects to perform object recognition. The multi-level voxel\nrepresentation consists of a coarse voxel grid that contains volumetric\ninformation of the 3D object. In addition, each voxel in the coarse grid that\ncontains a portion of the object boundary is subdivided into multiple\nfine-level voxel grids. The performance of our multi-level learning algorithm\nfor object recognition is comparable to dense voxel representations while using\nsignificantly lower memory.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 22:53:28 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 18:37:27 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Ghadai", "Sambit", ""], ["Lee", "Xian", ""], ["Balu", "Aditya", ""], ["Sarkar", "Soumik", ""], ["Krishnamurthy", "Adarsh", ""]]}, {"id": "1805.12262", "submitter": "Ghalia Hemrit", "authors": "Ghalia Hemrit, Graham D. Finlayson, Arjan Gijsenij, Peter Gehler,\n  Simone Bianco, Brian Funt, Mark Drew and Lilong Shi", "title": "Rehabilitating the ColorChecker Dataset for Illuminant Estimation", "comments": "4 pages, 3 figures, 2 tables, Proceedings of the 26th Color and\n  Imaging Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work, it was shown that there is a curious problem with the\nbenchmark ColorChecker dataset for illuminant estimation. To wit, this dataset\nhas at least 3 different sets of ground-truths. Typically, for a single\nalgorithm a single ground-truth is used. But then different algorithms, whose\nperformance is measured with respect to different ground-truths, are compared\nagainst each other and then ranked. This makes no sense. We show in this paper\nthat there are also errors in how each ground-truth set was calculated. As a\nresult, all performance rankings based on the ColorChecker dataset - and there\nare scores of these - are inaccurate.\n  In this paper, we re-generate a new 'recommended' set of ground-truth based\non the calculation methodology described by Shi and Funt. We then review the\nperformance evaluation of a range of illuminant estimation algorithms. Compared\nwith the legacy ground-truths, we find that the difference in how algorithms\nperform can be large, with many local rankings of algorithms being reversed.\n  Finally, we draw the readers attention to our new 'open' data repository\nwhich, we hope, will allow the ColorChecker set to be rehabilitated and once\nagain to become a useful benchmark for illuminant estimation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 23:41:17 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 11:30:31 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 16:53:27 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Hemrit", "Ghalia", ""], ["Finlayson", "Graham D.", ""], ["Gijsenij", "Arjan", ""], ["Gehler", "Peter", ""], ["Bianco", "Simone", ""], ["Funt", "Brian", ""], ["Drew", "Mark", ""], ["Shi", "Lilong", ""]]}, {"id": "1805.12277", "submitter": "Mahsa Baktashmotlagh", "authors": "Mahsa Baktashmotlagh, Masoud Faraki, Tom Drummond, Mathieu Salzmann", "title": "Learning Factorized Representations for Open-set Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation for visual recognition has undergone great progress in the\npast few years. Nevertheless, most existing methods work in the so-called\nclosed-set scenario, assuming that the classes depicted by the target images\nare exactly the same as those of the source domain. In this paper, we tackle\nthe more challenging, yet more realistic case of open-set domain adaptation,\nwhere new, unknown classes can be present in the target data. While, in the\nunsupervised scenario, one cannot expect to be able to identify each specific\nnew class, we aim to automatically detect which samples belong to these new\nclasses and discard them from the recognition process. To this end, we rely on\nthe intuition that the source and target samples depicting the known classes\ncan be generated by a shared subspace, whereas the target samples from unknown\nclasses come from a different, private subspace. We therefore introduce a\nframework that factorizes the data into shared and private parts, while\nencouraging the shared representation to be discriminative. Our experiments on\nstandard benchmarks evidence that our approach significantly outperforms the\nstate-of-the-art in open-set domain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 01:08:20 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Baktashmotlagh", "Mahsa", ""], ["Faraki", "Masoud", ""], ["Drummond", "Tom", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1805.12279", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Umut \\c{S}im\\c{s}ekli, M. Onur Eken, Slobodan Ilic", "title": "Bayesian Pose Graph Optimization via Bingham Distributions and Tempered\n  Geodesic MCMC", "comments": "Published at NeurIPS 2018, 25 pages with supplements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm\nfor initializing pose graph optimization problems, arising in various scenarios\nsuch as SFM (structure from motion) or SLAM (simultaneous localization and\nmapping). TG-MCMC is first of its kind as it unites asymptotically global\nnon-convex optimization on the spherical manifold of quaternions with posterior\nsampling, in order to provide both reliable initial poses and uncertainty\nestimates that are informative about the quality of individual solutions. We\ndevise rigorous theoretical convergence guarantees for our method and\nextensively evaluate it on synthetic and real benchmark datasets. Besides its\nelegance in formulation and theory, we show that our method is robust to\nmissing data, noise and the estimated uncertainties capture intuitive\nproperties of the data.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 01:14:57 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:23:30 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Birdal", "Tolga", ""], ["\u015eim\u015fekli", "Umut", ""], ["Eken", "M. Onur", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1805.12289", "submitter": "Shuo Liu", "authors": "Yuchen Yang, Shuo Liu, Wei Ma, Qiuyuan Wang, Zheng Liu", "title": "Efficient Traffic-Sign Recognition with Scale-aware CNN", "comments": "This paper has been published on BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a Traffic Sign Recognition (TSR) system, which can fast\nand accurately recognize traffic signs of different sizes in images. The system\nconsists of two well-designed Convolutional Neural Networks (CNNs), one for\nregion proposals of traffic signs and one for classification of each region. In\nthe proposal CNN, a Fully Convolutional Network (FCN) with a dual multi-scale\narchitecture is proposed to achieve scale invariant detection. In training the\nproposal network, a modified \"Online Hard Example Mining\" (OHEM) scheme is\nadopted to suppress false positives. The classification network fuses\nmulti-scale features as representation and adopts an \"Inception\" module for\nefficiency. We evaluate the proposed TSR system and its components with\nextensive experiments. Our method obtains $99.88\\%$ precision and $96.61\\%$\nrecall on the Swedish Traffic Signs Dataset (STSD), higher than\nstate-of-the-art methods. Besides, our system is faster and more lightweight\nthan state-of-the-art deep learning networks for traffic sign recognition.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 02:09:20 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Yang", "Yuchen", ""], ["Liu", "Shuo", ""], ["Ma", "Wei", ""], ["Wang", "Qiuyuan", ""], ["Liu", "Zheng", ""]]}, {"id": "1805.12295", "submitter": "David Minnen", "authors": "David Minnen, George Toderici, Saurabh Singh, Sung Jin Hwang, Michele\n  Covell", "title": "Image-Dependent Local Entropy Models for Learned Image Compression", "comments": null, "journal-ref": "International Conference on Image Processing 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leading approach for image compression with artificial neural networks\n(ANNs) is to learn a nonlinear transform and a fixed entropy model that are\noptimized for rate-distortion performance. We show that this approach can be\nsignificantly improved by incorporating spatially local, image-dependent\nentropy models. The key insight is that existing ANN-based methods learn an\nentropy model that is shared between the encoder and decoder, but they do not\ntransmit any side information that would allow the model to adapt to the\nstructure of a specific image. We present a method for augmenting ANN-based\nimage coders with image-dependent side information that leads to a 17.8% rate\nreduction over a state-of-the-art ANN-based baseline model on a standard\nevaluation set, and 70-98% reductions on images with low visual complexity that\nare poorly captured by a fixed, global entropy model.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 02:32:48 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Minnen", "David", ""], ["Toderici", "George", ""], ["Singh", "Saurabh", ""], ["Hwang", "Sung Jin", ""], ["Covell", "Michele", ""]]}, {"id": "1805.12301", "submitter": "Benjamin Chidester", "authors": "Benjamin Chidester, Minh N. Do, Jian Ma", "title": "Rotation Equivariance and Invariance in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of neural networks can be significantly improved by encoding\nknown invariance for particular tasks. Many image classification tasks, such as\nthose related to cellular imaging, exhibit invariance to rotation. We present a\nnovel scheme using the magnitude response of the 2D-discrete-Fourier transform\n(2D-DFT) to encode rotational invariance in neural networks, along with a new,\nefficient convolutional scheme for encoding rotational equivariance throughout\nconvolutional layers. We implemented this scheme for several image\nclassification tasks and demonstrated improved performance, in terms of\nclassification accuracy, time required to train the model, and robustness to\nhyperparameter selection, over a standard CNN and another state-of-the-art\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 03:13:41 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Chidester", "Benjamin", ""], ["Do", "Minh N.", ""], ["Ma", "Jian", ""]]}, {"id": "1805.12302", "submitter": "Avishek Bose", "authors": "Avishek Joey Bose and Parham Aarabi", "title": "Adversarial Attacks on Face Detectors using Neural Net based Constrained\n  Optimization", "comments": "Accepted to IEEE MMSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks involve adding, small, often imperceptible, perturbations\nto inputs with the goal of getting a machine learning model to misclassifying\nthem. While many different adversarial attack strategies have been proposed on\nimage classification models, object detection pipelines have been much harder\nto break. In this paper, we propose a novel strategy to craft adversarial\nexamples by solving a constrained optimization problem using an adversarial\ngenerator network. Our approach is fast and scalable, requiring only a forward\npass through our trained generator network to craft an adversarial sample.\nUnlike in many attack strategies, we show that the same trained generator is\ncapable of attacking new images without explicitly optimizing on them. We\nevaluate our attack on a trained Faster R-CNN face detector on the cropped\n300-W face dataset where we manage to reduce the number of detected faces to\n$0.5\\%$ of all originally detected faces. In a different experiment, also on\n300-W, we demonstrate the robustness of our attack to a JPEG compression based\ndefense typical JPEG compression level of $75\\%$ reduces the effectiveness of\nour attack from only $0.5\\%$ of detected faces to a modest $5.0\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 03:18:32 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Bose", "Avishek Joey", ""], ["Aarabi", "Parham", ""]]}, {"id": "1805.12323", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Bolei Zhou, Diondra Peck, Scott Hsieh, Vandana Dialani,\n  Lester Mackey, Genevieve Patterson", "title": "DeepMiner: Discovering Interpretable Representations for Mammogram\n  Classification and Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepMiner, a framework to discover interpretable representations\nin deep neural networks and to build explanations for medical predictions. By\nprobing convolutional neural networks (CNNs) trained to classify cancer in\nmammograms, we show that many individual units in the final convolutional layer\nof a CNN respond strongly to diseased tissue concepts specified by the BI-RADS\nlexicon. After expert annotation of the interpretable units, our proposed\nmethod is able to generate explanations for CNN mammogram classification that\nare correlated with ground truth radiology reports on the DDSM dataset. We show\nthat DeepMiner not only enables better understanding of the nuances of CNN\nclassification decisions, but also possibly discovers new visual knowledge\nrelevant to medical diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 05:42:45 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Wu", "Jimmy", ""], ["Zhou", "Bolei", ""], ["Peck", "Diondra", ""], ["Hsieh", "Scott", ""], ["Dialani", "Vandana", ""], ["Mackey", "Lester", ""], ["Patterson", "Genevieve", ""]]}, {"id": "1805.12326", "submitter": "Timo Stoffregen", "authors": "Timo Stoffregen and Lindsay Kleeman", "title": "Simultaneous Optical Flow and Segmentation (SOFAS) using Dynamic Vision\n  Sensor", "comments": null, "journal-ref": "Australasian Conference on Robotics and Automation 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm (SOFAS) to estimate the optical flow of events\ngenerated by a dynamic vision sensor (DVS). Where traditional cameras produce\nframes at a fixed rate, DVSs produce asynchronous events in response to\nintensity changes with a high temporal resolution. Our algorithm uses the fact\nthat events are generated by edges in the scene to not only estimate the\noptical flow but also to simultaneously segment the image into objects which\nare travelling at the same velocity. This way it is able to avoid the aperture\nproblem which affects other implementations such as Lucas-Kanade. Finally, we\nshow that SOFAS produces more accurate results than traditional optic flow\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 05:57:09 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Stoffregen", "Timo", ""], ["Kleeman", "Lindsay", ""]]}, {"id": "1805.12353", "submitter": "Daigo Shoji", "authors": "Daigo Shoji, Rina Noguchi, Shizuka Otsuki, Hideitsu Hino", "title": "Classification of volcanic ash particles using a convolutional neural\n  network and probability", "comments": "25 pages, 4 tables, 7 figurs, published in Scientific Reports", "journal-ref": "D. Shoji, R. Noguchi, S. Otsuki and H. Hino. Classification of\n  volcanic ash particles using a convolutional neural network and probability.\n  Scientific Reports 8, 8111 (2018)", "doi": "10.1038/s41598-018-26200-2", "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of volcanic ash are typically performed either by qualitatively\nclassifying ash particles by eye or by quantitatively parameterizing its shape\nand texture. While complex shapes can be classified through qualitative\nanalyses, the results are subjective due to the difficulty of categorizing\ncomplex shapes into a single class. Although quantitative analyses are\nobjective, selection of shape parameters is required. Here, we applied a\nconvolutional neural network (CNN) for the classification of volcanic ash.\nFirst, we defined four basal particle shapes (blocky, vesicular, elongated,\nrounded) generated by different eruption mechanisms (e.g., brittle\nfragmentation), and then trained the CNN using particles composed of only one\nbasal shape. The CNN could recognize the basal shapes with over 90% accuracy.\nUsing the trained network, we classified ash particles composed of multiple\nbasal shapes based on the output of the network, which can be interpreted as a\nmixing ratio of the four basal shapes. Clustering of samples by the averaged\nprobabilities and the intensity is consistent with the eruption type. The\nmixing ratio output by the CNN can be used to quantitatively classify complex\nshapes in nature without categorizing forcibly and without the need for shape\nparameters, which may lead to a new taxonomy.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 07:30:48 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Shoji", "Daigo", ""], ["Noguchi", "Rina", ""], ["Otsuki", "Shizuka", ""], ["Hino", "Hideitsu", ""]]}, {"id": "1805.12358", "submitter": "Jie Chen", "authors": "Jie Chen, Junhui Hou, and Lap-Pui Chau", "title": "Light Field Denoising via Anisotropic Parallax Analysis in a CNN\n  Framework", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2018.2861212", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field (LF) cameras provide perspective information of scenes by taking\ndirectional measurements of the focusing light rays. The raw outputs are\nusually dark with additive camera noise, which impedes subsequent processing\nand applications. We propose a novel LF denoising framework based on\nanisotropic parallax analysis (APA). Two convolutional neural networks are\njointly designed for the task: first, the structural parallax synthesis network\npredicts the parallax details for the entire LF based on a set of anisotropic\nparallax features. These novel features can efficiently capture the high\nfrequency perspective components of a LF from noisy observations. Second, the\nview-dependent detail compensation network restores non-Lambertian variation to\neach LF view by involving view-specific spatial energies. Extensive experiments\nshow that the proposed APA LF denoiser provides a much better denoising\nperformance than state-of-the-art methods in terms of visual quality and in\npreservation of parallax details.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 07:41:37 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 02:36:57 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Chen", "Jie", ""], ["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1805.12369", "submitter": "Zhanxing Zhu", "authors": "Ju Xu, Zhanxing Zhu", "title": "Reinforced Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most artificial intelligence models have limiting ability to solve new tasks\nfaster, without forgetting previously acquired knowledge. The recently emerging\nparadigm of continual learning aims to solve this issue, in which the model\nlearns various tasks in a sequential fashion. In this work, a novel approach\nfor continual learning is proposed, which searches for the best neural\narchitecture for each coming task via sophisticatedly designed reinforcement\nlearning strategies. We name it as Reinforced Continual Learning. Our method\nnot only has good performance on preventing catastrophic forgetting but also\nfits new tasks well. The experiments on sequential classification tasks for\nvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach\noutperforms existing continual learning alternatives for deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 08:11:12 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Xu", "Ju", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1805.12371", "submitter": "Dharin Parekh Mr.", "authors": "Dharin Parekh, Ankitesh Gupta, Shharrnam Chhatpar, Anmol Yash Kumar,\n  Manasi Kulkarni", "title": "Lip Reading Using Convolutional Auto Encoders as Feature Extractor", "comments": "6 pages, 6 tables, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition of speech using the lip movement is called Lip-reading.\nRecent developments in this nascent field uses different neural networks as\nfeature extractors which serve as input to a model which can map the temporal\nrelationship and classify. Though end to end sentence level Lip-reading is the\ncurrent trend, we proposed a new model which employs word level classification\nand breaks the set benchmarks for standard datasets. In our model we use\nconvolutional autoencoders as feature extractors which are then fed to a Long\nshort-term memory model. We tested our proposed model on BBC's LRW dataset,\nMIRACL-VC1 and GRID dataset. Achieving a classification accuracy of 98% on\nMIRACL-VC1 as compared to 93.4% of the set benchmark (Rekik et al., 2014). On\nBBC's LRW the proposed model performed better than the baseline model of\nconvolutional neural networks and Long short-term memory model (Garg et al.,\n2016). Showing the features learned by the models we clearly indicate how the\nproposed model works better than the baseline model. The same model can also be\nextended for end to end sentence level classification.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 08:20:12 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Parekh", "Dharin", ""], ["Gupta", "Ankitesh", ""], ["Chhatpar", "Shharrnam", ""], ["Kumar", "Anmol Yash", ""], ["Kulkarni", "Manasi", ""]]}, {"id": "1805.12395", "submitter": "Adel Hafiane", "authors": "M.Dian. Bah, Adel Hafiane, Raphael Canals", "title": "Deep Learning with unsupervised data labeling for weeds detection on UAV\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern agriculture, usually weeds control consists in spraying herbicides\nall over the agricultural field. This practice involves significant waste and\ncost of herbicide for farmers and environmental pollution. One way to reduce\nthe cost and environmental impact is to allocate the right doses of herbicide\nat the right place and at the right time (Precision Agriculture). Nowadays,\nUnmanned Aerial Vehicle (UAV) is becoming an interesting acquisition system for\nweeds localization and management due to its ability to obtain the images of\nthe entire agricultural field with a very high spatial resolution and at low\ncost. Despite the important advances in UAV acquisition systems, automatic\nweeds detection remains a challenging problem because of its strong similarity\nwith the crops. Recently Deep Learning approach has shown impressive results in\ndifferent complex classification problem. However, this approach needs a\ncertain amount of training data but, creating large agricultural datasets with\npixel-level annotations by expert is an extremely time consuming task. In this\npaper, we propose a novel fully automatic learning method using Convolutional\nNeuronal Networks (CNNs) with unsupervised training dataset collection for\nweeds detection from UAV images. The proposed method consists in three main\nphases. First we automatically detect the crop lines and using them to identify\nthe interline weeds. In the second phase, interline weeds are used to\nconstitute the training dataset. Finally, we performed CNNs on this dataset to\nbuild a model able to detect the crop and weeds in the images. The results\nobtained are comparable to the traditional supervised training data labeling.\nThe accuracy gaps are 1.5% in the spinach field and 6% in the bean field.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 09:43:40 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Bah", "M. Dian.", ""], ["Hafiane", "Adel", ""], ["Canals", "Raphael", ""]]}, {"id": "1805.12415", "submitter": "Sergi Valverde", "authors": "Sergi Valverde, Mostafa Salem, Mariano Cabezas, Deborah Pareto, Joan\n  C. Vilanova, Llu\\'is Rami\\'o-Torrent\\`a, \\`Alex Rovira, Joaquim Salvi, Arnau\n  Oliver and Xavier Llad\\'o", "title": "One-shot domain adaptation in multiple sclerosis lesion segmentation\n  using convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, several convolutional neural network (CNN) methods have been\nproposed for the automated white matter lesion segmentation of multiple\nsclerosis (MS) patient images, due to their superior performance compared with\nthose of other state-of-the-art methods. However, the accuracies of CNN methods\ntend to decrease significantly when evaluated on different image domains\ncompared with those used for training, which demonstrates the lack of\nadaptability of CNNs to unseen imaging data. In this study, we analyzed the\neffect of intensity domain adaptation on our recently proposed CNN-based MS\nlesion segmentation method. Given a source model trained on two public MS\ndatasets, we investigated the transferability of the CNN model when applied to\nother MRI scanners and protocols, evaluating the minimum number of annotated\nimages needed from the new domain and the minimum number of layers needed to\nre-train to obtain comparable accuracy. Our analysis comprised MS patient data\nfrom both a clinical center and the public ISBI2015 challenge database, which\npermitted us to compare the domain adaptation capability of our model to that\nof other state-of-the-art methods. For the ISBI2015 challenge, our one-shot\ndomain adaptation model trained using only a single image showed a performance\nsimilar to that of other CNN methods that were fully trained using the entire\navailable training set, yielding a comparable human expert rater performance.\nWe believe that our experiments will encourage the MS community to incorporate\nits use in different clinical settings with reduced amounts of annotated data.\nThis approach could be meaningful not only in terms of the accuracy in\ndelineating MS lesions but also in the related reductions in time and economic\ncosts derived from manual lesion labeling.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 10:50:11 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Valverde", "Sergi", ""], ["Salem", "Mostafa", ""], ["Cabezas", "Mariano", ""], ["Pareto", "Deborah", ""], ["Vilanova", "Joan C.", ""], ["Rami\u00f3-Torrent\u00e0", "Llu\u00eds", ""], ["Rovira", "\u00c0lex", ""], ["Salvi", "Joaquim", ""], ["Oliver", "Arnau", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1805.12443", "submitter": "Shafeeq Elanattil Mr", "authors": "Agniva Sengupta, Shafeeq Elanattil", "title": "New Feature Detection Mechanism for Extended Kalman Filter Based\n  Monocular SLAM with 1-Point RANSAC", "comments": "Accepted in Third International Conference of Mining Intelligence and\n  Knowledge Exploration (MIKE) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a different approach of feature point detection for improving the\naccuracy of SLAM using single, monocular camera. Traditionally, Harris Corner\ndetection, SURF or FAST corner detectors are used for finding feature points of\ninterest in the image. We replace this with another approach, which involves\nbuilding a non-linear scale-space representation of images using Perona and\nMalik Diffusion equation and computing the scale normalized Hessian at multiple\nscale levels (KAZE feature). The feature points so detected are used to\nestimate the state and pose of a mono camera using extended Kalman filter. By\nusing accelerated KAZE features and a more rigorous feature rejection routine\ncombined with 1-point RANSAC for outlier rejection, short baseline matching of\nfeatures are significantly improved, even with a lesser number of feature\npoints, especially in the presence of motion blur. We present a comparative\nstudy of our proposal with FAST and show improved localization accuracy in\nterms of absolute trajectory error.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 12:49:13 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Sengupta", "Agniva", ""], ["Elanattil", "Shafeeq", ""]]}, {"id": "1805.12462", "submitter": "Eitan Richardson", "authors": "Eitan Richardson and Yair Weiss", "title": "On GANs and GMMs", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A longstanding problem in machine learning is to find unsupervised methods\nthat can learn the statistical structure of high dimensional signals. In recent\nyears, GANs have gained much attention as a possible solution to the problem,\nand in particular have shown the ability to generate remarkably realistic high\nresolution sampled images. At the same time, many authors have pointed out that\nGANs may fail to model the full distribution (\"mode collapse\") and that using\nthe learned models for anything other than generating samples may be very\ndifficult. In this paper, we examine the utility of GANs in learning\nstatistical models of images by comparing them to perhaps the simplest\nstatistical model, the Gaussian Mixture Model. First, we present a simple\nmethod to evaluate generative models based on relative proportions of samples\nthat fall into predetermined bins. Unlike previous automatic methods for\nevaluating models, our method does not rely on an additional neural network nor\ndoes it require approximating intractable computations. Second, we compare the\nperformance of GANs to GMMs trained on the same datasets. While GMMs have\npreviously been shown to be successful in modeling small patches of images, we\nshow how to train them on full sized images despite the high dimensionality.\nOur results show that GMMs can generate realistic samples (although less sharp\nthan those of GANs) but also capture the full distribution, which GANs fail to\ndo. Furthermore, GMMs allow efficient inference and explicit representation of\nthe underlying statistical structure. Finally, we discuss how GMMs can be used\nto generate sharp images.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 13:37:59 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 18:58:50 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Richardson", "Eitan", ""], ["Weiss", "Yair", ""]]}, {"id": "1805.12467", "submitter": "Naoya Sogi", "authors": "Naoya Sogi, Taku Nakayama, Kazuhiro Fukui", "title": "A Method Based on Convex Cone Model for Image-Set Classification with\n  CNN Features", "comments": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for image-set classification based on\nconvex cone models, focusing on the effectiveness of convolutional neural\nnetwork (CNN) features as inputs. CNN features have non-negative values when\nusing the rectified linear unit as an activation function. This naturally leads\nus to model a set of CNN features by a convex cone and measure the geometric\nsimilarity of convex cones for classification. To establish this framework, we\nsequentially define multiple angles between two convex cones by repeating the\nalternating least squares method and then define the geometric similarity\nbetween the cones using the obtained angles. Moreover, to enhance our method,\nwe introduce a discriminant space, maximizing the between-class variance (gaps)\nand minimizes the within-class variance of the projected convex cones onto the\ndiscriminant space, similar to a Fisher discriminant analysis. Finally,\nclassification is based on the similarity between projected convex cones. The\neffectiveness of the proposed method was demonstrated experimentally using a\nprivate, multi-view hand shape dataset and two public databases.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 13:47:09 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Sogi", "Naoya", ""], ["Nakayama", "Taku", ""], ["Fukui", "Kazuhiro", ""]]}, {"id": "1805.12487", "submitter": "Seong Joon Oh", "authors": "Edgar Tretschk, Seong Joon Oh, Mario Fritz", "title": "Sequential Attacks on Agents for Long-Term Adversarial Goals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has advanced greatly in the past few years with\nthe employment of effective deep neural networks (DNNs) on the policy networks.\nWith the great effectiveness came serious vulnerability issues with DNNs that\nsmall adversarial perturbations on the input can change the output of the\nnetwork. Several works have pointed out that learned agents with a DNN policy\nnetwork can be manipulated against achieving the original task through a\nsequence of small perturbations on the input states. In this paper, we\ndemonstrate furthermore that it is also possible to impose an arbitrary\nadversarial reward on the victim policy network through a sequence of attacks.\nOur method involves the latest adversarial attack technique, Adversarial\nTransformer Network (ATN), that learns to generate the attack and is easy to\nintegrate into the policy network. As a result of our attack, the victim agent\nis misguided to optimise for the adversarial reward over time. Our results\nexpose serious security threats for RL applications in safety-critical systems\nincluding drones, medical analysis, and self-driving cars.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 14:22:09 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 17:31:14 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Tretschk", "Edgar", ""], ["Oh", "Seong Joon", ""], ["Fritz", "Mario", ""]]}, {"id": "1805.12506", "submitter": "Arno Solin", "authors": "Santiago Cort\\'es Reina and Arno Solin and Juho Kannala", "title": "Robust Gyroscope-Aided Camera Self-Calibration", "comments": "Appearing in Proceedings of the International Conference on\n  Information Fusion (FUSION 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera calibration for estimating the intrinsic parameters and lens\ndistortion is a prerequisite for various monocular vision applications\nincluding feature tracking and video stabilization. This application paper\nproposes a model for estimating the parameters on the fly by fusing gyroscope\nand camera data, both readily available in modern day smartphones. The model is\nbased on joint estimation of visual feature positions, camera parameters, and\nthe camera pose, the movement of which is assumed to follow the movement\npredicted by the gyroscope. Our model assumes the camera movement to be free,\nbut continuous and differentiable, and individual features are assumed to stay\nstationary. The estimation is performed online using an extended Kalman filter,\nand it is shown to outperform existing methods in robustness and insensitivity\nto initialization. We demonstrate the method using simulated data and empirical\ndata from an iPad.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 15:08:35 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Reina", "Santiago Cort\u00e9s", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1805.12510", "submitter": "Alessandro Corbetta", "authors": "Werner Kroneman, Alessandro Corbetta, Federico Toschi", "title": "Accurate pedestrian localization in overhead depth images via\n  Height-Augmented HOG", "comments": "8 pages", "journal-ref": "Collective Dynamics, v. 5, p. 33-40, mar. 2020. ISSN 2366-8539", "doi": "10.17815/CD.2020.30", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenge of reliably and automatically localizing pedestrians\nin real-life conditions through overhead depth imaging at unprecedented\nhigh-density conditions. Leveraging upon a combination of Histogram of Oriented\nGradients-like feature descriptors, neural networks, data augmentation and\ncustom data annotation strategies, this work contributes a robust and scalable\nmachine learning-based localization algorithm, which delivers near-human\nlocalization performance in real-time, even with local pedestrian density of\nabout 3 ped/m2, a case in which most state-of-the art algorithms degrade\nsignificantly in performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 15:16:55 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Kroneman", "Werner", ""], ["Corbetta", "Alessandro", ""], ["Toschi", "Federico", ""]]}, {"id": "1805.12521", "submitter": "Jae Kyu Choi", "authors": "Chenglong Bao, Jae Kyu Choi, Bin Dong", "title": "Whole Brain Susceptibility Mapping Using Harmonic Incompatibility\n  Removal", "comments": "Accepted for publication in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative susceptibility mapping (QSM) aims to visualize the three\ndimensional susceptibility distribution by solving the field-to-source inverse\nproblem using the phase data in magnetic resonance signal. However, the inverse\nproblem is ill-posed since the Fourier transform of integral kernel has zeroes\nin the frequency domain. Although numerous regularization based models have\nbeen proposed to overcome this problem, the incompatibility in the field data\nhas not received enough attention, which leads to deterioration of the\nrecovery. In this paper, we show that the data acquisition process of QSM\ninherently generates a harmonic incompatibility in the measured local field.\nBased on such discovery, we propose a novel regularization based susceptibility\nreconstruction model with an additional sparsity based regularization term on\nthe harmonic incompatibility. Numerical experiments show that the proposed\nmethod achieves better performance than the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 15:40:54 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 13:16:54 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Bao", "Chenglong", ""], ["Choi", "Jae Kyu", ""], ["Dong", "Bin", ""]]}, {"id": "1805.12526", "submitter": "Anjali Balagopal", "authors": "Anjali Balagopal, Samaneh Kazemifar, Dan Nguyen, Mu-Han Lin, Raquibul\n  Hannan, Amir Owrangi, Steve Jiang", "title": "Fully Automated Organ Segmentation in Male Pelvic CT Images", "comments": "21 pages; 11 figures; 4 tables", "journal-ref": "Balagopal A, Kazemifar S, Nguyen D, Lin M H, Hannan R, Owrangi A\n  and Jiang S 2018 Fully automated organ segmentation in male pelvic CT images\n  Phys. Med. Biol. 63 245015", "doi": "10.1088/1361-6560/aaf11c", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of prostate and surrounding organs at risk is important\nfor prostate cancer radiotherapy treatment planning. We present a fully\nautomated workflow for male pelvic CT image segmentation using deep learning.\nThe architecture consists of a 2D localization network followed by a 3D\nsegmentation network for volumetric segmentation of prostate, bladder, rectum,\nand femoral heads. We used a multi-channel 2D U-Net followed by a 3D U-Net with\nencoding arm modified with aggregated residual networks, known as ResNeXt. The\nmodels were trained and tested on a pelvic CT image dataset comprising 136\npatients. Test results show that 3D U-Net based segmentation achieves mean (SD)\nDice coefficient values of 90 (2.0)% ,96 (3.0)%, 95 (1.3)%, 95 (1.5)%, and 84\n(3.7)% for prostate, left femoral head, right femoral head, bladder, and\nrectum, respectively, using the proposed fully automated segmentation method.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 15:56:22 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:54:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Balagopal", "Anjali", ""], ["Kazemifar", "Samaneh", ""], ["Nguyen", "Dan", ""], ["Lin", "Mu-Han", ""], ["Hannan", "Raquibul", ""], ["Owrangi", "Amir", ""], ["Jiang", "Steve", ""]]}, {"id": "1805.12549", "submitter": "Weizhe Hua", "authors": "Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, and G. Edward\n  Suh", "title": "Channel Gating Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces channel gating, a dynamic, fine-grained, and\nhardware-efficient pruning scheme to reduce the computation cost for\nconvolutional neural networks (CNNs). Channel gating identifies regions in the\nfeatures that contribute less to the classification result, and skips the\ncomputation on a subset of the input channels for these ineffective regions.\nUnlike static network pruning, channel gating optimizes CNN inference at\nrun-time by exploiting input-specific characteristics, which allows\nsubstantially reducing the compute cost with almost no accuracy loss. We\nexperimentally show that applying channel gating in state-of-the-art networks\nachieves 2.7-8.0$\\times$ reduction in floating-point operations (FLOPs) and\n2.0-4.4$\\times$ reduction in off-chip memory accesses with a minimal accuracy\nloss on CIFAR-10. Combining our method with knowledge distillation reduces the\ncompute cost of ResNet-18 by 2.6$\\times$ without accuracy drop on ImageNet. We\nfurther demonstrate that channel gating can be realized in hardware\nefficiently. Our approach exhibits sparsity patterns that are well-suited to\ndense systolic arrays with minimal additional hardware. We have designed an\naccelerator for channel gating networks, which can be implemented using either\nFPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our\naccelerator achieves an encouraging speedup of 2.4$\\times$ on average, with a\ntheoretical FLOP reduction of 2.8$\\times$.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 20:11:56 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 23:53:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Hua", "Weizhe", ""], ["Zhou", "Yuan", ""], ["De Sa", "Christopher", ""], ["Zhang", "Zhiru", ""], ["Suh", "G. Edward", ""]]}, {"id": "1805.12564", "submitter": "Xiang Li", "authors": "Yu Zhao, Xiang Li, Wei Zhang, Shijie Zhao, Milad Makkie, Mo Zhang,\n  Quanzheng Li, Tianming Liu", "title": "Modeling 4D fMRI Data via Spatio-Temporal Convolutional Neural Networks\n  (ST-CNN)", "comments": "Yu Zhao and Xiang Li contribute equally to this work", "journal-ref": null, "doi": "10.1109/TCDS.2019.2916916", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous modeling of the spatio-temporal variation patterns of brain\nfunctional network from 4D fMRI data has been an important yet challenging\nproblem for the field of cognitive neuroscience and medical image analysis.\nInspired by the recent success in applying deep learning for functional brain\ndecoding and encoding, in this work we propose a spatio-temporal convolutional\nneural network (ST-CNN)to jointly learn the spatial and temporal patterns of\ntargeted network from the training data and perform automatic, pin-pointing\nfunctional network identification. The proposed ST-CNN is evaluated by the task\nof identifying the Default Mode Network (DMN) from fMRI data. Results show that\nwhile the framework is only trained on one fMRI dataset,it has the sufficient\ngeneralizability to identify the DMN from different populations of data as well\nas different cognitive tasks. Further investigation into the results show that\nthe superior performance of ST-CNN is driven by the jointly-learning scheme,\nwhich capture the intrinsic relationship between the spatial and temporal\ncharacteristic of DMN and ensures the accurate identification.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 17:08:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 20:58:59 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 18:29:39 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhao", "Yu", ""], ["Li", "Xiang", ""], ["Zhang", "Wei", ""], ["Zhao", "Shijie", ""], ["Makkie", "Milad", ""], ["Zhang", "Mo", ""], ["Li", "Quanzheng", ""], ["Liu", "Tianming", ""]]}, {"id": "1805.12589", "submitter": "Aditya Deshpande", "authors": "Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander Schwing, D. A.\n  Forsyth", "title": "Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech", "comments": "12 pages with references and appendix. To appear CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is an ambiguous problem, with many suitable captions for an\nimage. To address ambiguity, beam search is the de facto method for sampling\nmultiple captions. However, beam search is computationally expensive and known\nto produce generic captions. To address this concern, some variational\nauto-encoder (VAE) and generative adversarial net (GAN) based methods have been\nproposed. Though diverse, GAN and VAE are less accurate. In this paper, we\nfirst predict a meaningful summary of the image, then generate the caption\nbased on that summary. We use part-of-speech as summaries, since our summary\nshould drive caption generation. We achieve the trifecta: (1) High accuracy for\nthe diverse captions as evaluated by standard captioning metrics and user\nstudies; (2) Faster computation of diverse captions compared to beam search and\ndiverse beam search; and (3) High diversity as evaluated by counting novel\nsentences, distinct n-grams and mutual overlap (i.e., mBleu-4) scores.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 17:56:17 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 08:57:02 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 00:44:34 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Deshpande", "Aditya", ""], ["Aneja", "Jyoti", ""], ["Wang", "Liwei", ""], ["Schwing", "Alexander", ""], ["Forsyth", "D. A.", ""]]}]