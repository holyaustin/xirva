[{"id": "1702.00045", "submitter": "Le Lu", "authors": "Holger R. Roth, Le Lu, Nathan Lay, Adam P. Harrison, Amal Farag,\n  Andrew Sohn, Ronald M. Summers", "title": "Spatial Aggregation of Holistically-Nested Convolutional Neural Networks\n  for Automated Pancreas Localization and Segmentation", "comments": "This version was submitted to IEEE Trans. on Medical Imaging on Dec.\n  18th, 2016. The content of this article is covered by US Patent Applications\n  of 62/345,606# and 62/450,681#", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and automatic organ segmentation from 3D radiological scans is an\nimportant yet challenging problem for medical image analysis. Specifically, the\npancreas demonstrates very high inter-patient anatomical variability in both\nits shape and volume. In this paper, we present an automated system using 3D\ncomputed tomography (CT) volumes via a two-stage cascaded approach: pancreas\nlocalization and segmentation. For the first step, we localize the pancreas\nfrom the entire 3D CT scan, providing a reliable bounding box for the more\nrefined segmentation step. We introduce a fully deep-learning approach, based\non an efficient application of holistically-nested convolutional networks\n(HNNs) on the three orthogonal axial, sagittal, and coronal views. The\nresulting HNN per-pixel probability maps are then fused using pooling to\nreliably produce a 3D bounding box of the pancreas that maximizes the recall.\nWe show that our introduced localizer compares favorably to both a conventional\nnon-deep-learning method and a recent hybrid approach based on spatial\naggregation of superpixels using random forest classification. The second,\nsegmentation, phase operates within the computed bounding box and integrates\nsemantic mid-level cues of deeply-learned organ interior and boundary maps,\nobtained by two additional and separate realizations of HNNs. By integrating\nthese two mid-level cues, our method is capable of generating\nboundary-preserving pixel-wise class label maps that result in the final\npancreas segmentation. Quantitative evaluation is performed on a publicly\navailable dataset of 82 patient CT scans using 4-fold cross-validation (CV). We\nachieve a Dice similarity coefficient (DSC) of 81.27+/-6.27% in validation,\nwhich significantly outperforms previous state-of-the art methods that report\nDSCs of 71.80+/-10.70% and 78.01+/-8.20%, respectively, using the same dataset.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 20:22:15 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Lay", "Nathan", ""], ["Harrison", "Adam P.", ""], ["Farag", "Amal", ""], ["Sohn", "Andrew", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1702.00061", "submitter": "Kirk Scheper", "authors": "Bas J. Pijnacker Hordijk, Kirk Y.W. Scheper, Guido C.H.E. de Croon", "title": "Vertical Landing for Micro Air Vehicles using Event-Based Optical Flow", "comments": "29 pages, 14 figures, under peer review", "journal-ref": null, "doi": "10.1002/rob.21764", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Small flying robots can perform landing maneuvers using bio-inspired optical\nflow by maintaining a constant divergence. However, optical flow is typically\nestimated from frame sequences recorded by standard miniature cameras. This\nrequires processing full images on-board, limiting the update rate of\ndivergence measurements, and thus the speed of the control loop and the robot.\nEvent-based cameras overcome these limitations by only measuring pixel-level\nbrightness changes at microsecond temporal accuracy, hence providing an\nefficient mechanism for optical flow estimation. This paper presents, to the\nbest of our knowledge, the first work integrating event-based optical flow\nestimation into the control loop of a flying robot. We extend an existing\n'local plane fitting' algorithm to obtain an improved and more computationally\nefficient optical flow estimation method, valid for a wide range of optical\nflow velocities. This method is validated for real event sequences. In\naddition, a method for estimating the divergence from event-based optical flow\nis introduced, which accounts for the aperture problem. The developed\nalgorithms are implemented in a constant divergence landing controller on-board\nof a quadrotor. Experiments show that, using event-based optical flow, accurate\ndivergence estimates can be obtained over a wide range of speeds. This enables\nthe quadrotor to perform very fast landing maneuvers.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 21:43:23 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 09:21:41 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 09:33:33 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hordijk", "Bas J. Pijnacker", ""], ["Scheper", "Kirk Y. W.", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "1702.00098", "submitter": "Deyu Meng", "authors": "Yang Chen and Xiangyong Cao and Qian Zhao and Deyu Meng and Zongben Xu", "title": "Denoising Hyperspectral Image with Non-i.i.d. Noise Structure", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) denoising has been attracting much research\nattention in remote sensing area due to its importance in improving the HSI\nqualities. The existing HSI denoising methods mainly focus on specific spectral\nand spatial prior knowledge in HSIs, and share a common underlying assumption\nthat the embedded noise in HSI is independent and identically distributed\n(i.i.d.). In real scenarios, however, the noise existed in a natural HSI is\nalways with much more complicated non-i.i.d. statistical structures and the\nunder-estimation to this noise complexity often tends to evidently degenerate\nthe robustness of current methods. To alleviate this issue, this paper attempts\nthe first effort to model the HSI noise using a non-i.i.d. mixture of Gaussians\n(NMoG) noise assumption, which is finely in accordance with the noise\ncharacteristics possessed by a natural HSI and thus is capable of adapting\nvarious noise shapes encountered in real applications. Then we integrate such\nnoise modeling strategy into the low-rank matrix factorization (LRMF) model and\npropose a NMoG-LRMF model in the Bayesian framework. A variational Bayes\nalgorithm is designed to infer the posterior of the proposed model. All\ninvolved parameters can be recursively updated in closed-form. Compared with\nthe current techniques, the proposed method performs more robust beyond the\nstate-of-the-arts, as substantiated by our experiments implemented on synthetic\nand real noisy HSIs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 00:52:01 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Chen", "Yang", ""], ["Cao", "Xiangyong", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Xu", "Zongben", ""]]}, {"id": "1702.00156", "submitter": "Anjan Dutta", "authors": "Anjan Dutta and Hichem Sahbi", "title": "Stochastic Graphlet Embedding", "comments": "Accepted in IEEE TNNLS (14 pages, 7 figures, 10 tables)", "journal-ref": "IEEE TNNLS, pages 1-14, 2018", "doi": "10.1109/TNNLS.2018.2884700", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of --\nexplicit/implicit -- graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 08:16:03 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 11:01:06 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 01:59:34 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Dutta", "Anjan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1702.00158", "submitter": "Xiaqing Pan", "authors": "Xiaqing Pan, Yueru Chen, C.-C. Jay Kuo", "title": "Design, Analysis and Application of A Volumetric Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design, analysis and application of a volumetric convolutional neural\nnetwork (VCNN) are studied in this work. Although many CNNs have been proposed\nin the literature, their design is empirical. In the design of the VCNN, we\npropose a feed-forward K-means clustering algorithm to determine the filter\nnumber and size at each convolutional layer systematically. For the analysis of\nthe VCNN, the cause of confusing classes in the output of the VCNN is explained\nby analyzing the relationship between the filter weights (also known as anchor\nvectors) from the last fully-connected layer to the output. Furthermore, a\nhierarchical clustering method followed by a random forest classification\nmethod is proposed to boost the classification performance among confusing\nclasses. For the application of the VCNN, we examine the 3D shape\nclassification problem and conduct experiments on a popular ModelNet40 dataset.\nThe proposed VCNN offers the state-of-the-art performance among all\nvolume-based CNN methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 08:32:11 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Pan", "Xiaqing", ""], ["Chen", "Yueru", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1702.00186", "submitter": "Bastian Wandt", "authors": "Bastian Wandt, Hanno Ackermann, Bodo Rosenhahn", "title": "A Kinematic Chain Space for Monocular Motion Capture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with motion capture of kinematic chains (e.g. human\nskeletons) from monocular image sequences taken by uncalibrated cameras. We\npresent a method based on projecting an observation into a kinematic chain\nspace (KCS). An optimization of the nuclear norm is proposed that implicitly\nenforces structural properties of the kinematic chain. Unlike other approaches\nour method does not require specific camera or object motion and is not relying\non training data or previously determined constraints such as particular body\nlengths. The proposed algorithm is able to reconstruct scenes with limited\ncamera motion and previously unseen motions. It is not only applicable to human\nskeletons but also to other kinematic chains for instance animals or industrial\nrobots. We achieve state-of-the-art results on different benchmark data bases\nand real world scenes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:14:24 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Wandt", "Bastian", ""], ["Ackermann", "Hanno", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1702.00187", "submitter": "Fr\\'ed\\'eric Rayar", "authors": "Fr\\'ed\\'eric Rayar", "title": "ImageNet MPEG-7 Visual Descriptors - Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ImageNet is a large scale and publicly available image database. It currently\noffers more than 14 millions of images, organised according to the WordNet\nhierarchy. One of the main objective of the creators is to provide to the\nresearch community a relevant database for visual recognition applications such\nas object recognition, image classification or object localisation. However,\nonly a few visual descriptors of the images are available to be used by the\nresearchers. Only SIFT-based features have been extracted from a subset of the\ncollection. This technical report presents the extraction of some MPEG-7 visual\ndescriptors from the ImageNet database. These descriptors are made publicly\navailable in an effort towards open research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:15:13 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Rayar", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1702.00254", "submitter": "Hao Ye", "authors": "Li Wang, Yao Lu, Hong Wang, Yingbin Zheng, Hao Ye, Xiangyang Xue", "title": "Evolving Boxes for Fast Vehicle Detection", "comments": null, "journal-ref": "IEEE International Conference on Multimedia and Expo (ICME), 2017,\n  pp. 1135-1140", "doi": "10.1109/ICME.2017.8019461", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform fast vehicle detection from traffic surveillance cameras. A novel\ndeep learning framework, namely Evolving Boxes, is developed that proposes and\nrefines the object boxes under different feature representations. Specifically,\nour framework is embedded with a light-weight proposal network to generate\ninitial anchor boxes as well as to early discard unlikely regions; a\nfine-turning network produces detailed features for these candidate boxes. We\nshow intriguingly that by applying different feature fusion techniques, the\ninitial boxes can be refined for both localization and recognition. We evaluate\nour network on the recent DETRAC benchmark and obtain a significant improvement\nover the state-of-the-art Faster RCNN by 9.5% mAP. Further, our network\nachieves 9-13 FPS detection speed on a moderate commercial GPU.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 13:53:12 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 02:45:41 GMT"}, {"version": "v3", "created": "Wed, 29 Mar 2017 08:46:08 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wang", "Li", ""], ["Lu", "Yao", ""], ["Wang", "Hong", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1702.00307", "submitter": "\\v{Z}iga Emer\\v{s}i\\v{c}", "authors": "\\v{Z}iga Emer\\v{s}i\\v{c}, Luka Lan Gabriel, Vitomir \\v{S}truc, Peter\n  Peer", "title": "Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and segmentation represents the basis for many tasks in\ncomputer and machine vision. In biometric recognition systems the detection of\nthe region-of-interest (ROI) is one of the most crucial steps in the overall\nprocessing pipeline, significantly impacting the performance of the entire\nrecognition system. Existing approaches to ear detection, for example, are\ncommonly susceptible to the presence of severe occlusions, ear accessories or\nvariable illumination conditions and often deteriorate in their performance if\napplied on ear images captured in unconstrained settings. To address these\nshortcomings, we present in this paper a novel ear detection technique based on\nconvolutional encoder-decoder networks (CEDs). For our technique, we formulate\nthe problem of ear detection as a two-class segmentation problem and train a\nconvolutional encoder-decoder network based on the SegNet architecture to\ndistinguish between image-pixels belonging to either the ear or the non-ear\nclass. The output of the network is then post-processed to further refine the\nsegmentation result and return the final locations of the ears in the input\nimage. Different from competing techniques from the literature, our approach\ndoes not simply return a bounding box around the detected ear, but provides\ndetailed, pixel-wise information about the location of the ears in the image.\nOur experiments on a dataset gathered from the web (a.k.a. in the wild) show\nthat the proposed technique ensures good detection results in the presence of\nvarious covariate factors and significantly outperforms the existing\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:16:50 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 07:56:10 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Emer\u0161i\u010d", "\u017diga", ""], ["Gabriel", "Luka Lan", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""]]}, {"id": "1702.00338", "submitter": "Eng-Jon Ong", "authors": "Eng-Jon Ong and Sameed Husain and Miroslaw Bober", "title": "Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of large scale image retrieval, with the aim\nof accurately ranking the similarity of a large number of images to a given\nquery image. To achieve this, we propose a novel Siamese network. This network\nconsists of two computational strands, each comprising of a CNN component\nfollowed by a Fisher vector component. The CNN component produces dense, deep\nconvolutional descriptors that are then aggregated by the Fisher Vector method.\nCrucially, we propose to simultaneously learn both the CNN filter weights and\nFisher Vector model parameters. This allows us to account for the evolving\ndistribution of deep descriptors over the course of the learning process. We\nshow that the proposed approach gives significant improvements over the\nstate-of-the-art methods on the Oxford and Paris image retrieval datasets.\nAdditionally, we provide a baseline performance measure for both these datasets\nwith the inclusion of 1 million distractors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 16:20:00 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Ong", "Eng-Jon", ""], ["Husain", "Sameed", ""], ["Bober", "Miroslaw", ""]]}, {"id": "1702.00372", "submitter": "Samuel Dodge", "authors": "Samuel Dodge and Lina Karam", "title": "Visual Saliency Prediction Using a Mixture of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2834826", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency models have recently begun to incorporate deep learning to\nachieve predictive capacity much greater than previous unsupervised methods.\nHowever, most existing models predict saliency using local mechanisms limited\nto the receptive field of the network. We propose a model that incorporates\nglobal scene semantic information in addition to local information gathered by\na convolutional neural network. Our model is formulated as a mixture of\nexperts. Each expert network is trained to predict saliency for a set of\nclosely related images. The final saliency map is computed as a weighted\nmixture of the expert networks' output, with weights determined by a separate\ngating network. This gating network is guided by global scene information to\npredict weights. The expert networks and the gating network are trained\nsimultaneously in an end-to-end manner. We show that our mixture formulation\nleads to improvement in performance over an otherwise identical non-mixture\nmodel that does not incorporate global scene information.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 17:45:55 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Dodge", "Samuel", ""], ["Karam", "Lina", ""]]}, {"id": "1702.00382", "submitter": "Ivet Rafegas", "authors": "Ivet Rafegas, Maria Vanrell, Luis A. Alexandre, Guillem Arias", "title": "Understanding trained CNNs by indexing neuron selectivity", "comments": "Under review on Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2019.10.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive performance of Convolutional Neural Networks (CNNs) when\nsolving different vision problems is shadowed by their black-box nature and our\nconsequent lack of understanding of the representations they build and how\nthese representations are organized. To help understanding these issues, we\npropose to describe the activity of individual neurons by their Neuron Feature\nvisualization and quantify their inherent selectivity with two specific\nproperties. We explore selectivity indexes for: an image feature (color); and\nan image label (class membership). Our contribution is a framework to seek or\nclassify neurons by indexing on these selectivity properties. It helps to find\ncolor selective neurons, such as a red-mushroom neuron in layer Conv4 or class\nselective neurons such as dog-face neurons in layer Conv5 in VGG-M, and\nestablishes a methodology to derive other selectivity properties. Indexing on\nneuron selectivity can statistically draw how features and classes are\nrepresented through layers in a moment when the size of trained nets is growing\nand automatic tools to index neurons can be helpful.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 18:19:37 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 15:37:50 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Rafegas", "Ivet", ""], ["Vanrell", "Maria", ""], ["Alexandre", "Luis A.", ""], ["Arias", "Guillem", ""]]}, {"id": "1702.00391", "submitter": "Anjan Dutta", "authors": "Anjan Dutta, Josep Llad\\'os, Horst Bunke and Umapada Pal", "title": "Product Graph-based Higher Order Contextual Similarities for Inexact\n  Subgraph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms formulate graph matching as an optimization of an objective\nfunction of pairwise quantification of nodes and edges of two graphs to be\nmatched. Pairwise measurements usually consider local attributes but disregard\ncontextual information involved in graph structures. We address this issue by\nproposing contextual similarities between pairs of nodes. This is done by\nconsidering the tensor product graph (TPG) of two graphs to be matched, where\neach node is an ordered pair of nodes of the operand graphs. Contextual\nsimilarities between a pair of nodes are computed by accumulating weighted\nwalks (normalized pairwise similarities) terminating at the corresponding\npaired node in TPG. Once the contextual similarities are obtained, we formulate\nsubgraph matching as a node and edge selection problem in TPG. We use\ncontextual similarities to construct an objective function and optimize it with\na linear programming approach. Since random walk formulation through TPG takes\ninto account higher order information, it is not a surprise that we obtain more\nreliable similarities and better discrimination among the nodes and edges.\nExperimental results shown on synthetic as well as real benchmarks illustrate\nthat higher order contextual similarities add discriminating power and allow\none to find approximate solutions to the subgraph matching problem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 18:53:53 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Dutta", "Anjan", ""], ["Llad\u00f3s", "Josep", ""], ["Bunke", "Horst", ""], ["Pal", "Umapada", ""]]}, {"id": "1702.00503", "submitter": "Yi-Ling Chen", "authors": "Yi-Ling Chen, Jan Klopp, Min Sun, Shao-Yi Chien, Kwan-Liu Ma", "title": "Learning to Compose with Professional Photographs on the Web", "comments": "Scripts and pre-trained models available at\n  https://github.com/yiling-chen/view-finding-network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo composition is an important factor affecting the aesthetics in\nphotography. However, it is a highly challenging task to model the aesthetic\nproperties of good compositions due to the lack of globally applicable rules to\nthe wide variety of photographic styles. Inspired by the thinking process of\nphoto taking, we formulate the photo composition problem as a view finding\nprocess which successively examines pairs of views and determines their\naesthetic preferences. We further exploit the rich professional photographs on\nthe web to mine unlimited high-quality ranking samples and demonstrate that an\naesthetics-aware deep ranking network can be trained without explicitly\nmodeling any photographic rules. The resulting model is simple and effective in\nterms of its architectural design and data sampling method. It is also generic\nsince it naturally learns any photographic rules implicitly encoded in\nprofessional photographs. The experiments show that the proposed view finding\nnetwork achieves state-of-the-art performance with sliding window search\nstrategy on two image cropping datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 23:49:36 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 02:12:17 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Chen", "Yi-Ling", ""], ["Klopp", "Jan", ""], ["Sun", "Min", ""], ["Chien", "Shao-Yi", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1702.00505", "submitter": "Luigi Nardi", "authors": "Luigi Nardi, Bruno Bodin, Sajad Saeedi, Emanuele Vespa, Andrew J.\n  Davison, Paul H. J. Kelly", "title": "Algorithmic Performance-Accuracy Trade-off in 3D Vision Applications\n  Using HyperMapper", "comments": "10 pages, Keywords: design space exploration, machine learning,\n  computer vision, SLAM, embedded systems, GPU, crowd-sourcing", "journal-ref": "31st IEEE International Parallel and Distributed Processing\n  Symposium May 29 - June 2, 2017 Orlando, Florida USA", "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate an emerging application, 3D scene understanding,\nlikely to be significant in the mobile space in the near future. The goal of\nthis exploration is to reduce execution time while meeting our quality of\nresult objectives. In previous work we showed for the first time that it is\npossible to map this application to power constrained embedded systems,\nhighlighting that decision choices made at the algorithmic design-level have\nthe most impact.\n  As the algorithmic design space is too large to be exhaustively evaluated, we\nuse a previously introduced multi-objective Random Forest Active Learning\nprediction framework dubbed HyperMapper, to find good algorithmic designs. We\nshow that HyperMapper generalizes on a recent cutting edge 3D scene\nunderstanding algorithm and on a modern GPU-based computer architecture.\nHyperMapper is able to beat an expert human hand-tuning the algorithmic\nparameters of the class of Computer Vision applications taken under\nconsideration in this paper automatically. In addition, we use crowd-sourcing\nusing a 3D scene understanding Android app to show that the Pareto front\nobtained on an embedded system can be used to accelerate the same application\non all the 83 smart-phones and tablets crowd-sourced with speedups ranging from\n2 to over 12.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 00:01:46 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 21:58:41 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Nardi", "Luigi", ""], ["Bodin", "Bruno", ""], ["Saeedi", "Sajad", ""], ["Vespa", "Emanuele", ""], ["Davison", "Andrew J.", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "1702.00506", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Hao Zhou, Walter Forkel, Ronen Basri, Tom\n  Goldstein, and David W. Jacobs", "title": "Solving Uncalibrated Photometric Stereo Using Fewer Images by Jointly\n  Optimizing Low-rank Matrix Completion and Integrability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, integrated approach to uncalibrated photometric stereo.\nWe perform 3D reconstruction of Lambertian objects using multiple images\nproduced by unknown, directional light sources. We show how to formulate a\nsingle optimization that includes rank and integrability constraints, allowing\nalso for missing data. We then solve this optimization using the Alternate\nDirection Method of Multipliers (ADMM). We conduct extensive experimental\nevaluation on real and synthetic data sets. Our integrated approach is\nparticularly valuable when performing photometric stereo using as few as 4-6\nimages, since the integrability constraint is capable of improving estimation\nof the linear subspace of possible solutions. We show good improvements over\nprior work in these cases.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 00:07:46 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Zhou", "Hao", ""], ["Forkel", "Walter", ""], ["Basri", "Ronen", ""], ["Goldstein", "Tom", ""], ["Jacobs", "David W.", ""]]}, {"id": "1702.00509", "submitter": "Jen Hong Tan", "authors": "Jen Hong Tan, U. Rajendra Acharya, Sulatha V. Bhandary, Kuang Chua\n  Chua, Sobha Sivaprasad", "title": "Segmentation of optic disc, fovea and retinal vasculature using a single\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed and trained a convolutional neural network to automatically\nand simultaneously segment optic disc, fovea and blood vessels. Fundus images\nwere normalised before segmentation was performed to enforce consistency in\nbackground lighting and contrast. For every effective point in the fundus\nimage, our algorithm extracted three channels of input from the neighbourhood\nof the point and forward the response across the 7 layer network. In average,\nour segmentation achieved an accuracy of 92.68 percent on the testing set from\nDrive database.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 00:37:22 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Tan", "Jen Hong", ""], ["Acharya", "U. Rajendra", ""], ["Bhandary", "Sulatha V.", ""], ["Chua", "Kuang Chua", ""], ["Sivaprasad", "Sobha", ""]]}, {"id": "1702.00523", "submitter": "Satish Palaniappan", "authors": "Satish Palaniappan and Ronojoy Adhikari", "title": "Deep Learning the Indus Script", "comments": "17 pages, 10 figures, 7 supporting figures (2 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized corpora of undeciphered scripts, a necessary starting point for\ncomputational epigraphy, requires laborious human effort for their preparation\nfrom raw archaeological records. Automating this process through machine\nlearning algorithms can be of significant aid to epigraphical research. Here,\nwe take the first steps in this direction and present a deep learning pipeline\nthat takes as input images of the undeciphered Indus script, as found in\narchaeological artifacts, and returns as output a string of graphemes, suitable\nfor inclusion in a standard corpus. The image is first decomposed into regions\nusing Selective Search and these regions are classified as containing textual\nand/or graphical information using a convolutional neural network. Regions\nclassified as potentially containing text are hierarchically merged and trimmed\nto remove non-textual information. The remaining textual part of the image is\nsegmented using standard image processing techniques to isolate individual\ngraphemes. This set is finally passed to a second convolutional neural network\nto classify the graphemes, based on a standard corpus. The classifier can\nidentify the presence or absence of the most frequent Indus grapheme, the \"jar\"\nsign, with an accuracy of 92%. Our results demonstrate the great potential of\ndeep learning approaches in computational epigraphy and, more generally, in the\ndigital humanities.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 01:56:22 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Palaniappan", "Satish", ""], ["Adhikari", "Ronojoy", ""]]}, {"id": "1702.00583", "submitter": "Mikhail Breslav", "authors": "Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke", "title": "Automating Image Analysis by Annotating Landmarks with Deep Neural\n  Networks", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video analysis is often a crucial step in the study of animal\nbehavior and kinematics. Often these analyses require that the position of one\nor more animal landmarks are annotated (marked) in numerous images. The process\nof annotating landmarks can require a significant amount of time and tedious\nlabor, which motivates the need for algorithms that can automatically annotate\nlandmarks. In the community of scientists that use image and video analysis to\nstudy the 3D flight of animals, there has been a trend of developing more\nautomated approaches for annotating landmarks, yet they fall short of being\ngenerally applicable. Inspired by the success of Deep Neural Networks (DNNs) on\nmany problems in the field of computer vision, we investigate how suitable DNNs\nare for accurate and automatic annotation of landmarks in video datasets\nrepresentative of those collected by scientists studying animals.\n  Our work shows, through extensive experimentation on videos of hawkmoths,\nthat DNNs are suitable for automatic and accurate landmark localization. In\nparticular, we show that one of our proposed DNNs is more accurate than the\ncurrent best algorithm for automatic localization of landmarks on hawkmoth\nvideos. Moreover, we demonstrate how these annotations can be used to\nquantitatively analyze the 3D flight of a hawkmoth. To facilitate the use of\nDNNs by scientists from many different fields, we provide a self contained\nexplanation of what DNNs are, how they work, and how to apply them to other\ndatasets using the freely available library Caffe and supplemental code that we\nprovide.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 08:53:10 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Breslav", "Mikhail", ""], ["Hedrick", "Tyson L.", ""], ["Sclaroff", "Stan", ""], ["Betke", "Margrit", ""]]}, {"id": "1702.00615", "submitter": "Xuanyang Xi", "authors": "Xuanyang Xi, Yongkang Luo, Fengfu Li, Peng Wang and Hong Qiao", "title": "A Fast and Compact Saliency Score Regression Network Based on Fully\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency detection aims at identifying the most visually distinctive\nparts in an image, and serves as a pre-processing step for a variety of\ncomputer vision and image processing tasks. To this end, the saliency detection\nprocedure must be as fast and compact as possible and optimally processes input\nimages in a real time manner. It is an essential application requirement for\nthe saliency detection task. However, contemporary detection methods often\nutilize some complicated procedures to pursue feeble improvements on the\ndetection precession, which always take hundreds of milliseconds and make them\nnot easy to be applied practically. In this paper, we tackle this problem by\nproposing a fast and compact saliency score regression network which employs\nfully convolutional network, a special deep convolutional neural network, to\nestimate the saliency of objects in images. It is an extremely simplified\nend-to-end deep neural network without any pre-processings and\npost-processings. When given an image, the network can directly predict a dense\nfull-resolution saliency map (image-to-image prediction). It works like a\ncompact pipeline which effectively simplifies the detection procedure. Our\nmethod is evaluated on six public datasets, and experimental results show that\nit can achieve comparable or better precision performance than the\nstate-of-the-art methods while get a significant improvement in detection speed\n(35 FPS, processing in real time).\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 11:07:51 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 14:15:31 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Xi", "Xuanyang", ""], ["Luo", "Yongkang", ""], ["Li", "Fengfu", ""], ["Wang", "Peng", ""], ["Qiao", "Hong", ""]]}, {"id": "1702.00648", "submitter": "Niannan Xue", "authors": "Niannan Xue, Yannis Panagakis, Stefanos Zafeiriou", "title": "Side Information in Robust Principal Component Analysis: Algorithms and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Principal Component Analysis (RPCA) aims at recovering a low-rank\nsubspace from grossly corrupted high-dimensional (often visual) data and is a\ncornerstone in many machine learning and computer vision applications. Even\nthough RPCA has been shown to be very successful in solving many rank\nminimisation problems, there are still cases where degenerate or suboptimal\nsolutions are obtained. This is likely to be remedied by taking into account of\ndomain-dependent prior knowledge. In this paper, we propose two models for the\nRPCA problem with the aid of side information on the low-rank structure of the\ndata. The versatility of the proposed methods is demonstrated by applying them\nto four applications, namely background subtraction, facial image denoising,\nface and facial expression recognition. Experimental results on synthetic and\nfive real world datasets indicate the robustness and effectiveness of the\nproposed methods on these application domains, largely outperforming six\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 12:42:50 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 16:23:44 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Xue", "Niannan", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1702.00714", "submitter": "Antoine Coutrot", "authors": "Antoine Coutrot and Nathalie Guyader", "title": "Learning a time-dependent master saliency map from eye-tracking data in\n  videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To predict the most salient regions of complex natural scenes, saliency\nmodels commonly compute several feature maps (contrast, orientation, motion...)\nand linearly combine them into a master saliency map. Since feature maps have\ndifferent spatial distribution and amplitude dynamic ranges, determining their\ncontributions to overall saliency remains an open problem. Most\nstate-of-the-art models do not take time into account and give feature maps\nconstant weights across the stimulus duration. However, visual exploration is a\nhighly dynamic process shaped by many time-dependent factors. For instance,\nsome systematic viewing patterns such as the center bias are known to\ndramatically vary across the time course of the exploration. In this paper, we\nuse maximum likelihood and shrinkage methods to dynamically and jointly learn\nfeature map and systematic viewing pattern weights directly from eye-tracking\ndata recorded on videos. We show that these weights systematically vary as a\nfunction of time, and heavily depend upon the semantic visual category of the\nvideos being processed. Our fusion method allows taking these variations into\naccount, and outperforms other state-of-the-art fusion schemes using constant\nweights over time. The code, videos and eye-tracking data we used for this\nstudy are available online:\nhttp://antoinecoutrot.magix.net/public/research.html\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 15:30:55 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Coutrot", "Antoine", ""], ["Guyader", "Nathalie", ""]]}, {"id": "1702.00723", "submitter": "Nilam Nur Amir Sjarif", "authors": "Norhidayu Abdul Hamid and Nilam Nur Amir Sjarif", "title": "Handwritten Recognition Using SVM, KNN and Neural Network", "comments": "11 pages ; 22 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten recognition (HWR) is the ability of a computer to receive and\ninterpret intelligible handwritten input from source such as paper documents,\nphotographs, touch-screens and other devices. In this paper we will using three\n(3) classification t o re cognize the handwritten which is SVM, KNN and Neural\nNetwork.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 18:32:12 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Hamid", "Norhidayu Abdul", ""], ["Sjarif", "Nilam Nur Amir", ""]]}, {"id": "1702.00754", "submitter": "Dilip K. Prasad", "authors": "D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Rajabally, C.\n  Quek", "title": "Maritime situational awareness using adaptive multi-sensor management\n  under hazy conditions", "comments": "11 pages, 2 figures, MTEC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-sensor architecture with an adaptive multi-sensor\nmanagement system suitable for control and navigation of autonomous maritime\nvessels in hazy and poor-visibility conditions. This architecture resides in\nthe autonomous maritime vessels. It augments the data from on-board imaging\nsensors and weather sensors with the AIS data and weather data from sensors on\nother vessels and the on-shore vessel traffic surveillance system. The combined\ndata is analyzed using computational intelligence and data analytics to\ndetermine suitable course of action while utilizing historically learnt\nknowledge and performing live learning from the current situation. Such\nframework is expected to be useful in diverse weather conditions and shall be a\nuseful architecture to provide autonomy to maritime vessels.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 17:19:16 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Prasad", "D. K.", ""], ["Prasath", "C. K.", ""], ["Rajan", "D.", ""], ["Rachmawati", "L.", ""], ["Rajabally", "E.", ""], ["Quek", "C.", ""]]}, {"id": "1702.00758", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Mingsheng Long, Jianmin Wang, Philip S. Yu", "title": "HashNet: Deep Learning to Hash by Continuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to hash has been widely applied to approximate nearest neighbor\nsearch for large-scale multimedia retrieval, due to its computation efficiency\nand retrieval quality. Deep learning to hash, which improves retrieval quality\nby end-to-end representation learning and hash encoding, has received\nincreasing attention recently. Subject to the ill-posed gradient difficulty in\nthe optimization with sign activations, existing deep learning to hash methods\nneed to first learn continuous representations and then generate binary hash\ncodes in a separated binarization step, which suffer from substantial loss of\nretrieval quality. This work presents HashNet, a novel deep architecture for\ndeep learning to hash by continuation method with convergence guarantees, which\nlearns exactly binary hash codes from imbalanced similarity data. The key idea\nis to attack the ill-posed gradient problem in optimizing deep networks with\nnon-smooth binary activations by continuation method, in which we begin from\nlearning an easier network with smoothed activation function and let it evolve\nduring the training, until it eventually goes back to being the original,\ndifficult to optimize, deep network with the sign activation function.\nComprehensive empirical evidence shows that HashNet can generate exactly binary\nhash codes and yield state-of-the-art multimedia retrieval performance on\nstandard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 17:29:24 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 22:14:09 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 14:59:45 GMT"}, {"version": "v4", "created": "Sat, 29 Jul 2017 17:55:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cao", "Zhangjie", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1702.00783", "submitter": "Ryan Dahl", "authors": "Ryan Dahl, Mohammad Norouzi, Jonathon Shlens", "title": "Pixel Recursive Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pixel recursive super resolution model that synthesizes\nrealistic details into images while enhancing their resolution. A low\nresolution image may correspond to multiple plausible high resolution images,\nthus modeling the super resolution process with a pixel independent conditional\nmodel often results in averaging different details--hence blurry edges. By\ncontrast, our model is able to represent a multimodal conditional distribution\nby properly modeling the statistical dependencies among the high resolution\nimage pixels, conditioned on a low resolution input. We employ a PixelCNN\narchitecture to define a strong prior over natural images and jointly optimize\nthis prior with a deep conditioning convolutional network. Human evaluations\nindicate that samples from our proposed model look more photo realistic than a\nstrong L2 regression baseline.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 18:59:17 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 16:13:21 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Dahl", "Ryan", ""], ["Norouzi", "Mohammad", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1702.00824", "submitter": "Esteban Real", "authors": "Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan and Vincent\n  Vanhoucke", "title": "YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set\n  for Object Detection in Video", "comments": "Accepted at the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new large-scale data set of video URLs with densely-sampled\nobject bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data\nset consists of approximately 380,000 video segments about 19s long,\nautomatically selected to feature objects in natural settings without editing\nor post-processing, with a recording quality often akin to that of a hand-held\ncell phone camera. The objects represent a subset of the MS COCO label set. All\nvideo segments were human-annotated with high-precision classification labels\nand bounding boxes at 1 frame per second. The use of a cascade of increasingly\nprecise human annotations ensures a label accuracy above 95% for every class\nand tight bounding boxes. Finally, we train and evaluate well-known deep\nnetwork architectures and report baseline figures for per-frame classification\nand localization to provide a point of comparison for future work. We also\ndemonstrate how the temporal contiguity of video can potentially be used to\nimprove such inferences. Please see the PDF file to find the URL to download\nthe data. We hope the availability of such large curated corpus will spur new\nadvances in video object detection and tracking.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 20:58:02 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 02:22:23 GMT"}, {"version": "v3", "created": "Wed, 8 Feb 2017 03:43:02 GMT"}, {"version": "v4", "created": "Thu, 23 Mar 2017 02:49:29 GMT"}, {"version": "v5", "created": "Fri, 24 Mar 2017 18:52:39 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Real", "Esteban", ""], ["Shlens", "Jonathon", ""], ["Mazzocchi", "Stefano", ""], ["Pan", "Xin", ""], ["Vanhoucke", "Vincent", ""]]}, {"id": "1702.00871", "submitter": "Wenguan Wang", "authors": "Wenguan Wang and Jianbing Shen and Ling Shao", "title": "Video Salient Object Detection via Fully Convolutional Networks", "comments": "W. Wang, J. Shen, and L. Shao, Video salient object detection via\n  fully convolutional networks, IEEE Trans. on Image Processing, 27(1):38-49,\n  2018 Code and results: https://github.com/wenguanwang/deepvideosaliency", "journal-ref": "IEEE Transactions on Image Processing, Vol. 27, No. 1, pp 38-49,\n  2018", "doi": "10.1109/TIP.2017.2754941", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a deep learning model to efficiently detect salient\nregions in videos. It addresses two important issues: (1) deep video saliency\nmodel training with the absence of sufficiently large and pixel-wise annotated\nvideo data, and (2) fast video saliency training and detection. The proposed\ndeep video saliency network consists of two modules, for capturing the spatial\nand temporal saliency information, respectively. The dynamic saliency model,\nexplicitly incorporating saliency estimates from the static saliency model,\ndirectly produces spatiotemporal saliency inference without time-consuming\noptical flow computation. We further propose a novel data augmentation\ntechnique that simulates video training data from existing annotated image\ndatasets, which enables our network to learn diverse saliency information and\nprevents overfitting with the limited number of training videos. Leveraging our\nsynthetic video data (150K video sequences) and real videos, our deep video\nsaliency model successfully learns both spatial and temporal saliency cues,\nthus producing accurate spatiotemporal saliency estimate. We advance the\nstate-of-the-art on the DAVIS dataset (MAE of .06) and the FBMS dataset (MAE of\n.07), and do so with much improved speed (2fps with all steps).\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 23:44:28 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 01:45:42 GMT"}, {"version": "v3", "created": "Sat, 9 Dec 2017 01:42:49 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "1702.00882", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Marwan Torki", "title": "Seeded Laplaican: An Eigenfunction Solution for Scribble Based\n  Interactive Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we cast the scribble-based interactive image segmentation as a\nsemi-supervised learning problem. Our novel approach alleviates the need to\nsolve an expensive generalized eigenvector problem by approximating the\neigenvectors using efficiently computed eigenfunctions. The smoothness operator\ndefined on feature densities at the limit n tends to infinity recovers the\nexact eigenvectors of the graph Laplacian, where n is the number of nodes in\nthe graph. To further reduce the computational complexity without scarifying\nour accuracy, we select pivots pixels from user annotations. In our\nexperiments, we evaluate our approach using both human scribble and \"robot\nuser\" annotations to guide the foreground/background segmentation. We developed\na new unbiased collection of five annotated images datasets to standardize the\nevaluation procedure for any scribble-based segmentation method. We\nexperimented with several variations, including different feature vectors,\npivot count and the number of eigenvectors. Experiments are carried out on\ndatasets that contain a wide variety of natural images. We achieve better\nqualitative and quantitative results compared to state-of-the-art interactive\nsegmentation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 01:13:07 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 20:12:37 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Taha", "Ahmed", ""], ["Torki", "Marwan", ""]]}, {"id": "1702.00926", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin,\n  Kwanghoon Sohn", "title": "FCSS: Fully Convolutional Self-Similarity for Dense Semantic\n  Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a descriptor, called fully convolutional self-similarity (FCSS),\nfor dense semantic correspondence. To robustly match points among different\ninstances within the same object class, we formulate FCSS using local\nself-similarity (LSS) within a fully convolutional network. In contrast to\nexisting CNN-based descriptors, FCSS is inherently insensitive to intra-class\nappearance variations because of its LSS-based structure, while maintaining the\nprecise localization ability of deep neural networks. The sampling patterns of\nlocal structure and the self-similarity measure are jointly learned within the\nproposed network in an end-to-end and multi-scale manner. As training data for\nsemantic correspondence is rather limited, we propose to leverage object\ncandidate priors provided in existing image datasets and also correspondence\nconsistency between object pairs to enable weakly-supervised learning.\nExperiments demonstrate that FCSS outperforms conventional handcrafted\ndescriptors and CNN-based descriptors on various benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 07:36:44 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Kim", "Seungryong", ""], ["Min", "Dongbo", ""], ["Ham", "Bumsub", ""], ["Jeon", "Sangryul", ""], ["Lin", "Stephen", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1702.00932", "submitter": "James Geraci", "authors": "James R. Geraci and Parichay Kapoor", "title": "A method of limiting performance loss of CNNs in noisy environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) recognition rates drop in the presence of\nnoise. We demonstrate a novel method of counteracting this drop in recognition\nrate by adjusting the biases of the neurons in the convolutional layers\naccording to the noise conditions encountered at runtime. We compare our\ntechnique to training one network for all possible noise levels, dehazing via\npreprocessing a signal with a denoising autoencoder, and training a network\nspecifically for each noise level. Our system compares favorably in terms of\nrobustness, computational complexity and recognition rate.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 08:21:46 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Geraci", "James R.", ""], ["Kapoor", "Parichay", ""]]}, {"id": "1702.00953", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Xiaodong He, Jian Sun, Nuno Vasconcelos", "title": "Deep Learning with Low Precision by Half-wave Gaussian Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quantizing the activations of a deep neural network is\nconsidered. An examination of the popular binary quantization approach shows\nthat this consists of approximating a classical non-linearity, the hyperbolic\ntangent, by two functions: a piecewise constant sign function, which is used in\nfeedforward network computations, and a piecewise linear hard tanh function,\nused in the backpropagation step during network learning. The problem of\napproximating the ReLU non-linearity, widely used in the recent deep learning\nliterature, is then considered. An half-wave Gaussian quantizer (HWGQ) is\nproposed for forward approximation and shown to have efficient implementation,\nby exploiting the statistics of of network activations and batch normalization\noperations commonly used in the literature. To overcome the problem of gradient\nmismatch, due to the use of different forward and backward approximations,\nseveral piece-wise backward approximators are then investigated. The\nimplementation of the resulting quantized network, denoted as HWGQ-Net, is\nshown to achieve much closer performance to full precision networks, such as\nAlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision\nnetworks, with 1-bit binary weights and 2-bit quantized activations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 10:11:40 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Cai", "Zhaowei", ""], ["He", "Xiaodong", ""], ["Sun", "Jian", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1702.01005", "submitter": "Rudrasis Chakraborty Dr.", "authors": "Rudrasis Chakraborty, S{\\o}ren Hauberg, Baba C. Vemuri", "title": "Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear\n  Subspace Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) and Kernel Principal Component Analysis\n(KPCA) are fundamental methods in machine learning for dimensionality\nreduction. The former is a technique for finding this approximation in finite\ndimensions and the latter is often in an infinite dimensional Reproducing\nKernel Hilbert-space (RKHS). In this paper, we present a geometric framework\nfor computing the principal linear subspaces in both situations as well as for\nthe robust PCA case, that amounts to computing the intrinsic average on the\nspace of all subspaces: the Grassmann manifold. Points on this manifold are\ndefined as the subspaces spanned by $K$-tuples of observations. The intrinsic\nGrassmann average of these subspaces are shown to coincide with the principal\ncomponents of the observations when they are drawn from a Gaussian\ndistribution. We show similar results in the RKHS case and provide an efficient\nalgorithm for computing the projection onto the this average subspace. The\nresult is a method akin to KPCA which is substantially faster. Further, we\npresent a novel online version of the KPCA using our geometric framework.\nCompetitive performance of all our algorithms are demonstrated on a variety of\nreal and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 13:44:44 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 02:29:39 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Hauberg", "S\u00f8ren", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1702.01105", "submitter": "Iro Armeni", "authors": "Iro Armeni, Sasha Sax, Amir R. Zamir and Silvio Savarese", "title": "Joint 2D-3D-Semantic Data for Indoor Scene Understanding", "comments": "The dataset is available http://3Dsemantics.stanford.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataset of large-scale indoor spaces that provides a variety of\nmutually registered modalities from 2D, 2.5D and 3D domains, with\ninstance-level semantic and geometric annotations. The dataset covers over\n6,000m2 and contains over 70,000 RGB images, along with the corresponding\ndepths, surface normals, semantic annotations, global XYZ images (all in forms\nof both regular and 360{\\deg} equirectangular images) as well as camera\ninformation. It also includes registered raw and semantically annotated 3D\nmeshes and point clouds. The dataset enables development of joint and\ncross-modal learning models and potentially unsupervised approaches utilizing\nthe regularities present in large-scale indoor spaces. The dataset is available\nhere: http://3Dsemantics.stanford.edu/\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 18:28:33 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 01:46:13 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Armeni", "Iro", ""], ["Sax", "Sasha", ""], ["Zamir", "Amir R.", ""], ["Savarese", "Silvio", ""]]}, {"id": "1702.01167", "submitter": "Andrey Kuehlkamp", "authors": "Andrey Kuehlkamp and Kevin W. Bowyer", "title": "An Analysis of 1-to-First Matching in Iris Recognition", "comments": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": null, "doi": "10.1109/WACV.2016.7477687", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition systems are a mature technology that is widely used\nthroughout the world. In identification (as opposed to verification) mode, an\niris to be recognized is typically matched against all N enrolled irises. This\nis the classic \"1-to-N search\". In order to improve the speed of large-scale\nidentification, a modified \"1-to-First\" search has been used in some\noperational systems. A 1-to-First search terminates with the first\nbelow-threshold match that is found, whereas a 1-to-N search always finds the\nbest match across all enrollments. We know of no previous studies that evaluate\nhow the accuracy of 1-to-First search differs from that of 1-to-N search. Using\na dataset of over 50,000 iris images from 2,800 different irises, we perform\nexperiments to evaluate the relative accuracy of 1-to-First and 1-to-N search.\nWe evaluate how the accuracy difference changes with larger numbers of enrolled\nirises, and with larger ranges of rotational difference allowed between iris\nimages. We find that False Match error rate for 1-to-First is higher than for\n1-to-N, and the the difference grows with larger number of enrolled irises and\nwith larger range of rotation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:24:10 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Kuehlkamp", "Andrey", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "1702.01238", "submitter": "Eyasu Zemene", "authors": "Eyasu Zemene, Yonatan Tariku, Haroon Idrees, Andrea Prati, Marcello\n  Pelillo and Mubarak Shah", "title": "Large-scale Image Geo-Localization Using Dominant Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for the challenging problem of\ngeo-locating an image using image matching in a structured database of\ncity-wide reference images with known GPS coordinates. We cast the\ngeo-localization as a clustering problem on local image features. Akin to\nexisting approaches on the problem, our framework builds on low-level features\nwhich allow partial matching between images. For each local feature in the\nquery image, we find its approximate nearest neighbors in the reference set.\nNext, we cluster the features from reference images using Dominant Set\nclustering, which affords several advantages over existing approaches. First,\nit permits variable number of nodes in the cluster which we use to dynamically\nselect the number of nearest neighbors (typically coming from multiple\nreference images) for each query feature based on its discrimination value.\nSecond, as we also quantify in our experiments, this approach is several orders\nof magnitude faster than existing approaches. Thus, we obtain multiple clusters\n(different local maximizers) and obtain a robust final solution to the problem\nusing multiple weak solutions through constrained Dominant Set clustering on\nglobal image features, where we enforce the constraint that the query image\nmust be included in the cluster. This second level of clustering also bypasses\nheuristic approaches to voting and selecting the reference image that matches\nto the query. We evaluated the proposed framework on an existing dataset of\n102k street view images as well as a new dataset of 300k images, and show that\nit outperforms the state-of-the-art by 20% and 7%, respectively, on the two\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 05:18:01 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 15:16:20 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 15:17:41 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Zemene", "Eyasu", ""], ["Tariku", "Yonatan", ""], ["Idrees", "Haroon", ""], ["Prati", "Andrea", ""], ["Pelillo", "Marcello", ""], ["Shah", "Mubarak", ""]]}, {"id": "1702.01243", "submitter": "Youngwan Lee", "authors": "Youngwan Lee, Byeonghak Yim, Huien Kim, Eunsoo Park, Xuenan Cui,\n  Taekang Woo, Hakil Kim", "title": "Wide-Residual-Inception Networks for Real-time Object Detection", "comments": "IV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since convolutional neural network(CNN)models emerged,several tasks in\ncomputer vision have actively deployed CNN models for feature extraction.\nHowever,the conventional CNN models have a high computational cost and require\nhigh memory capacity, which is impractical and unaffordable for commercial\napplications such as real-time on-road object detection on embedded boards or\nmobile platforms. To tackle this limitation of CNN models, this paper proposes\na wide-residual-inception (WR-Inception) network, which constructs the\narchitecture based on a residual inception unit that captures objects of\nvarious sizes on the same feature map, as well as shallower and wider layers,\ncompared to state-of-the-art networks like ResNet. To verify the proposed\nnetworks, this paper conducted two experiments; one is a classification task on\nCIFAR-10/100 and the other is an on-road object detection task using a\nSingle-Shot Multi-box Detector(SSD) on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 06:34:31 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 23:59:50 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 07:56:21 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lee", "Youngwan", ""], ["Yim", "Byeonghak", ""], ["Kim", "Huien", ""], ["Park", "Eunsoo", ""], ["Cui", "Xuenan", ""], ["Woo", "Taekang", ""], ["Kim", "Hakil", ""]]}, {"id": "1702.01247", "submitter": "Feras Dayoub", "authors": "David Hall, Feras Dayoub, Jason Kulk and Chris McCool", "title": "Towards Unsupervised Weed Scouting for Agricultural Robotics", "comments": "to appear in the proceedings of the IEEE International Conference on\n  Robotics and Automation ICRA2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weed scouting is an important part of modern integrated weed management but\ncan be time consuming and sparse when performed manually. Automated weed\nscouting and weed destruction has typically been performed using classification\nsystems able to classify a set group of species known a priori. This greatly\nlimits deployability as classification systems must be retrained for any field\nwith a different set of weed species present within them. In order to overcome\nthis limitation, this paper works towards developing a clustering approach to\nweed scouting which can be utilized in any field without the need for prior\nspecies knowledge. We demonstrate our system using challenging data collected\nin the field from an agricultural robotics platform. We show that considerable\nimprovements can be made by (i) learning low-dimensional (bottleneck) features\nusing a deep convolutional neural network to represent plants in general and\n(ii) tying views of the same area (plant) together. Deploying this algorithm on\nin-field data collected by AgBotII, we are able to successfully cluster cotton\nplants from grasses without prior knowledge or training for the specific plants\nin the field.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 07:08:36 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 05:00:04 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Hall", "David", ""], ["Dayoub", "Feras", ""], ["Kulk", "Jason", ""], ["McCool", "Chris", ""]]}, {"id": "1702.01276", "submitter": "Hamid Shahdoosti", "authors": "Seyede Mahya Hazavei, Hamid Reza Shahdoosti", "title": "Using Complex Wavelet Transform and Bilateral Filtering for Image\n  Denoising", "comments": "6 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral filter is a useful nonlinear filter which without smoothing\nedges, it does spatial averaging. In the literature, the effectiveness of this\nmethod for image denoising is shown. In this paper, an extension of this method\nis proposed which is based on complex wavelet transform. In fact, the bilateral\nfiltering is applied to the low-frequency (approximation) subbands of the\ndecomposed image using complex wavelet transform, while the thresholding\napproach is applied to the high frequency subbands. Using the bilateral filter\nin the complex wavelet domain forms a new image denoising framework.\nExperimental results for real data are provided, by which one can see the\neffectiveness of the proposed method in eliminating noise.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 11:51:46 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Hazavei", "Seyede Mahya", ""], ["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1702.01293", "submitter": "Margarita Osadchy", "authors": "Dolev Raviv and Margarita Osadchy", "title": "Latent Hinge-Minimax Risk Minimization for Inference from a Small Number\n  of Training Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) methods show very good performance when trained on large,\nbalanced data sets. However, many practical problems involve imbalanced data\nsets, or/and classes with a small number of training samples. The performance\nof DL methods as well as more traditional classifiers drops significantly in\nsuch settings. Most of the existing solutions for imbalanced problems focus on\ncustomizing the data for training. A more principled solution is to use mixed\nHinge-Minimax risk [19] specifically designed to solve binary problems with\nimbalanced training sets. Here we propose a Latent Hinge Minimax (LHM) risk and\na training algorithm that generalizes this paradigm to an ensemble of\nhyperplanes that can form arbitrary complex, piecewise linear boundaries. To\nextract good features, we combine LHM model with CNN via transfer learning. To\nsolve multi-class problem we map pre-trained category-specific LHM classifiers\nto a multi-class neural network and adjust the weights with very fast tuning.\nLHM classifier enables the use of unlabeled data in its training and the\nmapping allows for multi-class inference, resulting in a classifier that\nperforms better than alternatives when trained on a small number of training\nsamples.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 14:33:16 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Raviv", "Dolev", ""], ["Osadchy", "Margarita", ""]]}, {"id": "1702.01304", "submitter": "Andrey Kuehlkamp", "authors": "Andrey Kuehlkamp and Benedict Becker and Kevin Bowyer", "title": "Gender-From-Iris or Gender-From-Mascara?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting a person's gender based on the iris texture has been explored by\nseveral researchers. This paper considers several dimensions of experimental\nwork on this problem, including person-disjoint train and test, and the effect\nof cosmetics on eyelash occlusion and imperfect segmentation. We also consider\nthe use of multi-layer perceptron and convolutional neural networks as\nclassifiers, comparing the use of data-driven and hand-crafted features. Our\nresults suggest that the gender-from-iris problem is more difficult than has so\nfar been appreciated. Estimating accuracy using a mean of N person-disjoint\ntrain and test partitions, and considering the effect of makeup - a combination\nof experimental conditions not present in any previous work - we find a much\nweaker ability to predict gender-from-iris texture than has been suggested in\nprevious work.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 16:39:07 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Kuehlkamp", "Andrey", ""], ["Becker", "Benedict", ""], ["Bowyer", "Kevin", ""]]}, {"id": "1702.01315", "submitter": "Nahum Kiryati", "authors": "Naftali Zon, Rana Hanocka, Nahum Kiryati", "title": "Fast and easy blind deblurring using an inverse filter and PROBE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PROBE (Progressive Removal of Blur Residual) is a recursive framework for\nblind deblurring. Using the elementary modified inverse filter at its core,\nPROBE's experimental performance meets or exceeds the state of the art, both\nvisually and quantitatively. Remarkably, PROBE lends itself to analysis that\nreveals its convergence properties. PROBE is motivated by recent ideas on\nprogressive blind deblurring, but breaks away from previous research by its\nsimplicity, speed, performance and potential for analysis. PROBE is neither a\nfunctional minimization approach, nor an open-loop sequential method (blur\nkernel estimation followed by non-blind deblurring). PROBE is a feedback\nscheme, deriving its unique strength from the closed-loop architecture rather\nthan from the accuracy of its algorithmic components.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 17:55:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zon", "Naftali", ""], ["Hanocka", "Rana", ""], ["Kiryati", "Nahum", ""]]}, {"id": "1702.01334", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi and Yao Wang", "title": "An Experimental Study of Deep Convolutional Features For Iris\n  Recognition", "comments": "IEEE Signal Processing in Medicine and Biology Symposium, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris is one of the popular biometrics that is widely used for identity\nauthentication. Different features have been used to perform iris recognition\nin the past. Most of them are based on hand-crafted features designed by\nbiometrics experts. Due to tremendous success of deep learning in computer\nvision problems, there has been a lot of interest in applying features learned\nby convolutional neural networks on general image recognition to other tasks\nsuch as segmentation, face recognition, and object detection. In this paper, we\nhave investigated the application of deep features extracted from VGG-Net for\niris recognition. The proposed scheme has been tested on two well-known iris\ndatabases, and has shown promising results with the best accuracy rate of\n99.4\\%, which outperforms the previous best result.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 19:54:48 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""], ["Wang", "Yao", ""]]}, {"id": "1702.01339", "submitter": "Uche Nnolim", "authors": "U. A. Nnolim", "title": "Entropy-guided Retinex anisotropic diffusion algorithm based on partial\n  differential equations (PDE) for illumination correction", "comments": "31 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the experimental results obtained using a proposed\nvariational Retinex algorithm for controlled illumination correction. Two\ncolour restoration and enhancement schemes of the algorithm are presented for\ndrastically improved results. The algorithm modifies the reflectance image\nusing global and local contrast enhancement approaches and gradually removes\nthe residual illumination to yield highly pleasing results. The proposed\nalgorithms are optimized by way of simultaneous perceptual quality metric (PQM)\nstabilization and entropy maximization for fully automated processing solving\nthe problem of determination of stopping time. The usage of the HSI or HSV\ncolour space ensures a unique solution to the optimization problem unlike in\nthe RGB space where there is none (forcing manual selection of number of\niteration. The proposed approach preserves and enhances details in both bright\nand dark regions of underexposed images in addition to eliminating the colour\ndistortion, over-exposure in bright image regions, halo effect and grey-world\nviolations observed in Retinex-based approaches. Extensive experiments indicate\nconsistent performance as the proposed approach exploits and augments the\nadvantages of PDE-based formulation, performing illumination correction, colour\nenhancement correction and restoration, contrast enhancement and noise\nsuppression. Comparisons shows that the proposed approach surpasses most of the\nother conventional algorithms found in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 20:05:30 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Nnolim", "U. A.", ""]]}, {"id": "1702.01381", "submitter": "Iaroslav Melekhov", "authors": "Iaroslav Melekhov, Juha Ylioinas, Juho Kannala and Esa Rahtu", "title": "Relative Camera Pose Estimation Using Convolutional Neural Networks", "comments": "To be published in proceedings of Advanced Concepts for Intelligent\n  Vision Systems (ACIVS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convolutional neural network based approach for\nestimating the relative pose between two cameras. The proposed network takes\nRGB images from both cameras as input and directly produces the relative\nrotation and translation as output. The system is trained in an end-to-end\nmanner utilising transfer learning from a large scale classification dataset.\nThe introduced approach is compared with widely used local feature based\nmethods (SURF, ORB) and the results indicate a clear improvement over the\nbaseline. In addition, a variant of the proposed architecture containing a\nspatial pyramid pooling (SPP) layer is evaluated and shown to further improve\nthe performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 08:53:24 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 05:31:38 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 07:34:50 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Melekhov", "Iaroslav", ""], ["Ylioinas", "Juha", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "1702.01426", "submitter": "Nadav Israel", "authors": "Nadav Israel, Lior Wolf, Ran Barzilay, Gal Shoval", "title": "Robust features for facial action recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of facial gestures is becoming increasingly important\nas real world AI agents become a reality. In this paper, we present an\nautomated system that recognizes facial gestures by capturing local changes and\nencoding the motion into a histogram of frequencies. We evaluate the proposed\nmethod by demonstrating its effectiveness on spontaneous face action\nbenchmarks: the FEEDTUM dataset, the Pain dataset and the HMDB51 dataset. The\nresults show that, compared to known methods, the new encoding methods\nsignificantly improve the recognition accuracy and the robustness of analysis\nfor a variety of applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 16:28:26 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 17:08:43 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Israel", "Nadav", ""], ["Wolf", "Lior", ""], ["Barzilay", "Ran", ""], ["Shoval", "Gal", ""]]}, {"id": "1702.01444", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Printed Arabic Text Recognition using Linear and Nonlinear Regression", "comments": "http://thesai.org/Downloads/Volume8No1/Paper_29-Printed_Arabic_Text_Recognition_using_Linear.pdf", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 8(1), 2017", "doi": "10.14569/IJACSA.2017.080129", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Arabic language is one of the most popular languages in the world. Hundreds\nof millions of people in many countries around the world speak Arabic as their\nnative speaking. However, due to complexity of Arabic language, recognition of\nprinted and handwritten Arabic text remained untouched for a very long time\ncompared with English and Chinese. Although, in the last few years, significant\nnumber of researches has been done in recognizing printed and handwritten\nArabic text, it stills an open research field due to cursive nature of Arabic\nscript. This paper proposes automatic printed Arabic text recognition technique\nbased on linear and ellipse regression techniques. After collecting all\npossible forms of each character, unique code is generated to represent each\ncharacter form. Each code contains a sequence of lines and ellipses. To\nrecognize fonts, a unique list of codes is identified to be used as a\nfingerprint of font. The proposed technique has been evaluated using over 14000\ndifferent Arabic words with different fonts and experimental results show that\naverage recognition rate of the proposed technique is 86%.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 19:23:59 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1702.01478", "submitter": "Kota Hara", "authors": "Kota Hara, Ming-Yu Liu, Oncel Tuzel, Amir-massoud Farahmand", "title": "Attentional Network for Visual Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose augmenting deep neural networks with an attention mechanism for\nthe visual object detection task. As perceiving a scene, humans have the\ncapability of multiple fixation points, each attended to scene content at\ndifferent locations and scales. However, such a mechanism is missing in the\ncurrent state-of-the-art visual object detection methods. Inspired by the human\nvision system, we propose a novel deep network architecture that imitates this\nattention mechanism. As detecting objects in an image, the network adaptively\nplaces a sequence of glimpses of different shapes at different locations in the\nimage. Evidences of the presence of an object and its location are extracted\nfrom these glimpses, which are then fused for estimating the object class and\nbounding box coordinates. Due to lacks of ground truth annotations of the\nvisual attention mechanism, we train our network using a reinforcement learning\nalgorithm with policy gradients. Experiment results on standard object\ndetection benchmarks show that the proposed network consistently outperforms\nthe baseline networks that does not model the attention mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 00:50:36 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Hara", "Kota", ""], ["Liu", "Ming-Yu", ""], ["Tuzel", "Oncel", ""], ["Farahmand", "Amir-massoud", ""]]}, {"id": "1702.01486", "submitter": "Sen Wang", "authors": "Xinxin Zuo and Sen Wang and Jiangbin Zheng and Ruigang Yang", "title": "Detailed Surface Geometry and Albedo Recovery from RGB-D Video Under\n  Natural Illumination", "comments": "TPAMI accepted", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2955459", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel approach for depth map enhancement from an\nRGB-D video sequence. The basic idea is to exploit the shading information in\nthe color image. Instead of making assumption about surface albedo or\ncontrolled object motion and lighting, we use the lighting variations\nintroduced by casual object movement. We are effectively calculating\nphotometric stereo from a moving object under natural illuminations. The key\ntechnical challenge is to establish correspondences over the entire image set.\nWe therefore develop a lighting insensitive robust pixel matching technique\nthat out-performs optical flow method in presence of lighting variations. In\naddition we present an expectation-maximization framework to recover the\nsurface normal and albedo simultaneously, without any regularization term. We\nhave validated our method on both synthetic and real datasets to show its\nsuperior performance on both surface details recovery and intrinsic\ndecomposition.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 02:28:25 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 23:55:05 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 16:39:43 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 05:06:40 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Zuo", "Xinxin", ""], ["Wang", "Sen", ""], ["Zheng", "Jiangbin", ""], ["Yang", "Ruigang", ""]]}, {"id": "1702.01499", "submitter": "Kota Hara", "authors": "Kota Hara, Raviteja Vemulapalli, Rama Chellappa", "title": "Designing Deep Convolutional Neural Networks for Continuous Object\n  Orientation Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNN) have been proven to be effective\nfor various computer vision problems. In this work, we demonstrate its\neffectiveness on a continuous object orientation estimation task, which\nrequires prediction of 0 to 360 degrees orientation of the objects. We do so by\nproposing and comparing three continuous orientation prediction approaches\ndesigned for the DCNNs. The first two approaches work by representing an\norientation as a point on a unit circle and minimizing either L2 loss or\nangular difference loss. The third method works by first converting the\ncontinuous orientation estimation task into a set of discrete orientation\nestimation tasks and then converting the discrete orientation outputs back to\nthe continuous orientation using a mean-shift algorithm. By evaluating on a\nvehicle orientation estimation task and a pedestrian orientation estimation\ntask, we demonstrate that the discretization-based approach not only works\nbetter than the other two approaches but also achieves state-of-the-art\nperformance. We also demonstrate that finding an appropriate feature\nrepresentation is critical to achieve a good performance when adapting a DCNN\ntrained for an image recognition task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 05:33:45 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Hara", "Kota", ""], ["Vemulapalli", "Raviteja", ""], ["Chellappa", "Rama", ""]]}, {"id": "1702.01507", "submitter": "Yong Wang", "authors": "Yong Wang, Ke Lu", "title": "Challenge of Multi-Camera Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-camera tracking is quite different from single camera tracking, and it\nfaces new technology and system architecture challenges. By analyzing the\ncorresponding characteristics and disadvantages of the existing algorithms,\nproblems in multi-camera tracking are summarized and some new directions for\nfuture work are also generalized.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 06:32:14 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Wang", "Yong", ""], ["Lu", "Ke", ""]]}, {"id": "1702.01528", "submitter": "Jinsoo Choi", "authors": "Jinsoo Choi, Tae-Hyun Oh, In So Kweon", "title": "Contextually Customized Video Summaries via Natural Language", "comments": "To appear in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best summary of a long video differs among different people due to its\nhighly subjective nature. Even for the same person, the best summary may change\nwith time or mood. In this paper, we introduce the task of generating\ncustomized video summaries through simple text. First, we train a deep\narchitecture to effectively learn semantic embeddings of video frames by\nleveraging the abundance of image-caption data via a progressive and residual\nmanner. Given a user-specific text description, our algorithm is able to select\nsemantically relevant video segments and produce a temporally aligned video\nsummary. In order to evaluate our textually customized video summaries, we\nconduct experimental comparison with baseline methods that utilize ground-truth\ninformation. Despite the challenging baselines, our method still manages to\nshow comparable or even exceeding performance. We also show that our method is\nable to generate semantically diverse video summaries by only utilizing the\nlearned visual embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 08:31:44 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 11:37:58 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 06:13:45 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1702.01636", "submitter": "Enzo Ferrante", "authors": "Enzo Ferrante and Nikos Paragios", "title": "Slice-to-volume medical image registration: a survey", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2017.04.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades, the research community of medical imaging has\nwitnessed continuous advances in image registration methods, which pushed the\nlimits of the state-of-the-art and enabled the development of novel medical\nprocedures. A particular type of image registration problem, known as\nslice-to-volume registration, played a fundamental role in areas like image\nguided surgeries and volumetric image reconstruction. However, to date, and\ndespite the extensive literature available on this topic, no survey has been\nwritten to discuss this challenging problem. This paper introduces the first\ncomprehensive survey of the literature about slice-to-volume registration,\npresenting a categorical study of the algorithms according to an ad-hoc\ntaxonomy and analyzing advantages and disadvantages of every category. We draw\nsome general conclusions from this analysis and present our perspectives on the\nfuture of the field.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 14:51:29 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 14:49:15 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ferrante", "Enzo", ""], ["Paragios", "Nikos", ""]]}, {"id": "1702.01638", "submitter": "Xinyu Li", "authors": "Xinyu Li, Yanyi Zhang, Jianyu Zhang, Shuhong Chen, Ivan Marsic,\n  Richard A. Farneth, Randall S. Burd", "title": "Concurrent Activity Recognition with Multimodal CNN-LSTM Structure", "comments": "14 pages, 12 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a system that recognizes concurrent activities from real-world\ndata captured by multiple sensors of different types. The recognition is\nachieved in two steps. First, we extract spatial and temporal features from the\nmultimodal data. We feed each datatype into a convolutional neural network that\nextracts spatial features, followed by a long-short term memory network that\nextracts temporal information in the sensory data. The extracted features are\nthen fused for decision making in the second step. Second, we achieve\nconcurrent activity recognition with a single classifier that encodes a binary\noutput vector in which elements indicate whether the corresponding activity\ntypes are currently in progress. We tested our system with three datasets from\ndifferent domains recorded using different sensors and achieved performance\ncomparable to existing systems designed specifically for those domains. Our\nsystem is the first to address the concurrent activity recognition with\nmultisensory data using a single model, which is scalable, simple to train and\neasy to deploy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 15:01:45 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Li", "Xinyu", ""], ["Zhang", "Yanyi", ""], ["Zhang", "Jianyu", ""], ["Chen", "Shuhong", ""], ["Marsic", "Ivan", ""], ["Farneth", "Richard A.", ""], ["Burd", "Randall S.", ""]]}, {"id": "1702.01721", "submitter": "Afshin Dehghan", "authors": "Afshin Dehghan, Syed Zain Masood, Guang Shu, Enrique. G. Ortiz", "title": "View Independent Vehicle Make, Model and Color Recognition Using\n  Convolutional Neural Network", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the details of Sighthound's fully automated vehicle\nmake, model and color recognition system. The backbone of our system is a deep\nconvolutional neural network that is not only computationally inexpensive, but\nalso provides state-of-the-art results on several competitive benchmarks.\nAdditionally, our deep network is trained on a large dataset of several million\nimages which are labeled through a semi-automated process. Finally we test our\nsystem on several public datasets as well as our own internal test dataset. Our\nresults show that we outperform other methods on all benchmarks by significant\nmargins. Our model is available to developers through the Sighthound Cloud API\nat https://www.sighthound.com/products/cloud\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:47:08 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Dehghan", "Afshin", ""], ["Masood", "Syed Zain", ""], ["Shu", "Guang", ""], ["Ortiz", "Enrique. G.", ""]]}, {"id": "1702.01731", "submitter": "Mohammadreza Babaee", "authors": "Mohammadreza Babaee, Duc Tung Dinh, Gerhard Rigoll", "title": "A Deep Convolutional Neural Network for Background Subtraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel background subtraction system that uses a\ndeep Convolutional Neural Network (CNN) to perform the segmentation. With this\napproach, feature engineering and parameter tuning become unnecessary since the\nnetwork parameters can be learned from data by training a single CNN that can\nhandle various video scenes. Additionally, we propose a new approach to\nestimate background model from video. For the training of the CNN, we employed\nrandomly 5 percent video frames and their ground truth segmentations taken from\nthe Change Detection challenge 2014(CDnet 2014). We also utilized\nspatial-median filtering as the post-processing of the network outputs. Our\nmethod is evaluated with different data-sets, and the network outperforms the\nexisting algorithms with respect to the average ranking over different\nevaluation metrics. Furthermore, due to the network architecture, our CNN is\ncapable of real time processing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 18:24:04 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Babaee", "Mohammadreza", ""], ["Dinh", "Duc Tung", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1702.01846", "submitter": "Masatoshi Hidaka", "authors": "Masatoshi Hidaka, Ken Miura and Tatsuya Harada", "title": "Development of JavaScript-based deep learning platform and application\n  to distributed training", "comments": "Workshop paper for ICLR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is increasingly attracting attention for processing big data.\nExisting frameworks for deep learning must be set up to specialized computer\nsystems. Gaining sufficient computing resources therefore entails high costs of\ndeployment and maintenance. In this work, we implement a matrix library and\ndeep learning framework that uses JavaScript. It can run on web browsers\noperating on ordinary personal computers and smartphones. Using JavaScript,\ndeep learning can be accomplished in widely diverse environments without the\nnecessity for software installation. Using GPGPU from WebCL framework, our\nframework can train large scale convolutional neural networks such as VGGNet\nand ResNet. In the experiments, we demonstrate their practicality by training\nVGGNet in a distributed manner using web browsers as the client.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 02:02:08 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 03:30:49 GMT"}, {"version": "v3", "created": "Mon, 27 Mar 2017 09:28:06 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Hidaka", "Masatoshi", ""], ["Miura", "Ken", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1702.01847", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Low Rank Matrix Recovery with Simultaneous Presence of Outliers and\n  Sparse Corruption", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2876604", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a data model in which the data matrix D can be expressed as D = L +\nS + C, where L is a low rank matrix, S an element-wise sparse matrix and C a\nmatrix whose non-zero columns are outlying data points. To date, robust PCA\nalgorithms have solely considered models with either S or C, but not both. As\nsuch, existing algorithms cannot account for simultaneous element-wise and\ncolumn-wise corruptions. In this paper, a new robust PCA algorithm that is\nrobust to simultaneous types of corruption is proposed. Our approach hinges on\nthe sparse approximation of a sparsely corrupted column so that the sparse\nexpansion of a column with respect to the other data points is used to\ndistinguish a sparsely corrupted inlier column from an outlying data point. We\nalso develop a randomized design which provides a scalable implementation of\nthe proposed approach. The core idea of sparse approximation is analyzed\nanalytically where we show that the underlying ell_1-norm minimization can\nobtain the representation of an inlier in presence of sparse corruptions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 02:08:51 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1702.01870", "submitter": "Ameer Pasha Hosseinbor", "authors": "A. Pasha Hosseinbor, Renat Zhdanov, Alexander Ushveridze", "title": "A New Point-set Registration Algorithm for Fingerprint Matching", "comments": "Point pattern matching, point-set registration, fingerprint, minutia\n  matching, alignment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel minutia-based fingerprint matching algorithm is proposed that employs\niterative global alignment on two minutia sets. The matcher considers all\npossible minutia pairings and iteratively aligns the two sets until the number\nof minutia pairs does not exceed the maximum number of allowable one-to-one\npairings. The optimal alignment parameters are derived analytically via linear\nleast squares. The first alignment establishes a region of overlap between the\ntwo minutia sets, which is then (iteratively) refined by each successive\nalignment. After each alignment, minutia pairs that exhibit weak correspondence\nare discarded. The process is repeated until the number of remaining pairs no\nlonger exceeds the maximum number of allowable one-to-one pairings. The\nproposed algorithm is tested on both the FVC2000 and FVC2002 databases, and the\nresults indicate that the proposed matcher is both effective and efficient for\nfingerprint authentication; it is fast and does not utilize any computationally\nexpensive mathematical functions (e.g. trigonometric, exponential). In addition\nto the proposed matcher, another contribution of the paper is the analytical\nderivation of the least squares solution for the optimal alignment parameters\nfor two point-sets lacking exact correspondence.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 03:43:31 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Hosseinbor", "A. Pasha", ""], ["Zhdanov", "Renat", ""], ["Ushveridze", "Alexander", ""]]}, {"id": "1702.01933", "submitter": "Shubham Pachori", "authors": "Shubham Pachori, Ameya Deshpande, Shanmuganathan Raman", "title": "Hashing in the Zero Shot Framework with Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques to learn hash codes which can store and retrieve large dimensional\nmultimedia data efficiently have attracted broad research interests in the\nrecent years. With rapid explosion of newly emerged concepts and online data,\nexisting supervised hashing algorithms suffer from the problem of scarcity of\nground truth annotations due to the high cost of obtaining manual annotations.\nTherefore, we propose an algorithm to learn a hash function from training\nimages belonging to `seen' classes which can efficiently encode images of\n`unseen' classes to binary codes. Specifically, we project the image features\nfrom visual space and semantic features from semantic space into a common\nHamming subspace. Earlier works to generate hash codes have tried to relax the\ndiscrete constraints on hash codes and solve the continuous optimization\nproblem. However, it often leads to quantization errors. In this work, we use\nthe max-margin classifier to learn an efficient hash function. To address the\nconcern of domain-shift which may arise due to the introduction of new classes,\nwe also introduce an unsupervised domain adaptation model in the proposed\nhashing framework. Results on the three datasets show the advantage of using\ndomain adaptation in learning a high-quality hash function and superiority of\nour method for the task of image retrieval performance as compared to several\nstate-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 09:22:11 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 19:43:41 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Pachori", "Shubham", ""], ["Deshpande", "Ameya", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1702.01961", "submitter": "Renato Budinich", "authors": "Renato Budinich", "title": "A Region Based Easy Path Wavelet Transform For Sparse Image\n  Representation", "comments": "Fixed use of HaarPSI - see Figure 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Easy Path Wavelet Transform is an adaptive transform for bivariate\nfunctions (in particular natural images) which has been proposed in [1]. It\nprovides a sparse representation by finding a path in the domain of the\nfunction leveraging the local correlations of the function values. It then\napplies a one dimensional wavelet transform to the obtained vector, decimates\nthe points and iterates the procedure. The main drawback of such method is the\nneed to store, for each level of the transform, the path which vectorizes the\ntwo dimensional data. Here we propose a variation on the method which consists\nof firstly applying a segmentation procedure to the function domain,\npartitioning it into regions where the variation in the function values is low;\nin a second step, inside each such region, a path is found in some\ndeterministic way, i.e. not data-dependent. This circumvents the need to store\nthe paths at each level, while still obtaining good quality lossy compression.\nThis method is particularly well suited to encode a Region of Interest in the\nimage with different quality than the rest of the image.\n  [1] Gerlind Plonka. The easy path wavelet transform: A new adaptive wavelet\ntransform for sparse representation of two-dimensional data. Multiscale\nModeling & Simulation, 7(3):1474$-$1496, 2008.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 11:13:08 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 10:41:38 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Budinich", "Renato", ""]]}, {"id": "1702.01970", "submitter": "Naushad Ansari", "authors": "Naushad Ansari, Anubha Gupta", "title": "Image Reconstruction using Matched Wavelet Estimated from Data Sensed\n  Compressively using Partial Canonical Identity Matrix", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2700719", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a joint framework wherein lifting-based, separable,\nimage-matched wavelets are estimated from compressively sensed (CS) images and\nused for the reconstruction of the same. Matched wavelet can be easily designed\nif full image is available. Also matched wavelet may provide better\nreconstruction results in CS application compared to standard wavelet\nsparsifying basis. Since in CS application, we have compressively sensed image\ninstead of full image, existing methods of designing matched wavelet cannot be\nused. Thus, we propose a joint framework that estimates matched wavelet from\nthe compressively sensed images and also reconstructs full images. This paper\nhas three significant contributions. First, lifting-based, image-matched\nseparable wavelet is designed from compressively sensed images and is also used\nto reconstruct the same. Second, a simple sensing matrix is employed to sample\ndata at sub-Nyquist rate such that sensing and reconstruction time is reduced\nconsiderably without any noticeable degradation in the reconstruction\nperformance. Third, a new multi-level L-Pyramid wavelet decomposition strategy\nis provided for separable wavelet implementation on images that leads to\nimproved reconstruction performance. Compared to CS-based reconstruction using\nstandard wavelets with Gaussian sensing matrix and with existing wavelet\ndecomposition strategy, the proposed methodology provides faster and better\nimage reconstruction in compressive sensing application.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 11:46:49 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ansari", "Naushad", ""], ["Gupta", "Anubha", ""]]}, {"id": "1702.01983", "submitter": "Grigory Antipov", "authors": "Grigory Antipov, Moez Baccouche, Jean-Luc Dugelay", "title": "Face Aging With Conditional Generative Adversarial Networks", "comments": "5 pages, 3 figures, accepted at ICIP 2017. With respect to v1: (1)\n  changed the abbreviation of the main model from \"acGAN\" to \"Age-cGAN\" in\n  order to avoid confusion with \"Auxiliary Classifier Generative Adversarial\n  Networks\" introduced by Odena et al.; (2) corrected a typo in Formula 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that Generative Adversarial Networks (GANs) can\nproduce synthetic images of exceptional visual fidelity. In this work, we\npropose the GAN-based method for automatic face aging. Contrary to previous\nworks employing GANs for altering of facial attributes, we make a particular\nemphasize on preserving the original person's identity in the aged version of\nhis/her face. To this end, we introduce a novel approach for\n\"Identity-Preserving\" optimization of GAN's latent vectors. The objective\nevaluation of the resulting aged and rejuvenated face images by the\nstate-of-the-art face recognition and age estimation solutions demonstrate the\nhigh potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 12:40:35 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 14:02:03 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Antipov", "Grigory", ""], ["Baccouche", "Moez", ""], ["Dugelay", "Jean-Luc", ""]]}, {"id": "1702.02012", "submitter": "Tanushri Chakravorty", "authors": "Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger", "title": "Tracking using Numerous Anchor points", "comments": "Revised text version. Accepted for publication in Journal of Machine\n  Vision and Applications, December, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an online adaptive model-free tracker is proposed to track\nsingle objects in video sequences to deal with real-world tracking challenges\nlike low-resolution, object deformation, occlusion and motion blur. The novelty\nlies in the construction of a strong appearance model that captures features\nfrom the initialized bounding box and then are assembled into anchor-point\nfeatures. These features memorize the global pattern of the object and have an\ninternal star graph-like structure. These features are unique and flexible and\nhelps tracking generic and deformable objects with no limitation on specific\nobjects. In addition, the relevance of each feature is evaluated online using\nshort-term consistency and long-term consistency. These parameters are adapted\nto retain consistent features that vote for the object location and that deal\nwith outliers for long-term tracking scenarios. Additionally, voting in a\nGaussian manner helps in tackling inherent noise of the tracking system and in\naccurate object localization. Furthermore, the proposed tracker uses pairwise\ndistance measure to cope with scale variations and combines pixel-level binary\nfeatures and global weighted color features for model update. Finally,\nexperimental results on a visual tracking benchmark dataset are presented to\ndemonstrate the effectiveness and competitiveness of the proposed tracker.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:51:06 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 21:31:28 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chakravorty", "Tanushri", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Granger", "Eric", ""]]}, {"id": "1702.02138", "submitter": "Xinlei Chen", "authors": "Xinlei Chen, Abhinav Gupta", "title": "An Implementation of Faster RCNN with Study for Region Sampling", "comments": "Technical Report, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapted the join-training scheme of Faster RCNN framework from Caffe to\nTensorFlow as a baseline implementation for object detection. Our code is made\npublicly available. This report documents the simplifications made to the\noriginal pipeline, with justifications from ablation analysis on both PASCAL\nVOC 2007 and COCO 2014. We further investigated the role of non-maximal\nsuppression (NMS) in selecting regions-of-interest (RoIs) for region\nclassification, and found that a biased sampling toward small regions helps\nperformance and can achieve on-par mAP to NMS-based sampling when converged\nsufficiently.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 18:54:34 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 07:02:25 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Chen", "Xinlei", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1702.02175", "submitter": "Francis Engelmann", "authors": "Anton Kasyanov, Francis Engelmann, J\\\"org St\\\"uckler, Bastian Leibe", "title": "Keyframe-Based Visual-Inertial Online SLAM with Relocalization", "comments": null, "journal-ref": null, "doi": "10.1109/IROS.2017.8206581", "report-no": "RWTH-2018-221873", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementing images with inertial measurements has become one of the most\npopular approaches to achieve highly accurate and robust real-time camera pose\ntracking. In this paper, we present a keyframe-based approach to\nvisual-inertial simultaneous localization and mapping (SLAM) for monocular and\nstereo cameras. Our visual-inertial SLAM system is based on a real-time capable\nvisual-inertial odometry method that provides locally consistent trajectory and\nmap estimates. We achieve global consistency in the estimate through online\nloop-closing and non-linear optimization. Furthermore, our system supports\nrelocalization in a map that has been previously obtained and allows for\ncontinued SLAM operation. We evaluate our approach in terms of accuracy,\nrelocalization capability and run-time efficiency on public indoor benchmark\ndatasets and on newly recorded outdoor sequences. We demonstrate\nstate-of-the-art performance of our system compared to a visual-inertial\nodometry method and baseline visual SLAM approaches in recovering the\ntrajectory of the camera.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:32:50 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 15:18:57 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Kasyanov", "Anton", ""], ["Engelmann", "Francis", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Leibe", "Bastian", ""]]}, {"id": "1702.02223", "submitter": "Zongwei Zhou", "authors": "Hongkai Wang, Zongwei Zhou, Yingci Li, Zhonghua Chen, Peiou Lu, Wenzhi\n  Wang, Wanyu Liu and Lijuan Yu", "title": "Comparison of machine learning methods for classifying mediastinal lymph\n  node metastasis of non-small cell lung cancer from 18F-FDG PET/CT images", "comments": null, "journal-ref": null, "doi": "10.1186/s13550-017-0260-9", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study shows that the performance of CNN is not significantly\ndifferent from the best classical methods and human doctors for classifying\nmediastinal lymph node metastasis of NSCLC from PET/CT images. Because CNN does\nnot need tumor segmentation or feature calculation, it is more convenient and\nmore objective than the classical methods. However, CNN does not make use of\nthe import diagnostic features, which have been proved more discriminative than\nthe texture features for classifying small-sized lymph nodes. Therefore,\nincorporating the diagnostic features into CNN is a promising direction for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 23:12:45 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Wang", "Hongkai", ""], ["Zhou", "Zongwei", ""], ["Li", "Yingci", ""], ["Chen", "Zhonghua", ""], ["Lu", "Peiou", ""], ["Wang", "Wenzhi", ""], ["Liu", "Wanyu", ""], ["Yu", "Lijuan", ""]]}, {"id": "1702.02235", "submitter": "Pei Wang", "authors": "Pei Wang, Guochao Bu, Ronghao Li, Rui Zhao", "title": "Automated Low-cost Terrestrial Laser Scanner for Measuring Diameters at\n  Breast Height and Heights of Forest Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrestrial laser scanner is a kind of fast, high-precision data acquisition\ndevice, which had been more and more applied to the research areas of forest\ninventory. In this study, a kind of automated low-cost terrestrial laser\nscanner was designed and implemented based on a two-dimensional laser radar\nsensor SICK LMS-511 and a stepper motor. The new scanner was named as BEE,\nwhich can scan the forest trees in three dimension. The BEE scanner and its\nsupporting software are specifically designed for forest inventory. The\nexperiments have been performed by using the BEE scanner in an artificial\nginkgo forest which was located in Haidian district of Beijing. Four square\nplots were selected to do the experiments. The BEE scanner scanned in the four\nplots and acquired the single scan data respectively. The DBH, tree height and\ntree position of trees in the four plots were estimated and analyzed. For\ncomparison, the manual measured data was also collected in the four plots. The\ntree stem detection rate for all four plots was 92.75%; the root mean square\nerror of the DBH estimation was 1.27cm; the root mean square error of the tree\nheight estimation was 0.24m; the tree position estimation was in line with the\nactual position. Experimental results show that the BEE scanner can efficiently\nestimate the structure parameters of forest trees and has a good potential in\npractical application of forest inventory.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 00:50:29 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 03:52:49 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Wang", "Pei", ""], ["Bu", "Guochao", ""], ["Li", "Ronghao", ""], ["Zhao", "Rui", ""]]}, {"id": "1702.02258", "submitter": "Ehsan Jahangiri", "authors": "Ehsan Jahangiri, Alan L. Yuille", "title": "Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with\n  2D Joint Detections", "comments": "accepted to ICCV 2017 (PeopleCap)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method to generate multiple diverse and valid human pose\nhypotheses in 3D all consistent with the 2D detection of joints in a monocular\nRGB image. We use a novel generative model uniform (unbiased) in the space of\nanatomically plausible 3D poses. Our model is compositional (produces a pose by\ncombining parts) and since it is restricted only by anatomical constraints it\ncan generalize to every plausible human 3D pose. Removing the model bias\nintrinsically helps to generate more diverse 3D pose hypotheses. We argue that\ngenerating multiple pose hypotheses is more reasonable than generating only a\nsingle 3D pose based on the 2D joint detection given the depth ambiguity and\nthe uncertainty due to occlusion and imperfect 2D joint detection. We hope that\nthe idea of generating multiple consistent pose hypotheses can give rise to a\nnew line of future work that has not received much attention in the literature.\nWe used the Human3.6M dataset for empirical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 02:54:25 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 19:39:19 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Jahangiri", "Ehsan", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.02295", "submitter": "Yi Zhu", "authors": "Yi Zhu, Zhenzhong Lan, Shawn Newsam, Alexander G. Hauptmann", "title": "Guided Optical Flow Learning", "comments": "CVPR17 Workshop. Code available at\n  https://github.com/bryanyzhu/GuidedNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the unsupervised learning of CNNs for optical flow estimation using\nproxy ground truth data. Supervised CNNs, due to their immense learning\ncapacity, have shown superior performance on a range of computer vision\nproblems including optical flow prediction. They however require the ground\ntruth flow which is usually not accessible except on limited synthetic data.\nWithout the guidance of ground truth optical flow, unsupervised CNNs often\nperform worse as they are naturally ill-conditioned. We therefore propose a\nnovel framework in which proxy ground truth data generated from classical\napproaches is used to guide the CNN learning. The models are further refined in\nan unsupervised fashion using an image reconstruction loss. Our guided learning\napproach is competitive with or superior to state-of-the-art approaches on\nthree standard benchmark datasets yet is completely unsupervised and can run in\nreal time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 05:42:09 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 18:12:09 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhu", "Yi", ""], ["Lan", "Zhenzhong", ""], ["Newsam", "Shawn", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1702.02359", "submitter": "Bolun Cai", "authors": "Lingke Zeng, Xiangmin Xu, Bolun Cai, Suo Qiu, Tong Zhang", "title": "Multi-scale Convolutional Neural Networks for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting on static images is a challenging problem due to scale\nvariations. Recently deep neural networks have been shown to be effective in\nthis task. However, existing neural-networks-based methods often use the\nmulti-column or multi-network model to extract the scale-relevant features,\nwhich is more complicated for optimization and computation wasting. To this\nend, we propose a novel multi-scale convolutional neural network (MSCNN) for\nsingle image crowd counting. Based on the multi-scale blobs, the network is\nable to generate scale-relevant features for higher crowd counting performances\nin a single-column architecture, which is both accuracy and cost effective for\npractical applications. Complemental results show that our method outperforms\nthe state-of-the-art methods on both accuracy and robustness with far less\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 10:30:32 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Zeng", "Lingke", ""], ["Xu", "Xiangmin", ""], ["Cai", "Bolun", ""], ["Qiu", "Suo", ""], ["Zhang", "Tong", ""]]}, {"id": "1702.02382", "submitter": "Mateusz Kozi\\'nski", "authors": "Mateusz Kozi\\'nski and Lo\\\"ic Simon and Fr\\'ed\\'eric Jurie", "title": "An Adversarial Regularisation for Semi-Supervised Training of Structured\n  Output Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for semi-supervised training of structured-output neural\nnetworks. Inspired by the framework of Generative Adversarial Networks (GAN),\nwe train a discriminator network to capture the notion of a quality of network\noutput. To this end, we leverage the qualitative difference between outputs\nobtained on the labelled training data and unannotated data. We then use the\ndiscriminator as a source of error signal for unlabelled data. This effectively\nboosts the performance of a network on a held out test set. Initial experiments\nin image segmentation demonstrate that the proposed framework enables achieving\nthe same network performance as in a fully supervised scenario, while using two\ntimes less annotations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 11:40:15 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Kozi\u0144ski", "Mateusz", ""], ["Simon", "Lo\u00efc", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1702.02445", "submitter": "Afonso Teodoro", "authors": "Afonso M. Teodoro, Jos\\'e M. Bioucas-Dias, M\\'ario A. T. Figueiredo", "title": "Scene-adapted plug-and-play algorithm with convergence guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent frameworks, such as the so-called plug-and-play, allow us to leverage\nthe developments in image denoising to tackle other, and more involved,\nproblems in image processing. As the name suggests, state-of-the-art denoisers\nare plugged into an iterative algorithm that alternates between a denoising\nstep and the inversion of the observation operator. While these tools offer\nflexibility, the convergence of the resulting algorithm may be difficult to\nanalyse. In this paper, we plug a state-of-the-art denoiser, based on a\nGaussian mixture model, in the iterations of an alternating direction method of\nmultipliers and prove the algorithm is guaranteed to converge. Moreover, we\nbuild upon the concept of scene-adapted priors where we learn a model targeted\nto a specific scene being imaged, and apply the proposed method to address the\nhyperspectral sharpening problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 14:42:05 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 14:02:14 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Teodoro", "Afonso M.", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1702.02447", "submitter": "Hengkai Guo", "authors": "Hengkai Guo, Guijin Wang, Xinghao Chen, Cairong Zhang, Fei Qiao,\n  Huazhong Yang", "title": "Region Ensemble Network: Improving Convolutional Network for Hand Pose\n  Estimation", "comments": "Accepted to ICIP 2017. Project:\n  https://github.com/guohengkai/region-ensemble-network", "journal-ref": null, "doi": "10.1109/ICIP.2017.8297136", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from monocular depth images is an important and\nchallenging problem for human-computer interaction. Recently deep convolutional\nnetworks (ConvNet) with sophisticated design have been employed to address it,\nbut the improvement over traditional methods is not so apparent. To promote the\nperformance of directly 3D coordinate regression, we propose a tree-structured\nRegion Ensemble Network (REN), which partitions the convolution outputs into\nregions and integrates the results from multiple regressors on each regions.\nCompared with multi-model ensemble, our model is completely end-to-end\ntraining. The experimental results demonstrate that our approach achieves the\nbest performance among state-of-the-arts on two public datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 14:44:31 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 03:07:31 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Guo", "Hengkai", ""], ["Wang", "Guijin", ""], ["Chen", "Xinghao", ""], ["Zhang", "Cairong", ""], ["Qiao", "Fei", ""], ["Yang", "Huazhong", ""]]}, {"id": "1702.02463", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Raymond A. Yeh, Xiaoou Tang, Yiming Liu, Aseem Agarwala", "title": "Video Frame Synthesis using Deep Voxel Flow", "comments": "To appear in ICCV 2017 as an oral paper. More details at the project\n  page: https://liuziwei7.github.io/projects/VoxelFlow.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of synthesizing new video frames in an existing video,\neither in-between existing frames (interpolation), or subsequent to them\n(extrapolation). This problem is challenging because video appearance and\nmotion can be highly complex. Traditional optical-flow-based solutions often\nfail where flow estimation is challenging, while newer neural-network-based\nmethods that hallucinate pixel values directly often produce blurry results. We\ncombine the advantages of these two methods by training a deep network that\nlearns to synthesize video frames by flowing pixel values from existing ones,\nwhich we call deep voxel flow. Our method requires no human supervision, and\nany video can be used as training data by dropping, and then learning to\npredict, existing frames. The technique is efficient, and can be applied at any\nvideo resolution. We demonstrate that our method produces results that both\nquantitatively and qualitatively improve upon the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 15:20:14 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 04:43:44 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Liu", "Ziwei", ""], ["Yeh", "Raymond A.", ""], ["Tang", "Xiaoou", ""], ["Liu", "Yiming", ""], ["Agarwala", "Aseem", ""]]}, {"id": "1702.02508", "submitter": "Corneliu Arsene Dr", "authors": "Corneliu Arsene, Peter Pormann, William Sellers, Siam Bhayro", "title": "Computational Techniques in Multispectral Image Processing: Application\n  to the Syriac Galen Palimpsest", "comments": "29 February - 2 March 2016, Second International Conference on\n  Natural Sciences and Technology in Manuscript Analysis, Centre for the study\n  of Manuscript Cultures, Hamburg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral and hyperspectral image analysis has experienced much\ndevelopment in the last decade. The application of these methods to palimpsests\nhas produced significant results, enabling researchers to recover texts that\nwould be otherwise lost under the visible overtext, by improving the contrast\nbetween the undertext and the overtext. In this paper we explore an extended\nnumber of multispectral and hyperspectral image analysis methods, consisting of\nsupervised and unsupervised dimensionality reduction techniques, on a part of\nthe Syriac Galen Palimpsest dataset (www.digitalgalen.net). Of this extended\nset of methods, eight methods gave good results: three were supervised methods\nGeneralized Discriminant Analysis (GDA), Linear Discriminant Analysis (LDA),\nand Neighborhood Component Analysis (NCA); and the other five methods were\nunsupervised methods (but still used in a supervised way) Gaussian Process\nLatent Variable Model (GPLVM), Isomap, Landmark Isomap, Principal Component\nAnalysis (PCA), and Probabilistic Principal Component Analysis (PPCA). The\nrelative success of these methods was determined visually, using color\npictures, on the basis of whether the undertext was distinguishable from the\novertext, resulting in the following ranking of the methods: LDA, NCA, GDA,\nIsomap, Landmark Isomap, PPCA, PCA, and GPLVM. These results were compared with\nthose obtained using the Canonical Variates Analysis (CVA) method on the same\ndataset, which showed remarkably accuracy (LDA is a particular case of CVA\nwhere the objects are classified to two classes).\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 13:03:20 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Arsene", "Corneliu", ""], ["Pormann", "Peter", ""], ["Sellers", "William", ""], ["Bhayro", "Siam", ""]]}, {"id": "1702.02512", "submitter": "Yi Zhou", "authors": "Yi Zhou, Laurent Kneip and Hongdong Li", "title": "Semi-Dense Visual Odometry for RGB-D Cameras Using Approximate Nearest\n  Neighbour Fields", "comments": "ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust and efficient semi-dense visual odometry\nsolution for RGB-D cameras. The core of our method is a 2D-3D ICP pipeline\nwhich estimates the pose of the sensor by registering the projection of a 3D\nsemi-dense map of the reference frame with the 2D semi-dense region extracted\nin the current frame. The processing is speeded up by efficiently implemented\napproximate nearest neighbour fields under the Euclidean distance criterion,\nwhich permits the use of compact Gauss-Newton updates in the optimization. The\nregistration is formulated as a maximum a posterior problem to deal with\noutliers and sensor noises, and consequently the equivalent weighted least\nsquares problem is solved by iteratively reweighted least squares method. A\nvariety of robust weight functions are tested and the optimum is determined\nbased on the characteristics of the sensor model. Extensive evaluation on\npublicly available RGB-D datasets shows that the proposed method predominantly\noutperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 00:12:37 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Zhou", "Yi", ""], ["Kneip", "Laurent", ""], ["Li", "Hongdong", ""]]}, {"id": "1702.02514", "submitter": "Markus H\\\"oll BSc", "authors": "Markus H\\\"oll, Vincent Lepetit", "title": "Monocular LSD-SLAM Integration within AR System", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.10054.27205", "report-no": "ICG--CVARLab-TR--ICGCVARLab-TR003", "categories": "cs.CV cs.GR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we cover the process of integrating Large-Scale Direct\nSimultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR\nstereo engine, developed for our modified \"Augmented Reality Oculus Rift\". With\nthat, we are able to track one of our realworld cameras which are mounted on\nthe rift, within a complete unknown environment. This makes it possible to\nachieve a constant and full augmentation, synchronizing our 3D movement (x, y,\nz) in both worlds, the real world and the virtual world. The development for\nthe basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully\ndocumented in our previous paper. After an introduction to image-based\nregistration, we detail the LSD-SLAM algorithm and document our code\nimplementing our integration. The AR stereo engine with Oculus Rift support can\nbe accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and\nthe modified LSD-SLAM project used for the integration is available here\nhttps://github.com/MaXvanHeLL/LSD-SLAM.git.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 16:52:19 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["H\u00f6ll", "Markus", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1702.02537", "submitter": "Olasimbo Ayodeji Arigbabu", "authors": "Olasimbo Ayodeji Arigbabu, Sharifah Mumtazah Syed Ahmad, Wan Azizun\n  Wan Adnan, Salman Yussof, Saif Mahmood", "title": "Soft Biometrics: Gender Recognition from Unconstrained Face Images using\n  Local Feature Descriptor", "comments": null, "journal-ref": "Journal of Information and Communication Technology (JICT), 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender recognition from unconstrained face images is a challenging task due\nto the high degree of misalignment, pose, expression, and illumination\nvariation. In previous works, the recognition of gender from unconstrained face\nimages is approached by utilizing image alignment, exploiting multiple samples\nper individual to improve the learning ability of the classifier, or learning\ngender based on prior knowledge about pose and demographic distributions of the\ndataset. However, image alignment increases the complexity and time of\ncomputation, while the use of multiple samples or having prior knowledge about\ndata distribution is unrealistic in practical applications. This paper presents\nan approach for gender recognition from unconstrained face images. Our\ntechnique exploits the robustness of local feature descriptor to photometric\nvariations to extract the shape description of the 2D face image using a single\nsample image per individual. The results obtained from experiments on Labeled\nFaces in the Wild (LFW) dataset describe the effectiveness of the proposed\nmethod. The essence of this study is to investigate the most suitable functions\nand parameter settings for recognizing gender from unconstrained face images.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:34:53 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Arigbabu", "Olasimbo Ayodeji", ""], ["Ahmad", "Sharifah Mumtazah Syed", ""], ["Adnan", "Wan Azizun Wan", ""], ["Yussof", "Salman", ""], ["Mahmood", "Saif", ""]]}, {"id": "1702.02549", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Fabian Groh, Hendrik P.A. Lensch", "title": "Backpropagation Training for Fisher Vectors within Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher-Vectors (FV) encode higher-order statistics of a set of multiple local\ndescriptors like SIFT features. They already show good performance in\ncombination with shallow learning architectures on visual recognitions tasks.\nCurrent methods using FV as a feature descriptor in deep architectures assume\nthat all original input features are static. We propose a framework to jointly\nlearn the representation of original features, FV parameters and parameters of\nthe classifier in the style of traditional neural networks. Our proof of\nconcept implementation improves the performance of FV on the Pascal Voc 2007\nchallenge in a multi-GPU setting in comparison to a default SVM setting. We\ndemonstrate that FV can be embedded into neural networks at arbitrary\npositions, allowing end-to-end training with back-propagation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 18:19:05 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Groh", "Fabian", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1702.02680", "submitter": "Rongjie Lai", "authors": "Rongjie Lai, Jia Li", "title": "Manifold Based Low-rank Regularization for Image Restoration and\n  Semi-supervised Learning", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank structures play important role in recent advances of many problems\nin image science and data science. As a natural extension of low-rank\nstructures for data with nonlinear structures, the concept of the\nlow-dimensional manifold structure has been considered in many data processing\nproblems. Inspired by this concept, we consider a manifold based low-rank\nregularization as a linear approximation of manifold dimension. This\nregularization is less restricted than the global low-rank regularization, and\nthus enjoy more flexibility to handle data with nonlinear structures. As\napplications, we demonstrate the proposed regularization to classical inverse\nproblems in image sciences and data sciences including image inpainting, image\nsuper-resolution, X-ray computer tomography (CT) image reconstruction and\nsemi-supervised learning. We conduct intensive numerical experiments in several\nimage restoration problems and a semi-supervised learning problem of\nclassifying handwritten digits using the MINST data. Our numerical tests\ndemonstrate the effectiveness of the proposed methods and illustrate that the\nnew regularization methods produce outstanding results by comparing with many\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 02:19:24 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Lai", "Rongjie", ""], ["Li", "Jia", ""]]}, {"id": "1702.02706", "submitter": "Yevhen Kuznietsov", "authors": "Yevhen Kuznietsov, J\\\"org St\\\"uckler, Bastian Leibe", "title": "Semi-Supervised Deep Learning for Monocular Depth Map Prediction", "comments": "CVPR 2017 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning often suffers from the lack of sufficient training\ndata. Specifically in the context of monocular depth map prediction, it is\nbarely possible to determine dense ground truth depth images in realistic\ndynamic outdoor environments. When using LiDAR sensors, for instance, noise is\npresent in the distance measurements, the calibration between sensors cannot be\nperfect, and the measurements are typically much sparser than the camera\nimages. In this paper, we propose a novel approach to depth map prediction from\nmonocular images that learns in a semi-supervised way. While we use sparse\nground-truth depth for supervised learning, we also enforce our deep network to\nproduce photoconsistent dense depth maps in a stereo setup using a direct image\nalignment loss. In experiments we demonstrate superior performance in depth map\nprediction from single images compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 05:08:22 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 16:57:12 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 21:58:52 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Kuznietsov", "Yevhen", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Leibe", "Bastian", ""]]}, {"id": "1702.02709", "submitter": "Nikolaos Sarafianos", "authors": "Nikolaos Sarafianos, Christophoros Nikou, Ioannis A. Kakadiaris", "title": "Predicting Privileged Information for Height Estimation", "comments": "Published in ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel regression-based method for employing\nprivileged information to estimate the height using human metrology. The actual\nvalues of the anthropometric measurements are difficult to estimate accurately\nusing state-of-the-art computer vision algorithms. Hence, we use ratios of\nanthropometric measurements as features. Since many anthropometric measurements\nare not available at test time in real-life scenarios, we employ a learning\nusing privileged information (LUPI) framework in a regression setup. Instead of\nusing the LUPI paradigm for regression in its original form (i.e.,\n\\epsilon-SVR+), we train regression models that predict the privileged\ninformation at test time. The predictions are then used, along with observable\nfeatures, to perform height estimation. Once the height is estimated, a mapping\nto classes is performed. We demonstrate that the proposed approach can estimate\nthe height better and faster than the \\epsilon-SVR+ algorithm and report\nresults for different genders and quartiles of humans.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 05:30:26 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Sarafianos", "Nikolaos", ""], ["Nikou", "Christophoros", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1702.02719", "submitter": "Zongping Deng", "authors": "Zongping Deng, Ke Li, Qijun Zhao, Yi Zhang and Hu Chen", "title": "Effective face landmark localization via single deep network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel face alignment method using single deep\nnetwork (SDN) on existing limited training data. Rather than using a\nmax-pooling layer followed one convolutional layer in typical convolutional\nneural networks (CNN), SDN adopts a stack of 3 layer groups instead. Each group\nlayer contains two convolutional layers and a max-pooling layer, which can\nextract the features hierarchically. Moreover, an effective data augmentation\nstrategy and corresponding training skills are also proposed to over-come the\nlack of training images on COFW and 300-W da-tasets. The experiment results\nshow that our method outper-forms state-of-the-art methods in both detection\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 06:25:54 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Deng", "Zongping", ""], ["Li", "Ke", ""], ["Zhao", "Qijun", ""], ["Zhang", "Yi", ""], ["Chen", "Hu", ""]]}, {"id": "1702.02738", "submitter": "Jean-Baptiste Alayrac", "authors": "Jean-Baptiste Alayrac, Josev Sivic, Ivan Laptev, Simon Lacoste-Julien", "title": "Joint Discovery of Object States and Manipulation Actions", "comments": "Appears in: International Conference on Computer Vision 2017 (ICCV\n  2017). 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many human activities involve object manipulations aiming to modify the\nobject state. Examples of common state changes include full/empty bottle,\nopen/closed door, and attached/detached car wheel. In this work, we seek to\nautomatically discover the states of objects and the associated manipulation\nactions. Given a set of videos for a particular task, we propose a joint model\nthat learns to identify object states and to localize state-modifying actions.\nOur model is formulated as a discriminative clustering cost with constraints.\nWe assume a consistent temporal order for the changes in object states and\nmanipulation actions, and introduce new optimization techniques to learn model\nparameters without additional supervision. We demonstrate successful discovery\nof seven manipulation actions and corresponding object states on a new dataset\nof videos depicting real-life object manipulations. We show that our joint\nformulation results in an improvement of object state discovery by action\nrecognition and vice versa.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:04:33 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 08:23:00 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 08:04:18 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Alayrac", "Jean-Baptiste", ""], ["Sivic", "Josev", ""], ["Laptev", "Ivan", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1702.02741", "submitter": "Bukweon Kim", "authors": "Jaeseong Jang, Yejin Park, Bukweon Kim, Sung Min Lee, Ja-Young Kwon,\n  and Jin Keun Seo", "title": "Automatic Estimation of Fetal Abdominal Circumference from Ultrasound\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound diagnosis is routinely used in obstetrics and gynecology for fetal\nbiometry, and owing to its time-consuming process, there has been a great\ndemand for automatic estimation. However, the automated analysis of ultrasound\nimages is complicated because they are patient-specific, operator-dependent,\nand machine-specific. Among various types of fetal biometry, the accurate\nestimation of abdominal circumference (AC) is especially difficult to perform\nautomatically because the abdomen has low contrast against surroundings,\nnon-uniform contrast, and irregular shape compared to other parameters.We\npropose a method for the automatic estimation of the fetal AC from 2D\nultrasound data through a specially designed convolutional neural network\n(CNN), which takes account of doctors' decision process, anatomical structure,\nand the characteristics of the ultrasound image. The proposed method uses CNN\nto classify ultrasound images (stomach bubble, amniotic fluid, and umbilical\nvein) and Hough transformation for measuring AC. We test the proposed method\nusing clinical ultrasound data acquired from 56 pregnant women. Experimental\nresults show that, with relatively small training samples, the proposed CNN\nprovides sufficient classification results for AC estimation through the Hough\ntransformation. The proposed method automatically estimates AC from ultrasound\nimages. The method is quantitatively evaluated, and shows stable performance in\nmost cases and even for ultrasound images deteriorated by shadowing artifacts.\nAs a result of experiments for our acceptance check, the accuracies are 0.809\nand 0.771 with the expert 1 and expert 2, respectively, while the accuracy\nbetween the two experts is 0.905. However, for cases of oversized fetus, when\nthe amniotic fluid is not observed or the abdominal area is distorted, it could\nnot correctly estimate AC.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:18:32 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 05:59:49 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Jang", "Jaeseong", ""], ["Park", "Yejin", ""], ["Kim", "Bukweon", ""], ["Lee", "Sung Min", ""], ["Kwon", "Ja-Young", ""], ["Seo", "Jin Keun", ""]]}, {"id": "1702.02743", "submitter": "Juan Felipe Perez-Juste Abascal", "authors": "Juan F P J Abascal (CREATIS), Manuel Desco, Juan Parra-Robles", "title": "Incorporation of prior knowledge of the signal behavior into the\n  reconstruction to accelerate the acquisition of MR diffusion data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion MRI measurements using hyperpolarized gases are generally acquired\nduring patient breath hold, which yields a compromise between achievable image\nresolution, lung coverage and number of b-values. In this work, we propose a\nnovel method that accelerates the acquisition of MR diffusion data by\nundersampling in both spatial and b-value dimensions, thanks to incorporating\nknowledge about the signal decay into the reconstruction (SIDER). SIDER is\ncompared to total variation (TV) reconstruction by assessing their effect on\nboth the recovery of ventilation images and estimated mean alveolar dimensions\n(MAD). Both methods are assessed by retrospectively undersampling diffusion\ndatasets of normal volunteers and COPD patients (n=8) for acceleration factors\nbetween x2 and x10. TV led to large errors and artefacts for acceleration\nfactors equal or larger than x5. SIDER improved TV, presenting lower errors and\nhistograms of MAD closer to those obtained from fully sampled data for\naccelerations factors up to x10. SIDER preserved image quality at all\nacceleration factors but images were slightly smoothed and some details were\nlost at x10. In conclusion, we have developed and validated a novel compressed\nsensing method for lung MRI imaging and achieved high acceleration factors,\nwhich can be used to increase the amount of data acquired during a breath-hold.\nThis methodology is expected to improve the accuracy of estimated lung\nmicrostructure dimensions and widen the possibilities of studying lung diseases\nwith MRI.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:26:53 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Abascal", "Juan F P J", "", "CREATIS"], ["Desco", "Manuel", ""], ["Parra-Robles", "Juan", ""]]}, {"id": "1702.02744", "submitter": "Jubin Johnson", "authors": "Jubin Johnson, Hisham Cholakkal and Deepu Rajan", "title": "L1-regularized Reconstruction Error as Alpha Matte", "comments": "5 pages, 5 figure, Accepted in IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2017.2666180", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling-based alpha matting methods have traditionally followed the\ncompositing equation to estimate the alpha value at a pixel from a pair of\nforeground (F) and background (B) samples. The (F,B) pair that produces the\nleast reconstruction error is selected, followed by alpha estimation. The\nsignificance of that residual error has been left unexamined. In this letter,\nwe propose a video matting algorithm that uses L1-regularized reconstruction\nerror of F and B samples as a measure of the alpha matte. A multi-frame\nnon-local means framework using coherency sensitive hashing is utilized to\nensure temporal coherency in the video mattes. Qualitative and quantitative\nevaluations on a dataset exclusively for video matting demonstrate the\neffectiveness of the proposed matting algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:29:58 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Johnson", "Jubin", ""], ["Cholakkal", "Hisham", ""], ["Rajan", "Deepu", ""]]}, {"id": "1702.02779", "submitter": "Tommaso Cavallari", "authors": "Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin,\n  Luigi Di Stefano, Philip H. S. Torr", "title": "On-the-Fly Adaptation of Regression Forests for Online Camera\n  Relocalisation", "comments": "To appear in the proceedings of CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera relocalisation is an important problem in computer vision, with\napplications in simultaneous localisation and mapping, virtual/augmented\nreality and navigation. Common techniques either match the current image\nagainst keyframes with known poses coming from a tracker, or establish 2D-to-3D\ncorrespondences between keypoints in the current image and points in the scene\nin order to estimate the camera pose. Recently, regression forests have become\na popular alternative to establish such correspondences. They achieve accurate\nresults, but must be trained offline on the target scene, preventing\nrelocalisation in new environments. In this paper, we show how to circumvent\nthis limitation by adapting a pre-trained forest to a new scene on the fly. Our\nadapted forests achieve relocalisation performance that is on par with that of\noffline forests, and our approach runs in under 150ms, making it desirable for\nreal-time systems that require online relocalisation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 10:39:05 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 10:11:30 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Cavallari", "Tommaso", ""], ["Golodetz", "Stuart", ""], ["Lord", "Nicholas A.", ""], ["Valentin", "Julien", ""], ["Di Stefano", "Luigi", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1702.02805", "submitter": "Qi Guo", "authors": "Qi Guo, Ce Zhu, Zhiqiang Xia, Zhengtao Wang, Yipeng Liu", "title": "Attribute-controlled face photo synthesis from simple line drawing", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face photo synthesis from simple line drawing is a one-to-many task as simple\nline drawing merely contains the contour of human face. Previous exemplar-based\nmethods are over-dependent on the datasets and are hard to generalize to\ncomplicated natural scenes. Recently, several works utilize deep neural\nnetworks to increase the generalization, but they are still limited in the\ncontrollability of the users. In this paper, we propose a deep generative model\nto synthesize face photo from simple line drawing controlled by face attributes\nsuch as hair color and complexion. In order to maximize the controllability of\nface attributes, an attribute-disentangled variational auto-encoder (AD-VAE) is\nfirstly introduced to learn latent representations disentangled with respect to\nspecified attributes. Then we conduct photo synthesis from simple line drawing\nbased on AD-VAE. Experiments show that our model can well disentangle the\nvariations of attributes from other variations of face photos and synthesize\ndetailed photorealistic face images with desired attributes. Regarding\nbackground and illumination as the style and human face as the content, we can\nalso synthesize face photos with the target style of a style photo.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 12:21:36 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Guo", "Qi", ""], ["Zhu", "Ce", ""], ["Xia", "Zhiqiang", ""], ["Wang", "Zhengtao", ""], ["Liu", "Yipeng", ""]]}, {"id": "1702.02925", "submitter": "Wei Li", "authors": "Wei Li, Farnaz Abtahi, Zhigang Zhu, Lijun Yin", "title": "EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial\n  Action Unit Detection", "comments": "The paper is accepted by FG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a deep learning based approach for facial action\nunit detection by enhancing and cropping the regions of interest. The approach\nis implemented by adding two novel nets (layers): the enhancing layers and the\ncropping layers, to a pretrained CNN model. For the enhancing layers, we\ndesigned an attention map based on facial landmark features and applied it to a\npretrained neural network to conduct enhanced learning (The E-Net). For the\ncropping layers, we crop facial regions around the detected landmarks and\ndesign convolutional layers to learn deeper features for each facial region\n(C-Net). We then fuse the E-Net and the C-Net to obtain our Enhancing and\nCropping (EAC) Net, which can learn both feature enhancing and region cropping\nfunctions. Our approach shows significant improvement in performance compared\nto the state-of-the-art methods applied to BP4D and DISFA AU datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 18:16:44 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Li", "Wei", ""], ["Abtahi", "Farnaz", ""], ["Zhu", "Zhigang", ""], ["Yin", "Lijun", ""]]}, {"id": "1702.03000", "submitter": "Joseph Camilo", "authors": "Joseph A. Camilo, Leslie M. Collins, Jordan M. Malof", "title": "A large comparison of feature-based approaches for buried target\n  classification in forward-looking ground-penetrating radar", "comments": "11 pages, 14 figures, for submission to IEEE TGARS", "journal-ref": null, "doi": "10.1109/TGRS.2017.2751461", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward-looking ground-penetrating radar (FLGPR) has recently been\ninvestigated as a remote sensing modality for buried target detection (e.g.,\nlandmines). In this context, raw FLGPR data is beamformed into images and then\ncomputerized algorithms are applied to automatically detect subsurface buried\ntargets. Most existing algorithms are supervised, meaning they are trained to\ndiscriminate between labeled target and non-target imagery, usually based on\nfeatures extracted from the imagery. A large number of features have been\nproposed for this purpose, however thus far it is unclear which are the most\neffective. The first goal of this work is to provide a comprehensive comparison\nof detection performance using existing features on a large collection of FLGPR\ndata. Fusion of the decisions resulting from processing each feature is also\nconsidered. The second goal of this work is to investigate two modern feature\nlearning approaches from the object recognition literature: the\nbag-of-visual-words and the Fisher vector for FLGPR processing. The results\nindicate that the new feature learning approaches outperform existing methods.\nResults also show that fusion between existing features and new features yields\nlittle additional performance improvements.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 22:06:04 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Camilo", "Joseph A.", ""], ["Collins", "Leslie M.", ""], ["Malof", "Jordan M.", ""]]}, {"id": "1702.03023", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W.\n  Jacobs, Amit Singer and Ronen Basri", "title": "A New Rank Constraint on Multi-view Fundamental Matrices, and its\n  Application to Camera Location Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of camera matrices is an important step in structure from\nmotion algorithms. In this paper we introduce a novel rank constraint on\ncollections of fundamental matrices in multi-view settings. We show that in\ngeneral, with the selection of proper scale factors, a matrix formed by\nstacking fundamental matrices between pairs of images has rank 6. Moreover,\nthis matrix forms the symmetric part of a rank 3 matrix whose factors relate\ndirectly to the corresponding camera matrices. We use this new characterization\nto produce better estimations of fundamental matrices by optimizing an L1-cost\nfunction using Iterative Re-weighted Least Squares and Alternate Direction\nMethod of Multiplier. We further show that this procedure can improve the\nrecovery of camera locations, particularly in multi-view settings in which\nfewer images are available.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 00:31:17 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Amir", "Tal", ""], ["Galun", "Meirav", ""], ["Goldstein", "Tom", ""], ["Jacobs", "David W.", ""], ["Singer", "Amit", ""], ["Basri", "Ronen", ""]]}, {"id": "1702.03041", "submitter": "Xi Peng", "authors": "Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris Metaxas, and Manmohan\n  Chandraker", "title": "Reconstruction-Based Disentanglement for Pose-invariant Face Recognition", "comments": "IEEE International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) trained on large-scale datasets have recently\nachieved impressive improvements in face recognition. But a persistent\nchallenge remains to develop methods capable of handling large pose variations\nthat are relatively underrepresented in training data. This paper presents a\nmethod for learning a feature representation that is invariant to pose, without\nrequiring extensive pose coverage in training data. We first propose to\ngenerate non-frontal views from a single frontal face, in order to increase the\ndiversity of training data while preserving accurate facial details that are\ncritical for identity discrimination. Our next contribution is to seek a rich\nembedding that encodes identity features, as well as non-identity ones such as\npose and landmark locations. Finally, we propose a new feature reconstruction\nmetric learning to explicitly disentangle identity and pose, by demanding\nalignment between the feature reconstructions through various combinations of\nidentity and pose features, which is obtained from two images of the same\nsubject. Experiments on both controlled and in-the-wild face datasets, such as\nMultiPIE, 300WLP and the profile view database CFP, show that our method\nconsistently outperforms the state-of-the-art, especially on images with large\nhead pose variations. Detail results and resource are referred to\nhttps://sites.google.com/site/xipengcshomepage/iccv2017\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 02:03:31 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 15:52:01 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Peng", "Xi", ""], ["Yu", "Xiang", ""], ["Sohn", "Kihyuk", ""], ["Metaxas", "Dimitris", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1702.03044", "submitter": "Anbang Yao", "authors": "Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen", "title": "Incremental Network Quantization: Towards Lossless CNNs with\n  Low-Precision Weights", "comments": "Published by ICLR 2017, and the code is available at\n  https://github.com/Zhouaojun/Incremental-Network-Quantization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents incremental network quantization (INQ), a novel method,\ntargeting to efficiently convert any pre-trained full-precision convolutional\nneural network (CNN) model into a low-precision version whose weights are\nconstrained to be either powers of two or zero. Unlike existing methods which\nare struggled in noticeable accuracy loss, our INQ has the potential to resolve\nthis issue, as benefiting from two innovations. On one hand, we introduce three\ninterdependent operations, namely weight partition, group-wise quantization and\nre-training. A well-proven measure is employed to divide the weights in each\nlayer of a pre-trained CNN model into two disjoint groups. The weights in the\nfirst group are responsible to form a low-precision base, thus they are\nquantized by a variable-length encoding method. The weights in the other group\nare responsible to compensate for the accuracy loss from the quantization, thus\nthey are the ones to be re-trained. On the other hand, these three operations\nare repeated on the latest re-trained group in an iterative manner until all\nthe weights are converted into low-precision ones, acting as an incremental\nnetwork quantization and accuracy enhancement procedure. Extensive experiments\non the ImageNet classification task using almost all known deep CNN\narchitectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the\nefficacy of the proposed method. Specifically, at 5-bit quantization, our\nmodels have improved accuracy than the 32-bit floating-point references. Taking\nResNet-18 as an example, we further show that our quantized models with 4-bit,\n3-bit and 2-bit ternary weights have improved or very similar accuracy against\nits 32-bit floating-point baseline. Besides, impressive results with the\ncombination of network pruning and INQ are also reported. The code is available\nat https://github.com/Zhouaojun/Incremental-Network-Quantization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 02:30:22 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 13:21:18 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Zhou", "Aojun", ""], ["Yao", "Anbang", ""], ["Guo", "Yiwen", ""], ["Xu", "Lin", ""], ["Chen", "Yurong", ""]]}, {"id": "1702.03105", "submitter": "Weng-Tai Su", "authors": "Weng-Tai Su, Gene Cheung, Chia-Wen Lin", "title": "Graph Fourier Transform with Negative Edges for Depth Image Coding", "comments": "5 pages, submitted to submitted to IEEE International Conference on\n  Image Processing, Beijing, China, September, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advent in graph signal processing (GSP) has led to the development of\nnew graph-based transforms and wavelets for image / video coding, where the\nunderlying graph describes inter-pixel correlations. In this paper, we develop\na new transform called signed graph Fourier transform (SGFT), where the\nunderlying graph G contains negative edges that describe anti-correlations\nbetween pixel pairs. Specifically, we first construct a one-state Markov\nprocess that models both inter-pixel correlations and anti-correlations. We\nthen derive the corresponding precision matrix, and show that the loopy graph\nLaplacian matrix Q of a graph G with a negative edge and two self-loops at its\nend nodes is approximately equivalent. This proves that the eigenvectors of Q -\ncalled SGFT - approximates the optimal Karhunen-Lo`eve Transform (KLT). We show\nthe importance of the self-loops in G to ensure Q is positive semi-definite. We\nprove that the first eigenvector of Q is piecewise constant (PWC), and thus can\nwell approximate a piecewise smooth (PWS) signal like a depth image.\nExperimental results show that a block-based coding scheme based on SGFT\noutperforms a previous scheme using graph transforms with only positive edges\nfor several depth images.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 09:11:12 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 06:28:34 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Su", "Weng-Tai", ""], ["Cheung", "Gene", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "1702.03115", "submitter": "Gui-Song Xia", "authors": "Gui-Song Xia, Gang Liu, Xiang Bai, Liangpei Zhang", "title": "Texture Characterization by Using Shape Co-occurrence Patterns", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, Vol. 26, No.10, pp. 5005 -\n  5018, 2017", "doi": "10.1109/TIP.2017.2726182", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture characterization is a key problem in image understanding and pattern\nrecognition. In this paper, we present a flexible shape-based texture\nrepresentation using shape co-occurrence patterns. More precisely, texture\nimages are first represented by tree of shapes, each of which is associated\nwith several geometrical and radiometric attributes. Then four typical kinds of\nshape co-occurrence patterns based on the hierarchical relationship of the\nshapes in the tree are learned as codewords. Three different coding methods are\ninvestigated to learn the codewords, with which, any given texture image can be\nencoded into a descriptive vector. In contrast with existing works, the\nproposed method not only inherits the strong ability to depict geometrical\naspects of textures and the high robustness to variations of imaging conditions\nfrom the shape-based method, but also provides a flexible way to consider shape\nrelationships and to compute high-order statistics on the tree. To our\nknowledge, this is the first time to use co-occurrence patterns of explicit\nshapes as a tool for texture analysis. Experiments on various texture datasets\nand scene datasets demonstrate the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 09:37:33 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Xia", "Gui-Song", ""], ["Liu", "Gang", ""], ["Bai", "Xiang", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1702.03176", "submitter": "Luigi Tommaso Luppino", "authors": "Luigi Tommaso Luppino, Stian Normann Anfinsen, Gabriele Moser, Robert\n  Jenssen, Filippo Maria Bianchi, Sebastiano Serpico, Gregoire Mercier", "title": "A clustering approach to heterogeneous change detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in heterogeneous multitemporal satellite images is a\nchallenging and still not much studied topic in remote sensing and earth\nobservation. This paper focuses on comparison of image pairs covering the same\ngeographical area and acquired by two different sensors, one optical radiometer\nand one synthetic aperture radar, at two different times. We propose a\nclustering-based technique to detect changes, identified as clusters that split\nor merge in the different images. To evaluate potentials and limitations of our\nmethod, we perform experiments on real data. Preliminary results confirm the\nrelationship between splits and merges of clusters and the occurrence of\nchanges. However, it becomes evident that it is necessary to incorporate prior,\nancillary, or application-specific information to improve the interpretation of\nclustering results and to identify unambiguously the areas of change.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 14:12:14 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Luppino", "Luigi Tommaso", ""], ["Anfinsen", "Stian Normann", ""], ["Moser", "Gabriele", ""], ["Jenssen", "Robert", ""], ["Bianchi", "Filippo Maria", ""], ["Serpico", "Sebastiano", ""], ["Mercier", "Gregoire", ""]]}, {"id": "1702.03267", "submitter": "Amarjot Singh", "authors": "Amarjot Singh and Nick Kingsbury", "title": "Dual-Tree Wavelet Scattering Network with Parametric Log Transformation\n  for Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a ScatterNet that uses a parametric log transformation with\nDual-Tree complex wavelets to extract translation invariant representations\nfrom a multi-resolution image. The parametric transformation aids the OLS\npruning algorithm by converting the skewed distributions into relatively\nmean-symmetric distributions while the Dual-Tree wavelets improve the\ncomputational efficiency of the network. The proposed network is shown to\noutperform Mallat's ScatterNet on two image datasets, both for classification\naccuracy and computational efficiency. The advantages of the proposed network\nover other supervised and some unsupervised methods are also presented using\nexperiments performed on different training dataset sizes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 18:02:05 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Singh", "Amarjot", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1702.03345", "submitter": "Amarjot Singh", "authors": "Amarjot Singh and Nick Kingsbury", "title": "Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Deep Scattering network that utilizes Dual-Tree\ncomplex wavelets to extract translation invariant representations from an input\nsignal. The computationally efficient Dual-Tree wavelets decompose the input\nsignal into densely spaced representations over scales. Translation invariance\nis introduced in the representations by applying a non-linearity over a region\nfollowed by averaging. The discriminatory information in the densely spaced,\nlocally smooth, signal representations aids the learning of the classifier. The\nproposed network is shown to outperform Mallat's ScatterNet on four datasets\nwith different modalities on classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 22:52:13 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Singh", "Amarjot", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1702.03349", "submitter": "Pavel Kral", "authors": "Pavel Kr\\'al, Ladislav Lenc, Anton\\'in Vrba", "title": "Enhanced Local Binary Patterns for Automatic Face Recognition", "comments": "Submitted for VCIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel automatic face recognition approach based on\nlocal binary patterns. This descriptor considers a local neighbourhood of a\npixel to compute the feature vector values. This method is not very robust to\nhandle image noise, variances and different illumination conditions. We address\nthese issues by proposing a novel descriptor which considers more pixels and\ndifferent neighbourhoods to compute the feature vector values. The proposed\nmethod is evaluated on two benchmark corpora, namely UFI and FERET face\ndatasets. We experimentally show that our approach outperforms state-of-the-art\nmethods and is efficient particularly in the real conditions where the above\nmentioned issues are obvious. We further show that the proposed method handles\nwell one training sample issue and is also robust to the image resolution.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 23:10:14 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 21:56:13 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Kr\u00e1l", "Pavel", ""], ["Lenc", "Ladislav", ""], ["Vrba", "Anton\u00edn", ""]]}, {"id": "1702.03407", "submitter": "Ben Glocker", "authors": "Vanya V. Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos\n  Kamnitsas, Eric O. Aboagye, Andrea G. Rockall, Daniel Rueckert, Ben Glocker", "title": "Reverse Classification Accuracy: Predicting Segmentation Performance in\n  the Absence of Ground Truth", "comments": "Accepted article to appear in IEEE Transactions on Medical Imaging\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When integrating computational tools such as automatic segmentation into\nclinical practice, it is of utmost importance to be able to assess the level of\naccuracy on new data, and in particular, to detect when an automatic method\nfails. However, this is difficult to achieve due to absence of ground truth.\nSegmentation accuracy on clinical data might be different from what is found\nthrough cross-validation because validation data is often used during\nincremental method development, which can lead to overfitting and unrealistic\nperformance expectations. Before deployment, performance is quantified using\ndifferent metrics, for which the predicted segmentation is compared to a\nreference segmentation, often obtained manually by an expert. But little is\nknown about the real performance after deployment when a reference is\nunavailable. In this paper, we introduce the concept of reverse classification\naccuracy (RCA) as a framework for predicting the performance of a segmentation\nmethod on new data. In RCA we take the predicted segmentation from a new image\nto train a reverse classifier which is evaluated on a set of reference images\nwith available ground truth. The hypothesis is that if the predicted\nsegmentation is of good quality, then the reverse classifier will perform well\non at least some of the reference images. We validate our approach on\nmulti-organ segmentation with different classifiers and segmentation methods.\nOur results indicate that it is indeed possible to predict the quality of\nindividual segmentations, in the absence of ground truth. Thus, RCA is ideal\nfor integration into automatic processing pipelines in clinical routine and as\npart of large-scale image analysis studies.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 10:16:34 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Valindria", "Vanya V.", ""], ["Lavdas", "Ioannis", ""], ["Bai", "Wenjia", ""], ["Kamnitsas", "Konstantinos", ""], ["Aboagye", "Eric O.", ""], ["Rockall", "Andrea G.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1702.03410", "submitter": "Chee Seng Chan", "authors": "Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, Kiyoshi Tanaka", "title": "ArtGAN: Artwork Synthesis with Conditional Categorical GANs", "comments": "10 pages, 10 figures, submitted to ICIP2017 (extension version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an extension to the Generative Adversarial Networks\n(GANs), namely as ARTGAN to synthetically generate more challenging and complex\nimages such as artwork that have abstract characteristics. This is in contrast\nto most of the current solutions that focused on generating natural images such\nas room interiors, birds, flowers and faces. The key innovation of our work is\nto allow back-propagation of the loss function w.r.t. the labels (randomly\nassigned to each generated images) to the generator from the discriminator.\nWith the feedback from the label information, the generator is able to learn\nfaster and achieve better generated image quality. Empirically, we show that\nthe proposed ARTGAN is capable to create realistic artwork, as well as generate\ncompelling real world images that globally look natural with clear shape on\nCIFAR-10.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 11:19:20 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 10:32:34 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Tan", "Wei Ren", ""], ["Chan", "Chee Seng", ""], ["Aguirre", "Hernan", ""], ["Tanaka", "Kiyoshi", ""]]}, {"id": "1702.03431", "submitter": "Chengde Wan Mr", "authors": "Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao", "title": "Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for\n  Hand Pose Estimation", "comments": "10 pages, 5 figures, accepted in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for 3D hand pose estimation from depth images\nrequire large amounts of annotated training data. We propose to model the\nstatistical relationships of 3D hand poses and corresponding depth images using\ntwo deep generative models with a shared latent space. By design, our\narchitecture allows for learning from unlabeled image data in a semi-supervised\nmanner. Assuming a one-to-one mapping between a pose and a depth map, any given\npoint in the shared latent space can be projected into both a hand pose and a\ncorresponding depth map. Regressing the hand pose can then be done by learning\na discriminator to estimate the posterior of the latent pose given some depth\nmaps. To improve generalization and to better exploit unlabeled depth maps, we\njointly train a generator and a discriminator. At each iteration, the generator\nis updated with the back-propagated gradient from the discriminator to\nsynthesize realistic depth maps of the articulated hand, while the\ndiscriminator benefits from an augmented training set of synthesized and\nunlabeled samples. The proposed discriminator network architecture is highly\nefficient and runs at 90 FPS on the CPU with accuracies comparable or better\nthan state-of-art on 3 publicly available benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 15:38:12 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 18:30:33 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Wan", "Chengde", ""], ["Probst", "Thomas", ""], ["Van Gool", "Luc", ""], ["Yao", "Angela", ""]]}, {"id": "1702.03435", "submitter": "Siddharth Choudhary", "authors": "Siddharth Choudhary, Luca Carlone, Carlos Nieto, John Rogers, Henrik\n  I. Christensen, Frank Dellaert", "title": "Distributed Mapping with Privacy and Communication Constraints:\n  Lightweight Algorithms and Object-based Models", "comments": "preprint for IJRR submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: a team of robots is deployed in an unknown\nenvironment and it has to collaboratively build a map of the area without a\nreliable infrastructure for communication. The backbone for modern mapping\ntechniques is pose graph optimization, which estimates the trajectory of the\nrobots, from which the map can be easily built. The first contribution of this\npaper is a set of distributed algorithms for pose graph optimization: rather\nthan sending all sensor data to a remote sensor fusion server, the robots\nexchange very partial and noisy information to reach an agreement on the pose\ngraph configuration. Our approach can be considered as a distributed\nimplementation of the two-stage approach of Carlone et al., where we use the\nSuccessive Over-Relaxation (SOR) and the Jacobi Over-Relaxation (JOR) as\nworkhorses to split the computation among the robots. As a second contribution,\nwe extend %and demonstrate the applicability of the proposed distributed\nalgorithms to work with object-based map models. The use of object-based models\navoids the exchange of raw sensor measurements (e.g., point clouds) further\nreducing the communication burden. Our third contribution is an extensive\nexperimental evaluation of the proposed techniques, including tests in\nrealistic Gazebo simulations and field experiments in a military test facility.\nAbundant experimental evidence suggests that one of the proposed algorithms\n(the Distributed Gauss-Seidel method or DGS) has excellent performance. The DGS\nrequires minimal information exchange, has an anytime flavor, scales well to\nlarge teams, is robust to noise, and is easy to implement. Our field tests show\nthat the combined use of our distributed algorithms and object-based models\nreduces the communication requirements by several orders of magnitude and\nenables distributed mapping with large teams of robots in real-world problems.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 16:08:22 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Choudhary", "Siddharth", ""], ["Carlone", "Luca", ""], ["Nieto", "Carlos", ""], ["Rogers", "John", ""], ["Christensen", "Henrik I.", ""], ["Dellaert", "Frank", ""]]}, {"id": "1702.03505", "submitter": "Takashi Matsubara", "authors": "Ryo Takahashi, Takashi Matsubara, and Kuniaki Uehara", "title": "A Novel Weight-Shared Multi-Stage CNN for Scale Robustness", "comments": "accepted version, 13 pages", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  vol. 29, no. 4, 2019, pp. 1090-1101", "doi": "10.1109/TCSVT.2018.2822773", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated remarkable results in\nimage classification for benchmark tasks and practical applications. The CNNs\nwith deeper architectures have achieved even higher performance recently thanks\nto their robustness to the parallel shift of objects in images as well as their\nnumerous parameters and the resulting high expression ability. However, CNNs\nhave a limited robustness to other geometric transformations such as scaling\nand rotation. This limits the performance improvement of the deep CNNs, but\nthere is no established solution. This study focuses on scale transformation\nand proposes a network architecture called the weight-shared multi-stage\nnetwork (WSMS-Net), which consists of multiple stages of CNNs. The proposed\nWSMS-Net is easily combined with existing deep CNNs such as ResNet and DenseNet\nand enables them to acquire robustness to object scaling. Experimental results\non the CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate that existing\ndeep CNNs combined with the proposed WSMS-Net achieve higher accuracies for\nimage classification tasks with only a minor increase in the number of\nparameters and computation time.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 08:29:53 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 02:31:51 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 02:44:54 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Takahashi", "Ryo", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "1702.03515", "submitter": "Jungong Han", "authors": "Qiang Zhang, Yi Liu, Rick S. Blum, Jungong Han, and Dacheng Tao", "title": "Sparse Representation based Multi-sensor Image Fusion: A Review", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of several successful applications in computer vision and image\nprocessing, sparse representation (SR) has attracted significant attention in\nmulti-sensor image fusion. Unlike the traditional multiscale transforms (MSTs)\nthat presume the basis functions, SR learns an over-complete dictionary from a\nset of training images for image fusion, and it achieves more stable and\nmeaningful representations of the source images. By doing so, the SR-based\nfusion methods generally outperform the traditional MST-based image fusion\nmethods in both subjective and objective tests. In addition, they are less\nsusceptible to mis-registration among the source images, thus facilitating the\npractical applications. This survey paper proposes a systematic review of the\nSR-based multi-sensor image fusion literature, highlighting the pros and cons\nof each category of approaches. Specifically, we start by performing a\ntheoretical investigation of the entire system from three key algorithmic\naspects, (1) sparse representation models; (2) dictionary learning methods; and\n(3) activity levels and fusion rules. Subsequently, we show how the existing\nworks address these scientific problems and design the appropriate fusion rules\nfor each application, such as multi-focus image fusion and multi-modality\n(e.g., infrared and visible) image fusion. At last, we carry out some\nexperiments to evaluate the impact of these three algorithmic components on the\nfusion performance when dealing with different applications. This article is\nexpected to serve as a tutorial and source of reference for researchers\npreparing to enter the field or who desire to employ the sparse representation\ntheory in other fields.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 11:43:09 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Zhang", "Qiang", ""], ["Liu", "Yi", ""], ["Blum", "Rick S.", ""], ["Han", "Jungong", ""], ["Tao", "Dacheng", ""]]}, {"id": "1702.03600", "submitter": "Huimin Lu", "authors": "Huimin Lu, Yujie Li, Yudong Zhang, Min Chen, Seiichi Serikawa,\n  Hyoungseop Kim", "title": "Underwater Optical Image Processing: A Comprehensive Review", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater cameras are widely used to observe the sea floor. They are usually\nincluded in autonomous underwater vehicles, unmanned underwater vehicles, and\nin situ ocean sensor networks. Despite being an important sensor for monitoring\nunderwater scenes, there exist many issues with recent underwater camera\nsensors. Because of lights transportation characteristics in water and the\nbiological activity at the sea floor, the acquired underwater images often\nsuffer from scatters and large amounts of noise. Over the last five years, many\nmethods have been proposed to overcome traditional underwater imaging problems.\nThis paper aims to review the state-of-the-art techniques in underwater image\nprocessing by highlighting the contributions and challenges presented in over\n40 papers. We present an overview of various underwater image processing\napproaches, such as underwater image descattering, underwater image color\nrestoration, and underwater image quality assessments. Finally, we summarize\nthe future trends and challenges in designing and processing underwater imaging\nsensors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 00:40:59 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Lu", "Huimin", ""], ["Li", "Yujie", ""], ["Zhang", "Yudong", ""], ["Chen", "Min", ""], ["Serikawa", "Seiichi", ""], ["Kim", "Hyoungseop", ""]]}, {"id": "1702.03684", "submitter": "Sebastian Bodenstedt", "authors": "Sebastian Bodenstedt (1), Martin Wagner (2), Darko Kati\\'c (1),\n  Patrick Mietkowski (2), Benjamin Mayer (2), Hannes Kenngott (2), Beat\n  M\\\"uller-Stich (2), R\\\"udiger Dillmann (1), Stefanie Speidel (1) ((1)\n  Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology,\n  Karlsruhe, (2) Department of General, Visceral and Transplant Surgery,\n  University of Heidelberg, Heidelberg)", "title": "Unsupervised temporal context learning using convolutional neural\n  networks for laparoscopic workflow analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-assisted surgery (CAS) aims to provide the surgeon with the right\ntype of assistance at the right moment. Such assistance systems are especially\nrelevant in laparoscopic surgery, where CAS can alleviate some of the drawbacks\nthat surgeons incur. For many assistance functions, e.g. displaying the\nlocation of a tumor at the appropriate time or suggesting what instruments to\nprepare next, analyzing the surgical workflow is a prerequisite. Since\nlaparoscopic interventions are performed via endoscope, the video signal is an\nobvious sensor modality to rely on for workflow analysis.\n  Image-based workflow analysis tasks in laparoscopy, such as phase\nrecognition, skill assessment, video indexing or automatic annotation, require\na temporal distinction between video frames. Generally computer vision based\nmethods that generalize from previously seen data are used. For training such\nmethods, large amounts of annotated data are necessary. Annotating surgical\ndata requires expert knowledge, therefore collecting a sufficient amount of\ndata is difficult, time-consuming and not always feasible.\n  In this paper, we address this problem by presenting an unsupervised method\nfor training a convolutional neural network (CNN) to differentiate between\nlaparoscopic video frames on a temporal basis. We extract video frames at\nregular intervals from 324 unlabeled laparoscopic interventions, resulting in a\ndataset of approximately 2.2 million images. From this dataset, we extract\nimage pairs from the same video and train a CNN to determine their temporal\norder. To solve this problem, the CNN has to extract features that are relevant\nfor comprehending laparoscopic workflow.\n  Furthermore, we demonstrate that such a CNN can be adapted for surgical\nworkflow segmentation. We performed image-based workflow segmentation on a\npublicly available dataset of 7 cholecystectomies and 9 colorectal\ninterventions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 09:29:50 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Bodenstedt", "Sebastian", ""], ["Wagner", "Martin", ""], ["Kati\u0107", "Darko", ""], ["Mietkowski", "Patrick", ""], ["Mayer", "Benjamin", ""], ["Kenngott", "Hannes", ""], ["M\u00fcller-Stich", "Beat", ""], ["Dillmann", "R\u00fcdiger", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1702.03690", "submitter": "Jiaqian Yu", "authors": "Jiaqian Yu and Matthew B. Blaschko", "title": "An Efficient Decomposition Framework for Discriminative Segmentation\n  with Supermodular Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several supermodular losses have been shown to improve the perceptual quality\nof image segmentation in a discriminative framework such as a structured output\nsupport vector machine (SVM). These loss functions do not necessarily have the\nsame structure as the one used by the segmentation inference algorithm, and in\ngeneral, we may have to resort to generic submodular minimization algorithms\nfor loss augmented inference. Although these come with polynomial time\nguarantees, they are not practical to apply to image scale data. Many\nsupermodular losses come with strong optimization guarantees, but are not\nreadily incorporated in a loss augmented graph cuts procedure. This motivates\nour strategy of employing the alternating direction method of multipliers\n(ADMM) decomposition for loss augmented inference. In doing so, we create a new\nAPI for the structured SVM that separates the maximum a posteriori (MAP)\ninference of the model from the loss augmentation during training. In this way,\nwe gain computational efficiency, making new choices of loss functions\npractical for the first time, while simultaneously making the inference\nalgorithm employed during training closer to the test time procedure. We show\nimprovement both in accuracy and computational performance on the Microsoft\nResearch Grabcut database and a brain structure segmentation task, empirically\nvalidating the use of several supermodular loss functions during training, and\nthe improved computational properties of the proposed ADMM approach over the\nFujishige-Wolfe minimum norm point algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 09:49:35 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Yu", "Jiaqian", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1702.03824", "submitter": "Xinyu Li", "authors": "Xinyu Li, Yanyi Zhang, Ivan Marsic, and Randall S. Burd", "title": "Online People Tracking and Identification with RFID and Kinect", "comments": "8 Pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel, accurate and practical system for real-time people\ntracking and identification. We used a Kinect V2 sensor for tracking that\ngenerates a body skeleton for up to six people in the view. We perform\nidentification using both Kinect and passive RFID, by first measuring the\nvelocity vector of person's skeleton and of their RFID tag using the position\nof the RFID reader antennas as reference points and then finding the best match\nbetween skeletons and tags. We introduce a method for synchronizing Kinect\ndata, which is captured regularly, with irregular or missing RFID data\nreadouts. Our experiments show centimeter-level people tracking resolution with\n80% average identification accuracy for up to six people in indoor\nenvironments, which meets the needs of many applications. Our system can\npreserve user privacy and work with different lighting.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 05:05:20 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Li", "Xinyu", ""], ["Zhang", "Yanyi", ""], ["Marsic", "Ivan", ""], ["Burd", "Randall S.", ""]]}, {"id": "1702.03833", "submitter": "Fangzhou Liao", "authors": "Fangzhou Liao, Xi Chen, Xiaolin Hu, Sen Song", "title": "Estimation of the volume of the left ventricle from MRI images using\n  deep neural networks", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TCYB.2017.2778799", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI)\nimages and calculating its volume are important for diagnosing cardiac\ndiseases. In 2016, Kaggle organized a competition to estimate the volume of LV\nfrom MRI images. The dataset consisted of a large number of cases, but only\nprovided systole and diastole volumes as labels. We designed a system based on\nneural networks to solve this problem. It began with a detector combined with a\nneural network classifier for detecting regions of interest (ROIs) containing\nLV chambers. Then a deep neural network named hypercolumns fully convolutional\nnetwork was used to segment LV in ROIs. The 2D segmentation results were\nintegrated across different images to estimate the volume. With ground-truth\nvolume labels, this model was trained end-to-end. To improve the result, an\nadditional dataset with only segmentation label was used. The model was trained\nalternately on these two datasets with different types of teaching signals. We\nalso proposed a variance estimation method for the final prediction. Our\nalgorithm ranked the 4th on the test set in this competition.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 15:43:52 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Liao", "Fangzhou", ""], ["Chen", "Xi", ""], ["Hu", "Xiaolin", ""], ["Song", "Sen", ""]]}, {"id": "1702.03920", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul\n  Sukthankar, Jitendra Malik", "title": "Cognitive Mapping and Planning for Visual Navigation", "comments": "Extended IJCV Version of the original paper at CVPR17. Project\n  website with code, models, simulation environment and videos:\n  https://sites.google.com/view/cognitive-mapping-and-planning/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural architecture for navigation in novel environments. Our\nproposed architecture learns to map from first-person views and plans a\nsequence of actions towards goals in the environment. The Cognitive Mapper and\nPlanner (CMP) is based on two key ideas: a) a unified joint architecture for\nmapping and planning, such that the mapping is driven by the needs of the task,\nand b) a spatial memory with the ability to plan given an incomplete set of\nobservations about the world. CMP constructs a top-down belief map of the world\nand applies a differentiable neural net planner to produce the next action at\neach time step. The accumulated belief of the world enables the agent to track\nvisited regions of the environment. We train and test CMP on navigation\nproblems in simulation environments derived from scans of real world buildings.\nOur experiments demonstrate that CMP outperforms alternate learning-based\narchitectures, as well as, classical mapping and path planning approaches in\nmany cases. Furthermore, it naturally extends to semantically specified goals,\nsuch as 'going to a chair'. We also deploy CMP on physical robots in indoor\nenvironments, where it achieves reasonable performance, even though it is\ntrained entirely in simulation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 18:52:04 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 01:59:30 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 18:54:58 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Gupta", "Saurabh", ""], ["Tolani", "Varun", ""], ["Davidson", "James", ""], ["Levine", "Sergey", ""], ["Sukthankar", "Rahul", ""], ["Malik", "Jitendra", ""]]}, {"id": "1702.03970", "submitter": "Ray Smith", "authors": "Raymond Smith, Chunhui Gu, Dar-Shyang Lee, Huiyi Hu, Ranjith\n  Unnikrishnan, Julian Ibarz, Sacha Arnoud, Sophia Lin", "title": "End-to-End Interpretation of the French Street Name Signs Dataset", "comments": "Presented at the IWRR workshop at ECCV 2016", "journal-ref": "Computer Vision - ECCV 2016 Workshops Volume 9913 of the series\n  Lecture Notes in Computer Science pp 411-426", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the French Street Name Signs (FSNS) Dataset consisting of more\nthan a million images of street name signs cropped from Google Street View\nimages of France. Each image contains several views of the same street name\nsign. Every image has normalized, title case folded ground-truth text as it\nwould appear on a map. We believe that the FSNS dataset is large and complex\nenough to train a deep network of significant complexity to solve the street\nname extraction problem \"end-to-end\" or to explore the design trade-offs\nbetween a single complex engineered network and multiple sub-networks designed\nand trained to solve sub-problems. We present such an \"end-to-end\"\nnetwork/graph for Tensor Flow and its results on the FSNS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 20:18:18 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Smith", "Raymond", ""], ["Gu", "Chunhui", ""], ["Lee", "Dar-Shyang", ""], ["Hu", "Huiyi", ""], ["Unnikrishnan", "Ranjith", ""], ["Ibarz", "Julian", ""], ["Arnoud", "Sacha", ""], ["Lin", "Sophia", ""]]}, {"id": "1702.04037", "submitter": "Yang Wang", "authors": "Yang Wang, Vinh Tran, Minh Hoai", "title": "Evolution-Preserving Dense Trajectory Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Trajectory-pooled Deep-learning Descriptors were shown to achieve\nstate-of-the-art human action recognition results on a number of datasets. This\npaper improves their performance by applying rank pooling to each trajectory,\nencoding the temporal evolution of deep learning features computed along the\ntrajectory. This leads to Evolution-Preserving Trajectory (EPT) descriptors, a\nnovel type of video descriptor that significantly outperforms Trajectory-pooled\nDeep-learning Descriptors. EPT descriptors are defined based on dense\ntrajectories, and they provide complimentary benefits to video descriptors that\nare not based on trajectories. In particular, we show that the combination of\nEPT descriptors and VideoDarwin leads to state-of-the-art performance on\nHollywood2 and UCF101 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 00:54:52 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Wang", "Yang", ""], ["Tran", "Vinh", ""], ["Hoai", "Minh", ""]]}, {"id": "1702.04040", "submitter": "Shan Gao", "authors": "Shan Gao, Xiaogang Chen, Qixiang Ye, Junliang Xing, Arjan Kuijper,\n  Xiangyang Ji", "title": "A Graphical Social Topology Model for Multi-Object Tracking", "comments": "there is an input error in experiments, so we should change the\n  results in all results tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking multiple objects is a challenging task when objects move in groups\nand occlude each other. Existing methods have investigated the problems of\ngroup division and group energy-minimization; however, lacking overall\nobject-group topology modeling limits their ability in handling complex object\nand group dynamics. Inspired with the social affinity property of moving\nobjects, we propose a Graphical Social Topology (GST) model, which estimates\nthe group dynamics by jointly modeling the group structure and the states of\nobjects using a topological representation. With such topology representation,\nmoving objects are not only assigned to groups, but also dynamically connected\nwith each other, which enables in-group individuals to be correctly associated\nand the cohesion of each group to be precisely modeled. Using well-designed\ntopology learning modules and topology training, we infer the birth/death and\nmerging/splitting of dynamic groups. With the GST model, the proposed\nmulti-object tracker can naturally facilitate the occlusion problem by treating\nthe occluded object and other in-group members as a whole while leveraging\noverall state transition. Experiments on both RGB and RGB-D datasets confirm\nthat the proposed multi-object tracker improves the state-of-the-arts\nespecially in crowded scenes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 01:27:47 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 11:39:20 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Gao", "Shan", ""], ["Chen", "Xiaogang", ""], ["Ye", "Qixiang", ""], ["Xing", "Junliang", ""], ["Kuijper", "Arjan", ""], ["Ji", "Xiangyang", ""]]}, {"id": "1702.04069", "submitter": "Sungeun Hong", "authors": "Sungeun Hong, Woobin Im, Jongbin Ryu, Hyun S. Yang", "title": "SSPP-DAN: Deep Domain Adaptation Network for Face Recognition with\n  Single Sample Per Person", "comments": "Accepted to ICIP 2017 Oral, Code is available at\n  https://github.com/csehong/SSPP-DAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world face recognition using a single sample per person (SSPP) is a\nchallenging task. The problem is exacerbated if the conditions under which the\ngallery image and the probe set are captured are completely different. To\naddress these issues from the perspective of domain adaptation, we introduce an\nSSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain\nadaptation, feature extraction, and classification are performed jointly using\na deep architecture with domain-adversarial training. However, the SSPP\ncharacteristic of one training sample per class is insufficient to train the\ndeep architecture. To overcome this shortage, we generate synthetic images with\nvarying poses using a 3D face model. Experimental evaluations using a realistic\nSSPP dataset show that deep domain adaptation and image synthesis complement\neach other and dramatically improve accuracy. Experiments on a benchmark\ndataset using the proposed approach show state-of-the-art performance. All the\ndataset and the source code can be found in our online repository\n(https://github.com/csehong/SSPP-DAN).\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 04:02:07 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 06:22:46 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 12:41:07 GMT"}, {"version": "v4", "created": "Sat, 28 Apr 2018 12:48:28 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Hong", "Sungeun", ""], ["Im", "Woobin", ""], ["Ryu", "Jongbin", ""], ["Yang", "Hyun S.", ""]]}, {"id": "1702.04111", "submitter": "Markus Rempfler", "authors": "Markus Rempfler, Jan-Hendrik Lange, Florian Jug, Corinna Blasse,\n  Eugene W. Myers, Bjoern H. Menze, Bjoern Andres", "title": "Efficient Algorithms for Moral Lineage Tracing", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lineage tracing, the joint segmentation and tracking of living cells as they\nmove and divide in a sequence of light microscopy images, is a challenging\ntask. Jug et al. have proposed a mathematical abstraction of this task, the\nmoral lineage tracing problem (MLTP), whose feasible solutions define both a\nsegmentation of every image and a lineage forest of cells. Their branch-and-cut\nalgorithm, however, is prone to many cuts and slow convergence for large\ninstances. To address this problem, we make three contributions: (i) we devise\nthe first efficient primal feasible local search algorithms for the MLTP, (ii)\nwe improve the branch-and-cut algorithm by separating tighter cutting planes\nand by incorporating our primal algorithms, (iii) we show in experiments that\nour algorithms find accurate solutions on the problem instances of Jug et al.\nand scale to larger instances, leveraging moral lineage tracing to practical\nsignificance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 08:39:33 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 09:33:06 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Rempfler", "Markus", ""], ["Lange", "Jan-Hendrik", ""], ["Jug", "Florian", ""], ["Blasse", "Corinna", ""], ["Myers", "Eugene W.", ""], ["Menze", "Bjoern H.", ""], ["Andres", "Bjoern", ""]]}, {"id": "1702.04114", "submitter": "Yizhak Ben-Shabat", "authors": "Yizhak Ben-Shabat, Tamar Avraham, Michael Lindenbaum, Anath Fischer", "title": "Graph Based Over-Segmentation Methods for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2018.06.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-segmentation, or super-pixel generation, is a common preliminary stage\nfor many computer vision applications. New acquisition technologies enable the\ncapturing of 3D point clouds that contain color and geometrical information.\nThis 3D information introduces a new conceptual change that can be utilized to\nimprove the results of over-segmentation, which uses mainly color information,\nand to generate clusters of points we call super-points. We consider a variety\nof possible 3D extensions of the Local Variation (LV) graph based\nover-segmentation algorithms, and compare them thoroughly. We consider\ndifferent alternatives for constructing the connectivity graph, for assigning\nthe edge weights, and for defining the merge criterion, which must now account\nfor the geometric information and not only color. Following this evaluation, we\nderive a new generic algorithm for over-segmentation of 3D point clouds. We\ncall this new algorithm Point Cloud Local Variation (PCLV). The advantages of\nthe new over-segmentation algorithm are demonstrated on both outdoor and\ncluttered indoor scenes. Performance analysis of the proposed approach compared\nto state-of-the-art 2D and 3D over-segmentation algorithms shows significant\nimprovement according to the common performance measures.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 08:53:13 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Ben-Shabat", "Yizhak", ""], ["Avraham", "Tamar", ""], ["Lindenbaum", "Michael", ""], ["Fischer", "Anath", ""]]}, {"id": "1702.04125", "submitter": "Vedran Vukoti\\'c", "authors": "Vedran Vukoti\\'c, Silvia-Laura Pintea, Christian Raymond, Guillaume\n  Gravier, Jan Van Gemert", "title": "One-Step Time-Dependent Future Video Frame Prediction with a\n  Convolutional Encoder-Decoder Neural Network", "comments": "11 pages, 1 figures, published in the International Conference of\n  Image Analysis and Processing (ICIAP) 2017 and in the Netherlands Conference\n  on Computer Vision (NCCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an inherent need for autonomous cars, drones, and other robots to\nhave a notion of how their environment behaves and to anticipate changes in the\nnear future. In this work, we focus on anticipating future appearance given the\ncurrent frame of a video. Existing work focuses on either predicting the future\nappearance as the next frame of a video, or predicting future motion as optical\nflow or motion trajectories starting from a single video frame. This work\nstretches the ability of CNNs (Convolutional Neural Networks) to predict an\nanticipation of appearance at an arbitrarily given future time, not necessarily\nthe next video frame. We condition our predicted future appearance on a\ncontinuous time variable that allows us to anticipate future frames at a given\ntemporal distance, directly from the input video frame. We show that CNNs can\nlearn an intrinsic representation of typical appearance changes over time and\nsuccessfully generate realistic predictions at a deliberate time difference in\nthe near future.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 09:21:23 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 11:11:25 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Vukoti\u0107", "Vedran", ""], ["Pintea", "Silvia-Laura", ""], ["Raymond", "Christian", ""], ["Gravier", "Guillaume", ""], ["Van Gemert", "Jan", ""]]}, {"id": "1702.04174", "submitter": "Enrique S\\'anchez Lozano", "authors": "Michel F. Valstar, Enrique S\\'anchez-Lozano, Jeffrey F. Cohn,\n  L\\'aszl\\'o A. Jeni, Jeffrey M. Girard, Zheng Zhang, Lijun Yin, and Maja\n  Pantic", "title": "FERA 2017 - Addressing Head Pose in the Third Facial Expression\n  Recognition and Analysis Challenge", "comments": "FERA 2017 Baseline Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Automatic Facial Expression Analysis has grown rapidly in recent\nyears. However, despite progress in new approaches as well as benchmarking\nefforts, most evaluations still focus on either posed expressions, near-frontal\nrecordings, or both. This makes it hard to tell how existing expression\nrecognition approaches perform under conditions where faces appear in a wide\nrange of poses (or camera views), displaying ecologically valid expressions.\nThe main obstacle for assessing this is the availability of suitable data, and\nthe challenge proposed here addresses this limitation. The FG 2017 Facial\nExpression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to\nthe estimation of Action Units occurrence and intensity under different camera\nviews. In this paper we present the third challenge in automatic recognition of\nfacial expressions, to be held in conjunction with the 12th IEEE conference on\nFace and Gesture Recognition, May 2017, in Washington, United States. Two\nsub-challenges are defined: the detection of AU occurrence, and the estimation\nof AU intensity. In this work we outline the evaluation protocol, the data\nused, and the results of a baseline method for both sub-challenges.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 12:22:30 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Valstar", "Michel F.", ""], ["S\u00e1nchez-Lozano", "Enrique", ""], ["Cohn", "Jeffrey F.", ""], ["Jeni", "L\u00e1szl\u00f3 A.", ""], ["Girard", "Jeffrey M.", ""], ["Zhang", "Zheng", ""], ["Yin", "Lijun", ""], ["Pantic", "Maja", ""]]}, {"id": "1702.04179", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang", "title": "Structured Deep Hashing with Convolutional Neural Networks for Fast\n  Person Re-identification", "comments": "To appear at Computer Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2017.11.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pedestrian image as a query, the purpose of person re-identification\nis to identify the correct match from a large collection of gallery images\ndepicting the same person captured by disjoint camera views. The critical\nchallenge is how to construct a robust yet discriminative feature\nrepresentation to capture the compounded variations in pedestrian appearance.\nTo this end, deep learning methods have been proposed to extract hierarchical\nfeatures against extreme variability of appearance. However, existing methods\nin this category generally neglect the efficiency in the matching stage whereas\nthe searching speed of a re-identification system is crucial in real-world\napplications. In this paper, we present a novel deep hashing framework with\nConvolutional Neural Networks (CNNs) for fast person re-identification.\nTechnically, we simultaneously learn both CNN features and hash functions/codes\nto get robust yet discriminative features and similarity-preserving hash codes.\nThereby, person re-identification can be resolved by efficiently computing and\nranking the Hamming distances between images. A structured loss function\ndefined over positive pairs and hard negatives is proposed to formulate a novel\noptimization problem so that fast convergence and more stable optimized\nsolution can be obtained. Extensive experiments on two benchmarks CUHK03\n\\cite{FPNN} and Market-1501 \\cite{Market1501} show that the proposed deep\narchitecture is efficacy over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 12:35:53 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 01:36:53 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 02:41:08 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""]]}, {"id": "1702.04267", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff", "title": "On Detecting Adversarial Perturbations", "comments": "Final version for ICLR2017 (see\n  https://openreview.net/forum?id=SJzCSf9xg&noteId=SJzCSf9xg)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 15:44:26 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 06:53:38 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Genewein", "Tim", ""], ["Fischer", "Volker", ""], ["Bischoff", "Bastian", ""]]}, {"id": "1702.04280", "submitter": "Afshin Dehghan", "authors": "Afshin Dehghan and Enrique G. Ortiz and Guang Shu and Syed Zain Masood", "title": "DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional\n  Neural Network", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the details of Sighthound's fully automated age, gender\nand emotion recognition system. The backbone of our system consists of several\ndeep convolutional neural networks that are not only computationally\ninexpensive, but also provide state-of-the-art results on several competitive\nbenchmarks. To power our novel deep networks, we collected large labeled\ndatasets through a semi-supervised pipeline to reduce the annotation\neffort/time. We tested our system on several public benchmarks and report\noutstanding results. Our age, gender and emotion recognition models are\navailable to developers through the Sighthound Cloud API at\nhttps://www.sighthound.com/products/cloud\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:34:05 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 01:43:04 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Dehghan", "Afshin", ""], ["Ortiz", "Enrique G.", ""], ["Shu", "Guang", ""], ["Masood", "Syed Zain", ""]]}, {"id": "1702.04292", "submitter": "Amir Rasouli", "authors": "Amir Rasouli and John K. Tsotsos", "title": "Integrating Three Mechanisms of Visual Attention for Active Visual\n  Search", "comments": "Presented at International Symposium On Attention in Cognitive\n  Systems (ISACS) in Association with IROS, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for robotic visual search can benefit from the use of visual\nattention methods in order to reduce computational costs. Here, we describe how\nthree distinct mechanisms of visual attention can be integrated and\nproductively used to improve search performance. The first is viewpoint\nselection as has been proposed earlier using a greedy search over a\nprobabilistic occupancy grid representation. The second is top-down\nobject-based attention using a histogram backprojection method, also previously\ndescribed. The third is visual saliency. This is novel in the sense that it is\nnot used as a region-of-interest method for the current image but rather as a\nnoncombinatorial form of look-ahead in search for future viewpoint selection.\nAdditionally, the integration of these three attentional schemes within a\nsingle framework is unique and not previously studied. We examine our proposed\nmethod in scenarios where little or no information regarding the environment is\navailable. Through extensive experiments on a mobile robot, we show that our\nmethod improves visual search performance by reducing the time and number of\nactions required.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 17:06:37 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Rasouli", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1702.04377", "submitter": "Amir Ghaderi", "authors": "Ali Sharifara, Mohd Shafry Mohd Rahim, Farhad Navabifar, Dylan Ebert,\n  Amir Ghaderi, Michalis Papakostas", "title": "Enhanced Facial Recognition Framework based on Skin Tone and False Alarm\n  Rejection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is one of the challenging tasks in computer vision. Human face\ndetection plays an essential role in the first stage of face processing\napplications such as face recognition, face tracking, image database\nmanagement, etc. In these applications, face objects often come from an\ninconsequential part of images that contain variations, namely different\nillumination, poses, and occlusion. These variations can decrease face\ndetection rate noticeably. Most existing face detection approaches are not\naccurate, as they have not been able to resolve unstructured images due to\nlarge appearance variations and can only detect human faces under one\nparticular variation. Existing frameworks of face detection need enhancements\nto detect human faces under the stated variations to improve detection rate and\nreduce detection time. In this study, an enhanced face detection framework is\nproposed to improve detection rate based on skin color and provide a validation\nprocess. A preliminary segmentation of the input images based on skin color can\nsignificantly reduce search space and accelerate the process of human face\ndetection. The primary detection is based on Haar-like features and the\nAdaboost algorithm. A validation process is introduced to reject non-face\nobjects, which might occur during the face detection process. The validation\nprocess is based on two-stage Extended Local Binary Patterns. The experimental\nresults on the CMU-MIT and Caltech 10000 datasets over a wide range of facial\nvariations in different colors, positions, scales, and lighting conditions\nindicated a successful face detection rate.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 20:21:09 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Sharifara", "Ali", ""], ["Rahim", "Mohd Shafry Mohd", ""], ["Navabifar", "Farhad", ""], ["Ebert", "Dylan", ""], ["Ghaderi", "Amir", ""], ["Papakostas", "Michalis", ""]]}, {"id": "1702.04405", "submitter": "Angela Dai", "authors": "Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas\n  Funkhouser, Matthias Nie{\\ss}ner", "title": "ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for leveraging supervised deep learning methods is the\navailability of large, labeled datasets. Unfortunately, in the context of RGB-D\nscene understanding, very little data is available -- current datasets cover a\nsmall range of scene views and have limited semantic annotations. To address\nthis issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views\nin 1513 scenes annotated with 3D camera poses, surface reconstructions, and\nsemantic segmentations. To collect this data, we designed an easy-to-use and\nscalable RGB-D capture system that includes automated surface reconstruction\nand crowdsourced semantic annotation. We show that using this data helps\nachieve state-of-the-art performance on several 3D scene understanding tasks,\nincluding 3D object classification, semantic voxel labeling, and CAD model\nretrieval. The dataset is freely available at http://www.scan-net.org.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 22:08:03 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 08:09:33 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Dai", "Angela", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""], ["Halber", "Maciej", ""], ["Funkhouser", "Thomas", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1702.04455", "submitter": "Ching-Hui Chen", "authors": "Ching-Hui Chen, Vishal M. Patel, Rama Chellappa", "title": "Learning from Ambiguously Labeled Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a classifier from ambiguously labeled face images is challenging\nsince training images are not always explicitly-labeled. For instance, face\nimages of two persons in a news photo are not explicitly labeled by their names\nin the caption. We propose a Matrix Completion for Ambiguity Resolution (MCar)\nmethod for predicting the actual labels from ambiguously labeled images. This\nstep is followed by learning a standard supervised classifier from the\ndisambiguated labels to classify new images. To prevent the majority labels\nfrom dominating the result of MCar, we generalize MCar to a weighted MCar\n(WMCar) that handles label imbalance. Since WMCar outputs a soft labeling\nvector of reduced ambiguity for each instance, we can iteratively refine it by\nfeeding it as the input to WMCar. Nevertheless, such an iterative\nimplementation can be affected by the noisy soft labeling vectors, and thus the\nperformance may degrade. Our proposed Iterative Candidate Elimination (ICE)\nprocedure makes the iterative ambiguity resolution possible by gradually\neliminating a portion of least likely candidates in ambiguously labeled face.\nWe further extend MCar to incorporate the labeling constraints between\ninstances when such prior knowledge is available. Compared to existing methods,\nour approach demonstrates improvement on several ambiguously labeled datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 03:31:17 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 05:35:22 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Chen", "Ching-Hui", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1702.04463", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xin Yuan, Bei Li, Xinggan Zhang, Xin Liu, Lan Tang and\n  Ying-Chang Liang", "title": "Analyzing the Weighted Nuclear Norm Minimization and Nuclear Norm\n  Minimization based on Group Sparse Representation", "comments": "arXiv admin note: substantial text overlap with arXiv:1611.08983", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rank minimization methods have attracted considerable interest in various\nareas, such as computer vision and machine learning. The most representative\nwork is nuclear norm minimization (NNM), which can recover the matrix rank\nexactly under some restricted and theoretical guarantee conditions. However,\nfor many real applications, NNM is not able to approximate the matrix rank\naccurately, since it often tends to over-shrink the rank components. To rectify\nthe weakness of NNM, recent advances have shown that weighted nuclear norm\nminimization (WNNM) can achieve a better matrix rank approximation than NNM,\nwhich heuristically set the weight being inverse to the singular values.\nHowever, it still lacks a sound mathematical explanation on why WNNM is more\nfeasible than NNM. In this paper, we propose a scheme to analyze WNNM and NNM\nfrom the perspective of the group sparse representation. Specifically, we\ndesign an adaptive dictionary to bridge the gap between the group sparse\nrepresentation and the rank minimization models. Based on this scheme, we\nprovide a mathematical derivation to explain why WNNM is more feasible than\nNNM. Moreover, due to the heuristical set of the weight, WNNM sometimes pops\nout error in the operation of SVD, and thus we present an adaptive weight\nsetting scheme to avoid this error. We then employ the proposed scheme on two\nlow-level vision tasks including image denoising and image inpainting.\nExperimental results demonstrate that WNNM is more feasible than NNM and the\nproposed scheme outperforms many current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 04:28:52 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 02:24:41 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 01:11:05 GMT"}, {"version": "v4", "created": "Wed, 31 May 2017 11:02:36 GMT"}, {"version": "v5", "created": "Thu, 19 Jul 2018 03:11:49 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Yuan", "Xin", ""], ["Li", "Bei", ""], ["Zhang", "Xinggan", ""], ["Liu", "Xin", ""], ["Tang", "Lan", ""], ["Liang", "Ying-Chang", ""]]}, {"id": "1702.04471", "submitter": "Navaneeth Bodla", "authors": "Navaneeth Bodla, Jingxiao Zheng, Hongyu Xu, Jun-Cheng Chen, Carlos\n  Castillo, Rama Chellappa", "title": "Deep Heterogeneous Feature Fusion for Template-Based Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has yielded impressive performance for face\nrecognition, many studies have shown that different networks learn different\nfeature maps: while some networks are more receptive to pose and illumination\nothers appear to capture more local information. Thus, in this work, we propose\na deep heterogeneous feature fusion network to exploit the complementary\ninformation present in features generated by different deep convolutional\nneural networks (DCNNs) for template-based face recognition, where a template\nrefers to a set of still face images or video frames from different sources\nwhich introduces more blur, pose, illumination and other variations than\ntraditional face datasets. The proposed approach efficiently fuses the\ndiscriminative information of different deep features by 1) jointly learning\nthe non-linear high-dimensional projection of the deep features and 2)\ngenerating a more discriminative template representation which preserves the\ninherent geometry of the deep features in the feature space. Experimental\nresults on the IARPA Janus Challenge Set 3 (Janus CS3) dataset demonstrate that\nthe proposed method can effectively improve the recognition performance. In\naddition, we also present a series of covariate experiments on the face\nverification task for in-depth qualitative evaluations for the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 06:23:05 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Bodla", "Navaneeth", ""], ["Zheng", "Jingxiao", ""], ["Xu", "Hongyu", ""], ["Chen", "Jun-Cheng", ""], ["Castillo", "Carlos", ""], ["Chellappa", "Rama", ""]]}, {"id": "1702.04479", "submitter": "Sungeun Hong", "authors": "Sungeun Hong, Jongbin Ryu, Woobin Im, Hyun S. Yang", "title": "Recognizing Dynamic Scenes with Deep Dual Descriptor based on Key Frames\n  and Key Segments", "comments": "10 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing dynamic scenes is one of the fundamental problems in scene\nunderstanding, which categorizes moving scenes such as a forest fire,\nlandslide, or avalanche. While existing methods focus on reliable capturing of\nstatic and dynamic information, few works have explored frame selection from a\ndynamic scene sequence. In this paper, we propose dynamic scene recognition\nusing a deep dual descriptor based on `key frames' and `key segments.' Key\nframes that reflect the feature distribution of the sequence with a small\nnumber are used for capturing salient static appearances. Key segments, which\nare captured from the area around each key frame, provide an additional\ndiscriminative power by dynamic patterns within short time intervals. To this\nend, two types of transferred convolutional neural network features are used in\nour approach. A fully connected layer is used to select the key frames and key\nsegments, while the convolutional layer is used to describe them. We conducted\nexperiments using public datasets as well as a new dataset comprised of 23\ndynamic scene classes with 10 videos per class. The evaluation results\ndemonstrated the state-of-the-art performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 06:59:01 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 07:14:19 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Hong", "Sungeun", ""], ["Ryu", "Jongbin", ""], ["Im", "Woobin", ""], ["Yang", "Hyun S.", ""]]}, {"id": "1702.04517", "submitter": "Wei Zhang", "authors": "Wei Zhang, Lei Han, Juanzhen Sun, Hanyang Guo and Jie Dai", "title": "Application of Multi-channel 3D-cube Successive Convolution Network for\n  Convective Storm Nowcasting", "comments": "9 pages, 9 figures, 3 tables, This is an expanded version of the\n  paper accepted by 2019 IEEE International Conference on Big Data. The\n  copyright of this paper has been transferred to the IEEE, please comply with\n  the copyright of the IEEE", "journal-ref": "2019 IEEE International Conference on Big Data (Big Data), Los\n  Angeles, CA, USA, 2019, pp. 1705-1710", "doi": "10.1109/BigData47090.2019.9005568", "report-no": "pp. 1705-1710", "categories": "cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convective storm nowcasting has attracted substantial attention in various\nfields. Existing methods under a deep learning framework rely primarily on\nradar data. Although they perform nowcast storm advection well, it is still\nchallenging to nowcast storm initiation and growth, due to the limitations of\nthe radar observations. This paper describes the first attempt to nowcast storm\ninitiation, growth, and advection simultaneously under a deep learning\nframework using multi-source meteorological data. To this end, we present a\nmulti-channel 3D-cube successive convolution network (3D-SCN). As real-time\nre-analysis meteorological data can now provide valuable atmospheric boundary\nlayer thermal dynamic information, which is essential to predict storm\ninitiation and growth, both raw 3D radar and re-analysis data are used directly\nwithout any handcraft feature engineering. These data are formulated as\nmulti-channel 3D cubes, to be fed into our network, which are convolved by\ncross-channel 3D convolutions. By stacking successive convolutional layers\nwithout pooling, we build an end-to-end trainable model for nowcasting.\nExperimental results show that deep learning methods achieve better performance\nthan traditional extrapolation methods. The qualitative analyses of 3D-SCN show\nencouraging results of nowcasting of storm initiation, growth, and advection.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 09:35:45 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 11:04:17 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 04:08:10 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 02:10:02 GMT"}, {"version": "v5", "created": "Tue, 5 Nov 2019 00:27:48 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Wei", ""], ["Han", "Lei", ""], ["Sun", "Juanzhen", ""], ["Guo", "Hanyang", ""], ["Dai", "Jie", ""]]}, {"id": "1702.04528", "submitter": "Xiaomei Zhao", "authors": "Xiaomei Zhao, Yihong Wu, Guidong Song, Zhenye Li, Yazhuo Zhang, and\n  Yong Fan", "title": "A deep learning model integrating FCNNs and CRFs for brain tumor\n  segmentation", "comments": "This version was accepted in the journal Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2017.10.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable brain tumor segmentation is a critical component in\ncancer diagnosis, treatment planning, and treatment outcome evaluation. Build\nupon successful deep learning techniques, a novel brain tumor segmentation\nmethod is developed by integrating fully convolutional neural networks (FCNNs)\nand Conditional Random Fields (CRFs) in a unified framework to obtain\nsegmentation results with appearance and spatial consistency. We train a deep\nlearning based segmentation model using 2D image patches and image slices in\nfollowing steps: 1) training FCNNs using image patches; 2) training CRFs as\nRecurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs\nfixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices.\nParticularly, we train 3 segmentation models using 2D image patches and slices\nobtained in axial, coronal and sagittal views respectively, and combine them to\nsegment brain tumors using a voting based fusion strategy. Our method could\nsegment brain images slice-by-slice, much faster than those based on image\npatches. We have evaluated our method based on imaging data provided by the\nMultimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015\nand BRATS 2016. The experimental results have demonstrated that our method\ncould build a segmentation model with Flair, T1c, and T2 scans and achieve\ncompetitive performance as those built with Flair, T1, T1c, and T2 scans.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 10:06:13 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 00:56:42 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 02:49:49 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Zhao", "Xiaomei", ""], ["Wu", "Yihong", ""], ["Song", "Guidong", ""], ["Li", "Zhenye", ""], ["Zhang", "Yazhuo", ""], ["Fan", "Yong", ""]]}, {"id": "1702.04562", "submitter": "Shujie Chen", "authors": "Shu-Jie Chen and Hui-Liang Shen", "title": "Normalized Total Gradient: A New Measure for Multispectral Image\n  Registration", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TIP.2017.2776753", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a fundamental issue in multispectral image processing.\nIn filter wheel based multispectral imaging systems, the non-coplanar placement\nof the filters always causes the misalignment of multiple channel images. The\nselective characteristic of spectral response in multispectral imaging raises\ntwo challenges to image registration. First, the intensity levels of a local\nregion may be different in individual channel images. Second, the local\nintensity may vary rapidly in some channel images while keeps stationary in\nothers. Conventional multimodal measures, such as mutual information,\ncorrelation coefficient, and correlation ratio, can register images with\ndifferent regional intensity levels, but will fail in the circumstance of\nsevere local intensity variation. In this paper, a new measure, namely\nnormalized total gradient (NTG), is proposed for multispectral image\nregistration. The NTG is applied on the difference between two channel images.\nThis measure is based on the key assumption (observation) that the gradient of\ndifference image between two aligned channel images is sparser than that\nbetween two misaligned ones. A registration framework, which incorporates image\npyramid and global/local optimization, is further introduced for rigid\ntransform. Experimental results validate that the proposed method is effective\nfor multispectral image registration and performs better than conventional\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 11:52:38 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chen", "Shu-Jie", ""], ["Shen", "Hui-Liang", ""]]}, {"id": "1702.04593", "submitter": "Tatjana Chavdarova", "authors": "Tatjana Chavdarova and Fran\\c{c}ois Fleuret", "title": "Deep Multi-camera People Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of multi-view people occupancy map\nestimation. Existing solutions for this problem either operate per-view, or\nrely on a background subtraction pre-processing. Both approaches lessen the\ndetection performance as scenes become more crowded. The former does not\nexploit joint information, whereas the latter deals with ambiguous input due to\nthe foreground blobs becoming more and more interconnected as the number of\ntargets increases.\n  Although deep learning algorithms have proven to excel on remarkably numerous\ncomputer vision tasks, such a method has not been applied yet to this problem.\nIn large part this is due to the lack of large-scale multi-camera data-set.\n  The core of our method is an architecture which makes use of monocular\npedestrian data-set, available at larger scale then the multi-view ones,\napplies parallel processing to the multiple video streams, and jointly utilises\nit. Our end-to-end deep learning method outperforms existing methods by large\nmargins on the commonly used PETS 2009 data-set. Furthermore, we make publicly\navailable a new three-camera HD data-set. Our source code and trained models\nwill be made available under an open-source license.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 13:16:41 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 11:35:26 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 19:14:01 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Chavdarova", "Tatjana", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1702.04595", "submitter": "Luisa Zintgraf", "authors": "Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling", "title": "Visualizing Deep Neural Network Decisions: Prediction Difference\n  Analysis", "comments": "ICLR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the prediction difference analysis method for\nvisualizing the response of a deep neural network to a specific input. When\nclassifying images, the method highlights areas in a given input image that\nprovide evidence for or against a certain class. It overcomes several\nshortcoming of previous methods and provides great additional insight into the\ndecision making process of classifiers. Making neural network decisions\ninterpretable through visualization is important both to improve models and to\naccelerate the adoption of black-box classifiers in application areas such as\nmedicine. We illustrate the method in experiments on natural images (ImageNet\ndata), as well as medical images (MRI brain scans).\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 13:25:26 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Zintgraf", "Luisa M", ""], ["Cohen", "Taco S", ""], ["Adel", "Tameem", ""], ["Welling", "Max", ""]]}, {"id": "1702.04641", "submitter": "Franziska Lippoldt", "authors": "Franziska Lippoldt and Hartmut Schwandt", "title": "Filling missing data in point clouds by merging structured and\n  unstructured point clouds", "comments": "6 pages, 1 figure, in preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds arising from structured data, mainly as a result of CT scans,\nprovides special properties on the distribution of points and the distances\nbetween those. Yet often, the amount of data provided can not compare to\nunstructured point clouds, i.e. data that arises from 3D light scans or laser\nscans. This article hereby proposes an approach to extend structured data and\nenhance the quality by inserting selected points from an unstructured point\ncloud. The resulting point cloud still has a partial structure that is called\n\"half-structure\". In this way, missing data that can not be optimally recovered\nthrough other surface reconstruction methods can be completed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 14:59:10 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Lippoldt", "Franziska", ""], ["Schwandt", "Hartmut", ""]]}, {"id": "1702.04657", "submitter": "Olivier Le Meur", "authors": "Olivier Le Meur, Antoine Coutrot, Zhi Liu, Adrien Le Roch, Andrea Helo\n  and Pia Rama", "title": "Computational Model for Predicting Visual Fixations from Childhood to\n  Adulthood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How people look at visual information reveals fundamental information about\nthemselves, their interests and their state of mind. While previous visual\nattention models output static 2-dimensional saliency maps, saccadic models aim\nto predict not only where observers look at but also how they move their eyes\nto explore the scene. Here we demonstrate that saccadic models are a flexible\nframework that can be tailored to emulate observer's viewing tendencies. More\nspecifically, we use the eye data from 101 observers split in 5 age groups\n(adults, 8-10 y.o., 6-8 y.o., 4-6 y.o. and 2 y.o.) to train our saccadic model\nfor different stages of the development of the human visual system. We show\nthat the joint distribution of saccade amplitude and orientation is a visual\nsignature specific to each age group, and can be used to generate age-dependent\nscanpaths. Our age-dependent saccadic model not only outputs human-like,\nage-specific visual scanpath, but also significantly outperforms other\nstate-of-the-art saliency models. In this paper, we demonstrate that the\ncomputational modelling of visual attention, through the use of saccadic model,\ncan be efficiently adapted to emulate the gaze behavior of a specific group of\nobservers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 15:48:45 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Meur", "Olivier Le", ""], ["Coutrot", "Antoine", ""], ["Liu", "Zhi", ""], ["Roch", "Adrien Le", ""], ["Helo", "Andrea", ""], ["Rama", "Pia", ""]]}, {"id": "1702.04663", "submitter": "Abdul Kawsar Tushar", "authors": "Akm Ashiquzzaman and Abdul Kawsar Tushar", "title": "Handwritten Arabic Numeral Recognition using Deep Learning Neural\n  Networks", "comments": "Conference Name - 2017 IEEE International Conference on Imaging,\n  Vision & Pattern Recognition (icIVPR17) 4 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten character recognition is an active area of research with\napplications in numerous fields. Past and recent works in this field have\nconcentrated on various languages. Arabic is one language where the scope of\nresearch is still widespread, with it being one of the most popular languages\nin the world and being syntactically different from other major languages. Das\net al. \\cite{DBLP:journals/corr/abs-1003-1891} has pioneered the research for\nhandwritten digit recognition in Arabic. In this paper, we propose a novel\nalgorithm based on deep learning neural networks using appropriate activation\nfunction and regularization layer, which shows significantly improved accuracy\ncompared to the existing Arabic numeral recognition methods. The proposed model\ngives 97.4 percent accuracy, which is the recorded highest accuracy of the\ndataset used in the experiment. We also propose a modification of the method\ndescribed in \\cite{DBLP:journals/corr/abs-1003-1891}, where our method scores\nidentical accuracy as that of \\cite{DBLP:journals/corr/abs-1003-1891}, with the\nvalue of 93.8 percent.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 16:06:15 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Ashiquzzaman", "Akm", ""], ["Tushar", "Abdul Kawsar", ""]]}, {"id": "1702.04680", "submitter": "Dmitry Kislyuk", "authors": "Andrew Zhai, Dmitry Kislyuk, Yushi Jing, Michael Feng, Eric Tzeng,\n  Jeff Donahue, Yue Li Du, Trevor Darrell", "title": "Visual Discovery at Pinterest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past three years Pinterest has experimented with several visual\nsearch and recommendation services, including Related Pins (2014), Similar\nLooks (2015), Flashlight (2016) and Lens (2017). This paper presents an\noverview of our visual discovery engine powering these services, and shares the\nrationales behind our technical and product decisions such as the use of object\ndetection and interactive user interfaces. We conclude that this visual\ndiscovery engine significantly improves engagement in both search and\nrecommendation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 16:52:52 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 18:31:39 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Zhai", "Andrew", ""], ["Kislyuk", "Dmitry", ""], ["Jing", "Yushi", ""], ["Feng", "Michael", ""], ["Tzeng", "Eric", ""], ["Donahue", "Jeff", ""], ["Du", "Yue Li", ""], ["Darrell", "Trevor", ""]]}, {"id": "1702.04710", "submitter": "Xi Yin", "authors": "Xi Yin and Xiaoming Liu", "title": "Multi-Task Convolutional Neural Network for Pose-Invariant Face\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2765830", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores multi-task learning (MTL) for face recognition. We answer\nthe questions of how and why MTL can improve the face recognition performance.\nFirst, we propose a multi-task Convolutional Neural Network (CNN) for face\nrecognition where identity classification is the main task and pose,\nillumination, and expression estimations are the side tasks. Second, we develop\na dynamic-weighting scheme to automatically assign the loss weight to each side\ntask, which is a crucial problem in MTL. Third, we propose a pose-directed\nmulti-task CNN by grouping different poses to learn pose-specific identity\nfeatures, simultaneously across all poses. Last but not least, we propose an\nenergy-based weight analysis method to explore how CNN-based MTL works. We\nobserve that the side tasks serve as regularizations to disentangle the\nvariations from the learnt identity features. Extensive experiments on the\nentire Multi-PIE dataset demonstrate the effectiveness of the proposed\napproach. To the best of our knowledge, this is the first work using all data\nin Multi-PIE for face recognition. Our approach is also applicable to\nin-the-wild datasets for pose-invariant face recognition and achieves\ncomparable or better performance than state of the art on LFW, CFP, and IJB-A\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 18:41:21 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 03:09:04 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Yin", "Xi", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1702.04843", "submitter": "Abdul Kawsar Tushar", "authors": "Mohammad Asiful Hossain and Abdul Kawsar Tushar", "title": "Chord Angle Deviation using Tangent (CADT), an Efficient and Robust\n  Contour-based Corner Detector", "comments": "Conference Name - 2017 IEEE International Conference on Imaging,\n  Vision & Pattern Recognition (icIVPR17); Conference Date - 13 Feb, 2017;\n  Conference Venue - University of Dhaka, Dhaka, Bangladesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of corner is the most essential process in a large number of\ncomputer vision and image processing applications. We have mentioned a number\nof popular contour-based corner detectors in our paper. Among all these\ndetectors chord to triangular arm angle (CTAA) has been demonstrated as the\nmost dominant corner detector in terms of average repeatability. We introduce a\nnew effective method to calculate the value of curvature in this paper. By\ndemonstrating experimental results, our proposed technique outperforms CTAA and\nother detectors mentioned in this paper. The results exhibit that our proposed\nmethod is simple yet efficient at finding out corners more accurately and\nreliably.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 02:52:36 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Hossain", "Mohammad Asiful", ""], ["Tushar", "Abdul Kawsar", ""]]}, {"id": "1702.04858", "submitter": "Jingqing Zhu", "authors": "Jianqing Zhu, Huanqiang Zeng, Shengcai Liao, Zhen Lei, Canhui Cai, and\n  LiXin Zheng", "title": "Deep Hybrid Similarity Learning for Person Re-identification", "comments": "10 pages, 12 figures, fixed typo errors in Fig.8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-IDentification (Re-ID) aims to match person images captured from\ntwo non-overlapping cameras. In this paper, a deep hybrid similarity learning\n(DHSL) method for person Re-ID based on a convolution neural network (CNN) is\nproposed. In our approach, a CNN learning feature pair for the input image pair\nis simultaneously extracted. Then, both the element-wise absolute difference\nand multiplication of the CNN learning feature pair are calculated. Finally, a\nhybrid similarity function is designed to measure the similarity between the\nfeature pair, which is realized by learning a group of weight coefficients to\nproject the element-wise absolute difference and multiplication into a\nsimilarity score. Consequently, the proposed DHSL method is able to reasonably\nassign parameters of feature learning and metric learning in a CNN so that the\nperformance of person Re-ID is improved. Experiments on three challenging\nperson Re-ID databases, QMUL GRID, VIPeR and CUHK03, illustrate that the\nproposed DHSL method is superior to multiple state-of-the-art person Re-ID\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 04:59:01 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 10:36:38 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Zhu", "Jianqing", ""], ["Zeng", "Huanqiang", ""], ["Liao", "Shengcai", ""], ["Lei", "Zhen", ""], ["Cai", "Canhui", ""], ["Zheng", "LiXin", ""]]}, {"id": "1702.04869", "submitter": "Sergi Valverde", "authors": "Sergi Valverde, Mariano Cabezas, Eloy Roura, Sandra\n  Gonz\\'alez-Vill\\`a, Deborah Pareto, Joan-Carles Vilanova, LLu\\'is\n  Rami\\'o-Torrent\\`a, \\`Alex Rovira, Arnau Oliver and Xavier Llad\\'o", "title": "Improving automated multiple sclerosis lesion segmentation with a\n  cascaded 3D convolutional neural network approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel automated method for White Matter (WM)\nlesion segmentation of Multiple Sclerosis (MS) patient images. Our approach is\nbased on a cascade of two 3D patch-wise convolutional neural networks (CNN).\nThe first network is trained to be more sensitive revealing possible candidate\nlesion voxels while the second network is trained to reduce the number of\nmisclassified voxels coming from the first network. This cascaded CNN\narchitecture tends to learn well from small sets of training data, which can be\nvery interesting in practice, given the difficulty to obtain manual label\nannotations and the large amount of available unlabeled Magnetic Resonance\nImaging (MRI) data. We evaluate the accuracy of the proposed method on the\npublic MS lesion segmentation challenge MICCAI2008 dataset, comparing it with\nrespect to other state-of-the-art MS lesion segmentation tools. Furthermore,\nthe proposed method is also evaluated on two private MS clinical datasets,\nwhere the performance of our method is also compared with different recent\npublic available state-of-the-art MS lesion segmentation methods. At the time\nof writing this paper, our method is the best ranked approach on the MICCAI2008\nchallenge, outperforming the rest of 60 participant methods when using all the\navailable input modalities (T1-w, T2-w and FLAIR), while still in the top-rank\n(3rd position) when using only T1-w and FLAIR modalities. On clinical MS data,\nour approach exhibits a significant increase in the accuracy segmenting of WM\nlesions when compared with the rest of evaluated methods, highly correlating\n($r \\ge 0.97$) also with the expected lesion volume.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 06:23:14 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Valverde", "Sergi", ""], ["Cabezas", "Mariano", ""], ["Roura", "Eloy", ""], ["Gonz\u00e1lez-Vill\u00e0", "Sandra", ""], ["Pareto", "Deborah", ""], ["Vilanova", "Joan-Carles", ""], ["Rami\u00f3-Torrent\u00e0", "LLu\u00eds", ""], ["Rovira", "\u00c0lex", ""], ["Oliver", "Arnau", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1702.05068", "submitter": "Adam Santoro", "authors": "David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy\n  Lillicrap, Peter Battaglia", "title": "Discovering objects and their relations from entangled scene\n  representations", "comments": "ICLR Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our world can be succinctly and compactly described as structured scenes of\nobjects and relations. A typical room, for example, contains salient objects\nsuch as tables, chairs and books, and these objects typically relate to each\nother by their underlying causes and semantics. This gives rise to correlated\nfeatures, such as position, function and shape. Humans exploit knowledge of\nobjects and their relations for learning a wide spectrum of tasks, and more\ngenerally when learning the structure underlying observed data. In this work,\nwe introduce relation networks (RNs) - a general purpose neural network\narchitecture for object-relation reasoning. We show that RNs are capable of\nlearning object relations from scene description data. Furthermore, we show\nthat RNs can act as a bottleneck that induces the factorization of objects from\nentangled scene description inputs, and from distributed deep representations\nof scene images provided by a variational autoencoder. The model can also be\nused in conjunction with differentiable memory mechanisms for implicit relation\ndiscovery in one-shot learning tasks. Our results suggest that relation\nnetworks are a potentially powerful architecture for solving a variety of\nproblems that require object relation reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 18:08:27 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Raposo", "David", ""], ["Santoro", "Adam", ""], ["Barrett", "David", ""], ["Pascanu", "Razvan", ""], ["Lillicrap", "Timothy", ""], ["Battaglia", "Peter", ""]]}, {"id": "1702.05085", "submitter": "Amit Kumar", "authors": "Amit Kumar, Azadeh Alavi and Rama Chellappa", "title": "KEPLER: Keypoint and Pose Estimation of Unconstrained Faces by Learning\n  Efficient H-CNN Regressors", "comments": "Accept as Oral FG'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint detection is one of the most important pre-processing steps in tasks\nsuch as face modeling, recognition and verification. In this paper, we present\nan iterative method for Keypoint Estimation and Pose prediction of\nunconstrained faces by Learning Efficient H-CNN Regressors (KEPLER) for\naddressing the face alignment problem. Recent state of the art methods have\nshown improvements in face keypoint detection by employing Convolution Neural\nNetworks (CNNs). Although a simple feed forward neural network can learn the\nmapping between input and output spaces, it cannot learn the inherent\nstructural dependencies. We present a novel architecture called H-CNN\n(Heatmap-CNN) which captures structured global and local features and thus\nfavors accurate keypoint detecion. HCNN is jointly trained on the visibility,\nfiducials and 3D-pose of the face. As the iterations proceed, the error\ndecreases making the gradients small and thus requiring efficient training of\nDCNNs to mitigate this. KEPLER performs global corrections in pose and\nfiducials for the first four iterations followed by local corrections in the\nsubsequent stage. As a by-product, KEPLER also provides 3D pose (pitch, yaw and\nroll) of the face accurately. In this paper, we show that without using any 3D\ninformation, KEPLER outperforms state of the art methods for alignment on\nchallenging datasets such as AFW and AFLW.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 18:44:59 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Kumar", "Amit", ""], ["Alavi", "Azadeh", ""], ["Chellappa", "Rama", ""]]}, {"id": "1702.05089", "submitter": "Dena Bazazian", "authors": "Dena Bazazian, Raul Gomez, Anguelos Nicolaou, Lluis Gomez, Dimosthenis\n  Karatzas, Andrew D. Bagdanov", "title": "Improving Text Proposals for Scene Images with Fully Convolutional\n  Networks", "comments": "6 pages, 8 figures, International Conference on Pattern Recognition\n  (ICPR) - DLPR (Deep Learning for Pattern Recognition) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Proposals have emerged as a class-dependent version of object proposals\n- efficient approaches to reduce the search space of possible text object\nlocations in an image. Combined with strong word classifiers, text proposals\ncurrently yield top state of the art results in end-to-end scene text\nrecognition. In this paper we propose an improvement over the original Text\nProposals algorithm of Gomez and Karatzas (2016), combining it with Fully\nConvolutional Networks to improve the ranking of proposals. Results on the\nICDAR RRC and the COCO-text datasets show superior performance over current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 18:56:53 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Bazazian", "Dena", ""], ["Gomez", "Raul", ""], ["Nicolaou", "Anguelos", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1702.05147", "submitter": "Siham Tabik", "authors": "Roberto Olmos, Siham Tabik and Francisco Herrera", "title": "Automatic Handgun Detection Alarm in Videos Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current surveillance and control systems still require human supervision and\nintervention. This work presents a novel automatic handgun detection system in\nvideos appropriate for both, surveillance and control purposes. We reformulate\nthis detection problem into the problem of minimizing false positives and solve\nit by building the key training data-set guided by the results of a deep\nConvolutional Neural Networks (CNN) classifier, then assessing the best\nclassification model under two approaches, the sliding window approach and\nregion proposal approach. The most promising results are obtained by Faster\nR-CNN based model trained on our new database. The best detector show a high\npotential even in low quality youtube videos and provides satisfactory results\nas automatic alarm system. Among 30 scenes, it successfully activates the alarm\nafter five successive true positives in less than 0.2 seconds, in 27 scenes. We\nalso define a new metric, Alarm Activation per Interval (AApI), to assess the\nperformance of a detection model as an automatic detection system in videos.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 20:30:33 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Olmos", "Roberto", ""], ["Tabik", "Siham", ""], ["Herrera", "Francisco", ""]]}, {"id": "1702.05150", "submitter": "Zoya Bylinskii", "authors": "Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin, Krzysztof Z. Gajos,\n  Aude Oliva, Fredo Durand, Hanspeter Pfister", "title": "BubbleView: an interface for crowdsourcing image importance maps and\n  tracking visual attention", "comments": null, "journal-ref": "TOCHI 2017", "doi": "10.1145/3131275", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present BubbleView, an alternative methodology for eye\ntracking using discrete mouse clicks to measure which information people\nconsciously choose to examine. BubbleView is a mouse-contingent, moving-window\ninterface in which participants are presented with a series of blurred images\nand click to reveal \"bubbles\" - small, circular areas of the image at original\nresolution, similar to having a confined area of focus like the eye fovea.\nAcross 10 experiments with 28 different parameter combinations, we evaluated\nBubbleView on a variety of image types: information visualizations, natural\nimages, static webpages, and graphic designs, and compared the clicks to eye\nfixations collected with eye-trackers in controlled lab settings. We found that\nBubbleView clicks can both (i) successfully approximate eye fixations on\ndifferent images, and (ii) be used to rank image and design elements by\nimportance. BubbleView is designed to collect clicks on static images, and\nworks best for defined tasks such as describing the content of an information\nvisualization or measuring image importance. BubbleView data is cleaner and\nmore consistent than related methodologies that use continuous mouse movements.\nOur analyses validate the use of mouse-contingent, moving-window methodologies\nas approximating eye fixations for different image and task types.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 20:49:26 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 23:37:19 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 14:23:54 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kim", "Nam Wook", ""], ["Bylinskii", "Zoya", ""], ["Borkin", "Michelle A.", ""], ["Gajos", "Krzysztof Z.", ""], ["Oliva", "Aude", ""], ["Durand", "Fredo", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1702.05156", "submitter": "Peter Henderson", "authors": "Peter Henderson and Matthew Vertescher", "title": "An Analysis of Parallelized Motion Masking Using Dual-Mode Single\n  Gaussian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion detection in video is important for a number of applications and\nfields. In video surveillance, motion detection is an essential accompaniment\nto activity recognition for early warning systems. Robotics also has much to\ngain from motion detection and segmentation, particularly in high speed motion\ntracking for tactile systems. There are a myriad of techniques for detecting\nand masking motion in an image. Successful systems have used Gaussian Models to\ndiscern background from foreground in an image (motion from static imagery).\nHowever, particularly in the case of a moving camera or frame of reference, it\nis necessary to compensate for the motion of the camera when attempting to\ndiscern objects moving in the foreground. For example, it is possible to\nestimate motion of the camera through optical flow methods or temporal\ndifferencing and then compensate for this motion in a background subtraction\nmodel. We selection a method by Yi et al. using Dual-Mode Single Gaussian\nModels which does just this. We implement the technique in Intel's Thread\nBuilding Blocks (TBB) and NVIDIA's CUDA libraries. We then compare\nparallelization improvements with a theoretical analysis of speedups based on\nthe characteristics of our selected model and attributes of both TBB and CUDA.\nWe make our implementation available to the public.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 21:10:13 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Henderson", "Peter", ""], ["Vertescher", "Matthew", ""]]}, {"id": "1702.05174", "submitter": "Michal Drozdzal", "authors": "Michal Drozdzal, Gabriel Chartrand, Eugene Vorontsov, Lisa Di Jorio,\n  An Tang, Adriana Romero, Yoshua Bengio, Chris Pal, Samuel Kadoury", "title": "Learning Normalized Inputs for Iterative Estimation in Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a simple, yet powerful pipeline for medical image\nsegmentation that combines Fully Convolutional Networks (FCNs) with Fully\nConvolutional Residual Networks (FC-ResNets). We propose and examine a design\nthat takes particular advantage of recent advances in the understanding of both\nConvolutional Neural Networks as well as ResNets. Our approach focuses upon the\nimportance of a trainable pre-processing when using FC-ResNets and we show that\na low-capacity FCN model can serve as a pre-processor to normalize medical\ninput data. In our image segmentation pipeline, we use FCNs to obtain\nnormalized images, which are then iteratively refined by means of a FC-ResNet\nto generate a segmentation prediction. As in other fully convolutional\napproaches, our pipeline can be used off-the-shelf on different image\nmodalities. We show that using this pipeline, we exhibit state-of-the-art\nperformance on the challenging Electron Microscopy benchmark, when compared to\nother 2D methods. We improve segmentation results on CT images of liver\nlesions, when contrasting with standard FCN methods. Moreover, when applying\nour 2D pipeline on a challenging 3D MRI prostate segmentation challenge we\nreach results that are competitive even when compared to 3D methods. The\nobtained results illustrate the strong potential and versatility of the\npipeline by achieving highly accurate results on multi-modality images from\ndifferent anatomical regions and organs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 22:10:37 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Drozdzal", "Michal", ""], ["Chartrand", "Gabriel", ""], ["Vorontsov", "Eugene", ""], ["Di Jorio", "Lisa", ""], ["Tang", "An", ""], ["Romero", "Adriana", ""], ["Bengio", "Yoshua", ""], ["Pal", "Chris", ""], ["Kadoury", "Samuel", ""]]}, {"id": "1702.05270", "submitter": "Sandro Pezzelle", "authors": "Sandro Pezzelle, Marco Marelli, Raffaella Bernardi", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers\n  from Vision", "comments": "Accepted at EACL2017. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can refer to quantities in a visual scene by using either exact\ncardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,\nmost, all). In humans, these two processes underlie fairly different cognitive\nand neural mechanisms. Inspired by this evidence, the present study proposes\ntwo models for learning the objective meaning of cardinals and quantifiers from\nvisual scenes containing multiple objects. We show that a model capitalizing on\na 'fuzzy' measure of similarity is effective for learning quantifiers, whereas\nthe learning of exact cardinals is better accomplished when information about\nnumber is provided.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 09:26:10 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Pezzelle", "Sandro", ""], ["Marelli", "Marco", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1702.05373", "submitter": "Gregory Cohen", "authors": "Gregory Cohen, Saeed Afshar, Jonathan Tapson, Andr\\'e van Schaik", "title": "EMNIST: an extension of MNIST to handwritten letters", "comments": "The dataset is now available for download from\n  https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist. This\n  link is also included in the revised article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MNIST dataset has become a standard benchmark for learning,\nclassification and computer vision systems. Contributing to its widespread\nadoption are the understandable and intuitive nature of the task, its\nrelatively small size and storage requirements and the accessibility and\nease-of-use of the database itself. The MNIST database was derived from a\nlarger dataset known as the NIST Special Database 19 which contains digits,\nuppercase and lowercase handwritten letters. This paper introduces a variant of\nthe full NIST dataset, which we have called Extended MNIST (EMNIST), which\nfollows the same conversion paradigm used to create the MNIST dataset. The\nresult is a set of datasets that constitute a more challenging classification\ntasks involving letters and digits, and that shares the same image structure\nand parameters as the original MNIST task, allowing for direct compatibility\nwith all existing classifiers and systems. Benchmark results are presented\nalong with a validation of the conversion process through the comparison of the\nclassification results on converted NIST digits and the MNIST digits.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:06:14 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 08:55:36 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Cohen", "Gregory", ""], ["Afshar", "Saeed", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1702.05374", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka", "title": "Domain Adaptation for Visual Applications: A Comprehensive Survey", "comments": "Book chapter to appear in \"Domain Adaptation in Computer Vision\n  Applications\", Springer Series: Advances in Computer Vision and Pattern\n  Recognition, Edited by Gabriela Csurka", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to give an overview of domain adaptation and\ntransfer learning with a specific view on visual applications. After a general\nmotivation, we first position domain adaptation in the larger transfer learning\nproblem. Second, we try to address and analyze briefly the state-of-the-art\nmethods for different types of scenarios, first describing the historical\nshallow methods, addressing both the homogeneous and the heterogeneous domain\nadaptation methods. Third, we discuss the effect of the success of deep\nconvolutional architectures which led to new type of domain adaptation methods\nthat integrate the adaptation within the deep architecture. Fourth, we overview\nthe methods that go beyond image categorization, such as object detection or\nimage segmentation, video analyses or learning visual attributes. Finally, we\nconclude the paper with a section where we relate domain adaptation to other\nmachine learning solutions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:07:40 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 15:42:12 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Csurka", "Gabriela", ""]]}, {"id": "1702.05388", "submitter": "Itoro Ikon", "authors": "Itoro Ikon", "title": "Vehicle Speed Detecting App", "comments": "Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The report presents the measurement of vehicular speed using a smartphone\ncamera. The speed measurement is accomplished by detecting the position of the\nvehicle on a camera frame using the LBP cascade classifier of OpenCV API, the\ndisplacement of the detected vehicle with time is used to compute the speed.\nConversion coefficient is determined to map the pixel displacement to actual\nvehicle distance. The speeds measured are proportional to the ground truth\nspeeds.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:34:02 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 16:01:05 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ikon", "Itoro", ""]]}, {"id": "1702.05413", "submitter": "Julian Arz", "authors": "Julian Arz and Peter Sanders and Johannes Stegmaier and Ralf Mikut", "title": "3D Cell Nuclei Segmentation with Balanced Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell nuclei segmentation is one of the most important tasks in the analysis\nof biomedical images. With ever-growing sizes and amounts of three-dimensional\nimages to be processed, there is a need for better and faster segmentation\nmethods. Graph-based image segmentation has seen a rise in popularity in recent\nyears, but is seen as very costly with regard to computational demand. We\npropose a new segmentation algorithm which overcomes these limitations. Our\nmethod uses recursive balanced graph partitioning to segment foreground\ncomponents of a fast and efficient binarization. We construct a model for the\ncell nuclei to guide the partitioning process. Our algorithm is compared to\nother state-of-the-art segmentation algorithms in an experimental evaluation on\ntwo sets of realistically simulated inputs. Our method is faster, has similar\nor better quality and an acceptable memory overhead.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:01:50 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Arz", "Julian", ""], ["Sanders", "Peter", ""], ["Stegmaier", "Johannes", ""], ["Mikut", "Ralf", ""]]}, {"id": "1702.05421", "submitter": "Amir Rasouli", "authors": "Amir Rasouli and John K. Tsotsos", "title": "The Effect of Color Space Selection on Detectability and\n  Discriminability of Colored Objects", "comments": "submitted to CRV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the effect of color space selection on\ndetectability and discriminability of colored objects under various conditions.\n20 color spaces from the literature are evaluated on a large dataset of\nsimulated and real images. We measure the suitability of color spaces from two\ndifferent perspectives: detectability and discriminability of various color\ngroups. Through experimental evaluation, we found that there is no single\noptimal color space suitable for all color groups. The color spaces have\ndifferent levels of sensitivity to different color groups and they are useful\ndepending on the color of the sought object. Overall, the best results were\nachieved in both simulated and real images using color spaces C1C2C3, UVW and\nXYZ. In addition, using a simulated environment, we show a practical\napplication of color space selection in the context of top-down control in\nactive visual search. The results indicate that on average color space C1C2C3\nfollowed by HSI and XYZ achieve the best time in searching for objects of\nvarious colors. Here, the right choice of color space can improve time of\nsearch on average by 20%. As part of our contribution, we also introduce a\nlarge dataset of simulated 3D objects\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 17:13:30 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Rasouli", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1702.05448", "submitter": "Yu-Wei Chao", "authors": "Yu-Wei Chao and Yunfan Liu and Xieyang Liu and Huayi Zeng and Jia Deng", "title": "Learning to Detect Human-Object Interactions", "comments": "Accepted in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting human-object interactions (HOI) in static\nimages, defined as predicting a human and an object bounding box with an\ninteraction class label that connects them. HOI detection is a fundamental\nproblem in computer vision as it provides semantic information about the\ninteractions among the detected objects. We introduce HICO-DET, a new large\nbenchmark for HOI detection, by augmenting the current HICO classification\nbenchmark with instance annotations. To solve the task, we propose Human-Object\nRegion-based Convolutional Neural Networks (HO-RCNN). At the core of our\nHO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the\nspatial relations between two bounding boxes. Experiments on HICO-DET\ndemonstrate that our HO-RCNN, by exploiting human-object spatial relations\nthrough Interaction Patterns, significantly improves the performance of HOI\ndetection over baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 17:21:30 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 00:23:31 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Chao", "Yu-Wei", ""], ["Liu", "Yunfan", ""], ["Liu", "Xieyang", ""], ["Zeng", "Huayi", ""], ["Deng", "Jia", ""]]}, {"id": "1702.05464", "submitter": "Eric Tzeng", "authors": "Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell", "title": "Adversarial Discriminative Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning methods are a promising approach to training robust deep\nnetworks, and can generate complex samples across diverse domains. They also\ncan improve recognition despite the presence of domain shift or dataset bias:\nseveral adversarial approaches to unsupervised domain adaptation have recently\nbeen introduced, which reduce the difference between the training and test\ndomain distributions and thus improve generalization performance. Prior\ngenerative approaches show compelling visualizations, but are not optimal on\ndiscriminative tasks and can be limited to smaller shifts. Prior discriminative\napproaches could handle larger domain shifts, but imposed tied weights on the\nmodel and did not exploit a GAN-based loss. We first outline a novel\ngeneralized framework for adversarial adaptation, which subsumes recent\nstate-of-the-art approaches as special cases, and we use this generalized view\nto better relate the prior approaches. We propose a previously unexplored\ninstance of our general framework which combines discriminative modeling,\nuntied weight sharing, and a GAN loss, which we call Adversarial Discriminative\nDomain Adaptation (ADDA). We show that ADDA is more effective yet considerably\nsimpler than competing domain-adversarial methods, and demonstrate the promise\nof our approach by exceeding state-of-the-art unsupervised adaptation results\non standard cross-domain digit classification tasks and a new more difficult\ncross-modality object classification task.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:10:53 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Tzeng", "Eric", ""], ["Hoffman", "Judy", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1702.05506", "submitter": "S L Happy", "authors": "Pranav Kumar, S L Happy, Swarnadip Chatterjee, Debdoot Sheet,\n  Aurobinda Routray", "title": "An Unsupervised Approach for Overlapping Cervical Cell Cytoplasm\n  Segmentation", "comments": "4 pages, 4 figures, Biomedical Engineering and Sciences (IECBES),\n  2016 IEEE EMBS Conference on. IEEE, 2016", "journal-ref": "IEEE EMBS Conference on Biomedical Engineering and Sciences\n  (IECBES), 2016", "doi": "10.1109/IECBES.2016.7843424", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The poor contrast and the overlapping of cervical cell cytoplasm are the\nmajor issues in the accurate segmentation of cervical cell cytoplasm. This\npaper presents an automated unsupervised cytoplasm segmentation approach which\ncan effectively find the cytoplasm boundaries in overlapping cells. The\nproposed approach first segments the cell clumps from the cervical smear image\nand detects the nuclei in each cell clump. A modified Otsu method with prior\nclass probability is proposed for accurate segmentation of nuclei from the cell\nclumps. Using distance regularized level set evolution, the contour around each\nnucleus is evolved until it reaches the cytoplasm boundaries. Promising results\nwere obtained by experimenting on ISBI 2015 challenge dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 19:29:57 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Kumar", "Pranav", ""], ["Happy", "S L", ""], ["Chatterjee", "Swarnadip", ""], ["Sheet", "Debdoot", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1702.05552", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan and Clinton Fookes", "title": "Soft + Hardwired Attention: An LSTM Framework for Human Trajectory\n  Prediction and Abnormal Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans we possess an intuitive ability for navigation which we master\nthrough years of practice; however existing approaches to model this trait for\ndiverse tasks including monitoring pedestrian flow and detecting abnormal\nevents have been limited by using a variety of hand-crafted features. Recent\nresearch in the area of deep-learning has demonstrated the power of learning\nfeatures directly from the data; and related research in recurrent neural\nnetworks has shown exemplary results in sequence-to-sequence problems such as\nneural machine translation and neural image caption generation. Motivated by\nthese approaches, we propose a novel method to predict the future motion of a\npedestrian given a short history of their, and their neighbours, past\nbehaviour. The novelty of the proposed method is the combined attention model\nwhich utilises both \"soft attention\" as well as \"hard-wired\" attention in order\nto map the trajectory information from the local neighbourhood to the future\npositions of the pedestrian of interest. We illustrate how a simple\napproximation of attention weights (i.e hard-wired) can be merged together with\nsoft attention weights in order to make our model applicable for challenging\nreal world scenarios with hundreds of neighbours. The navigational capability\nof the proposed method is tested on two challenging publicly available\nsurveillance databases where our model outperforms the current-state-of-the-art\nmethods. Additionally, we illustrate how the proposed architecture can be\ndirectly applied for the task of abnormal event detection without handcrafting\nthe features.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 01:08:18 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1702.05555", "submitter": "Miao Yu", "authors": "Chunlei Li, Guangshuai Gao, Zhoufeng Liu, Di Huang, Sheng Liu, Miao Yu", "title": "Defect detection for patterned fabric images based on GHOG and low-rank\n  decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to accurately detect defects in patterned fabric images, a novel\ndetection algorithm based on Gabor-HOG (GHOG) and low-rank decomposition is\nproposed in this paper. Defect-free pattern fabric images have the specified\ndirection, while defects damage their regularity of direction. Therefore, a\ndirection-aware descriptor is designed, denoted as GHOG, a combination of Gabor\nand HOG, which is extremely valuable for localizing the defect region. Upon\ndevising a powerful directional descriptor, an efficient low-rank decomposition\nmodel is constructed to divide the matrix generated by the directional feature\nextracted from image blocks into a low-rank matrix (background information) and\na sparse matrix (defect information). A nonconvex log det(.) as a smooth\nsurrogate function for the rank instead of the nuclear norm is also exploited\nto improve the efficiency of the low-rank model. Moreover, the computational\nefficiency is further improved by utilizing the alternative direction method of\nmultipliers (ADMM). Thereafter, the saliency map generated by the sparse matrix\nis segmented via the optimal threshold algorithm to locate the defect regions.\nExperimental results show that the proposed method can effectively detect\npatterned fabric defects and outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 01:24:49 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Li", "Chunlei", ""], ["Gao", "Guangshuai", ""], ["Liu", "Zhoufeng", ""], ["Huang", "Di", ""], ["Liu", "Sheng", ""], ["Yu", "Miao", ""]]}, {"id": "1702.05564", "submitter": "Angus Galloway", "authors": "Angus Galloway, Graham W. Taylor, Aaron Ramsay, Medhat Moussa", "title": "The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a\n  Marine Aquaculture Environment", "comments": "Submitted to the Conference on Computer and Robot Vision (CRV) 2017", "journal-ref": null, "doi": "10.5683/SP/NTUOK9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An original dataset for semantic segmentation, Ciona17, is introduced, which\nto the best of the authors' knowledge, is the first dataset of its kind with\npixel-level annotations pertaining to invasive species in a marine environment.\nDiverse outdoor illumination, a range of object shapes, colour, and severe\nocclusion provide a significant real world challenge for the computer vision\ncommunity. An accompanying ground-truthing tool for superpixel labeling, Truth\nand Crop, is also introduced. Finally, we provide a baseline using a variant of\nFully Convolutional Networks, and report results in terms of the standard mean\nintersection over union (mIoU) metric.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 03:40:33 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Galloway", "Angus", ""], ["Taylor", "Graham W.", ""], ["Ramsay", "Aaron", ""], ["Moussa", "Medhat", ""]]}, {"id": "1702.05573", "submitter": "Bo Xin", "authors": "Xiangyu Kong, Bo Xin, Yizhou Wang and Gang Hua", "title": "Collaborative Deep Reinforcement Learning for Joint Object Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of joint top-down active search of multiple objects\nunder interaction, e.g., person riding a bicycle, cups held by the table, etc..\nSuch objects under interaction often can provide contextual cues to each other\nto facilitate more efficient search. By treating each detector as an agent, we\npresent the first collaborative multi-agent deep reinforcement learning\nalgorithm to learn the optimal policy for joint active object localization,\nwhich effectively exploits such beneficial contextual information. We learn\ninter-agent communication through cross connections with gates between the\nQ-networks, which is facilitated by a novel multi-agent deep Q-learning\nalgorithm with joint exploitation sampling. We verify our proposed method on\nmultiple object detection benchmarks. Not only does our model help to improve\nthe performance of state-of-the-art active localization models, it also reveals\ninteresting co-detection patterns that are intuitively interpretable.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 06:00:45 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Kong", "Xiangyu", ""], ["Xin", "Bo", ""], ["Wang", "Yizhou", ""], ["Hua", "Gang", ""]]}, {"id": "1702.05596", "submitter": "Shitao Chen", "authors": "Shitao Chen, Songyi Zhang, Jinghao Shang, Badong Chen, Nanning Zheng", "title": "Brain Inspired Cognitive Model with Attention for Self-Driving Cars", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception-driven approach and end-to-end system are two major vision-based\nframeworks for self-driving cars. However, it is difficult to introduce\nattention and historical information of autonomous driving process, which are\nthe essential factors for achieving human-like driving into these two methods.\nIn this paper, we propose a novel model for self-driving cars named\nbrain-inspired cognitive model with attention (CMA). This model consists of\nthree parts: a convolutional neural network for simulating human visual cortex,\na cognitive map built to describe relationships between objects in complex\ntraffic scene and a recurrent neural network that combines with the real-time\nupdated cognitive map to implement attention mechanism and long-short term\nmemory. The benefit of our model is that can accurately solve three tasks\nsimultaneously:1) detection of the free space and boundaries of the current and\nadjacent lanes. 2)estimation of obstacle distance and vehicle attitude, and 3)\nlearning of driving behavior and decision making from human driver. More\nsignificantly, the proposed model could accept external navigating instructions\nduring an end-to-end driving process. For evaluation, we build a large-scale\nroad-vehicle dataset which contains more than forty thousand labeled road\nimages captured by three cameras on our self-driving car. Moreover, human\ndriving activities and vehicle states are recorded in the meanwhile.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 10:47:16 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Chen", "Shitao", ""], ["Zhang", "Songyi", ""], ["Shang", "Jinghao", ""], ["Chen", "Badong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1702.05619", "submitter": "Luo Jiang", "authors": "Luo Jiang, Juyong Zhang, Bailin Deng, Hao Li, Ligang Liu", "title": "3D Face Reconstruction with Geometry Details from a Single Image", "comments": "Accepted by IEEE Transactions on Image Processing, 2018", "journal-ref": null, "doi": "10.1109/TIP.2018.2845697", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction from a single image is a classical and challenging\nproblem, with wide applications in many areas. Inspired by recent works in face\nanimation from RGB-D or monocular video inputs, we develop a novel method for\nreconstructing 3D faces from unconstrained 2D images, using a coarse-to-fine\noptimization strategy. First, a smooth coarse 3D face is generated from an\nexample-based bilinear face model, by aligning the projection of 3D face\nlandmarks with 2D landmarks detected from the input image. Afterwards, using\nlocal corrective deformation fields, the coarse 3D face is refined using\nphotometric consistency constraints, resulting in a medium face shape. Finally,\na shape-from-shading method is applied on the medium face to recover fine\ngeometric details. Our method outperforms state-of-the-art approaches in terms\nof accuracy and detail recovery, which is demonstrated in extensive experiments\nusing real world models and publicly available datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 15:03:14 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 11:47:34 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jiang", "Luo", ""], ["Zhang", "Juyong", ""], ["Deng", "Bailin", ""], ["Li", "Hao", ""], ["Liu", "Ligang", ""]]}, {"id": "1702.05650", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Fuyong Xing, Hanzi Wang, Yan Yan, Ying Huang, Xiaoshuang\n  Shi, Lin Yang", "title": "Revisiting Graph Construction for Fast Image Segmentation", "comments": "To appear in PR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple but effective method for fast image\nsegmentation. We re-examine the locality-preserving character of spectral\nclustering by constructing a graph over image regions with both global and\nlocal connections. Our novel approach to build graph connections relies on two\nkey observations: 1) local region pairs that co-occur frequently will have a\nhigh probability to reside on a common object; 2) spatially distant regions in\na common object often exhibit similar visual saliency, which implies their\nneighborship in a manifold. We present a novel energy function to efficiently\nconduct graph partitioning. Based on multiple high quality partitions, we show\nthat the generated eigenvector histogram based representation can automatically\ndrive effective unary potentials for a hierarchical random field model to\nproduce multi-class segmentation. Sufficient experiments, on the BSDS500\nbenchmark, large-scale PASCAL VOC and COCO datasets, demonstrate the\ncompetitive segmentation accuracy and significantly improved efficiency of our\nproposed method compared with other state of the arts.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 20:37:42 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 22:40:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zhang", "Zizhao", ""], ["Xing", "Fuyong", ""], ["Wang", "Hanzi", ""], ["Yan", "Yan", ""], ["Huang", "Ying", ""], ["Shi", "Xiaoshuang", ""], ["Yang", "Lin", ""]]}, {"id": "1702.05658", "submitter": "Chang Liu", "authors": "Chang Liu, Fuchun Sun, Changhu Wang, Feng Wang, Alan Yuille", "title": "MAT: A Multimodal Attentive Translator for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we formulate the problem of image captioning as a multimodal\ntranslation task. Analogous to machine translation, we present a\nsequence-to-sequence recurrent neural networks (RNN) model for image caption\ngeneration. Different from most existing work where the whole image is\nrepresented by convolutional neural network (CNN) feature, we propose to\nrepresent the input image as a sequence of detected objects which feeds as the\nsource sequence of the RNN model. In this way, the sequential representation of\nan image can be naturally translated to a sequence of words, as the target\nsequence of the RNN model. To represent the image in a sequential way, we\nextract the objects features in the image and arrange them in a order using\nconvolutional neural networks. To further leverage the visual information from\nthe encoded objects, a sequential attention layer is introduced to selectively\nattend to the objects that are related to generate corresponding words in the\nsentences. Extensive experiments are conducted to validate the proposed\napproach on popular benchmark dataset, i.e., MS COCO, and the proposed model\nsurpasses the state-of-the-art methods in all metrics following the dataset\nsplits of previous work. The proposed approach is also evaluated by the\nevaluation server of MS COCO captioning challenge, and achieves very\ncompetitive results, e.g., a CIDEr of 1.029 (c5) and 1.064 (c40).\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 21:35:06 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 18:39:02 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 14:29:19 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Liu", "Chang", ""], ["Sun", "Fuchun", ""], ["Wang", "Changhu", ""], ["Wang", "Feng", ""], ["Yuille", "Alan", ""]]}, {"id": "1702.05663", "submitter": "Zhao Chen", "authors": "Zhao Chen and Darvin Yi", "title": "The Game Imitation: Deep Supervised Convolutional Networks for Quick\n  Video Game AI", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a vision-only model for gaming AI which uses a late integration\ndeep convolutional network architecture trained in a purely supervised\nimitation learning context. Although state-of-the-art deep learning models for\nvideo game tasks generally rely on more complex methods such as deep-Q\nlearning, we show that a supervised model which requires substantially fewer\nresources and training time can already perform well at human reaction speeds\non the N64 classic game Super Smash Bros. We frame our learning task as a\n30-class classification problem, and our CNN model achieves 80% top-1 and 95%\ntop-3 validation accuracy. With slight test-time fine-tuning, our model is also\ncompetitive during live simulation with the highest-level AI built into the\ngame. We will further show evidence through network visualizations that the\nnetwork is successfully leveraging temporal information during inference to aid\nin decision making. Our work demonstrates that supervised CNN models can\nprovide good performance in challenging policy prediction tasks while being\nsignificantly simpler and more lightweight than alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 22:15:25 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Chen", "Zhao", ""], ["Yi", "Darvin", ""]]}, {"id": "1702.05664", "submitter": "Abhishek Kolagunda", "authors": "Abhishek Kolagunda, Scott Sorensen, Philip Saponaro, Wayne Treible and\n  Chandra Kambhamettu", "title": "Robust Shape Registration using Fuzzy Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape registration is the process of aligning one 3D model to another. Most\nprevious methods to align shapes with no known correspondences attempt to solve\nfor both the transformation and correspondences iteratively. We present a shape\nregistration approach that solves for the transformation using fuzzy\ncorrespondences to maximize the overlap between the given shape and the target\nshape. A coarse to fine approach with Levenberg-Marquardt method is used for\noptimization. Real and synthetic experiments show our approach is robust and\noutperforms other state of the art methods when point clouds are noisy, sparse,\nand have non-uniform density. Experiments show our method is more robust to\ninitialization and can handle larger scale changes and rotation than other\nmethods. We also show that the approach can be used for 2D-3D alignment via\nray-point alignment.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 22:22:57 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Kolagunda", "Abhishek", ""], ["Sorensen", "Scott", ""], ["Saponaro", "Philip", ""], ["Treible", "Wayne", ""], ["Kambhamettu", "Chandra", ""]]}, {"id": "1702.05693", "submitter": "Shanshan Zhang", "authors": "Shanshan Zhang, Rodrigo Benenson and Bernt Schiele", "title": "CityPersons: A Diverse Dataset for Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convnets have enabled significant progress in pedestrian detection recently,\nbut there are still open questions regarding suitable architectures and\ntraining data. We revisit CNN design and point out key adaptations, enabling\nplain FasterRCNN to obtain state-of-the-art results on the Caltech dataset.\n  To achieve further improvement from more and better data, we introduce\nCityPersons, a new set of person annotations on top of the Cityscapes dataset.\nThe diversity of CityPersons allows us for the first time to train one single\nCNN model that generalizes well over multiple benchmarks. Moreover, with\nadditional training with CityPersons, we obtain top results using FasterRCNN on\nCaltech, improving especially for more difficult cases (heavy occlusion and\nsmall scale) and providing higher localization quality.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 03:01:55 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Zhang", "Shanshan", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1702.05698", "submitter": "Wei Xiao", "authors": "Wei Xiao, Xiaolin Huang, Jorge Silva, Saba Emrani and Arin Chaudhuri", "title": "Online Robust Principal Component Analysis with Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA methods are typically batch algorithms which requires loading all\nobservations into memory before processing. This makes them inefficient to\nprocess big data. In this paper, we develop an efficient online robust\nprincipal component methods, namely online moving window robust principal\ncomponent analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can\nsuccessfully track not only slowly changing subspace but also abruptly changed\nsubspace. By embedding hypothesis testing into the algorithm, OMWRPCA can\ndetect change points of the underlying subspaces. Extensive simulation studies\ndemonstrate the superior performance of OMWRPCA compared with other\nstate-of-art approaches. We also apply the algorithm for real-time background\nsubtraction of surveillance video.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 04:08:18 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 19:49:02 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Xiao", "Wei", ""], ["Huang", "Xiaolin", ""], ["Silva", "Jorge", ""], ["Emrani", "Saba", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1702.05711", "submitter": "Hongyang Li", "authors": "Hongyang Li and Yu Liu and Wanli Ouyang and Xiaogang Wang", "title": "Zoom Out-and-In Network with Recursive Training for Object Proposal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a zoom-out-and-in network for generating object\nproposals. We utilize different resolutions of feature maps in the network to\ndetect object instances of various sizes. Specifically, we divide the anchor\ncandidates into three clusters based on the scale size and place them on\nfeature maps of distinct strides to detect small, medium and large objects,\nrespectively. Deeper feature maps contain region-level semantics which can help\nshallow counterparts to identify small objects. Therefore we design a zoom-in\nsub-network to increase the resolution of high level features via a\ndeconvolution operation. The high-level features with high resolution are then\ncombined and merged with low-level features to detect objects. Furthermore, we\ndevise a recursive training pipeline to consecutively regress region proposals\nat the training stage in order to match the iterative regression at the testing\nstage. We demonstrate the effectiveness of the proposed method on ILSVRC DET\nand MS COCO datasets, where our algorithm performs better than the\nstate-of-the-arts in various evaluation metrics. It also increases average\nprecision by around 2% in the detection system.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 07:43:27 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Li", "Hongyang", ""], ["Liu", "Yu", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.05729", "submitter": "Shuang Li", "authors": "Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, Xiaogang\n  Wang", "title": "Person Search with Natural Language Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching persons in large-scale image databases with the query of natural\nlanguage description has important applications in video surveillance. Existing\nmethods mainly focused on searching persons with image-based or attribute-based\nqueries, which have major limitations for a practical usage. In this paper, we\nstudy the problem of person search with natural language description. Given the\ntextual description of a person, the algorithm of the person search is required\nto rank all the samples in the person database then retrieve the most relevant\nsample corresponding to the queried description. Since there is no person\ndataset or benchmark with textual description available, we collect a\nlarge-scale person description dataset with detailed natural language\nannotations and person samples from various sources, termed as CUHK Person\nDescription Dataset (CUHK-PEDES). A wide range of possible models and baselines\nhave been evaluated and compared on the person search benchmark. An Recurrent\nNeural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to\nestablish the state-of-the art performance on person search.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 10:01:33 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 07:51:10 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Li", "Shuang", ""], ["Xiao", "Tong", ""], ["Li", "Hongsheng", ""], ["Zhou", "Bolei", ""], ["Yue", "Dayu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.05743", "submitter": "Hantao Yao", "authors": "Hantao Yao, Feng Dai, Dongming Zhang, Yike Ma, Shiliang Zhang,\n  Yongdong Zhang, Qi Tian", "title": "DR2-Net: Deep Residual Reconstruction Network for Image Compressive\n  Sensing", "comments": "Add the code link", "journal-ref": null, "doi": "10.1016/j.neucom.2019.05.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most traditional algorithms for compressive sensing image reconstruction\nsuffer from the intensive computation. Recently, deep learning-based\nreconstruction algorithms have been reported, which dramatically reduce the\ntime complexity than iterative reconstruction algorithms. In this paper, we\npropose a novel \\textbf{D}eep \\textbf{R}esidual \\textbf{R}econstruction Network\n(DR$^{2}$-Net) to reconstruct the image from its Compressively Sensed (CS)\nmeasurement. The DR$^{2}$-Net is proposed based on two observations: 1) linear\nmapping could reconstruct a high-quality preliminary image, and 2) residual\nlearning could further improve the reconstruction quality. Accordingly,\nDR$^{2}$-Net consists of two components, \\emph{i.e.,} linear mapping network\nand residual network, respectively. Specifically, the fully-connected layer in\nneural network implements the linear mapping network. We then expand the linear\nmapping network to DR$^{2}$-Net by adding several residual learning blocks to\nenhance the preliminary image. Extensive experiments demonstrate that the\nDR$^{2}$-Net outperforms traditional iterative methods and recent deep\nlearning-based methods by large margins at measurement rates 0.01, 0.04, 0.1,\nand 0.25, respectively. The code of DR$^{2}$-Net has been released on:\nhttps://github.com/coldrainyht/caffe\\_dr2\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 12:09:32 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 02:23:14 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 02:47:14 GMT"}, {"version": "v4", "created": "Thu, 16 Nov 2017 13:06:18 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Yao", "Hantao", ""], ["Dai", "Feng", ""], ["Zhang", "Dongming", ""], ["Ma", "Yike", ""], ["Zhang", "Shiliang", ""], ["Zhang", "Yongdong", ""], ["Tian", "Qi", ""]]}, {"id": "1702.05747", "submitter": "Geert Litjens", "authors": "Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra\n  Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A.W.M. van der\n  Laak, Bram van Ginneken, Clara I. S\\'anchez", "title": "A Survey on Deep Learning in Medical Image Analysis", "comments": "Revised survey includes expanded discussion section and reworked\n  introductory section on common deep architectures. Added missed papers from\n  before Feb 1st 2017", "journal-ref": "Med Image Anal. (2017) 42:60-88", "doi": "10.1016/j.media.2017.07.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms, in particular convolutional networks, have rapidly\nbecome a methodology of choice for analyzing medical images. This paper reviews\nthe major deep learning concepts pertinent to medical image analysis and\nsummarizes over 300 contributions to the field, most of which appeared in the\nlast year. We survey the use of deep learning for image classification, object\ndetection, segmentation, registration, and other tasks and provide concise\noverviews of studies per application area. Open challenges and directions for\nfuture research are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 13:02:28 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 10:21:55 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Litjens", "Geert", ""], ["Kooi", "Thijs", ""], ["Bejnordi", "Babak Ehteshami", ""], ["Setio", "Arnaud Arindra Adiyoso", ""], ["Ciompi", "Francesco", ""], ["Ghafoorian", "Mohsen", ""], ["van der Laak", "Jeroen A. W. M.", ""], ["van Ginneken", "Bram", ""], ["S\u00e1nchez", "Clara I.", ""]]}, {"id": "1702.05803", "submitter": "Babak Ehteshami Bejnordi", "authors": "Babak Ehteshami Bejnordi, Jimmy Linz, Ben Glass, Maeve Mullooly,\n  Gretchen L Gierach, Mark E Sherman, Nico Karssemeijer, Jeroen van der Laak,\n  Andrew H Beck", "title": "Deep learning-based assessment of tumor-associated stroma for diagnosing\n  breast cancer in histopathology images", "comments": "5 pages, 2 figures, ISBI 2017 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis of breast carcinomas has so far been limited to the morphological\ninterpretation of epithelial cells and the assessment of epithelial tissue\narchitecture. Consequently, most of the automated systems have focused on\ncharacterizing the epithelial regions of the breast to detect cancer. In this\npaper, we propose a system for classification of hematoxylin and eosin (H&E)\nstained breast specimens based on convolutional neural networks that primarily\ntargets the assessment of tumor-associated stroma to diagnose breast cancer\npatients. We evaluate the performance of our proposed system using a large\ncohort containing 646 breast tissue biopsies. Our evaluations show that the\nproposed system achieves an area under ROC of 0.92, demonstrating the\ndiscriminative power of previously neglected tumor-associated stroma as a\ndiagnostic biomarker.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 21:59:43 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Bejnordi", "Babak Ehteshami", ""], ["Linz", "Jimmy", ""], ["Glass", "Ben", ""], ["Mullooly", "Maeve", ""], ["Gierach", "Gretchen L", ""], ["Sherman", "Mark E", ""], ["Karssemeijer", "Nico", ""], ["van der Laak", "Jeroen", ""], ["Beck", "Andrew H", ""]]}, {"id": "1702.05839", "submitter": "Ruimao Zhang", "authors": "Ruimao Zhang, Wei Yang, Zhanglin Peng, Xiaogang Wang, Liang Lin", "title": "Progressively Diffused Networks for Semantic Image Segmentation", "comments": "Sun Yat-sen University, The Chinese University of Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Progressively Diffused Networks (PDNs) for unifying\nmulti-scale context modeling with deep feature learning, by taking semantic\nimage segmentation as an exemplar application. Prior neural networks, such as\nResNet, tend to enhance representational power by increasing the depth of\narchitectures and driving the training objective across layers. However, we\nargue that spatial dependencies in different layers, which generally represent\nthe rich contexts among data elements, are also critical to building deep and\ndiscriminative representations. To this end, our PDNs enables to progressively\nbroadcast information over the learned feature maps by inserting a stack of\ninformation diffusion layers, each of which exploits multi-dimensional\nconvolutional LSTMs (Long-Short-Term Memory Structures). In each LSTM unit, a\nspecial type of atrous filters are designed to capture the short range and long\nrange dependencies from various neighbors to a certain site of the feature map\nand pass the accumulated information to the next layer. From the extensive\nexperiments on semantic image segmentation benchmarks (e.g., ImageNet Parsing,\nPASCAL VOC2012 and PASCAL-Part), our framework demonstrates the effectiveness\nto substantially improve the performances over the popular existing neural\nnetwork models, and achieves state-of-the-art on ImageNet Parsing for large\nscale semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 02:34:26 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Zhang", "Ruimao", ""], ["Yang", "Wei", ""], ["Peng", "Zhanglin", ""], ["Wang", "Xiaogang", ""], ["Lin", "Liang", ""]]}, {"id": "1702.05878", "submitter": "Mengfan Tang", "authors": "Mengfan Tang, Feiping Nie, Siripen Pongpaichet, Ramesh Jain", "title": "From Photo Streams to Evolving Situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photos are becoming spontaneous, objective, and universal sources of\ninformation. This paper develops evolving situation recognition using photo\nstreams coming from disparate sources combined with the advances of deep\nlearning. Using visual concepts in photos together with space and time\ninformation, we formulate the situation detection into a semi-supervised\nlearning framework and propose new graph-based models to solve the problem. To\nextend the method for unknown situations, we introduce a soft label method\nwhich enables the traditional semi-supervised learning framework to accurately\npredict predefined labels as well as effectively form new clusters. To overcome\nthe noisy data which degrades graph quality, leading to poor recognition\nresults, we take advantage of two kinds of noise-robust norms which can\neliminate the adverse effects of outliers in visual concepts and improve the\naccuracy of situation recognition. Finally, we demonstrate the idea and the\neffectiveness of the proposed model on Yahoo Flickr Creative Commons 100\nMillion.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 06:53:21 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Tang", "Mengfan", ""], ["Nie", "Feiping", ""], ["Pongpaichet", "Siripen", ""], ["Jain", "Ramesh", ""]]}, {"id": "1702.05888", "submitter": "Thalaiyasingam Ajanthan", "authors": "Thalaiyasingam Ajanthan, Richard Hartley and Mathieu Salzmann", "title": "Memory Efficient Max Flow for Multi-label Submodular MRFs", "comments": "15 Pages, 13 Figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label submodular Markov Random Fields (MRFs) have been shown to be\nsolvable using max-flow based on an encoding of the labels proposed by\nIshikawa, in which each variable $X_i$ is represented by $\\ell$ nodes (where\n$\\ell$ is the number of labels) arranged in a column. However, this method in\ngeneral requires $2\\,\\ell^2$ edges for each pair of neighbouring variables.\nThis makes it inapplicable to realistic problems with many variables and\nlabels, due to excessive memory requirement. In this paper, we introduce a\nvariant of the max-flow algorithm that requires much less storage.\nConsequently, our algorithm makes it possible to optimally solve multi-label\nsubmodular problems involving large numbers of variables and labels on a\nstandard computer.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 08:09:09 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Hartley", "Richard", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1702.05891", "submitter": "Feng Zhu", "authors": "Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang Wang", "title": "Learning Spatial Regularization with Image-level Supervisions for\n  Multi-label Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image classification is a fundamental but challenging task in\ncomputer vision. Great progress has been achieved by exploiting semantic\nrelations between labels in recent years. However, conventional approaches are\nunable to model the underlying spatial relations between labels in multi-label\nimages, because spatial annotations of the labels are generally not provided.\nIn this paper, we propose a unified deep neural network that exploits both\nsemantic and spatial relations between labels with only image-level\nsupervisions. Given a multi-label image, our proposed Spatial Regularization\nNetwork (SRN) generates attention maps for all labels and captures the\nunderlying relations between them via learnable convolutions. By aggregating\nthe regularized classification results with original results by a ResNet-101\nnetwork, the classification performance can be consistently improved. The whole\ndeep neural network is trained end-to-end with only image-level annotations,\nthus requires no additional efforts on image annotations. Extensive evaluations\non 3 public datasets with different types of labels show that our approach\nsignificantly outperforms state-of-the-arts and has strong generalization\ncapability. Analysis of the learned SRN model demonstrates that it can\neffectively capture both semantic and spatial relations of labels for improving\nclassification performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 08:21:58 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 08:49:43 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Zhu", "Feng", ""], ["Li", "Hongsheng", ""], ["Ouyang", "Wanli", ""], ["Yu", "Nenghai", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.05911", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Oliver Wang, Alexander Sorkine-Hornung, Hendrik\n  P.A. Lensch", "title": "Efficient Large-scale Approximate Nearest Neighbor Search on the GPU", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), pp. 2027 - 2035 (2016)", "doi": "10.1109/CVPR.2016.223", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for efficient approximate nearest neighbor (ANN)\nsearch in high dimensional spaces, extending the idea of Product Quantization.\nWe propose a two-level product and vector quantization tree that reduces the\nnumber of vector comparisons required during tree traversal. Our approach also\nincludes a novel highly parallelizable re-ranking method for candidate vectors\nby efficiently reusing already computed intermediate values. Due to its small\nmemory footprint during traversal, the method lends itself to an efficient,\nparallel GPU implementation. This Product Quantization Tree (PQT) approach\nsignificantly outperforms recent state of the art methods for high dimensional\nnearest neighbor queries on standard reference datasets. Ours is the first work\nthat demonstrates GPU performance superior to CPU performance on high\ndimensional, large scale ANN problems in time-critical real-world applications,\nlike loop-closing in videos.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 09:57:11 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Wang", "Oliver", ""], ["Sorkine-Hornung", "Alexander", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1702.05931", "submitter": "Francesco Ciompi", "authors": "Francesco Ciompi, Oscar Geessink, Babak Ehteshami Bejnordi, Gabriel\n  Silva de Souza, Alexi Baidoshvili, Geert Litjens, Bram van Ginneken, Iris\n  Nagtegaal, Jeroen van der Laak", "title": "The importance of stain normalization in colorectal tissue\n  classification with convolutional networks", "comments": "Published in Proceedings of IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of reliable imaging biomarkers for the analysis of colorectal\ncancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images\nrequires an accurate and reproducible classification of the main tissue\ncomponents in the image. In this paper, we propose a system for CRC tissue\nclassification based on convolutional networks (ConvNets). We investigate the\nimportance of stain normalization in tissue classification of CRC tissue\nsamples in H&E-stained images. Furthermore, we report the performance of\nConvNets on a cohort of rectal cancer samples and on an independent publicly\navailable dataset of colorectal H&E images.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 11:11:50 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 12:34:17 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Ciompi", "Francesco", ""], ["Geessink", "Oscar", ""], ["Bejnordi", "Babak Ehteshami", ""], ["de Souza", "Gabriel Silva", ""], ["Baidoshvili", "Alexi", ""], ["Litjens", "Geert", ""], ["van Ginneken", "Bram", ""], ["Nagtegaal", "Iris", ""], ["van der Laak", "Jeroen", ""]]}, {"id": "1702.05941", "submitter": "Patrick Christ", "authors": "Patrick Ferdinand Christ, Florian Ettlinger, Georgios Kaissis,\n  Sebastian Schlecht, Freba Ahmaddy, Felix Gr\\\"un, Alexander Valentinitsch,\n  Seyed-Ahmad Ahmadi, Rickmer Braren, Bjoern Menze", "title": "SurvivalNet: Predicting patient survival from diffusion weighted\n  magnetic resonance images using cascaded fully convolutional and 3D\n  convolutional neural networks", "comments": "Accepted at IEEE ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic non-invasive assessment of hepatocellular carcinoma (HCC)\nmalignancy has the potential to substantially enhance tumor treatment\nstrategies for HCC patients. In this work we present a novel framework to\nautomatically characterize the malignancy of HCC lesions from DWI images. We\npredict HCC malignancy in two steps: As a first step we automatically segment\nHCC tumor lesions using cascaded fully convolutional neural networks (CFCN). A\n3D neural network (SurvivalNet) then predicts the HCC lesions' malignancy from\nthe HCC tumor segmentation. We formulate this task as a classification problem\nwith classes being \"low risk\" and \"high risk\" represented by longer or shorter\nsurvival times than the median survival. We evaluated our method on DWI of 31\nHCC patients. Our proposed framework achieves an end-to-end accuracy of 65%\nwith a Dice score for the automatic lesion segmentation of 69% and an accuracy\nof 68% for tumor malignancy classification based on expert annotations. We\ncompared the SurvivalNet to classical handcrafted features such as Histogram\nand Haralick and show experimentally that SurvivalNet outperforms the\nhandcrafted features in HCC malignancy classification. End-to-end assessment of\ntumor malignancy based on our proposed fully automatic framework corresponds to\nassessment based on expert annotations with high significance (p>0.95).\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 12:05:30 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Christ", "Patrick Ferdinand", ""], ["Ettlinger", "Florian", ""], ["Kaissis", "Georgios", ""], ["Schlecht", "Sebastian", ""], ["Ahmaddy", "Freba", ""], ["Gr\u00fcn", "Felix", ""], ["Valentinitsch", "Alexander", ""], ["Ahmadi", "Seyed-Ahmad", ""], ["Braren", "Rickmer", ""], ["Menze", "Bjoern", ""]]}, {"id": "1702.05958", "submitter": "Ofer Springer", "authors": "Ofer Springer, Yair Weiss", "title": "Reflection Separation Using Guided Annotation", "comments": "To be presented at ICIP 2017, 6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photographs taken through a glass surface often contain an approximately\nlinear superposition of reflected and transmitted layers. Decomposing an image\ninto these layers is generally an ill-posed task and the use of an additional\nimage prior and user provided cues is presently necessary in order to obtain\ngood results. Current annotation approaches rely on a strong sparsity\nassumption. For images with significant texture this assumption does not\ntypically hold, thus rendering the annotation process unviable. In this paper\nwe show that using a Gaussian Mixture Model patch prior, the correct local\ndecomposition can almost always be found as one of 100 likely modes of the\nposterior. Thus, the user need only choose one of these modes in a sparse set\nof patches and the decomposition may then be completed automatically. We\ndemonstrate the performance of our method using synthesized and real reflection\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:21:20 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 10:45:49 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Springer", "Ofer", ""], ["Weiss", "Yair", ""]]}, {"id": "1702.05970", "submitter": "Patrick Christ", "authors": "Patrick Ferdinand Christ, Florian Ettlinger, Felix Gr\\\"un, Mohamed\n  Ezzeldin A. Elshaera, Jana Lipkova, Sebastian Schlecht, Freba Ahmaddy, Sunil\n  Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Felix Hofmann, Melvin\n  D Anastasi, Seyed-Ahmad Ahmadi, Georgios Kaissis, Julian Holch, Wieland\n  Sommer, Rickmer Braren, Volker Heinemann, Bjoern Menze", "title": "Automatic Liver and Tumor Segmentation of CT and MRI Volumes using\n  Cascaded Fully Convolutional Neural Networks", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of the liver and hepatic lesions is an important step\ntowards deriving quantitative biomarkers for accurate clinical diagnosis and\ncomputer-aided decision support systems. This paper presents a method to\nautomatically segment liver and lesions in CT and MRI abdomen images using\ncascaded fully convolutional neural networks (CFCNs) enabling the segmentation\nof a large-scale medical trial or quantitative image analysis. We train and\ncascade two FCNs for a combined segmentation of the liver and its lesions. In\nthe first step, we train a FCN to segment the liver as ROI input for a second\nFCN. The second FCN solely segments lesions within the predicted liver ROIs of\nstep 1. CFCN models were trained on an abdominal CT dataset comprising 100\nhepatic tumor volumes. Validations on further datasets show that CFCN-based\nsemantic liver and lesion segmentation achieves Dice scores over 94% for liver\nwith computation times below 100s per volume. We further experimentally\ndemonstrate the robustness of the proposed method on an 38 MRI liver tumor\nvolumes and the public 3DIRCAD dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:52:57 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 15:02:59 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Christ", "Patrick Ferdinand", ""], ["Ettlinger", "Florian", ""], ["Gr\u00fcn", "Felix", ""], ["Elshaera", "Mohamed Ezzeldin A.", ""], ["Lipkova", "Jana", ""], ["Schlecht", "Sebastian", ""], ["Ahmaddy", "Freba", ""], ["Tatavarty", "Sunil", ""], ["Bickel", "Marc", ""], ["Bilic", "Patrick", ""], ["Rempfler", "Markus", ""], ["Hofmann", "Felix", ""], ["Anastasi", "Melvin D", ""], ["Ahmadi", "Seyed-Ahmad", ""], ["Kaissis", "Georgios", ""], ["Holch", "Julian", ""], ["Sommer", "Wieland", ""], ["Braren", "Rickmer", ""], ["Heinemann", "Volker", ""], ["Menze", "Bjoern", ""]]}, {"id": "1702.05993", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka, Boris Chidlovski, Stephane Clinchant and Sophia\n  Michel", "title": "An Extended Framework for Marginalized Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extended framework for marginalized domain adaptation, aimed at\naddressing unsupervised, supervised and semi-supervised scenarios. We argue\nthat the denoising principle should be extended to explicitly promote\ndomain-invariant features as well as help the classification task. Therefore we\npropose to jointly learn the data auto-encoders and the target classifiers.\nFirst, in order to make the denoised features domain-invariant, we propose a\ndomain regularization that may be either a domain prediction loss or a maximum\nmean discrepancy between the source and target data. The noise marginalization\nin this case is reduced to solving the linear matrix system $AX=B$ which has a\nclosed-form solution. Second, in order to help the classification, we include a\nclass regularization term. Adding this component reduces the learning problem\nto solving a Sylvester linear matrix equation $AX+BX=C$, for which an efficient\niterative procedure exists as well. We did an extensive study to assess how\nthese regularization terms improve the baseline performance in the three domain\nadaptation scenarios and present experimental results on two image and one text\nbenchmark datasets, conventionally used for validating domain adaptation\nmethods. We report our findings and comparison with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 15:00:13 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Csurka", "Gabriela", ""], ["Chidlovski", "Boris", ""], ["Clinchant", "Stephane", ""], ["Michel", "Sophia", ""]]}, {"id": "1702.06085", "submitter": "Mario Figueiredo", "authors": "Mario A. T. Figueiredo", "title": "Synthesis versus analysis in patch-based image priors", "comments": "To appear in ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In global models/priors (for example, using wavelet frames), there is a well\nknown analysis vs synthesis dichotomy in the way signal/image priors are\nformulated. In patch-based image models/priors, this dichotomy is also present\nin the choice of how each patch is modeled. This paper shows that there is\nanother analysis vs synthesis dichotomy, in terms of how the whole image is\nrelated to the patches, and that all existing patch-based formulations that\nprovide a global image prior belong to the analysis category. We then propose a\nsynthesis formulation, where the image is explicitly modeled as being\nsynthesized by additively combining a collection of independent patches. We\nformally establish that these analysis and synthesis formulations are not\nequivalent in general and that both formulations are compatible with analysis\nand synthesis formulations at the patch level. Finally, we present an instance\nof the alternating direction method of multipliers (ADMM) that can be used to\nperform image denoising under the proposed synthesis formulation, showing its\ncomputational feasibility. Rather than showing the superiority of the synthesis\nor analysis formulations, the contributions of this paper is to establish the\nexistence of both alternatives, thus closing the corresponding gap in the field\nof patch-based image processing.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 17:59:38 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Figueiredo", "Mario A. T.", ""]]}, {"id": "1702.06086", "submitter": "Wei Shen", "authors": "Wei Shen, Kai Zhao, Yilu Guo, Alan Yuille", "title": "Label Distribution Learning Forests", "comments": "Accepted by NIPS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label distribution learning (LDL) is a general learning framework, which\nassigns to an instance a distribution over a set of labels rather than a single\nlabel or multiple labels. Current LDL methods have either restricted\nassumptions on the expression form of the label distribution or limitations in\nrepresentation learning, e.g., to learn deep features in an end-to-end manner.\nThis paper presents label distribution learning forests (LDLFs) - a novel label\ndistribution learning algorithm based on differentiable decision trees, which\nhave several advantages: 1) Decision trees have the potential to model any\ngeneral form of label distributions by a mixture of leaf node predictions. 2)\nThe learning of differentiable decision trees can be combined with\nrepresentation learning. We define a distribution-based loss function for a\nforest, enabling all the trees to be learned jointly, and show that an update\nfunction for leaf node predictions, which guarantees a strict decrease of the\nloss function, can be derived by variational bounding. The effectiveness of the\nproposed LDLFs is verified on several LDL tasks and a computer vision\napplication, showing significant improvements to the state-of-the-art LDL\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 18:04:31 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 19:17:32 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 06:48:22 GMT"}, {"version": "v4", "created": "Mon, 16 Oct 2017 21:05:45 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Shen", "Wei", ""], ["Zhao", "Kai", ""], ["Guo", "Yilu", ""], ["Yuille", "Alan", ""]]}, {"id": "1702.06118", "submitter": "Gal Ness", "authors": "G. Ness, A. Oved, I. Kakon", "title": "Derivative Based Focal Plane Array Nonuniformity Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast and robust method for fixed pattern noise\nnonuniformity correction of infrared focal plane arrays. The proposed method\nrequires neither shutter nor elaborate calibrations and therefore enables a\nreal time correction with no interruptions. Based on derivative estimation of\nthe fixed pattern noise from pixel sized translations of the focal plane array,\nthe proposed method has the advantages of being invariant to the noise\nmagnitude and robust to unknown camera and inter-scene movements while being\nvirtually transparent to the end-user.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 12:21:59 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Ness", "G.", ""], ["Oved", "A.", ""], ["Kakon", "I.", ""]]}, {"id": "1702.06151", "submitter": "Tal Yarkoni", "authors": "Quinten McNamara, Alejandro de la Vega, and Tal Yarkoni", "title": "Developing a comprehensive framework for multimodal feature extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a critical component of many applied data science\nworkflows. In recent years, rapid advances in artificial intelligence and\nmachine learning have led to an explosion of feature extraction tools and\nservices that allow data scientists to cheaply and effectively annotate their\ndata along a vast array of dimensions---ranging from detecting faces in images\nto analyzing the sentiment expressed in coherent text. Unfortunately, the\nproliferation of powerful feature extraction services has been mirrored by a\ncorresponding expansion in the number of distinct interfaces to feature\nextraction services. In a world where nearly every new service has its own API,\ndocumentation, and/or client library, data scientists who need to combine\ndiverse features obtained from multiple sources are often forced to write and\nmaintain ever more elaborate feature extraction pipelines. To address this\nchallenge, we introduce a new open-source framework for comprehensive\nmultimodal feature extraction. Pliers is an open-source Python package that\nsupports standardized annotation of diverse data types (video, images, audio,\nand text), and is expressly with both ease-of-use and extensibility in mind.\nUsers can apply a wide range of pre-existing feature extraction tools to their\ndata in just a few lines of Python code, and can also easily add their own\ncustom extractors by writing modular classes. A graph-based API enables rapid\ndevelopment of complex feature extraction pipelines that output results in a\nsingle, standardized format. We describe the package's architecture, detail its\nmajor advantages over previous feature extraction toolboxes, and use a sample\napplication to a large functional MRI dataset to illustrate how pliers can\nsignificantly reduce the time and effort required to construct sophisticated\nfeature extraction workflows while increasing code clarity and maintainability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 19:22:21 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["McNamara", "Quinten", ""], ["de la Vega", "Alejandro", ""], ["Yarkoni", "Tal", ""]]}, {"id": "1702.06188", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz, Marco A. Contreras, Jun Zhang", "title": "Forest understory trees can be segmented accurately within sufficiently\n  dense airborne laser scanning point clouds", "comments": "arXiv admin note: text overlap with arXiv:1701.00169", "journal-ref": "Scientific Reports, 7(1), 6770 (2017)", "doi": "10.1038/s41598-017-07200-0", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airborne laser scanning (LiDAR) point clouds over large forested areas can be\nprocessed to segment individual trees and subsequently extract tree-level\ninformation. Existing segmentation procedures typically detect more than 90% of\noverstory trees, yet they barely detect 60% of understory trees because of the\nocclusion effect of higher canopy layers. Although understory trees provide\nlimited financial value, they are an essential component of ecosystem\nfunctioning by offering habitat for numerous wildlife species and influencing\nstand development. Here we model the occlusion effect in terms of point\ndensity. We estimate the fractions of points representing different canopy\nlayers (one overstory and multiple understory) and also pinpoint the required\ndensity for reasonable tree segmentation (where accuracy plateaus). We show\nthat at a density of ~170 pt/m-sqr understory trees can likely be segmented as\naccurately as overstory trees. Given the advancements of LiDAR sensor\ntechnology, point clouds will affordably reach this required density. Using\nmodern computational approaches for big data, the denser point clouds can\nefficiently be processed to ultimately allow accurate remote quantification of\nforest resources. The methodology can also be adopted for other similar remote\nsensing or advanced imaging applications such as geological subsurface\nmodelling or biomedical tissue analysis.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:07:53 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 01:32:01 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Hamraz", "Hamid", ""], ["Contreras", "Marco A.", ""], ["Zhang", "Jun", ""]]}, {"id": "1702.06212", "submitter": "Rui Yao", "authors": "Rui Yao, Guosheng Lin, Qinfeng Shi, Damith Ranasinghe", "title": "Efficient Dense Labeling of Human Activity Sequences from Wearables\n  using Fully Convolutional Networks", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human activities in a sequence is a challenging area of research\nin ubiquitous computing. Most approaches use a fixed size sliding window over\nconsecutive samples to extract features---either handcrafted or learned\nfeatures---and predict a single label for all samples in the window. Two key\nproblems emanate from this approach: i) the samples in one window may not\nalways share the same label. Consequently, using one label for all samples\nwithin a window inevitably lead to loss of information; ii) the testing phase\nis constrained by the window size selected during training while the best\nwindow size is difficult to tune in practice. We propose an efficient algorithm\nthat can predict the label of each sample, which we call dense labeling, in a\nsequence of human activities of arbitrary length using a fully convolutional\nnetwork. In particular, our approach overcomes the problems posed by the\nsliding window step. Additionally, our algorithm learns both the features and\nclassifier automatically. We release a new daily activity dataset based on a\nwearable sensor with hospitalized patients. We conduct extensive experiments\nand demonstrate that our proposed approach is able to outperform the\nstate-of-the-arts in terms of classification and label misalignment measures on\nthree challenging datasets: Opportunity, Hand Gesture, and our new dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 23:44:54 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Yao", "Rui", ""], ["Lin", "Guosheng", ""], ["Shi", "Qinfeng", ""], ["Ranasinghe", "Damith", ""]]}, {"id": "1702.06228", "submitter": "Yanwei  Fu", "authors": "Yu-ting Qiang, Yanwei Fu, Xiao Yu, Yanwen Guo, Zhi-Hua Zhou and Leonid\n  Sigal", "title": "Learning to Generate Posters of Scientific Papers by Probabilistic\n  Graphical Models", "comments": "10 pages, submission to IEEE TPAMI. arXiv admin note: text overlap\n  with arXiv:1604.01219", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often summarize their work in the form of scientific posters.\nPosters provide a coherent and efficient way to convey core ideas expressed in\nscientific papers. Generating a good scientific poster, however, is a complex\nand time consuming cognitive task, since such posters need to be readable,\ninformative, and visually aesthetic. In this paper, for the first time, we\nstudy the challenging problem of learning to generate posters from scientific\npapers. To this end, a data-driven framework, that utilizes graphical models,\nis proposed. Specifically, given content to display, the key elements of a good\nposter, including attributes of each panel and arrangements of graphical\nelements are learned and inferred from data. During the inference stage, an MAP\ninference framework is employed to incorporate some design principles. In order\nto bridge the gap between panel attributes and the composition within each\npanel, we also propose a recursive page splitting algorithm to generate the\npanel layout for a poster. To learn and validate our model, we collect and\nrelease a new benchmark dataset, called NJU-Fudan Paper-Poster dataset, which\nconsists of scientific papers and corresponding posters with exhaustively\nlabelled panels and attributes. Qualitative and quantitative results indicate\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:02:56 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Qiang", "Yu-ting", ""], ["Fu", "Yanwei", ""], ["Yu", "Xiao", ""], ["Guo", "Yanwen", ""], ["Zhou", "Zhi-Hua", ""], ["Sigal", "Leonid", ""]]}, {"id": "1702.06257", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Mark Sandler, Andrey Zhmoginov", "title": "The Power of Sparsity in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks are well-known for their high computational and\nmemory demands. Given limited resources, how does one design a network that\nbalances its size, training time, and prediction accuracy? A surprisingly\neffective approach to trade accuracy for size and speed is to simply reduce the\nnumber of channels in each convolutional layer by a fixed fraction and retrain\nthe network. In many cases this leads to significantly smaller networks with\nonly minimal changes to accuracy. In this paper, we take a step further by\nempirically examining a strategy for deactivating connections between filters\nin convolutional layers in a way that allows us to harvest savings both in\nrun-time and memory for many network architectures. More specifically, we\ngeneralize 2D convolution to use a channel-wise sparse connection structure and\nshow that this leads to significantly better results than the baseline approach\nfor large networks including VGG and Inception V3.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 04:10:31 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Sandler", "Mark", ""], ["Zhmoginov", "Andrey", ""]]}, {"id": "1702.06264", "submitter": "Jihua Zhu", "authors": "Rui Guo and Jihua Zhu and Yaochen Li and Dapeng Chen and Zhongyu Li\n  and Yongqin Zhang", "title": "Weighted Motion Averaging for the Registration of Multi-View Range Scans", "comments": "9 pages, 6 figures, 2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-view registration is a fundamental but challenging problem in 3D\nreconstruction and robot vision. Although the original motion averaging\nalgorithm has been introduced as an effective means to solve the multi-view\nregistration problem, it does not consider the reliability and accuracy of each\nrelative motion. Accordingly, this paper proposes a novel motion averaging\nalgorithm for multi-view registration. Firstly, it utilizes the pair-wise\nregistration algorithm to estimate the relative motion and overlapping\npercentage of each scan pair with a certain degree of overlap. With the\noverlapping percentage available, it views the overlapping percentage as the\ncorresponding weight of each scan pair and proposes the weight motion averaging\nalgorithm, which can pay more attention to reliable and accurate relative\nmotions. By treating each relative motion distinctively, more accurate\nregistration can be achieved by applying the weighted motion averaging to\nmulti-view range scans. Experimental results demonstrate the superiority of our\nproposed approach compared with the state-of-the-art methods in terms of\naccuracy, robustness and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 04:53:01 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 01:57:04 GMT"}, {"version": "v3", "created": "Sun, 9 Apr 2017 08:13:09 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Guo", "Rui", ""], ["Zhu", "Jihua", ""], ["Li", "Yaochen", ""], ["Chen", "Dapeng", ""], ["Li", "Zhongyu", ""], ["Zhang", "Yongqin", ""]]}, {"id": "1702.06277", "submitter": "Li Li", "authors": "Li Li, Zhu Li, Madhukar Budagavi, and Houqiang Li", "title": "Projection based advanced motion model for cubic mapping for 360-degree\n  video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel advanced motion model to handle the irregular\nmotion for the cubic map projection of 360-degree video. Since the irregular\nmotion is mainly caused by the projection from the sphere to the cube map, we\nfirst try to project the pixels in both the current picture and reference\npicture from unfolding cube back to the sphere. Then through utilizing the\ncharacteristic that most of the motions in the sphere are uniform, we can\nderive the relationship between the motion vectors of various pixels in the\nunfold cube. The proposed advanced motion model is implemented in the High\nEfficiency Video Coding reference software. Experimental results demonstrate\nthat quite obvious performance improvement can be achieved for the sequences\nwith obvious motions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 06:35:01 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Li", "Li", ""], ["Li", "Zhu", ""], ["Budagavi", "Madhukar", ""], ["Li", "Houqiang", ""]]}, {"id": "1702.06291", "submitter": "Janghoon Choi", "authors": "Janghoon Choi, Junseok Kwon, Kyoung Mu Lee", "title": "Real-time visual tracking by deep reinforced decision making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges of model-free visual tracking problem has been\nthe difficulty originating from the unpredictable and drastic changes in the\nappearance of objects we target to track. Existing methods tackle this problem\nby updating the appearance model on-line in order to adapt to the changes in\nthe appearance. Despite the success of these methods however, inaccurate and\nerroneous updates of the appearance model result in a tracker drift. In this\npaper, we introduce a novel real-time visual tracking algorithm based on a\ntemplate selection strategy constructed by deep reinforcement learning methods.\nThe tracking algorithm utilizes this strategy to choose the appropriate\ntemplate for tracking a given frame. The template selection strategy is\nself-learned by utilizing a simple policy gradient method on numerous training\nepisodes randomly generated from a tracking benchmark dataset. Our proposed\nreinforcement learning framework is generally applicable to other confidence\nmap based tracking algorithms. The experiment shows that our tracking algorithm\nruns in real-time speed of 43 fps and the proposed policy network effectively\ndecides the appropriate template for successful visual tracking.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 08:15:51 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 05:21:47 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Choi", "Janghoon", ""], ["Kwon", "Junseok", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1702.06294", "submitter": "Wei Zhang", "authors": "Wei Zhang, Shengnan Hu, Kan Liu, Zhengjun Zha", "title": "Learning Compact Appearance Representation for Video-based Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for video-based person re-identification\nusing multiple Convolutional Neural Networks (CNNs). Unlike previous work, we\nintend to extract a compact yet discriminative appearance representation from\nseveral frames rather than the whole sequence. Specifically, given a video, the\nrepresentative frames are selected based on the walking profile of consecutive\nframes. A multiple CNN architecture incorporated with feature pooling is\nproposed to learn and compile the features of the selected representative\nframes into a compact description about the pedestrian for identification.\nExperiments are conducted on benchmark datasets to demonstrate the superiority\nof the proposed method over existing person re-identification approaches.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 08:58:28 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 06:40:28 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zhang", "Wei", ""], ["Hu", "Shengnan", ""], ["Liu", "Kan", ""], ["Zha", "Zhengjun", ""]]}, {"id": "1702.06318", "submitter": "Ferda Ofli", "authors": "Ferda Ofli, Yusuf Aytar, Ingmar Weber, Raggi al Hammouri, Antonio\n  Torralba", "title": "Is Saki #delicious? The Food Perception Gap on Instagram and Its\n  Relation to Health", "comments": "This is a pre-print of our paper accepted to appear in the\n  Proceedings of 2017 International World Wide Web Conference (WWW'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food is an integral part of our life and what and how much we eat crucially\naffects our health. Our food choices largely depend on how we perceive certain\ncharacteristics of food, such as whether it is healthy, delicious or if it\nqualifies as a salad. But these perceptions differ from person to person and\none person's \"single lettuce leaf\" might be another person's \"side salad\".\nStudying how food is perceived in relation to what it actually is typically\ninvolves a laboratory setup. Here we propose to use recent advances in image\nrecognition to tackle this problem. Concretely, we use data for 1.9 million\nimages from Instagram from the US to look at systematic differences in how a\nmachine would objectively label an image compared to how a human subjectively\ndoes. We show that this difference, which we call the \"perception gap\", relates\nto a number of health outcomes observed at the county level. To the best of our\nknowledge, this is the first time that image recognition is being used to study\nthe \"misalignment\" of how people describe food images vs. what they actually\ndepict.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 10:36:28 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Ofli", "Ferda", ""], ["Aytar", "Yusuf", ""], ["Weber", "Ingmar", ""], ["Hammouri", "Raggi al", ""], ["Torralba", "Antonio", ""]]}, {"id": "1702.06332", "submitter": "Fabio Maria Carlucci", "authors": "Fabio Maria Carlucci and Lorenzo Porzi and Barbara Caputo and Elisa\n  Ricci and Samuel Rota Bul\\`o", "title": "Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirical fact that classifiers, trained on given data collections,\nperform poorly when tested on data acquired in different settings is\ntheoretically explained in domain adaptation through a shift among\ndistributions of the source and target domains. Alleviating the domain shift\nproblem, especially in the challenging setting where no labeled data are\navailable for the target domain, is paramount for having visual recognition\nsystems working in the wild. As the problem stems from a shift among\ndistributions, intuitively one should try to align them. In the literature,\nthis has resulted in a stream of works attempting to align the feature\nrepresentations learned from the source and target domains. Here we take a\ndifferent route. Rather than introducing regularization terms aiming to promote\nthe alignment of the two representations, we act at the distribution level\nthrough the introduction of \\emph{DomaIn Alignment Layers} (\\DIAL), able to\nmatch the observed source and target data distributions to a reference one.\nThorough experiments on three different public benchmarks we confirm the power\nof our approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 11:23:48 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 12:34:28 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 08:45:08 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Carlucci", "Fabio Maria", ""], ["Porzi", "Lorenzo", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""], ["Bul\u00f2", "Samuel Rota", ""]]}, {"id": "1702.06355", "submitter": "Kai Kang", "authors": "Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui\n  Liu, Xiaogang Wang", "title": "Object Detection in Videos with Tubelet Proposal Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": "10.1109/CVPR.2017.101", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection in videos has drawn increasing attention recently with the\nintroduction of the large-scale ImageNet VID dataset. Different from object\ndetection in static images, temporal information in videos is vital for object\ndetection. To fully utilize temporal information, state-of-the-art methods are\nbased on spatiotemporal tubelets, which are essentially sequences of associated\nbounding boxes across time. However, the existing methods have major\nlimitations in generating tubelets in terms of quality and efficiency.\nMotion-based methods are able to obtain dense tubelets efficiently, but the\nlengths are generally only several frames, which is not optimal for\nincorporating long-term temporal information. Appearance-based methods, usually\ninvolving generic object tracking, could generate long tubelets, but are\nusually computationally expensive. In this work, we propose a framework for\nobject detection in videos, which consists of a novel tubelet proposal network\nto efficiently generate spatiotemporal proposals, and a Long Short-term Memory\n(LSTM) network that incorporates temporal information from tubelet proposals\nfor achieving high object detection accuracy in videos. Experiments on the\nlarge-scale ImageNet VID dataset demonstrate the effectiveness of the proposed\nframework for object detection in videos.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 12:38:11 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 08:39:06 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Kang", "Kai", ""], ["Li", "Hongsheng", ""], ["Xiao", "Tong", ""], ["Ouyang", "Wanli", ""], ["Yan", "Junjie", ""], ["Liu", "Xihui", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.06376", "submitter": "Byungju Kim", "authors": "Byungju Kim, Youngsoo Kim, Yeakang Lee and Junmo Kim", "title": "Mimicking Ensemble Learning with Deep Branched Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a branched residual network for image classification. It\nis known that high-level features of deep neural network are more\nrepresentative than lower-level features. By sharing the low-level features,\nthe network can allocate more memory to high-level features. The upper layers\nof our proposed network are branched, so that it mimics the ensemble learning.\nBy mimicking ensemble learning with single network, we have achieved better\nperformance on ImageNet classification task.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:33:37 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Kim", "Byungju", ""], ["Kim", "Youngsoo", ""], ["Lee", "Yeakang", ""], ["Kim", "Junmo", ""]]}, {"id": "1702.06383", "submitter": "Biswa Sengupta", "authors": "Y Qian and E Vazquez and B Sengupta", "title": "Differential Geometric Retrieval of Deep Features", "comments": "5th ICDM Workshop on High Dimensional Data Mining (HDM 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing images to recommend items from an image-inventory is a subject of\ncontinued interest. Added with the scalability of deep-learning architectures\nthe once `manual' job of hand-crafting features have been largely alleviated,\nand images can be compared according to features generated from a deep\nconvolutional neural network. In this paper, we compare distance metrics (and\ndivergences) to rank features generated from a neural network, for\ncontent-based image retrieval. Specifically, after modelling individual images\nusing approximations of mixture models or sparse covariance estimators, we\nresort to their information-theoretic and Riemann geometric comparisons. We\nshow that using approximations of mixture models enable us to compute a\ndistance measure based on the Wasserstein metric that requires less effort than\nother computationally intensive optimal transport plans; finally, an affine\ninvariant metric is used to compare the optimal transport metric to its Riemann\ngeometric counterpart -- we conclude that although expensive, retrieval metric\nbased on Wasserstein geometry is more suitable than information theoretic\ncomparison of images. In short, we combine GPU scalability in learning deep\nfeature vectors with statistically efficient metrics that we foresee being\nutilised in a commercial setting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:55:47 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:01:35 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Qian", "Y", ""], ["Vazquez", "E", ""], ["Sengupta", "B", ""]]}, {"id": "1702.06392", "submitter": "Yixing Li", "authors": "Yixing Li, Zichuan Liu, Kai Xu, Hao Yu and Fengbo Ren", "title": "A GPU-Outperforming FPGA Accelerator Architecture for Binary\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA-based hardware accelerators for convolutional neural networks (CNNs)\nhave obtained great attentions due to their higher energy efficiency than GPUs.\nHowever, it is challenging for FPGA-based solutions to achieve a higher\nthroughput than GPU counterparts. In this paper, we demonstrate that FPGA\nacceleration can be a superior solution in terms of both throughput and energy\nefficiency when a CNN is trained with binary constraints on weights and\nactivations. Specifically, we propose an optimized FPGA accelerator\narchitecture tailored for bitwise convolution and normalization that features\nmassive spatial parallelism with deep pipelines stages. A key advantage of the\nFPGA accelerator is that its performance is insensitive to data batch size,\nwhile the performance of GPU acceleration varies largely depending on the batch\nsize of the data. Experiment results show that the proposed accelerator\narchitecture for binary CNNs running on a Virtex-7 FPGA is 8.3x faster and 75x\nmore energy-efficient than a Titan X GPU for processing online individual\nrequests in small batch sizes. For processing static data in large batch sizes,\nthe proposed solution is on a par with a Titan X GPU in terms of throughput\nwhile delivering 9.5x higher energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 05:21:34 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 16:09:55 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Li", "Yixing", ""], ["Liu", "Zichuan", ""], ["Xu", "Kai", ""], ["Yu", "Hao", ""], ["Ren", "Fengbo", ""]]}, {"id": "1702.06397", "submitter": "Siheng Chen", "authors": "Siheng Chen and Dong Tian and Chen Feng and Anthony Vetro and Jelena\n  Kova\\v{c}evi\\'c", "title": "Fast Resampling of 3D Point Clouds via Graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2771730", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce cost in storing, processing and visualizing a large-scale point\ncloud, we consider a randomized resampling strategy to select a representative\nsubset of points while preserving application-dependent features. The proposed\nstrategy is based on graphs, which can represent underlying surfaces and lend\nthemselves well to efficient computation. We use a general feature-extraction\noperator to represent application-dependent features and propose a general\nreconstruction error to evaluate the quality of resampling. We obtain a general\nform of optimal resampling distribution by minimizing the reconstruction error.\nThe proposed optimal resampling distribution is guaranteed to be shift,\nrotation and scale-invariant in the 3D space. We next specify the\nfeature-extraction operator to be a graph filter and study specific resampling\nstrategies based on all-pass, low-pass, high-pass graph filtering and graph\nfilter banks. We finally apply the proposed methods to three applications:\nlarge-scale visualization, accurate registration and robust shape modeling. The\nempirical performance validates the effectiveness and efficiency of the\nproposed resampling methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 15:31:24 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chen", "Siheng", ""], ["Tian", "Dong", ""], ["Feng", "Chen", ""], ["Vetro", "Anthony", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1702.06408", "submitter": "Vikram Venkatraghavan", "authors": "Vikram Venkatraghavan, Esther Bron, Wiro Niessen, Stefan Klein", "title": "A Discriminative Event Based Model for Alzheimer's Disease Progression\n  Modeling", "comments": "Information Processing in Medical Imaging (IPMI), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The event-based model (EBM) for data-driven disease progression modeling\nestimates the sequence in which biomarkers for a disease become abnormal. This\nhelps in understanding the dynamics of disease progression and facilitates\nearly diagnosis by staging patients on a disease progression timeline. Existing\nEBM methods are all generative in nature. In this work we propose a novel\ndiscriminative approach to EBM, which is shown to be more accurate as well as\ncomputationally more efficient than existing state-of-the art EBM methods. The\nmethod first estimates for each subject an approximate ordering of events, by\nranking the posterior probabilities of individual biomarkers being abnormal.\nSubsequently, the central ordering over all subjects is estimated by fitting a\ngeneralized Mallows model to these approximate subject-specific orderings based\non a novel probabilistic Kendall's Tau distance. To evaluate the accuracy, we\nperformed extensive experiments on synthetic data simulating the progression of\nAlzheimer's disease. Subsequently, the method was applied to the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) data to estimate the central event\nordering in the dataset. The experiments benchmark the accuracy of the new\nmodel under various conditions and compare it with existing state-of-the-art\nEBM methods. The results indicate that discriminative EBM could be a simple and\nelegant approach to disease progression modeling.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 14:41:15 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Venkatraghavan", "Vikram", ""], ["Bron", "Esther", ""], ["Niessen", "Wiro", ""], ["Klein", "Stefan", ""]]}, {"id": "1702.06441", "submitter": "Jakub Sochor", "authors": "Jakub Sochor, Roman Jur\\'anek, Jakub \\v{S}pa\\v{n}hel, Luk\\'a\\v{s}\n  Mar\\v{s}\\'ik, Adam \\v{S}irok\\'y, Adam Herout, Pavel Zem\\v{c}\\'ik", "title": "Comprehensive Data Set for Automatic Single Camera Visual Speed\n  Measurement", "comments": null, "journal-ref": "IEEE Transactions on Intelligent Transportation Systems 2018", "doi": "10.1109/TITS.2018.2825609", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on traffic camera calibration and a visual speed\nmeasurement from a single monocular camera, which is an important task of\nvisual traffic surveillance. Existing methods addressing this problem are\ndifficult to compare due to a lack of a common data set with reliable ground\ntruth. Therefore, it is not clear how the methods compare in various aspects\nand what factors are affecting their performance. We captured a new data set of\n18 full-HD videos, each around 1 hr long, captured at six different locations.\nVehicles in the videos (20865 instances in total) are annotated with the\nprecise speed measurements from optical gates using LiDAR and verified with\nseveral reference GPS tracks. We made the data set available for download and\nit contains the videos and metadata (calibration, lengths of features in image,\nannotations, and so on) for future comparison and evaluation. Camera\ncalibration is the most crucial part of the speed measurement; therefore, we\nprovide a brief overview of the methods and analyze a recently published method\nfor fully automatic camera calibration and vehicle speed measurement and report\nthe results on this data set in detail.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 15:34:20 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 07:51:55 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Sochor", "Jakub", ""], ["Jur\u00e1nek", "Roman", ""], ["\u0160pa\u0148hel", "Jakub", ""], ["Mar\u0161\u00edk", "Luk\u00e1\u0161", ""], ["\u0160irok\u00fd", "Adam", ""], ["Herout", "Adam", ""], ["Zem\u010d\u00edk", "Pavel", ""]]}, {"id": "1702.06451", "submitter": "Jakub Sochor", "authors": "Jakub Sochor, Roman Jur\\'anek, Adam Herout", "title": "Traffic Surveillance Camera Calibration by 3D Model Bounding Box\n  Alignment for Accurate Vehicle Speed Measurement", "comments": "accepted to CVIU", "journal-ref": null, "doi": "10.1016/j.cviu.2017.05.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on fully automatic traffic surveillance camera\ncalibration, which we use for speed measurement of passing vehicles. We improve\nover a recent state-of-the-art camera calibration method for traffic\nsurveillance based on two detected vanishing points. More importantly, we\npropose a novel automatic scene scale inference method. The method is based on\nmatching bounding boxes of rendered 3D models of vehicles with detected\nbounding boxes in the image. The proposed method can be used from arbitrary\nviewpoints, since it has no constraints on camera placement. We evaluate our\nmethod on the recent comprehensive dataset for speed measurement BrnoCompSpeed.\nExperiments show that our automatic camera calibration method by detection of\ntwo vanishing points reduces error by 50% (mean distance ratio error reduced\nfrom 0.18 to 0.09) compared to the previous state-of-the-art method. We also\nshow that our scene scale inference method is more precise, outperforming both\nstate-of-the-art automatic calibration method for speed measurement (error\nreduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error\nreduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results\nof the proposed automatic camera calibration method on video sequences obtained\nfrom real surveillance cameras in various places, and under different lighting\nconditions (night, dawn, day).\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 15:50:10 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 11:57:49 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Sochor", "Jakub", ""], ["Jur\u00e1nek", "Roman", ""], ["Herout", "Adam", ""]]}, {"id": "1702.06456", "submitter": "Yanis Bahroun", "authors": "Yanis Bahroun, Andrea Soltoggio", "title": "Online Representation Learning with Single and Multi-layer Hebbian\n  Networks for Image Classification", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning permits the development of algorithms that are able to\nadapt to a variety of different data sets using the same underlying rules\nthanks to the autonomous discovery of discriminating features during training.\nRecently, a new class of Hebbian-like and local unsupervised learning rules for\nneural networks have been developed that minimise a similarity matching\ncost-function. These have been shown to perform sparse representation learning.\nThis study tests the effectiveness of one such learning rule for learning\nfeatures from images. The rule implemented is derived from a nonnegative\nclassical multidimensional scaling cost-function, and is applied to both single\nand multi-layer architectures. The features learned by the algorithm are then\nused as input to an SVM to test their effectiveness in classification on the\nestablished CIFAR-10 image dataset. The algorithm performs well in comparison\nto other unsupervised learning algorithms and multi-layer networks, thus\nsuggesting its validity in the design of a new class of compact, online\nlearning networks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:01:28 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 12:51:08 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 12:03:51 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Bahroun", "Yanis", ""], ["Soltoggio", "Andrea", ""]]}, {"id": "1702.06461", "submitter": "Dmitrij Schlesinger", "authors": "Dmitrij Schlesinger and Florian Jug and Gene Myers and Carsten Rother\n  and Dagmar Kainm\\\"uller", "title": "Crowd Sourcing Image Segmentation with iaSTAPLE", "comments": "Accepted to ISBI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel label fusion technique as well as a crowdsourcing protocol\nto efficiently obtain accurate epithelial cell segmentations from non-expert\ncrowd workers. Our label fusion technique simultaneously estimates the true\nsegmentation, the performance levels of individual crowd workers, and an image\nsegmentation model in the form of a pairwise Markov random field. We term our\napproach image-aware STAPLE (iaSTAPLE) since our image segmentation model\nseamlessly integrates into the well-known and widely used STAPLE approach. In\nan evaluation on a light microscopy dataset containing more than 5000 membrane\nlabeled epithelial cells of a fly wing, we show that iaSTAPLE outperforms\nSTAPLE in terms of segmentation accuracy as well as in terms of the accuracy of\nestimated crowd worker performance levels, and is able to correctly segment 99%\nof all cells when compared to expert segmentations. These results show that\niaSTAPLE is a highly useful tool for crowd sourcing image segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:12:18 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Schlesinger", "Dmitrij", ""], ["Jug", "Florian", ""], ["Myers", "Gene", ""], ["Rother", "Carsten", ""], ["Kainm\u00fcller", "Dagmar", ""]]}, {"id": "1702.06506", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, Deva Ramanan", "title": "PixelNet: Representation of the pixels, by the pixels, and for the\n  pixels", "comments": "Project Page: http://www.cs.cmu.edu/~aayushb/pixelNet/. arXiv admin\n  note: substantial text overlap with arXiv:1609.06694", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore design principles for general pixel-level prediction problems,\nfrom low-level edge detection to mid-level surface normal estimation to\nhigh-level semantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that stratified sampling of pixels allows one to (1) add diversity\nduring batch updates, speeding up learning; (2) explore complex nonlinear\npredictors, improving accuracy; and (3) efficiently train state-of-the-art\nmodels tabula rasa (i.e., \"from scratch\") for diverse pixel-labeling tasks. Our\nsingle architecture produces state-of-the-art results for semantic segmentation\non PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,\nand edge detection on BSDS.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:20:30 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Bansal", "Aayush", ""], ["Chen", "Xinlei", ""], ["Russell", "Bryan", ""], ["Gupta", "Abhinav", ""], ["Ramanan", "Deva", ""]]}, {"id": "1702.06521", "submitter": "Ronald Clark", "authors": "Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, Hongkai Wen", "title": "VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization", "comments": "To appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques, namely convolutional neural networks (CNN) and\nregression forests, have recently shown great promise in performing 6-DoF\nlocalization of monocular images. However, in most cases image-sequences,\nrather only single images, are readily available. To this extent, none of the\nproposed learning-based approaches exploit the valuable constraint of temporal\nsmoothness, often leading to situations where the per-frame error is larger\nthan the camera motion. In this paper we propose a recurrent model for\nperforming 6-DoF localization of video-clips. We find that, even by considering\nonly short sequences (20 frames), the pose estimates are smoothed and the\nlocalization error can be drastically reduced. Finally, we consider means of\nobtaining probabilistic pose estimates from our model. We evaluate our method\non openly-available real-world autonomous driving and indoor localization\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:49:26 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 10:18:08 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Clark", "Ronald", ""], ["Wang", "Sen", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""], ["Wen", "Hongkai", ""]]}, {"id": "1702.06619", "submitter": "Ganghun Kim", "authors": "Ganghun Kim, Kyle Isaacson, Racheal Palmer, Rajesh Menon", "title": "Lensless Photography with only an image sensor", "comments": null, "journal-ref": null, "doi": "10.1364/ao.56.006450", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photography usually requires optics in conjunction with a recording device\n(an image sensor). Eliminating the optics could lead to new form factors for\ncameras. Here, we report a simple demonstration of imaging using a bare CMOS\nsensor that utilizes computation. The technique relies on the space variant\npoint-spread functions resulting from the interaction of a point source in the\nfield of view with the image sensor. These space-variant point-spread functions\nare combined with a reconstruction algorithm in order to image simple objects\ndisplayed on a discrete LED array as well as on an LCD screen. We extended the\napproach to video imaging at the native frame rate of the sensor. Finally, we\nperformed experiments to analyze the parametric impact of the object distance.\nImproving the sensor designs and reconstruction algorithms can lead to useful\ncameras without optics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 23:20:54 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Kim", "Ganghun", ""], ["Isaacson", "Kyle", ""], ["Palmer", "Racheal", ""], ["Menon", "Rajesh", ""]]}, {"id": "1702.06674", "submitter": "Yun Cao", "authors": "Yun Cao, Zhiming Zhou, Weinan Zhang, Yong Yu", "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorization of grayscale images has been a hot topic in computer vision.\nPrevious research mainly focuses on producing a colored image to match the\noriginal one. However, since many colors share the same gray value, an input\ngrayscale image could be diversely colored while maintaining its reality. In\nthis paper, we design a novel solution for unsupervised diverse colorization.\nSpecifically, we leverage conditional generative adversarial networks to model\nthe distribution of real-world item colors, in which we develop a fully\nconvolutional generator with multi-layer noise to enhance diversity, with\nmulti-layer condition concatenation to maintain reality, and with stride 1 to\nkeep spatial information. With such a novel network architecture, the model\nyields highly competitive performance on the open LSUN bedroom dataset. The\nTuring test of 80 humans further indicates our generated color schemes are\nhighly convincible.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 04:34:31 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 10:57:03 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Cao", "Yun", ""], ["Zhou", "Zhiming", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1702.06683", "submitter": "Timnit Gebru", "authors": "Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez\n  Lieberman Aiden, Li Fei-Fei", "title": "Using Deep Learning and Google Street View to Estimate the Demographic\n  Makeup of the US", "comments": "41 pages including supplementary material. Under review at PNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States spends more than $1B each year on initiatives such as the\nAmerican Community Survey (ACS), a labor-intensive door-to-door study that\nmeasures statistics relating to race, gender, education, occupation,\nunemployment, and other demographic factors. Although a comprehensive source of\ndata, the lag between demographic changes and their appearance in the ACS can\nexceed half a decade. As digital imagery becomes ubiquitous and machine vision\ntechniques improve, automated data analysis may provide a cheaper and faster\nalternative. Here, we present a method that determines socioeconomic trends\nfrom 50 million images of street scenes, gathered in 200 American cities by\nGoogle Street View cars. Using deep learning-based computer vision techniques,\nwe determined the make, model, and year of all motor vehicles encountered in\nparticular neighborhoods. Data from this census of motor vehicles, which\nenumerated 22M automobiles in total (8% of all automobiles in the US), was used\nto accurately estimate income, race, education, and voting patterns, with\nsingle-precinct resolution. (The average US precinct contains approximately\n1000 people.) The resulting associations are surprisingly simple and powerful.\nFor instance, if the number of sedans encountered during a 15-minute drive\nthrough a city is higher than the number of pickup trucks, the city is likely\nto vote for a Democrat during the next Presidential election (88% chance);\notherwise, it is likely to vote Republican (82%). Our results suggest that\nautomated systems for monitoring demographic trends may effectively complement\nlabor-intensive approaches, with the potential to detect trends with fine\nspatial resolution, in close to real time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 06:20:13 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 05:11:11 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Gebru", "Timnit", ""], ["Krause", "Jonathan", ""], ["Wang", "Yilun", ""], ["Chen", "Duyun", ""], ["Deng", "Jia", ""], ["Aiden", "Erez Lieberman", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1702.06700", "submitter": "Yuetan Lin", "authors": "Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang", "title": "Task-driven Visual Saliency and Attention-based Visual Question\n  Answering", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) has witnessed great progress since May, 2015\nas a classic problem unifying visual and textual data into a system. Many\nenlightening VQA works explore deep into the image and question encodings and\nfusing methods, of which attention is the most effective and infusive\nmechanism. Current attention based methods focus on adequate fusion of visual\nand textual features, but lack the attention to where people focus to ask\nquestions about the image. Traditional attention based methods attach a single\nvalue to the feature at each spatial location, which losses many useful\ninformation. To remedy these problems, we propose a general method to perform\nsaliency-like pre-selection on overlapped region features by the interrelation\nof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication\nbased attention method to capture more competent correlation information\nbetween visual and textual features. We conduct experiments on the large-scale\nCOCO-VQA dataset and analyze the effectiveness of our model demonstrated by\nstrong empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 08:19:38 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lin", "Yuetan", ""], ["Pang", "Zhangyang", ""], ["Wang", "Donghui", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1702.06722", "submitter": "Canggih Puspo Wibowo", "authors": "Adityo Priyandito Utomo, Canggih Puspo Wibowo", "title": "3D Reconstruction of Temples in the Special Region of Yogyakarta By\n  Using Close-Range Photogrammetry", "comments": "Semnasteknomedia 2017, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object reconstruction is one of the main problems in cultural heritage\npreservation. This problem is due to lack of data in documentation. Thus in\nthis research we presented a method of 3D reconstruction using close-range\nphotogrammetry. We collected 1319 photos from five temples in Yogyakarta. Using\nA-KAZE algorithm, keypoints of each image were obtained. Then we employed LIOP\nto create feature descriptor from it. After performing feature matching, L1RA\nwas utilized to create sparse point clouds. In order to generate the geometry\nshape, MVS was used. Finally, FSSR and Large Scale Texturing were employed to\ndeal with the surface and texture of the object. The quality of the\nreconstructed 3D model was measured by comparing the 3D images of the model\nwith the original photos utilizing SSIM. The results showed that in terms of\nquality, our method was on par with other commercial method such as\nPhotoModeler and PhotoScan.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 09:32:09 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Utomo", "Adityo Priyandito", ""], ["Wibowo", "Canggih Puspo", ""]]}, {"id": "1702.06767", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Shijie Qiu, Youyong Kong, Yang Chen, Lotfi Senhadji,\n  Huazhong Shu", "title": "MomentsNet: a simple learning-free method for binary image recognition", "comments": "5 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new simple and learning-free deep learning\nnetwork named MomentsNet, whose convolution layer, nonlinear processing layer\nand pooling layer are constructed by Moments kernels, binary hashing and\nblock-wise histogram, respectively. Twelve typical moments (including\ngeometrical moment, Zernike moment, Tchebichef moment, etc.) are used to\nconstruct the MomentsNet whose recognition performance for binary image is\nstudied. The results reveal that MomentsNet has better recognition performance\nthan its corresponding moments in almost all cases and ZernikeNet achieves the\nbest recognition performance among MomentsNet constructed by twelve moments.\nZernikeNet also shows better recognition performance on binary image database\nthan that of PCANet, which is a learning-based deep learning network.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 12:08:09 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Wu", "Jiasong", ""], ["Qiu", "Shijie", ""], ["Kong", "Youyong", ""], ["Chen", "Yang", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1702.06799", "submitter": "Alptekin Temizel", "authors": "Fatih Ozkan, Mehmet Ali Arabaci, Elif Surer, Alptekin Temizel", "title": "Boosted Multiple Kernel Learning for First-Person Activity Recognition", "comments": "First published in the Proceedings of the 25th European Signal\n  Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition from first-person (ego-centric) videos has recently\ngained attention due to the increasing ubiquity of the wearable cameras. There\nhas been a surge of efforts adapting existing feature descriptors and designing\nnew descriptors for the first-person videos. An effective activity recognition\nsystem requires selection and use of complementary features and appropriate\nkernels for each feature. In this study, we propose a data-driven framework for\nfirst-person activity recognition which effectively selects and combines\nfeatures and their respective kernels during the training. Our experimental\nresults show that use of Multiple Kernel Learning (MKL) and Boosted MKL in\nfirst-person activity recognition problem exhibits improved results in\ncomparison to the state-of-the-art. In addition, these techniques enable the\nexpansion of the framework with new features in an efficient and convenient\nway.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 13:56:54 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 08:23:41 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ozkan", "Fatih", ""], ["Arabaci", "Mehmet Ali", ""], ["Surer", "Elif", ""], ["Temizel", "Alptekin", ""]]}, {"id": "1702.06813", "submitter": "Julian Ryde", "authors": "Julian Ryde, Xuchu (Dennis) Ding", "title": "RenderMap: Exploiting the Link Between Perception and Rendering for\n  Dense Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for the real-time (2Hz) creation of a dense map and\nalignment of a moving robotic agent within that map by rendering using a\nGraphics Processing Unit (GPU). This is done by recasting the scan alignment\npart of the dense mapping process as a rendering task. Alignment errors are\ncomputed from rendering the scene, comparing with range data from the sensors,\nand minimized by an optimizer. The proposed approach takes advantage of the\nadvances in rendering techniques for computer graphics and GPU hardware to\naccelerate the algorithm. Moreover, it allows one to exploit information not\nused in classic dense mapping algorithms such as Iterative Closest Point (ICP)\nby rendering interfaces between the free space, occupied space and the unknown.\nThe proposed approach leverages directly the rendering capabilities of the GPU,\nin contrast to other GPU-based approaches that deploy the GPU as a general\npurpose parallel computation platform.\n  We argue that the proposed concept is a general consequence of treating\nperception problems as inverse problems of rendering. Many perception problems\ncan be recast into a form where much of the computation is replaced by render\noperations. This is not only efficient since rendering is fast, but also\nsimpler to implement and will naturally benefit from future advancements in GPU\nspeed and rendering techniques. Furthermore, this general concept can go beyond\naddressing perception problems and can be used for other problem domains such\nas path planning.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 00:12:32 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ryde", "Julian", "", "Dennis"], ["Xuchu", "", "", "Dennis"], ["Ding", "", ""]]}, {"id": "1702.06850", "submitter": "Jobin Wilson", "authors": "Jobin Wilson and Muhammad Arif", "title": "Scene Recognition by Combining Local and Global Image Descriptors", "comments": "A full implementation of our model is available at\n  https://github.com/flytxtds/scene-recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is an important problem in computer vision, having diverse\napplications. In this work, we construct an end-to-end scene recognition\npipeline consisting of feature extraction, encoding, pooling and\nclassification. Our approach simultaneously utilize global feature descriptors\nas well as local feature descriptors from images, to form a hybrid feature\ndescriptor corresponding to each image. We utilize DAISY features associated\nwith key points within images as our local feature descriptor and histogram of\noriented gradients (HOG) corresponding to an entire image as a global\ndescriptor. We make use of a bag-of-visual-words encoding and apply Mini- Batch\nK-Means algorithm to reduce the complexity of our feature encoding scheme. A\n2-level pooling procedure is used to combine DAISY and HOG features\ncorresponding to each image. Finally, we experiment with a multi-class SVM\nclassifier with several kernels, in a cross-validation setting, and tabulate\nour results on the fifteen scene categories dataset. The average accuracy of\nour model was 76.4% in the case of a 40%-60% random split of images into\ntraining and testing datasets respectively. The primary objective of this work\nis to clearly outline the practical implementation of a basic\nscrene-recognition pipeline having a reasonable accuracy, in python, using\nopen-source libraries. A full implementation of the proposed model is available\nin our github repository.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 06:57:37 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Wilson", "Jobin", ""], ["Arif", "Muhammad", ""]]}, {"id": "1702.06890", "submitter": "Hongyang Li", "authors": "Yu Liu and Hongyang Li and Xiaogang Wang", "title": "Learning Deep Features via Congenerous Cosine Loss for Person\n  Recognition", "comments": "Post-rebuttal update. Add some comparison results; correct some\n  technical part; rewrite some sections to make it more readable; code link\n  available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person recognition aims at recognizing the same identity across time and\nspace with complicated scenes and similar appearance. In this paper, we propose\na novel method to address this task by training a network to obtain robust and\nrepresentative features. The intuition is that we directly compare and optimize\nthe cosine distance between two features - enlarging inter-class distinction as\nwell as alleviating inner-class variance. We propose a congenerous cosine loss\nby minimizing the cosine distance between samples and their cluster centroid in\na cooperative way. Such a design reduces the complexity and could be\nimplemented via softmax with normalized inputs. Our method also differs from\nprevious work in person recognition that we do not conduct a second training on\nthe test subset. The identity of a person is determined by measuring the\nsimilarity from several body regions in the reference set. Experimental results\nshow that the proposed approach achieves better classification accuracy against\nprevious state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:45:48 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 17:27:50 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Liu", "Yu", ""], ["Li", "Hongyang", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.06925", "submitter": "Xiang Xiang", "authors": "Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter,\n  Gregory D. Hager, Harry Quon, Jian Cheng, Alan L. Yuille", "title": "Regularizing Face Verification Nets For Pain Intensity Regression", "comments": "5 pages, 3 figure; Camera-ready version to appear at IEEE ICIP 2017", "journal-ref": null, "doi": "10.13140/RG.2.2.20841.49765", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited labeled data are available for the research of estimating facial\nexpression intensities. For instance, the ability to train deep networks for\nautomated pain assessment is limited by small datasets with labels of\npatient-reported pain intensities. Fortunately, fine-tuning from a\ndata-extensive pre-trained domain, such as face verification, can alleviate\nthis problem. In this paper, we propose a network that fine-tunes a\nstate-of-the-art face verification network using a regularized regression loss\nand additional data with expression labels. In this way, the expression\nintensity regression task can benefit from the rich feature representations\ntrained on a huge amount of data for face verification. The proposed\nregularized deep regressor is applied to estimate the pain expression intensity\nand verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achieving\nthe state-of-the-art performance. A weighted evaluation metric is also proposed\nto address the imbalance issue of different pain intensities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:15:42 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:58:58 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 17:49:56 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Wang", "Feng", ""], ["Xiang", "Xiang", ""], ["Liu", "Chang", ""], ["Tran", "Trac D.", ""], ["Reiter", "Austin", ""], ["Hager", "Gregory D.", ""], ["Quon", "Harry", ""], ["Cheng", "Jian", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.07006", "submitter": "Christina Funke", "authors": "Christina M. Funke, Leon A. Gatys, Alexander S. Ecker, Matthias Bethge", "title": "Synthesising Dynamic Textures using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present a parametric model for dynamic textures. The model is based\non spatiotemporal summary statistics computed from the feature representations\nof a Convolutional Neural Network (CNN) trained on object recognition. We\ndemonstrate how the model can be used to synthesise new samples of dynamic\ntextures and to predict motion in simple movies.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:03:49 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Funke", "Christina M.", ""], ["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1702.07019", "submitter": "Qingsong Yang", "authors": "Qingsong Yang, Pingkun Yan, Mannudeep K. Kalra, and Ge Wang", "title": "CT Image Denoising with Perceptive Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing use of CT in modern medical practice has raised concerns over\nassociated radiation dose. Reduction of radiation dose associated with CT can\nincrease noise and artifacts, which can adversely affect diagnostic confidence.\nDenoising of low-dose CT images on the other hand can help improve diagnostic\nconfidence, which however is a challenging problem due to its ill-posed nature,\nsince one noisy image patch may correspond to many different output patches. In\nthe past decade, machine learning based approaches have made quite impressive\nprogress in this direction. However, most of those methods, including the\nrecently popularized deep learning techniques, aim for minimizing\nmean-squared-error (MSE) between a denoised CT image and the ground truth,\nwhich results in losing important structural details due to over-smoothing,\nalthough the PSNR based performance measure looks great. In this work, we\nintroduce a new perceptual similarity measure as the objective function for a\ndeep convolutional neural network to facilitate CT image denoising. Instead of\ndirectly computing MSE for pixel-to-pixel intensity loss, we compare the\nperceptual features of a denoised output against those of the ground truth in a\nfeature space. Therefore, our proposed method is capable of not only reducing\nthe image noise levels, but also keeping the critical structural information at\nthe same time. Promising results have been obtained in our experiments with a\nlarge number of CT images.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:50:55 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Yang", "Qingsong", ""], ["Yan", "Pingkun", ""], ["Kalra", "Mannudeep K.", ""], ["Wang", "Ge", ""]]}, {"id": "1702.07025", "submitter": "Cristina Vasconcelos", "authors": "Cristina Nader Vasconcelos, B\\'arbara Nader Vasconcelos", "title": "Convolutional Neural Network Committees for Melanoma Classification with\n  Classical And Expert Knowledge Based Image Transforms Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Skin cancer is a major public health problem, as is the most common type of\ncancer and represents more than half of cancer diagnoses worldwide. Early\ndetection influences the outcome of the disease and motivates our work. We\ninvestigate the composition of CNN committees and data augmentation for the the\nISBI 2017 Melanoma Classification Challenge (named Skin Lesion Analysis towards\nMelanoma Detection) facing the peculiarities of dealing with such a small,\nunbalanced, biological database. For that, we explore committees of\nConvolutional Neural Networks trained over the ISBI challenge training dataset\nartificially augmented by both classical image processing transforms and image\nwarping guided by specialist knowledge about the lesion axis and improve the\nfinal classifier invariance to common melanoma variations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 22:17:13 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 11:50:58 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Vasconcelos", "Cristina Nader", ""], ["Vasconcelos", "B\u00e1rbara Nader", ""]]}, {"id": "1702.07054", "submitter": "Wanli Ouyang", "authors": "Wanli Ouyang, Ku Wang, Xin Zhu, Xiaogang Wang", "title": "Learning Chained Deep Features and Classifiers for Cascade in Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascade is a widely used approach that rejects obvious negative samples at\nearly stages for learning better classifier and faster inference. This paper\npresents chained cascade network (CC-Net). In this CC-Net, the cascaded\nclassifier at a stage is aided by the classification scores in previous stages.\nFeature chaining is further proposed so that the feature learning for the\ncurrent cascade stage uses the features in previous stages as the prior\ninformation. The chained ConvNet features and classifiers of multiple stages\nare jointly learned in an end-to-end network. In this way, features and\nclassifiers at latter stages handle more difficult samples with the help of\nfeatures and classifiers in previous stages. It yields consistent boost in\ndetection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined\nwith better region proposal, CC-Net leads to state-of-the-art result of 81.1%\nmAP on PASCAL VOC 2007.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 00:40:29 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Ouyang", "Wanli", ""], ["Wang", "Ku", ""], ["Zhu", "Xin", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.07059", "submitter": "Neslisah Torosdagli", "authors": "Neslisah Torosdagli, Denise K. Liberton, Payal Verma, Murat Sincan\n  Janice Lee, Sumanta Pattanaik, Ulas Bagci", "title": "Robust and fully automated segmentation of mandible from CT scans", "comments": "4 pages, 5 figures, IEEE International Symposium on Biomedical\n  Imaging (ISBI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mandible bone segmentation from computed tomography (CT) scans is challenging\ndue to mandible's structural irregularities, complex shape patterns, and lack\nof contrast in joints. Furthermore, connections of teeth to mandible and\nmandible to remaining parts of the skull make it extremely difficult to\nidentify mandible boundary automatically. This study addresses these challenges\nby proposing a novel framework where we define the segmentation as two\ncomplementary tasks: recognition and delineation. For recognition, we use\nrandom forest regression to localize mandible in 3D. For delineation, we\npropose to use 3D gradient-based fuzzy connectedness (FC) image segmentation\nalgorithm, operating on the recognized mandible sub-volume. Despite heavy CT\nartifacts and dental fillings, consisting half of the CT image data in our\nexperiments, we have achieved highly accurate detection and delineation\nresults. Specifically, detection accuracy more than 96% (measured by union of\nintersection (UoI)), the delineation accuracy of 91% (measured by dice\nsimilarity coefficient), and less than 1 mm in shape mismatch (Hausdorff\nDistance) were found.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 01:23:45 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Torosdagli", "Neslisah", ""], ["Liberton", "Denise K.", ""], ["Verma", "Payal", ""], ["Lee", "Murat Sincan Janice", ""], ["Pattanaik", "Sumanta", ""], ["Bagci", "Ulas", ""]]}, {"id": "1702.07189", "submitter": "David Malmgren-Hansen Mr.", "authors": "David Malmgren-Hansen, Allan Aasbjerg Nielsen and Rasmus Engholm", "title": "Analyzing Learned Convnet Features with Dirichlet Process Gaussian\n  Mixture Models", "comments": "Presented at NIPS 2016 Workshop: Practical Bayesian Nonparametrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (Convnets) have achieved good results in a\nrange of computer vision tasks the recent years. Though given a lot of\nattention, visualizing the learned representations to interpret Convnets, still\nremains a challenging task. The high dimensionality of internal representations\nand the high abstractions of deep layers are the main challenges when\nvisualizing Convnet functionality. We present in this paper a technique based\non clustering internal Convnet representations with a Dirichlet Process\nGaussian Mixture Model, for visualization of learned representations in\nConvnets. Our method copes with the high dimensionality of a Convnet by\nclustering representations across all nodes of each layer. We will discuss how\nthis application is useful when considering transfer learning, i.e.\\\ntransferring a model trained on one dataset to solve a task on a different one.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:11:42 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Malmgren-Hansen", "David", ""], ["Nielsen", "Allan Aasbjerg", ""], ["Engholm", "Rasmus", ""]]}, {"id": "1702.07191", "submitter": "Yikang Li", "authors": "Yikang Li, Wanli Ouyang, Xiaogang Wang, Xiao'ou Tang", "title": "ViP-CNN: Visual Phrase Guided Convolutional Neural Network", "comments": "10 pages, 5 figures, accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the intermediate level task connecting image captioning and object\ndetection, visual relationship detection started to catch researchers'\nattention because of its descriptive power and clear structure. It detects the\nobjects and captures their pair-wise interactions with a\nsubject-predicate-object triplet, e.g. person-ride-horse. In this paper, each\nvisual relationship is considered as a phrase with three components. We\nformulate the visual relationship detection as three inter-connected\nrecognition problems and propose a Visual Phrase guided Convolutional Neural\nNetwork (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a\nPhrase-guided Message Passing Structure (PMPS) to establish the connection\namong relationship components and help the model consider the three problems\njointly. Corresponding non-maximum suppression method and model training\nstrategy are also proposed. Experimental results show that our ViP-CNN\noutperforms the state-of-art method both in speed and accuracy. We further\npretrain ViP-CNN on our cleansed Visual Genome Relationship dataset, which is\nfound to perform better than the pretraining on the ImageNet for this task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:28:18 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 16:45:53 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Li", "Yikang", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiao'ou", ""]]}, {"id": "1702.07333", "submitter": "David Alvarez", "authors": "David Alvarez, Monica Iglesias", "title": "k-Means Clustering and Ensemble of Regressions: An Algorithm for the\n  ISIC 2017 Skin Lesion Segmentation Challenge", "comments": "Abstract submitted to arXiv as prerequisite to participate in the\n  ISIC2017 Skin Lesion Segmentation Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This abstract briefly describes a segmentation algorithm developed for the\nISIC 2017 Skin Lesion Detection Competition hosted at [ref]. The objective of\nthe competition is to perform a segmentation (in the form of a binary mask\nimage) of skin lesions in dermoscopic images as close as possible to a\nsegmentation performed by trained clinicians, which is taken as ground truth.\nThis project only takes part in the segmentation phase of the challenge. The\nother phases of the competition (feature extraction and lesion identification)\nare not considered.\n  The proposed algorithm consists of 4 steps: (1) lesion image preprocessing,\n(2) image segmentation using k-means clustering of pixel colors, (3)\ncalculation of a set of features describing the properties of each segmented\nregion, and (4) calculation of a final score for each region, representing the\nlikelihood of corresponding to a suitable lesion segmentation. The scores in\nstep (4) are obtained by averaging the results of 2 different regression models\nusing the scores of each region as input. Before using the algorithm these\nregression models must be trained using the training set of images and ground\ntruth masks provided by the Competition. Steps 2 to 4 are repeated with an\nincreasing number of clusters (and therefore the image is segmented into more\nregions) until there is no further improvement of the calculated scores.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 18:43:30 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Alvarez", "David", ""], ["Iglesias", "Monica", ""]]}, {"id": "1702.07343", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "Improving high-pass fusion method using wavelets", "comments": "7 pages, in Persian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an appropriate image fusion method, spatial information of the\npanchromatic image is injected into the multispectral images such that the\nspectral information is not distorted. The high-pass modulation method is a\nsuccessful method in image fusion. However, the main drawback of this method is\nthat this technique uses the boxcar filter to extract the high frequency\ninformation of the panchromatic image. Using the boxcar filter introduces the\nringing effect into the fused image. To cope with this problem, we use the\nwavelet transform instead of boxcar filters. Then, the results of the proposed\nmethod and those of other methods such as, Brovey, IHS, and PCA ones are\ncompared. Experiments show the superiority of the proposed method in terms of\ncorrelation coefficient and mutual information.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:09:46 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1702.07371", "submitter": "Sunil Kumar", "authors": "Tanu Srivastava, Raj Shree Singh, Sunil Kumar, Pavan Chakraborty", "title": "Feasibility of Principal Component Analysis in hand gesture recognition\n  system", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays actions are increasingly being handled in electronic ways, instead\nof physical interaction. From earlier times biometrics is used in the\nauthentication of a person. It recognizes a person by using a human trait\nassociated with it like eyes (by calculating the distance between the eyes) and\nusing hand gestures, fingerprint detection, face detection etc. Advantages of\nusing these traits for identification are that they uniquely identify a person\nand cannot be forgotten or lost. These are unique features of a human being\nwhich are being used widely to make the human life simpler. Hand gesture\nrecognition system is a powerful tool that supports efficient interaction\nbetween the user and the computer. The main moto of hand gesture recognition\nresearch is to create a system which can recognise specific hand gestures and\nuse them to convey useful information for device control. This paper presents\nan experimental study over the feasibility of principal component analysis in\nhand gesture recognition system. PCA is a powerful tool for analyzing data. The\nprimary goal of PCA is dimensionality reduction. Frames are extracted from the\nSheffield KInect Gesture (SKIG) dataset. The implementation is done by creating\na training set and then training the recognizer. It uses Eigen space by\nprocessing the eigenvalues and eigenvectors of the images in training set.\nEuclidean distance with the threshold value is used as similarity metric to\nrecognize the gestures. The experimental results show that PCA is feasible to\nbe used for hand gesture recognition system.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 19:34:25 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Srivastava", "Tanu", ""], ["Singh", "Raj Shree", ""], ["Kumar", "Sunil", ""], ["Chakraborty", "Pavan", ""]]}, {"id": "1702.07386", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar, David Budden, Alexander Matveev, Heather Berlin,\n  Hayk Saribekyan, Yaron Meirovitch and Nir Shavit", "title": "Toward Streaming Synapse Detection with Compositional ConvNets", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectomics is an emerging field in neuroscience that aims to reconstruct\nthe 3-dimensional morphology of neurons from electron microscopy (EM) images.\nRecent studies have successfully demonstrated the use of convolutional neural\nnetworks (ConvNets) for segmenting cell membranes to individuate neurons.\nHowever, there has been comparatively little success in high-throughput\nidentification of the intercellular synaptic connections required for deriving\nconnectivity graphs.\n  In this study, we take a compositional approach to segmenting synapses,\nmodeling them explicitly as an intercellular cleft co-located with an\nasymmetric vesicle density along a cell membrane. Instead of requiring a deep\nnetwork to learn all natural combinations of this compositionality, we train\nlighter networks to model the simpler marginal distributions of membranes,\nclefts and vesicles from just 100 electron microscopy samples. These feature\nmaps are then combined with simple rules-based heuristics derived from prior\nbiological knowledge.\n  Our approach to synapse detection is both more accurate than previous\nstate-of-the-art (7% higher recall and 5% higher F1-score) and yields a 20-fold\nspeed-up compared to the previous fastest implementations. We demonstrate by\nreconstructing the first complete, directed connectome from the largest\navailable anisotropic microscopy dataset (245 GB) of mouse somatosensory cortex\n(S1) in just 9.7 hours on a single shared-memory CPU system. We believe that\nthis work marks an important step toward the goal of a microscope-pace\nstreaming connectomics pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 20:48:13 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Santurkar", "Shibani", ""], ["Budden", "David", ""], ["Matveev", "Alexander", ""], ["Berlin", "Heather", ""], ["Saribekyan", "Hayk", ""], ["Meirovitch", "Yaron", ""], ["Shavit", "Nir", ""]]}, {"id": "1702.07389", "submitter": "Elias Mueggler", "authors": "Elias Mueggler, Guillermo Gallego, Henri Rebecq, Davide Scaramuzza", "title": "Continuous-Time Visual-Inertial Odometry for Event Cameras", "comments": "15 pages, 12 figures, 6 tables, IEEE Transactions on Robotics, 2018", "journal-ref": "IEEE Transactions on Robotics, Vol. 34, No. 6, pp.1425-1440, Dec.\n  2018", "doi": "10.1109/TRO.2018.2858287", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired vision sensors that output pixel-level\nbrightness changes instead of standard intensity frames. They offer significant\nadvantages over standard cameras, namely a very high dynamic range, no motion\nblur, and a latency in the order of microseconds. However, due to the\nfundamentally different structure of the sensor's output, new algorithms that\nexploit the high temporal resolution and the asynchronous nature of the sensor\nare required. Recent work has shown that a continuous-time representation of\nthe event camera pose can deal with the high temporal resolution and\nasynchronous nature of this sensor in a principled way. In this paper, we\nleverage such a continuous-time representation to perform visual-inertial\nodometry with an event camera. This representation allows direct integration of\nthe asynchronous events with micro-second accuracy and the inertial\nmeasurements at high frequency. The event camera trajectory is approximated by\na smooth curve in the space of rigid-body motions using cubic splines. This\nformulation significantly reduces the number of variables in trajectory\nestimation problems. We evaluate our method on real data from several scenes\nand compare the results against ground truth from a motion-capture system. We\nshow that our method provides improved accuracy over the result of a\nstate-of-the-art visual odometry method for event cameras. We also show that\nboth the map orientation and scale can be recovered accurately by fusing events\nand inertial data. To the best of our knowledge, this is the first work on\nvisual-inertial fusion with event cameras using a continuous-time framework.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 20:56:09 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 17:42:56 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Mueggler", "Elias", ""], ["Gallego", "Guillermo", ""], ["Rebecq", "Henri", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1702.07392", "submitter": "Jie Li", "authors": "Jie Li, Katherine A. Skinner, Ryan M. Eustice and Matthew\n  Johnson-Roberson", "title": "WaterGAN: Unsupervised Generative Network to Enable Real-time Color\n  Correction of Monocular Underwater Images", "comments": "8 pages, 16 figures, published by RA-letter 2018. Source code\n  available at: https://github.com/kskin/WaterGAN", "journal-ref": "IEEE Robotics and Automation Letters IEEE Robotics and Automation\n  Letters IEEE Robotics and Automation Letters 387 - 394 (2018)", "doi": "10.1109/LRA.2017.2730363", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on WaterGAN, a generative adversarial network (GAN) for\ngenerating realistic underwater images from in-air image and depth pairings in\nan unsupervised pipeline used for color correction of monocular underwater\nimages. Cameras onboard autonomous and remotely operated vehicles can capture\nhigh resolution images to map the seafloor, however, underwater image formation\nis subject to the complex process of light propagation through the water\ncolumn. The raw images retrieved are characteristically different than images\ntaken in air due to effects such as absorption and scattering, which cause\nattenuation of light at different rates for different wavelengths. While this\nphysical process is well described theoretically, the model depends on many\nparameters intrinsic to the water column as well as the objects in the scene.\nThese factors make recovery of these parameters difficult without simplifying\nassumptions or field calibration, hence, restoration of underwater images is a\nnon-trivial problem. Deep learning has demonstrated great success in modeling\ncomplex nonlinear systems but requires a large amount of training data, which\nis difficult to compile in deep sea environments. Using WaterGAN, we generate a\nlarge training dataset of paired imagery, both raw underwater and true color\nin-air, as well as depth data. This data serves as input to a novel end-to-end\nnetwork for color correction of monocular underwater images. Due to the\ndepth-dependent water column effects inherent to underwater environments, we\nshow that our end-to-end network implicitly learns a coarse depth estimate of\nthe underwater scene from monocular underwater images. Our proposed pipeline is\nvalidated with testing on real data collected from both a pure water tank and\nfrom underwater surveys in field testing. Source code is made publicly\navailable with sample datasets and pretrained models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:06:51 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 20:18:25 GMT"}, {"version": "v3", "created": "Thu, 26 Oct 2017 14:46:14 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Li", "Jie", ""], ["Skinner", "Katherine A.", ""], ["Eustice", "Ryan M.", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1702.07424", "submitter": "Andriy Miranskyy", "authors": "Domenic Curro, Konstantinos G. Derpanis, Andriy V. Miranskyy", "title": "Building Usage Profiles Using Deep Neural Nets", "comments": null, "journal-ref": "Proceedings of the 39th International Conference on Software\n  Engineering: New Ideas and Emerging Results Track (ICSE-NIER '17). IEEE\n  Press, Piscataway, NJ, USA, 43-46, 2017", "doi": "10.1109/ICSE-NIER.2017.12", "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve software quality, one needs to build test scenarios resembling the\nusage of a software product in the field. This task is rendered challenging\nwhen a product's customer base is large and diverse. In this scenario, existing\nprofiling approaches, such as operational profiling, are difficult to apply. In\nthis work, we consider publicly available video tutorials of a product to\nprofile usage. Our goal is to construct an automatic approach to extract\ninformation about user actions from instructional videos. To achieve this goal,\nwe use a Deep Convolutional Neural Network (DCNN) to recognize user actions.\nOur pilot study shows that a DCNN trained to recognize user actions in video\ncan classify five different actions in a collection of 236 publicly available\nMicrosoft Word tutorial videos (published on YouTube). In our empirical\nevaluation we report a mean average precision of 94.42% across all actions.\nThis study demonstrates the efficacy of DCNN-based methods for extracting\nsoftware usage information from videos. Moreover, this approach may aid in\nother software engineering activities that require information about customer\nusage of a product.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 23:39:21 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Curro", "Domenic", ""], ["Derpanis", "Konstantinos G.", ""], ["Miranskyy", "Andriy V.", ""]]}, {"id": "1702.07432", "submitter": "Wei Yang", "authors": "Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, Xiaogang\n  Wang", "title": "Multi-Context Attention for Human Pose Estimation", "comments": "The first two authors contribute equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to incorporate convolutional neural networks with a\nmulti-context attention mechanism into an end-to-end framework for human pose\nestimation. We adopt stacked hourglass networks to generate attention maps from\nfeatures at multiple resolutions with various semantics. The Conditional Random\nField (CRF) is utilized to model the correlations among neighboring regions in\nthe attention map. We further combine the holistic attention model, which\nfocuses on the global consistency of the full human body, and the body part\nattention model, which focuses on the detailed description for different body\nparts. Hence our model has the ability to focus on different granularity from\nlocal salient regions to global semantic-consistent spaces. Additionally, we\ndesign novel Hourglass Residual Units (HRUs) to increase the receptive field of\nthe network. These units are extensions of residual units with a side branch\nincorporating filters with larger receptive fields, hence features with various\nscales are learned and combined within the HRUs. The effectiveness of the\nproposed multi-context attention mechanism and the hourglass residual units is\nevaluated on two widely used human pose estimation benchmarks. Our approach\noutperforms all existing methods on both benchmarks over all the body parts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 01:10:53 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Chu", "Xiao", ""], ["Yang", "Wei", ""], ["Ouyang", "Wanli", ""], ["Ma", "Cheng", ""], ["Yuille", "Alan L.", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.07451", "submitter": "Patrick Wang", "authors": "Patrick Wang and Kenneth Morton and Peter Torrione and Leslie Collins", "title": "Viewpoint Adaptation for Rigid Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An object detector performs suboptimally when applied to image data taken\nfrom a viewpoint different from the one with which it was trained. In this\npaper, we present a viewpoint adaptation algorithm that allows a trained\nsingle-view object detector to be adapted to a new, distinct viewpoint. We\nfirst illustrate how a feature space transformation can be inferred from a\nknown homography between the source and target viewpoints. Second, we show that\na variety of trained classifiers can be modified to behave as if that\ntransformation were applied to each testing instance. The proposed algorithm is\nevaluated on a person detection task using images from the PETS 2007 and CAVIAR\ndatasets, as well as from a new synthetic multi-view person detection dataset.\nIt yields substantial performance improvements when adapting single-view person\ndetectors to new viewpoints, and simultaneously reduces computational\ncomplexity. This work has the potential to improve detection performance for\ncameras viewing objects from arbitrary viewpoints, while simplifying data\ncollection and feature extraction.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 02:37:15 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Wang", "Patrick", ""], ["Morton", "Kenneth", ""], ["Torrione", "Peter", ""], ["Collins", "Leslie", ""]]}, {"id": "1702.07472", "submitter": "Yunjin Chen", "authors": "Peng Qiao, Yong Dou, Wensen Feng and Yunjin Chen", "title": "Learning Non-local Image Diffusion for Image Denoising", "comments": "under review in a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image diffusion plays a fundamental role for the task of image denoising.\nRecently proposed trainable nonlinear reaction diffusion (TNRD) model defines a\nsimple but very effective framework for image denoising. However, as the TNRD\nmodel is a local model, the diffusion behavior of which is purely controlled by\ninformation of local patches, it is prone to create artifacts in the homogenous\nregions and over-smooth highly textured regions, especially in the case of\nstrong noise levels. Meanwhile, it is widely known that the non-local\nself-similarity (NSS) prior stands as an effective image prior for image\ndenoising, which has been widely exploited in many non-local methods. In this\nwork, we are highly motivated to embed the NSS prior into the TNRD model to\ntackle its weaknesses. In order to preserve the expected property that\nend-to-end training is available, we exploit the NSS prior by a set of\nnon-local filters, and derive our proposed trainable non-local reaction\ndiffusion (TNLRD) model for image denoising. Together with the local filters\nand influence functions, the non-local filters are learned by employing\nloss-specific training. The experimental results show that the trained TNLRD\nmodel produces visually plausible recovered images with more textures and less\nartifacts, compared to its local versions. Moreover, the trained TNLRD model\ncan achieve strongly competitive performance to recent state-of-the-art image\ndenoising methods in terms of peak signal-to-noise ratio (PSNR) and structural\nsimilarity index (SSIM).\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 06:12:55 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Qiao", "Peng", ""], ["Dou", "Yong", ""], ["Feng", "Wensen", ""], ["Chen", "Yunjin", ""]]}, {"id": "1702.07474", "submitter": "Fei Han", "authors": "Fei Han, Xue Yang, Christopher Reardon, Yu Zhang, Hao Zhang", "title": "Simultaneous Feature and Body-Part Learning for Real-Time Robot\n  Awareness of Human Behaviors", "comments": "8 pages, 6 figures, accepted by ICRA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot awareness of human actions is an essential research problem in robotics\nwith many important real-world applications, including human-robot\ncollaboration and teaming. Over the past few years, depth sensors have become a\nstandard device widely used by intelligent robots for 3D perception, which can\nalso offer human skeletal data in 3D space. Several methods based on skeletal\ndata were designed to enable robot awareness of human actions with satisfactory\naccuracy. However, previous methods treated all body parts and features equally\nimportant, without the capability to identify discriminative body parts and\nfeatures. In this paper, we propose a novel simultaneous Feature And Body-part\nLearning (FABL) approach that simultaneously identifies discriminative body\nparts and features, and efficiently integrates all available information\ntogether to enable real-time robot awareness of human behaviors. We formulate\nFABL as a regression-like optimization problem with structured\nsparsity-inducing norms to model interrelationships of body parts and features.\nWe also develop an optimization algorithm to solve the formulated problem,\nwhich possesses a theoretical guarantee to find the optimal solution. To\nevaluate FABL, three experiments were performed using public benchmark\ndatasets, including the MSR Action3D and CAD-60 datasets, as well as a Baxter\nrobot in practical assistive living applications. Experimental results show\nthat our FABL approach obtains a high recognition accuracy with a processing\nspeed of the order-of-magnitude of 10e4 Hz, which makes FABL a promising method\nto enable real-time robot awareness of human behaviors in practical robotics\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 06:35:10 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Han", "Fei", ""], ["Yang", "Xue", ""], ["Reardon", "Christopher", ""], ["Zhang", "Yu", ""], ["Zhang", "Hao", ""]]}, {"id": "1702.07475", "submitter": "Fei Han", "authors": "Fei Han, Xue Yang, Yu Zhang, Hao Zhang", "title": "Sequence-based Multimodal Apprenticeship Learning For Robot Perception\n  and Decision Making", "comments": "8 pages, 6 figures, accepted by ICRA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apprenticeship learning has recently attracted a wide attention due to its\ncapability of allowing robots to learn physical tasks directly from\ndemonstrations provided by human experts. Most previous techniques assumed that\nthe state space is known a priori or employed simple state representations that\nusually suffer from perceptual aliasing. Different from previous research, we\npropose a novel approach named Sequence-based Multimodal Apprenticeship\nLearning (SMAL), which is capable to simultaneously fusing temporal information\nand multimodal data, and to integrate robot perception with decision making. To\nevaluate the SMAL approach, experiments are performed using both simulations\nand real-world robots in the challenging search and rescue scenarios. The\nempirical study has validated that our SMAL approach can effectively learn\nplans for robots to make decisions using sequence of multimodal observations.\nExperimental results have also showed that SMAL outperforms the baseline\nmethods using individual images.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 06:37:06 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Han", "Fei", ""], ["Yang", "Xue", ""], ["Zhang", "Yu", ""], ["Zhang", "Hao", ""]]}, {"id": "1702.07482", "submitter": "Yunjin Chen", "authors": "Wensen Feng, Yunjin Chen", "title": "Speckle Reduction with Trained Nonlinear Diffusion Filtering", "comments": "to appear in Journal of Mathematical Imaging and Vision. Demo codes\n  are available from https://1drv.ms/u/s!ApXF85Oq1kvqgcscP8GqUvPE-dF7ig", "journal-ref": null, "doi": "10.1007/s10851-016-0697-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speckle reduction is a prerequisite for many image processing tasks in\nsynthetic aperture radar (SAR) images, as well as all coherent images. In\nrecent years, predominant state-of-the-art approaches for despeckling are\nusually based on nonlocal methods which mainly concentrate on achieving utmost\nimage restoration quality, with relatively low computational efficiency.\nTherefore, in this study we aim to propose an efficient despeckling model with\nboth high computational efficiency and high recovery quality. To this end, we\nexploit a newly-developed trainable nonlinear reaction diffusion(TNRD)\nframework which has proven a simple and effective model for various image\nrestoration problems. {In the original TNRD applications, the diffusion network\nis usually derived based on the direct gradient descent scheme. However, this\napproach will encounter some problem for the task of multiplicative noise\nreduction exploited in this study. To solve this problem, we employed a new\narchitecture derived from the proximal gradient descent method.} {Taking into\naccount the speckle noise statistics, the diffusion process for the despeckling\ntask is derived. We then retrain all the model parameters in the presence of\nspeckle noise. Finally, optimized nonlinear diffusion filtering models are\nobtained, which are specialized for despeckling with various noise levels.\nExperimental results substantiate that the trained filtering models provide\ncomparable or even better results than state-of-the-art nonlocal approaches.\nMeanwhile, our proposed model merely contains convolution of linear filters\nwith an image, which offers high level parallelism on GPUs. As a consequence,\nfor images of size $512 \\times 512$, our GPU implementation takes less than 0.1\nseconds to produce state-of-the-art despeckling performance.}\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 07:34:31 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Feng", "Wensen", ""], ["Chen", "Yunjin", ""]]}, {"id": "1702.07486", "submitter": "Judith B\\\"utepage", "authors": "Judith B\\\"utepage, Michael Black, Danica Kragic, Hedvig Kjellstr\\\"om", "title": "Deep representation learning for human motion prediction and\n  classification", "comments": "This paper is published at the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models of 3D human motion are often restricted to a small number\nof activities and can therefore not generalize well to novel movements or\napplications. In this work we propose a deep learning framework for human\nmotion capture data that learns a generic representation from a large corpus of\nmotion capture data and generalizes well to new, unseen, motions. Using an\nencoding-decoding network that learns to predict future 3D poses from the most\nrecent past, we extract a feature representation of human motion. Most work on\ndeep learning for sequence prediction focuses on video and speech. Since\nskeletal data has a different structure, we present and evaluate different\nnetwork architectures that make different assumptions about time dependencies\nand limb correlations. To quantify the learned features, we use the output of\ndifferent layers for action classification and visualize the receptive fields\nof the network units. Our method outperforms the recent state of the art in\nskeletal motion prediction even though these use action specific training data.\nOur results show that deep feedforward networks, trained from a generic mocap\ndatabase, can successfully be used for feature extraction from human motion\ndata and that this representation can be used as a foundation for\nclassification and prediction.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 07:51:29 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 07:06:34 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["B\u00fctepage", "Judith", ""], ["Black", "Michael", ""], ["Kragic", "Danica", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1702.07492", "submitter": "Ahmed Qureshi", "authors": "Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi\n  Ishiguro", "title": "Robot gains Social Intelligence through Multimodal Deep Reinforcement\n  Learning", "comments": "The paper is published in IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to coexist with humans in a social world like ours, it is crucial\nthat they possess human-like social interaction skills. Programming a robot to\npossess such skills is a challenging task. In this paper, we propose a\nMultimodal Deep Q-Network (MDQN) to enable a robot to learn human-like\ninteraction skills through a trial and error method. This paper aims to develop\na robot that gathers data during its interaction with a human and learns human\ninteraction behaviour from the high-dimensional sensory information using\nend-to-end reinforcement learning. This paper demonstrates that the robot was\nable to learn basic interaction skills successfully, after 14 days of\ninteracting with people.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 08:30:43 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Qureshi", "Ahmed Hussain", ""], ["Nakamura", "Yutaka", ""], ["Yoshikawa", "Yuichiro", ""], ["Ishiguro", "Hiroshi", ""]]}, {"id": "1702.07508", "submitter": "Lianwen Jin", "authors": "Songxuan Lai, Lianwen Jin, Weixin Yang", "title": "Toward high-performance online HCCR: a CNN approach with DropDistortion,\n  path signature and spatial stochastic max-pooling", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.patrec.2017.02.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an investigation of several techniques that increase the\naccuracy of online handwritten Chinese character recognition (HCCR). We propose\na new training strategy named DropDistortion to train a deep convolutional\nneural network (DCNN) with distorted samples. DropDistortion gradually lowers\nthe degree of character distortion during training, which allows the DCNN to\nbetter generalize. Path signature is used to extract effective features for\nonline characters. Further improvement is achieved by employing spatial\nstochastic max-pooling as a method of feature map distortion and model\naveraging. Experiments were carried out on three publicly available datasets,\nnamely CASIA-OLHWDB 1.0, CASIA-OLHWDB 1.1, and the ICDAR2013 online HCCR\ncompetition dataset. The proposed techniques yield state-of-the-art recognition\naccuracies of 97.67%, 97.30%, and 97.99%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 09:26:15 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Lai", "Songxuan", ""], ["Jin", "Lianwen", ""], ["Yang", "Weixin", ""]]}, {"id": "1702.07600", "submitter": "Klaas Kelchtermans", "authors": "Klaas Kelchtermans and Tinne Tuytelaars", "title": "How hard is it to cross the room? -- Training (Recurrent) Neural\n  Networks to steer a UAV", "comments": "12 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the feasibility of steering a drone with a (recurrent)\nneural network, based on input from a forward looking camera, in the context of\na high-level navigation task. We set up a generic framework for training a\nnetwork to perform navigation tasks based on imitation learning. It can be\napplied to both aerial and land vehicles. As a proof of concept we apply it to\na UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a\nroom containing a number of obstacles. So far only feedforward neural networks\n(FNNs) have been used to train UAV control. To cope with more complex tasks, we\npropose the use of recurrent neural networks (RNN) instead and successfully\ntrain an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision\nbased control is a sequential prediction problem, known for its highly\ncorrelated input data. The correlation makes training a network hard,\nespecially an RNN. To overcome this issue, we investigate an alternative\nsampling method during training, namely window-wise truncated backpropagation\nthrough time (WW-TBPTT). Further, end-to-end training requires a lot of data\nwhich often is not available. Therefore, we compare the performance of\nretraining only the Fully Connected (FC) and LSTM control layers with networks\nwhich are trained end-to-end. Performing the relatively simple task of crossing\na room already reveals important guidelines and good practices for training\nneural control networks. Different visualizations help to explain the behavior\nlearned.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 14:29:35 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Kelchtermans", "Klaas", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1702.07611", "submitter": "Amy Tabb", "authors": "Amy Tabb and Henry Medeiros", "title": "Automatic segmentation of trees in dynamic outdoor environments", "comments": "14 pages", "journal-ref": "Computers in Industry 98, 90-99. 2018", "doi": "10.1016/j.compind.2018.03.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation in dynamic outdoor environments can be difficult when the\nillumination levels and other aspects of the scene cannot be controlled.\nSpecifically in orchard and vineyard automation contexts, a background material\nis often used to shield a camera's field of view from other rows of crops. In\nthis paper, we describe a method that uses superpixels to determine low texture\nregions of the image that correspond to the background material, and then show\nhow this information can be integrated with the color distribution of the image\nto compute optimal segmentation parameters to segment objects of interest.\nQuantitative and qualitative experiments demonstrate the suitability of this\napproach for dynamic outdoor environments, specifically for tree reconstruction\nand apple flower detection applications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 14:46:55 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 16:48:32 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 17:13:11 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Tabb", "Amy", ""], ["Medeiros", "Henry", ""]]}, {"id": "1702.07619", "submitter": "Amy Tabb", "authors": "Amy Tabb and Henry Medeiros", "title": "Fast and robust curve skeletonization for real-world elongated objects", "comments": "47 pages; IEEE WACV 2018, main paper and supplementary material", "journal-ref": null, "doi": "10.1109/WACV.2018.00214", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of extracting curve skeletons of three-dimensional,\nelongated objects given a noisy surface, which has applications in agricultural\ncontexts such as extracting the branching structure of plants. We describe an\nefficient and robust method based on breadth-first search that can determine\ncurve skeletons in these contexts. Our approach is capable of automatically\ndetecting junction points as well as spurious segments and loops. All of that\nis accomplished with only one user-adjustable parameter. The run time of our\nmethod ranges from hundreds of milliseconds to less than four seconds on large,\nchallenging datasets, which makes it appropriate for situations where real-time\ndecision making is needed. Experiments on synthetic models as well as on data\nfrom real world objects, some of which were collected in challenging field\nconditions, show that our approach compares favorably to classical thinning\nalgorithms as well as to recent contributions to the field.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 15:01:22 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:03:31 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 17:35:54 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2018 14:44:28 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Tabb", "Amy", ""], ["Medeiros", "Henry", ""]]}, {"id": "1702.07630", "submitter": "Yannick Deville", "authors": "Charlotte Revel, Yannick Deville, V\\'eronique Achard, Xavier Briottet", "title": "Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a\n  Hyperspectral Unmixing Method Dealing with Intra-class Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation is a common processing tool to analyse the\nconstitution of pixels of hyperspectral images. Such methods usually suppose\nthat pure pixel spectra (endmembers) are the same in all the image for each\nclass of materials. In the framework of remote sensing, such an assumption is\nno more valid in the presence of intra-class variabilities due to illumination\nconditions, weathering, slight variations of the pure materials, etc... In this\npaper, we first describe the results of investigations highlighting intra-class\nvariability measured in real images. Considering these results, a new\nformulation of the linear mixing model is presented leading to two new methods.\nUnconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation\nmethod based on the assumption of a linear mixing model, which can deal with\nintra-class variability. To overcome UP-NMF limitations an extended method is\nproposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each\nsensed spectrum, these extended versions of NMF extract a corresponding set of\nsource spectra. A constraint is set to limit the spreading of each source's\nestimates in IP-NMF. The methods are tested on a semi-synthetic data set built\nwith spectra extracted from a real hyperspectral image and then numerically\nmixed. We thus demonstrate the interest of our methods for realistic source\nvariabilities. Finally, IP-NMF is tested on a real data set and it is shown to\nyield better performance than state of the art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 15:43:10 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Revel", "Charlotte", ""], ["Deville", "Yannick", ""], ["Achard", "V\u00e9ronique", ""], ["Briottet", "Xavier", ""]]}, {"id": "1702.07664", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Marios Savvides", "title": "How ConvNets model Non-linear Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we theoretically address three fundamental problems involving\ndeep convolutional networks regarding invariance, depth and hierarchy. We\nintroduce the paradigm of Transformation Networks (TN) which are a direct\ngeneralization of Convolutional Networks (ConvNets). Theoretically, we show\nthat TNs (and thereby ConvNets) are can be invariant to non-linear\ntransformations of the input despite pooling over mere local translations. Our\nanalysis provides clear insights into the increase in invariance with depth in\nthese networks. Deeper networks are able to model much richer classes of\ntransformations. We also find that a hierarchical architecture allows the\nnetwork to generate invariance much more efficiently than a non-hierarchical\nnetwork. Our results provide useful insight into these three fundamental\nproblems in deep learning using ConvNets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:09:22 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1702.07679", "submitter": "Alfredo Nava-Tudela", "authors": "Alfredo Nava-Tudela", "title": "A recommender system to restore images with impulse noise", "comments": "22 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a collaborative filtering recommender system to restore images with\nimpulse noise for which the noisy pixels have been previously identified. We\ndefine this recommender system in terms of a new color image representation\nusing three matrices that depend on the noise-free pixels of the image to\nrestore, and two parameters: $k$, the number of features; and $\\lambda$, the\nregularization factor. We perform experiments on a well known image database to\ntest our algorithm and we provide image quality statistics for the results\nobtained. We discuss the roles of bias and variance in the performance of our\nalgorithm as determined by the values of $k$ and $\\lambda$, and provide\nguidance on how to choose the values of these parameters. Finally, we discuss\nthe possibility of using our collaborative filtering recommender system to\nperform image inpainting and super-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:36:46 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Nava-Tudela", "Alfredo", ""]]}, {"id": "1702.07759", "submitter": "Gilles Puy", "authors": "Gilles Puy and Srdan Kitic and Patrick P\\'erez", "title": "Unifying local and non-local signal processing with graph CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the unification of local and non-local signal\nprocessing on graphs within a single convolutional neural network (CNN)\nframework. Building upon recent works on graph CNNs, we propose to use\nconvolutional layers that take as inputs two variables, a signal and a graph,\nallowing the network to adapt to changes in the graph structure. In this\narticle, we explain how this framework allows us to design a novel method to\nperform style transfer.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 20:56:26 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 16:04:12 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Puy", "Gilles", ""], ["Kitic", "Srdan", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1702.07772", "submitter": "Aneeq Zia", "authors": "Aneeq Zia, Yachna Sharma, Vinay Bettadapura, Eric L. Sarin and Irfan\n  Essa", "title": "Video and Accelerometer-Based Motion Analysis for Automated Surgical\n  Skills Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Basic surgical skills of suturing and knot tying are an essential\npart of medical training. Having an automated system for surgical skills\nassessment could help save experts time and improve training efficiency. There\nhave been some recent attempts at automated surgical skills assessment using\neither video analysis or acceleration data. In this paper, we present a novel\napproach for automated assessment of OSATS based surgical skills and provide an\nanalysis of different features on multi-modal data (video and accelerometer\ndata). Methods: We conduct the largest study, to the best of our knowledge, for\nbasic surgical skills assessment on a dataset that contained video and\naccelerometer data for suturing and knot-tying tasks. We introduce \"entropy\nbased\" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy\n(XApEn), which quantify the amount of predictability and regularity of\nfluctuations in time-series data. The proposed features are compared to\nexisting methods of Sequential Motion Texture (SMT), Discrete Cosine Transform\n(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.\nResults: We report average performance of different features across all\napplicable OSATS criteria for suturing and knot tying tasks. Our analysis shows\nthat the proposed entropy based features out-perform previous state-of-the-art\nmethods using video data. For accelerometer data, our method performs better\nfor suturing only. We also show that fusion of video and acceleration features\ncan improve overall performance with the proposed entropy features achieving\nhighest accuracy. Conclusions: Automated surgical skills assessment can be\nachieved with high accuracy using the proposed entropy features. Such a system\ncan significantly improve the efficiency of surgical training in medical\nschools and teaching hospitals.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 21:30:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Zia", "Aneeq", ""], ["Sharma", "Yachna", ""], ["Bettadapura", "Vinay", ""], ["Sarin", "Eric L.", ""], ["Essa", "Irfan", ""]]}, {"id": "1702.07811", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama", "title": "Adaptive Neural Networks for Efficient Inference", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:527-536, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 00:22:51 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 18:14:49 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Wang", "Joseph", ""], ["Dekel", "Ofer", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1702.07836", "submitter": "Georgios Georgakis", "authors": "Georgios Georgakis, Arsalan Mousavian, Alexander C. Berg, Jana Kosecka", "title": "Synthesizing Training Data for Object Detection in Indoor Scenes", "comments": "Added more experiments and link to project webpage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of objects in cluttered indoor environments is one of the key\nenabling functionalities for service robots. The best performing object\ndetection approaches in computer vision exploit deep Convolutional Neural\nNetworks (CNN) to simultaneously detect and categorize the objects of interest\nin cluttered scenes. Training of such models typically requires large amounts\nof annotated training data which is time consuming and costly to obtain. In\nthis work we explore the ability of using synthetically generated composite\nimages for training state-of-the-art object detectors, especially for object\ninstance detection. We superimpose 2D images of textured object models into\nimages of real environments at variety of locations and scales. Our experiments\nevaluate different superimposition strategies ranging from purely image-based\nblending all the way to depth and semantics informed positioning of the object\nmodels into real scenes. We demonstrate the effectiveness of these object\ndetector training strategies on two publicly available datasets, the\nGMU-Kitchens and the Washington RGB-D Scenes v2. As one observation, augmenting\nsome hand-labeled training data with synthetic examples carefully composed onto\nscenes yields object detectors with comparable performance to using much more\nhand-labeled data. Broadly, this work charts new opportunities for training\ndetectors for new objects by exploiting existing object model repositories in\neither a purely automatic fashion or with only a very small number of\nhuman-annotated examples.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 06:04:42 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 00:29:55 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Georgakis", "Georgios", ""], ["Mousavian", "Arsalan", ""], ["Berg", "Alexander C.", ""], ["Kosecka", "Jana", ""]]}, {"id": "1702.07841", "submitter": "Mohsen Ghafoorian", "authors": "Mohsen Ghafoorian, Alireza Mehrtash, Tina Kapur, Nico Karssemeijer,\n  Elena Marchiori, Mehran Pesteie, Charles R. G. Guttmann, Frank-Erik de Leeuw,\n  Clare M. Tempany, Bram van Ginneken, Andriy Fedorov, Purang Abolmaesumi, Bram\n  Platel, William M. Wells III", "title": "Transfer Learning for Domain Adaptation in MRI: Application in Brain\n  Lesion Segmentation", "comments": "8 pages, 3 figures", "journal-ref": "Medical Image Computing and Computer-Assisted Intervention 2017,\n  Vol 10435, 516-524", "doi": "10.1007/978-3-319-66179-7_59", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis\nand treatment. However, variations in MRI acquisition protocols result in\ndifferent appearances of normal and diseased tissue in the images.\nConvolutional neural networks (CNNs), which have shown to be successful in many\nmedical image analysis tasks, are typically sensitive to the variations in\nimaging protocols. Therefore, in many cases, networks trained on data acquired\nwith one MRI protocol, do not perform satisfactorily on data acquired with\ndifferent protocols. This limits the use of models trained with large annotated\nlegacy datasets on a new dataset with a different domain which is often a\nrecurring situation in clinical settings. In this study, we aim to answer the\nfollowing central questions regarding domain adaptation in medical image\nanalysis: Given a fitted legacy model, 1) How much data from the new domain is\nrequired for a decent adaptation of the original network?; and, 2) What portion\nof the pre-trained model parameters should be retrained given a certain number\nof the new domain training samples? To address these questions, we conducted\nextensive experiments in white matter hyperintensity segmentation task. We\ntrained a CNN on legacy MR images of brain and evaluated the performance of the\ndomain-adapted network on the same task with images from a different domain. We\nthen compared the performance of the model to the surrogate scenarios where\neither the same trained network is used or a new network is trained from\nscratch on the new dataset.The domain-adapted network tuned only by two\ntraining examples achieved a Dice score of 0.63 substantially outperforming a\nsimilar network trained on the same set of examples from scratch.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 07:04:25 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Ghafoorian", "Mohsen", ""], ["Mehrtash", "Alireza", ""], ["Kapur", "Tina", ""], ["Karssemeijer", "Nico", ""], ["Marchiori", "Elena", ""], ["Pesteie", "Mehran", ""], ["Guttmann", "Charles R. G.", ""], ["de Leeuw", "Frank-Erik", ""], ["Tempany", "Clare M.", ""], ["van Ginneken", "Bram", ""], ["Fedorov", "Andriy", ""], ["Abolmaesumi", "Purang", ""], ["Platel", "Bram", ""], ["Wells", "William M.", "III"]]}, {"id": "1702.07884", "submitter": "Mehran Safayani", "authors": "Mehran Safayani, Seyed Hashem Ahmadi, Homayun Afrabandpey and\n  Abdolreza Mirzaei", "title": "An EM Based Probabilistic Two-Dimensional CCA with Application to Face\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1007/s10489-017-1012-2", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, two-dimensional canonical correlation analysis (2DCCA) has been\nsuccessfully applied for image feature extraction. The method instead of\nconcatenating the columns of the images to the one-dimensional vectors,\ndirectly works with two-dimensional image matrices. Although 2DCCA works well\nin different recognition tasks, it lacks a probabilistic interpretation. In\nthis paper, we present a probabilistic framework for 2DCCA called probabilistic\n2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the\nparameters. Experimental results on synthetic and real data demonstrate\nsuperior performance in loading factor estimation for P2DCCA compared to 2DCCA.\nFor real data, three subsets of AR face database and also the UMIST face\ndatabase confirm the robustness of the proposed algorithm in face recognition\ntasks with different illumination conditions, facial expressions, poses and\nocclusions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 12:50:35 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Safayani", "Mehran", ""], ["Ahmadi", "Seyed Hashem", ""], ["Afrabandpey", "Homayun", ""], ["Mirzaei", "Abdolreza", ""]]}, {"id": "1702.07898", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Samuel Rota Bul\\`o, Elisa Ricci, Barbara Caputo", "title": "Learning Deep NBNN Representations for Robust Place Categorization", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters, Vol. 2, n. 3, July 2017", "doi": "10.1109/LRA.2017.2705282", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for semantic place categorization using data\nobtained from RGB cameras. Previous studies on visual place recognition and\nclassification have shown that, by considering features derived from\npre-trained Convolutional Neural Networks (CNNs) in combination with part-based\nclassification models, high recognition accuracy can be achieved, even in\npresence of occlusions and severe viewpoint changes. Inspired by these works,\nwe propose to exploit local deep representations, representing images as set of\nregions applying a Na\\\"{i}ve Bayes Nearest Neighbor (NBNN) model for image\nclassification. As opposed to previous methods where CNNs are merely used as\nfeature extractors, our approach seamlessly integrates the NBNN model into a\nfully-convolutional neural network. Experimental results show that the proposed\nalgorithm outperforms previous methods based on pre-trained CNN models and\nthat, when employed in challenging robot place recognition tasks, it is robust\nto occlusions, environmental and sensor changes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 14:50:43 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 16:33:32 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""]]}, {"id": "1702.07908", "submitter": "Sabri Pllana", "authors": "Andre Viebke, Suejb Memeti, Sabri Pllana, Ajith Abraham", "title": "CHAOS: A Parallelization Scheme for Training Convolutional Neural\n  Networks on Intel Xeon Phi", "comments": "The Journal of Supercomputing, 2017", "journal-ref": null, "doi": "10.1007/s11227-017-1994-x", "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is an important component of big-data analytic tools and\nintelligent applications, such as, self-driving cars, computer vision, speech\nrecognition, or precision medicine. However, the training process is\ncomputationally intensive, and often requires a large amount of time if\nperformed sequentially. Modern parallel computing systems provide the\ncapability to reduce the required training time of deep neural networks. In\nthis paper, we present our parallelization scheme for training convolutional\nneural networks (CNN) named Controlled Hogwild with Arbitrary Order of\nSynchronization (CHAOS). Major features of CHAOS include the support for thread\nand vector parallelism, non-instant updates of weight parameters during\nback-propagation without a significant delay, and implicit synchronization in\narbitrary order. CHAOS is tailored for parallel computing systems that are\naccelerated with the Intel Xeon Phi. We evaluate our parallelization approach\nempirically using measurement techniques and performance modeling for various\nnumbers of threads and CNN architectures. Experimental results for the MNIST\ndataset of handwritten digits using the total number of threads on the Xeon Phi\nshow speedups of up to 103x compared to the execution on one thread of the Xeon\nPhi, 14x compared to the sequential execution on Intel Xeon E5, and 58x\ncompared to the sequential execution on Intel Core i5.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 15:48:44 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Viebke", "Andre", ""], ["Memeti", "Suejb", ""], ["Pllana", "Sabri", ""], ["Abraham", "Ajith", ""]]}, {"id": "1702.07935", "submitter": "Tianzhu Xiang", "authors": "Tian-Zhu Xiang, Gui-Song Xia, Xiang Bai, Liangpei Zhang", "title": "Image Stitching by Line-guided Local Warping with Global Similarity\n  Constraint", "comments": null, "journal-ref": "Pattern Recognition, Vol. 83, 2018, PP. 481-497", "doi": "10.1016/j.patcog.2018.06.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-textured image stitching remains a challenging problem. It is difficult\nto achieve good alignment and it is easy to break image structures due to\ninsufficient and unreliable point correspondences. Moreover, because of the\nviewpoint variations between multiple images, the stitched images suffer from\nprojective distortions. To solve these problems, this paper presents a\nline-guided local warping method with a global similarity constraint for image\nstitching. Line features which serve well for geometric descriptions and scene\nconstraints, are employed to guide image stitching accurately. On one hand, the\nline features are integrated into a local warping model through a designed\nweight function. On the other hand, line features are adopted to impose strong\ngeometric constraints, including line correspondence and line colinearity, to\nimprove the stitching performance through mesh optimization. To mitigate\nprojective distortions, we adopt a global similarity constraint, which is\nintegrated with the projective warps via a designed weight strategy. This\nconstraint causes the final warp to slowly change from a projective to a\nsimilarity transformation across the image. Finally, the images undergo a\ntwo-stage alignment scheme that provides accurate alignment and reduces\nprojective distortion. We evaluate our method on a series of images and compare\nit with several other methods. The experimental results demonstrate that the\nproposed method provides a convincing stitching performance and that it\noutperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 18:15:51 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 09:36:09 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Xiang", "Tian-Zhu", ""], ["Xia", "Gui-Song", ""], ["Bai", "Xiang", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1702.07942", "submitter": "Laurent Duval", "authors": "Camille Couprie, Laurent Duval, Maxime Moreaud, Sophie H\\'enon,\n  M\\'elinda Tebib, Vincent Souchon", "title": "BARCHAN: Blob Alignment for Robust CHromatographic ANalysis", "comments": "15 pages, published in the Special issue for RIVA 2016, 40th\n  International Symposium on Capillary Chromatography and 13th GCxGC Symposium", "journal-ref": "Journal of Chromatography A, Volume 1484, February 2017, Pages\n  65-72", "doi": "10.1016/j.chroma.2017.01.003", "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role\ninto the elucidation of complex samples. The automation of the identification\nof peak areas is of prime interest to obtain a fast and repeatable analysis of\nchromatograms. To determine the concentration of compounds or pseudo-compounds,\ntemplates of blobs are defined and superimposed on a reference chromatogram.\nThe templates then need to be modified when different chromatograms are\nrecorded. In this study, we present a chromatogram and template alignment\nmethod based on peak registration called BARCHAN. Peaks are identified using a\nrobust mathematical morphology tool. The alignment is performed by a\nprobabilistic estimation of a rigid transformation along the first dimension,\nand a non-rigid transformation in the second dimension, taking into account\nnoise, outliers and missing peaks in a fully automated way. Resulting aligned\nchromatograms and masks are presented on two datasets. The proposed algorithm\nproves to be fast and reliable. It significantly reduces the time to results\nfor GCxGC analysis.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 19:59:39 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Couprie", "Camille", ""], ["Duval", "Laurent", ""], ["Moreaud", "Maxime", ""], ["H\u00e9non", "Sophie", ""], ["Tebib", "M\u00e9linda", ""], ["Souchon", "Vincent", ""]]}, {"id": "1702.07959", "submitter": "Abraham Smith", "authors": "Abraham Smith and Paul Bendich and John Harer and Alex Pieloch and Jay\n  Hineman", "title": "Supervised Learning of Labeled Pointcloud Differences via Cover-Tree\n  Entropy Reduction", "comments": "Distribution Statement A - Approved for public release, distribution\n  is unlimited. Version 2: added link to code, and some minor improvements.\n  Version 3: updated authors and thanks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm, called CDER, for supervised machine learning\nthat merges the multi-scale geometric properties of Cover Trees with the\ninformation-theoretic properties of entropy. CDER applies to a training set of\nlabeled pointclouds embedded in a common Euclidean space. If typical\npointclouds corresponding to distinct labels tend to differ at any scale in any\nsub-region, CDER can identify these differences in (typically) linear time,\ncreating a set of distributional coordinates which act as a feature extraction\nmechanism for supervised learning. We describe theoretical properties and\nimplementation details of CDER, and illustrate its benefits on several\nsynthetic examples.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 00:17:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 14:15:31 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 19:30:37 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Smith", "Abraham", ""], ["Bendich", "Paul", ""], ["Harer", "John", ""], ["Pieloch", "Alex", ""], ["Hineman", "Jay", ""]]}, {"id": "1702.07963", "submitter": "Mohamed Attia Mr", "authors": "M. Attia, M. Hossny, S. Nahavandi and A. Yazdabadi", "title": "Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning\n  Techniques", "comments": "ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed using a hybrid method that utilises deep\nconvolutional and recurrent neural networks for accurate delineation of skin\nlesion of images supplied with ISBI 2017 lesion segmentation challenge. The\nproposed method was trained using 1800 images and tested on 150 images from\nISBI 2017 challenge.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 00:56:25 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Attia", "M.", ""], ["Hossny", "M.", ""], ["Nahavandi", "S.", ""], ["Yazdabadi", "A.", ""]]}, {"id": "1702.07971", "submitter": "Jin Sun", "authors": "Jin Sun and David W. Jacobs", "title": "Seeing What Is Not There: Learning Context to Determine Where Objects\n  Are Missing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of computer vision focuses on what is in an image. We propose to train a\nstandalone object-centric context representation to perform the opposite task:\nseeing what is not there. Given an image, our context model can predict where\nobjects should exist, even when no object instances are present. Combined with\nobject detection results, we can perform a novel vision task: finding where\nobjects are missing in an image. Our model is based on a convolutional neural\nnetwork structure. With a specially designed training strategy, the model\nlearns to ignore objects and focus on context only. It is fully convolutional\nthus highly efficient. Experiments show the effectiveness of the proposed\napproach in one important accessibility task: finding city street regions where\ncurb ramps are missing, which could help millions of people with mobility\ndisabilities.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 01:56:38 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Sun", "Jin", ""], ["Jacobs", "David W.", ""]]}, {"id": "1702.07975", "submitter": "Lianwen Jin", "authors": "Xuefeng Xiao, Lianwen Jin, Yafeng Yang, Weixin Yang, Jun Sun, Tianhai\n  Chang", "title": "Building Fast and Compact Convolutional Neural Networks for Offline\n  Handwritten Chinese Character Recognition", "comments": "15 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like other problems in computer vision, offline handwritten Chinese character\nrecognition (HCCR) has achieved impressive results using convolutional neural\nnetwork (CNN)-based methods. However, larger and deeper networks are needed to\ndeliver state-of-the-art results in this domain. Such networks intuitively\nappear to incur high computational cost, and require the storage of a large\nnumber of parameters, which renders them unfeasible for deployment in portable\ndevices. To solve this problem, we propose a Global Supervised Low-rank\nExpansion (GSLRE) method and an Adaptive Drop-weight (ADW) technique to solve\nthe problems of speed and storage capacity. We design a nine-layer CNN for HCCR\nconsisting of 3,755 classes, and devise an algorithm that can reduce the\nnetworks computational cost by nine times and compress the network to 1/18 of\nthe original size of the baseline model, with only a 0.21% drop in accuracy. In\ntests, the proposed algorithm surpassed the best single-network performance\nreported thus far in the literature while requiring only 2.3 MB for storage.\nFurthermore, when integrated with our effective forward implementation, the\nrecognition of an offline character image took only 9.7 ms on a CPU. Compared\nwith the state-of-the-art CNN model for HCCR, our approach is approximately 30\ntimes faster, yet 10 times more cost efficient.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 02:13:20 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Xiao", "Xuefeng", ""], ["Jin", "Lianwen", ""], ["Yang", "Yafeng", ""], ["Yang", "Weixin", ""], ["Sun", "Jun", ""], ["Chang", "Tianhai", ""]]}, {"id": "1702.07985", "submitter": "Fan Zhang", "authors": "Fan Zhang, Bo Du and Liangpei Zhang", "title": "A multi-task convolutional neural network for mega-city analysis using\n  very high resolution satellite imagery and geospatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mega-city analysis with very high resolution (VHR) satellite images has been\ndrawing increasing interest in the fields of city planning and social\ninvestigation. It is known that accurate land-use, urban density, and\npopulation distribution information is the key to mega-city monitoring and\nenvironmental studies. Therefore, how to generate land-use, urban density, and\npopulation distribution maps at a fine scale using VHR satellite images has\nbecome a hot topic. Previous studies have focused solely on individual tasks\nwith elaborate hand-crafted features and have ignored the relationship between\ndifferent tasks. In this study, we aim to propose a universal framework which\ncan: 1) automatically learn the internal feature representation from the raw\nimage data; and 2) simultaneously produce fine-scale land-use, urban density,\nand population distribution maps. For the first target, a deep convolutional\nneural network (CNN) is applied to learn the hierarchical feature\nrepresentation from the raw image data. For the second target, a novel\nCNN-based universal framework is proposed to process the VHR satellite images\nand generate the land-use, urban density, and population distribution maps. To\nthe best of our knowledge, this is the first CNN-based mega-city analysis\nmethod which can process a VHR remote sensing image with such a large data\nvolume. A VHR satellite image (1.2 m spatial resolution) of the center of Wuhan\ncovering an area of 2606 km2 was used to evaluate the proposed method. The\nexperimental results confirm that the proposed method can achieve a promising\naccuracy for land-use, urban density, and population distribution maps.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 04:23:36 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Zhang", "Fan", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1702.08001", "submitter": "J\\\"urgen Hahn", "authors": "J\\\"urgen Hahn and Abdelhak M. Zoubir", "title": "Bayesian Nonparametric Feature and Policy Learning for Decision-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from demonstrations has gained increasing interest in the recent\npast, enabling an agent to learn how to make decisions by observing an\nexperienced teacher. While many approaches have been proposed to solve this\nproblem, there is only little work that focuses on reasoning about the observed\nbehavior. We assume that, in many practical problems, an agent makes its\ndecision based on latent features, indicating a certain action. Therefore, we\npropose a generative model for the states and actions. Inference reveals the\nnumber of features, the features, and the policies, allowing us to learn and to\nanalyze the underlying structure of the observed behavior. Further, our\napproach enables prediction of actions for new states. Simulations are used to\nassess the performance of the algorithm based upon this model. Moreover, the\nproblem of learning a driver's behavior is investigated, demonstrating the\nperformance of the proposed model in a real-world scenario.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 08:34:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Hahn", "J\u00fcrgen", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1702.08007", "submitter": "J\\\"urgen Hahn", "authors": "J\\\"urgen Hahn and Abdelhak M.Zoubir", "title": "Bayesian Nonparametric Unmixing of Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging is an important tool in remote sensing, allowing for\naccurate analysis of vast areas. Due to a low spatial resolution, a pixel of a\nhyperspectral image rarely represents a single material, but rather a mixture\nof different spectra. HSU aims at estimating the pure spectra present in the\nscene of interest, referred to as endmembers, and their fractions in each\npixel, referred to as abundances. Today, many HSU algorithms have been\nproposed, based either on a geometrical or statistical model. While most\nmethods assume that the number of endmembers present in the scene is known,\nthere is only little work about estimating this number from the observed data.\nIn this work, we propose a Bayesian nonparametric framework that jointly\nestimates the number of endmembers, the endmembers itself, and their\nabundances, by making use of the Indian Buffet Process as a prior for the\nendmembers. Simulation results and experiments on real data demonstrate the\neffectiveness of the proposed algorithm, yielding results comparable with\nstate-of-the-art methods while being able to reliably infer the number of\nendmembers. In scenarios with strong noise, where other algorithms provide only\npoor results, the proposed approach tends to overestimate the number of\nendmembers slightly. The additional endmembers, however, often simply represent\nnoisy replicas of present endmembers and could easily be merged in a\npost-processing step.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 09:10:45 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Hahn", "J\u00fcrgen", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1702.08009", "submitter": "Michael Ying Yang", "authors": "Omid Hosseini Jafari, Oliver Groth, Alexander Kirillov, Michael Ying\n  Yang, Carsten Rother", "title": "Analyzing Modular CNN Architectures for Joint Depth Prediction and\n  Semantic Segmentation", "comments": "Accepted to ICRA 2017", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989537", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of designing a modular neural network\narchitecture that jointly solves different tasks. As an example we use the\ntasks of depth estimation and semantic segmentation given a single RGB image.\nThe main focus of this work is to analyze the cross-modality influence between\ndepth and semantic prediction maps on their joint refinement. While most\nprevious works solely focus on measuring improvements in accuracy, we propose a\nway to quantify the cross-modality influence. We show that there is a\nrelationship between final accuracy and cross-modality influence, although not\na simple linear one. Hence a larger cross-modality influence does not\nnecessarily translate into an improved accuracy. We find that a beneficial\nbalance between the cross-modality influences can be achieved by network\narchitecture and conjecture that this relationship can be utilized to\nunderstand different network design choices. Towards this end we propose a\nConvolutional Neural Network (CNN) architecture that fuses the state of the\nstate-of-the-art results for depth estimation and semantic labeling. By\nbalancing the cross-modality influences between depth and semantic prediction,\nwe achieve improved results for both tasks using the NYU-Depth v2 benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 09:30:08 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Jafari", "Omid Hosseini", ""], ["Groth", "Oliver", ""], ["Kirillov", "Alexander", ""], ["Yang", "Michael Ying", ""], ["Rother", "Carsten", ""]]}, {"id": "1702.08014", "submitter": "Simon Kohl", "authors": "Simon Kohl, David Bonekamp, Heinz-Peter Schlemmer, Kaneschka Yaqubi,\n  Markus Hohenfellner, Boris Hadaschik, Jan-Philipp Radtke and Klaus Maier-Hein", "title": "Adversarial Networks for the Detection of Aggressive Prostate Cancer", "comments": "8 pages, 3 figures; under review as a conference paper at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation constitutes an integral part of medical image analyses\nfor which breakthroughs in the field of deep learning were of high relevance.\nThe large number of trainable parameters of deep neural networks however\nrenders them inherently data hungry, a characteristic that heavily challenges\nthe medical imaging community. Though interestingly, with the de facto standard\ntraining of fully convolutional networks (FCNs) for semantic segmentation being\nagnostic towards the `structure' of the predicted label maps, valuable\ncomplementary information about the global quality of the segmentation lies\nidle. In order to tap into this potential, we propose utilizing an adversarial\nnetwork which discriminates between expert and generated annotations in order\nto train FCNs for semantic segmentation. Because the adversary constitutes a\nlearned parametrization of what makes a good segmentation at a global level, we\nhypothesize that the method holds particular advantages for segmentation tasks\non complex structured, small datasets. This holds true in our experiments: We\nlearn to segment aggressive prostate cancer utilizing MRI images of 152\npatients and show that the proposed scheme is superior over the de facto\nstandard in terms of the detection sensitivity and the dice-score for\naggressive prostate cancer. The achieved relative gains are shown to be\nparticularly pronounced in the small dataset limit.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 10:08:49 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Kohl", "Simon", ""], ["Bonekamp", "David", ""], ["Schlemmer", "Heinz-Peter", ""], ["Yaqubi", "Kaneschka", ""], ["Hohenfellner", "Markus", ""], ["Hadaschik", "Boris", ""], ["Radtke", "Jan-Philipp", ""], ["Maier-Hein", "Klaus", ""]]}, {"id": "1702.08112", "submitter": "Chuong Nguyen", "authors": "Chuong V Nguyen, Jurgen Fripp, David R Lovell, Robert Furbank, Peter\n  Kuffner, Helen Daily, Xavier Sirault", "title": "3D Scanning System for Automatic High-Resolution Plant Phenotyping", "comments": "8 papes, DICTA 2016", "journal-ref": "In Digital Image Computing: Techniques and Applications (DICTA),\n  2016 International Conference on, pp. 1-8. IEEE, 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thin leaves, fine stems, self-occlusion, non-rigid and slowly changing\nstructures make plants difficult for three-dimensional (3D) scanning and\nreconstruction -- two critical steps in automated visual phenotyping. Many\ncurrent solutions such as laser scanning, structured light, and multiview\nstereo can struggle to acquire usable 3D models because of limitations in\nscanning resolution and calibration accuracy. In response, we have developed a\nfast, low-cost, 3D scanning platform to image plants on a rotating stage with\ntwo tilting DSLR cameras centred on the plant. This uses new methods of camera\ncalibration and background removal to achieve high-accuracy 3D reconstruction.\nWe assessed the system's accuracy using a 3D visual hull reconstruction\nalgorithm applied on 2 plastic models of dicotyledonous plants, 2 sorghum\nplants and 2 wheat plants across different sets of tilt angles. Scan times\nranged from 3 minutes (to capture 72 images using 2 tilt angles), to 30 minutes\n(to capture 360 images using 10 tilt angles). The leaf lengths, widths, areas\nand perimeters of the plastic models were measured manually and compared to\nmeasurements from the scanning system: results were within 3-4% of each other.\nThe 3D reconstructions obtained with the scanning system show excellent\ngeometric agreement with all six plant specimens, even plants with thin leaves\nand fine stems.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 23:52:00 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Nguyen", "Chuong V", ""], ["Fripp", "Jurgen", ""], ["Lovell", "David R", ""], ["Furbank", "Robert", ""], ["Kuffner", "Peter", ""], ["Daily", "Helen", ""], ["Sirault", "Xavier", ""]]}, {"id": "1702.08115", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David M. W. Powers, Trent W. Lewis", "title": "Bioplausible multiscale filtering in retino-cortical processing as a\n  mechanism in perceptual grouping", "comments": "23 pages, 8 figures, Brain Informatics journal: Full text access:\n  https://link.springer.com/article/10.1007/s40708-017-0072-8", "journal-ref": null, "doi": "10.1007/s40708-017-0072-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why does our visual system fail to reconstruct reality, when we look at\ncertain patterns? Where do Geometrical illusions start to emerge in the visual\npathway? How far should we take computational models of vision with the same\nvisual ability to detect illusions as we do? This study addresses these\nquestions, by focusing on a specific underlying neural mechanism involved in\nour visual experiences that affects our final perception. Among many types of\nvisual illusion, Geometrical and, in particular, Tilt Illusions are rather\nimportant, being characterized by misperception of geometric patterns involving\nlines and tiles in combination with contrasting orientation, size or position.\nOver the last decade, many new neurophysiological experiments have led to new\ninsights as to how, when and where retinal processing takes place, and the\nencoding nature of the retinal representation that is sent to the cortex for\nfurther processing. Based on these neurobiological discoveries, we provide\ncomputer simulation evidence from modelling retinal ganglion cells responses to\nsome complex Tilt Illusions, suggesting that the emergence of tilt in these\nillusions is partially related to the interaction of multiscale visual\nprocessing performed in the retina. The output of our low-level filtering model\nis presented for several types of Tilt Illusion, predicting that the final tilt\npercept arises from multiple-scale processing of the Differences of Gaussians\nand the perceptual interaction of foreground and background elements. Our\nresults suggest that this model has a high potential in revealing the\nunderlying mechanism connecting low-level filtering approaches to mid- and\nhigh-level explanations such as Anchoring theory and Perceptual grouping.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 00:38:33 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 11:39:01 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 03:54:03 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David M. W.", ""], ["Lewis", "Trent W.", ""]]}, {"id": "1702.08155", "submitter": "Holger Roth", "authors": "Holger R. Roth, Kai Nagara, Hirohisa Oda, Masahiro Oda, Tomoshi\n  Sugiyama, Shota Nakamura, Kensaku Mori", "title": "Multi-scale Image Fusion Between Pre-operative Clinical CT and X-ray\n  Microtomography of Lung Pathology", "comments": "In proceedings of International Forum on Medical Imaging, IFMIA 2017,\n  Okinawa, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational anatomy allows the quantitative analysis of organs in medical\nimages. However, most analysis is constrained to the millimeter scale because\nof the limited resolution of clinical computed tomography (CT). X-ray\nmicrotomography ($\\mu$CT) on the other hand allows imaging of ex-vivo tissues\nat a resolution of tens of microns. In this work, we use clinical CT to image\nlung cancer patients before partial pneumonectomy (resection of pathological\nlung tissue). The resected specimen is prepared for $\\mu$CT imaging at a voxel\nresolution of 50 $\\mu$m (0.05 mm). This high-resolution image of the lung\ncancer tissue allows further insides into understanding of tumor growth and\ncategorization. For making full use of this additional information, image\nfusion (registration) needs to be performed in order to re-align the $\\mu$CT\nimage with clinical CT. We developed a multi-scale non-rigid registration\napproach. After manual initialization using a few landmark points and rigid\nalignment, several levels of non-rigid registration between down-sampled (in\nthe case of $\\mu$CT) and up-sampled (in the case of clinical CT)\nrepresentations of the image are performed. Any non-lung tissue is ignored\nduring the computation of the similarity measure used to guide the registration\nduring optimization. We are able to recover the volume differences introduced\nby the resection and preparation of the lung specimen. The average ($\\pm$ std.\ndev.) minimum surface distance between $\\mu$CT and clinical CT at the resected\nlung surface is reduced from 3.3 $\\pm$ 2.9 (range: [0.1, 15.9]) to 2.3 mm $\\pm$\n2.8 (range: [0.0, 15.3]) mm. The alignment of clinical CT with $\\mu$CT will\nallow further registration with even finer resolutions of $\\mu$CT (up to 10\n$\\mu$m resolution) and ultimately with histopathological microscopy images for\nfurther macro to micro image fusion that can aid medical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 06:04:52 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Roth", "Holger R.", ""], ["Nagara", "Kai", ""], ["Oda", "Hirohisa", ""], ["Oda", "Masahiro", ""], ["Sugiyama", "Tomoshi", ""], ["Nakamura", "Shota", ""], ["Mori", "Kensaku", ""]]}, {"id": "1702.08160", "submitter": "J. D. Curt\\'o", "authors": "J. D. Curt\\'o and I. C. Zarza and Alex Smola and Luc van Gool", "title": "Segmentation of Objects by Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to address the problem of Simultaneous Detection\nand Segmentation introduced in [Hariharan et al 2014]. Using the hierarchical\nstructures first presented in [Arbel\\'aez et al 2011] we use an efficient and\naccurate procedure that exploits the feature information of the hierarchy using\nLocality Sensitive Hashing. We build on recent work that utilizes convolutional\nneural networks to detect bounding boxes in an image [Ren et al 2015] and then\nuse the top similar hierarchical region that best fits each bounding box after\nhashing, we call this approach C&Z Segmentation. We then refine our final\nsegmentation results by automatic hierarchical pruning. C&Z Segmentation\nintroduces a train-free alternative to Hypercolumns [Hariharan et al 2015]. We\nconduct extensive experiments on PASCAL VOC 2012 segmentation dataset, showing\nthat C&Z gives competitive state-of-the-art segmentations of objects.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 06:40:22 GMT"}, {"version": "v10", "created": "Wed, 20 Feb 2019 17:26:29 GMT"}, {"version": "v11", "created": "Sun, 24 Mar 2019 17:23:51 GMT"}, {"version": "v12", "created": "Tue, 31 Dec 2019 19:17:21 GMT"}, {"version": "v13", "created": "Fri, 17 Apr 2020 16:33:28 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 14:08:37 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 12:37:00 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 12:26:29 GMT"}, {"version": "v5", "created": "Thu, 10 May 2018 12:17:53 GMT"}, {"version": "v6", "created": "Wed, 30 May 2018 17:12:59 GMT"}, {"version": "v7", "created": "Sun, 10 Jun 2018 11:05:52 GMT"}, {"version": "v8", "created": "Wed, 2 Jan 2019 18:39:36 GMT"}, {"version": "v9", "created": "Sun, 6 Jan 2019 16:39:21 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Curt\u00f3", "J. D.", ""], ["Zarza", "I. C.", ""], ["Smola", "Alex", ""], ["van Gool", "Luc", ""]]}, {"id": "1702.08192", "submitter": "Christian Wachinger", "authors": "Christian Wachinger, Martin Reuter, Tassilo Klein", "title": "DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy", "comments": "Accepted for publication in NeuroImage, special issue \"Brain\n  Segmentation and Parcellation\", 2017", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.02.035", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeepNAT, a 3D Deep convolutional neural network for the\nautomatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance\nimages. DeepNAT is an end-to-end learning-based approach to brain segmentation\nthat jointly learns an abstract feature representation and a multi-class\nclassification. We propose a 3D patch-based approach, where we do not only\npredict the center voxel of the patch but also neighbors, which is formulated\nas multi-task learning. To address a class imbalance problem, we arrange two\nnetworks hierarchically, where the first one separates foreground from\nbackground, and the second one identifies 25 brain structures on the\nforeground. Since patches lack spatial context, we augment them with\ncoordinates. To this end, we introduce a novel intrinsic parameterization of\nthe brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As\nnetwork architecture, we use three convolutional layers with pooling, batch\nnormalization, and non-linearities, followed by fully connected layers with\ndropout. The final segmentation is inferred from the probabilistic output of\nthe network with a 3D fully connected conditional random field, which ensures\nlabel agreement between close voxels. The roughly 2.7 million parameters in the\nnetwork are learned with stochastic gradient descent. Our results show that\nDeepNAT compares favorably to state-of-the-art methods. Finally, the purely\nlearning-based method may have a high potential for the adaptation to young,\nold, or diseased brains by fine-tuning the pre-trained network with a small\ntraining sample on the target application, where the availability of larger\ndatasets with manual annotations may boost the overall segmentation accuracy in\nthe future.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 08:53:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wachinger", "Christian", ""], ["Reuter", "Martin", ""], ["Klein", "Tassilo", ""]]}, {"id": "1702.08212", "submitter": "Judith B\\\"utepage", "authors": "Judith B\\\"utepage, Hedvig Kjellstr\\\"om, Danica Kragic", "title": "Anticipating many futures: Online human motion prediction and synthesis\n  for human-robot collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluent and safe interactions of humans and robots require both partners to\nanticipate the others' actions. A common approach to human intention inference\nis to model specific trajectories towards known goals with supervised\nclassifiers. However, these approaches do not take possible future movements\ninto account nor do they make use of kinematic cues, such as legible and\npredictable motion. The bottleneck of these methods is the lack of an accurate\nmodel of general human motion. In this work, we present a conditional\nvariational autoencoder that is trained to predict a window of future human\nmotion given a window of past frames. Using skeletal data obtained from RGB\ndepth images, we show how this unsupervised approach can be used for online\nmotion prediction for up to 1660 ms. Additionally, we demonstrate online target\nprediction within the first 300-500 ms after motion onset without the use of\ntarget specific training data. The advantage of our probabilistic approach is\nthe possibility to draw samples of possible future motions. Finally, we\ninvestigate how movements and kinematic cues are represented on the learned low\ndimensional manifold.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 10:02:40 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["B\u00fctepage", "Judith", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Kragic", "Danica", ""]]}, {"id": "1702.08231", "submitter": "Benjamin Graham", "authors": "Benjamin Graham", "title": "Low-Precision Batch-Normalized Activations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks can be trained with relatively low-precision\nfloating-point and fixed-point arithmetic, using between one and 16 bits.\nPrevious works have focused on relatively wide-but-shallow, feed-forward\nnetworks. We introduce a quantization scheme that is compatible with training\nvery deep neural networks. Quantizing the network activations in the middle of\neach batch-normalization module can greatly reduce the amount of memory and\ncomputational power needed, with little loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 11:10:54 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Graham", "Benjamin", ""]]}, {"id": "1702.08259", "submitter": "Hiroshi Inoue", "authors": "Hiroshi Inoue", "title": "Adaptive Ensemble Prediction for Deep Neural Networks based on\n  Confidence Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling multiple predictions is a widely used technique for improving the\naccuracy of various machine learning tasks. One obvious drawback of ensembling\nis its higher execution cost during inference. In this paper, we first describe\nour insights on the relationship between the probability of prediction and the\neffect of ensembling with current deep neural networks; ensembling does not\nhelp mispredictions for inputs predicted with a high probability even when\nthere is a non-negligible number of mispredicted inputs. This finding motivated\nus to develop a way to adaptively control the ensembling. If the prediction for\nan input reaches a high enough probability, i.e., the output from the softmax\nfunction, on the basis of the confidence level, we stop ensembling for this\ninput to avoid wasting computation power. We evaluated the adaptive ensembling\nby using various datasets and showed that it reduces the computation cost\nsignificantly while achieving accuracy similar to that of static ensembling\nusing a pre-defined number of local predictions. We also show that our\nstatistically rigorous confidence-level-based early-exit condition reduces the\nburden of task-dependent threshold tuning better compared with naive early exit\nbased on a pre-defined threshold in addition to yielding a better accuracy with\nthe same cost.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:54:54 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 13:35:42 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 11:46:51 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Inoue", "Hiroshi", ""]]}, {"id": "1702.08272", "submitter": "Phil Ammirato", "authors": "Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana Kosecka, Alexander\n  C. Berg", "title": "A Dataset for Developing and Benchmarking Active Vision", "comments": "To appear at ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new public dataset with a focus on simulating robotic vision\ntasks in everyday indoor environments using real imagery. The dataset includes\n20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely\ncaptured in 9 unique scenes. We train a fast object category detector for\ninstance detection on our data. Using the dataset we show that, although\nincreasingly accurate and fast, the state of the art for object detection is\nstill severely impacted by object scale, occlusion, and viewing direction all\nof which matter for robotics applications. We next validate the dataset for\nsimulating active vision, and use the dataset to develop and evaluate a\ndeep-network-based system for next best move prediction for object\nclassification using reinforcement learning. Our dataset is available for\ndownload at cs.unc.edu/~ammirato/active_vision_dataset_website/.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 13:23:35 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 20:06:58 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Ammirato", "Phil", ""], ["Poirson", "Patrick", ""], ["Park", "Eunbyung", ""], ["Kosecka", "Jana", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1702.08318", "submitter": "Xin Jin", "authors": "Xin Jin, Peng Yuan, Xiaodong Li, Chenggen Song, Shiming Ge, Geng Zhao,\n  Yingya Chen", "title": "Efficient Privacy Preserving Viola-Jones Type Object Detection via\n  Random Base Image Representation", "comments": "6 pages, 3 figures, To appear in the proceedings of the IEEE\n  International Conference on Multimedia and Expo (ICME), Jul 10, 2017 - Jul\n  14, 2017, Hong Kong, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cloud server spent a lot of time, energy and money to train a Viola-Jones\ntype object detector with high accuracy. Clients can upload their photos to the\ncloud server to find objects. However, the client does not want the leakage of\nthe content of his/her photos. In the meanwhile, the cloud server is also\nreluctant to leak any parameters of the trained object detectors. 10 years ago,\nAvidan & Butman introduced Blind Vision, which is a method for securely\nevaluating a Viola-Jones type object detector. Blind Vision uses standard\ncryptographic tools and is painfully slow to compute, taking a couple of hours\nto scan a single image. The purpose of this work is to explore an efficient\nmethod that can speed up the process. We propose the Random Base Image (RBI)\nRepresentation. The original image is divided into random base images. Only the\nbase images are submitted randomly to the cloud server. Thus, the content of\nthe image can not be leaked. In the meanwhile, a random vector and the secure\nMillionaire protocol are leveraged to protect the parameters of the trained\nobject detector. The RBI makes the integral-image enable again for the great\nacceleration. The experimental results reveal that our method can retain the\ndetection accuracy of that of the plain vision algorithm and is significantly\nfaster than the traditional blind vision, with only a very low probability of\nthe information leakage theoretically.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 15:16:23 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 14:06:21 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Jin", "Xin", ""], ["Yuan", "Peng", ""], ["Li", "Xiaodong", ""], ["Song", "Chenggen", ""], ["Ge", "Shiming", ""], ["Zhao", "Geng", ""], ["Chen", "Yingya", ""]]}, {"id": "1702.08319", "submitter": "Hanwang Zhang", "authors": "Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua", "title": "Visual Translation Embedding Network for Visual Relation Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relations, such as \"person ride bike\" and \"bike next to car\", offer a\ncomprehensive scene understanding of an image, and have already shown their\ngreat utility in connecting computer vision and natural language. However, due\nto the challenging combinatorial complexity of modeling\nsubject-predicate-object relation triplets, very little work has been done to\nlocalize and predict visual relations. Inspired by the recent advances in\nrelational representation learning of knowledge bases and convolutional object\ndetection networks, we propose a Visual Translation Embedding network (VTransE)\nfor visual relation detection. VTransE places objects in a low-dimensional\nrelation space where a relation can be modeled as a simple vector translation,\ni.e., subject + predicate $\\approx$ object. We propose a novel feature\nextraction layer that enables object-relation knowledge transfer in a\nfully-convolutional fashion that supports training and inference in a single\nforward/backward pass. To the best of our knowledge, VTransE is the first\nend-to-end relation detection network. We demonstrate the effectiveness of\nVTransE over other state-of-the-art methods on two large-scale datasets: Visual\nRelationship and Visual Genome. Note that even though VTransE is a purely\nvisual model, it is still competitive to the Lu's multi-modal model with\nlanguage priors.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 15:16:47 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Zhang", "Hanwang", ""], ["Kyaw", "Zawlin", ""], ["Chang", "Shih-Fu", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1702.08336", "submitter": "Ja-Keoung Koo", "authors": "Byung-Woo Hong, Ja-Keoung Koo, Stefano Soatto", "title": "Multi-Label Segmentation via Residual-Driven Adaptive Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variational multi-label segmentation algorithm based on a robust\nHuber loss for both the data and the regularizer, minimized within a convex\noptimization framework. We introduce a novel constraint on the common areas, to\nbias the solution towards mutually exclusive regions. We also propose a\nregularization scheme that is adapted to the spatial statistics of the residual\nat each iteration, resulting in a varying degree of regularization being\napplied as the algorithm proceeds: the effect of the regularizer is strongest\nat initialization, and wanes as the solution increasingly fits the data. This\nminimizes the bias induced by the regularizer at convergence. We design an\nefficient convex optimization algorithm based on the alternating direction\nmethod of multipliers using the equivalent relation between the Huber function\nand the proximal operator of the one-norm. We empirically validate our proposed\nalgorithm on synthetic and real images and offer an information-theoretic\nderivation of the cost-function that highlights the modeling choices made.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 15:50:35 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Hong", "Byung-Woo", ""], ["Koo", "Ja-Keoung", ""], ["Soatto", "Stefano", ""]]}, {"id": "1702.08379", "submitter": "Paul Jaeger", "authors": "Paul Jaeger, Sebastian Bickelhaupt, Frederik Bernd Laun, Wolfgang\n  Lederer, Daniel Heidi, Tristan Anselm Kuder, Daniel Paech, David Bonekamp,\n  Alexander Radbruch, Stefan Delorme, Heinz-Peter Schlemmer, Franziska Steudle\n  and Klaus H. Maier-Hein", "title": "Revealing Hidden Potentials of the q-Space Signal in Breast Cancer", "comments": "Accepted conference paper at MICCAI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-66182-7_76", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography screening for early detection of breast lesions currently suffers\nfrom high amounts of false positive findings, which result in unnecessary\ninvasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many\nof these false-positive findings prior to biopsy. Current approaches estimate\ntissue properties by means of quantitative parameters taken from generative,\nbiophysical models fit to the q-space encoded signal under certain assumptions\nregarding noise and spatial homogeneity. This process is prone to fitting\ninstability and partial information loss due to model simplicity. We reveal\nunexplored potentials of the signal by integrating all data processing\ncomponents into a convolutional neural network (CNN) architecture that is\ndesigned to propagate clinical target information down to the raw input images.\nThis approach enables simultaneous and target-specific optimization of image\nnormalization, signal exploitation, global representation learning and\nclassification. Using a multicentric data set of 222 patients, we demonstrate\nthat our approach significantly improves clinical decision making with respect\nto the current state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:06:20 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 09:02:54 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 08:14:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Jaeger", "Paul", ""], ["Bickelhaupt", "Sebastian", ""], ["Laun", "Frederik Bernd", ""], ["Lederer", "Wolfgang", ""], ["Heidi", "Daniel", ""], ["Kuder", "Tristan Anselm", ""], ["Paech", "Daniel", ""], ["Bonekamp", "David", ""], ["Radbruch", "Alexander", ""], ["Delorme", "Stefan", ""], ["Schlemmer", "Heinz-Peter", ""], ["Steudle", "Franziska", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1702.08400", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Yoshitaka Ushiku and Tatsuya Harada", "title": "Asymmetric Tri-training for Unsupervised Domain Adaptation", "comments": "TBA on ICML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-layered models trained on a large number of labeled samples boost the\naccuracy of many tasks. It is important to apply such models to different\ndomains because collecting many labeled samples in various domains is\nexpensive. In unsupervised domain adaptation, one needs to train a classifier\nthat works well on a target domain when provided with labeled source samples\nand unlabeled target samples. Although many methods aim to match the\ndistributions of source and target samples, simply matching the distribution\ncannot ensure accuracy on the target domain. To learn discriminative\nrepresentations for the target domain, we assume that artificially labeling\ntarget samples can result in a good representation. Tri-training leverages\nthree classifiers equally to give pseudo-labels to unlabeled samples, but the\nmethod does not assume labeling samples generated from a different domain.In\nthis paper, we propose an asymmetric tri-training method for unsupervised\ndomain adaptation, where we assign pseudo-labels to unlabeled samples and train\nneural networks as if they are true labels. In our work, we use three networks\nasymmetrically. By asymmetric, we mean that two networks are used to label\nunlabeled target samples and one network is trained by the samples to obtain\ntarget-discriminative representations. We evaluate our method on digit\nrecognition and sentiment analysis datasets. Our proposed method achieves\nstate-of-the-art performance on the benchmark digit recognition datasets of\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:48:17 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:11:14 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 05:44:03 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Saito", "Kuniaki", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1702.08423", "submitter": "Zhifei Zhang", "authors": "Zhifei Zhang, Yang Song, Hairong Qi", "title": "Age Progression/Regression by Conditional Adversarial Autoencoder", "comments": "Accepted by The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"If I provide you a face image of mine (without telling you the actual age\nwhen I took the picture) and a large amount of face images that I crawled\n(containing labeled faces of different ages but not necessarily paired), can\nyou show me what I would look like when I am 80 or what I was like when I was\n5?\" The answer is probably a \"No.\" Most existing face aging works attempt to\nlearn the transformation between age groups and thus would require the paired\nsamples as well as the labeled query image. In this paper, we look at the\nproblem from a generative modeling perspective such that no paired samples is\nrequired. In addition, given an unlabeled image, the generative model can\ndirectly produce the image with desired age attribute. We propose a conditional\nadversarial autoencoder (CAAE) that learns a face manifold, traversing on which\nsmooth age progression and regression can be realized simultaneously. In CAAE,\nthe face is first mapped to a latent vector through a convolutional encoder,\nand then the vector is projected to the face manifold conditional on age\nthrough a deconvolutional generator. The latent vector preserves personalized\nface features (i.e., personality) and the age condition controls progression\nvs. regression. Two adversarial networks are imposed on the encoder and\ngenerator, respectively, forcing to generate more photo-realistic faces.\nExperimental results demonstrate the appealing performance and flexibility of\nthe proposed framework by comparing with the state-of-the-art and ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:28:58 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 20:02:15 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Zhang", "Zhifei", ""], ["Song", "Yang", ""], ["Qi", "Hairong", ""]]}, {"id": "1702.08434", "submitter": "Amirreza Mahbod", "authors": "Amirreza Mahbod, Gerald Schaefer, Chunliang Wang, Rupert Ecker,\n  Isabella Ellinger", "title": "Skin Lesion Classification Using Hybrid Deep Neural Networks", "comments": "Accepted for the 44th International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skin cancer is one of the major types of cancers with an increasing incidence\nover the past decades. Accurately diagnosing skin lesions to discriminate\nbetween benign and malignant skin lesions is crucial to ensure appropriate\npatient treatment. While there are many computerised methods for skin lesion\nclassification, convolutional neural networks (CNNs) have been shown to be\nsuperior over classical methods. In this work, we propose a fully automatic\ncomputerised method for skin lesion classification which employs optimised deep\nfeatures from a number of well-established CNNs and from different abstraction\nlevels. We use three pre-trained deep models, namely AlexNet, VGG16 and\nResNet-18, as deep feature generators. The extracted features then are used to\ntrain support vector machine classifiers. In the final stage, the classifier\noutputs are fused to obtain a classification. Evaluated on the 150 validation\nimages from the ISIC 2017 classification challenge, the proposed method is\nshown to achieve very good classification performance, yielding an area under\nreceiver operating characteristic curve of 83.83% for melanoma classification\nand of 97.55% for seborrheic keratosis classification.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:54:41 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 09:39:53 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Mahbod", "Amirreza", ""], ["Schaefer", "Gerald", ""], ["Wang", "Chunliang", ""], ["Ecker", "Rupert", ""], ["Ellinger", "Isabella", ""]]}, {"id": "1702.08481", "submitter": "Nenad Marku\\v{s}", "authors": "Nenad Marku\\v{s}, Ivan Gogi\\'c, Igor S. Pand\\v{z}i\\'c, J\\\"orgen\n  Ahlberg", "title": "Memory-Efficient Global Refinement of Decision-Tree Ensembles and its\n  Application to Face Alignment", "comments": "BMVC Newcastle 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ren et al. recently introduced a method for aggregating multiple decision\ntrees into a strong predictor by interpreting a path taken by a sample down\neach tree as a binary vector and performing linear regression on top of these\nvectors stacked together. They provided experimental evidence that the method\noffers advantages over the usual approaches for combining decision trees\n(random forests and boosting). The method truly shines when the regression\ntarget is a large vector with correlated dimensions, such as a 2D face shape\nrepresented with the positions of several facial landmarks. However, we argue\nthat their basic method is not applicable in many practical scenarios due to\nlarge memory requirements. This paper shows how this issue can be solved\nthrough the use of quantization and architectural changes of the predictor that\nmaps decision tree-derived encodings to the desired output.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:25:53 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 08:49:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Marku\u0161", "Nenad", ""], ["Gogi\u0107", "Ivan", ""], ["Pand\u017ei\u0107", "Igor S.", ""], ["Ahlberg", "J\u00f6rgen", ""]]}, {"id": "1702.08502", "submitter": "Panqu Wang", "authors": "Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou,\n  Garrison Cottrell", "title": "Understanding Convolution for Semantic Segmentation", "comments": "WACV 2018. Updated acknowledgements. Source code:\n  https://github.com/TuSimple/TuSimple-DUC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning, especially deep convolutional neural\nnetworks (CNNs), have led to significant improvement over previous semantic\nsegmentation systems. Here we show how to improve pixel-wise semantic\nsegmentation by manipulating convolution-related operations that are of both\ntheoretical and practical value. First, we design dense upsampling convolution\n(DUC) to generate pixel-level prediction, which is able to capture and decode\nmore detailed information that is generally missing in bilinear upsampling.\nSecond, we propose a hybrid dilated convolution (HDC) framework in the encoding\nphase. This framework 1) effectively enlarges the receptive fields (RF) of the\nnetwork to aggregate global information; 2) alleviates what we call the\n\"gridding issue\" caused by the standard dilated convolution operation. We\nevaluate our approaches thoroughly on the Cityscapes dataset, and achieve a\nstate-of-art result of 80.1% mIOU in the test set at the time of submission. We\nalso have achieved state-of-the-art overall on the KITTI road estimation\nbenchmark and the PASCAL VOC2012 segmentation task. Our source code can be\nfound at https://github.com/TuSimple/TuSimple-DUC .\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 20:05:11 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 01:12:21 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 01:15:23 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Wang", "Panqu", ""], ["Chen", "Pengfei", ""], ["Yuan", "Ye", ""], ["Liu", "Ding", ""], ["Huang", "Zehua", ""], ["Hou", "Xiaodi", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1702.08513", "submitter": "Nizar Massouh", "authors": "Nizar Massouh, Francesca Babiloni, Tatiana Tommasi, Jay Young, Nick\n  Hawes and Barbara Caputo", "title": "Learning Deep Visual Object Models From Noisy Web Data: How to Make it\n  Work", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": "2017 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "doi": "10.1109/IROS.2017.8206444", "report-no": null, "categories": "cs.CV cs.DB cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks thrive when trained on large scale data collections. This has\ngiven ImageNet a central role in the development of deep architectures for\nvisual object classification. However, ImageNet was created during a specific\nperiod in time, and as such it is prone to aging, as well as dataset bias\nissues. Moving beyond fixed training datasets will lead to more robust visual\nsystems, especially when deployed on robots in new environments which must\ntrain on the objects they encounter there. To make this possible, it is\nimportant to break free from the need for manual annotators. Recent work has\nbegun to investigate how to use the massive amount of images available on the\nWeb in place of manual image annotations. We contribute to this research thread\nwith two findings: (1) a study correlating a given level of noisily labels to\nthe expected drop in accuracy, for two deep architectures, on two different\ntypes of noise, that clearly identifies GoogLeNet as a suitable architecture\nfor learning from Web data; (2) a recipe for the creation of Web datasets with\nminimal noise and maximum visual variability, based on a visual and natural\nlanguage processing concept expansion strategy. By combining these two results,\nwe obtain a method for learning powerful deep object models automatically from\nthe Web. We confirm the effectiveness of our approach through object\ncategorization experiments using our Web-derived version of ImageNet on a\npopular robot vision benchmark database, and on a lifelong object discovery\ntask on a mobile robot.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:02:36 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Massouh", "Nizar", ""], ["Babiloni", "Francesca", ""], ["Tommasi", "Tatiana", ""], ["Young", "Jay", ""], ["Hawes", "Nick", ""], ["Caputo", "Barbara", ""]]}, {"id": "1702.08516", "submitter": "Ayan Sinha", "authors": "Ayan Sinha, Justin Lee, Shuai Li, and George Barbastathis", "title": "Lensless computational imaging through deep learning", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been proven to yield reliably generalizable answers to\nnumerous classification and decision tasks. Here, we demonstrate for the first\ntime, to our knowledge, that deep neural networks (DNNs) can be trained to\nsolve inverse problems in computational imaging. We experimentally demonstrate\na lens-less imaging system where a DNN was trained to recover a phase object\ngiven a raw intensity image recorded some distance away.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 20:55:26 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 05:49:50 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Sinha", "Ayan", ""], ["Lee", "Justin", ""], ["Li", "Shuai", ""], ["Barbastathis", "George", ""]]}, {"id": "1702.08534", "submitter": "Laurent Duval", "authors": "Caroline Chaux, Laurent Duval, Jean-Christophe Pesquet", "title": "Image Analysis Using a Dual-Tree $M$-Band Wavelet Transform", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, August 2006, Volume 15,\n  Issue 8, p. 2397-2412", "doi": "10.1109/TIP.2006.875178", "report-no": null, "categories": "physics.data-an cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 2D generalization to the $M$-band case of the dual-tree\ndecomposition structure (initially proposed by N. Kingsbury and further\ninvestigated by I. Selesnick) based on a Hilbert pair of wavelets. We\nparticularly address (\\textit{i}) the construction of the dual basis and\n(\\textit{ii}) the resulting directional analysis. We also revisit the necessary\npre-processing stage in the $M$-band case. While several reconstructions are\npossible because of the redundancy of the representation, we propose a new\noptimal signal reconstruction technique, which minimizes potential estimation\nerrors. The effectiveness of the proposed $M$-band decomposition is\ndemonstrated via denoising comparisons on several image types (natural,\ntexture, seismics), with various $M$-band wavelets and thresholding strategies.\nSignificant improvements in terms of both overall noise reduction and direction\npreservation are observed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 21:15:17 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Chaux", "Caroline", ""], ["Duval", "Laurent", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1702.08558", "submitter": "Ziyan Wu", "authors": "Benjamin Planche, Ziyan Wu, Kai Ma, Shanhui Sun, Stefan Kluckner,\n  Terrence Chen, Andreas Hutter, Sergey Zakharov, Harald Kosch, Jan Ernst", "title": "DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD\n  Models for 2.5D Recognition", "comments": "International Conference on 3D Vision 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in computer vision has been dominated by deep neural networks\ntrained over large amounts of labeled data. Collecting such datasets is however\na tedious, often impossible task; hence a surge in approaches relying solely on\nsynthetic data for their training. For depth images however, discrepancies with\nreal scans still noticeably affect the end performance. We thus propose an\nend-to-end framework which simulates the whole mechanism of these devices,\ngenerating realistic depth data from 3D models by comprehensively modeling\nvital factors e.g. sensor noise, material reflectance, surface geometry. Not\nonly does our solution cover a wider range of sensors and achieve more\nrealistic results than previous methods, assessed through extended evaluation,\nbut we go further by measuring the impact on the training of neural networks\nfor various recognition tasks; demonstrating how our pipeline seamlessly\nintegrates such architectures and consistently enhances their performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:12:25 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 17:34:22 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Planche", "Benjamin", ""], ["Wu", "Ziyan", ""], ["Ma", "Kai", ""], ["Sun", "Shanhui", ""], ["Kluckner", "Stefan", ""], ["Chen", "Terrence", ""], ["Hutter", "Andreas", ""], ["Zakharov", "Sergey", ""], ["Kosch", "Harald", ""], ["Ernst", "Jan", ""]]}, {"id": "1702.08597", "submitter": "Sheng Li Dr.", "authors": "Sheng Li and Jongsoo Park and Ping Tak Peter Tang", "title": "Enabling Sparse Winograd Convolution by Native Pruning", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse methods and the use of Winograd convolutions are two orthogonal\napproaches, each of which significantly accelerates convolution computations in\nmodern CNNs. Sparse Winograd merges these two and thus has the potential to\noffer a combined performance benefit. Nevertheless, training convolution layers\nso that the resulting Winograd kernels are sparse has not hitherto been very\nsuccessful. By introducing a Winograd layer in place of a standard convolution\nlayer, we can learn and prune Winograd coefficients \"natively\" and obtain\nsparsity level beyond 90% with only 0.1% accuracy loss with AlexNet on ImageNet\ndataset. Furthermore, we present a sparse Winograd convolution algorithm and\nimplementation that exploits the sparsity, achieving up to 31.7 effective\nTFLOP/s in 32-bit precision on a latest Intel Xeon CPU, which corresponds to a\n5.4x speedup over a state-of-the-art dense convolution implementation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 01:37:57 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 18:10:39 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Li", "Sheng", ""], ["Park", "Jongsoo", ""], ["Tang", "Ping Tak Peter", ""]]}, {"id": "1702.08601", "submitter": "Siyu Zhu", "authors": "Siyu Zhu and Tianwei Shen and Lei Zhou and Runze Zhang and Jinglu Wang\n  and Tian Fang and Long Quan", "title": "Parallel Structure from Motion from Local Increment to Global Averaging", "comments": "Under review at the International Conference on Computer Vision\n  (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the accurate and consistent Structure from Motion\n(SfM) problem, in particular camera registration, far exceeding the memory of a\nsingle computer in parallel. Different from the previous methods which\ndrastically simplify the parameters of SfM and sacrifice the accuracy of the\nfinal reconstruction, we try to preserve the connectivities among cameras by\nproposing a camera clustering algorithm to divide a large SfM problem into\nsmaller sub-problems in terms of camera clusters with overlapping. We then\nexploit a hybrid formulation that applies the relative poses from local\nincremental SfM into a global motion averaging framework and produce accurate\nand consistent global camera poses. Our scalable formulation in terms of camera\nclusters is highly applicable to the whole SfM pipeline including track\ngeneration, local SfM, 3D point triangulation and bundle adjustment. We are\neven able to reconstruct the camera poses of a city-scale data-set containing\nmore than one million high-resolution images with superior accuracy and\nrobustness evaluated on benchmark, Internet, and sequential data-sets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 01:46:51 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 07:08:49 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 03:05:15 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhu", "Siyu", ""], ["Shen", "Tianwei", ""], ["Zhou", "Lei", ""], ["Zhang", "Runze", ""], ["Wang", "Jinglu", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1702.08606", "submitter": "Yuncong Chen", "authors": "Yuncong Chen, Lauren McElvain, Alex Tolpygo, Daniel Ferrante, Harvey\n  Karten, Partha Mitra, David Kleinfeld, Yoav Freund", "title": "The Active Atlas: Combining 3D Anatomical Models with Texture Detectors", "comments": "8 pages, 10 figures, appeared in proceeding of MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern imaging technologies such as fMRI have opened exciting new\npossibilities for studying the brain in vivo, histological sections remain the\nbest way to study the anatomy of the brain at the level of single neurons. The\nhistological atlas changed little since 1909 and localizing brain regions is a\nstill a labor intensive process performed only by experienced neuro-anatomists.\nExisting digital atlases such as the Allen Brain atlas are limited to low\nresolution images which cannot identify the detailed structure of the neurons.\nWe have developed a digital atlas methodology that combines information about\nthe 3D organization of the brain and the detailed texture of neurons in\ndifferent structures. Using the methodology we developed an atlas for the mouse\nbrainstem and mid-brain, two regions for which there are currently no good\natlases. Our atlas is \"active\" in that it can be used to automatically align a\nhistological stack to the atlas, thus reducing the work of the neuroanatomist.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 02:18:47 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 23:55:35 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 18:33:24 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chen", "Yuncong", ""], ["McElvain", "Lauren", ""], ["Tolpygo", "Alex", ""], ["Ferrante", "Daniel", ""], ["Karten", "Harvey", ""], ["Mitra", "Partha", ""], ["Kleinfeld", "David", ""], ["Freund", "Yoav", ""]]}, {"id": "1702.08626", "submitter": "Ahmed Qureshi", "authors": "Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi\n  Ishiguro", "title": "Show, Attend and Interact: Perceivable Human-Robot Social Interaction\n  through Neural Attention Q-Network", "comments": "7 pages, 5 figures, accepted by IEEE-RAS ICRA'17", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989193", "report-no": null, "categories": "cs.RO cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a safe, natural and effective human-robot social interaction, it is\nessential to develop a system that allows a robot to demonstrate the\nperceivable responsive behaviors to complex human behaviors. We introduce the\nMultimodal Deep Attention Recurrent Q-Network using which the robot exhibits\nhuman-like social interaction skills after 14 days of interacting with people\nin an uncontrolled real world. Each and every day during the 14 days, the\nsystem gathered robot interaction experiences with people through a\nhit-and-trial method and then trained the MDARQN on these experiences using\nend-to-end reinforcement learning approach. The results of interaction based\nlearning indicate that the robot has learned to respond to complex human\nbehaviors in a perceivable and socially acceptable manner.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:16:40 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Qureshi", "Ahmed Hussain", ""], ["Nakamura", "Yutaka", ""], ["Yoshikawa", "Yuichiro", ""], ["Ishiguro", "Hiroshi", ""]]}, {"id": "1702.08627", "submitter": "Xiaoliang Song", "authors": "Yiyang Wang, Risheng Liu, Xiaoliang Song, Zhixun Su", "title": "An Optimization Framework with Flexible Inexact Inner Iterations for\n  Nonconvex and Nonsmooth Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, numerous vision and learning tasks have been (re)formulated\nas nonconvex and nonsmooth programmings(NNPs). Although some algorithms have\nbeen proposed for particular problems, designing fast and flexible optimization\nschemes with theoretical guarantee is a challenging task for general NNPs. It\nhas been investigated that performing inexact inner iterations often benefit to\nspecial applications case by case, but their convergence behaviors are still\nunclear. Motivated by these practical experiences, this paper designs a novel\nalgorithmic framework, named inexact proximal alternating direction method\n(IPAD) for solving general NNPs. We demonstrate that any numerical algorithms\ncan be incorporated into IPAD for solving subproblems and the convergence of\nthe resulting hybrid schemes can be consistently guaranteed by a series of\nsimple error conditions. Beyond the guarantee in theory, numerical experiments\non both synthesized and real-world data further demonstrate the superiority and\nflexibility of our IPAD framework for practical use.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:22:55 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 09:04:41 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 02:28:54 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Wang", "Yiyang", ""], ["Liu", "Risheng", ""], ["Song", "Xiaoliang", ""], ["Su", "Zhixun", ""]]}, {"id": "1702.08634", "submitter": "Wenguan Wang", "authors": "Wenguan Wang and Jianbing Shen and Jianwen Xie and Fatih Porikli", "title": "Super-Trajectory for Video Segmentation", "comments": "This paper has been published in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel semi-supervised video segmentation approach based on an\nefficient video representation, called as \"super-trajectory\". Each\nsuper-trajectory corresponds to a group of compact trajectories that exhibit\nconsistent motion patterns, similar appearance and close spatiotemporal\nrelationships. We generate trajectories using a probabilistic model, which\nhandles occlusions and drifts in a robust and natural way. To reliably group\ntrajectories, we adopt a modified version of the density peaks based clustering\nalgorithm that allows capturing rich spatiotemporal relations among\ntrajectories in the clustering process. The presented video representation is\ndiscriminative enough to accurately propagate the initial annotations in the\nfirst frame onto the remaining video frames. Extensive experimental analysis on\nchallenging benchmarks demonstrate our method is capable of distinguishing the\ntarget objects from complex backgrounds and even reidentifying them after\nlong-term occlusions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:51:42 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 17:52:44 GMT"}, {"version": "v3", "created": "Fri, 17 Mar 2017 05:50:48 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 01:14:51 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""], ["Xie", "Jianwen", ""], ["Porikli", "Fatih", ""]]}, {"id": "1702.08640", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Jianbing Shen and Fatih Porikli", "title": "Selective Video Object Cutout", "comments": "W. Wang, J. Shen, and F. Porikli. \"Selective video object cutout.\"\n  IEEE Transactions on Image Processing 26.12 (2017): 5645-5655", "journal-ref": "IEEE Transactions on Image Processing, Vol. 26, No. 12, pp\n  5645-5655, 2017", "doi": "10.1109/TIP.2017.2745098", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional video segmentation approaches rely heavily on appearance models.\nSuch methods often use appearance descriptors that have limited discriminative\npower under complex scenarios. To improve the segmentation performance, this\npaper presents a pyramid histogram based confidence map that incorporates\nstructure information into appearance statistics. It also combines geodesic\ndistance based dynamic models. Then, it employs an efficient measure of\nuncertainty propagation using local classifiers to determine the image regions\nwhere the object labels might be ambiguous. The final foreground cutout is\nobtained by refining on the uncertain regions. Additionally, to reduce manual\nlabeling, our method determines the frames to be labeled by the human operator\nin a principled manner, which further boosts the segmentation performance and\nminimizes the labeling effort. Our extensive experimental analyses on two big\nbenchmarks demonstrate that our solution achieves superior performance,\nfavorable computational efficiency, and reduced manual labeling in comparison\nto the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 04:33:00 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 04:22:32 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 03:49:30 GMT"}, {"version": "v4", "created": "Fri, 8 Dec 2017 23:16:37 GMT"}, {"version": "v5", "created": "Thu, 22 Mar 2018 23:01:47 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""], ["Porikli", "Fatih", ""]]}, {"id": "1702.08646", "submitter": "Peng Lei", "authors": "Peng Lei, Fuxin Li, Sinisa Todorovic", "title": "Boundary Flow: A Siamese Network that Predicts Boundary Motion without\n  Training on Motion", "comments": "To appear at CVPR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep learning, this paper addresses the problem of joint object\nboundary detection and boundary motion estimation in videos, which we named\nboundary flow estimation. Boundary flow is an important mid-level visual cue as\nboundaries characterize objects spatial extents, and the flow indicates objects\nmotions and interactions. Yet, most prior work on motion estimation has focused\non dense object motion or feature points that may not necessarily reside on\nboundaries. For boundary flow estimation, we specify a new fully convolutional\nSiamese network (FCSN) that jointly estimates object-level boundaries in two\nconsecutive frames. Boundary correspondences in the two frames are predicted by\nthe same FCSN with a new, unconventional deconvolution approach. Finally, the\nboundary flow estimate is improved with an edgelet-based filtering. Evaluation\nis conducted on three tasks: boundary detection in videos, boundary flow\nestimation, and optical flow estimation. On boundary detection, we achieve the\nstate-of-the-art performance on the benchmark VSB100 dataset. On boundary flow\nestimation, we present the first results on the Sintel training dataset. For\noptical flow estimation, we run the recent approach CPMFlow but on the\naugmented input with our boundary-flow matches, and achieve significant\nperformance improvement on the Sintel benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 04:57:12 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 03:45:43 GMT"}, {"version": "v3", "created": "Sun, 8 Apr 2018 07:00:11 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Lei", "Peng", ""], ["Li", "Fuxin", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "1702.08652", "submitter": "Pichao Wang", "authors": "Pichao Wang and Wanqing Li and Zhimin Gao and Yuyao Zhang and Chang\n  Tang and Philip Ogunbona", "title": "Scene Flow to Action Map: A New Representation for RGB-D based Action\n  Recognition with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow describes the motion of 3D objects in real world and potentially\ncould be the basis of a good feature for 3D action recognition. However, its\nuse for action recognition, especially in the context of convolutional neural\nnetworks (ConvNets), has not been previously studied. In this paper, we propose\nthe extraction and use of scene flow for action recognition from RGB-D data.\nPrevious works have considered the depth and RGB modalities as separate\nchannels and extract features for later fusion. We take a different approach\nand consider the modalities as one entity, thus allowing feature extraction for\naction recognition at the beginning. Two key questions about the use of scene\nflow for action recognition are addressed: how to organize the scene flow\nvectors and how to represent the long term dynamics of videos based on scene\nflow. In order to calculate the scene flow correctly on the available datasets,\nwe propose an effective self-calibration method to align the RGB and depth data\nspatially without knowledge of the camera parameters. Based on the scene flow\nvectors, we propose a new representation, namely, Scene Flow to Action Map\n(SFAM), that describes several long term spatio-temporal dynamics for action\nrecognition. We adopt a channel transform kernel to transform the scene flow\nvectors to an optimal color space analogous to RGB. This transformation takes\nbetter advantage of the trained ConvNets models over ImageNet. Experimental\nresults indicate that this new representation can surpass the performance of\nstate-of-the-art methods on two large public datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 05:39:25 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 07:33:51 GMT"}, {"version": "v3", "created": "Mon, 27 Mar 2017 00:52:21 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Gao", "Zhimin", ""], ["Zhang", "Yuyao", ""], ["Tang", "Chang", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1702.08675", "submitter": "Yuan Gan", "authors": "Pengyu Wang, Yuan Gan, Panpan Shui, Fenggen Yu, Yan Zhang, Songle\n  Chen, Zhengxing Sun", "title": "3D Shape Segmentation via Shape Fully Convolutional Networks", "comments": "We update some missing references about intrinsic CNNs (2018.5.24)", "journal-ref": "Computer & Graphics 70 (2),(2018), p. 128-139", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We desgin a novel fully convolutional network architecture for shapes,\ndenoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are represented\nas graph structures in the SFCN architecture, based on novel graph convolution\nand pooling operations, which are similar to convolution and pooling operations\nused on images. Meanwhile, to build our SFCN architecture in the original image\nsegmentation fully convolutional network (FCN) architecture, we also design and\nimplement a generating operation} with bridging function. This ensures that the\nconvolution and pooling operation we have designed can be successfully applied\nin the original FCN architecture. In this paper, we also present a new shape\nsegmentation approach based on SFCN. Furthermore, we allow more general and\nchallenging input, such as mixed datasets of different categories of shapes}\nwhich can prove the ability of our generalisation. In our approach, SFCNs are\ntrained triangles-to-triangles by using three low-level geometric features as\ninput. Finally, the feature voting-based multi-label graph cuts is adopted to\noptimise the segmentation results obtained by SFCN prediction. The experiment\nresults show that our method can effectively learn and predict mixed shape\ndatasets of either similar or different characteristics, and achieve excellent\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 07:26:55 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 08:39:05 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 01:45:21 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wang", "Pengyu", ""], ["Gan", "Yuan", ""], ["Shui", "Panpan", ""], ["Yu", "Fenggen", ""], ["Zhang", "Yan", ""], ["Chen", "Songle", ""], ["Sun", "Zhengxing", ""]]}, {"id": "1702.08681", "submitter": "Hao Yang Dr", "authors": "Hao Yang, Joey Tianyi Zhou, Jianfei Cai and Yew Soon Ong", "title": "MIML-FCN+: Multi-instance Multi-label Learning via Fully Convolutional\n  Networks with Privileged Information", "comments": "Accepted in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance multi-label (MIML) learning has many interesting applications\nin computer visions, including multi-object recognition and automatic image\ntagging. In these applications, additional information such as bounding-boxes,\nimage captions and descriptions is often available during training phrase,\nwhich is referred as privileged information (PI). However, as existing works on\nlearning using PI only consider instance-level PI (privileged instances), they\nfail to make use of bag-level PI (privileged bags) available in MIML learning.\nTherefore, in this paper, we propose a two-stream fully convolutional network,\nnamed MIML-FCN+, unified by a novel PI loss to solve the problem of MIML\nlearning with privileged bags. Compared to the previous works on PI, the\nproposed MIML-FCN+ utilizes the readily available privileged bags, instead of\nhard-to-obtain privileged instances, making the system more general and\npractical in real world applications. As the proposed PI loss is convex and SGD\ncompatible and the framework itself is a fully convolutional network, MIML-FCN+\ncan be easily integrated with state of-the-art deep learning networks.\nMoreover, the flexibility of convolutional layers allows us to exploit\nstructured correlations among instances to facilitate more effective training\nand testing. Experimental results on three benchmark datasets demonstrate the\neffectiveness of the proposed MIML-FCN+, outperforming state-of-the-art methods\nin the application of multi-object recognition.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 07:54:22 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Yang", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Cai", "Jianfei", ""], ["Ong", "Yew Soon", ""]]}, {"id": "1702.08690", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Yizhou Yu", "title": "Borrowing Treasures from the Wealthy: Deep Transfer Learning through\n  Selective Joint Fine-tuning", "comments": "To appear in 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks require a large amount of labeled training data during\nsupervised learning. However, collecting and labeling so much data might be\ninfeasible in many cases. In this paper, we introduce a source-target selective\njoint fine-tuning scheme for improving the performance of deep learning tasks\nwith insufficient training data. In this scheme, a target learning task with\ninsufficient training data is carried out simultaneously with another source\nlearning task with abundant training data. However, the source learning task\ndoes not use all existing training data. Our core idea is to identify and use a\nsubset of training images from the original source learning task whose\nlow-level characteristics are similar to those from the target learning task,\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\nwe compute descriptors from linear or nonlinear filter bank responses on\ntraining images from both tasks, and use such descriptors to search for a\ndesired subset of training samples for the source learning task.\n  Experiments demonstrate that our selective joint fine-tuning scheme achieves\nstate-of-the-art performance on multiple visual classification tasks with\ninsufficient training data for deep learning. Such tasks include Caltech 256,\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\nfine-tuning without a source domain, the proposed method can improve the\nclassification accuracy by 2% - 10% using a single model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:40:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 11:51:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ge", "Weifeng", ""], ["Yu", "Yizhou", ""]]}, {"id": "1702.08692", "submitter": "Long Chen", "authors": "Long Chen, Junyu Dong, ShengKe Wang, Kin-Man Lam, Muwei Jian, Hua\n  Zhang, XiaoChun Cao", "title": "Cascade one-vs-rest detection network for fine-grained recognition\n  without part annotations", "comments": "Part of authors has changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained recognition is a challenging task due to the small\nintra-category variances. Most of top-performing fine-grained recognition\nmethods leverage parts of objects for better performance. Therefore, part\nannotations which are extremely computationally expensive are required. In this\npaper, we propose a novel cascaded deep CNN detection framework for\nfine-grained recognition which is trained to detect the whole object without\nconsidering parts. Nevertheless, most of current top-performing detection\nnetworks use the N+1 class (N object categories plus background) softmax loss,\nand the background category with much more training samples dominates the\nfeature learning progress so that the features are not good for object\ncategories with fewer samples. To bridge this gap, we introduce a cascaded\nstructure to eliminate background and exploit a one-vs-rest loss to capture\nmore minute variances among different subordinate categories. Experiments show\nthat our proposed recognition framework achieves comparable performance with\nstate-of-the-art, part-free, fine-grained recognition methods on the\nCUB-200-2011 Bird dataset. Moreover, our method even outperforms most of\npart-based methods while does not need part annotations at the training stage\nand is free from any annotations at test stage.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:45:15 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 07:05:42 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Chen", "Long", ""], ["Dong", "Junyu", ""], ["Wang", "ShengKe", ""], ["Lam", "Kin-Man", ""], ["Jian", "Muwei", ""], ["Zhang", "Hua", ""], ["Cao", "XiaoChun", ""]]}, {"id": "1702.08699", "submitter": "Hongdiao Wen", "authors": "Hongdiao Wen", "title": "II-FCN for skin lesion analysis towards melanoma detection", "comments": "4 page abstract about our solution to the challenge of Lesion\n  Segmentation in ISIC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dermoscopy image detection stays a tough task due to the weak distinguishable\nproperty of the object.Although the deep convolution neural network\nsignifigantly boosted the performance on prevelance computer vision tasks in\nrecent years,there remains a room to explore more robust and precise models to\nthe problem of low contrast image segmentation.Towards the challenge of Lesion\nSegmentation in ISBI 2017,we built a symmetrical identity inception fully\nconvolution network which is based on only 10 reversible inception blocks,every\nblock composed of four convolution branches with combination of different layer\ndepth and kernel size to extract sundry semantic features.Then we proposed an\napproximate loss function for jaccard index metrics to train our model.To\novercome the drawbacks of traditional convolution,we adopted the dilation\nconvolution and conditional random field method to rectify our segmentation.We\nalso introduced multiple ways to prevent the problem of overfitting.The\nexperimental results shows that our model achived jaccard index of 0.82 and\nkept learning from epoch to epoch.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:56:02 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 13:40:27 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Wen", "Hongdiao", ""]]}, {"id": "1702.08717", "submitter": "Wiselin Jiji G", "authors": "G Wiselin Jiji, P Johnson Durai Raj", "title": "An Extensive Technique to Detect and Analyze Melanoma: A Challenge at\n  the International Symposium on Biomedical Imaging (ISBI) 2017", "comments": "4 Page Abstract for ISIC2017 Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An automated method to detect and analyze the melanoma is presented to\nimprove diagnosis which will leads to the exact treatment. Image processing\ntechniques such as segmentation, feature descriptors and classification models\nare involved in this method. In the First phase the lesion region is segmented\nusing CIELAB Color space Based Segmentation. Then feature descriptors such as\nshape, color and texture are extracted. Finally, in the third phase lesion\nregion is classified as melanoma, seborrheic keratosis or nevus using multi\nclass O-A SVM model. Experiment with ISIC 2017 Archive skin image database has\nbeen done and analyzed the results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 09:45:32 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Jiji", "G Wiselin", ""], ["Raj", "P Johnson Durai", ""]]}, {"id": "1702.08734", "submitter": "Matthijs Douze", "authors": "Jeff Johnson and Matthijs Douze and Herv\\'e J\\'egou", "title": "Billion-scale similarity search with GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:42:31 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Johnson", "Jeff", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1702.08740", "submitter": "Ziang Yan", "authors": "Ziang Yan, Jian Liang, Weishen Pan, Jin Li, Changshui Zhang", "title": "Weakly- and Semi-Supervised Object Detection with\n  Expectation-Maximization Algorithm", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection when provided image-level labels instead of instance-level\nlabels (i.e., bounding boxes) during training is an important problem in\ncomputer vision, since large scale image datasets with instance-level labels\nare extremely costly to obtain. In this paper, we address this challenging\nproblem by developing an Expectation-Maximization (EM) based object detection\nmethod using deep convolutional neural networks (CNNs). Our method is\napplicable to both the weakly-supervised and semi-supervised settings.\nExtensive experiments on PASCAL VOC 2007 benchmark show that (1) in the weakly\nsupervised setting, our method provides significant detection performance\nimprovement over current state-of-the-art methods, (2) having access to a small\nnumber of strongly (instance-level) annotated images, our method can almost\nmatch the performace of the fully supervised Fast RCNN. We share our source\ncode at https://github.com/ZiangYan/EM-WSD.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 11:03:39 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Yan", "Ziang", ""], ["Liang", "Jian", ""], ["Pan", "Weishen", ""], ["Li", "Jin", ""], ["Zhang", "Changshui", ""]]}, {"id": "1702.08780", "submitter": "Lei Han", "authors": "Lei Han, Lu Fang", "title": "MILD: Multi-Index hashing for Loop closure Detection", "comments": "6 pages, 5 figures; accepted by IEEE ICME 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop Closure Detection (LCD) has been proved to be extremely useful in global\nconsistent visual Simultaneously Localization and Mapping (SLAM) and\nappearance-based robot relocalization. Methods exploiting binary features in\nbag of words representation have recently gained a lot of popularity for their\nefficiency, but suffer from low recall due to the inherent drawback that high\ndimensional binary feature descriptors lack well-defined centroids. In this\npaper, we propose a realtime LCD approach called MILD (Multi-Index Hashing for\nLoop closure Detection), in which image similarity is measured by feature\nmatching directly to achieve high recall without introducing extra\ncomputational complexity with the aid of Multi-Index Hashing (MIH). A\ntheoretical analysis of the approximate image similarity measurement using MIH\nis presented, which reveals the trade-off between efficiency and accuracy from\na probabilistic perspective. Extensive comparisons with state-of-the-art LCD\nmethods demonstrate the superiority of MILD in both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 13:30:06 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Han", "Lei", ""], ["Fang", "Lu", ""]]}, {"id": "1702.08782", "submitter": "Alexandre Boulch", "authors": "Alexandre Boulch", "title": "ShaResNet: reducing residual network parameter number by sharing weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Residual Networks have reached the state of the art in many image\nprocessing tasks such image classification. However, the cost for a gain in\naccuracy in terms of depth and memory is prohibitive as it requires a higher\nnumber of residual blocks, up to double the initial value. To tackle this\nproblem, we propose in this paper a way to reduce the redundant information of\nthe networks. We share the weights of convolutional layers between residual\nblocks operating at the same spatial scale. The signal flows multiple times in\nthe same convolutional layer. The resulting architecture, called ShaResNet,\ncontains block specific layers and shared layers. These ShaResNet are trained\nexactly in the same fashion as the commonly used residual networks. We show, on\nthe one hand, that they are almost as efficient as their sequential\ncounterparts while involving less parameters, and on the other hand that they\nare more efficient than a residual network with the same number of parameters.\nFor example, a 152-layer-deep residual network can be reduced to 106\nconvolutional layers, i.e. a parameter gain of 39\\%, while loosing less than\n0.2\\% accuracy on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 13:37:59 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 13:49:15 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Boulch", "Alexandre", ""]]}, {"id": "1702.08798", "submitter": "Shanshan Huang", "authors": "Shanshan Huang, Yichao Xiong, Ya Zhang and Jia Wang", "title": "Unsupervised Triplet Hashing for Fast Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has played a pivotal role in large-scale image retrieval. With the\ndevelopment of Convolutional Neural Network (CNN), hashing learning has shown\ngreat promise. But existing methods are mostly tuned for classification, which\nare not optimized for retrieval tasks, especially for instance-level retrieval.\nIn this study, we propose a novel hashing method for large-scale image\nretrieval. Considering the difficulty in obtaining labeled datasets for image\nretrieval task in large scale, we propose a novel CNN-based unsupervised\nhashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised\nhashing network is designed under the following three principles: 1) more\ndiscriminative representations for image retrieval; 2) minimum quantization\nloss between the original real-valued feature descriptors and the learned hash\ncodes; 3) maximum information entropy for the learned hash codes. Extensive\nexperiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH\noutperforms several state-of-the-art unsupervised hashing methods in terms of\nretrieval accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 14:26:14 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Huang", "Shanshan", ""], ["Xiong", "Yichao", ""], ["Zhang", "Ya", ""], ["Wang", "Jia", ""]]}, {"id": "1702.08891", "submitter": "Bernhard Kainz", "authors": "Benjamin Hou, Amir Alansary, Steven McDonagh, Alice Davidson, Mary\n  Rutherford, Jo V. Hajnal, Daniel Rueckert, Ben Glocker and Bernhard Kainz", "title": "Predicting Slice-to-Volume Transformation in Presence of Arbitrary\n  Subject Motion", "comments": "8 pages, 4 figures, 6 pages supplemental material, currently under\n  review for MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to solve a fundamental problem in intensity-based 2D/3D\nregistration, which concerns the limited capture range and need for very good\ninitialization of state-of-the-art image registration methods. We propose a\nregression approach that learns to predict rotation and translations of\narbitrary 2D image slices from 3D volumes, with respect to a learned canonical\natlas co-ordinate system. To this end, we utilize Convolutional Neural Networks\n(CNNs) to learn the highly complex regression function that maps 2D image\nslices into their correct position and orientation in 3D space. Our approach is\nattractive in challenging imaging scenarios, where significant subject motion\ncomplicates reconstruction performance of 3D volumes from 2D slice data. We\nextensively evaluate the effectiveness of our approach quantitatively on\nsimulated MRI brain data with extreme random motion. We further demonstrate\nqualitative results on fetal MRI where our method is integrated into a full\nreconstruction and motion compensation pipeline. With our CNN regression\napproach we obtain an average prediction error of 7mm on simulated data, and\nconvincing reconstruction quality of images of very young fetuses where\nprevious methods fail. We further discuss applications to Computed Tomography\nand X-ray projections. Our approach is a general solution to the 2D/3D\ninitialization problem. It is computationally efficient, with prediction times\nper slice of a few milliseconds, making it suitable for real-time scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:06:14 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 12:32:52 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Hou", "Benjamin", ""], ["Alansary", "Amir", ""], ["McDonagh", "Steven", ""], ["Davidson", "Alice", ""], ["Rutherford", "Mary", ""], ["Hajnal", "Jo V.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Kainz", "Bernhard", ""]]}]