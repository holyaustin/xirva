[{"id": "1006.1187", "submitter": "Secretary Aircc Journal", "authors": "S. V. Sheela and K. R. Radhika (B. M. S. College of Engineering -\n  Bangalore, India)", "title": "Biometric Authentication using Nonparametric Methods", "comments": "20 pages", "journal-ref": "International Journal of Computer Science and Information\n  Technology 2.3 (2010) 114-133", "doi": "10.5121/ijcsit.2010.2309", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The physiological and behavioral trait is employed to develop biometric\nauthentication systems. The proposed work deals with the authentication of iris\nand signature based on minimum variance criteria. The iris patterns are\npreprocessed based on area of the connected components. The segmented image\nused for authentication consists of the region with large variations in the\ngray level values. The image region is split into quadtree components. The\ncomponents with minimum variance are determined from the training samples. Hu\nmoments are applied on the components. The summation of moment values\ncorresponding to minimum variance components are provided as input vector to\nk-means and fuzzy kmeans classifiers. The best performance was obtained for MMU\ndatabase consisting of 45 subjects. The number of subjects with zero False\nRejection Rate [FRR] was 44 and number of subjects with zero False Acceptance\nRate [FAR] was 45. This paper addresses the computational load reduction in\noff-line signature verification based on minimal features using k-means, fuzzy\nk-means, k-nn, fuzzy k-nn and novel average-max approaches. FRR of 8.13% and\nFAR of 10% was achieved using k-nn classifier. The signature is a biometric,\nwhere variations in a genuine case, is a natural expectation. In the genuine\nsignature, certain parts of signature vary from one instance to another. The\nsystem aims to provide simple, fast and robust system using less number of\nfeatures when compared to state of art works.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 07:15:37 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Sheela", "S. V.", "", "B. M. S. College of Engineering -\n  Bangalore, India"], ["Radhika", "K. R.", "", "B. M. S. College of Engineering -\n  Bangalore, India"]]}, {"id": "1006.1346", "submitter": "Ignacio Ramirez", "authors": "Pablo Sprechmann, Ignacio Ram\\'irez, Guillermo Sapiro, Yonina Eldar", "title": "C-HiLasso: A Collaborative Hierarchical Sparse Modeling Framework", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2157912", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse modeling is a powerful framework for data analysis and processing.\nTraditionally, encoding in this framework is performed by solving an\nL1-regularized linear regression problem, commonly referred to as Lasso or\nBasis Pursuit. In this work we combine the sparsity-inducing property of the\nLasso model at the individual feature level, with the block-sparsity property\nof the Group Lasso model, where sparse groups of features are jointly encoded,\nobtaining a sparsity pattern hierarchically structured. This results in the\nHierarchical Lasso (HiLasso), which shows important practical modeling\nadvantages. We then extend this approach to the collaborative case, where a set\nof simultaneously coded signals share the same sparsity pattern at the higher\n(group) level, but not necessarily at the lower (inside the group) level,\nobtaining the collaborative HiLasso model (C-HiLasso). Such signals then share\nthe same active groups, or classes, but not necessarily the same active set.\nThis model is very well suited for applications such as source identification\nand separation. An efficient optimization procedure, which guarantees\nconvergence to the global optimum, is developed for these new models. The\nunderlying presentation of the new framework and optimization approach is\ncomplemented with experimental examples and theoretical results regarding\nrecovery guarantees for the proposed models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 19:46:56 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2011 11:58:16 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Sprechmann", "Pablo", ""], ["Ram\u00edrez", "Ignacio", ""], ["Sapiro", "Guillermo", ""], ["Eldar", "Yonina", ""]]}, {"id": "1006.2368", "submitter": "Oleg Pianykh", "authors": "Oleg Pianykh", "title": "L2-optimal image interpolation and its applications to medical imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital medical images are always displayed scaled to fit particular view.\nInterpolation is responsible for this scaling, and if not done properly, can\nsignificantly degrade diagnostic image quality. However, theoretically-optimal\ninterpolation algorithms may also be the most time-consuming and impractical.\nWe propose a new approach, adapted to the needs of digital medical imaging, to\ncombine high interpolation speed and superior L2-optimal image quality.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 19:05:05 GMT"}], "update_date": "2010-06-14", "authors_parsed": [["Pianykh", "Oleg", ""]]}, {"id": "1006.2700", "submitter": "Sheng Xu", "authors": "Robert Sheng Xu, Oleg Michailovich, Magdy Salama", "title": "Image Segmentation Using Weak Shape Priors", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of image segmentation is known to become particularly challenging\nin the case of partial occlusion of the object(s) of interest, background\nclutter, and the presence of strong noise. To overcome this problem, the\npresent paper introduces a novel approach segmentation through the use of\n\"weak\" shape priors. Specifically, in the proposed method, an segmenting active\ncontour is constrained to converge to a configuration at which its geometric\nparameters attain their empirical probability densities closely matching the\ncorresponding model densities that are learned based on training samples. It is\nshown through numerical experiments that the proposed shape modeling can be\nregarded as \"weak\" in the sense that it minimally influences the segmentation,\nwhich is allowed to be dominated by data-related forces. On the other hand, the\npriors provide sufficient constraints to regularize the convergence of\nsegmentation, while requiring substantially smaller training sets to yield less\nbiased results as compared to the case of PCA-based regularization methods. The\nmain advantages of the proposed technique over some existing alternatives is\ndemonstrated in a series of experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 12:43:37 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Xu", "Robert Sheng", ""], ["Michailovich", "Oleg", ""], ["Salama", "Magdy", ""]]}, {"id": "1006.2734", "submitter": "Ariel Baya", "authors": "Ariel E. Baya and Pablo M. Granitto", "title": "Penalized K-Nearest-Neighbor-Graph Based Metrics for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A difficult problem in clustering is how to handle data with a manifold\nstructure, i.e. data that is not shaped in the form of compact clouds of\npoints, forming arbitrary shapes or paths embedded in a high-dimensional space.\nIn this work we introduce the Penalized k-Nearest-Neighbor-Graph (PKNNG) based\nmetric, a new tool for evaluating distances in such cases. The new metric can\nbe used in combination with most clustering algorithms. The PKNNG metric is\nbased on a two-step procedure: first it constructs the k-Nearest-Neighbor-Graph\nof the dataset of interest using a low k-value and then it adds edges with an\nexponentially penalized weight for connecting the sub-graphs produced by the\nfirst step. We discuss several possible schemes for connecting the different\nsub-graphs. We use three artificial datasets in four different embedding\nsituations to evaluate the behavior of the new metric, including a comparison\namong different clustering methods. We also evaluate the new metric in a real\nworld application, clustering the MNIST digits dataset. In all cases the PKNNG\nmetric shows promising clustering results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 15:07:45 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Baya", "Ariel E.", ""], ["Granitto", "Pablo M.", ""]]}, {"id": "1006.2804", "submitter": "Jenny Blight", "authors": "Minakshi Gogoi and D K Bhattacharyya", "title": "An Effective Fingerprint Verification Technique", "comments": "Submitted to Journal of Computer Science and Engineering, see\n  http://sites.google.com/site/jcseuk/volume-1-issue-1-may-2010", "journal-ref": "Journal of Computer Science and Engineering, Volume 1, Issue 1,\n  p27-35, May 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective method for fingerprint verification based on\na data mining technique called minutiae clustering and a graph-theoretic\napproach to analyze the process of fingerprint comparison to give a feature\nspace representation of minutiae and to produce a lower bound on the number of\ndetectably distinct fingerprints. The method also proving the invariance of\neach individual fingerprint by using both the topological behavior of the\nminutiae graph and also using a distance measure called Hausdorff distance.The\nmethod provides a graph based index generation mechanism of fingerprint\nbiometric data. The self-organizing map neural network is also used for\nclassifying the fingerprints.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 18:57:35 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Gogoi", "Minakshi", ""], ["Bhattacharyya", "D K", ""]]}, {"id": "1006.3056", "submitter": "Guoshen Yu", "authors": "Guoshen Yu, Guillermo Sapiro, St\\'ephane Mallat", "title": "Solving Inverse Problems with Piecewise Linear Estimators: From Gaussian\n  Mixture Models to Structured Sparsity", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework for solving image inverse problems is introduced in this\npaper. The approach is based on Gaussian mixture models, estimated via a\ncomputationally efficient MAP-EM algorithm. A dual mathematical interpretation\nof the proposed framework with structured sparse estimation is described, which\nshows that the resulting piecewise linear estimate stabilizes the estimation\nwhen compared to traditional sparse inverse problem techniques. This\ninterpretation also suggests an effective dictionary motivated initialization\nfor the MAP-EM algorithm. We demonstrate that in a number of image inverse\nproblems, including inpainting, zooming, and deblurring, the same algorithm\nproduces either equal, often significantly better, or very small margin worse\nresults than the best published ones, at a lower computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 19:29:08 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Yu", "Guoshen", ""], ["Sapiro", "Guillermo", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1006.3275", "submitter": "Paul Vitanyi", "authors": "Sebastiaan A. Terwijn (Radboud Univ. Nijmegen), Leen Torenvliet (Univ.\n  Amsterdam), and Paul M.B. Vitanyi (CWI, Amsterdam)", "title": "Normalized Information Distance is Not Semicomputable", "comments": "9 pages, LaTeX, No figures, To appear in J. Comput. Syst. Sci", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized information distance (NID) uses the theoretical notion of\nKolmogorov complexity, which for practical purposes is approximated by the\nlength of the compressed version of the file involved, using a real-world\ncompression program. This practical application is called 'normalized\ncompression distance' and it is trivially computable. It is a parameter-free\nsimilarity measure based on compression, and is used in pattern recognition,\ndata mining, phylogeny, clustering, and classification. The complexity\nproperties of its theoretical precursor, the NID, have been open. We show that\nthe NID is neither upper semicomputable nor lower semicomputable.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2010 17:17:09 GMT"}], "update_date": "2010-06-17", "authors_parsed": [["Terwijn", "Sebastiaan A.", "", "Radboud Univ. Nijmegen"], ["Torenvliet", "Leen", "", "Univ.\n  Amsterdam"], ["Vitanyi", "Paul M. B.", "", "CWI, Amsterdam"]]}, {"id": "1006.3403", "submitter": "Atanas Atanassov", "authors": "Atanas Marinov Atanassov", "title": "Image processing of a spectrogram produced by Spectrometer Airglow\n  Temperature Imager", "comments": "4 pages, 5 figures; In Conference Proceedings \"Fundamental Space\n  Research\", Sunny Beach, Bulgaria, 21-28 Sep 2008, 328-331", "journal-ref": null, "doi": "10.1016/j.asr.2011.01.029", "report-no": null, "categories": "physics.comp-ph cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Spectral Airglow Temperature Imager is an instrument, specially designed\nfor investigation of the wave processes in the Mesosphere-Lower Thermosphere.\nIn order to determine the kinematics parameters of a wave, the values of a\nphysical quantity in different space points and their changes in the time\nshould be known. An approach for image processing of registered spectrograms is\nproposed. A detailed description is made of the steps of this approach, related\nto recovering CCD pixel values, influenced by cosmic particles, dark image\ncorrection and filter parameters determination.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 08:36:15 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Atanassov", "Atanas Marinov", ""]]}, {"id": "1006.3468", "submitter": "Atanas Atanassov", "authors": "Atanas Marinov Atanassov", "title": "Algorithm for Sector Spectra Calculation from Images Registered by the\n  Spectral Airglow Temperature Imager", "comments": "4 pages, 4 figures, In Proceedings of \"Fundamental Space Research\n  2009\", 193-196", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Spectral Airglow Temperature Imager is an instrument, specially designed\nfor investigation of the wave processes in the Mesosphere-Lower Thermosphere.\nIn order to determine the kinematic parameters of a wave, the values of a\nphysical quantity in different space points and their changes in the time\nshould be known. As a result of the possibilities of the SATI instrument for\nspace scanning, different parts of the images (sectors of spectrograms)\ncorrespond to the respective mesopause areas (where the radiation is\ngenerated). Algorithms for sector spectra calculation are proposed. In contrast\nto the original algorithms where twelve sectors with angles of 30 degrees are\nonly determined now sectors with arbitrary orientation and angles are\ncalculated. An algorithm is presented for sector calculation based on pixel\ndivision into sub pixels. A comparative results are shown.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 13:49:28 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2011 09:40:07 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Atanassov", "Atanas Marinov", ""]]}, {"id": "1006.3506", "submitter": "Ana  Lopes", "authors": "Ana Paula Brand\\~ao Lopes, Eduardo Alves do Valle Jr., Jussara Marques\n  de Almeida, Arnaldo Albuquerque de Ara\\'ujo", "title": "Action Recognition in Videos: from Motion Capture Labs to the Web", "comments": "Preprint submitted to CVIU, survey paper, 46 pages, 2 figures, 4\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a survey of human action recognition approaches based on\nvisual data recorded from a single video camera. We propose an organizing\nframework which puts in evidence the evolution of the area, with techniques\nmoving from heavily constrained motion capture scenarios towards more\nchallenging, realistic, \"in the wild\" videos. The proposed organization is\nbased on the representation used as input for the recognition task, emphasizing\nthe hypothesis assumed and thus, the constraints imposed on the type of video\nthat each technique is able to address. Expliciting the hypothesis and\nconstraints makes the framework particularly useful to select a method, given\nan application. Another advantage of the proposed organization is that it\nallows categorizing newest approaches seamlessly with traditional ones, while\nproviding an insightful perspective of the evolution of the action recognition\ntask up to now. That perspective is the basis for the discussion in the end of\nthe paper, where we also present the main open issues in the area.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 16:27:35 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Lopes", "Ana Paula Brand\u00e3o", ""], ["Valle", "Eduardo Alves do", "Jr."], ["de Almeida", "Jussara Marques", ""], ["de Ara\u00fajo", "Arnaldo Albuquerque", ""]]}, {"id": "1006.3679", "submitter": "Hossein Mobahi", "authors": "Hossein Mobahi, Shankar R. Rao, Allen Y. Yang, Shankar S. Sastry and\n  Yi Ma", "title": "Segmentation of Natural Images by Texture and Boundary Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for segmentation of natural images that\nharnesses the principle of minimum description length (MDL). Our method is\nbased on observations that a homogeneously textured region of a natural image\ncan be well modeled by a Gaussian distribution and the region boundary can be\neffectively coded by an adaptive chain code. The optimal segmentation of an\nimage is the one that gives the shortest coding length for encoding all\ntextures and boundaries in the image, and is obtained via an agglomerative\nclustering process applied to a hierarchy of decreasing window sizes as\nmulti-scale texture features. The optimal segmentation also provides an\naccurate estimate of the overall coding length and hence the true entropy of\nthe image. We test our algorithm on the publicly available Berkeley\nSegmentation Dataset. It achieves state-of-the-art segmentation results\ncompared to other existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 12:37:28 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Mobahi", "Hossein", ""], ["Rao", "Shankar R.", ""], ["Yang", "Allen Y.", ""], ["Sastry", "Shankar S.", ""], ["Ma", "Yi", ""]]}, {"id": "1006.4175", "submitter": "Noha El-Zehiry", "authors": "Noha El-Zehiry and Leo Grady", "title": "Optimization of Weighted Curvature for Image Segmentation", "comments": "15 pages , 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimization of boundary curvature is a classic regularization technique for\nimage segmentation in the presence of noisy image data. Techniques for\nminimizing curvature have historically been derived from descent methods which\ncould be trapped in a local minimum and therefore required a good\ninitialization. Recently, combinatorial optimization techniques have been\napplied to the optimization of curvature which provide a solution that achieves\nnearly a global optimum. However, when applied to image segmentation these\nmethods required a meaningful data term. Unfortunately, for many images,\nparticularly medical images, it is difficult to find a meaningful data term.\nTherefore, we propose to remove the data term completely and instead weight the\ncurvature locally, while still achieving a global optimum.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 20:59:43 GMT"}], "update_date": "2010-06-23", "authors_parsed": [["El-Zehiry", "Noha", ""], ["Grady", "Leo", ""]]}, {"id": "1006.4330", "submitter": "Ana Georgina Flesia MS", "authors": "Valeria Rulloni, Oscar Bustos and Ana Georgina Flesia", "title": "Large gaps imputation in remote sensed imagery of the environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation of missing data in large regions of satellite imagery is necessary\nwhen the acquired image has been damaged by shadows due to clouds, or\ninformation gaps produced by sensor failure.\n  The general approach for imputation of missing data, that could not be\nconsidered missed at random, suggests the use of other available data. Previous\nwork, like local linear histogram matching, take advantage of a co-registered\nolder image obtained by the same sensor, yielding good results in filling\nhomogeneous regions, but poor results if the scenes being combined have radical\ndifferences in target radiance due, for example, to the presence of sun glint\nor snow.\n  This study proposes three different alternatives for filling the data gaps.\nThe first two involves merging radiometric information from a lower resolution\nimage acquired at the same time, in the Fourier domain (Method A), and using\nlinear regression (Method B). The third method consider segmentation as the\nmain target of processing, and propose a method to fill the gaps in the map of\nclasses, avoiding direct imputation (Method C).\n  All the methods were compared by means of a large simulation study,\nevaluating performance with a multivariate response vector with four measures:\nQ, RMSE, Kappa and Overall Accuracy coefficients. Difference in performance\nwere tested with a MANOVA mixed model design with two main effects, imputation\nmethod and type of lower resolution extra data, and a blocking third factor\nwith a nested sub-factor, introduced by the real Landsat image and the\nsub-images that were used. Method B proved to be the best for all criteria.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2010 16:54:56 GMT"}], "update_date": "2010-06-23", "authors_parsed": [["Rulloni", "Valeria", ""], ["Bustos", "Oscar", ""], ["Flesia", "Ana Georgina", ""]]}, {"id": "1006.4588", "submitter": "William Jackson", "authors": "S. Sadek, A. Al-Hamadi, B. Michaelis and U. Sayed", "title": "Efficient Region-Based Image Querying", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving images from large and varied repositories using visual contents\nhas been one of major research items, but a challenging task in the image\nmanagement community. In this paper we present an efficient approach for\nregion-based image classification and retrieval using a fast multi-level neural\nnetwork model. The advantages of this neural model in image classification and\nretrieval domain will be highlighted. The proposed approach accomplishes its\ngoal in three main steps. First, with the help of a mean-shift based\nsegmentation algorithm, significant regions of the image are isolated.\nSecondly, color and texture features of each region are extracted by using\ncolor moments and 2D wavelets decomposition technique. Thirdly the multi-level\nneural classifier is trained in order to classify each region in a given image\ninto one of five predefined categories, i.e., \"Sky\", \"Building\", \"SandnRock\",\n\"Grass\" and \"Water\". Simulation results show that the proposed method is\npromising in terms of classification and retrieval accuracy results. These\nresults compare favorably with the best published results obtained by other\nstate-of-the-art image retrieval techniques.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 16:52:26 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Sadek", "S.", ""], ["Al-Hamadi", "A.", ""], ["Michaelis", "B.", ""], ["Sayed", "U.", ""]]}, {"id": "1006.4801", "submitter": "Soosan Beheshti", "authors": "Soosan Beheshti, Masoud Hashemi, Xiao-Ping Zhang, and Nima Nikvand", "title": "Noise Invalidation Denoising", "comments": "9 pages, journal submission", "journal-ref": null, "doi": "10.1109/TSP.2010.2074199", "report-no": null, "categories": "stat.ME cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A denoising technique based on noise invalidation is proposed. The adaptive\napproach derives a noise signature from the noise order statistics and utilizes\nthe signature to denoise the data. The novelty of this approach is in\npresenting a general-purpose denoising in the sense that it does not need to\nemploy any particular assumption on the structure of the noise-free signal,\nsuch as data smoothness or sparsity of the coefficients. An advantage of the\nmethod is in denoising the corrupted data in any complete basis transformation\n(orthogonal or non-orthogonal). Experimental results show that the proposed\nmethod, called Noise Invalidation Denoising (NIDe), outperforms existing\ndenoising approaches in terms of Mean Square Error (MSE).\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 14:23:09 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Beheshti", "Soosan", ""], ["Hashemi", "Masoud", ""], ["Zhang", "Xiao-Ping", ""], ["Nikvand", "Nima", ""]]}, {"id": "1006.4910", "submitter": "Burak Bayramli", "authors": "Burak Bayramli", "title": "3D Visual Tracking with Particle and Kalman Filters", "comments": "The concepts are outdated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most visually demonstrable and straightforward uses of filtering\nis in the field of Computer Vision. In this document we will try to outline the\nissues encountered while designing and implementing a particle and kalman\nfilter based tracking system.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 04:51:32 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 09:16:48 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 07:09:39 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bayramli", "Burak", ""]]}, {"id": "1006.5739", "submitter": "Ognyan Kounchev", "authors": "Ognyan Kounchev, Damyan Kalaglarsky, Milcho Tsvetkov", "title": "Polyharmonic Daubechies type wavelets in Image Processing and Astronomy,\n  II", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the application of the polyharmonic subdivision wavelets (of\nDaubechies type) to Image Processing, in particular to Astronomical Images. The\nresults show an essential advantage over some standard multivariate wavelets\nand a potential for better compression.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2010 22:53:14 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Kounchev", "Ognyan", ""], ["Kalaglarsky", "Damyan", ""], ["Tsvetkov", "Milcho", ""]]}, {"id": "1006.5902", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Debotosh Bhattacharjee, Mita Nasipuri, L. Malik, M.\n  Kundu, and D. K. Basu", "title": "Performance Comparison of SVM and ANN for Handwritten Devnagari\n  Character Recognition", "comments": null, "journal-ref": "IJCSI 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification methods based on learning from examples have been widely\napplied to character recognition from the 1990s and have brought forth\nsignificant improvements of recognition accuracies. This class of methods\nincludes statistical methods, artificial neural networks, support vector\nmachines (SVM), multiple classifier combination, etc. In this paper, we discuss\nthe characteristics of the some classification methods that have been\nsuccessfully applied to handwritten Devnagari character recognition and results\nof SVM and ANNs classification method, applied on Handwritten Devnagari\ncharacters. After preprocessing the character image, we extracted shadow\nfeatures, chain code histogram features, view based features and longest run\nfeatures. These features are then fed to Neural classifier and in support\nvector machine for classification. In neural classifier, we explored three ways\nof combining decisions of four MLP's designed for four different features.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 16:16:43 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Malik", "L.", ""], ["Kundu", "M.", ""], ["Basu", "D. K.", ""]]}, {"id": "1006.5908", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Debotosh Bhattacharjee, Mita Nasipuri, D. K. Basu, and\n  M. Kundu", "title": "Recognition of Non-Compound Handwritten Devnagari Characters using a\n  Combination of MLP and Minimum Edit Distance", "comments": null, "journal-ref": "IJCSS 4(1): 107-120 (2010)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a new method for recognition of offline Handwritten\nnon-compound Devnagari Characters in two stages. It uses two well known and\nestablished pattern recognition techniques: one using neural networks and the\nother one using minimum edit distance. Each of these techniques is applied on\ndifferent sets of characters for recognition. In the first stage, two sets of\nfeatures are computed and two classifiers are applied to get higher recognition\naccuracy. Two MLP's are used separately to recognize the characters. For one of\nthe MLP's the characters are represented with their shadow features and for the\nother chain code histogram feature is used. The decision of both MLP's is\ncombined using weighted majority scheme. Top three results produced by combined\nMLP's in the first stage are used to calculate the relative difference values.\nIn the second stage, based on these relative differences character set is\ndivided into two. First set consists of the characters with distinct shapes and\nsecond set consists of confused characters, which appear very similar in\nshapes. Characters of distinct shapes of first set are classified using MLP.\nConfused characters in second set are classified using minimum edit distance\nmethod. Method of minimum edit distance makes use of corner detected in a\ncharacter image using modified Harris corner detection technique. Experiment on\nthis method is carried out on a database of 7154 samples. The overall\nrecognition is found to be 90.74%.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 16:25:21 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "D. K.", ""], ["Kundu", "M.", ""]]}, {"id": "1006.5911", "submitter": "Debotosh Bhattacharjee", "authors": "S. Arora, Debotosh Bhattacharjee, M. Nasipuri, D.K. Basu, and M.Kundu", "title": "Application of Statistical Features in Handwritten Devnagari Character\n  Recognition", "comments": null, "journal-ref": "IJRTE 2(2):40-42(2009)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a scheme for offline Handwritten Devnagari Character\nRecognition is proposed, which uses different feature extraction methodologies\nand recognition algorithms. The proposed system assumes no constraints in\nwriting style or size. First the character is preprocessed and features namely\n: Chain code histogram and moment invariant features are extracted and fed to\nMultilayer Perceptrons as a preliminary recognition step. Finally the results\nof both MLP's are combined using weighted majority scheme. The proposed system\nis tested on 1500 handwritten devnagari character database collected from\ndifferent people. It is observed that the proposed system achieves recognition\nrates 98.03% for top 5 results and 89.46% for top 1 result.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 16:33:01 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "S.", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "M.", ""], ["Basu", "D. K.", ""], ["Kundu", "M.", ""]]}, {"id": "1006.5913", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar\n  Basu, and Mahantapas Kundu", "title": "Multiple Classifier Combination for Off-line Handwritten Devnagari\n  Character Recognition", "comments": null, "journal-ref": "ICSC 2008", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the application of weighted majority voting technique for\ncombination of classification decision obtained from three Multi_Layer\nPerceptron(MLP) based classifiers for Recognition of Handwritten Devnagari\ncharacters using three different feature sets. The features used are\nintersection, shadow feature and chain code histogram features. Shadow features\nare computed globally for character image while intersection features and chain\ncode histogram features are computed by dividing the character image into\ndifferent segments. On experimentation with a dataset of 4900 samples the\noverall recognition rate observed is 92.16% as we considered top five choices\nresults. This method is compared with other recent methods for Handwritten\nDevnagari Character Recognition and it has been observed that this approach has\nbetter success rate than other methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 16:38:02 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1006.5920", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Debotosh Bhattacharjee, Mita Nasipuri, and Latesh Malik", "title": "A Two Stage Classification Approach for Handwritten Devanagari\n  Characters", "comments": null, "journal-ref": "ICCIMA 2007", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a two stage classification approach for handwritten\ndevanagari characters The first stage is using structural properties like\nshirorekha, spine in character and second stage exploits some intersection\nfeatures of characters which are fed to a feedforward neural network. Simple\nhistogram based method does not work for finding shirorekha, vertical bar\n(Spine) in handwritten devnagari characters. So we designed a differential\ndistance based technique to find a near straight line for shirorekha and spine.\nThis approach has been tested for 50000 samples and we got 89.12% success\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 16:54:43 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Malik", "Latesh", ""]]}, {"id": "1006.5924", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Latesh Malik, Debotosh Bhattacharjee, and Mita Nasipuri", "title": "A novel approach for handwritten Devnagari character recognition", "comments": null, "journal-ref": "ICSIP 2006", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a method for recognition of handwritten devanagari characters\nis described. Here, feature vector is constituted by accumulated directional\ngradient changes in different segments, number of intersections points for the\ncharacter, type of spine present and type of shirorekha present in the\ncharacter. One Multi-layer Perceptron with conjugate-gradient training is used\nto classify these feature vectors. This method is applied to a database with\n1000 sample characters and the recognition rate obtained is 88.12%\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 17:09:39 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Malik", "Latesh", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1006.5927", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Latesh Malik, Debotosh Bhattacharjee, and Mita Nasipuri", "title": "Classification Of Gradient Change Features Using MLP For Handwritten\n  Character Recognition", "comments": null, "journal-ref": "EAIT 2006", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel, generic scheme for off-line handwritten English alphabets character\nimages is proposed. The advantage of the technique is that it can be applied in\na generic manner to different applications and is expected to perform better in\nuncertain and noisy environments. The recognition scheme is using a multilayer\nperceptron(MLP) neural networks. The system was trained and tested on a\ndatabase of 300 samples of handwritten characters. For improved generalization\nand to avoid overtraining, the whole available dataset has been divided into\ntwo subsets: training set and test set. We achieved 99.10% and 94.15% correct\nrecognition rates on training and test sets respectively. The purposed scheme\nis robust with respect to various writing styles and size as well as presence\nof considerable noise.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 17:14:40 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Malik", "Latesh", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1006.5942", "submitter": "Debotosh Bhattacharjee", "authors": "Santanu Halder, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar\n  Basu, Mahantapas Kundu", "title": "FPGA Based Assembling of Facial Components for Human Face Construction", "comments": null, "journal-ref": "IJRTE 1(1):541-545(2009)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at VLSI realization for generation of a new face from textual\ndescription. The FASY (FAce SYnthesis) System is a Face Database Retrieval and\nnew Face generation System that is under development. One of its main features\nis the generation of the requested face when it is not found in the existing\ndatabase. The new face generation system works in three steps - searching\nphase, assembling phase and tuning phase. In this paper the tuning phase using\nhardware description language and its implementation in a Field Programmable\nGate Array (FPGA) device is presented.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 18:01:47 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Halder", "Santanu", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1006.5945", "submitter": "Debotosh Bhattacharjee", "authors": "S. Halder, Debotosh Bhattacharjee, M. Nasipuri, D. K. Basu, and M.\n  Kundu", "title": "Fuzzy Classification of Facial Component Parameters", "comments": null, "journal-ref": "IJRTE 2(2): 66-70(2009)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel type-2 Fuzzy logic System to define the Shape of\na facial component with the crisp output. This work is the part of our main\nresearch effort to design a system (called FASY) which offers a novel face\nconstruction approach based on the textual description and also extracts and\nanalyzes the facial components from a face image by an efficient technique. The\nFuzzy model, designed in this paper, takes crisp value of width and height of a\nfacial component and produces the crisp value of Shape for different facial\ncomponents. This method is designed using Matlab 6.5 and Visual Basic 6.0 and\ntested with the facial components extracted from 200 male and female face\nimages of different ages from different face databases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 18:07:35 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2010 05:22:49 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Halder", "S.", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "M.", ""], ["Basu", "D. K.", ""], ["Kundu", "M.", ""]]}]