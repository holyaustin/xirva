[{"id": "1606.00021", "submitter": "Wieland Brendel", "authors": "Ivan Ustyuzhaninov, Wieland Brendel, Leon A. Gatys and Matthias Bethge", "title": "Texture Synthesis Using Shallow Convolutional Networks with Random\n  Filters", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we demonstrate that the feature space of random shallow convolutional\nneural networks (CNNs) can serve as a surprisingly good model of natural\ntextures. Patches from the same texture are consistently classified as being\nmore similar then patches from different textures. Samples synthesized from the\nmodel capture spatial correlations on scales much larger then the receptive\nfield size, and sometimes even rival or surpass the perceptual quality of state\nof the art texture models (but show less variability). The current state of the\nart in parametric texture synthesis relies on the multi-layer feature space of\ndeep CNNs that were trained on natural images. Our finding suggests that such\noptimized multi-layer feature spaces are not imperative for texture modeling.\nInstead, much simpler shallow and convolutional networks can serve as the basis\nfor novel texture synthesis algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 20:03:13 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Ustyuzhaninov", "Ivan", ""], ["Brendel", "Wieland", ""], ["Gatys", "Leon A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1606.00061", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "comments": "11 pages, 7 figures, 3 tables in 2016 Conference on Neural\n  Information Processing Systems (NIPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works have proposed attention models for Visual Question\nAnswering (VQA) that generate spatial maps highlighting image regions relevant\nto answering the question. In this paper, we argue that in addition to modeling\n\"where to look\" or visual attention, it is equally important to model \"what\nwords to listen to\" or question attention. We present a novel co-attention\nmodel for VQA that jointly reasons about image and question attention. In\naddition, our model reasons about the question (and consequently the image via\nthe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional\nconvolution neural networks (CNN). Our model improves the state-of-the-art on\nthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA\ndataset. By using ResNet, the performance is further improved to 62.1% for VQA\nand 65.4% for COCO-QA.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 22:02:01 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 01:51:13 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 02:15:57 GMT"}, {"version": "v4", "created": "Fri, 13 Jan 2017 16:18:03 GMT"}, {"version": "v5", "created": "Thu, 19 Jan 2017 05:03:33 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Lu", "Jiasen", ""], ["Yang", "Jianwei", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.00103", "submitter": "Zhe Zhu", "authors": "Zhe Zhu, Jiaming Lu, Minxuan Wang, Songhai Zhang, Ralph Martin, Hantao\n  Liu, Shimin Hu", "title": "A Comparative Study of Algorithms for Realtime Panoramic Video Blending", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike image blending algorithms, video blending algorithms have been little\nstudied. In this paper, we investigate 6 popular blending algorithms---feather\nblending, multi-band blending, modified Poisson blending, mean value coordinate\nblending, multi-spline blending and convolution pyramid blending. We consider\nin particular realtime panoramic video blending, a key problem in various\nvirtual reality tasks. To evaluate the performance of the 6 algorithms on this\nproblem, we have created a video benchmark of several videos captured under\nvarious conditions. We analyze the time and memory needed by the above 6\nalgorithms, for both CPU and GPU implementations (where readily\nparallelizable). The visual quality provided by these algorithms is also\nevaluated both objectively and subjectively. The video benchmark and algorithm\nimplementations are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 03:26:43 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 03:01:32 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Zhu", "Zhe", ""], ["Lu", "Jiaming", ""], ["Wang", "Minxuan", ""], ["Zhang", "Songhai", ""], ["Martin", "Ralph", ""], ["Liu", "Hantao", ""], ["Hu", "Shimin", ""]]}, {"id": "1606.00110", "submitter": "Chris Thomas", "authors": "Christopher Thomas", "title": "OpenSalicon: An Open Source Implementation of the Salicon Saliency Model", "comments": "Github Repository: https://github.com/CLT29/OpenSALICON", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present our publicly downloadable implementation\nof the SALICON saliency model. At the time of this writing, SALICON is one of\nthe top performing saliency models on the MIT 300 fixation prediction dataset\nwhich evaluates how well an algorithm is able to predict where humans would\nlook in a given image. Recently, numerous models have achieved state-of-the-art\nperformance on this benchmark, but none of the top 5 performing models\n(including SALICON) are available for download. To address this issue, we have\ncreated a publicly downloadable implementation of the SALICON model. It is our\nhope that our model will engender further research in visual attention modeling\nby providing a baseline for comparison of other algorithms and a platform for\nextending this implementation. The model we provide supports both training and\ntesting, enabling researchers to quickly fine-tune the model on their own\ndataset. We also provide a pre-trained model and code for those users who only\nneed to generate saliency maps for images without training their own model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 04:28:10 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Thomas", "Christopher", ""]]}, {"id": "1606.00128", "submitter": "Yanbo Fan", "authors": "Yanbo Fan, Ran He, Jian Liang, Bao-Gang Hu", "title": "Self-Paced Learning: an Implicit Regularization Perspective", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-paced learning (SPL) mimics the cognitive mechanism of humans and\nanimals that gradually learns from easy to hard samples. One key issue in SPL\nis to obtain better weighting strategy that is determined by minimizer\nfunction. Existing methods usually pursue this by artificially designing the\nexplicit form of SPL regularizer. In this paper, we focus on the minimizer\nfunction, and study a group of new regularizer, named self-paced implicit\nregularizer that is deduced from robust loss function. Based on the convex\nconjugacy theory, the minimizer function for self-paced implicit regularizer\ncan be directly learned from the latent loss function, while the analytic form\nof the regularizer can be even known. A general framework (named SPL-IR) for\nSPL is developed accordingly. We demonstrate that the learning procedure of\nSPL-IR is associated with latent robust loss functions, thus can provide some\ntheoretical inspirations for its working mechanism. We further analyze the\nrelation between SPL-IR and half-quadratic optimization. Finally, we implement\nSPL-IR to both supervised and unsupervised tasks, and experimental results\ncorroborate our ideas and demonstrate the correctness and effectiveness of\nimplicit regularizers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 06:18:29 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 04:17:56 GMT"}, {"version": "v3", "created": "Sun, 18 Sep 2016 15:32:47 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Fan", "Yanbo", ""], ["He", "Ran", ""], ["Liang", "Jian", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1606.00151", "submitter": "Rafael Munoz-Salinas", "authors": "Rafael Mu\\~noz-Salinas, Manuel J. Mar\\'in-Jimenez, Enrique\n  Yeguas-Bolivar, Rafael Medina-Carnicer", "title": "Mapping and Localization from Planar Markers", "comments": "Paper submitted to journal. Code available. See webpage\n  http://www.uco.es/investiga/grupos/ava/node/57/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Squared planar markers are a popular tool for fast, accurate and robust\ncamera localization, but its use is frequently limited to a single marker, or\nat most, to a small set of them for which their relative pose is known\nbeforehand. Mapping and localization from a large set of planar markers is yet\na scarcely treated problem in favour of keypoint-based approaches. However,\nwhile keypoint detectors are not robust to rapid motion, large changes in\nviewpoint, or significant changes in appearance, fiducial markers can be\nrobustly detected under a wider range of conditions. This paper proposes a\nnovel method to simultaneously solve the problems of mapping and localization\nfrom a set of squared planar markers. First, a quiver of pairwise relative\nmarker poses is created, from which an initial pose graph is obtained. The pose\ngraph may contain small pairwise pose errors, that when propagated, leads to\nlarge errors. Thus, we distribute the rotational and translational error along\nthe basis cycles of the graph so as to obtain a corrected pose graph. Finally,\nwe perform a global pose optimization by minimizing the reprojection errors of\nthe planar markers in all observed frames. The experiments conducted show that\nour method performs better than Structure from Motion and visual SLAM\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 07:47:07 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 08:33:27 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Mu\u00f1oz-Salinas", "Rafael", ""], ["Mar\u00edn-Jimenez", "Manuel J.", ""], ["Yeguas-Bolivar", "Enrique", ""], ["Medina-Carnicer", "Rafael", ""]]}, {"id": "1606.00166", "submitter": "Shaodi You", "authors": "Shaodi You, Yasuyuki Matsushita, Sudipta Sinha, Yusuke Bou and\n  Katsushi Ikeuchi", "title": "Multiview Rectification of Folded Documents", "comments": "8 pages; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitally unwrapping images of paper sheets is crucial for accurate document\nscanning and text recognition. This paper presents a method for automatically\nrectifying curved or folded paper sheets from a few images captured from\nmultiple viewpoints. Prior methods either need expensive 3D scanners or model\ndeformable surfaces using over-simplified parametric representations. In\ncontrast, our method uses regular images and is based on general developable\nsurface models that can represent a wide variety of paper deformations. Our\nmain contribution is a new robust rectification method based on ridge-aware 3D\nreconstruction of a paper sheet and unwrapping the reconstructed surface using\nproperties of developable surfaces via $\\ell_1$ conformal mapping. We present\nresults on several examples including book pages, folded letters and shopping\nreceipts.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 08:29:51 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["You", "Shaodi", ""], ["Matsushita", "Yasuyuki", ""], ["Sinha", "Sudipta", ""], ["Bou", "Yusuke", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "1606.00185", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Ting Zhang, Jingkuan Song, Nicu Sebe, and Heng Tao Shen", "title": "A Survey on Learning to Hash", "comments": "To appear in IEEE Transactions On Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search is a problem of finding the data points from the\ndatabase such that the distances from them to the query point are the smallest.\nLearning to hash is one of the major solutions to this problem and has been\nwidely studied recently. In this paper, we present a comprehensive survey of\nthe learning to hash algorithms, categorize them according to the manners of\npreserving the similarities into: pairwise similarity preserving, multiwise\nsimilarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity\npreserving as the objective function is very different though quantization, as\nwe show, can be derived from preserving the pairwise similarities. In addition,\nwe present the evaluation protocols, and the general performance analysis, and\npoint out that the quantization algorithms perform superiorly in terms of\nsearch accuracy, search time cost, and space cost. Finally, we introduce a few\nemerging topics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:23:48 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 00:19:47 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wang", "Jingdong", ""], ["Zhang", "Ting", ""], ["Song", "Jingkuan", ""], ["Sebe", "Nicu", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1606.00219", "submitter": "Behnood Rasti", "authors": "Behnood Rasti, Magnus O. Ulfarsson, and Johannes R. Sveinsson", "title": "Hyperspectral Subspace Identification Using SURE", "comments": "Technical Report. A shorten version of this paper has been published\n  in the IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2015.2485999", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing hyperspectral sensors collect large volumes of high\ndimensional spectral and spatial data. However, due to spectral and spatial\nredundancy the true hyperspectral signal lies on a subspace of much lower\ndimension than the original data. The identification of the signal subspace is\na very important first step for most hyperspectral algorithms. In this paper we\ninvestigate the important problem of identifying the hyperspectral signal\nsubspace by minimizing the mean squared error (MSE) between the true signal and\nan estimate of the signal. Since the MSE is uncomputable in practice, due to\nits dependency on the true signal, we propose a method based on the Stein's\nunbiased risk estimator (SURE) that provides an unbiased estimate of the MSE.\nThe resulting method is simple and fully automatic and we evaluate it using\nboth simulated and real hyperspectral data sets. Experimental results shows\nthat our proposed method compares well to recent state-of-the-art subspace\nidentification methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:01:54 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Rasti", "Behnood", ""], ["Ulfarsson", "Magnus O.", ""], ["Sveinsson", "Johannes R.", ""]]}, {"id": "1606.00305", "submitter": "Yang Li", "authors": "Yang Li, Chunxiao Fan, Yong Li, Qiong Wu, Yue Ming", "title": "Improving Deep Neural Network with Multiple Parametric Exponential\n  Linear Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation function is crucial to the recent successes of deep neural\nnetworks. In this paper, we first propose a new activation function, Multiple\nParametric Exponential Linear Units (MPELU), aiming to generalize and unify the\nrectified and exponential linear units. As the generalized form, MPELU shares\nthe advantages of Parametric Rectified Linear Unit (PReLU) and Exponential\nLinear Unit (ELU), leading to better classification performance and convergence\nproperty. In addition, weight initialization is very important to train very\ndeep networks. The existing methods laid a solid foundation for networks using\nrectified linear units but not for exponential linear units. This paper\ncomplements the current theory and extends it to the wider range. Specifically,\nwe put forward a way of initialization, enabling training of very deep networks\nusing exponential linear units. Experiments demonstrate that the proposed\ninitialization not only helps the training process but leads to better\ngeneralization performance. Finally, utilizing the proposed activation function\nand initialization, we present a deep MPELU residual architecture that achieves\nstate-of-the-art performance on the CIFAR-10/100 datasets. The code is\navailable at https://github.com/Coldmooon/Code-for-MPELU.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:33:17 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 10:10:41 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 08:44:56 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Li", "Yang", ""], ["Fan", "Chunxiao", ""], ["Li", "Yong", ""], ["Wu", "Qiong", ""], ["Ming", "Yue", ""]]}, {"id": "1606.00373", "submitter": "Christian Rupprecht", "authors": "Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico\n  Tombari, Nassir Navab", "title": "Deeper Depth Prediction with Fully Convolutional Residual Networks", "comments": "Published at IEEE International Conference on 3D Vision (3DV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating the depth map of a scene given\na single RGB image. We propose a fully convolutional architecture, encompassing\nresidual learning, to model the ambiguous mapping between monocular images and\ndepth maps. In order to improve the output resolution, we present a novel way\nto efficiently learn feature map up-sampling within the network. For\noptimization, we introduce the reverse Huber loss that is particularly suited\nfor the task at hand and driven by the value distributions commonly present in\ndepth maps. Our model is composed of a single architecture that is trained\nend-to-end and does not rely on post-processing techniques, such as CRFs or\nother additional refinement steps. As a result, it runs in real-time on images\nor videos. In the evaluation, we show that the proposed model contains fewer\nparameters and requires fewer training data than the current state of the art,\nwhile outperforming all approaches on depth estimation. Code and models are\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:03:00 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 09:40:59 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Laina", "Iro", ""], ["Rupprecht", "Christian", ""], ["Belagiannis", "Vasileios", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""]]}, {"id": "1606.00474", "submitter": "Michael Grupp", "authors": "Michael Grupp, Philipp Kopp, Patrik Huber, Matthias R\\\"atsch", "title": "A 3D Face Modelling Approach for Pose-Invariant Face Recognition in a\n  Human-Robot Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face analysis techniques have become a crucial component of human-machine\ninteraction in the fields of assistive and humanoid robotics. However, the\nvariations in head-pose that arise naturally in these environments are still a\ngreat challenge. In this paper, we present a real-time capable 3D face\nmodelling framework for 2D in-the-wild images that is applicable for robotics.\nThe fitting of the 3D Morphable Model is based exclusively on automatically\ndetected landmarks. After fitting, the face can be corrected in pose and\ntransformed back to a frontal 2D representation that is more suitable for face\nrecognition. We conduct face recognition experiments with non-frontal images\nfrom the MUCT database and uncontrolled, in the wild images from the PaSC\ndatabase, the most challenging face recognition database to date, showing an\nimproved performance. Finally, we present our SCITOS G5 robot system, which\nincorporates our framework as a means of image pre-processing for face\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 21:28:27 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Grupp", "Michael", ""], ["Kopp", "Philipp", ""], ["Huber", "Patrik", ""], ["R\u00e4tsch", "Matthias", ""]]}, {"id": "1606.00487", "submitter": "Sepehr Valipour", "authors": "Sepehr Valipour, Mennatullah Siam, Martin Jagersand, Nilanjan Ray", "title": "Recurrent Fully Convolutional Networks for Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is an important step in most visual tasks. While\nconvolutional neural networks have shown to perform well on single image\nsegmentation, to our knowledge, no study has been been done on leveraging\nrecurrent gated architectures for video segmentation. Accordingly, we propose a\nnovel method for online segmentation of video sequences that incorporates\ntemporal data. The network is built from fully convolutional element and\nrecurrent unit that works on a sliding window over the temporal data. We also\nintroduce a novel convolutional gated recurrent unit that preserves the spatial\ninformation and reduces the parameters learned. Our method has the advantage\nthat it can work in an online fashion instead of operating over the whole input\nbatch of video frames. The network is tested on the change detection dataset,\nand proved to have 5.5\\% improvement in F-measure over a plain fully\nconvolutional network for per frame segmentation. It was also shown to have\nimprovement of 1.4\\% for the F-measure compared to our baseline network that we\ncall FCN 12s.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 22:27:41 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 07:24:00 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 00:05:49 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Valipour", "Sepehr", ""], ["Siam", "Mennatullah", ""], ["Jagersand", "Martin", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1606.00496", "submitter": "Paulo Adeodato Prof.", "authors": "Paulo J. L. Adeodato and S\\'ilvio B. Melo", "title": "On the equivalence between Kolmogorov-Smirnov and ROC curve metrics for\n  binary classification", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary decisions are very common in artificial intelligence. Applying a\nthreshold on the continuous score gives the human decider the power to control\nthe operating point to separate the two classes. The classifier,s\ndiscriminating power is measured along the continuous range of the score by the\nArea Under the ROC curve (AUC_ROC) in most application fields. Only finances\nuses the poor single point metric maximum Kolmogorov-Smirnov (KS) distance.\nThis paper proposes the Area Under the KS curve (AUC_KS) for performance\nassessment and proves AUC_ROC = 0.5 + AUC_KS, as a simpler way to calculate the\nAUC_ROC. That is even more important for ROC averaging in ensembles of\nclassifiers or n fold cross-validation. The proof is geometrically inspired on\nrotating all KS curve to make it lie on the top of the ROC chance diagonal. On\nthe practical side, the independent variable on the abscissa on the KS curve\nsimplifies the calculation of the AUC_ROC. On the theoretical side, this\nresearch gives insights on probabilistic interpretations of classifiers\nassessment and integrates the existing body of knowledge of the information\ntheoretical ROC approach with the proposed statistical approach based on the\nthoroughly known KS distribution.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 23:12:53 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Adeodato", "Paulo J. L.", ""], ["Melo", "S\u00edlvio B.", ""]]}, {"id": "1606.00538", "submitter": "Ludovic Trottier", "authors": "Ludovic Trottier, Philippe Gigu\\`ere, Brahim Chaib-draa", "title": "Dictionary Learning for Robotic Grasp Recognition and Detection", "comments": "Submitted at the 2016 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to grasp ordinary and potentially never-seen objects is an\nimportant feature in both domestic and industrial robotics. For a system to\naccomplish this, it must autonomously identify grasping locations by using\ninformation from various sensors, such as Microsoft Kinect 3D camera. Despite\nnumerous progress, significant work still remains to be done in this field. To\nthis effect, we propose a dictionary learning and sparse representation (DLSR)\nframework for representing RGBD images from 3D sensors in the context of\ndetermining such good grasping locations. In contrast to previously proposed\napproaches that relied on sophisticated regularization or very large datasets,\nthe derived perception system has a fast training phase and can work with small\ndatasets. It is also theoretically founded for dealing with masked-out entries,\nwhich are common with 3D sensors. We contribute by presenting a comparative\nstudy of several DLSR approach combinations for recognizing and detecting grasp\ncandidates on the standard Cornell dataset. Importantly, experimental results\nshow a performance improvement of 1.69% in detection and 3.16% in recognition\nover current state-of-the-art convolutional neural network (CNN). Even though\nnowadays most popular vision-based approach is CNN, this suggests that DLSR is\nalso a viable alternative with interesting advantages that CNN has not.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 05:20:14 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Trottier", "Ludovic", ""], ["Gigu\u00e8re", "Philippe", ""], ["Chaib-draa", "Brahim", ""]]}, {"id": "1606.00611", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Erhardt Barth, Thomas Martinetz", "title": "Recursive Autoconvolution for Unsupervised Learning of Convolutional\n  Neural Networks", "comments": "8 pages, accepted to International Joint Conference on Neural\n  Networks (IJCNN 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual recognition tasks, such as image classification, unsupervised\nlearning exploits cheap unlabeled data and can help to solve these tasks more\nefficiently. We show that the recursive autoconvolution operator, adopted from\nphysics, boosts existing unsupervised methods by learning more discriminative\nfilters. We take well established convolutional neural networks and train their\nfilters layer-wise. In addition, based on previous works we design a network\nwhich extracts more than 600k features per sample, but with the total number of\ntrainable parameters greatly reduced by introducing shared filters in higher\nlayers. We evaluate our networks on the MNIST, CIFAR-10, CIFAR-100 and STL-10\nimage classification benchmarks and report several state of the art results\namong other unsupervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 10:37:46 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 18:31:05 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Knyazev", "Boris", ""], ["Barth", "Erhardt", ""], ["Martinetz", "Thomas", ""]]}, {"id": "1606.00625", "submitter": "Yu Liu", "authors": "Yu Liu, Jianlong Fu, Tao Mei and Chang Wen Chen", "title": "Storytelling of Photo Stream with Bidirectional Multi-thread Recurrent\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual storytelling aims to generate human-level narrative language (i.e., a\nnatural paragraph with multiple sentences) from a photo streams. A typical\nphoto story consists of a global timeline with multi-thread local storylines,\nwhere each storyline occurs in one different scene. Such complex structure\nleads to large content gaps at scene transitions between consecutive photos.\nMost existing image/video captioning methods can only achieve limited\nperformance, because the units in traditional recurrent neural networks (RNN)\ntend to \"forget\" the previous state when the visual sequence is inconsistent.\nIn this paper, we propose a novel visual storytelling approach with\nBidirectional Multi-thread Recurrent Neural Network (BMRNN). First, based on\nthe mined local storylines, a skip gated recurrent unit (sGRU) with delay\ncontrol is proposed to maintain longer range visual information. Second, by\nusing sGRU as basic units, the BMRNN is trained to align the local storylines\ninto the global sequential timeline. Third, a new training scheme with a\nstoryline-constrained objective function is proposed by jointly considering\nboth global and local matches. Experiments on three standard storytelling\ndatasets show that the BMRNN model outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 11:13:04 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Liu", "Yu", ""], ["Fu", "Jianlong", ""], ["Mei", "Tao", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1606.00800", "submitter": "Brian Mitchell", "authors": "Brian A. Mitchell and Linda R. Petzold", "title": "Multi-View Treelet Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.SI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multi-view factorization methods make assumptions that are not\nacceptable for many kinds of data, and in particular, for graphical data with\nhierarchical structure. At the same time, current hierarchical methods work\nonly in the single-view setting. We generalize the Treelet Transform to the\nMulti-View Treelet Transform (MVTT) to allow for the capture of hierarchical\nstructure when multiple views are available. Further, we show how this\ngeneralization is consistent with the existing theory and how it might be used\nin denoising empirical networks and in computing the shared response of\nfunctional brain data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 18:51:09 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 19:07:46 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mitchell", "Brian A.", ""], ["Petzold", "Linda R.", ""]]}, {"id": "1606.00822", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi, Arvind K Bansal", "title": "Unifying Geometric Features and Facial Action Units for Improved\n  Performance of Facial Expression Analysis", "comments": "8 pages, ISBN: 978-1-61804-285-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches to model and analyze facial expression analysis use three\ndifferent techniques: facial action units, geometric features and graph based\nmodelling. However, previous approaches have treated these technique\nseparately. There is an interrelationship between these techniques. The facial\nexpression analysis is significantly improved by utilizing these mappings\nbetween major geometric features involved in facial expressions and the subset\nof facial action units whose presence or absence are unique to a facial\nexpression. This paper combines dimension reduction techniques and image\nclassification with search space pruning achieved by this unique subset of\nfacial action units to significantly prune the search space. The performance\nresults on the publicly facial expression database shows an improvement in\nperformance by 70% over time while maintaining the emotion recognition\ncorrectness.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 19:43:29 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Bansal", "Arvind K", ""]]}, {"id": "1606.00850", "submitter": "Tianfu Wu", "authors": "Yunzhu Li, Benyuan Sun, Tianfu Wu and Yizhou Wang", "title": "Face Detection with End-to-End Integration of a ConvNet and a 3D Model", "comments": "16 pages, Y. Li and B. Sun contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for face detection in the wild, which integrates\na ConvNet and a 3D mean face model in an end-to-end multi-task discriminative\nlearning framework. The 3D mean face model is predefined and fixed (e.g., we\nused the one provided in the AFLW dataset). The ConvNet consists of two\ncomponents: (i) The face pro- posal component computes face bounding box\nproposals via estimating facial key-points and the 3D transformation (rotation\nand translation) parameters for each predicted key-point w.r.t. the 3D mean\nface model. (ii) The face verification component computes detection results by\nprun- ing and refining proposals based on facial key-points based configuration\npooling. The proposed method addresses two issues in adapting state- of-the-art\ngeneric object detection ConvNets (e.g., faster R-CNN) for face detection: (i)\nOne is to eliminate the heuristic design of prede- fined anchor boxes in the\nregion proposals network (RPN) by exploit- ing a 3D mean face model. (ii) The\nother is to replace the generic RoI (Region-of-Interest) pooling layer with a\nconfiguration pooling layer to respect underlying object structures. The\nmulti-task loss consists of three terms: the classification Softmax loss and\nthe location smooth l1 -losses [14] of both the facial key-points and the face\nbounding boxes. In ex- periments, our ConvNet is trained on the AFLW dataset\nonly and tested on the FDDB benchmark with fine-tuning and on the AFW benchmark\nwithout fine-tuning. The proposed method obtains very competitive\nstate-of-the-art performance in the two benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 20:08:28 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 03:49:44 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 14:57:17 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Li", "Yunzhu", ""], ["Sun", "Benyuan", ""], ["Wu", "Tianfu", ""], ["Wang", "Yizhou", ""]]}, {"id": "1606.00915", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin\n  Murphy and Alan L. Yuille", "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,\n  Atrous Convolution, and Fully Connected CRFs", "comments": "Accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the task of semantic image segmentation with Deep\nLearning and make three main contributions that are experimentally shown to\nhave substantial practical merit. First, we highlight convolution with\nupsampled filters, or 'atrous convolution', as a powerful tool in dense\nprediction tasks. Atrous convolution allows us to explicitly control the\nresolution at which feature responses are computed within Deep Convolutional\nNeural Networks. It also allows us to effectively enlarge the field of view of\nfilters to incorporate larger context without increasing the number of\nparameters or the amount of computation. Second, we propose atrous spatial\npyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP\nprobes an incoming convolutional feature layer with filters at multiple\nsampling rates and effective fields-of-views, thus capturing objects as well as\nimage context at multiple scales. Third, we improve the localization of object\nboundaries by combining methods from DCNNs and probabilistic graphical models.\nThe commonly deployed combination of max-pooling and downsampling in DCNNs\nachieves invariance but has a toll on localization accuracy. We overcome this\nby combining the responses at the final DCNN layer with a fully connected\nConditional Random Field (CRF), which is shown both qualitatively and\nquantitatively to improve localization performance. Our proposed \"DeepLab\"\nsystem sets the new state-of-art at the PASCAL VOC-2012 semantic image\nsegmentation task, reaching 79.7% mIOU in the test set, and advances the\nresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, and\nCityscapes. All of our code is made publicly available online.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 21:52:21 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 03:25:47 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Papandreou", "George", ""], ["Kokkinos", "Iasonas", ""], ["Murphy", "Kevin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1606.00930", "submitter": "Jacques Wainer", "authors": "Jacques Wainer", "title": "Comparison of 14 different families of classification algorithms on 115\n  binary datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tested 14 very different classification algorithms (random forest,\ngradient boosting machines, SVM - linear, polynomial, and RBF - 1-hidden-layer\nneural nets, extreme learning machines, k-nearest neighbors and a bagging of\nknn, naive Bayes, learning vector quantization, elastic net logistic\nregression, sparse linear discriminant analysis, and a boosting of linear\nclassifiers) on 115 real life binary datasets. We followed the Demsar analysis\nand found that the three best classifiers (random forest, gbm and RBF SVM) are\nnot significantly different from each other. We also discuss that a change of\nless then 0.0112 in the error rate should be considered as an irrelevant\nchange, and used a Bayesian ANOVA analysis to conclude that with high\nprobability the differences between these three classifiers is not of practical\nconsequence. We also verified the execution time of \"standard implementations\"\nof these algorithms and concluded that RBF SVM is the fastest (significantly\nso) both in training time and in training plus testing time.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 23:01:25 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Wainer", "Jacques", ""]]}, {"id": "1606.00972", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Song-Chun Zhu, Ying Nian Wu", "title": "Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sequences contain rich dynamic patterns, such as dynamic texture\npatterns that exhibit stationarity in the temporal domain, and action patterns\nthat are non-stationary in either spatial or temporal domain. We show that a\nspatial-temporal generative ConvNet can be used to model and synthesize dynamic\npatterns. The model defines a probability distribution on the video sequence,\nand the log probability is defined by a spatial-temporal ConvNet that consists\nof multiple layers of spatial-temporal filters to capture spatial-temporal\npatterns of different scales. The model can be learned from the training video\nsequences by an \"analysis by synthesis\" learning algorithm that iterates the\nfollowing two steps. Step 1 synthesizes video sequences from the currently\nlearned model. Step 2 then updates the model parameters based on the difference\nbetween the synthesized video sequences and the observed training sequences. We\nshow that the learning algorithm can synthesize realistic dynamic patterns.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 05:36:06 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 23:26:38 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Xie", "Jianwen", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1606.01001", "submitter": "Alexander Hagg", "authors": "Alexander Hagg, Frederik Hegger, Paul Pl\\\"oger", "title": "On Recognizing Transparent Objects in Domestic Environments Using Fusion\n  of Multiple Sensor Modalities", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current object recognition methods fail on object sets that include both\ndiffuse, reflective and transparent materials, although they are very common in\ndomestic scenarios. We show that a combination of cues from multiple sensor\nmodalities, including specular reflectance and unavailable depth information,\nallows us to capture a larger subset of household objects by extending a state\nof the art object recognition method. This leads to a significant increase in\nrobustness of recognition over a larger set of commonly used objects.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 08:08:30 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Hagg", "Alexander", ""], ["Hegger", "Frederik", ""], ["Pl\u00f6ger", "Paul", ""]]}, {"id": "1606.01021", "submitter": "Mario Taschwer", "authors": "Mario Taschwer and Oge Marques", "title": "Automatic Separation of Compound Figures in Scientific Articles", "comments": "accepted for Multimedia Tools and Applications with minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based analysis and retrieval of digital images found in scientific\narticles is often hindered by images consisting of multiple subfigures\n(compound figures). We address this problem by proposing a method to\nautomatically classify and separate compound figures, which consists of two\nmain steps: (i) a supervised compound figure classifier (CFC) discriminates\nbetween compound and non-compound figures using task-specific image features;\nand (ii) an image processing algorithm is applied to predicted compound images\nto perform compound figure separation (CFS). Our CFC approach is shown to\nachieve state-of-the-art classification performance on a published dataset. Our\nCFS algorithm shows superior separation accuracy on two different datasets\ncompared to other known automatic approaches. Finally, we propose a method to\nevaluate the effectiveness of the CFC-CFS process chain and use it to optimize\nthe misclassification loss of CFC for maximal effectiveness in the process\nchain.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 09:53:01 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 06:26:58 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Taschwer", "Mario", ""], ["Marques", "Oge", ""]]}, {"id": "1606.01093", "submitter": "Joachim Behar", "authors": "Joachim Behar", "title": "Extraction of clinical information from the non-invasive fetal\n  electrocardiogram", "comments": "Diss. University of Oxford, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the fetal heart rate (FHR) has gained interest in the last\ncentury, low heart rate variability has been studied to identify intrauterine\ngrowth restricted fetuses (prepartum), and abnormal FHR patterns have been\nassociated with fetal distress during delivery (intrapartum). Several\nmonitoring techniques have been proposed for FHR estimation, including\nauscultation and Doppler ultrasound. This thesis focuses on the extraction of\nthe non-invasive fetal electrocardiogram (NI-FECG) recorded from a limited set\nof abdominal sensors. The main challenge with NI-FECG extraction techniques is\nthe low signal-to-noise ratio of the FECG signal on the abdominal mixture\nsignal which consists of a dominant maternal ECG component, FECG and noise.\nHowever the NI-FECG offers many advantages over the alternative fetal\nmonitoring techniques, the most important one being the opportunity to enable\nmorphological analysis of the FECG which is vital for determining whether an\nobserved FHR event is normal or pathological. In order to advance the field of\nNI-FECG signal processing, the development of standardised public databases and\nbenchmarking of a number of published and novel algorithms was necessary.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 09:31:34 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Behar", "Joachim", ""]]}, {"id": "1606.01100", "submitter": "Martin Rajchl PhD", "authors": "Martin Rajchl, Matthew C.H. Lee, Franklin Schrans, Alice Davidson,\n  Jonathan Passerat-Palmbach, Giacomo Tarroni, Amir Alansary, Ozan Oktay,\n  Bernhard Kainz, Daniel Rueckert", "title": "Learning under Distributed Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of training data for supervision is a frequently encountered\nbottleneck of medical image analysis methods. While typically established by a\nclinical expert rater, the increase in acquired imaging data renders\ntraditional pixel-wise segmentations less feasible. In this paper, we examine\nthe use of a crowdsourcing platform for the distribution of super-pixel weak\nannotation tasks and collect such annotations from a crowd of non-expert\nraters. The crowd annotations are subsequently used for training a fully\nconvolutional neural network to address the problem of fetal brain segmentation\nin T2-weighted MR images. Using this approach we report encouraging results\ncompared to highly targeted, fully supervised methods and potentially address a\nfrequent problem impeding image analysis research.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 14:28:28 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Rajchl", "Martin", ""], ["Lee", "Matthew C. H.", ""], ["Schrans", "Franklin", ""], ["Davidson", "Alice", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Tarroni", "Giacomo", ""], ["Alansary", "Amir", ""], ["Oktay", "Ozan", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1606.01166", "submitter": "Jean-Charles Vialatte", "authors": "Jean-Charles Vialatte, Vincent Gripon, Gr\\'egoire Mercier", "title": "Generalizing the Convolution Operator to extend CNNs to Irregular\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become the state-of-the-art in\nsupervised learning vision tasks. Their convolutional filters are of paramount\nimportance for they allow to learn patterns while disregarding their locations\nin input images. When facing highly irregular domains, generalized\nconvolutional operators based on an underlying graph structure have been\nproposed. However, these operators do not exactly match standard ones on grid\ngraphs, and introduce unwanted additional invariance (e.g. with regards to\nrotations). We propose a novel approach to generalize CNNs to irregular domains\nusing weight sharing and graph-based operators. Using experiments, we show that\nthese models resemble CNNs on regular domains and offer better performance than\nmultilayer perceptrons on distorded ones.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:18:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 09:41:55 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 11:46:46 GMT"}, {"version": "v4", "created": "Wed, 25 Oct 2017 12:28:26 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Vialatte", "Jean-Charles", ""], ["Gripon", "Vincent", ""], ["Mercier", "Gr\u00e9goire", ""]]}, {"id": "1606.01167", "submitter": "Sven Eberhardt", "authors": "Sven Eberhardt and Jonah Cader and Thomas Serre", "title": "How Deep is the Feature Analysis underlying Rapid Visual Categorization?", "comments": "Sven Eberhardt and Jonah Cader contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid categorization paradigms have a long history in experimental\npsychology: Characterized by short presentation times and speedy behavioral\nresponses, these tasks highlight the efficiency with which our visual system\nprocesses natural object categories. Previous studies have shown that\nfeed-forward hierarchical models of the visual cortex provide a good fit to\nhuman visual decisions. At the same time, recent work in computer vision has\ndemonstrated significant gains in object recognition accuracy with increasingly\ndeep hierarchical architectures. But it is unclear how well these models\naccount for human visual decisions and what they may reveal about the\nunderlying brain processes.\n  We have conducted a large-scale psychophysics study to assess the correlation\nbetween computational models and human participants on a rapid animal vs.\nnon-animal categorization task. We considered visual representations of varying\ncomplexity by analyzing the output of different stages of processing in three\nstate-of-the-art deep networks. We found that recognition accuracy increases\nwith higher stages of visual processing (higher level stages indeed\noutperforming human participants on the same task) but that human decisions\nagree best with predictions from intermediate stages.\n  Overall, these results suggest that human participants may rely on visual\nfeatures of intermediate complexity and that the complexity of visual\nrepresentations afforded by modern deep network models may exceed those used by\nhuman participants during rapid categorization.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:21:07 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Eberhardt", "Sven", ""], ["Cader", "Jonah", ""], ["Serre", "Thomas", ""]]}, {"id": "1606.01178", "submitter": "Md. Reza", "authors": "Md. Alimoor Reza and Jana Kosecka", "title": "Reinforcement Learning for Semantic Segmentation in Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future advancements in robot autonomy and sophistication of robotics tasks\nrest on robust, efficient, and task-dependent semantic understanding of the\nenvironment. Semantic segmentation is the problem of simultaneous segmentation\nand categorization of a partition of sensory data. The majority of current\napproaches tackle this using multi-class segmentation and labeling in a\nConditional Random Field (CRF) framework or by generating multiple object\nhypotheses and combining them sequentially. In practical settings, the subset\nof semantic labels that are needed depend on the task and particular scene and\nlabelling every single pixel is not always necessary. We pursue these\nobservations in developing a more modular and flexible approach to multi-class\nparsing of RGBD data based on learning strategies for combining independent\nbinary object-vs-background segmentations in place of the usual monolithic\nmulti-label CRF approach. Parameters for the independent binary segmentation\nmodels can be learned very efficiently, and the combination strategy---learned\nusing reinforcement learning---can be set independently and can vary over\ndifferent tasks and environments. Accuracy is comparable to state-of-art\nmethods on a subset of the NYU-V2 dataset of indoor scenes, while providing\nadditional flexibility and modularity.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:35:58 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Reza", "Md. Alimoor", ""], ["Kosecka", "Jana", ""]]}, {"id": "1606.01284", "submitter": "Wenshuo Wang", "authors": "Wenshuo Wang, Junqiang Xi, and Xiaohan Li", "title": "Statistical Pattern Recognition for Driving Styles Based on Bayesian\n  Probability and Kernel Density Estimation", "comments": "10 pages, 9 figures. Submitted to International Journal of Automotive\n  Technology", "journal-ref": "IET Intelligent Transportation Systems, 2018", "doi": "10.1049/iet-its.2017.0379", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving styles have a great influence on vehicle fuel economy, active safety,\nand drivability. To recognize driving styles of path-tracking behaviors for\ndifferent divers, a statistical pattern-recognition method is developed to deal\nwith the uncertainty of driving styles or characteristics based on probability\ndensity estimation. First, to describe driver path-tracking styles, vehicle\nspeed and throttle opening are selected as the discriminative parameters, and a\nconditional kernel density function of vehicle speed and throttle opening is\nbuilt, respectively, to describe the uncertainty and probability of two\nrepresentative driving styles, e.g., aggressive and normal. Meanwhile, a\nposterior probability of each element in feature vector is obtained using full\nBayesian theory. Second, a Euclidean distance method is involved to decide to\nwhich class the driver should be subject instead of calculating the complex\ncovariance between every two elements of feature vectors. By comparing the\nEuclidean distance between every elements in feature vector, driving styles are\nclassified into seven levels ranging from low normal to high aggressive.\nSubsequently, to show benefits of the proposed pattern-recognition method, a\ncross-validated method is used, compared with a fuzzy logic-based\npattern-recognition method. The experiment results show that the proposed\nstatistical pattern-recognition method for driving styles based on kernel\ndensity estimation is more efficient and stable than the fuzzy logic-based\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 21:48:53 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Wang", "Wenshuo", ""], ["Xi", "Junqiang", ""], ["Li", "Xiaohan", ""]]}, {"id": "1606.01286", "submitter": "Guillaume Berger", "authors": "G. Berger, R. Memisevic", "title": "Incorporating long-range consistency in CNN-based texture generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gatys et al. (2015) showed that pair-wise products of features in a\nconvolutional network are a very effective representation of image textures. We\npropose a simple modification to that representation which makes it possible to\nincorporate long-range structure into image generation, and to render images\nthat satisfy various symmetry constraints. We show how this can greatly improve\nrendering of regular textures and of images that contain other kinds of\nsymmetric structure. We also present applications to inpainting and season\ntransfer.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 21:57:48 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 01:11:15 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Berger", "G.", ""], ["Memisevic", "R.", ""]]}, {"id": "1606.01299", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, John Isidoro, and Peyman Milanfar", "title": "RAISR: Rapid and Accurate Image Super Resolution", "comments": "Supplementary material can be found at https://goo.gl/D0ETxG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image, we wish to produce an image of larger size with significantly\nmore pixels and higher image quality. This is generally known as the Single\nImage Super-Resolution (SISR) problem. The idea is that with sufficient\ntraining data (corresponding pairs of low and high resolution images) we can\nlearn set of filters (i.e. a mapping) that when applied to given image that is\nnot in the training set, will produce a higher resolution version of it, where\nthe learning is preferably low complexity. In our proposed approach, the\nrun-time is more than one to two orders of magnitude faster than the best\ncompeting methods currently available, while producing results comparable or\nbetter than state-of-the-art.\n  A closely related topic is image sharpening and contrast enhancement, i.e.,\nimproving the visual quality of a blurry image by amplifying the underlying\ndetails (a wide range of frequencies). Our approach additionally includes an\nextremely efficient way to produce an image that is significantly sharper than\nthe input blurry one, without introducing artifacts such as halos and noise\namplification. We illustrate how this effective sharpening algorithm, in\naddition to being of independent interest, can be used as a pre-processing step\nto induce the learning of more effective upscaling filters with built-in\nsharpening and contrast enhancement effect.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 22:56:49 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 08:39:18 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 21:22:51 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Romano", "Yaniv", ""], ["Isidoro", "John", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1606.01307", "submitter": "Pedro Felzenszwalb", "authors": "Jeroen Chua, Pedro F. Felzenszwalb", "title": "Scene Grammars, Factor Graphs, and Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general framework for probabilistic modeling of complex scenes\nand inference from ambiguous observations. The approach is motivated by\napplications in image analysis and is based on the use of priors defined by\nstochastic grammars. We define a class of grammars that capture relationships\nbetween the objects in a scene and provide important contextual cues for\nstatistical inference. The distribution over scenes defined by a probabilistic\nscene grammar can be represented by a graphical model and this construction can\nbe used for efficient inference with loopy belief propagation.\n  We show experimental results with two different applications. One application\ninvolves the reconstruction of binary contour maps. Another application\ninvolves detecting and localizing faces in images. In both applications the\nsame framework leads to robust inference algorithms that can effectively\ncombine local information to reason about a scene.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 23:49:02 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 22:44:27 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 16:06:15 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Chua", "Jeroen", ""], ["Felzenszwalb", "Pedro F.", ""]]}, {"id": "1606.01377", "submitter": "Rujie Yin", "authors": "Rujie Yin, Tingran Gao, Yue M. Lu, and Ingrid Daubechies", "title": "A Tale of Two Bases: Local-Nonlocal Regularization on Image Patches with\n  Convolution Framelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image representation scheme combining the local and nonlocal\ncharacterization of patches in an image. Our representation scheme can be shown\nto be equivalent to a tight frame constructed from convolving local bases (e.g.\nwavelet frames, discrete cosine transforms, etc.) with nonlocal bases (e.g.\nspectral basis induced by nonlinear dimension reduction on patches), and we\ncall the resulting frame elements {\\it convolution framelets}. Insight gained\nfrom analyzing the proposed representation leads to a novel interpretation of a\nrecent high-performance patch-based image inpainting algorithm using Point\nIntegral Method (PIM) and Low Dimension Manifold Model (LDMM) [Osher, Shi and\nZhu, 2016]. In particular, we show that LDMM is a weighted\n$\\ell_2$-regularization on the coefficients obtained by decomposing images into\nlinear combinations of convolution framelets; based on this understanding, we\nextend the original LDMM to a reweighted version that yields further improved\ninpainting results. In addition, we establish the energy concentration property\nof convolution framelet coefficients for the setting where the local basis is\nconstructed from a given nonlocal basis via a linear reconstruction framework;\na generalization of this framework to unions of local embeddings can provide a\nnatural setting for interpreting BM3D, one of the state-of-the-art image\ndenoising algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 13:39:18 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 03:31:17 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 23:07:29 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Yin", "Rujie", ""], ["Gao", "Tingran", ""], ["Lu", "Yue M.", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1606.01393", "submitter": "Karan Sharma", "authors": "Karan Sharma, Arun CS Kumar, Suchendra Bhandarkar", "title": "Automated Image Captioning for Rapid Prototyping and Resource\n  Constrained Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant performance gains in deep learning coupled with the exponential\ngrowth of image and video data on the Internet have resulted in the recent\nemergence of automated image captioning systems. Ensuring scalability of\nautomated image captioning systems with respect to the ever increasing volume\nof image and video data is a significant challenge. This paper provides a\nvaluable insight in that the detection of a few significant (top) objects in an\nimage allows one to extract other relevant information such as actions (verbs)\nin the image. We expect this insight to be useful in the design of scalable\nimage captioning systems. We address two parameters by which the scalability of\nimage captioning systems could be quantified, i.e., the traditional algorithmic\ntime complexity which is important given the resource limitations of the user\ndevice and the system development time since the programmers' time is a\ncritical resource constraint in many real-world scenarios. Additionally, we\naddress the issue of how word embeddings could be used to infer the verb\n(action) from the nouns (objects) in a given image in a zero-shot manner. Our\nresults show that it is possible to attain reasonably good performance on\npredicting actions and captioning images using our approaches with the added\nadvantage of simplicity of implementation.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 16:51:37 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Sharma", "Karan", ""], ["Kumar", "Arun CS", ""], ["Bhandarkar", "Suchendra", ""]]}, {"id": "1606.01453", "submitter": "Chiranjoy Chattopadhyay", "authors": "Pratik Kalshetti, Manas Bundele, Parag Rahangdale, Dinesh Jangra,\n  Chiranjoy Chattopadhyay, Gaurav Harit, Abhay Elhence", "title": "An Interactive Medical Image Segmentation Framework Using Iterative\n  Refinement", "comments": "19 pages, 19 figures, Submitted for review in Computers in Biology\n  and Medicine", "journal-ref": null, "doi": "10.1016/j.compbiomed.2017.02.002", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is often performed on medical images for identifying\ndiseases in clinical evaluation. Hence it has become one of the major research\nareas. Conventional image segmentation techniques are unable to provide\nsatisfactory segmentation results for medical images as they contain\nirregularities. They need to be pre-processed before segmentation. In order to\nobtain the most suitable method for medical image segmentation, we propose a\ntwo stage algorithm. The first stage automatically generates a binary marker\nimage of the region of interest using mathematical morphology. This marker\nserves as the mask image for the second stage which uses GrabCut on the input\nimage thus resulting in an efficient segmented result. The obtained result can\nbe further refined by user interaction which can be done using the Graphical\nUser Interface (GUI). Experimental results show that the proposed method is\naccurate and provides satisfactory segmentation results with minimum user\ninteraction on medical as well as natural images.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 02:35:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Kalshetti", "Pratik", ""], ["Bundele", "Manas", ""], ["Rahangdale", "Parag", ""], ["Jangra", "Dinesh", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Harit", "Gaurav", ""], ["Elhence", "Abhay", ""]]}, {"id": "1606.01455", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim,\n  Jung-Woo Ha, Byoung-Tak Zhang", "title": "Multimodal Residual Learning for Visual QA", "comments": "13 pages, 7 figures, accepted for NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks continue to advance the state-of-the-art of image\nrecognition tasks with various methods. However, applications of these methods\nto multimodality remain limited. We present Multimodal Residual Networks (MRN)\nfor the multimodal residual learning of visual question-answering, which\nextends the idea of the deep residual learning. Unlike the deep residual\nlearning, MRN effectively learns the joint representation from vision and\nlanguage information. The main idea is to use element-wise multiplication for\nthe joint residual mappings exploiting the residual learning of the attentional\nmodels in recent studies. Various alternative models introduced by\nmultimodality are explored based on our study. We achieve the state-of-the-art\nresults on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks.\nMoreover, we introduce a novel method to visualize the attention effect of the\njoint representations for each learning block using back-propagation algorithm,\neven though the visual features are collapsed without spatial information.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 02:38:20 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 08:28:38 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Lee", "Sang-Woo", ""], ["Kwak", "Dong-Hyun", ""], ["Heo", "Min-Oh", ""], ["Kim", "Jeonghee", ""], ["Ha", "Jung-Woo", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1606.01460", "submitter": "Jing Zhang", "authors": "Jing Zhang, Yang Cao and Zengfu Wang", "title": "Nighttime Haze Removal with Illumination Correction", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze removal is important for computational photography and computer vision\napplications. However, most of the existing methods for dehazing are designed\nfor daytime images, and cannot always work well in the nighttime. Different\nfrom the imaging conditions in the daytime, images captured in nighttime haze\ncondition may suffer from non-uniform illumination due to artificial light\nsources, which exhibit low brightness/contrast and color distortion. In this\npaper, we present a new nighttime hazy imaging model that takes into account\nboth the non-uniform illumination from artificial light sources and the\nscattering and attenuation effects of haze. Accordingly, we propose an\nefficient dehazing algorithm for nighttime hazy images. The proposed algorithm\nincludes three sequential steps. i) It enhances the overall brightness by\nperforming a gamma correction step after estimating the illumination from the\noriginal image. ii) Then it achieves a color-balance result by performing a\ncolor correction step after estimating the color characteristics of the\nincident light. iii) Finally, it remove the haze effect by applying the dark\nchannel prior and estimating the point-wise environmental light based on the\nprevious illumination-balance result. Experimental results show that the\nproposed algorithm can achieve illumination-balance and haze-free results with\ngood color rendition ability.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 04:15:53 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Zhang", "Jing", ""], ["Cao", "Yang", ""], ["Wang", "Zengfu", ""]]}, {"id": "1606.01481", "submitter": "Qiyang Zhao", "authors": "Qiyang Zhao, Lewis D Griffin", "title": "Better Image Segmentation by Exploiting Dense Semantic Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well accepted that image segmentation can benefit from utilizing\nmultilevel cues. The paper focuses on utilizing the FCNN-based dense semantic\npredictions in the bottom-up image segmentation, arguing to take semantic cues\ninto account from the very beginning. By this we can avoid merging regions of\nsimilar appearance but distinct semantic categories as possible. The semantic\ninefficiency problem is handled. We also propose a straightforward way to use\nthe contour cues to suppress the noise in multilevel cues, thus to improve the\nsegmentation robustness. The evaluation on the BSDS500 shows that we obtain the\ncompetitive region and boundary performance. Furthermore, since all individual\nregions can be assigned with appropriate semantic labels during the\ncomputation, we are capable of extracting the adjusted semantic segmentations.\nThe experiment on Pascal VOC 2012 shows our improvement to the original\nsemantic segmentations which derives directly from the dense predictions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 08:52:06 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Zhao", "Qiyang", ""], ["Griffin", "Lewis D", ""]]}, {"id": "1606.01519", "submitter": "Amir Adler", "authors": "Amir Adler, David Boublil, Michael Elad and Michael Zibulevsky", "title": "A Deep Learning Approach to Block-based Compressed Sensing of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is a signal processing framework for efficiently\nreconstructing a signal from a small number of measurements, obtained by linear\nprojections of the signal. Block-based CS is a lightweight CS approach that is\nmostly suitable for processing very high-dimensional images and videos: it\noperates on local patches, employs a low-complexity reconstruction operator and\nrequires significantly less memory to store the sensing matrix. In this paper\nwe present a deep learning approach for block-based CS, in which a\nfully-connected network performs both the block-based linear sensing and\nnon-linear reconstruction stages. During the training phase, the sensing matrix\nand the non-linear reconstruction operator are \\emph{jointly} optimized, and\nthe proposed approach outperforms state-of-the-art both in terms of\nreconstruction quality and computation time. For example, at a 25% sensing rate\nthe average PSNR advantage is 0.77dB and computation time is over 200-times\nfaster.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 14:40:54 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Adler", "Amir", ""], ["Boublil", "David", ""], ["Elad", "Michael", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1606.01535", "submitter": "Kevin Jarrett", "authors": "Kevin Jarrett, Koray Kvukcuoglu, Karol Gregor and Yann LeCun", "title": "What is the Best Feature Learning Procedure in Hierarchical Recognition\n  Architectures?", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (This paper was written in November 2011 and never published. It is posted on\narXiv.org in its original form in June 2016). Many recent object recognition\nsystems have proposed using a two phase training procedure to learn sparse\nconvolutional feature hierarchies: unsupervised pre-training followed by\nsupervised fine-tuning. Recent results suggest that these methods provide\nlittle improvement over purely supervised systems when the appropriate\nnonlinearities are included. This paper presents an empirical exploration of\nthe space of learning procedures for sparse convolutional networks to assess\nwhich method produces the best performance. In our study, we introduce an\naugmentation of the Predictive Sparse Decomposition method that includes a\ndiscriminative term (DPSD). We also introduce a new single phase supervised\nlearning procedure that places an L1 penalty on the output state of each layer\nof the network. This forces the network to produce sparse codes without the\nexpensive pre-training phase. Using DPSD with a new, complex predictor that\nincorporates lateral inhibition, combined with multi-scale feature pooling, and\nsupervised refinement, the system achieves a 70.6\\% recognition rate on\nCaltech-101. With the addition of convolutional training, a 77\\% recognition\nwas obtained on the CIfAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 17:31:39 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Jarrett", "Kevin", ""], ["Kvukcuoglu", "Koray", ""], ["Gregor", "Karol", ""], ["LeCun", "Yann", ""]]}, {"id": "1606.01550", "submitter": "Relja Arandjelovi\\'c", "authors": "Artem Babenko, Relja Arandjelovi\\'c, Victor Lempitsky", "title": "Pairwise Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of lossy compression of high-dimensional vectors through\nquantization. We propose the approach that learns quantization parameters by\nminimizing the distortion of scalar products and squared distances between\npairs of points. This is in contrast to previous works that obtain these\nparameters through the minimization of the reconstruction error of individual\npoints. The proposed approach proceeds by finding a linear transformation of\nthe data that effectively reduces the minimization of the pairwise distortions\nto the minimization of individual reconstruction errors. After such\ntransformation, any of the previously-proposed quantization approaches can be\nused. Despite the simplicity of this transformation, the experiments\ndemonstrate that it achieves considerable reduction of the pairwise distortions\ncompared to applying quantization directly to the untransformed data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 19:57:06 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Babenko", "Artem", ""], ["Arandjelovi\u0107", "Relja", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1606.01561", "submitter": "Khalid Ashraf", "authors": "Khalid Ashraf, Bichen Wu, Forrest N. Iandola, Mattthew W. Moskewicz,\n  Kurt Keutzer", "title": "Shallow Networks for High-Accuracy Road Object-Detection", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically detect other vehicles on the road is vital to\nthe safety of partially-autonomous and fully-autonomous vehicles. Most of the\nhigh-accuracy techniques for this task are based on R-CNN or one of its faster\nvariants. In the research community, much emphasis has been applied to using 3D\nvision or complex R-CNN variants to achieve higher accuracy. However, are there\nmore straightforward modifications that could deliver higher accuracy? Yes. We\nshow that increasing input image resolution (i.e. upsampling) offers up to 12\npercentage-points higher accuracy compared to an off-the-shelf baseline. We\nalso find situations where earlier/shallower layers of CNN provide higher\naccuracy than later/deeper layers. We further show that shallow models and\nupsampled images yield competitive accuracy. Our findings contrast with the\ncurrent trend towards deeper and larger models to achieve high accuracy in\ndomain specific detection tasks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 21:04:46 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ashraf", "Khalid", ""], ["Wu", "Bichen", ""], ["Iandola", "Forrest N.", ""], ["Moskewicz", "Mattthew W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1606.01568", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza and Vittorio Murino", "title": "Active Regression with Adaptive Huber Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the scalar regression problem through a novel solution\nto exactly optimize the Huber loss in a general semi-supervised setting, which\ncombines multi-view learning and manifold regularization. We propose a\nprincipled algorithm to 1) avoid computationally expensive iterative schemes\nwhile 2) adapting the Huber loss threshold in a data-driven fashion and 3)\nactively balancing the use of labelled data to remove noisy or inconsistent\nannotations at the training stage. In a wide experimental evaluation, dealing\nwith diverse applications, we assess the superiority of our paradigm which is\nable to combine robustness towards noise with both strong performance and low\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 21:59:34 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 07:24:43 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Cavazza", "Jacopo", ""], ["Murino", "Vittorio", ""]]}, {"id": "1606.01595", "submitter": "Chunhua Shen", "authors": "Lin Wu, Chunhua Shen, Anton van den Hengel", "title": "Deep Linear Discriminant Analysis on Fisher Networks: A Hybrid\n  Architecture for Person Re-identification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is to seek a correct match for a person of interest\nacross views among a large number of imposters. It typically involves two\nprocedures of non-linear feature extractions against dramatic appearance\nchanges, and subsequent discriminative analysis in order to reduce intra-\npersonal variations while enlarging inter-personal differences. In this paper,\nwe introduce a hybrid architecture which combines Fisher vectors and deep\nneural networks to learn non-linear representations of person images to a space\nwhere data can be linearly separable. We reinforce a Linear Discriminant\nAnalysis (LDA) on top of the deep neural network such that linearly separable\nlatent representations can be learnt in an end-to-end fashion. By optimizing an\nobjective function modified from LDA, the network is enforced to produce\nfeature distributions which have a low variance within the same class and high\nvariance between classes. The objective is essentially derived from the general\nLDA eigenvalue problem and allows to train the network with stochastic gradient\ndescent and back-propagate LDA gradients to compute the gradients involved in\nFisher vector encoding. For evaluation we test our approach on four benchmark\ndata sets in person re-identification (VIPeR [1], CUHK03 [2], CUHK01 [3], and\nMarket1501 [4]). Extensive experiments on these benchmarks show that our model\ncan achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 02:11:15 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Wu", "Lin", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1606.01601", "submitter": "Jiaping Zhao", "authors": "Jiaping Zhao and Laurent Itti", "title": "shapeDTW: shape Dynamic Time Warping", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Time Warping (DTW) is an algorithm to align temporal sequences with\npossible local non-linear distortions, and has been widely applied to audio,\nvideo and graphics data alignments. DTW is essentially a point-to-point\nmatching method under some boundary and temporal consistency constraints.\nAlthough DTW obtains a global optimal solution, it does not necessarily achieve\nlocally sensible matchings. Concretely, two temporal points with entirely\ndissimilar local structures may be matched by DTW. To address this problem, we\npropose an improved alignment algorithm, named shape Dynamic Time Warping\n(shapeDTW), which enhances DTW by taking point-wise local structural\ninformation into consideration. shapeDTW is inherently a DTW algorithm, but\nadditionally attempts to pair locally similar structures and to avoid matching\npoints with distinct neighborhood structures. We apply shapeDTW to align audio\nsignal pairs having ground-truth alignments, as well as artificially simulated\npairs of aligned sequences, and obtain quantitatively much lower alignment\nerrors than DTW and its two variants. When shapeDTW is used as a distance\nmeasure in a nearest neighbor classifier (NN-shapeDTW) to classify time series,\nit beats DTW on 64 out of 84 UCR time series datasets, with significantly\nimproved classification accuracies. By using a properly designed local\nstructure descriptor, shapeDTW improves accuracies by more than 10% on 18\ndatasets. To the best of our knowledge, shapeDTW is the first distance measure\nunder the nearest neighbor classifier scheme to significantly outperform DTW,\nwhich had been widely recognized as the best distance measure to date. Our code\nis publicly accessible at: https://github.com/jiapingz/shapeDTW.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 02:38:01 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Zhao", "Jiaping", ""], ["Itti", "Laurent", ""]]}, {"id": "1606.01609", "submitter": "Chunhua Shen", "authors": "Lin Wu, Chunhua Shen, Anton van den Hengel", "title": "Deep Recurrent Convolutional Networks for Video-based Person\n  Re-identification: An End-to-End Approach", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an end-to-end approach to simultaneously learn\nspatio-temporal features and corresponding similarity metric for video-based\nperson re-identification. Given the video sequence of a person, features from\neach frame that are extracted from all levels of a deep convolutional network\ncan preserve a higher spatial resolution from which we can model finer motion\npatterns. These low-level visual percepts are leveraged into a variant of\nrecurrent model to characterize the temporal variation between time-steps.\nFeatures from all time-steps are then summarized using temporal pooling to\nproduce an overall feature representation for the complete sequence. The deep\nconvolutional network, recurrent layer, and the temporal pooling are jointly\ntrained to extract comparable hidden-unit representations from input pair of\ntime series to compute their corresponding similarity value. The proposed\nframework combines time series modeling and metric learning to jointly learn\nrelevant features and a good similarity measure between time sequences of\nperson.\n  Experiments demonstrate that our approach achieves the state-of-the-art\nperformance for video-based person re-identification on iLIDS-VID and PRID\n2011, the two primary public datasets for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 04:29:16 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 10:52:09 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Wu", "Lin", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1606.01621", "submitter": "Shu Kong", "authors": "Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, Charless Fowlkes", "title": "Photo Aesthetics Ranking Network with Attributes and Content Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications could benefit from the ability to automatically\ngenerate a fine-grained ranking of photo aesthetics. However, previous methods\nfor image aesthetics analysis have primarily focused on the coarse, binary\ncategorization of images into high- or low-aesthetic categories. In this work,\nwe propose to learn a deep convolutional neural network to rank photo\naesthetics in which the relative ranking of photo aesthetics are directly\nmodeled in the loss function. Our model incorporates joint learning of\nmeaningful photographic attributes and image content information which can help\nregularize the complicated photo aesthetics rating problem.\n  To train and analyze this model, we have assembled a new aesthetics and\nattributes database (AADB) which contains aesthetic scores and meaningful\nattributes assigned to each image by multiple human raters. Anonymized rater\nidentities are recorded across images allowing us to exploit intra-rater\nconsistency using a novel sampling strategy when computing the ranking loss of\ntraining image pairs. We show the proposed sampling strategy is very effective\nand robust in face of subjective judgement of image aesthetics by individuals\nwith different aesthetic tastes. Experiments demonstrate that our unified model\ncan generate aesthetic rankings that are more consistent with human ratings. To\nfurther validate our model, we show that by simply thresholding the estimated\naesthetic scores, we are able to achieve state-or-the-art classification\nperformance on the existing AVA dataset benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 06:14:00 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 00:20:07 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kong", "Shu", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Mech", "Radomir", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1606.01672", "submitter": "Minkyu Choi", "authors": "Minkyu Choi and Jun Tani", "title": "Predictive Coding for Dynamic Vision : Development of Functional\n  Hierarchy in a Multiple Spatio-Temporal Scales RNN Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper presents a novel recurrent neural network model, the\npredictive multiple spatio-temporal scales RNN (P-MSTRNN), which can generate\nas well as recognize dynamic visual patterns in the predictive coding\nframework. The model is characterized by multiple spatio-temporal scales\nimposed on neural unit dynamics through which an adequate spatio-temporal\nhierarchy develops via learning from exemplars. The model was evaluated by\nconducting an experiment of learning a set of whole body human movement\npatterns which was generated by following a hierarchically defined movement\nsyntax. The analysis of the trained model clarifies what types of\nspatio-temporal hierarchy develop in dynamic neural activity as well as how\nrobust generation and recognition of movement patterns can be achieved by using\nthe error minimization principle.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 09:34:19 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 06:35:00 GMT"}, {"version": "v3", "created": "Fri, 17 Mar 2017 05:20:07 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Choi", "Minkyu", ""], ["Tani", "Jun", ""]]}, {"id": "1606.01721", "submitter": "John See", "authors": "Sze-Teng Liong, John See, KokSheik Wong, Raphael C.-W. Phan", "title": "Less is More: Micro-expression Recognition from Video using Apex Frame", "comments": "14 pages double-column, author affiliations updated, acknowledgment\n  of grant support added", "journal-ref": "Signal Processing: Image Communication, Vol. 62, March 2018, pages\n  82-92", "doi": "10.1016/j.image.2017.11.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent interest and advances in facial micro-expression research,\nthere is still plenty room for improvement in terms of micro-expression\nrecognition. Conventional feature extraction approaches for micro-expression\nvideo consider either the whole video sequence or a part of it, for\nrepresentation. However, with the high-speed video capture of micro-expressions\n(100-200 fps), are all frames necessary to provide a sufficiently meaningful\nrepresentation? Is the luxury of data a bane to accurate recognition? A novel\nproposition is presented in this paper, whereby we utilize only two images per\nvideo: the apex frame and the onset frame. The apex frame of a video contains\nthe highest intensity of expression changes among all frames, while the onset\nis the perfect choice of a reference frame with neutral expression. A new\nfeature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF) is proposed to\nencode essential expressiveness of the apex frame. We evaluated the proposed\nmethod on five micro-expression databases: CAS(ME)$^2$, CASME II, SMIC-HS,\nSMIC-NIR and SMIC-VIS. Our experiments lend credence to our hypothesis, with\nour proposed technique achieving a state-of-the-art F1-score recognition\nperformance of 61% and 62% in the high frame rate CASME II and SMIC-HS\ndatabases respectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 12:59:14 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 04:04:24 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 06:28:15 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Liong", "Sze-Teng", ""], ["See", "John", ""], ["Wong", "KokSheik", ""], ["Phan", "Raphael C. -W.", ""]]}, {"id": "1606.01735", "submitter": "Hakan Bilen", "authors": "Hakan Bilen and Andrea Vedaldi", "title": "Integrated perception with recurrent multi-task neural networks", "comments": "9 pages, 3 figures, 2 tables", "journal-ref": "Advances in Neural Information Processing (NIPS) 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern discriminative predictors have been shown to match natural\nintelligences in specific perceptual tasks in image classification, object and\npart detection, boundary extraction, etc. However, a major advantage that\nnatural intelligences still have is that they work well for \"all\" perceptual\nproblems together, solving them efficiently and coherently in an \"integrated\nmanner\". In order to capture some of these advantages in machine perception, we\nask two questions: whether deep neural networks can learn universal image\nrepresentations, useful not only for a single task but for all of them, and how\nthe solutions to the different tasks can be integrated in this framework. We\nanswer by proposing a new architecture, which we call \"MultiNet\", in which not\nonly deep image features are shared between tasks, but where tasks can interact\nin a recurrent manner by encoding the results of their analysis in a common\nshared representation of the data. In this manner, we show that the performance\nof individual tasks in standard benchmarks can be improved first by sharing\nfeatures between them and then, more significantly, by integrating their\nsolutions in the common representation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:27:25 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 14:38:00 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1606.01746", "submitter": "Amelia Sim\\'o", "authors": "Sonia Barahona, Ximo Gual-Arnau, Maria Victoria Ib\\'a\\~nez and Amelia\n  Sim\\'o", "title": "Unsupervised classification of children's bodies using currents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification according to their shape and size is of key importance\nin many scientific fields. This work focuses on the case where the size and\nshape of an object is characterized by a current}. A current is a mathematical\nobject which has been proved relevant to the modeling of geometrical data, like\nsubmanifolds, through integration of vector fields along them. As a consequence\nof the choice of a vector-valued Reproducing Kernel Hilbert Space (RKHS) as a\ntest space for integrating manifolds, it is possible to consider that shapes\nare embedded in this Hilbert Space. A vector-valued RKHS is a Hilbert space of\nvector fields; therefore, it is possible to compute a mean of shapes, or to\ncalculate a distance between two manifolds. This embedding enables us to\nconsider size-and-shape classification algorithms.\n  These algorithms are applied to a 3D database obtained from an anthropometric\nsurvey of the Spanish child population with a potential application to online\nsales of children's wear.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 13:52:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Barahona", "Sonia", ""], ["Gual-Arnau", "Ximo", ""], ["Ib\u00e1\u00f1ez", "Maria Victoria", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "1606.01847", "submitter": "Marcus Rohrbach", "authors": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor\n  Darrell, and Marcus Rohrbach", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and\n  Visual Grounding", "comments": "Accepted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling textual or visual information with vector representations trained\nfrom large language or visual datasets has been successfully explored in recent\nyears. However, tasks such as visual question answering require combining these\nvector representations with each other. Approaches to multimodal pooling\ninclude element-wise product or sum, as well as concatenation of the visual and\ntextual representations. We hypothesize that these methods are not as\nexpressive as an outer product of the visual and textual vectors. As the outer\nproduct is typically infeasible due to its high dimensionality, we instead\npropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and\nexpressively combine multimodal features. We extensively evaluate MCB on the\nvisual question answering and grounding tasks. We consistently show the benefit\nof MCB over ablations without MCB. For visual question answering, we present an\narchitecture which uses MCB twice, once for predicting attention over spatial\nfeatures and again to combine the attended representation with the question\nrepresentation. This model outperforms the state-of-the-art on the Visual7W\ndataset and the VQA challenge.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 17:59:56 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 19:52:41 GMT"}, {"version": "v3", "created": "Sat, 24 Sep 2016 01:58:59 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Fukui", "Akira", ""], ["Park", "Dong Huk", ""], ["Yang", "Daylen", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1606.01873", "submitter": "Matthias Hullin", "authors": "Jonathan Klein, Christoph Peters, Jaime Mart\\'in, Martin Laurenzis,\n  Matthias B. Hullin", "title": "Optically lightweight tracking of objects around a corner", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1038/srep32491", "report-no": null, "categories": "cs.CV cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The observation of objects located in inaccessible regions is a recurring\nchallenge in a wide variety of important applications. Recent work has shown\nthat indirect diffuse light reflections can be used to reconstruct objects and\ntwo-dimensional (2D) patterns around a corner. However, these prior methods\nalways require some specialized setup involving either ultrafast detectors or\nnarrowband light sources. Here we show that occluded objects can be tracked in\nreal time using a standard 2D camera and a laser pointer. Unlike previous\nmethods based on the backprojection approach, we formulate the problem in an\nanalysis-by-synthesis sense. By repeatedly simulating light transport through\nthe scene, we determine the set of object parameters that most closely fits the\nmeasured intensity distribution. We experimentally demonstrate that this\napproach is capable of following the translation of unknown objects, and\ntranslation and orientation of a known object, in real time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 09:17:02 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Klein", "Jonathan", ""], ["Peters", "Christoph", ""], ["Mart\u00edn", "Jaime", ""], ["Laurenzis", "Martin", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1606.01981", "submitter": "Paul Merolla", "authors": "Paul Merolla, Rathinakumar Appuswamy, John Arthur, Steve K. Esser,\n  Dharmendra Modha", "title": "Deep neural networks are robust to weight binarization and other\n  non-linear distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results show that deep neural networks achieve excellent performance\neven when, during training, weights are quantized and projected to a binary\nrepresentation. Here, we show that this is just the tip of the iceberg: these\nsame networks, during testing, also exhibit a remarkable robustness to\ndistortions beyond quantization, including additive and multiplicative noise,\nand a class of non-linear projections where binarization is just a special\ncase. To quantify this robustness, we show that one such network achieves 11%\ntest error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore,\nwe find that a common training heuristic--namely, projecting quantized weights\nduring backpropagation--can be altered (or even removed) and networks still\nachieve a base level of robustness during testing. Specifically, training with\nweight projections other than quantization also works, as does simply clipping\nthe weights, both of which have never been reported before. We confirm our\nresults for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas,\nwe propose a stochastic projection rule that leads to a new state of the art\nnetwork with 7.64% test error on CIFAR-10 using no data augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 00:28:42 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Merolla", "Paul", ""], ["Appuswamy", "Rathinakumar", ""], ["Arthur", "John", ""], ["Esser", "Steve K.", ""], ["Modha", "Dharmendra", ""]]}, {"id": "1606.02009", "submitter": "Salman Khan Mr.", "authors": "Salman H Khan, Xuming He, Fatih Porikli, Mohammed Bennamoun, Ferdous\n  Sohel, Roberto Togneri", "title": "Learning deep structured network for weakly supervised change detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional change detection methods require a large number of images to\nlearn background models or depend on tedious pixel-level labeling by humans. In\nthis paper, we present a weakly supervised approach that needs only image-level\nlabels to simultaneously detect and localize changes in a pair of images. To\nthis end, we employ a deep neural network with DAG topology to learn patterns\nof change from image-level labeled training data. On top of the initial CNN\nactivations, we define a CRF model to incorporate the local differences and\ncontext with the dense connections between individual pixels. We apply a\nconstrained mean-field algorithm to estimate the pixel-level labels, and use\nthe estimated labels to update the parameters of the CNN in an iterative EM\nframework. This enables imposing global constraints on the observed foreground\nprobability mass function. Our evaluations on four benchmark datasets\ndemonstrate superior detection and localization performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 03:20:37 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 01:22:06 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Khan", "Salman H", ""], ["He", "Xuming", ""], ["Porikli", "Fatih", ""], ["Bennamoun", "Mohammed", ""], ["Sohel", "Ferdous", ""], ["Togneri", "Roberto", ""]]}, {"id": "1606.02031", "submitter": "Li Cheng", "authors": "Chi Xu, Lakshmi Narasimhan Govindarajan, Li Cheng", "title": "Hand Action Detection from Ego-centric Depth Sequences with\n  Error-correcting Hough Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting hand actions from ego-centric depth sequences is a practically\nchallenging problem, owing mostly to the complex and dexterous nature of hand\narticulations as well as non-stationary camera motion. We address this problem\nvia a Hough transform based approach coupled with a discriminatively learned\nerror-correcting component to tackle the well known issue of incorrect votes\nfrom the Hough transform. In this framework, local parts vote collectively for\nthe start $\\&$ end positions of each action over time. We also construct an\nin-house annotated dataset of 300 long videos, containing 3,177 single-action\nsubsequences over 16 action classes collected from 26 individuals. Our system\nis empirically evaluated on this real-life dataset for both the action\nrecognition and detection tasks, and is shown to produce satisfactory results.\nTo facilitate reproduction, the new dataset and our implementation are also\nprovided online.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 05:02:14 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Xu", "Chi", ""], ["Govindarajan", "Lakshmi Narasimhan", ""], ["Cheng", "Li", ""]]}, {"id": "1606.02092", "submitter": "Johannes Berger", "authors": "Johannes Berger and Christoph Schn\\\"orr", "title": "Joint Recursive Monocular Filtering of Camera Motion and Disparity Map", "comments": "Preprint. The final publication will be available at Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular scene reconstruction is essential for modern applications such as\nrobotics or autonomous driving. Although stereo methods usually result in\nbetter accuracy than monocular methods, they are more expensive and more\ndifficult to calibrate. In this work, we present a novel second order optimal\nminimum energy filter that jointly estimates the camera motion, the disparity\nmap and also higher order kinematics recursively on a product Lie group\ncontaining a novel disparity group. This mathematical framework enables to cope\nwith non-Euclidean state spaces, non-linear observations and high dimensions\nwhich is infeasible for most classical filters. To be robust against outliers,\nwe use a generalized Charbonnier energy function in this framework rather than\na quadratic energy function as proposed in related work. Experiments confirm\nthat our method enables accurate reconstructions on-par with state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 10:53:19 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Berger", "Johannes", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1606.02147", "submitter": "Adam Paszke", "authors": "Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello", "title": "ENet: A Deep Neural Network Architecture for Real-Time Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform pixel-wise semantic segmentation in real-time is of\nparamount importance in mobile applications. Recent deep neural networks aimed\nat this task have the disadvantage of requiring a large number of floating\npoint operations and have long run-times that hinder their usability. In this\npaper, we propose a novel deep neural network architecture named ENet\n(efficient neural network), created specifically for tasks requiring low\nlatency operation. ENet is up to 18$\\times$ faster, requires 75$\\times$ less\nFLOPs, has 79$\\times$ less parameters, and provides similar or better accuracy\nto existing models. We have tested it on CamVid, Cityscapes and SUN datasets\nand report on comparisons with existing state-of-the-art methods, and the\ntrade-offs between accuracy and processing time of a network. We present\nperformance measurements of the proposed architecture on embedded systems and\nsuggest possible software improvements that could make ENet even faster.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 14:09:27 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Paszke", "Adam", ""], ["Chaurasia", "Abhishek", ""], ["Kim", "Sangpil", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1606.02170", "submitter": "Shangzhen Luan", "authors": "Shangzhen Luan, Baochang Zhang, Jungong Han, Chen Chen, Ling Shao,\n  Alessandro Perina and Linlin Shen", "title": "Latent Constrained Correlation Filters for Object Localization", "comments": "There are small problems and theories need to be perfected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a neglected fact in the traditional machine learning methods that\nthe data sampling can actually lead to the solution sampling. We consider this\nobservation to be important because having the solution sampling available\nmakes the variable distribution estimation, which is a problem in many\nlearning-related applications, more tractable. In this paper, we implement this\nidea on correlation filter, which has attracted much attention in the past few\nyears due to its high performance with a low computational cost. More\nspecifically, we propose a new method, named latent constrained correlation\nfilters (LCCF) by mapping the correlation filters to a given latent subspace,\nin which we establish a new learning framework that embeds distribution-related\nconstraints into the original problem. We further introduce a subspace based\nalternating direction method of multipliers (SADMM) to efficiently solve the\noptimization problem, which is proved to converge at the saddle point. Our\napproach is successfully applied to two different tasks inclduing eye\nlocalization and car detection. Extensive experiments demonstrate that LCCF\noutperforms the state-of-the-art methods when samples are suffered from noise\nand occlusion.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 15:13:00 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 09:05:04 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Luan", "Shangzhen", ""], ["Zhang", "Baochang", ""], ["Han", "Jungong", ""], ["Chen", "Chen", ""], ["Shao", "Ling", ""], ["Perina", "Alessandro", ""], ["Shen", "Linlin", ""]]}, {"id": "1606.02210", "submitter": "Amir Ghaderi", "authors": "Amir Ghaderi, Vassilis Athitsos", "title": "Selective Unsupervised Feature Learning with Convolutional Neural\n  Network (S-CNN)", "comments": null, "journal-ref": null, "doi": "10.1109/ICPR.2016.7900009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning of convolutional neural networks (CNNs) can require very\nlarge amounts of labeled data. Labeling thousands or millions of training\nexamples can be extremely time consuming and costly. One direction towards\naddressing this problem is to create features from unlabeled data. In this\npaper we propose a new method for training a CNN, with no need for labeled\ninstances. This method for unsupervised feature learning is then successfully\napplied to a challenging object recognition task. The proposed algorithm is\nrelatively simple, but attains accuracy comparable to that of more\nsophisticated methods. The proposed method is significantly easier to train,\ncompared to existing CNN methods, making fewer requirements on manually labeled\ntraining data. It is also shown to be resistant to overfitting. We provide\nresults on some well-known datasets, namely STL-10, CIFAR-10, and CIFAR-100.\nThe results show that our method provides competitive performance compared with\nexisting alternative methods. Selective Convolutional Neural Network (S-CNN) is\na simple and fast algorithm, it introduces a new way to do unsupervised feature\nlearning, and it provides discriminative features which generalize well.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 16:52:47 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Ghaderi", "Amir", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1606.02228", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas", "title": "Systematic evaluation of CNN advances on the ImageNet", "comments": "Submitted to CVIU Special Issue on Deep Learning. Updated dataset\n  quality experiment", "journal-ref": null, "doi": "10.1016/j.cviu.2017.05.007", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper systematically studies the impact of a range of recent advances in\nCNN architectures and learning methods on the object categorization (ILSVRC)\nproblem. The evalution tests the influence of the following choices of the\narchitecture: non-linearity (ReLU, ELU, maxout, compatibility with batch\nnormalization), pooling variants (stochastic, max, average, mixed), network\nwidth, classifier design (convolutional, fully-connected, SPP), image\npre-processing, and of learning parameters: learning rate, batch size,\ncleanliness of the data, etc.\n  The performance gains of the proposed modifications are first tested\nindividually and then in combination. The sum of individual gains is bigger\nthan the observed improvement when all modifications are introduced, but the\n\"deficit\" is small suggesting independence of their benefits. We show that the\nuse of 128x128 pixel images is sufficient to make qualitative conclusions about\noptimal network structure that hold for the full size Caffe and VGG nets. The\nresults are obtained an order of magnitude faster than with the standard 224\npixel images.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 17:38:06 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 13:48:39 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Sergievskiy", "Nikolay", ""], ["Matas", "Jiri", ""]]}, {"id": "1606.02254", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui", "title": "Longitudinal Face Modeling via Temporal Deep Restricted Boltzmann\n  Machines", "comments": "in The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2016", "journal-ref": null, "doi": "10.1007/s11263-019-01165-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the face aging process is a challenging task due to large and\nnon-linear variations present in different stages of face development. This\npaper presents a deep model approach for face age progression that can\nefficiently capture the non-linear aging process and automatically synthesize a\nseries of age-progressed faces in various age ranges. In this approach, we\nfirst decompose the long-term age progress into a sequence of short-term\nchanges and model it as a face sequence. The Temporal Deep Restricted Boltzmann\nMachines based age progression model together with the prototype faces are then\nconstructed to learn the aging transformation between faces in the sequence. In\naddition, to enhance the wrinkles of faces in the later age ranges, the wrinkle\nmodels are further constructed using Restricted Boltzmann Machines to capture\ntheir variations in different facial regions. The geometry constraints are also\ntaken into account in the last step for more consistent age-progressed results.\nThe proposed approach is evaluated using various face aging databases, i.e.\nFG-NET, Cross-Age Celebrity Dataset (CACD) and MORPH, and our collected\nlarge-scale aging database named AginG Faces in the Wild (AGFW). In addition,\nwhen ground-truth age is not available for input image, our proposed system is\nable to automatically estimate the age of the input face before aging process\nis employed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 18:37:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Quach", "Kha Gia", ""], ["Bui", "Tien D.", ""]]}, {"id": "1606.02276", "submitter": "Mercan Topkara", "authors": "Nikolaos Pappas, Miriam Redi, Mercan Topkara, Brendan Jou, Hongyi Liu,\n  Tao Chen, Shih-Fu Chang", "title": "Multilingual Visual Sentiment Concept Matching", "comments": null, "journal-ref": "Proceedings ICMR '16 Proceedings of the 2016 ACM on International\n  Conference on Multimedia Retrieval Pages 151-158", "doi": "10.1145/2911996.2912016", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:40:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Jou", "Brendan", ""], ["Liu", "Hongyi", ""], ["Chen", "Tao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1606.02280", "submitter": "Huiling Wang", "authors": "Huiling Wang, Tapani Raiko, Lasse Lensu, Tinghuai Wang, Juha Karhunen", "title": "Semi-Supervised Domain Adaptation for Weakly Labeled Semantic Video\n  Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been immensely successful in\nmany high-level computer vision tasks given large labeled datasets. However,\nfor video semantic object segmentation, a domain where labels are scarce,\neffectively exploiting the representation power of CNN with limited training\ndata remains a challenge. Simply borrowing the existing pretrained CNN image\nrecognition model for video segmentation task can severely hurt performance. We\npropose a semi-supervised approach to adapting CNN image recognition model\ntrained from labeled image data to the target domain exploiting both semantic\nevidence learned from CNN, and the intrinsic structures of video data. By\nexplicitly modeling and compensating for the domain shift from the source\ndomain to the target domain, this proposed approach underpins a robust semantic\nobject segmentation method against the changes in appearance, shape and\nocclusion in natural videos. We present extensive experiments on challenging\ndatasets that demonstrate the superior performance of our approach compared\nwith the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:54:53 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Wang", "Huiling", ""], ["Raiko", "Tapani", ""], ["Lensu", "Lasse", ""], ["Wang", "Tinghuai", ""], ["Karhunen", "Juha", ""]]}, {"id": "1606.02288", "submitter": "Canlin Zhou", "authors": "Minmin Wang, Guangliang Du, Canlin Zhou, Chaorui Zhang, Shuchun Si,\n  Hui Li, Zhenkun Lei, YanJie Li", "title": "Enhanced high dynamic range 3D shape measurement based on generalized\n  phase-shifting algorithm", "comments": "18 pages, 22 figures", "journal-ref": null, "doi": "10.1016/j.optcom.2016.10.023", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenge for Phase Measurement Profilometry (PMP) to measure objects\nwith a large range of reflectivity variation across the surface. Saturated or\ndark pixels in the deformed fringe patterns captured by the camera will lead to\nphase fluctuations and errors. Jiang et al. proposed a high dynamic range\nreal-time 3D shape measurement method without changing camera exposures. Three\ninverted phase-shifted fringe patterns are used to complement three regular\nphase-shifted fringe patterns for phase retrieval when any of the regular\nfringe patterns are saturated. But Jiang's method still has some drawbacks: (1)\nThe phases in saturated pixels are respectively estimated by different formulas\nfor different cases. It is shortage of an universal formula; (2) it cannot be\nextended to four-step phase-shifting algorithm because inverted fringe patterns\nare the repetition of regular fringe patterns; (3) only three unsaturated\nintensity values at every pixel of fringe patterns are chosen for phase\ndemodulation, lying idle the other unsaturated ones. We proposed a method for\nenhanced high dynamic range 3D shape measurement based on generalized\nphase-shifting algorithm, which combines the complementary technique of\ninverted and regular fringe patterns with generalized phase-shifting algorithm.\nFirstly, two sets of complementary phase-shifted fringe patterns, namely\nregular and inverted fringe patterns are projected and collected. Then all\nunsaturated intensity values at the same camera pixel from two sets of fringe\npatterns are selected, and employed to retrieve the phase by generalized\nphase-shifting algorithm. Finally, simulations and experiments are conducted to\nprove the validity of the proposed method. The results are analyzed and\ncompared with Jiang's method, which demonstrate that the proposed method not\nonly expands the scope of Jiang's method, but also improves the measurement\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 07:25:56 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Wang", "Minmin", ""], ["Du", "Guangliang", ""], ["Zhou", "Canlin", ""], ["Zhang", "Chaorui", ""], ["Si", "Shuchun", ""], ["Li", "Hui", ""], ["Lei", "Zhenkun", ""], ["Li", "YanJie", ""]]}, {"id": "1606.02378", "submitter": "Arunkumar Byravan", "authors": "Arunkumar Byravan and Dieter Fox", "title": "SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks", "comments": "8 pages. To appear at the IEEE International Conference on Robotics\n  and Automation (ICRA), 2017. V2 Update: Final version submitted to ICRA with\n  experiments testing the robustness of the system to noise and preliminary\n  results on real world data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SE3-Nets, which are deep neural networks designed to model and\nlearn rigid body motion from raw point cloud data. Based only on sequences of\ndepth images along with action vectors and point wise data associations,\nSE3-Nets learn to segment effected object parts and predict their motion\nresulting from the applied force. Rather than learning point wise flow vectors,\nSE3-Nets predict SE3 transformations for different parts of the scene. Using\nsimulated depth data of a table top scene and a robot manipulator, we show that\nthe structure underlying SE3-Nets enables them to generate a far more\nconsistent prediction of object motion than traditional flow based networks.\nAdditional experiments with a depth camera observing a Baxter robot pushing\nobjects on a table show that SE3-Nets also work well on real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 02:36:11 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 17:32:22 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 22:41:40 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Byravan", "Arunkumar", ""], ["Fox", "Dieter", ""]]}, {"id": "1606.02382", "submitter": "Petteri Teikari", "authors": "Petteri Teikari, Marc Santos, Charissa Poon, Kullervo Hynynen", "title": "Deep Learning Convolutional Networks for Multiphoton Microscopy\n  Vasculature Segmentation", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been an increasing trend to use deep learning frameworks\nfor both 2D consumer images and for 3D medical images. However, there has been\nlittle effort to use deep frameworks for volumetric vascular segmentation. We\nwanted to address this by providing a freely available dataset of 12 annotated\ntwo-photon vasculature microscopy stacks. We demonstrated the use of deep\nlearning framework consisting both 2D and 3D convolutional filters (ConvNet).\nOur hybrid 2D-3D architecture produced promising segmentation result. We\nderived the architectures from Lee et al. who used the ZNN framework initially\ndesigned for electron microscope image segmentation. We hope that by sharing\nour volumetric vasculature datasets, we will inspire other researchers to\nexperiment with vasculature dataset and improve the used network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 02:57:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Teikari", "Petteri", ""], ["Santos", "Marc", ""], ["Poon", "Charissa", ""], ["Hynynen", "Kullervo", ""]]}, {"id": "1606.02393", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Zhe Lin, Scott Cohen, Xiaohui Shen, Bohyung Han", "title": "Progressive Attention Networks for Visual Attribute Prediction", "comments": "BMVC 2018 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel attention model that can accurately attends to target\nobjects of various scales and shapes in images. The model is trained to\ngradually suppress irrelevant regions in an input image via a progressive\nattentive process over multiple layers of a convolutional neural network. The\nattentive process in each layer determines whether to pass or block features at\ncertain spatial locations for use in the subsequent layers. The proposed\nprogressive attention mechanism works well especially when combined with hard\nattention. We further employ local contexts to incorporate neighborhood\nfeatures of each location and estimate a better attention probability map. The\nexperiments on synthetic and real datasets show that the proposed attention\nnetworks outperform traditional attention methods in visual attribute\nprediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 04:27:52 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 08:48:44 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 05:39:56 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 14:07:07 GMT"}, {"version": "v5", "created": "Mon, 6 Aug 2018 22:03:19 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Lin", "Zhe", ""], ["Cohen", "Scott", ""], ["Shen", "Xiaohui", ""], ["Han", "Bohyung", ""]]}, {"id": "1606.02407", "submitter": "Rathinakumar Appuswamy", "authors": "Rathinakumar Appuswamy, Tapan Nayak, John Arthur, Steven Esser, Paul\n  Merolla, Jeffrey Mckinstry, Timothy Melano, Myron Flickner, Dharmendra Modha", "title": "Structured Convolution Matrices for Energy-efficient Deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a relationship between network representation in energy-efficient\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\nby this connection, we develop deep convolutional networks using a family of\nstructured convolutional matrices and achieve state-of-the-art trade-off\nbetween energy efficiency and classification accuracy for well-known image\nrecognition tasks. We also put forward a novel method to train binary\nconvolutional networks by utilising an existing connection between\nnoisy-rectified linear units and binary activations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 05:31:43 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Appuswamy", "Rathinakumar", ""], ["Nayak", "Tapan", ""], ["Arthur", "John", ""], ["Esser", "Steven", ""], ["Merolla", "Paul", ""], ["Mckinstry", "Jeffrey", ""], ["Melano", "Timothy", ""], ["Flickner", "Myron", ""], ["Modha", "Dharmendra", ""]]}, {"id": "1606.02467", "submitter": "Margret Keuper", "authors": "Margret Keuper and Thomas Brox", "title": "Point-wise mutual information-based video segmentation with high\n  temporal consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of temporally consistent boundary\ndetection and hierarchical segmentation in videos. While finding the best\nhigh-level reasoning of region assignments in videos is the focus of much\nrecent research, temporal consistency in boundary detection has so far only\nrarely been tackled. We argue that temporally consistent boundaries are a key\ncomponent to temporally consistent region assignment. The proposed method is\nbased on the point-wise mutual information (PMI) of spatio-temporal voxels.\nTemporal consistency is established by an evaluation of PMI-based point\naffinities in the spectral domain over space and time. Thus, the proposed\nmethod is independent of any optical flow computation or previously learned\nmotion models. The proposed low-level video segmentation method outperforms the\nlearning-based state of the art in terms of standard region metrics.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 09:33:17 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Keuper", "Margret", ""], ["Brox", "Thomas", ""]]}, {"id": "1606.02492", "submitter": "Shreyas Saxena", "authors": "Shreyas Saxena and Jakob Verbeek", "title": "Convolutional Neural Fabrics", "comments": "Corrected typos (In proceedings of NIPS16 )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of CNNs, selecting the optimal architecture for a given\ntask remains an open problem. Instead of aiming to select a single optimal\narchitecture, we propose a \"fabric\" that embeds an exponentially large number\nof architectures. The fabric consists of a 3D trellis that connects response\nmaps at different layers, scales, and channels with a sparse homogeneous local\nconnectivity pattern. The only hyper-parameters of a fabric are the number of\nchannels and layers. While individual architectures can be recovered as paths,\nthe fabric can in addition ensemble all embedded architectures together,\nsharing their weights where their paths overlap. Parameters can be learned\nusing standard methods based on back-propagation, at a cost that scales\nlinearly in the fabric size. We present benchmark results competitive with the\nstate of the art for image classification on MNIST and CIFAR10, and for\nsemantic segmentation on the Part Labels dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 10:17:51 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 16:21:57 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 13:10:05 GMT"}, {"version": "v4", "created": "Mon, 30 Jan 2017 12:28:29 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Saxena", "Shreyas", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1606.02546", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler", "title": "Estimation of solar irradiance using ground-based whole sky imagers", "comments": "Accepted in Proc. IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS), July 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-based whole sky imagers (WSIs) can provide localized images of the sky\nof high temporal and spatial resolution, which permits fine-grained cloud\nobservation. In this paper, we show how images taken by WSIs can be used to\nestimate solar radiation. Sky cameras are useful here because they provide\nadditional information about cloud movement and coverage, which are otherwise\nnot available from weather station data. Our setup includes ground-based\nweather stations at the same location as the imagers. We use their measurements\nto validate our methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 13:21:30 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 09:38:37 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1606.02556", "submitter": "Diane Bouchacourt", "authors": "Diane Bouchacourt, M. Pawan Kumar, Sebastian Nowozin", "title": "DISCO Nets: DISsimilarity COefficient Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new type of probabilistic model which we call DISsimilarity\nCOefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample\nfrom a posterior distribution parametrised by a neural network. During\ntraining, DISCO Nets are learned by minimising the dissimilarity coefficient\nbetween the true distribution and the estimated distribution. This allows us to\ntailor the training to the loss related to the task at hand. We empirically\nshow that (i) by modeling uncertainty on the output value, DISCO Nets\noutperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets\naccurately model the uncertainty of the output, outperforming existing\nprobabilistic models based on deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 13:57:44 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 16:01:04 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 07:45:20 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 15:19:45 GMT"}, {"version": "v5", "created": "Fri, 28 Oct 2016 11:27:45 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Bouchacourt", "Diane", ""], ["Kumar", "M. Pawan", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1606.02580", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse,\n  David Pfau, Max Jaderberg, Marc Lanctot, Daan Wierstra", "title": "Convolution by Evolution: Differentiable Pattern Producing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a differentiable version of the Compositional\nPattern Producing Network, called the DPPN. Unlike a standard CPPN, the\ntopology of a DPPN is evolved but the weights are learned. A Lamarckian\nalgorithm, that combines evolution and learning, produces DPPNs to reconstruct\nan image. Our main result is that DPPNs can be evolved/trained to compress the\nweights of a denoising autoencoder from 157684 to roughly 200 parameters, while\nachieving a reconstruction accuracy comparable to a fully connected network\nwith more than two orders of magnitude more parameters. The regularization\nability of the DPPN allows it to rediscover (approximate) convolutional network\narchitectures embedded within a fully connected architecture. Such\nconvolutional architectures are the current state of the art for many computer\nvision applications, so it is satisfying that DPPNs are capable of discovering\nthis structure rather than having to build it in by design. DPPNs exhibit\nbetter generalization when tested on the Omniglot dataset after being trained\non MNIST, than directly encoded fully connected autoencoders. DPPNs are\ntherefore a new framework for integrating learning and evolution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:37:39 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Banarse", "Dylan", ""], ["Reynolds", "Malcolm", ""], ["Besse", "Frederic", ""], ["Pfau", "David", ""], ["Jaderberg", "Max", ""], ["Lanctot", "Marc", ""], ["Wierstra", "Daan", ""]]}, {"id": "1606.02585", "submitter": "Jamie Sherrah", "authors": "Jamie Sherrah", "title": "Fully Convolutional Networks for Dense Semantic Labelling of\n  High-Resolution Aerial Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend towards higher resolution remote sensing imagery facilitates a\ntransition from land-use classification to object-level scene understanding.\nRather than relying purely on spectral content, appearance-based image features\ncome into play. In this work, deep convolutional neural networks (CNNs) are\napplied to semantic labelling of high-resolution remote sensing data. Recent\nadvances in fully convolutional networks (FCNs) are adapted to overhead data\nand shown to be as effective as in other domains. A full-resolution labelling\nis inferred using a deep FCN with no downsampling, obviating the need for\ndeconvolution or interpolation. To make better use of image features, a\npre-trained CNN is fine-tuned on remote sensing data in a hybrid network\ncontext, resulting in superior results compared to a network trained from\nscratch. The proposed approach is applied to the problem of labelling\nhigh-resolution aerial imagery, where fine boundary detail is important. The\ndense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and\nPotsdam benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:52:04 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Sherrah", "Jamie", ""]]}, {"id": "1606.02608", "submitter": "David Martins de Matos", "authors": "Jaime Ferreira and David Martins de Matos and Ricardo Ribeiro", "title": "Fast and Extensible Online Multivariate Kernel Density Estimation", "comments": "17 pages, 1 figure, 7 tables, submission to Pattern Recognition\n  Letters, review version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present xokde++, a state-of-the-art online kernel density estimation\napproach that maintains Gaussian mixture models input data streams. The\napproach follows state-of-the-art work on online density estimation, but was\nredesigned with computational efficiency, numerical robustness, and\nextensibility in mind. Our approach produces comparable or better results than\nthe current state-of-the-art, while achieving significant computational\nperformance gains and improved numerical stability. The use of diagonal\ncovariance Gaussian kernels, which further improve performance and stability,\nat a small loss of modelling quality, is also explored. Our approach is up to\n40 times faster, while requiring 90\\% less memory than the closest\nstate-of-the-art counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 15:39:17 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Ferreira", "Jaime", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1606.02753", "submitter": "Michael McCann", "authors": "Michael T. McCann and Matthew Fickus and Jelena Kovacevic", "title": "Rotation Invariant Angular Descriptor Via A Bandlimited Gaussian-like\n  Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new smooth, Gaussian-like kernel that allows the kernel density\nestimate for an angular distribution to be exactly represented by a finite\nnumber of its Fourier series coefficients. Distributions of angular quantities,\nsuch as gradients, are a central part of several state-of-the-art image\nprocessing algorithms, but these distributions are usually described via\nhistograms and therefore lack rotation invariance due to binning artifacts.\nReplacing histograming with kernel density estimation removes these binning\nartifacts and can provide a finite-dimensional descriptor of the distribution,\nprovided that the kernel is selected to be bandlimited. In this paper, we\npresent a new band-limited kernel that has the added advantage of being\nGaussian-like in the angular domain. We then show that it compares favorably to\ngradient histograms for patch matching, person detection, and texture\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 20:51:23 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["McCann", "Michael T.", ""], ["Fickus", "Matthew", ""], ["Kovacevic", "Jelena", ""]]}, {"id": "1606.02792", "submitter": "John See", "authors": "Sze-Teng Liong, John See, Raphael Chung-Wei Phan, Yee-Hui Oh, Anh Cat\n  Le Ngo, KokSheik Wong, Su-Wei Tan", "title": "Spontaneous Subtle Expression Detection and Recognition based on Facial\n  Strain", "comments": "21 pages (including references), single column format, accepted to\n  Signal Processing: Image Communication journal", "journal-ref": "Signal Proc. Image Comm. 47 (2016) 170-182", "doi": "10.1016/j.image.2016.06.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical strain is an extension of optical flow that is capable of quantifying\nsubtle changes on faces and representing the minute facial motion intensities\nat the pixel level. This is computationally essential for the relatively new\nfield of spontaneous micro-expression, where subtle expressions can be\ntechnically challenging to pinpoint. In this paper, we present a novel method\nfor detecting and recognizing micro-expressions by utilizing facial optical\nstrain magnitudes to construct optical strain features and optical strain\nweighted features. The two sets of features are then concatenated to form the\nresultant feature histogram. Experiments were performed on the CASME II and\nSMIC databases. We demonstrate on both databases, the usefulness of optical\nstrain information and more importantly, that our best approaches are able to\noutperform the original baseline results for both detection and recognition\ntasks. A comparison of the proposed method with other existing spatio-temporal\nfeature extraction approaches is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 00:47:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Liong", "Sze-Teng", ""], ["See", "John", ""], ["Phan", "Raphael Chung-Wei", ""], ["Oh", "Yee-Hui", ""], ["Ngo", "Anh Cat Le", ""], ["Wong", "KokSheik", ""], ["Tan", "Su-Wei", ""]]}, {"id": "1606.02811", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Bihan Wen, Yee Hui Lee and Stefan Winkler", "title": "Machine Learning Techniques and Applications For Ground-based Image\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground-based whole sky cameras have opened up new opportunities for\nmonitoring the earth's atmosphere. These cameras are an important complement to\nsatellite images by providing geoscientists with cheaper, faster, and more\nlocalized data. The images captured by whole sky imagers can have high spatial\nand temporal resolution, which is an important pre-requisite for applications\nsuch as solar energy modeling, cloud attenuation analysis, local weather\nprediction, etc.\n  Extracting valuable information from the huge amount of image data by\ndetecting and analyzing the various entities in these images is challenging.\nHowever, powerful machine learning techniques have become available to aid with\nthe image analysis. This article provides a detailed walk-through of recent\ndevelopments in these techniques and their applications in ground-based\nimaging. We aim to bridge the gap between computer vision and remote sensing\nwith the help of illustrative examples. We demonstrate the advantages of using\nmachine learning techniques in ground-based image analysis via three primary\napplications -- segmentation, classification, and denoising.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 03:33:11 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Wen", "Bihan", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1606.02819", "submitter": "Bharath Hariharan", "authors": "Bharath Hariharan and Ross Girshick", "title": "Low-shot Visual Recognition by Shrinking and Hallucinating Features", "comments": "ICCV 2017 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-shot visual learning---the ability to recognize novel object categories\nfrom very few examples---is a hallmark of human visual intelligence. Existing\nmachine learning approaches fail to generalize in the same way. To make\nprogress on this foundational problem, we present a low-shot learning benchmark\non complex images that mimics challenges faced by recognition systems in the\nwild. We then propose a) representation regularization techniques, and b)\ntechniques to hallucinate additional training examples for data-starved\nclasses. Together, our methods improve the effectiveness of convolutional\nnetworks in low-shot learning, improving the one-shot accuracy on novel classes\nby 2.3x on the challenging ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 04:28:07 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 01:30:54 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 16:05:37 GMT"}, {"version": "v4", "created": "Sat, 4 Nov 2017 15:52:48 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hariharan", "Bharath", ""], ["Girshick", "Ross", ""]]}, {"id": "1606.02861", "submitter": "Carsten Gottschlich", "authors": "Duy Hoang Thai and Carsten Gottschlich", "title": "Simultaneous Inpainting and Denoising by Directional Global Three-part\n  Decomposition: Connecting Variational and Fourier Domain Based Image\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the very challenging task of restoring images (i) which have a\nlarge number of missing pixels, (ii) whose existing pixels are corrupted by\nnoise and (iii) the ideal image to be restored contains both cartoon and\ntexture elements. The combination of these three properties makes this inverse\nproblem a very difficult one. The solution proposed in this manuscript is based\non directional global three-part decomposition (DG3PD) [ThaiGottschlich2016]\nwith directional total variation norm, directional G-norm and\n$\\ell_\\infty$-norm in curvelet domain as key ingredients of the model. Image\ndecomposition by DG3PD enables a decoupled inpainting and denoising of the\ncartoon and texture components. A comparison to existing approaches for\ninpainting and denoising shows the advantages of the proposed method. Moreover,\nwe regard the image restoration problem from the viewpoint of a Bayesian\nframework and we discuss the connections between the proposed solution by\nfunction space and related image representation by harmonic analysis and\npyramid decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 08:21:38 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Thai", "Duy Hoang", ""], ["Gottschlich", "Carsten", ""]]}, {"id": "1606.02894", "submitter": "Haz{\\i}m Kemal Ekenel", "authors": "Mostafa Mehdipour Ghazi and Hazim Kemal Ekenel", "title": "A Comprehensive Analysis of Deep Learning Based Representation for Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based approaches have been dominating the face recognition\nfield due to the significant performance improvement they have provided on the\nchallenging wild datasets. These approaches have been extensively tested on\nsuch unconstrained datasets, on the Labeled Faces in the Wild and YouTube\nFaces, to name a few. However, their capability to handle individual appearance\nvariations caused by factors such as head pose, illumination, occlusion, and\nmisalignment has not been thoroughly assessed till now. In this paper, we\npresent a comprehensive study to evaluate the performance of deep learning\nbased face representation under several conditions including the varying head\npose angles, upper and lower face occlusion, changing illumination of different\nstrengths, and misalignment due to erroneous facial feature localization. Two\nsuccessful and publicly available deep learning models, namely VGG-Face and\nLightened CNN have been utilized to extract face representations. The obtained\nresults show that although deep learning provides a powerful representation for\nface recognition, it can still benefit from preprocessing, for example, for\npose and illumination normalization in order to achieve better performance\nunder various conditions. Particularly, if these variations are not included in\nthe dataset used to train the deep learning model, the role of preprocessing\nbecomes more crucial. Experimental results also show that deep learning based\nrepresentation is robust to misalignment and can tolerate facial feature\nlocalization errors up to 10% of the interocular distance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 10:25:24 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Ekenel", "Hazim Kemal", ""]]}, {"id": "1606.02909", "submitter": "Haz{\\i}m Kemal Ekenel", "authors": "Refik Can Malli and Mehmet Aygun and Hazim Kemal Ekenel", "title": "Apparent Age Estimation Using Ensemble of Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of apparent age estimation. Different\nfrom estimating the real age of individuals, in which each face image has a\nsingle age label, in this problem, face images have multiple age labels,\ncorresponding to the ages perceived by the annotators, when they look at these\nimages. This provides an intriguing computer vision problem, since in generic\nimage or object classification tasks, it is typical to have a single ground\ntruth label per class. To account for multiple labels per image, instead of\nusing average age of the annotated face image as the class label, we have\ngrouped the face images that are within a specified age range. Using these age\ngroups and their age-shifted groupings, we have trained an ensemble of deep\nlearning models. Before feeding an input face image to a deep learning model,\nfive facial landmark points are detected and used for 2-D alignment. We have\nemployed and fine tuned convolutional neural networks (CNNs) that are based on\nVGG-16 [24] architecture and pretrained on the IMDB-WIKI dataset [22]. The\noutputs of these deep learning models are then combined to produce the final\nestimation. Proposed method achieves 0.3668 error in the final ChaLearn LAP\n2016 challenge test set [5].\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 11:00:21 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Malli", "Refik Can", ""], ["Aygun", "Mehmet", ""], ["Ekenel", "Hazim Kemal", ""]]}, {"id": "1606.03014", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, James K. Min, Guanglei Xiong", "title": "Implicit Tubular Surface Generation Guided by Centerline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning-based coronary artery segmentation methods represent\nthe vascular lumen surface in an implicit way by the centerline and the\nassociated lumen radii, which makes the subsequent modeling process to generate\na whole piece of watertight coronary artery tree model difficult. To solve this\nproblem, in this paper, we propose a modeling method with the learning-based\nsegmentation results by (1) considering mesh vertices as physical particles and\nusing interaction force model and particle expansion model to generate\nuniformly distributed point cloud on the implicit lumen surface and; (2) doing\nincremental Delaunay-based triangulation. Our method has the advantage of being\nable to consider the complex shape of the coronary artery tree as a whole\npiece; hence no extra stitching or intersection removal algorithm is needed to\ngenerate a watertight model. Experiment results demonstrate that our method is\ncapable of generating high quality mesh model which is highly consistent with\nthe given implicit vascular lumen surface, with an average error of 0.08 mm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 16:14:22 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Zhou", "Haoyin", ""], ["Min", "James K.", ""], ["Xiong", "Guanglei", ""]]}, {"id": "1606.03021", "submitter": "Minh-Duc Hua", "authors": "Minh-Duc Hua, Jochen Trumpf, Tarek Hamel, Robert Mahony, and Pascal\n  Morin", "title": "Feature-based Recursive Observer Design for Homography Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for online estimation of a sequence of\nhomographies applicable to image sequences obtained from robotic vehicles\nequipped with vision sensors. The approach taken exploits the underlying\nSpecial Linear group structure of the set of homographies along with gyroscope\nmeasurements and direct point-feature correspondences between images to develop\ntemporal filter for the homography estimate. Theoretical analysis and\nexperimental results are provided to demonstrate the robustness of the proposed\nalgorithm. The experimental results show excellent performance even in the case\nof very fast camera motion (relative to frame rate), severe occlusion, and in\nthe presence of specular reflections.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 16:35:46 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Hua", "Minh-Duc", ""], ["Trumpf", "Jochen", ""], ["Hamel", "Tarek", ""], ["Mahony", "Robert", ""], ["Morin", "Pascal", ""]]}, {"id": "1606.03073", "submitter": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk", "authors": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk, Umut G\\\"u\\c{c}l\\\"u, Rob van Lier,\n  Marcel A. J. van Gerven", "title": "Convolutional Sketch Inversion", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-46604-0_56", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use deep neural networks for inverting face sketches to\nsynthesize photorealistic face images. We first construct a semi-simulated\ndataset containing a very large number of computer-generated face sketches with\ndifferent styles and corresponding face images by expanding existing\nunconstrained face data sets. We then train models achieving state-of-the-art\nresults on both computer-generated sketches and hand-drawn sketches by\nleveraging recent advances in deep learning such as batch normalization, deep\nresidual learning, perceptual losses and stochastic optimization in combination\nwith our new dataset. We finally demonstrate potential applications of our\nmodels in fine arts and forensic arts. In contrast to existing patch-based\napproaches, our deep-neural-network-based approach can be used for synthesizing\nphotorealistic face images by inverting face sketches in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 19:27:41 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["G\u00fc\u00e7l\u00fct\u00fcrk", "Ya\u011fmur", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["van Lier", "Rob", ""], ["van Gerven", "Marcel A. J.", ""]]}, {"id": "1606.03141", "submitter": "Mehdi Sajjadi", "authors": "Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen", "title": "Mutual Exclusivity Loss for Semi-Supervised Deep Learning", "comments": "5 pages, 1 figures, ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of semi-supervised learning with deep\nConvolutional Neural Networks (ConvNets). Semi-supervised learning is motivated\non the observation that unlabeled data is cheap and can be used to improve the\naccuracy of classifiers. In this paper we propose an unsupervised\nregularization term that explicitly forces the classifier's prediction for\nmultiple classes to be mutually-exclusive and effectively guides the decision\nboundary to lie on the low density space between the manifolds corresponding to\ndifferent classes of data. Our proposed approach is general and can be used\nwith any backpropagation-based learning method. We show through different\nexperiments that our method can improve the object recognition performance of\nConvNets using unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 23:15:16 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Sajjadi", "Mehdi", ""], ["Javanmardi", "Mehran", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1606.03237", "submitter": "Ciprian Corneanu", "authors": "Ciprian Corneanu, Marc Oliu, Jeffrey F. Cohn, Sergio Escalera", "title": "Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial\n  Expression Recognition: History, Trends, and Affect-related Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are an important way through which humans interact\nsocially. Building a system capable of automatically recognizing facial\nexpressions from images and video has been an intense field of study in recent\nyears. Interpreting such expressions remains challenging and much research is\nneeded about the way they relate to human affect. This paper presents a general\noverview of automatic RGB, 3D, thermal and multimodal facial expression\nanalysis. We define a new taxonomy for the field, encompassing all steps from\nface detection to facial expression recognition, and describe and classify the\nstate of the art methods accordingly. We also present the important datasets\nand the bench-marking of most influential methods. We conclude with a general\ndiscussion about trends, important questions and future lines of research.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:12:05 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Corneanu", "Ciprian", ""], ["Oliu", "Marc", ""], ["Cohn", "Jeffrey F.", ""], ["Escalera", "Sergio", ""]]}, {"id": "1606.03238", "submitter": "Matteo Gadaleta", "authors": "Matteo Gadaleta and Michele Rossi", "title": "IDNet: Smartphone-based Gait Recognition with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.09.005", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present IDNet, a user authentication framework from\nsmartphone-acquired motion signals. Its goal is to recognize a target user from\ntheir way of walking, using the accelerometer and gyroscope (inertial) signals\nprovided by a commercial smartphone worn in the front pocket of the user's\ntrousers. IDNet features several innovations including: i) a robust and\nsmartphone-orientation-independent walking cycle extraction block, ii) a novel\nfeature extractor based on convolutional neural networks, iii) a one-class\nsupport vector machine to classify walking cycles, and the coherent integration\nof these into iv) a multi-stage authentication technique. IDNet is the first\nsystem that exploits a deep learning approach as universal feature extractors\nfor gait recognition, and that combines classification results from subsequent\nwalking cycles into a multi-stage decision making framework. Experimental\nresults show the superiority of our approach against state-of-the-art\ntechniques, leading to misclassification rates (either false negatives or\npositives) smaller than 0.15% with fewer than five walking cycles. Design\nchoices are discussed and motivated throughout, assessing their impact on the\nuser authentication performance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:14:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 13:15:53 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 12:11:57 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Gadaleta", "Matteo", ""], ["Rossi", "Michele", ""]]}, {"id": "1606.03369", "submitter": "Thiago Vallin Spina PhD", "authors": "Thiago Vallin Spina and Alexandre Xavier Falc\\~ao", "title": "FOMTrace: Interactive Video Segmentation By Image Graphs and Fuzzy\n  Object Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common users have changed from mere consumers to active producers of\nmultimedia data content. Video editing plays an important role in this\nscenario, calling for simple segmentation tools that can handle fast-moving and\ndeformable video objects with possible occlusions, color similarities with the\nbackground, among other challenges. We present an interactive video\nsegmentation method, named FOMTrace, which addresses the problem in an\neffective and efficient way. From a user-provided object mask in a first frame,\nthe method performs semi-automatic video segmentation on a spatiotemporal\nsuperpixel-graph, and then estimates a Fuzzy Object Model (FOM), which refines\nsegmentation of the second frame by constraining delineation on a pixel-graph\nwithin a region where the object's boundary is expected to be. The user can\ncorrect/accept the refined object mask in the second frame, which is then\nsimilarly used to improve the spatiotemporal video segmentation of the\nremaining frames. Both steps are repeated alternately, within interactive\nresponse times, until the segmentation refinement of the final frame is\naccepted by the user. Extensive experiments demonstrate FOMTrace's ability for\ntracing objects in comparison with state-of-the-art approaches for interactive\nvideo segmentation, supervised, and unsupervised object tracking.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 15:30:30 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Spina", "Thiago Vallin", ""], ["Falc\u00e3o", "Alexandre Xavier", ""]]}, {"id": "1606.03473", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang and Erik Learned-Miller", "title": "Face Detection with the Faster R-CNN", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Faster R-CNN has recently demonstrated impressive results on various\nobject detection benchmarks. By training a Faster R-CNN model on the large\nscale WIDER face dataset, we report state-of-the-art results on two widely used\nface detection benchmarks, FDDB and the recently released IJB-A.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 20:34:39 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Jiang", "Huaizu", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1606.03490", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton", "title": "The Mythos of Model Interpretability", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning models boast remarkable predictive capabilities.\nBut can you trust your model? Will it work in deployment? What else can it tell\nyou about the world? We want models to be not only good, but interpretable. And\nyet the task of interpretation appears underspecified. Papers provide diverse\nand sometimes non-overlapping motivations for interpretability, and offer\nmyriad notions of what attributes render models interpretable. Despite this\nambiguity, many papers proclaim interpretability axiomatically, absent further\nexplanation. In this paper, we seek to refine the discourse on\ninterpretability. First, we examine the motivations underlying interest in\ninterpretability, finding them to be diverse and occasionally discordant. Then,\nwe address model properties and techniques thought to confer interpretability,\nidentifying transparency to humans and post-hoc explanations as competing\nnotions. Throughout, we discuss the feasibility and desirability of different\nnotions, and question the oft-made assertions that linear models are\ninterpretable and that deep neural networks are not.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 21:28:47 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 21:21:04 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 08:51:10 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Lipton", "Zachary C.", ""]]}, {"id": "1606.03498", "submitter": "Ian Goodfellow", "authors": "Tim Salimans and Ian Goodfellow and Wojciech Zaremba and Vicki Cheung\n  and Alec Radford and Xi Chen", "title": "Improved Techniques for Training GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 22:53:35 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Salimans", "Tim", ""], ["Goodfellow", "Ian", ""], ["Zaremba", "Wojciech", ""], ["Cheung", "Vicki", ""], ["Radford", "Alec", ""], ["Chen", "Xi", ""]]}, {"id": "1606.03556", "submitter": "Abhishek Das", "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv\n  Batra", "title": "Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions?", "comments": "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 05:41:10 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 04:39:01 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Das", "Abhishek", ""], ["Agrawal", "Harsh", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1606.03558", "submitter": "Christopher Bongsoo Choy", "authors": "Christopher B. Choy, JunYoung Gwak, Silvio Savarese, Manmohan\n  Chandraker", "title": "Universal Correspondence Network", "comments": "To appear at NIPS 2016 as full oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for accurate visual correspondences and\ndemonstrate its effectiveness for both geometric and semantic matching,\nspanning across rigid motions to intra-class shape or appearance variations. In\ncontrast to previous CNN-based approaches that optimize a surrogate patch\nsimilarity objective, we use deep metric learning to directly learn a feature\nspace that preserves either geometric or semantic similarity. Our fully\nconvolutional architecture, along with a novel correspondence contrastive loss\nallows faster training by effective reuse of computations, accurate gradient\ncomputation through the use of thousands of examples per image pair and faster\ntesting with $O(n)$ feed forward passes for $n$ keypoints, instead of $O(n^2)$\nfor typical patch similarity methods. We propose a convolutional spatial\ntransformer to mimic patch normalization in traditional features like SIFT,\nwhich is shown to dramatically boost accuracy for semantic correspondences\nacross intra-class shape variations. Extensive experiments on KITTI, PASCAL,\nand CUB-2011 datasets demonstrate the significant advantages of our features\nover prior works that use either hand-constructed or learned features.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 06:27:09 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 23:16:13 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 06:32:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Choy", "Christopher B.", ""], ["Gwak", "JunYoung", ""], ["Savarese", "Silvio", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1606.03578", "submitter": "Naima Kaabouch", "authors": "Naima Kaabouch, Wen-Chen Hu, and Yi Chen", "title": "Alternative Technique to Asymmetry Analysis-Based Overlapping for Foot\n  Ulcer Examination: Scalable Scanning", "comments": "Journal of Diabetes & Metabolism, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetry analysis based on the overlapping of thermal images proved able to\ndetect inflammation and, predict foot ulceration. This technique involves three\nmain steps: segmentation, geometric transformation, and overlapping. However,\nthe overlapping technique, which consists of subtracting the intensity levels\nof the right foot from those of the left foot, can also detect false abnormal\nareas if the projections of the left and right feet are not the same. In this\npaper, we present an alternative technique to asymmetry analysis-based\noverlapping. The proposed technique, scalable scanning, allows for an effective\ncomparison even if the shapes and sizes of the feet projections appear\ndifferently in the image. The tested results show that asymmetry analysis-\nbased scalable scanning provides fewer false abnormal areas than does asymmetry\nanalysis -based overlapping.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 11:07:55 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Kaabouch", "Naima", ""], ["Hu", "Wen-Chen", ""], ["Chen", "Yi", ""]]}, {"id": "1606.03601", "submitter": "Mohamed Aly", "authors": "Mohamed Aly, Guangming Zang, Wolfgang Heidrich, Peter Wonka", "title": "TRex: A Tomography Reconstruction Proximal Framework for Robust Sparse\n  View X-Ray Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TRex, a flexible and robust Tomographic Reconstruction framework\nusing proximal algorithms. We provide an overview and perform an experimental\ncomparison between the famous iterative reconstruction methods in terms of\nreconstruction quality in sparse view situations. We then derive the proximal\noperators for the four best methods. We show the flexibility of our framework\nby deriving solvers for two noise models: Gaussian and Poisson; and by plugging\nin three powerful regularizers. We compare our framework to state of the art\nmethods, and show superior quality on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 14:19:28 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Aly", "Mohamed", ""], ["Zang", "Guangming", ""], ["Heidrich", "Wolfgang", ""], ["Wonka", "Peter", ""]]}, {"id": "1606.03647", "submitter": "Hyeonwoo Noh", "authors": "Hyeonwoo Noh and Bohyung Han", "title": "Training Recurrent Answering Units with Joint Loss Minimization for VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for visual question answering based on a\nrecurrent deep neural network, where every module in the network corresponds to\na complete answering unit with attention mechanism by itself. The network is\noptimized by minimizing loss aggregated from all the units, which share model\nparameters while receiving different information to compute attention\nprobability. For training, our model attends to a region within image feature\nmap, updates its memory based on the question and attended image feature, and\nanswers the question based on its memory state. This procedure is performed to\ncompute loss in each step. The motivation of this approach is our observation\nthat multi-step inferences are often required to answer questions while each\nproblem may have a unique desirable number of steps, which is difficult to\nidentify in practice. Hence, we always make the first unit in the network solve\nproblems, but allow it to learn the knowledge from the rest of units by\nbackpropagation unless it degrades the model. To implement this idea, we\nearly-stop training each unit as soon as it starts to overfit. Note that, since\nmore complex models tend to overfit on easier questions quickly, the last\nanswering unit in the unfolded recurrent neural network is typically killed\nfirst while the first one remains last. We make a single-step prediction for a\nnew question using the shared model. This strategy works better than the other\noptions within our framework since the selected model is trained effectively\nfrom all units without overfitting. The proposed algorithm outperforms other\nmulti-step attention based approaches using a single step prediction in VQA\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 00:47:46 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 03:31:53 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["Han", "Bohyung", ""]]}, {"id": "1606.03669", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Yee Hui Lee and Stefan Winkler", "title": "Color-based Segmentation of Sky/Cloud Images From Ground-based Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sky/cloud images captured by ground-based cameras (a.k.a. whole sky imagers)\nare increasingly used nowadays because of their applications in a number of\nfields, including climate modeling, weather prediction, renewable energy\ngeneration, and satellite communications. Due to the wide variety of cloud\ntypes and lighting conditions in such images, accurate and robust segmentation\nof clouds is challenging. In this paper, we present a supervised segmentation\nframework for ground-based sky/cloud images based on a systematic analysis of\ndifferent color spaces and components, using partial least squares (PLS)\nregression. Unlike other state-of-the-art methods, our proposed approach is\nentirely learning-based and does not require any manually-defined parameters.\nIn addition, we release the Singapore Whole Sky IMaging SEGmentation Database\n(SWIMSEG), a large database of annotated sky/cloud images, to the research\ncommunity.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 06:17:10 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1606.03671", "submitter": "Alexandre De Siqueira", "authors": "Alexandre Fioravante de Siqueira and Fl\\'avio Camargo Cabrera and\n  Aylton Pagamisse and Aldo Eloizo Job", "title": "Segmentation of scanning electron microscopy images from natural rubber\n  samples with gold nanoparticles using starlet wavelets", "comments": null, "journal-ref": null, "doi": "10.1002/jemt.22314", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic microscopy has been used for morphology evaluation of different\nmaterials structures. However, microscopy results may be affected by several\nfactors. Image processing methods can be used to correct and improve the\nquality of these results. In this paper we propose an algorithm based on\nstarlets to perform the segmentation of scanning electron microscopy images. An\napplication is presented in order to locate gold nanoparticles in natural\nrubber membranes. In this application, our method showed accuracy greater than\n85% for all test images. Results given by this method will be used in future\nstudies, to computationally estimate the density distribution of gold\nnanoparticles in natural rubber samples and to predict reduction kinetics of\ngold nanoparticles at different time periods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 06:33:20 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Cabrera", "Fl\u00e1vio Camargo", ""], ["Pagamisse", "Aylton", ""], ["Job", "Aldo Eloizo", ""]]}, {"id": "1606.03731", "submitter": "Junchi Yan", "authors": "Junchi Yan, Zhe Ren, Hongyuan Zha, Stephen Chu", "title": "A constrained clustering based approach for matching a collection of\n  feature sets", "comments": "submission to ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of finding the feature correspondences\namong a collection of feature sets, by using their point-wise unary features.\nThis is a fundamental problem in computer vision and pattern recognition, which\nalso closely relates to other areas such as operational research. Different\nfrom two-set matching which can be transformed to a quadratic assignment\nprogramming task that is known NP-hard, inclusion of merely unary attributes\nleads to a linear assignment problem for matching two feature sets. This\nproblem has been well studied and there are effective polynomial global optimum\nsolvers such as the Hungarian method. However, it becomes ill-posed when the\nunary attributes are (heavily) corrupted. The global optimal correspondence\nconcerning the best score defined by the attribute affinity/cost between the\ntwo sets can be distinct to the ground truth correspondence since the score\nfunction is biased by noises. To combat this issue, we devise a method for\nmatching a collection of feature sets by synergetically exploring the\ninformation across the sets. In general, our method can be perceived from a\n(constrained) clustering perspective: in each iteration, it assigns the\nfeatures of one set to the clusters formed by the rest of feature sets, and\nupdates the cluster centers in turn. Results on both synthetic data and real\nimages suggest the efficacy of our method against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 15:40:30 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Yan", "Junchi", ""], ["Ren", "Zhe", ""], ["Zha", "Hongyuan", ""], ["Chu", "Stephen", ""]]}, {"id": "1606.03765", "submitter": "Assaf Hoogi", "authors": "Assaf Hoogi, Christopher F. Beaulieu, Guilherme M. Cunha, Elhamy Heba,\n  Claude B. Sirlin, Sandy Napel, and Daniel L. Rubin", "title": "Adaptive Local Window for Level Set Segmentation of CT and MRI Liver\n  Lesions", "comments": "24 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method, the adaptive local window, for improving level set\nsegmentation technique. The window is estimated separately for each contour\npoint, over iterations of the segmentation process, and for each individual\nobject. Our method considers the object scale, the spatial texture, and changes\nof the energy functional over iterations. Global and local statistics are\nconsidered by calculating several gray level co-occurrence matrices. We\ndemonstrate the capabilities of the method in the domain of medical imaging for\nsegmenting 233 images with liver lesions. To illustrate the strength of our\nmethod, those images were obtained by either Computed Tomography or Magnetic\nResonance Imaging. Moreover, we analyzed images using three different energy\nmodels. We compare our method to a global level set segmentation and to local\nframework that uses predefined fixed-size square windows. The results indicate\nthat our proposed method outperforms the other methods in terms of agreement\nwith the manual marking and dependence on contour initialization or the energy\nmodel used. In case of complex lesions, such as low contrast lesions,\nheterogeneous lesions, or lesions with a noisy background, our method shows\nsignificantly better segmentation with an improvement of 0.25+- 0.13 in Dice\nsimilarity coefficient, compared with state of the art fixed-size local windows\n(Wilcoxon, p < 0.001).\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 21:31:38 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Hoogi", "Assaf", ""], ["Beaulieu", "Christopher F.", ""], ["Cunha", "Guilherme M.", ""], ["Heba", "Elhamy", ""], ["Sirlin", "Claude B.", ""], ["Napel", "Sandy", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1606.03774", "submitter": "Chenxia Wu", "authors": "Chenxia Wu, Jiemi Zhang, Ashutosh Saxena, Silvio Savarese", "title": "Human Centred Object Co-Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-segmentation is the automatic extraction of the common semantic regions\ngiven a set of images. Different from previous approaches mainly based on\nobject visuals, in this paper, we propose a human centred object\nco-segmentation approach, which uses the human as another strong evidence. In\norder to discover the rich internal structure of the objects reflecting their\nhuman-object interactions and visual similarities, we propose an unsupervised\nfully connected CRF auto-encoder incorporating the rich object features and a\nnovel human-object interaction representation. We propose an efficient learning\nand inference algorithm to allow the full connectivity of the CRF with the\nauto-encoder, that establishes pairwise relations on all pairs of the object\nproposals in the dataset. Moreover, the auto-encoder learns the parameters from\nthe data itself rather than supervised learning or manually assigned parameters\nin the conventional CRF. In the extensive experiments on four datasets, we show\nthat our approach is able to extract the common objects more accurately than\nthe state-of-the-art co-segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 22:36:53 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Wu", "Chenxia", ""], ["Zhang", "Jiemi", ""], ["Saxena", "Ashutosh", ""], ["Savarese", "Silvio", ""]]}, {"id": "1606.03788", "submitter": "Michael Jacobs", "authors": "Vishwa S. Parekh, Jeremy R. Jacobs, Michael A. Jacobs", "title": "Unsupervised Non Linear Dimensionality Reduction Machine Learning\n  methods applied to Multiparametric MRI in cerebral ischemia: Preliminary\n  Results", "comments": "9 pages", "journal-ref": "Proceedings of the SPIE, Volume 9034, id. 90342O 9 pp. (2014)", "doi": "10.1117/12.2044001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation and treatment of acute cerebral ischemia requires a technique\nthat can determine the total area of tissue at risk for infarction using\ndiagnostic magnetic resonance imaging (MRI) sequences. Typical MRI data sets\nconsist of T1- and T2-weighted imaging (T1WI, T2WI) along with advanced MRI\nparameters of diffusion-weighted imaging (DWI) and perfusion weighted imaging\n(PWI) methods. Each of these parameters has distinct radiological-pathological\nmeaning. For example, DWI interrogates the movement of water in the tissue and\nPWI gives an estimate of the blood flow, both are critical measures during the\nevolution of stroke. In order to integrate these data and give an estimate of\nthe tissue at risk or damaged, we have developed advanced machine learning\nmethods based on unsupervised non-linear dimensionality reduction (NLDR)\ntechniques. NLDR methods are a class of algorithms that uses mathematically\ndefined manifolds for statistical sampling of multidimensional classes to\ngenerate a discrimination rule of guaranteed statistical accuracy and they can\ngenerate a two- or three-dimensional map, which represents the prominent\nstructures of the data and provides an embedded image of meaningful\nlow-dimensional structures hidden in their high-dimensional observations. In\nthis manuscript, we develop NLDR methods on high dimensional MRI data sets of\npreclinical animals and clinical patients with stroke. On analyzing the\nperformance of these methods, we observed that there was a high of similarity\nbetween multiparametric embedded images from NLDR methods and the ADC map and\nperfusion map. It was also observed that embedded scattergram of abnormal\n(infarcted or at risk) tissue can be visualized and provides a mechanism for\nautomatic methods to delineate potential stroke volumes and early tissue at\nrisk.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 01:29:04 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Parekh", "Vishwa S.", ""], ["Jacobs", "Jeremy R.", ""], ["Jacobs", "Michael A.", ""]]}, {"id": "1606.03798", "submitter": "Tomasz Malisiewicz", "authors": "Daniel DeTone and Tomasz Malisiewicz and Andrew Rabinovich", "title": "Deep Image Homography Estimation", "comments": "RSS Workshop on Limits and Potentials of Deep Learning in Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep convolutional neural network for estimating the relative\nhomography between a pair of images. Our feed-forward network has 10 layers,\ntakes two stacked grayscale images as input, and produces an 8 degree of\nfreedom homography which can be used to map the pixels from the first image to\nthe second. We present two convolutional neural network architectures for\nHomographyNet: a regression network which directly estimates the real-valued\nhomography parameters, and a classification network which produces a\ndistribution over quantized homographies. We use a 4-point homography\nparameterization which maps the four corners from one image into the second\nimage. Our networks are trained in an end-to-end fashion using warped MS-COCO\nimages. Our approach works without the need for separate local feature\ndetection and transformation estimation stages. Our deep models are compared to\na traditional homography estimator based on ORB features and we highlight the\nscenarios where HomographyNet outperforms the traditional technique. We also\ndescribe a variety of applications powered by deep homography estimation, thus\nshowcasing the flexibility of a deep learning approach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 02:46:38 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["DeTone", "Daniel", ""], ["Malisiewicz", "Tomasz", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1606.03838", "submitter": "Boyue Wang", "authors": "Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin", "title": "Laplacian LRR on Product Grassmann Manifolds for Human Activity\n  Clustering in Multi-Camera Video Surveillance", "comments": "14pages,submitting to IEEE TCSVT with minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-camera video surveillance, it is challenging to represent videos\nfrom different cameras properly and fuse them efficiently for specific\napplications such as human activity recognition and clustering. In this paper,\na novel representation for multi-camera video data, namely the Product\nGrassmann Manifold (PGM), is proposed to model video sequences as points on the\nGrassmann manifold and integrate them as a whole in the product manifold form.\nAdditionally, with a new geometry metric on the product manifold, the\nconventional Low Rank Representation (LRR) model is extended onto PGM and the\nnew LRR model can be used for clustering non-linear data, such as multi-camera\nvideo data. To evaluate the proposed method, a number of clustering experiments\nare conducted on several multi-camera video datasets of human activity,\nincluding Dongzhimen Transport Hub Crowd action dataset, ACT 42 Human action\ndataset and SKIG action dataset. The experiment results show that the proposed\nmethod outperforms many state-of-the-art clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 07:09:39 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Yin", "Baocai", ""]]}, {"id": "1606.03871", "submitter": "Wenhan Yang", "authors": "Jiaying Liu, Wenhan Yang, Xiaoyan Sun, Wenjun Zeng", "title": "Photo Stylistic Brush: Robust Style Transfer via Superpixel-Based\n  Bipartite Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social network and multimedia technology,\ncustomized image and video stylization has been widely used for various\nsocial-media applications. In this paper, we explore the problem of\nexemplar-based photo style transfer, which provides a flexible and convenient\nway to invoke fantastic visual impression. Rather than investigating some fixed\nartistic patterns to represent certain styles as was done in some previous\nworks, our work emphasizes styles related to a series of visual effects in the\nphotograph, e.g. color, tone, and contrast. We propose a photo stylistic brush,\nan automatic robust style transfer approach based on Superpixel-based BIpartite\nGraph (SuperBIG). A two-step bipartite graph algorithm with different\ngranularity levels is employed to aggregate pixels into superpixels and find\ntheir correspondences. In the first step, with the extracted hierarchical\nfeatures, a bipartite graph is constructed to describe the content similarity\nfor pixel partition to produce superpixels. In the second step, superpixels in\nthe input/reference image are rematched to form a new superpixel-based\nbipartite graph, and superpixel-level correspondences are generated by a\nbipartite matching. Finally, the refined correspondence guides SuperBIG to\nperform the transformation in a decorrelated color space. Extensive\nexperimental results demonstrate the effectiveness and robustness of the\nproposed method for transferring various styles of exemplar images, even for\nsome challenging cases, such as night images.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 09:28:19 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 09:05:02 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Liu", "Jiaying", ""], ["Yang", "Wenhan", ""], ["Sun", "Xiaoyan", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1606.03968", "submitter": "Stefano Soatto", "authors": "Jingming Dong, Xiaohan Fei, Stefano Soatto", "title": "Visual-Inertial-Semantic Scene Representation for 3-D Object Detection", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": "CSD160005", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system to detect objects in three-dimensional space using video\nand inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile\nplatforms from phones to drones. Inertials afford the ability to impose\nclass-specific scale priors for objects, and provide a global orientation\nreference. A minimal sufficient representation, the posterior of semantic\n(identity) and syntactic (pose) attributes of objects in space, can be\ndecomposed into a geometric term, which can be maintained by a\nlocalization-and-mapping filter, and a likelihood function, which can be\napproximated by a discriminatively-trained convolutional neural network. The\nresulting system can process the video stream causally in real time, and\nprovides a representation of objects in the scene that is persistent:\nConfidence in the presence of objects grows with evidence, and objects\npreviously seen are kept in memory even when temporarily occluded, with their\nreturn into view automatically predicted to prime re-detection.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:22:10 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 20:25:26 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Dong", "Jingming", ""], ["Fei", "Xiaohan", ""], ["Soatto", "Stefano", ""]]}, {"id": "1606.04189", "submitter": "Andrey Zhmoginov", "authors": "Andrey Zhmoginov and Mark Sandler", "title": "Inverting face embeddings with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have dramatically advanced the state of the art for many\nareas of machine learning. Recently they have been shown to have a remarkable\nability to generate highly complex visual artifacts such as images and text\nrather than simply recognize them.\n  In this work we use neural networks to effectively invert low-dimensional\nface embeddings while producing realistically looking consistent images. Our\ncontribution is twofold, first we show that a gradient ascent style approaches\ncan be used to reproduce consistent images, with a help of a guiding image.\nSecond, we demonstrate that we can train a separate neural network to\neffectively solve the minimization problem in one pass, and generate images in\nreal-time. We then evaluate the loss imposed by using a neural network instead\nof the gradient descent by comparing the final values of the minimized loss\nfunction.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 01:35:12 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 18:52:57 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Zhmoginov", "Andrey", ""], ["Sandler", "Mark", ""]]}, {"id": "1606.04232", "submitter": "Maya Kabkab", "authors": "Maya Kabkab, Azadeh Alavi, Rama Chellappa", "title": "DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale supervised classification algorithms, especially those based on\ndeep convolutional neural networks (DCNNs), require vast amounts of training\ndata to achieve state-of-the-art performance. Decreasing this data requirement\nwould significantly speed up the training process and possibly improve\ngeneralization. Motivated by this objective, we consider the task of adaptively\nfinding concise training subsets which will be iteratively presented to the\nlearner. We use convex optimization methods, based on an objective criterion\nand feedback from the current performance of the classifier, to efficiently\nidentify informative samples to train on. We propose an algorithm to decompose\nthe optimization problem into smaller per-class problems, which can be solved\nin parallel. We test our approach on standard classification tasks and\ndemonstrate its effectiveness in decreasing the training set size without\ncompromising performance. We also show that our approach can make the\nclassifier more robust in the presence of label noise and class imbalance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:38:13 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Kabkab", "Maya", ""], ["Alavi", "Azadeh", ""], ["Chellappa", "Rama", ""]]}, {"id": "1606.04308", "submitter": "J\\\"urgen Leitner", "authors": "Donald G. Dansereau, Anders Eriksson and J\\\"urgen Leitner", "title": "Richardson-Lucy Deblurring for Moving Light Field Cameras", "comments": "Paper accepted for oral presentation at LF4CV workshop at CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Richardson-Lucy (RL) deblurring to 4-D light fields by\nreplacing the convolution steps with light field rendering of motion blur. The\nmethod deals correctly with blur caused by 6-degree-of-freedom camera motion in\ncomplex 3-D scenes, without performing depth estimation. We introduce a novel\nregularization term that maintains parallax information in the light field\nwhile reducing noise and ringing. We demonstrate the method operating\neffectively on rendered scenes and scenes captured using an off-the-shelf light\nfield camera. An industrial robot arm provides repeatable and known\ntrajectories, allowing us to establish quantitative performance in complex 3-D\nscenes. Qualitative and quantitative results confirm the effectiveness of the\nmethod, including commonly occurring cases for which previously published\nmethods fail. We include mathematical proof that the algorithm converges to the\nmaximum-likelihood estimate of the unblurred scene under Poisson noise. We\nexpect extension to blind methods to be possible following the generalization\nof 2-D Richardson-Lucy to blind deconvolution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 10:57:34 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 08:20:54 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Dansereau", "Donald G.", ""], ["Eriksson", "Anders", ""], ["Leitner", "J\u00fcrgen", ""]]}, {"id": "1606.04333", "submitter": "Sven Sickert", "authors": "Clemens-Alexander Brust and Sven Sickert and Marcel Simon and Erik\n  Rodner and Joachim Denzler", "title": "Neither Quick Nor Proper -- Evaluation of QuickProp for Learning Deep\n  Neural Networks", "comments": "Technical Report, 11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks and especially convolutional neural networks are of great\ninterest in current computer vision research. However, many techniques,\nextensions, and modifications have been published in the past, which are not\nyet used by current approaches. In this paper, we study the application of a\nmethod called QuickProp for training of deep neural networks. In particular, we\napply QuickProp during learning and testing of fully convolutional networks for\nthe task of semantic segmentation. We compare QuickProp empirically with\ngradient descent, which is the current standard method. Experiments suggest\nthat QuickProp can not compete with standard gradient descent techniques for\ncomplex computer vision tasks like semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 12:57:56 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 11:00:47 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["Sickert", "Sven", ""], ["Simon", "Marcel", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1606.04393", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Akshaya Mishra, and Alexander Wong", "title": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural\n  Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking inspiration from biological evolution, we explore the idea of \"Can\ndeep neural networks evolve naturally over successive generations into highly\nefficient deep neural networks?\" by introducing the notion of synthesizing new\nhighly efficient, yet powerful deep neural networks over successive generations\nvia an evolutionary process from ancestor deep neural networks. The\narchitectural traits of ancestor deep neural networks are encoded using\nsynaptic probability models, which can be viewed as the `DNA' of these\nnetworks. New descendant networks with differing network architectures are\nsynthesized based on these synaptic probability models from the ancestor\nnetworks and computational environmental factor models, in a random manner to\nmimic heredity, natural selection, and random mutation. These offspring\nnetworks are then trained into fully functional networks, like one would train\na newborn, and have more efficient, more diverse network architectures than\ntheir ancestor networks, while achieving powerful modeling capabilities.\nExperimental results for the task of visual saliency demonstrated that the\nsynthesized `evolved' offspring networks can achieve state-of-the-art\nperformance while having network architectures that are significantly more\nefficient (with a staggering $\\sim$48-fold decrease in synapses by the fourth\ngeneration) compared to the original ancestor network.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 14:36:55 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 16:15:40 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 22:51:33 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Mishra", "Akshaya", ""], ["Wong", "Alexander", ""]]}, {"id": "1606.04404", "submitter": "Hao Liu", "authors": "Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang and Shuicheng Yan", "title": "End-to-End Comparative Attention Networks for Person Re-identification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2700762", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification across disjoint camera views has been widely applied\nin video surveillance yet it is still a challenging problem. One of the major\nchallenges lies in the lack of spatial and temporal cues, which makes it\ndifficult to deal with large variations of lighting conditions, viewing angles,\nbody poses and occlusions. Recently, several deep learning based person\nre-identification approaches have been proposed and achieved remarkable\nperformance. However, most of those approaches extract discriminative features\nfrom the whole frame at one glimpse without differentiating various parts of\nthe persons to identify. It is essentially important to examine multiple highly\ndiscriminative local regions of the person images in details through multiple\nglimpses for dealing with the large appearance variance. In this paper, we\npropose a new soft attention based model, i.e., the end to-end Comparative\nAttention Network (CAN), specifically tailored for the task of person\nre-identification. The end-to-end CAN learns to selectively focus on parts of\npairs of person images after taking a few glimpses of them and adaptively\ncomparing their appearance. The CAN model is able to learn which parts of\nimages are relevant for discerning persons and automatically integrates\ninformation from different parts to determine whether a pair of images belongs\nto the same person. In other words, our proposed CAN model simulates the human\nperception process to verify whether two images are from the same person.\nExtensive experiments on three benchmark person re-identification datasets,\nincluding CUHK01, CHUHK03 and Market-1501, clearly demonstrate that our\nproposed end-to-end CAN for person re-identification outperforms well\nestablished baselines significantly and offer new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 14:51:59 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 16:02:15 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Liu", "Hao", ""], ["Feng", "Jiashi", ""], ["Qi", "Meibin", ""], ["Jiang", "Jianguo", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1606.04446", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris and Nikos Komodakis", "title": "Attend Refine Repeat: Active Box Proposal Generation via In-Out\n  Localization", "comments": "Technical report. Code as well as box proposals computed for several\n  datasets are available at:: https://github.com/gidariss/AttractioNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing category agnostic bounding box proposals is utilized\nas a core component in many computer vision tasks and thus has lately attracted\na lot of attention. In this work we propose a new approach to tackle this\nproblem that is based on an active strategy for generating box proposals that\nstarts from a set of seed boxes, which are uniformly distributed on the image,\nand then progressively moves its attention on the promising image areas where\nit is more likely to discover well localized bounding box proposals. We call\nour approach AttractioNet and a core component of it is a CNN-based category\nagnostic object location refinement module that is capable of yielding accurate\nand robust bounding box predictions regardless of the object category.\n  We extensively evaluate our AttractioNet approach on several image datasets\n(i.e. COCO, PASCAL, ImageNet detection and NYU-Depth V2 datasets) reporting on\nall of them state-of-the-art results that surpass the previous work in the\nfield by a significant margin and also providing strong empirical evidence that\nour approach is capable to generalize to unseen categories. Furthermore, we\nevaluate our AttractioNet proposals in the context of the object detection task\nusing a VGG16-Net based detector and the achieved detection performance on COCO\nmanages to significantly surpass all other VGG16-Net based detectors while even\nbeing competitive with a heavily tuned ResNet-101 based detector. Code as well\nas box proposals computed for several datasets are available at::\nhttps://github.com/gidariss/AttractioNet.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:35:08 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1606.04450", "submitter": "Massimo Camplani", "authors": "Massimo Camplani, Adeline Paiement, Majid Mirmehdi, Dima Damen, Sion\n  Hannuna, Tilo Burghardt, Lili Tao", "title": "Multiple Human Tracking in RGB-D Data: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple human tracking (MHT) is a fundamental task in many computer vision\napplications. Appearance-based approaches, primarily formulated on RGB data,\nare constrained and affected by problems arising from occlusions and/or\nillumination variations. In recent years, the arrival of cheap RGB-Depth\n(RGB-D) devices has {led} to many new approaches to MHT, and many of these\nintegrate color and depth cues to improve each and every stage of the process.\nIn this survey, we present the common processing pipeline of these methods and\nreview their methodology based (a) on how they implement this pipeline and (b)\non what role depth plays within each stage of it. We identify and introduce\nexisting, publicly available, benchmark datasets and software resources that\nfuse color and depth data for MHT. Finally, we present a brief comparative\nevaluation of the performance of those works that have applied their methods to\nthese datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:41:55 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Camplani", "Massimo", ""], ["Paiement", "Adeline", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""], ["Hannuna", "Sion", ""], ["Burghardt", "Tilo", ""], ["Tao", "Lili", ""]]}, {"id": "1606.04506", "submitter": "Yamuna Prasad", "authors": "Yamuna Prasad, Dinesh Khandelwal, K. K. Biswas", "title": "Max-Margin Feature Selection", "comments": "submitted to PR Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications such as in vision, biology and social\nnetworking deal with data in high dimensions. Feature selection is typically\nemployed to select a subset of features which im- proves generalization\naccuracy as well as reduces the computational cost of learning the model. One\nof the criteria used for feature selection is to jointly minimize the\nredundancy and maximize the rele- vance of the selected features. In this\npaper, we formulate the task of feature selection as a one class SVM problem in\na space where features correspond to the data points and instances correspond\nto the dimensions. The goal is to look for a representative subset of the\nfeatures (support vectors) which describes the boundary for the region where\nthe set of the features (data points) exists. This leads to a joint\noptimization of relevance and redundancy in a principled max-margin framework.\nAdditionally, our formulation enables us to leverage existing techniques for\noptimizing the SVM objective resulting in highly computationally efficient\nsolutions for the task of feature selection. Specifically, we employ the dual\ncoordinate descent algorithm (Hsieh et al., 2008), originally proposed for\nSVMs, for our formulation. We use a sparse representation to deal with data in\nvery high dimensions. Experiments on seven publicly available benchmark\ndatasets from a variety of domains show that our approach results in orders of\nmagnitude faster solutions even while retaining the same level of accuracy\ncompared to the state of the art feature selection techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 19:05:01 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Prasad", "Yamuna", ""], ["Khandelwal", "Dinesh", ""], ["Biswas", "K. K.", ""]]}, {"id": "1606.04535", "submitter": "Anna Pastuszczak", "authors": "Anna Pastuszczak, Bart{\\l}omiej Szczygie{\\l}, Micha{\\l}\n  Miko{\\l}ajczyk, and Rafa{\\l} Koty\\'nski", "title": "Efficient adaptation of complex-valued noiselet sensing matrices for\n  compressed single-pixel imaging", "comments": "9 pages, 3 figures, 1 table; accepted for publication in Applied\n  Optics (2016)", "journal-ref": "Appl. Opt. 55(19), 5141-5148 (2016)", "doi": "10.1364/AO.55.005141", "report-no": null, "categories": "cs.CV cs.IT math.IT physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal mutual coherence of discrete noiselets and Haar wavelets makes this\npair of bases an essential choice for the measurement and compression matrices\nin compressed-sensing-based single-pixel detectors. In this paper we propose an\nefficient way of using complex-valued and non-binary noiselet functions for\nobject sampling in single-pixel cameras with binary spatial light modulators\nand incoherent illumination. The proposed method allows to determine m complex\nnoiselet coefficients from m+1 binary sampling measurements. Further, we\nintroduce a modification to the complex fast noiselet transform, which enables\ncomputationally-efficient real-time generation of the binary noiselet-based\npatterns using efficient integer calculations on bundled patterns. The proposed\nmethod is verified experimentally with a single-pixel camera system using a\nbinary spatial light modulator.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:34:37 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Pastuszczak", "Anna", ""], ["Szczygie\u0142", "Bart\u0142omiej", ""], ["Miko\u0142ajczyk", "Micha\u0142", ""], ["Koty\u0144ski", "Rafa\u0142", ""]]}, {"id": "1606.04586", "submitter": "Mehdi Sajjadi", "authors": "Mehdi Sajjadi, Mehran Javanmardi, Tolga Tasdizen", "title": "Regularization With Stochastic Transformations and Perturbations for\n  Deep Semi-Supervised Learning", "comments": "9 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective convolutional neural networks are trained on large sets of labeled\ndata. However, creating large labeled datasets is a very costly and\ntime-consuming task. Semi-supervised learning uses unlabeled data to train a\nmodel with higher accuracy when there is a limited set of labeled data\navailable. In this paper, we consider the problem of semi-supervised learning\nwith convolutional neural networks. Techniques such as randomized data\naugmentation, dropout and random max-pooling provide better generalization and\nstability for classifiers that are trained using gradient descent. Multiple\npasses of an individual sample through the network might lead to different\npredictions due to the non-deterministic behavior of these techniques. We\npropose an unsupervised loss function that takes advantage of the stochastic\nnature of these methods and minimizes the difference between the predictions of\nmultiple passes of a training sample through the network. We evaluate the\nproposed method on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 22:30:08 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Sajjadi", "Mehdi", ""], ["Javanmardi", "Mehran", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1606.04590", "submitter": "Matvey Soloviev", "authors": "Yuka Kihara, Matvey Soloviev, Tsuhan Chen", "title": "In the Shadows, Shape Priors Shine: Using Occlusion to Improve\n  Multi-Region Segmentation", "comments": "Camera ready version accepted at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for multi-region segmentation of 2D images with\nobjects that may partially occlude each other. Our algorithm is based on the\nobservation hat human performance on this task is based both on prior knowledge\nabout plausible shapes and taking into account the presence of occluding\nobjects whose shape is already known - once an occluded region is identified,\nthe shape prior can be used to guess the shape of the missing part. We capture\nthe former aspect using a deep learning model of shape; for the latter, we\nsimultaneously minimize the energy of all regions and consider only unoccluded\npixels for data agreement.\n  Existing algorithms incorporating object shape priors consider every object\nseparately in turn and can't distinguish genuine deviation from the expected\nshape from parts missing due to occlusion. We show that our method\nsignificantly improves on the performance of a representative algorithm, as\nevaluated on both preprocessed natural and synthetic images. Furthermore, on\nthe synthetic images, we recover the ground truth segmentation with good\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 23:12:40 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Kihara", "Yuka", ""], ["Soloviev", "Matvey", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1606.04616", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Yong Xu, Cheng-Lin Liu", "title": "Natural Scene Character Recognition Using Robust PCA and Sparse\n  Representation", "comments": "The 12th IAPR International Workshop on Document Analysis Systems\n  (DAS); The natural scene character image features used in this paper have\n  been released at\n  http://www.yongxu.org/Natural%20Scene%20Character%20Recognition%20Datasets.html", "journal-ref": null, "doi": "10.1109/DAS.2016.32", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural scene character recognition is challenging due to the cluttered\nbackground, which is hard to separate from text. In this paper, we propose a\nnovel method for robust scene character recognition. Specifically, we first use\nrobust principal component analysis (PCA) to denoise character image by\nrecovering the missing low-rank component and filtering out the sparse noise\nterm, and then use a simple Histogram of oriented Gradient (HOG) to perform\nimage feature extraction, and finally, use a sparse representation based\nclassifier for recognition. In experiments on four public datasets, namely the\nChar74K dataset, ICADAR 2003 robust reading dataset, Street View Text (SVT)\ndataset and IIIT5K-word dataset, our method was demonstrated to be competitive\nwith the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 01:58:06 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1606.04621", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Chenliang Xu, Parker Koch, Jason J. Corso", "title": "Watch What You Just Said: Image Captioning with Text-Conditional\n  Attention", "comments": "source code is available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have attracted considerable interest in image captioning\ndue to its powerful performance. However, existing methods use only visual\ncontent as attention and whether textual context can improve attention in image\ncaptioning remains unsolved. To explore this problem, we propose a novel\nattention mechanism, called \\textit{text-conditional attention}, which allows\nthe caption generator to focus on certain image features given previously\ngenerated text. To obtain text-related image features for our attention model,\nwe adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture\nwith CNN fine-tuning. Our proposed method allows joint learning of the image\nembedding, text embedding, text-conditional attention and language model with\none network architecture in an end-to-end manner. We perform extensive\nexperiments on the MS-COCO dataset. The experimental results show that our\nmethod outperforms state-of-the-art captioning methods on various quantitative\nmetrics as well as in human evaluation, which supports the use of our\ntext-conditional attention in image captioning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 02:26:22 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 21:17:42 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 04:36:42 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Zhou", "Luowei", ""], ["Xu", "Chenliang", ""], ["Koch", "Parker", ""], ["Corso", "Jason J.", ""]]}, {"id": "1606.04637", "submitter": "Ryo Yonetani", "authors": "Ryo Yonetani and Kris M. Kitani and Yoichi Sato", "title": "Ego-Surfing: Person Localization in First-Person Videos Using Ego-Motion\n  Signatures", "comments": "To appear in IEEE TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2771767", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envision a future time when wearable cameras are worn by the masses and\nrecording first-person point-of-view videos of everyday life. While these\ncameras can enable new assistive technologies and novel research challenges,\nthey also raise serious privacy concerns. For example, first-person videos\npassively recorded by wearable cameras will necessarily include anyone who\ncomes into the view of a camera -- with or without consent. Motivated by these\nbenefits and risks, we developed a self-search technique tailored to\nfirst-person videos. The key observation of our work is that the egocentric\nhead motion of a target person (ie, the self) is observed both in the\npoint-of-view video of the target and observer. The motion correlation between\nthe target person's video and the observer's video can then be used to identify\ninstances of the self uniquely. We incorporate this feature into the proposed\napproach that computes the motion correlation over densely-sampled trajectories\nto search for a target individual in observer videos. Our approach\nsignificantly improves self-search performance over several well-known face\ndetectors and recognizers. Furthermore, we show how our approach can enable\nseveral practical applications such as privacy filtering, target video\nretrieval, and social group clustering.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 04:31:49 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 01:16:46 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Yonetani", "Ryo", ""], ["Kitani", "Kris M.", ""], ["Sato", "Yoichi", ""]]}, {"id": "1606.04702", "submitter": "Amir Ghodrati", "authors": "Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van\n  Gool", "title": "DeepProposals: Hunting Objects and Actions by Cascading Deep\n  Convolutional Layers", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method for generating object and action proposals in\nimages and videos is proposed. It builds on activations of different\nconvolutional layers of a pretrained CNN, combining the localization accuracy\nof the early layers with the high informative-ness (and hence recall) of the\nlater layers. To this end, we build an inverse cascade that, going backward\nfrom the later to the earlier convolutional layers of the CNN, selects the most\npromising locations and refines them in a coarse-to-fine manner. The method is\nefficient, because i) it re-uses the same features extracted for detection, ii)\nit aggregates features using integral images, and iii) it avoids a dense\nevaluation of the proposals thanks to the use of the inverse coarse-to-fine\ncascade. The method is also accurate. We show that our DeepProposals outperform\nmost of the previously proposed object proposal and action proposal approaches\nand, when plugged into a CNN-based object detector, produce state-of-the-art\ndetection performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 09:57:27 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Ghodrati", "Amir", ""], ["Diba", "Ali", ""], ["Pedersoli", "Marco", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1606.04766", "submitter": "Jianyu Lin", "authors": "Jianyu Lin, Neil T. Clancy, Xueqing Sun, Ji Qi, Mirek Janatka, Danail\n  Stoyanov, Daniel S. Elson", "title": "Probe-based Rapid Hybrid Hyperspectral and Tissue Surface Imaging Aided\n  by Fully Convolutional Networks", "comments": "This paper has been submitted to MICCAI2016 on 17 March, 2016, and\n  conditionally accepted on 2 June, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue surface shape and reflectance spectra provide rich intra-operative\ninformation useful in surgical guidance. We propose a hybrid system which\ndisplays an endoscopic image with a fast joint inspection of tissue surface\nshape using structured light (SL) and hyperspectral imaging (HSI). For SL a\nminiature fibre probe is used to project a coloured spot pattern onto the\ntissue surface. In HSI mode standard endoscopic illumination is used, with the\nfibre probe collecting reflected light and encoding the spatial information\ninto a linear format that can be imaged onto the slit of a spectrograph.\nCorrespondence between the arrangement of fibres at the distal and proximal\nends of the bundle was found using spectral encoding. Then during pattern\ndecoding, a fully convolutional network (FCN) was used for spot detection,\nfollowed by a matching propagation algorithm for spot identification. This\nmethod enabled fast reconstruction (12 frames per second) using a GPU. The\nhyperspectral image was combined with the white light image and the\nreconstructed surface, showing the spectral information of different areas.\nValidation of this system using phantom and ex vivo experiments has been\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:00:07 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Lin", "Jianyu", ""], ["Clancy", "Neil T.", ""], ["Sun", "Xueqing", ""], ["Qi", "Ji", ""], ["Janatka", "Mirek", ""], ["Stoyanov", "Danail", ""], ["Elson", "Daniel S.", ""]]}, {"id": "1606.04774", "submitter": "Olivier  Ruatta", "authors": "Ouiddad Labbani I. and Pauline Merveilleux O and Olivier Ruatta", "title": "Free Form based active contours for image segmentation and free space\n  perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present a novel approach for representing and evolving\ndeformable active contours. The method combines piecewise regular B{\\'e}zier\nmodels and curve evolution defined by local Free Form Deformation. The contour\ndeformation is locally constrained which allows contour convergence with almost\nlinear complexity while adapting to various shape settings and handling\ntopology changes of the active contour.\n  We demonstrate the effectiveness of the new active contour scheme for visual\nfree space perception and segmentation using omnidirectional images acquired by\na robot exploring unknown indoor and outdoor environments. Several experiments\nvalidate the approach with comparison to state-of-the art parametric and\ngeometric active contours and provide fast and real-time robot free space\nsegmentation and navigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:11:21 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["I.", "Ouiddad Labbani", ""], ["O", "Pauline Merveilleux", ""], ["Ruatta", "Olivier", ""]]}, {"id": "1606.04797", "submitter": "Fausto Milletari", "authors": "Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been recently employed to solve\nproblems from both the computer vision and medical image analysis fields.\nDespite their popularity, most approaches are only able to process 2D images\nwhile most medical data used in clinical practice consists of 3D volumes. In\nthis work we propose an approach to 3D image segmentation based on a\nvolumetric, fully convolutional, neural network. Our CNN is trained end-to-end\non MRI volumes depicting prostate, and learns to predict segmentation for the\nwhole volume at once. We introduce a novel objective function, that we optimise\nduring training, based on Dice coefficient. In this way we can deal with\nsituations where there is a strong imbalance between the number of foreground\nand background voxels. To cope with the limited number of annotated volumes\navailable for training, we augment the data applying random non-linear\ntransformations and histogram matching. We show in our experimental evaluation\nthat our approach achieves good performances on challenging test data while\nrequiring only a fraction of the processing time needed by other previous\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:55:09 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Milletari", "Fausto", ""], ["Navab", "Nassir", ""], ["Ahmadi", "Seyed-Ahmad", ""]]}, {"id": "1606.04801", "submitter": "Kun He Prof.", "authors": "Kun He and Yan Wang and John Hopcroft", "title": "A Powerful Generative Model Using Random Weights for the Deep Image\n  Representation", "comments": "10 pages, 10 figures, submited to NIPS 2016 conference. Computer\n  Vision and Pattern Recognition, Neurons and Cognition, Neural and\n  Evolutionary Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To what extent is the success of deep visualization due to the training?\nCould we do deep visualization using untrained, random weight networks? To\naddress this issue, we explore new and powerful generative models for three\npopular deep visualization tasks using untrained, random weight convolutional\nneural networks. First we invert representations in feature spaces and\nreconstruct images from white noise inputs. The reconstruction quality is\nstatistically higher than that of the same method applied on well trained\nnetworks with the same architecture. Next we synthesize textures using scaled\ncorrelations of representations in multiple layers and our results are almost\nindistinguishable with the original natural texture and the synthesized\ntextures based on the trained network. Third, by recasting the content of an\nimage in the style of various artworks, we create artistic images with high\nperceptual quality, highly competitive to the prior work of Gatys et al. on\npretrained networks. To our knowledge this is the first demonstration of image\nrepresentations using untrained deep neural networks. Our work provides a new\nand fascinating tool to study the representation of deep network architecture\nand sheds light on new understandings on deep visualization.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 14:56:42 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 06:53:55 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["He", "Kun", ""], ["Wang", "Yan", ""], ["Hopcroft", "John", ""]]}, {"id": "1606.04853", "submitter": "Patrick Flynn", "authors": "Kevin W. Bowyer and Patrick J. Flynn", "title": "The ND-IRIS-0405 Iris Image Dataset", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Computer Vision Research Lab at the University of Notre Dame began\ncollecting iris images in the spring semester of 2004. The initial data\ncollections used an LG 2200 iris imaging system for image acquisition. Image\ndatasets acquired in 2004-2005 at Notre Dame with this LG 2200 have been used\nin the ICE 2005 and ICE 2006 iris biometric evaluations. The ICE 2005 iris\nimage dataset has been distributed to over 100 research groups around the\nworld. The purpose of this document is to describe the content of the\nND-IRIS-0405 iris image dataset. This dataset is a superset of the iris image\ndatasets used in ICE 2005 and ICE 2006. The ND 2004-2005 iris image dataset\ncontains 64,980 images corresponding to 356 unique subjects, and 712 unique\nirises. The age range of the subjects is 18 to 75 years old. 158 of the\nsubjects are female, and 198 are male. 250 of the subjects are Caucasian, 82\nare Asian, and 24 are other ethnicities.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 16:40:51 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""]]}, {"id": "1606.04884", "submitter": "Hugh Perkins", "authors": "Hugh Perkins (ASAPP)", "title": "cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network\n  Library, Based on OpenCL", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents cltorch, a hardware-agnostic backend for the Torch neural\nnetwork framework. cltorch enables training of deep neural networks on GPUs\nfrom diverse hardware vendors, including AMD, NVIDIA, and Intel. cltorch\ncontains sufficient implementation to run models such as AlexNet, VGG,\nOverfeat, and GoogleNet. It is written using the OpenCL language, a portable\ncompute language, governed by the Khronos Group. cltorch is the top-ranked\nhardware-agnostic machine learning framework on Chintala's convnet-benchmarks\npage.\n  This paper presents the technical challenges encountered whilst creating the\ncltorch backend for Torch, and looks in detail at the challenges related to\nobtaining a fast hardware-agnostic implementation.\n  The convolutional layers are identified as the key area of focus for\naccelerating hardware-agnostic frameworks. Possible approaches to accelerating\nthe convolutional implementation are identified including: implementation of\nthe convolutions using the implicitgemm or winograd algorithm, using a GEMM\nimplementation adapted to the geometries associated with the convolutional\nalgorithm, or using a pluggable hardware-specific convolutional implementation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 17:59:31 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Perkins", "Hugh", "", "ASAPP"]]}, {"id": "1606.04940", "submitter": "Pierre Paleo", "authors": "Pierre Paleo, Michel Desvignes, Alessandro Mirone", "title": "A practical local tomography reconstruction algorithm based on known\n  subregion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to reconstruct data acquired in a local tomography\nsetup. This method uses an initial reconstruction and refines it by correcting\nthe low frequency artifacts known as the cupping effect. A basis of Gaussian\nfunctions is used to correct the initial reconstruction. The coefficients of\nthis basis are iteratively optimized under the constraint of a known subregion.\nUsing a coarse basis reduces the degrees of freedom of the problem while\nactually correcting the cupping effect. Simulations show that the known region\nconstraint yields an unbiased reconstruction, in accordance to uniqueness\ntheorems stated in local tomography.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 09:41:11 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Paleo", "Pierre", ""], ["Desvignes", "Michel", ""], ["Mirone", "Alessandro", ""]]}, {"id": "1606.04985", "submitter": "Yanwei Cui", "authors": "Yanwei Cui, Laetitia Chapel, S\\'ebastien Lef\\`evre", "title": "Combining multiscale features for classification of hyperspectral\n  images: a sequence based kernel approach", "comments": "8th IEEE GRSS Workshop on Hyperspectral Image and Signal Processing:\n  Evolution in Remote Sensing (WHISPERS 2016), UCLA in Los Angeles, California,\n  U.S", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, hyperspectral image classification widely copes with spatial\ninformation to improve accuracy. One of the most popular way to integrate such\ninformation is to extract hierarchical features from a multiscale segmentation.\nIn the classification context, the extracted features are commonly concatenated\ninto a long vector (also called stacked vector), on which is applied a\nconventional vector-based machine learning technique (e.g. SVM with Gaussian\nkernel). In this paper, we rather propose to use a sequence structured kernel:\nthe spectrum kernel. We show that the conventional stacked vector-based kernel\nis actually a special case of this kernel. Experiments conducted on various\npublicly available hyperspectral datasets illustrate the improvement of the\nproposed kernel w.r.t. conventional ones using the same hierarchical spatial\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:19:54 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Cui", "Yanwei", ""], ["Chapel", "Laetitia", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1606.04992", "submitter": "Ivan Lillo", "authors": "Ivan Lillo, Juan Carlos Niebles, Alvaro Soto", "title": "A Hierarchical Pose-Based Approach to Complex Action Understanding Using\n  Dictionaries of Actionlets and Motion Poselets", "comments": "10 pages, IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new hierarchical model for human action\nrecognition using body joint locations. Our model can categorize complex\nactions in videos, and perform spatio-temporal annotations of the atomic\nactions that compose the complex action being performed.That is, for each\natomic action, the model generates temporal action annotations by estimating\nits starting and ending times, as well as, spatial annotations by inferring the\nhuman body parts that are involved in executing the action. our model includes\nthree key novel properties: (i) it can be trained with no spatial supervision,\nas it can automatically discover active body parts from temporal action\nannotations only; (ii) it jointly learns flexible representations for motion\nposelets and actionlets that encode the visual variability of body parts and\natomic actions; (iii) a mechanism to discard idle or non-informative body parts\nwhich increases its robustness to common pose estimation errors. We evaluate\nthe performance of our method using multiple action recognition benchmarks. Our\nmodel consistently outperforms baselines and state-of-the-art action\nrecognition methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 21:51:25 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Lillo", "Ivan", ""], ["Niebles", "Juan Carlos", ""], ["Soto", "Alvaro", ""]]}, {"id": "1606.05002", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Daeyun Shin, Naren Sivagnanadasan, Derek Hoiem", "title": "3DFS: Deformable Dense Depth Fusion and Segmentation for Object\n  Reconstruction from a Handheld Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for 3D reconstruction and segmentation of a single\nobject placed on a flat surface from an input video. Our approach is to perform\ndense depth map estimation for multiple views using a proposed objective\nfunction that preserves detail. The resulting depth maps are then fused using a\nproposed implicit surface function that is robust to estimation error,\nproducing a smooth surface reconstruction of the entire scene. Finally, the\nobject is segmented from the remaining scene using a proposed 2D-3D\nsegmentation that incorporates image and depth cues with priors and\nregularization over the 3D volume and 2D segmentations. We evaluate 3D\nreconstructions qualitatively on our Object-Videos dataset, comparing to\nfusion, multiview stereo, and segmentation baselines. We also quantitatively\nevaluate the dense depth estimation using the RGBD Scenes V2 dataset [Henry et\nal. 2013] and the segmentation using keyframe annotations of the Object-Videos\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 23:23:08 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 20:38:19 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Gupta", "Tanmay", ""], ["Shin", "Daeyun", ""], ["Sivagnanadasan", "Naren", ""], ["Hoiem", "Derek", ""]]}, {"id": "1606.05032", "submitter": "Yang Yang", "authors": "Yang Yang, Weilun Chen, Yadan Luo, Fumin Shen, Jie Shao and Heng Tao\n  Shen", "title": "Zero-Shot Hashing via Transferring Supervised Knowledge", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has shown its efficiency and effectiveness in facilitating\nlarge-scale multimedia applications. Supervised knowledge e.g. semantic labels\nor pair-wise relationship) associated to data is capable of significantly\nimproving the quality of hash codes and hash functions. However, confronted\nwith the rapid growth of newly-emerging concepts and multimedia data on the\nWeb, existing supervised hashing approaches may easily suffer from the scarcity\nand validity of supervised information due to the expensive cost of manual\nlabelling. In this paper, we propose a novel hashing scheme, termed\n\\emph{zero-shot hashing} (ZSH), which compresses images of \"unseen\" categories\nto binary codes with hash functions learned from limited training data of\n\"seen\" categories. Specifically, we project independent data labels i.e.\n0/1-form label vectors) into semantic embedding space, where semantic\nrelationships among all the labels can be precisely characterized and thus seen\nsupervised knowledge can be transferred to unseen classes. Moreover, in order\nto cope with the semantic shift problem, we rotate the embedded space to more\nsuitably align the embedded semantics with the low-level visual feature space,\nthereby alleviating the influence of semantic gap. In the meantime, to exert\npositive effects on learning high-quality hash functions, we further propose to\npreserve local structural property and discrete nature in binary codes.\nBesides, we develop an efficient alternating algorithm to solve the ZSH model.\nExtensive experiments conducted on various real-life datasets show the superior\nzero-shot image retrieval performance of ZSH as compared to several\nstate-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 02:56:39 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Yang", "Yang", ""], ["Chen", "Weilun", ""], ["Luo", "Yadan", ""], ["Shen", "Fumin", ""], ["Shao", "Jie", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1606.05158", "submitter": "Joseph  Salmon", "authors": "C-A. Deledalle and N. Papadakis and J. Salmon and S. Vaiter", "title": "CLEAR: Covariant LEAst-square Re-fitting with applications to image\n  restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework to remove parts of the systematic\nerrors affecting popular restoration algorithms, with a special focus for image\nprocessing tasks. Generalizing ideas that emerged for $\\ell_1$ regularization,\nwe develop an approach re-fitting the results of standard methods towards the\ninput data. Total variation regularizations and non-local means are special\ncases of interest. We identify important covariant information that should be\npreserved by the re-fitting method, and emphasize the importance of preserving\nthe Jacobian (w.r.t. the observed signal) of the original estimator. Then, we\nprovide an approach that has a \"twicing\" flavor and allows re-fitting the\nrestored signal by adding back a local affine transformation of the residual\nterm. We illustrate the benefits of our method on numerical simulations for\nimage restoration tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 12:23:55 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 20:45:02 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Deledalle", "C-A.", ""], ["Papadakis", "N.", ""], ["Salmon", "J.", ""], ["Vaiter", "S.", ""]]}, {"id": "1606.05200", "submitter": "Qiang Guo", "authors": "Qiang Guo, Hongwei Chen, Yuxi Wang, Yong Guo, Peng Liu, Xiurui Zhu,\n  Zheng Cheng, Zhenming Yu, Minghua Chen, Sigang Yang, Shizhong Xie", "title": "High-speed real-time single-pixel microscopy based on Fourier sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-pixel cameras based on the concepts of compressed sensing (CS)\nleverage the inherent structure of images to retrieve them with far fewer\nmeasurements and operate efficiently over a significantly broader spectral\nrange than conventional silicon-based cameras. Recently, photonic time-stretch\n(PTS) technique facilitates the emergence of high-speed single-pixel cameras. A\nsignificant breakthrough in imaging speed of single-pixel cameras enables\nobservation of fast dynamic phenomena. However, according to CS theory, image\nreconstruction is an iterative process that consumes enormous amounts of\ncomputational time and cannot be performed in real time. To address this\nchallenge, we propose a novel single-pixel imaging technique that can produce\nhigh-quality images through rapid acquisition of their effective spatial\nFourier spectrum. We employ phase-shifting sinusoidal structured illumination\ninstead of random illumination for spectrum acquisition and apply inverse\nFourier transform to the obtained spectrum for image restoration. We evaluate\nthe performance of our prototype system by recognizing quick response (QR)\ncodes and flow cytometric screening of cells. A frame rate of 625 kHz and a\ncompression ratio of 10% are experimentally demonstrated in accordance with the\nrecognition rate of the QR code. An imaging flow cytometer enabling\nhigh-content screening with an unprecedented throughput of 100,000 cells/s is\nalso demonstrated. For real-time imaging applications, the proposed\nsingle-pixel microscope can significantly reduce the time required for image\nreconstruction by two orders of magnitude, which can be widely applied in\nindustrial quality control and label-free biomedical imaging.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 02:03:54 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Guo", "Qiang", ""], ["Chen", "Hongwei", ""], ["Wang", "Yuxi", ""], ["Guo", "Yong", ""], ["Liu", "Peng", ""], ["Zhu", "Xiurui", ""], ["Cheng", "Zheng", ""], ["Yu", "Zhenming", ""], ["Chen", "Minghua", ""], ["Yang", "Sigang", ""], ["Xie", "Shizhong", ""]]}, {"id": "1606.05228", "submitter": "Charles Zheng", "authors": "Charles Y. Zheng, Rakesh Achanta, and Yuval Benjamini", "title": "How many faces can be recognized? Performance extrapolation for\n  multi-class classification", "comments": "Submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of multi-class classification generally increases with the\nnumber of classes. Using data from a subset of the classes, can we predict how\nwell a classifier will scale with an increased number of classes? Under the\nassumption that the classes are sampled exchangeably, and under the assumption\nthat the classifier is generative (e.g. QDA or Naive Bayes), we show that the\nexpected accuracy when the classifier is trained on $k$ classes is the $k-1$st\nmoment of a \\emph{conditional accuracy distribution}, which can be estimated\nfrom data. This provides the theoretical foundation for performance\nextrapolation based on pseudolikelihood, unbiased estimation, and\nhigh-dimensional asymptotics. We investigate the robustness of our methods to\nnon-generative classifiers in simulations and one optical character recognition\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:38:20 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Zheng", "Charles Y.", ""], ["Achanta", "Rakesh", ""], ["Benjamini", "Yuval", ""]]}, {"id": "1606.05233", "submitter": "Jo\\~ao F. Henriques", "authors": "Luca Bertinetto, Jo\\~ao F. Henriques, Jack Valmadre, Philip H. S.\n  Torr, Andrea Vedaldi", "title": "Learning feed-forward one-shot learners", "comments": "The first three authors contributed equally, and are listed in\n  alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot learning is usually tackled by using generative models or\ndiscriminative embeddings. Discriminative methods based on deep learning, which\nare very effective in other learning scenarios, are ill-suited for one-shot\nlearning as they need large amounts of training data. In this paper, we propose\na method to learn the parameters of a deep model in one shot. We construct the\nlearner as a second deep network, called a learnet, which predicts the\nparameters of a pupil network from a single exemplar. In this manner we obtain\nan efficient feed-forward one-shot learner, trained end-to-end by minimizing a\none-shot classification objective in a learning to learn formulation. In order\nto make the construction feasible, we propose a number of factorizations of the\nparameters of the pupil network. We demonstrate encouraging results by learning\ncharacters from single exemplars in Omniglot, and by tracking visual objects\nfrom a single initial exemplar in the Visual Object Tracking benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 15:49:26 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Bertinetto", "Luca", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Valmadre", "Jack", ""], ["Torr", "Philip H. S.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1606.05255", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "3D zigzag for multislicing, multiband and video processing", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D zigzag rafter (first in literature) which allows us to obtain\nthe exact sequence of spectral components after application of Discrete Cosine\nTransform 3D (DCT-2D) over a cube. Such cube represents part of a video or\neventually a group of images such as multislicing (e.g., Magnetic Resonance or\nComputed Tomography imaging) and multi or hyperspectral imagery (optical\nsatellites). Besides, we present a new version of the traditional 2D zigzag,\nincluding the case of rectangular blocks. Finally, all the attached code is\ndone in MATLAB, and that code serves both blocks of pixels or blocks of blocks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 16:45:39 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1606.05262", "submitter": "Joel Moniz", "authors": "Joel Moniz and Christopher Pal", "title": "Convolutional Residual Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional neural networks (CNNs) yield state of the art results\non a wide variety of visual recognition problems. A number of state of the the\nart methods for image recognition are based on networks with well over 100\nlayers and the performance vs. depth trend is moving towards networks in excess\nof 1000 layers. In such extremely deep architectures the vanishing or exploding\ngradient problem becomes a key issue. Recent evidence also indicates that\nconvolutional networks could benefit from an interface to explicitly\nconstructed memory mechanisms interacting with a CNN feature processing\nhierarchy. Correspondingly, we propose and evaluate a memory mechanism enhanced\nconvolutional neural network architecture based on augmenting convolutional\nresidual networks with a long short term memory mechanism. We refer to this as\na convolutional residual memory network. To the best of our knowledge this\napproach can yield state of the art performance on the CIFAR-100 benchmark and\ncompares well with other state of the art techniques on the CIFAR-10 and SVHN\nbenchmarks. This is achieved using networks with more breadth, much less depth\nand much less overall computation relative to comparable deep ResNets without\nthe memory mechanism. Our experiments and analysis explore the importance of\nthe memory mechanism, network depth, breadth, and predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 16:54:39 GMT"}, {"version": "v2", "created": "Sun, 19 Jun 2016 05:47:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 18:40:24 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Moniz", "Joel", ""], ["Pal", "Christopher", ""]]}, {"id": "1606.05310", "submitter": "Mark Marsden", "authors": "M. Marsden, K. McGuinness, S. Little, N. E. O'Connor", "title": "Holistic Features For Real-Time Crowd Behaviour Anomaly Detection", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to crowd behaviour anomaly detection that\nuses a set of efficiently computed, easily interpretable, scene-level holistic\nfeatures. This low-dimensional descriptor combines two features from the\nliterature: crowd collectiveness [1] and crowd conflict [2], with two newly\ndeveloped crowd features: mean motion speed and a new formulation of crowd\ndensity. Two different anomaly detection approaches are investigated using\nthese features. When only normal training data is available we use a Gaussian\nMixture Model (GMM) for outlier detection. When both normal and abnormal\ntraining data is available we use a Support Vector Machine (SVM) for binary\nclassification. We evaluate on two crowd behaviour anomaly detection datasets,\nachieving both state-of-the-art classification performance on the violent-flows\ndataset [3] as well as better than real-time processing performance (40 frames\nper second).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 18:37:25 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Marsden", "M.", ""], ["McGuinness", "K.", ""], ["Little", "S.", ""], ["O'Connor", "N. E.", ""]]}, {"id": "1606.05328", "submitter": "A\\\"aron van den Oord", "authors": "Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt,\n  Alex Graves, Koray Kavukcuoglu", "title": "Conditional Image Generation with PixelCNN Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores conditional image generation with a new image density\nmodel based on the PixelCNN architecture. The model can be conditioned on any\nvector, including descriptive labels or tags, or latent embeddings created by\nother networks. When conditioned on class labels from the ImageNet database,\nthe model is able to generate diverse, realistic scenes representing distinct\nanimals, objects, landscapes and structures. When conditioned on an embedding\nproduced by a convolutional network given a single image of an unseen face, it\ngenerates a variety of new portraits of the same person with different facial\nexpressions, poses and lighting conditions. We also show that conditional\nPixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\nthe gated convolutional layers in the proposed model improve the log-likelihood\nof PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\nwith greatly reduced computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:40:56 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 15:44:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Oord", "Aaron van den", ""], ["Kalchbrenner", "Nal", ""], ["Vinyals", "Oriol", ""], ["Espeholt", "Lasse", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1606.05355", "submitter": "Nasim Souly", "authors": "Subhabrata Bhattacharya, Nasim Souly and Mubarak Shah", "title": "Covariance of Motion and Appearance Featuresfor Spatio Temporal\n  Recognition Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an end-to-end framework for video analysis\nfocused towards practical scenarios built on theoretical foundations from\nsparse representation, including a novel descriptor for general purpose video\nanalysis. In our approach, we compute kinematic features from optical flow and\nfirst and second-order derivatives of intensities to represent motion and\nappearance respectively. These features are then used to construct covariance\nmatrices which capture joint statistics of both low-level motion and appearance\nfeatures extracted from a video. Using an over-complete dictionary of the\ncovariance based descriptors built from labeled training samples, we formulate\nlow-level event recognition as a sparse linear approximation problem. Within\nthis, we pose the sparse decomposition of a covariance matrix, which also\nconforms to the space of semi-positive definite matrices, as a determinant\nmaximization problem. Also since covariance matrices lie on non-linear\nRiemannian manifolds, we compare our former approach with a sparse linear\napproximation alternative that is suitable for equivalent vector spaces of\ncovariance matrices. This is done by searching for the best projection of the\nquery data on a dictionary using an Orthogonal Matching pursuit algorithm. We\nshow the applicability of our video descriptor in two different application\ndomains - namely low-level event recognition in unconstrained scenarios and\ngesture recognition using one shot learning. Our experiments provide promising\ninsights in large scale video analysis.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 20:01:13 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Bhattacharya", "Subhabrata", ""], ["Souly", "Nasim", ""], ["Shah", "Mubarak", ""]]}, {"id": "1606.05381", "submitter": "Jie Feng", "authors": "Jie Feng, Svebor Karaman, I-Hong Jhuo and Shih-Fu Chang", "title": "Deep Image Set Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications involving matching of image sets, the information from\nmultiple images must be effectively exploited to represent each set.\nState-of-the-art methods use probabilistic distribution or subspace to model a\nset and use specific distance measure to compare two sets. These methods are\nslow to compute and not compact to use in a large scale scenario.\nLearning-based hashing is often used in large scale image retrieval as they\nprovide a compact representation of each sample and the Hamming distance can be\nused to efficiently compare two samples. However, most hashing methods encode\neach image separately and discard knowledge that multiple images in the same\nset represent the same object or person. We investigate the set hashing problem\nby combining both set representation and hashing in a single deep neural\nnetwork. An image set is first passed to a CNN module to extract image\nfeatures, then these features are aggregated using two types of set feature to\ncapture both set specific and database-wide distribution information. The\ncomputed set feature is then fed into a multilayer perceptron to learn a\ncompact binary embedding. Triplet loss is used to train the network by forming\nset similarity relations using class labels. We extensively evaluate our\napproach on datasets used for image matching and show highly competitive\nperformance compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:04:02 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 02:14:51 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Feng", "Jie", ""], ["Karaman", "Svebor", ""], ["Jhuo", "I-Hong", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1606.05413", "submitter": "Chenchen Zhu", "authors": "Chenchen Zhu, Yutong Zheng, Khoa Luu, Marios Savvides", "title": "CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust face detection in the wild is one of the ultimate components to\nsupport various facial related problems, i.e. unconstrained face recognition,\nfacial periocular recognition, facial landmarking and pose estimation, facial\nexpression recognition, 3D facial model construction, etc. Although the face\ndetection problem has been intensely studied for decades with various\ncommercial applications, it still meets problems in some real-world scenarios\ndue to numerous challenges, e.g. heavy facial occlusions, extremely low\nresolutions, strong illumination, exceptionally pose variations, image or video\ncompression artifacts, etc. In this paper, we present a face detection approach\nnamed Contextual Multi-Scale Region-based Convolution Neural Network (CMS-RCNN)\nto robustly solve the problems mentioned above. Similar to the region-based\nCNNs, our proposed network consists of the region proposal component and the\nregion-of-interest (RoI) detection component. However, far apart of that\nnetwork, there are two main contributions in our proposed network that play a\nsignificant role to achieve the state-of-the-art performance in face detection.\nFirstly, the multi-scale information is grouped both in region proposal and RoI\ndetection to deal with tiny face regions. Secondly, our proposed network allows\nexplicit body contextual reasoning in the network inspired from the intuition\nof human vision system. The proposed approach is benchmarked on two recent\nchallenging face detection databases, i.e. the WIDER FACE Dataset which\ncontains high degree of variability, as well as the Face Detection Dataset and\nBenchmark (FDDB). The experimental results show that our proposed approach\ntrained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE\nDataset by a large margin, and consistently achieves competitive results on\nFDDB against the recent state-of-the-art face detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 03:19:09 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhu", "Chenchen", ""], ["Zheng", "Yutong", ""], ["Luu", "Khoa", ""], ["Savvides", "Marios", ""]]}, {"id": "1606.05415", "submitter": "Zhiwei Li", "authors": "Zhiwei Li, Huanfeng Shen, Huifang Li, Guisong Xia, Paolo Gamba,\n  Liangpei Zhang", "title": "Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide\n  field of view imagery", "comments": "This manuscript has been accepted for publication in Remote Sensing\n  of Environment, vol. 191, pp.342-358, 2017.\n  (http://www.sciencedirect.com/science/article/pii/S003442571730038X)", "journal-ref": "Remote Sensing of Environment, vol. 191, pp.342-358, 2017", "doi": "10.1016/j.rse.2017.01.026", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide field of view (WFV) imaging system onboard the Chinese GaoFen-1\n(GF-1) optical satellite has a 16-m resolution and four-day revisit cycle for\nlarge-scale Earth observation. The advantages of the high temporal-spatial\nresolution and the wide field of view make the GF-1 WFV imagery very popular.\nHowever, cloud cover is an inevitable problem in GF-1 WFV imagery, which\ninfluences its precise application. Accurate cloud and cloud shadow detection\nin GF-1 WFV imagery is quite difficult due to the fact that there are only\nthree visible bands and one near-infrared band. In this paper, an automatic\nmulti-feature combined (MFC) method is proposed for cloud and cloud shadow\ndetection in GF-1 WFV imagery. The MFC algorithm first implements threshold\nsegmentation based on the spectral features and mask refinement based on guided\nfiltering to generate a preliminary cloud mask. The geometric features are then\nused in combination with the texture features to improve the cloud detection\nresults and produce the final cloud mask. Finally, the cloud shadow mask can be\nacquired by means of the cloud and shadow matching and follow-up correction\nprocess. The method was validated using 108 globally distributed scenes. The\nresults indicate that MFC performs well under most conditions, and the average\noverall accuracy of MFC cloud detection is as high as 96.8%. In the contrastive\nanalysis with the official provided cloud fractions, MFC shows a significant\nimprovement in cloud fraction estimation, and achieves a high accuracy for the\ncloud and cloud shadow detection in the GF-1 WFV imagery with fewer spectral\nbands. The proposed method could be used as a preprocessing step in the future\nto monitor land-cover change, and it could also be easily extended to other\noptical satellite imagery which has a similar spectral setting.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 03:26:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 10:51:46 GMT"}, {"version": "v3", "created": "Sun, 17 Jul 2016 09:12:40 GMT"}, {"version": "v4", "created": "Sun, 5 Feb 2017 04:59:29 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Li", "Zhiwei", ""], ["Shen", "Huanfeng", ""], ["Li", "Huifang", ""], ["Xia", "Guisong", ""], ["Gamba", "Paolo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1606.05426", "submitter": "Jose M. Alvarez", "authors": "Jose Alvarez and Lars Petersson", "title": "DecomposeMe: Simplifying ConvNets for End-to-End Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and convolutional neural networks (ConvNets) have been\nsuccessfully applied to most relevant tasks in the computer vision community.\nHowever, these networks are computationally demanding and not suitable for\nembedded devices where memory and time consumption are relevant.\n  In this paper, we propose DecomposeMe, a simple but effective technique to\nlearn features using 1D convolutions. The proposed architecture enables both\nsimplicity and filter sharing leading to increased learning capacity. A\ncomprehensive set of large-scale experiments on ImageNet and Places2\ndemonstrates the ability of our method to improve performance while\nsignificantly reducing the number of parameters required. Notably, on Places2,\nwe obtain an improvement in relative top-1 classification accuracy of 7.7\\%\nwith an architecture that requires 92% fewer parameters compared to VGG-B. The\nproposed network is also demonstrated to generalize to other tasks by\nconverting existing networks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 06:48:12 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Alvarez", "Jose", ""], ["Petersson", "Lars", ""]]}, {"id": "1606.05433", "submitter": "Chunhua Shen", "authors": "Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick", "title": "FVQA: Fact-based Visual Question Answering", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has attracted a lot of attention in both\nComputer Vision and Natural Language Processing communities, not least because\nit offers insight into the relationships between two important sources of\ninformation. Current datasets, and the models built upon them, have focused on\nquestions which are answerable by direct analysis of the question and image\nalone. The set of such questions that require no external information to answer\nis interesting, but very limited. It excludes questions which require common\nsense, or basic factual knowledge to answer, for example. Here we introduce\nFVQA, a VQA dataset which requires, and supports, much deeper reasoning. FVQA\nonly contains questions which require external information to answer.\n  We thus extend a conventional visual question answering dataset, which\ncontains image-question-answerg triplets, through additional\nimage-question-answer-supporting fact tuples. The supporting fact is\nrepresented as a structural triplet, such as <Cat,CapableOf,ClimbingTrees>.\n  We evaluate several baseline models on the FVQA dataset, and describe a novel\nmodel which is capable of reasoning about an image on the basis of supporting\nfacts.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 07:37:32 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 03:21:58 GMT"}, {"version": "v3", "created": "Sat, 10 Dec 2016 06:39:16 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 05:10:20 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Wang", "Peng", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Dick", "Anthony", ""]]}, {"id": "1606.05487", "submitter": "Renzo Andri", "authors": "Renzo Andri and Lukas Cavigelli and Davide Rossi and Luca Benini", "title": "YodaNN: An Architecture for Ultra-Low Power Binary-Weight CNN\n  Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have revolutionized the world of\ncomputer vision over the last few years, pushing image classification beyond\nhuman accuracy. The computational effort of today's CNNs requires power-hungry\nparallel processors or GP-GPUs. Recent developments in CNN accelerators for\nsystem-on-chip integration have reduced energy consumption significantly.\nUnfortunately, even these highly optimized devices are above the power envelope\nimposed by mobile and deeply embedded applications and face hard limitations\ncaused by CNN weight I/O and storage. This prevents the adoption of CNNs in\nfuture ultra-low power Internet of Things end-nodes for near-sensor analytics.\nRecent algorithmic and theoretical advancements enable competitive\nclassification accuracy even when limiting CNNs to binary (+1/-1) weights\nduring training. These new findings bring major optimization opportunities in\nthe arithmetic core by removing the need for expensive multiplications, as well\nas reducing I/O bandwidth and storage. In this work, we present an accelerator\noptimized for binary-weight CNNs that achieves 1510 GOp/s at 1.2 V on a core\narea of only 1.33 MGE (Million Gate Equivalent) or 0.19 mm$^2$ and with a power\ndissipation of 895 {\\mu}W in UMC 65 nm technology at 0.6 V. Our accelerator\nsignificantly outperforms the state-of-the-art in terms of energy and area\nefficiency achieving 61.2 TOp/s/W@0.6 V and 1135 GOp/s/MGE@1.2 V, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 11:48:29 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 11:00:58 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 10:54:26 GMT"}, {"version": "v4", "created": "Fri, 24 Feb 2017 08:46:12 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1606.05506", "submitter": "Sebastian Stabinger MSc", "authors": "Sebastian Stabinger, Antonio Rodriguez-Sanchez, Justus Piater", "title": "Learning Abstract Classes using Deep Learning", "comments": "To be published in the proceedings of the International Conference on\n  Bio-inspired Information and Communications Technologies 2015", "journal-ref": null, "doi": "10.4108/eai.3-12-2015.2262468", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are generally good at learning abstract concepts about objects and\nscenes (e.g.\\ spatial orientation, relative sizes, etc.). Over the last years\nconvolutional neural networks have achieved almost human performance in\nrecognizing concrete classes (i.e.\\ specific object categories). This paper\ntests the performance of a current CNN (GoogLeNet) on the task of\ndifferentiating between abstract classes which are trivially differentiable for\nhumans. We trained and tested the CNN on the two abstract classes of horizontal\nand vertical orientation and determined how well the network is able to\ntransfer the learned classes to other, previously unseen objects.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 12:51:23 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Stabinger", "Sebastian", ""], ["Rodriguez-Sanchez", "Antonio", ""], ["Piater", "Justus", ""]]}, {"id": "1606.05535", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej\n  Cichocki", "title": "Tensor Ring Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks have in recent years emerged as the powerful tools for\nsolving the large-scale optimization problems. One of the most popular tensor\nnetwork is tensor train (TT) decomposition that acts as the building blocks for\nthe complicated tensor networks. However, the TT decomposition highly depends\non permutations of tensor dimensions, due to its strictly sequential\nmultilinear products over latent cores, which leads to difficulties in finding\nthe optimal TT representation. In this paper, we introduce a fundamental tensor\ndecomposition model to represent a large dimensional tensor by a circular\nmultilinear products over a sequence of low dimensional cores, which can be\ngraphically interpreted as a cyclic interconnection of 3rd-order tensors, and\nthus termed as tensor ring (TR) decomposition. The key advantage of TR model is\nthe circular dimensional permutation invariance which is gained by employing\nthe trace operation and treating the latent cores equivalently. TR model can be\nviewed as a linear combination of TT decompositions, thus obtaining the\npowerful and generalized representation abilities. For optimization of latent\ncores, we present four different algorithms based on the sequential SVDs, ALS\nscheme, and block-wise ALS techniques. Furthermore, the mathematical properties\nof TR model are investigated, which shows that the basic multilinear algebra\ncan be performed efficiently by using TR representaions and the classical\ntensor decompositions can be conveniently transformed into the TR\nrepresentation. Finally, the experiments on both synthetic signals and\nreal-world datasets were conducted to evaluate the performance of different\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 14:40:18 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhou", "Guoxu", ""], ["Xie", "Shengli", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1606.05589", "submitter": "Abhishek Das", "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv\n  Batra", "title": "Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions?", "comments": "5 pages, 4 figures, 3 tables, presented at 2016 ICML Workshop on\n  Human Interpretability in Machine Learning (WHI 2016), New York, NY. arXiv\n  admin note: substantial text overlap with arXiv:1606.03556", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:00:02 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Das", "Abhishek", ""], ["Agrawal", "Harsh", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1606.05614", "submitter": "Cristiano Premebida", "authors": "C. Premebida, L. Garrote, A. Asvadi, A. Pedro Ribeiro, and U. Nunes", "title": "High-resolution LIDAR-based Depth Mapping using Bilateral Filter", "comments": "8 pages, 6 figures, submitted to IEEE-ITSC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution depth-maps, obtained by upsampling sparse range data from a\n3D-LIDAR, find applications in many fields ranging from sensory perception to\nsemantic segmentation and object detection. Upsampling is often based on\ncombining data from a monocular camera to compensate the low-resolution of a\nLIDAR. This paper, on the other hand, introduces a novel framework to obtain\ndense depth-map solely from a single LIDAR point cloud; which is a research\ndirection that has been barely explored. The formulation behind the proposed\ndepth-mapping process relies on local spatial interpolation, using\nsliding-window (mask) technique, and on the Bilateral Filter (BF) where the\nvariable of interest, the distance from the sensor, is considered in the\ninterpolation problem. In particular, the BF is conveniently modified to\nperform depth-map upsampling such that the edges (foreground-background\ndiscontinuities) are better preserved by means of a proposed method which\ninfluences the range-based weighting term. Other methods for spatial upsampling\nare discussed, evaluated and compared in terms of different error measures.\nThis paper also researches the role of the mask's size in the performance of\nthe implemented methods. Quantitative and qualitative results from experiments\non the KITTI Database, using LIDAR point clouds only, show very satisfactory\nperformance of the approach introduced in this work.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 18:14:59 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Premebida", "C.", ""], ["Garrote", "L.", ""], ["Asvadi", "A.", ""], ["Ribeiro", "A. Pedro", ""], ["Nunes", "U.", ""]]}, {"id": "1606.05675", "submitter": "Chang Liu", "authors": "Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane and\n  Yunsheng Ma", "title": "DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided\n  Dietary Assessment", "comments": "12 pages, 2 figures, 6 tables, ICOST 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Worldwide, in 2014, more than 1.9 billion adults, 18 years and older, were\noverweight. Of these, over 600 million were obese. Accurately documenting\ndietary caloric intake is crucial to manage weight loss, but also presents\nchallenges because most of the current methods for dietary assessment must rely\non memory to recall foods eaten. The ultimate goal of our research is to\ndevelop computer-aided technical solutions to enhance and improve the accuracy\nof current measurements of dietary intake. Our proposed system in this paper\naims to improve the accuracy of dietary assessment by analyzing the food images\ncaptured by mobile devices (e.g., smartphone). The key technique innovation in\nthis paper is the deep learning-based food image recognition algorithms.\nSubstantial research has demonstrated that digital imaging accurately estimates\ndietary intake in many environments and it has many advantages over other\nmethods. However, how to derive the food information (e.g., food type and\nportion size) from food image effectively and efficiently remains a challenging\nand open research problem. We propose a new Convolutional Neural Network\n(CNN)-based food image recognition algorithm to address this problem. We\napplied our proposed approach to two real-world food image data sets (UEC-256\nand Food-101) and achieved impressive results. To the best of our knowledge,\nthese results outperformed all other reported work using these two data sets.\nOur experiments have demonstrated that the proposed approach is a promising\nsolution for addressing the food image recognition problem. Our future work\nincludes further improving the performance of the algorithms and integrating\nour system into a real-world mobile and cloud computing-based system to enhance\nthe accuracy of current measurements of dietary intake.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 21:03:19 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Liu", "Chang", ""], ["Cao", "Yu", ""], ["Luo", "Yan", ""], ["Chen", "Guanling", ""], ["Vokkarane", "Vinod", ""], ["Ma", "Yunsheng", ""]]}, {"id": "1606.05681", "submitter": "{\\L}ukasz Olech Piotr", "authors": "{\\L}ukasz P. Olech, Micha{\\l} Spytkowski, Halina Kwa\\'snicka, Zbigniew\n  Michalewicz", "title": "Hierarchical Data Generator based on Tree-Structured Stick Breaking\n  Process for Benchmarking Clustering Methods", "comments": "This article was submitted to Elsevier Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Cluster Hierarchies is a new variant of Hierarchical Cluster Analysis\nthat gains interest in the field of Machine Learning. Being still at an early\nstage of development, the lack of tools for systematic analysis of Object\nCluster Hierarchies inhibits its further improvement. In this paper we address\nthis issue by proposing a generator of synthetic hierarchical data that can be\nused for benchmarking Object Cluster Hierarchy methods. The article presents a\nthorough empirical and theoretical analysis of the generator and provides\nguidance on how to control its parameters. Conducted experiments show the\nusefulness of the data generator that is capable of producing a wide range of\ndifferently structured data. Further, benchmarking datasets that mirror the\nmost common types of hierarchies are generated and made available to the\npublic, together with the developed generator\n(http://kio.pwr.edu.pl/?page\\_id=396).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 21:21:15 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 14:56:46 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 23:45:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Olech", "\u0141ukasz P.", ""], ["Spytkowski", "Micha\u0142", ""], ["Kwa\u015bnicka", "Halina", ""], ["Michalewicz", "Zbigniew", ""]]}, {"id": "1606.05703", "submitter": "Joan Duran", "authors": "Joan Duran, Antoni Buades, Bartomeu Coll, Catalina Sbert, Gwendoline\n  Blanchet", "title": "A Survey of Pansharpening Methods with A New Band-Decoupled Variational\n  Model", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 125, pp.\n  78-105, 2017", "doi": "10.1016/j.isprsjprs.2016.12.013", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most satellites decouple the acquisition of a panchromatic image at high\nspatial resolution from the acquisition of a multispectral image at lower\nspatial resolution. Pansharpening is a fusion technique used to increase the\nspatial resolution of the multispectral data while simultaneously preserving\nits spectral information. In this paper, we consider pansharpening as an\noptimization problem minimizing a cost function with a nonlocal regularization\nterm. The energy functional which is to be minimized decouples for each band,\nthus permitting the application to misregistered spectral components. This\nrequirement is achieved by dropping the, commonly used, assumption that relates\nthe spectral and panchromatic modalities by a linear transformation. Instead, a\nnew constraint that preserves the radiometric ratio between the panchromatic\nand each spectral component is introduced. An exhaustive performance comparison\nof the proposed fusion method with several classical and state-of-the-art\npansharpening techniques illustrates its superiority in preserving spatial\ndetails, reducing color distortions, and avoiding the creation of aliasing\nartifacts.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:12:32 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Duran", "Joan", ""], ["Buades", "Antoni", ""], ["Coll", "Bartomeu", ""], ["Sbert", "Catalina", ""], ["Blanchet", "Gwendoline", ""]]}, {"id": "1606.05705", "submitter": "Shoou-I Yu", "authors": "Shoou-I Yu, Yi Yang, Zhongwen Xu, Shicheng Xu, Deyu Meng, Zexi Mao,\n  Zhigang Ma, Ming Lin, Xuanchong Li, Huan Li, Zhenzhong Lan, Lu Jiang,\n  Alexander G. Hauptmann, Chuang Gan, Xingzhong Du, Xiaojun Chang", "title": "Strategies for Searching Video Content with Text Queries or Video\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large number of user-generated videos uploaded on to the Internet\neveryday has led to many commercial video search engines, which mainly rely on\ntext metadata for search. However, metadata is often lacking for user-generated\nvideos, thus these videos are unsearchable by current search engines.\nTherefore, content-based video retrieval (CBVR) tackles this metadata-scarcity\nproblem by directly analyzing the visual and audio streams of each video. CBVR\nencompasses multiple research topics, including low-level feature design,\nfeature fusion, semantic detector training and video search/reranking. We\npresent novel strategies in these topics to enhance CBVR in both accuracy and\nspeed under different query inputs, including pure textual queries and query by\nvideo examples. Our proposed strategies have been incorporated into our\nsubmission for the TRECVID 2014 Multimedia Event Detection evaluation, where\nour system outperformed other submissions in both text queries and video\nexample queries, thus demonstrating the effectiveness of our proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:27:06 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Yu", "Shoou-I", ""], ["Yang", "Yi", ""], ["Xu", "Zhongwen", ""], ["Xu", "Shicheng", ""], ["Meng", "Deyu", ""], ["Mao", "Zexi", ""], ["Ma", "Zhigang", ""], ["Lin", "Ming", ""], ["Li", "Xuanchong", ""], ["Li", "Huan", ""], ["Lan", "Zhenzhong", ""], ["Jiang", "Lu", ""], ["Hauptmann", "Alexander G.", ""], ["Gan", "Chuang", ""], ["Du", "Xingzhong", ""], ["Chang", "Xiaojun", ""]]}, {"id": "1606.05718", "submitter": "Andrew Beck", "authors": "Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, and Andrew\n  H. Beck", "title": "Deep Learning for Identifying Metastatic Breast Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The International Symposium on Biomedical Imaging (ISBI) held a grand\nchallenge to evaluate computational systems for the automated detection of\nmetastatic breast cancer in whole slide images of sentinel lymph node biopsies.\nOur team won both competitions in the grand challenge, obtaining an area under\nthe receiver operating curve (AUC) of 0.925 for the task of whole slide image\nclassification and a score of 0.7051 for the tumor localization task. A\npathologist independently reviewed the same images, obtaining a whole slide\nimage classification AUC of 0.966 and a tumor localization score of 0.733.\nCombining our deep learning system's predictions with the human pathologist's\ndiagnoses increased the pathologist's AUC to 0.995, representing an\napproximately 85 percent reduction in human error rate. These results\ndemonstrate the power of using deep learning to produce significant\nimprovements in the accuracy of pathological diagnoses.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 04:00:31 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Dayong", ""], ["Khosla", "Aditya", ""], ["Gargeya", "Rishab", ""], ["Irshad", "Humayun", ""], ["Beck", "Andrew H.", ""]]}, {"id": "1606.05729", "submitter": "Yao Guo", "authors": "Yao Guo, Youfu Li and Zhanpeng Shao", "title": "RRV: A Spatiotemporal Descriptor for Rigid Body Motion Recognition", "comments": "To be published in the future issue of IEEE Trans. on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2017.2705227", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion behaviors of a rigid body can be characterized by a 6-dimensional\nmotion trajectory, which contains position vectors of a reference point on the\nrigid body and rotations of this rigid body over time. This paper devises a\nRotation and Relative Velocity (RRV) descriptor by exploring the local\ntranslational and rotational invariants of motion trajectories of rigid bodies,\nwhich is insensitive to noise, invariant to rigid transformation and scaling. A\nflexible metric is also introduced to measure the distance between two RRV\ndescriptors. The RRV descriptor is then applied to characterize motions of a\nhuman body skeleton modeled as articulated interconnections of multiple rigid\nbodies. To illustrate the descriptive ability of the RRV descriptor, we explore\nit for different rigid body motion recognition tasks. The experimental results\non benchmark datasets demonstrate that this simple RRV descriptor outperforms\nthe previous ones regarding recognition accuracy without increasing\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 08:10:59 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 14:53:15 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Guo", "Yao", ""], ["Li", "Youfu", ""], ["Shao", "Zhanpeng", ""]]}, {"id": "1606.05763", "submitter": "Xu-Yao Zhang", "authors": "Xu-Yao Zhang and Yoshua Bengio and Cheng-Lin Liu", "title": "Online and Offline Handwritten Chinese Character Recognition: A\n  Comprehensive Study and New Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based methods have achieved the state-of-the-art\nperformance for handwritten Chinese character recognition (HCCR) by learning\ndiscriminative representations directly from raw data. Nevertheless, we believe\nthat the long-and-well investigated domain-specific knowledge should still help\nto boost the performance of HCCR. By integrating the traditional\nnormalization-cooperated direction-decomposed feature map (directMap) with the\ndeep convolutional neural network (convNet), we are able to obtain new highest\naccuracies for both online and offline HCCR on the ICDAR-2013 competition\ndatabase. With this new framework, we can eliminate the needs for data\naugmentation and model ensemble, which are widely used in other systems to\nachieve their best results. This makes our framework to be efficient and\neffective for both training and testing. Furthermore, although\ndirectMap+convNet can achieve the best results and surpass human-level\nperformance, we show that writer adaptation in this case is still effective. A\nnew adaptation layer is proposed to reduce the mismatch between training and\ntest data on a particular source layer. The adaptation process can be\nefficiently and effectively implemented in an unsupervised manner. By adding\nthe adaptation layer into the pre-trained convNet, it can adapt to the new\nhandwriting styles of particular writers, and the recognition accuracy can be\nfurther improved consistently and significantly. This paper gives an overview\nand comparison of recent deep learning based approaches for HCCR, and also sets\nnew benchmarks for both online and offline HCCR.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 14:49:32 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhang", "Xu-Yao", ""], ["Bengio", "Yoshua", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1606.05785", "submitter": "Atishay Jain", "authors": "Atishay Jain", "title": "Automatic 3D Reconstruction for Symmetric Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic 3D reconstruction from a single image is a difficult problem. A lot\nof data loss occurs in the projection. A domain based approach to\nreconstruction where we solve a smaller set of problems for a particular use\ncase lead to greater returns. The project provides a way to automatically\ngenerate full 3-D renditions of actual symmetric images that have some prior\ninformation provided in the pipeline by a recognition algorithm. We provide a\ncritical analysis on how this can be enhanced and improved to provide a general\nreconstruction framework for automatic reconstruction for any symmetric shape.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 17:42:27 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Jain", "Atishay", ""]]}, {"id": "1606.05814", "submitter": "Kyle Krafka", "authors": "Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan\n  and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba", "title": "Eye Tracking for Everyone", "comments": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From scientific research to commercial applications, eye tracking is an\nimportant tool across many domains. Despite its range of applications, eye\ntracking has yet to become a pervasive technology. We believe that we can put\nthe power of eye tracking in everyone's palm by building eye tracking software\nthat works on commodity hardware such as mobile phones and tablets, without the\nneed for additional sensors or devices. We tackle this problem by introducing\nGazeCapture, the first large-scale dataset for eye tracking, containing data\nfrom over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we\ntrain iTracker, a convolutional neural network for eye tracking, which achieves\na significant reduction in error over previous approaches while running in real\ntime (10-15fps) on a modern mobile device. Our model achieves a prediction\nerror of 1.71cm and 2.53cm without calibration on mobile phones and tablets\nrespectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further,\nwe demonstrate that the features learned by iTracker generalize well to other\ndatasets, achieving state-of-the-art results. The code, data, and models are\navailable at http://gazecapture.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 23:53:54 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Krafka", "Kyle", ""], ["Khosla", "Aditya", ""], ["Kellnhofer", "Petr", ""], ["Kannan", "Harini", ""], ["Bhandarkar", "Suchendra", ""], ["Matusik", "Wojciech", ""], ["Torralba", "Antonio", ""]]}, {"id": "1606.05897", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Matthias Bethge, Aaron Hertzmann and Eli Shechtman", "title": "Preserving Color in Neural Artistic Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents an extension to the neural artistic style transfer\nalgorithm (Gatys et al.). The original algorithm transforms an image to have\nthe style of another given image. For example, a photograph can be transformed\nto have the style of a famous painting. Here we address a potential shortcoming\nof the original method: the algorithm transfers the colors of the original\npainting, which can alter the appearance of the scene in undesirable ways. We\ndescribe simple linear methods for transferring style while preserving colors.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 18:34:41 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Gatys", "Leon A.", ""], ["Bethge", "Matthias", ""], ["Hertzmann", "Aaron", ""], ["Shechtman", "Eli", ""]]}, {"id": "1606.05929", "submitter": "Hengyue Pan", "authors": "Hengyue Pan and Hui Jiang", "title": "Learning Convolutional Neural Networks using Hybrid Orthogonal\n  Projection and Estimation", "comments": "7 Pages, 5 figures, submitted to AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have yielded the excellent performance\nin a variety of computer vision tasks, where CNNs typically adopt a similar\nstructure consisting of convolution layers, pooling layers and fully connected\nlayers. In this paper, we propose to apply a novel method, namely Hybrid\nOrthogonal Projection and Estimation (HOPE), to CNNs in order to introduce\northogonality into the CNN structure. The HOPE model can be viewed as a hybrid\nmodel to combine feature extraction using orthogonal linear projection with\nmixture models. It is an effective model to extract useful information from the\noriginal high-dimension feature vectors and meanwhile filter out irrelevant\nnoises. In this work, we present three different ways to apply the HOPE models\nto CNNs, i.e., {\\em HOPE-Input}, {\\em single-HOPE-Block} and {\\em\nmulti-HOPE-Blocks}. For {\\em HOPE-Input} CNNs, a HOPE layer is directly used\nright after the input to de-correlate high-dimension input feature vectors.\nAlternatively, in {\\em single-HOPE-Block} and {\\em multi-HOPE-Blocks} CNNs, we\nconsider to use HOPE layers to replace one or more blocks in the CNNs, where\none block may include several convolutional layers and one pooling layer. The\nexperimental results on both Cifar-10 and Cifar-100 data sets have shown that\nthe orthogonal constraints imposed by the HOPE layers can significantly improve\nthe performance of CNNs in these image classification tasks (we have achieved\none of the best performance when image augmentation has not been applied, and\ntop 5 performance with image augmentation).\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 00:19:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 20:42:04 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 08:46:52 GMT"}, {"version": "v4", "created": "Sun, 11 Sep 2016 03:25:25 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pan", "Hengyue", ""], ["Jiang", "Hui", ""]]}, {"id": "1606.05973", "submitter": "Siddharth Gupta", "authors": "Siddharth Gupta, Diana Palsetia, Md. Mostofa Ali Patwary, Ankit\n  Agrawal, Alok Choudhary", "title": "A New Parallel Algorithm for Two-Pass Connected Component Labeling", "comments": "Parallel & Distributed Processing Symposium Workshops (IPDPSW), 2014", "journal-ref": null, "doi": "10.1109/IPDPSW.2014.152", "report-no": null, "categories": "cs.DS cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected Component Labeling (CCL) is an important step in pattern\nrecognition and image processing. It assigns labels to the pixels such that\nadjacent pixels sharing the same features are assigned the same label.\nTypically, CCL requires several passes over the data. We focus on two-pass\ntechnique where each pixel is given a provisional label in the first pass\nwhereas an actual label is assigned in the second pass.\n  We present a scalable parallel two-pass CCL algorithm, called PAREMSP, which\nemploys a scan strategy and the best union-find technique called REMSP, which\nuses REM's algorithm for storing label equivalence information of pixels in a\n2-D image. In the first pass, we divide the image among threads and each thread\nruns the scan phase along with REMSP simultaneously. In the second phase, we\nassign the final labels to the pixels. As REMSP is easily parallelizable, we\nuse the parallel version of REMSP for merging the pixels on the boundary. Our\nexperiments show the scalability of PAREMSP achieving speedups up to $20.1$\nusing $24$ cores on shared memory architecture using OpenMP for an image of\nsize $465.20$ MB. We find that our proposed parallel algorithm achieves linear\nscaling for a large resolution fixed problem size as the number of processing\nelements are increased. Additionally, the parallel algorithm does not make use\nof any hardware specific routines, and thus is highly portable.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 05:13:28 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Gupta", "Siddharth", ""], ["Palsetia", "Diana", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Agrawal", "Ankit", ""], ["Choudhary", "Alok", ""]]}, {"id": "1606.06007", "submitter": "Carsten Gottschlich", "authors": "Carsten Gottschlich, Benjamin Tams, and Stephan Huckemann", "title": "Perfect Fingerprint Orientation Fields by Locally Adaptive Global Models", "comments": null, "journal-ref": "IET Biometrics, vol. 6, no.3, pp. 183-190, May 2017", "doi": "10.1049/iet-bmt.2016.0087", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition is widely used for verification and identification in\nmany commercial, governmental and forensic applications. The orientation field\n(OF) plays an important role at various processing stages in fingerprint\nrecognition systems. OFs are used for image enhancement, fingerprint alignment,\nfor fingerprint liveness detection, fingerprint alteration detection and\nfingerprint matching. In this paper, a novel approach is presented to globally\nmodel an OF combined with locally adaptive methods. We show that this model\nadapts perfectly to the 'true OF' in the limit. This perfect OF is described by\na small number of parameters with straightforward geometric interpretation.\nApplications are manifold: Quick expert marking of very poor quality (for\ninstance latent) OFs, high fidelity low parameter OF compression and a direct\nroad to ground truth OFs markings for large databases, say. In this\ncontribution we describe an algorithm to perfectly estimate OF parameters\nautomatically or semi-automatically, depending on image quality, and we\nestablish the main underlying claim of high fidelity low parameter OF\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 08:23:36 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Gottschlich", "Carsten", ""], ["Tams", "Benjamin", ""], ["Huckemann", "Stephan", ""]]}, {"id": "1606.06108", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada", "title": "DualNet: Domain-Invariant Network for Visual Question Answering", "comments": "Accepted as an oral paper by ICME 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) task not only bridges the gap between images\nand language, but also requires that specific contents within the image are\nunderstood as indicated by linguistic context of the question, in order to\ngenerate the accurate answers. Thus, it is critical to build an efficient\nembedding of images and texts. We implement DualNet, which fully takes\nadvantage of discriminative power of both image and textual features by\nseparately performing two operations. Building an ensemble of DualNet further\nboosts the performance. Contrary to common belief, our method proved effective\nin both real images and abstract scenes, in spite of significantly different\nproperties of respective domain. Our method was able to outperform previous\nstate-of-the-art methods in real images category even without explicitly\nemploying attention mechanism, and also outperformed our own state-of-the-art\nmethod in abstract scenes category, which recently won the first place in VQA\nChallenge 2016.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 13:28:35 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 07:54:06 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Saito", "Kuniaki", ""], ["Shin", "Andrew", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1606.06127", "submitter": "Mitko Veta", "authors": "Mitko Veta, Paul J. van Diest and Josien P.W. Pluim", "title": "Cutting out the middleman: measuring nuclear area in histopathology\n  slides without segmentation", "comments": "Conditionally accepted for MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The size of nuclei in histological preparations from excised breast tumors is\npredictive of patient outcome (large nuclei indicate poor outcome).\nPathologists take into account nuclear size when performing breast cancer\ngrading. In addition, the mean nuclear area (MNA) has been shown to have\nindependent prognostic value. The straightforward approach to measuring nuclear\nsize is by performing nuclei segmentation. We hypothesize that given an image\nof a tumor region with known nuclei locations, the area of the individual\nnuclei and region statistics such as the MNA can be reliably computed directly\nfrom the image data by employing a machine learning model, without the\nintermediate step of nuclei segmentation. Towards this goal, we train a deep\nconvolutional neural network model that is applied locally at each nucleus\nlocation, and can reliably measure the area of the individual nuclei and the\nMNA. Furthermore, we show how such an approach can be extended to perform\ncombined nuclei detection and measurement, which is reminiscent of\ngranulometry.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:10:32 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Veta", "Mitko", ""], ["van Diest", "Paul J.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1606.06135", "submitter": "Markus Rempfler", "authors": "Markus Rempfler, Bjoern Andres, Bjoern H. Menze", "title": "The Minimum Cost Connected Subgraph Problem in Medical Image Analysis", "comments": "accepted at MICCAI 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46726-9_46", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important tasks in medical image analysis can be stated in the form\nof an optimization problem whose feasible solutions are connected subgraphs.\nExamples include the reconstruction of neural or vascular structures under\nconnectedness constraints. We discuss the minimum cost connected subgraph\n(MCCS) problem and its approximations from the perspective of medical\napplications. We propose a) objective-dependent constraints and b) novel\nconstraint generation schemes to solve this optimization problem exactly by\nmeans of a branch-and-cut algorithm. These are shown to improve scalability and\nallow us to solve instances of two medical benchmark datasets to optimality for\nthe first time. This enables us to perform a quantitative comparison between\nexact and approximative algorithms, where we identify the geodesic tree\nalgorithm as an excellent alternative to exact inference on the examined\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:22:31 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rempfler", "Markus", ""], ["Andres", "Bjoern", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1606.06164", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg, Roser Morante, Desmond Elliott", "title": "Pragmatic factors in image description: the case of negations", "comments": "Accepted as a short paper for the 5th Workshop on Vision and\n  Language, collocated with ACL 2016, Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a qualitative analysis of the descriptions containing negations\n(no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of\nnegation uses. Based on this analysis, we provide a set of requirements that an\nimage description system should have in order to generate negation sentences.\nAs a pilot experiment, we used our categorization to manually annotate\nsentences containing negations in the Flickr30K corpus, with an agreement score\nof K=0.67. With this paper, we hope to open up a broader discussion of\nsubjective language in image descriptions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 15:08:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 09:06:28 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["van Miltenburg", "Emiel", ""], ["Morante", "Roser", ""], ["Elliott", "Desmond", ""]]}, {"id": "1606.06266", "submitter": "Connor Schenck", "authors": "Connor Schenck and Dieter Fox", "title": "Detection and Tracking of Liquids with Fully Convolutional Networks", "comments": "Published in the Proceedings of Robotics Science & Systems (RSS) 2016\n  Workshop Are the Skeptics Right? Limits and Potentials of Deep Learning in\n  Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in AI and robotics have claimed many incredible results with\ndeep learning, yet no work to date has applied deep learning to the problem of\nliquid perception and reasoning. In this paper, we apply fully-convolutional\ndeep neural networks to the tasks of detecting and tracking liquids. We\nevaluate three models: a single-frame network, multi-frame network, and a LSTM\nrecurrent network. Our results show that the best liquid detection results are\nachieved when aggregating data over multiple frames, in contrast to standard\nimage segmentation. They also show that the LSTM network outperforms the other\ntwo in both tasks. This suggests that LSTM-based neural networks have the\npotential to be a key component for enabling robots to handle liquids using\nrobust, closed-loop controllers.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 19:40:29 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Schenck", "Connor", ""], ["Fox", "Dieter", ""]]}, {"id": "1606.06314", "submitter": "Mohammad Haris Baig", "authors": "Mohammad Haris Baig, Lorenzo Torresani", "title": "Multiple Hypothesis Colorization", "comments": "16 Pages (including Appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on the problem of colorization for image compression.\nSince color information occupies a large proportion of the total storage size\nof an image, a method that can predict accurate color from its grayscale\nversion can produce dramatic reduction in image file size. But colorization for\ncompression poses several challenges. First, while colorization for artistic\npurposes simply involves predicting plausible chroma, colorization for\ncompression requires generating output colors that are as close as possible to\nthe ground truth. Second, many objects in the real world exhibit multiple\npossible colors. Thus, to disambiguate the colorization problem some additional\ninformation must be stored to reproduce the true colors with good accuracy. To\naccount for the multimodal color distribution of objects we propose a deep\ntree-structured network that generates multiple color hypotheses for every\npixel from a grayscale picture (as opposed to a single color produced by most\nprior colorization approaches). We show how to leverage the multimodal output\nof our model to reproduce with high fidelity the true colors of an image by\nstoring very little additional information. In the experiments we show that our\nproposed method outperforms traditional JPEG color coding by a large margin,\nproducing colors that are nearly indistinguishable from the ground truth at the\nstorage cost of just a few hundred bytes for high-resolution pictures!\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 20:15:34 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 22:48:50 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 17:02:29 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Baig", "Mohammad Haris", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1606.06329", "submitter": "Robert DiPietro", "authors": "Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S. Swaroop\n  Vedula, Gyusung I. Lee, Mija R. Lee, and Gregory D. Hager", "title": "Recognizing Surgical Activities with Recurrent Neural Networks", "comments": "Conditionally accepted at MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply recurrent neural networks to the task of recognizing surgical\nactivities from robot kinematics. Prior work in this area focuses on\nrecognizing short, low-level activities, or gestures, and has been based on\nvariants of hidden Markov models and conditional random fields. In contrast, we\nwork on recognizing both gestures and longer, higher-level activites, or\nmaneuvers, and we model the mapping from kinematics to gestures/maneuvers with\nrecurrent neural networks. To our knowledge, we are the first to apply\nrecurrent neural networks to this task. Using a single model and a single set\nof hyperparameters, we match state-of-the-art performance for gesture\nrecognition and advance state-of-the-art performance for maneuver recognition,\nin terms of both accuracy and edit distance. Code is available at\nhttps://github.com/rdipietro/miccai-2016-surgical-activity-rec .\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 20:56:47 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 14:07:56 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["DiPietro", "Robert", ""], ["Lea", "Colin", ""], ["Malpani", "Anand", ""], ["Ahmidi", "Narges", ""], ["Vedula", "S. Swaroop", ""], ["Lee", "Gyusung I.", ""], ["Lee", "Mija R.", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1606.06354", "submitter": "Alina Zare", "authors": "Alina Zare, Changzhe Jiao and Taylor Glenn", "title": "Multiple Instance Hyperspectral Target Characterization", "comments": "accepted version after revisions based on reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two methods for multiple instance target characterization,\nMI-SMF and MI-ACE, are presented. MI-SMF and MI-ACE estimate a discriminative\ntarget signature from imprecisely-labeled and mixed training data. In many\napplications, such as sub-pixel target detection in remotely-sensed\nhyperspectral imagery, accurate pixel-level labels on training data is often\nunavailable and infeasible to obtain. Furthermore, since sub-pixel targets are\nsmaller in size than the resolution of a single pixel, training data is\ncomprised only of mixed data points (in which target training points are\nmixtures of responses from both target and non-target classes). Results show\nimproved, consistent performance over existing multiple instance concept\nlearning methods on several hyperspectral sub-pixel target detection problems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:35:12 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 17:07:35 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 16:25:03 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zare", "Alina", ""], ["Jiao", "Changzhe", ""], ["Glenn", "Taylor", ""]]}, {"id": "1606.06437", "submitter": "Raghudeep Gadde", "authors": "Raghudeep Gadde and Varun Jampani and Renaud Marlet and Peter V.\n  Gehler", "title": "Efficient 2D and 3D Facade Segmentation using Auto-Context", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast and efficient segmentation technique for 2D\nimages and 3D point clouds of building facades. Facades of buildings are highly\nstructured and consequently most methods that have been proposed for this\nproblem aim to make use of this strong prior information. Contrary to most\nprior work, we are describing a system that is almost domain independent and\nconsists of standard segmentation methods. We train a sequence of boosted\ndecision trees using auto-context features. This is learned using stacked\ngeneralization. We find that this technique performs better, or comparable with\nall previous published methods and present empirical results on all available\n2D and 3D facade benchmark datasets. The proposed method is simple to\nimplement, easy to extend, and very efficient at test-time inference.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 06:50:35 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Gadde", "Raghudeep", ""], ["Jampani", "Varun", ""], ["Marlet", "Renaud", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1606.06439", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL, NEUROSPIN), Matthieu Kowalski (PARIETAL,\n  L2S), Bertrand Thirion (NEUROSPIN, PARIETAL)", "title": "Social-sparsity brain decoders: faster spatial sparsity", "comments": "in Pattern Recognition in NeuroImaging, Jun 2016, Trento, Italy. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-sparse predictors are good models for brain decoding: they give\naccurate predictions and their weight maps are interpretable as they focus on a\nsmall number of regions. However, the state of the art, based on total\nvariation or graph-net, is computationally costly. Here we introduce sparsity\nin the local neighborhood of each voxel with social-sparsity, a structured\nshrinkage operator. We find that, on brain imaging classification problems,\nsocial-sparsity performs almost as well as total-variation models and better\nthan graph-net, for a fraction of the computational cost. It also very clearly\noutlines predictive regions. We give details of the model and the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 06:51:57 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"], ["Kowalski", "Matthieu", "", "PARIETAL,\n  L2S"], ["Thirion", "Bertrand", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1606.06472", "submitter": "Linjie Xing", "authors": "Linjie Xing, Yu Qiao", "title": "DeepWriter: A Multi-Stream Deep CNN for Text-independent Writer\n  Identification", "comments": "This article will be presented at ICFHR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-independent writer identification is challenging due to the huge\nvariation of written contents and the ambiguous written styles of different\nwriters. This paper proposes DeepWriter, a deep multi-stream CNN to learn deep\npowerful representation for recognizing writers. DeepWriter takes local\nhandwritten patches as input and is trained with softmax classification loss.\nThe main contributions are: 1) we design and optimize multi-stream structure\nfor writer identification task; 2) we introduce data augmentation learning to\nenhance the performance of DeepWriter; 3) we introduce a patch scanning\nstrategy to handle text image with different lengths. In addition, we find that\ndifferent languages such as English and Chinese may share common features for\nwriter identification, and joint training can yield better performance.\nExperimental results on IAM and HWDB datasets show that our models achieve high\nidentification accuracy: 99.01% on 301 writers and 97.03% on 657 writers with\none English sentence input, 93.85% on 300 writers with one Chinese character\ninput, which outperform previous methods with a large margin. Moreover, our\nmodels obtain accuracy of 98.01% on 301 writers with only 4 English alphabets\nas input.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 08:25:25 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 03:26:58 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Xing", "Linjie", ""], ["Qiao", "Yu", ""]]}, {"id": "1606.06539", "submitter": "Xu-Yao Zhang", "authors": "Xu-Yao Zhang and Fei Yin and Yan-Ming Zhang and Cheng-Lin Liu and\n  Yoshua Bengio", "title": "Drawing and Recognizing Chinese Characters with Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based approaches have achieved great success on\nhandwriting recognition. Chinese characters are among the most widely adopted\nwriting systems in the world. Previous research has mainly focused on\nrecognizing handwritten Chinese characters. However, recognition is only one\naspect for understanding a language, another challenging and interesting task\nis to teach a machine to automatically write (pictographic) Chinese characters.\nIn this paper, we propose a framework by using the recurrent neural network\n(RNN) as both a discriminative model for recognizing Chinese characters and a\ngenerative model for drawing (generating) Chinese characters. To recognize\nChinese characters, previous methods usually adopt the convolutional neural\nnetwork (CNN) models which require transforming the online handwriting\ntrajectory into image-like representations. Instead, our RNN based approach is\nan end-to-end system which directly deals with the sequential structure and\ndoes not require any domain-specific knowledge. With the RNN system (combining\nan LSTM and GRU), state-of-the-art performance can be achieved on the\nICDAR-2013 competition database. Furthermore, under the RNN framework, a\nconditional generative model with character embedding is proposed for\nautomatically drawing recognizable Chinese characters. The generated characters\n(in vector format) are human-readable and also can be recognized by the\ndiscriminative RNN model with high accuracy. Experimental results verify the\neffectiveness of using RNNs as both generative and discriminative models for\nthe tasks of drawing and recognizing Chinese characters.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 12:35:31 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zhang", "Xu-Yao", ""], ["Yin", "Fei", ""], ["Zhang", "Yan-Ming", ""], ["Liu", "Cheng-Lin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1606.06582", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Kibok Lee, Honglak Lee", "title": "Augmenting Supervised Neural Networks with Unsupervised Objectives for\n  Large-scale Image Classification", "comments": "International Conference on Machine Learning (ICML), 2016", "journal-ref": "PMLR 48:612-621, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning and supervised learning are key research topics in deep\nlearning. However, as high-capacity supervised neural networks trained with a\nlarge amount of labels have achieved remarkable success in many computer vision\ntasks, the availability of large-scale labeled images reduced the significance\nof unsupervised learning. Inspired by the recent trend toward revisiting the\nimportance of unsupervised learning, we investigate joint supervised and\nunsupervised learning in a large-scale setting by augmenting existing neural\nnetworks with decoding pathways for reconstruction. First, we demonstrate that\nthe intermediate activations of pretrained large-scale classification networks\npreserve almost all the information of input images except a portion of local\nspatial details. Then, by end-to-end training of the entire augmented\narchitecture with the reconstructive objective, we show improvement of the\nnetwork performance for supervised tasks. We evaluate several variants of\nautoencoders, including the recently proposed \"what-where\" autoencoder that\nuses the encoder pooling switches, to study the importance of the architecture\ndesign. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012\nprotocol as a strong baseline for image classification, our methods improve the\nvalidation-set accuracy by a noticeable margin.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 14:12:52 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Zhang", "Yuting", ""], ["Lee", "Kibok", ""], ["Lee", "Honglak", ""]]}, {"id": "1606.06622", "submitter": "Gordon Christie", "authors": "Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh", "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise\n  Questions", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is the task of answering natural-language\nquestions about images. We introduce the novel problem of determining the\nrelevance of questions to images in VQA. Current VQA models do not reason about\nwhether a question is even related to the given image (e.g. What is the capital\nof Argentina?) or if it requires information from external resources to answer\ncorrectly. This can break the continuity of a dialogue in human-machine\ninteraction. Our approaches for determining relevance are composed of two\nstages. Given an image and a question, (1) we first determine whether the\nquestion is visual or not, (2) if visual, we determine whether the question is\nrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA\nmodel uncertainty, and caption-question similarity, are able to outperform\nstrong baselines on both relevance tasks. We also present human studies showing\nthat VQA models augmented with such question relevance reasoning are perceived\nas more intelligent, reasonable, and human-like.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:38:27 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 02:56:00 GMT"}, {"version": "v3", "created": "Mon, 26 Sep 2016 15:24:28 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Ray", "Arijit", ""], ["Christie", "Gordon", ""], ["Bansal", "Mohit", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.06650", "submitter": "\\\"Ozg\\\"un \\c{C}i\\c{c}ek", "authors": "\\\"Ozg\\\"un \\c{C}i\\c{c}ek, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas\n  Brox, and Olaf Ronneberger", "title": "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation", "comments": "Conditionally accepted for MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a network for volumetric segmentation that learns from\nsparsely annotated volumetric images. We outline two attractive use cases of\nthis method: (1) In a semi-automated setup, the user annotates some slices in\nthe volume to be segmented. The network learns from these sparse annotations\nand provides a dense 3D segmentation. (2) In a fully-automated setup, we assume\nthat a representative, sparsely annotated training set exists. Trained on this\ndata set, the network densely segments new volumetric images. The proposed\nnetwork extends the previous u-net architecture from Ronneberger et al. by\nreplacing all 2D operations with their 3D counterparts. The implementation\nperforms on-the-fly elastic deformations for efficient data augmentation during\ntraining. It is trained end-to-end from scratch, i.e., no pre-trained network\nis required. We test the performance of the proposed method on a complex,\nhighly variable 3D structure, the Xenopus kidney, and achieve good results for\nboth use cases.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 16:42:20 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["\u00c7i\u00e7ek", "\u00d6zg\u00fcn", ""], ["Abdulkadir", "Ahmed", ""], ["Lienkamp", "Soeren S.", ""], ["Brox", "Thomas", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1606.06681", "submitter": "Humayun Irshad Dr.", "authors": "Humayun Irshad, Eun-Yeong Oh, Daniel Schmolze, Liza M Quintana, Laura\n  Collins, Rulla M. Tamimi, Andrew H. Beck", "title": "Crowdsourcing scoring of immunohistochemistry images: Evaluating\n  Performance of the Crowd and an Automated Computational Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The assessment of protein expression in immunohistochemistry (IHC) images\nprovides important diagnostic, prognostic and predictive information for\nguiding cancer diagnosis and therapy. Manual scoring of IHC images represents a\nlogistical challenge, as the process is labor intensive and time consuming.\nSince the last decade, computational methods have been developed to enable the\napplication of quantitative methods for the analysis and interpretation of\nprotein expression in IHC images. These methods have not yet replaced manual\nscoring for the assessment of IHC in the majority of diagnostic laboratories\nand in many large-scale research studies. An alternative approach is\ncrowdsourcing the quantification of IHC images to an undefined crowd. The aim\nof this study is to quantify IHC images for labeling of ER status with two\ndifferent crowdsourcing approaches, image labeling and nuclei labeling, and\ncompare their performance with automated methods. Crowdsourcing-derived scores\nobtained greater concordance with the pathologist interpretations for both\nimage labeling and nuclei labeling tasks (83% and 87%), as compared to the\npathologist concordance achieved by the automated method (81%) on 5,483 TMA\nimages from 1,909 breast cancer patients. This analysis shows that\ncrowdsourcing the scoring of protein expression in IHC images is a promising\nnew approach for large scale cancer molecular pathology studies.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 17:50:38 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 20:15:30 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Irshad", "Humayun", ""], ["Oh", "Eun-Yeong", ""], ["Schmolze", "Daniel", ""], ["Quintana", "Liza M", ""], ["Collins", "Laura", ""], ["Tamimi", "Rulla M.", ""], ["Beck", "Andrew H.", ""]]}, {"id": "1606.06724", "submitter": "Klaus Greff", "authors": "Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, J\\\"urgen\n  Schmidhuber, Harri Valpola", "title": "Tagger: Deep Unsupervised Perceptual Grouping", "comments": "14 pages + 5 pages supplementary, accepted at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for efficient perceptual inference that explicitly\nreasons about the segmentation of its inputs and features. Rather than being\ntrained for any specific segmentation, our framework learns the grouping\nprocess in an unsupervised manner or alongside any supervised task. By\nenriching the representations of a neural network, we enable it to group the\nrepresentations of different objects in an iterative manner. By allowing the\nsystem to amortize the iterative inference of the groupings, we achieve very\nfast convergence. In contrast to many other recently proposed methods for\naddressing multi-object scenes, our system does not assume the inputs to be\nimages and can therefore directly handle other modalities. For multi-digit\nclassification of very cluttered images that require texture segmentation, our\nmethod offers improved classification performance over convolutional networks\ndespite being fully connected. Furthermore, we observe that our system greatly\nimproves on the semi-supervised result of a baseline Ladder network on our\ndataset, indicating that segmentation can also improve sample efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 19:55:32 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 18:59:28 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Greff", "Klaus", ""], ["Rasmus", "Antti", ""], ["Berglund", "Mathias", ""], ["Hao", "Tele Hotloo", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Valpola", "Harri", ""]]}, {"id": "1606.06811", "submitter": "Jiewei Cao", "authors": "Jiewei Cao, Lingqiao Liu, Peng Wang, Zi Huang, Chunhua Shen, and Heng\n  Tao Shen", "title": "Where to Focus: Query Adaptive Matching for Instance Retrieval Using\n  Convolutional Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance retrieval requires one to search for images that contain a\nparticular object within a large corpus. Recent studies show that using image\nfeatures generated by pooling convolutional layer feature maps (CFMs) of a\npretrained convolutional neural network (CNN) leads to promising performance\nfor this task. However, due to the global pooling strategy adopted in those\nworks, the generated image feature is less robust to image clutter and tends to\nbe contaminated by the irrelevant image patterns. In this article, we alleviate\nthis drawback by proposing a novel reranking algorithm using CFMs to refine the\nretrieval result obtained by existing methods. Our key idea, called query\nadaptive matching (QAM), is to first represent the CFMs of each image by a set\nof base regions which can be freely combined into larger regions-of-interest.\nThen the similarity between the query and a candidate image is measured by the\nbest similarity score that can be attained by comparing the query feature and\nthe feature pooled from a combined region. We show that the above procedure can\nbe cast as an optimization problem and it can be solved efficiently with an\noff-the-shelf solver. Besides this general framework, we also propose two\npractical ways to create the base regions. One is based on the property of the\nCFM and the other one is based on a multi-scale spatial pyramid scheme. Through\nextensive experiments, we show that our reranking approaches bring substantial\nperformance improvement and by applying them we can outperform the state of the\nart on several instance retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 03:33:25 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Cao", "Jiewei", ""], ["Liu", "Lingqiao", ""], ["Wang", "Peng", ""], ["Huang", "Zi", ""], ["Shen", "Chunhua", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1606.06854", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Qingfu Wan, Wei Zhang, Xiangyang Xue, Yichen Wei", "title": "Model-based Deep Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous learning based hand pose estimation methods does not fully exploit\nthe prior information in hand model geometry. Instead, they usually rely a\nseparate model fitting step to generate valid hand poses. Such a post\nprocessing is inconvenient and sub-optimal. In this work, we propose a model\nbased deep learning approach that adopts a forward kinematics based layer to\nensure the geometric validity of estimated poses. For the first time, we show\nthat embedding such a non-linear generative process in deep learning is\nfeasible for hand pose estimation. Our approach is verified on challenging\npublic datasets and achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 08:47:06 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Zhou", "Xingyi", ""], ["Wan", "Qingfu", ""], ["Zhang", "Wei", ""], ["Xue", "Xiangyang", ""], ["Wei", "Yichen", ""]]}, {"id": "1606.07015", "submitter": "Alexander Kirillov", "authors": "Alexander Kirillov, Alexander Shekhovtsov, Carsten Rother, Bogdan\n  Savchynskyy", "title": "Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of jointly inferring the M-best diverse labelings for\na binary (high-order) submodular energy of a graphical model. Recently, it was\nshown that this problem can be solved to a global optimum, for many practically\ninteresting diversity measures. It was noted that the labelings are, so-called,\nnested. This nestedness property also holds for labelings of a class of\nparametric submodular minimization problems, where different values of the\nglobal parameter $\\gamma$ give rise to different solutions. The popular example\nof the parametric submodular minimization is the monotonic parametric max-flow\nproblem, which is also widely used for computing multiple labelings. As the\nmain contribution of this work we establish a close relationship between\ndiversity with submodular energies and the parametric submodular minimization.\nIn particular, the joint M-best diverse labelings can be obtained by running a\nnon-parametric submodular minimization (in the special case - max-flow) solver\nfor M different values of $\\gamma$ in parallel, for certain diversity measures.\nImportantly, the values for $\\gamma$ can be computed in a closed form in\nadvance, prior to any optimization. These theoretical results suggest two\nsimple yet efficient algorithms for the joint M-best diverse problem, which\noutperform competitors in terms of runtime and quality of results. In\nparticular, as we show in the paper, the new methods compute the exact M-best\ndiverse labelings faster than a popular method of Batra et al., which in some\nsense only obtains approximate solutions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 17:26:17 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 08:10:46 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Kirillov", "Alexander", ""], ["Shekhovtsov", "Alexander", ""], ["Rother", "Carsten", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "1606.07166", "submitter": "Hyoseok Hwang", "authors": "Hyoseok Hwang, Hyun Sung Chang, Dongkyung Nam, In So Kweon", "title": "3D Display Calibration by Visual Pattern Analysis", "comments": "13 pages, 10 figures.submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2017.2665043", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all 3D displays need calibration for correct rendering. More often\nthan not, the optical elements in a 3D display are misaligned from the designed\nparameter setting. As a result, 3D magic does not perform well as intended. The\nobserved images tend to get distorted. In this paper, we propose a novel\ndisplay calibration method to fix the situation. In our method, a pattern image\nis displayed on the panel and a camera takes its pictures twice at different\npositions. Then, based on a quantitative model, we extract all display\nparameters (i.e., pitch, slanted angle, gap or thickness, offset) from the\nobserved patterns in the captured images. For high accuracy and robustness, our\nmethod analyzes the patterns mostly in frequency domain. We conduct two types\nof experiments for validation; one with optical simulation for quantitative\nresults and the other with real-life displays for qualitative assessment.\nExperimental results demonstrate that our method is quite accurate, about a\nhalf order of magnitude higher than prior work; is efficient, spending less\nthan 2 s for computation; and is robust to noise, working well in the SNR\nregime as low as 6 dB.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 02:20:20 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Hwang", "Hyoseok", ""], ["Chang", "Hyun Sung", ""], ["Nam", "Dongkyung", ""], ["Kweon", "In So", ""]]}, {"id": "1606.07230", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Deep Learning Markov Random Field for Semantic Segmentation", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), 2017. Extended version of our previous ICCV 2015 paper\n  (arXiv:1509.02634)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation tasks can be well modeled by Markov Random Field (MRF).\nThis paper addresses semantic segmentation by incorporating high-order\nrelations and mixture of label contexts into MRF. Unlike previous works that\noptimized MRFs using iterative algorithm, we solve MRF by proposing a\nConvolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which\nenables deterministic end-to-end computation in a single forward pass.\nSpecifically, DPN extends a contemporary CNN to model unary terms and\nadditional layers are devised to approximate the mean field (MF) algorithm for\npairwise terms. It has several appealing properties. First, different from the\nrecent works that required many iterations of MF during back-propagation, DPN\nis able to achieve high performance by approximating one iteration of MF.\nSecond, DPN represents various types of pairwise terms, making many existing\nmodels as its special cases. Furthermore, pairwise terms in DPN provide a\nunified framework to encode rich contextual information in high-dimensional\ndata, such as images and videos. Third, DPN makes MF easier to be parallelized\nand speeded up, thus enabling efficient inference. DPN is thoroughly evaluated\non standard semantic image/video segmentation benchmarks, where a single DPN\nmodel yields state-of-the-art segmentation accuracies on PASCAL VOC 2012,\nCityscapes dataset and CamVid dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 08:52:39 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 09:24:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Liu", "Ziwei", ""], ["Li", "Xiaoxiao", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1606.07239", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean, Pierrick Coup\\'e and Maxime Descoteaux", "title": "Non Local Spatial and Angular Matching : Enabling higher spatial\n  resolution diffusion MRI datasets through adaptive denoising", "comments": "Code available : https://github.com/samuelstjean/nlsam Datasets\n  available : https://github.com/samuelstjean/nlsam_data, Medical Image\n  Analysis, 2016", "journal-ref": "Medical Image Analysis , Volume 32 , 115 - 130, 2016", "doi": "10.1016/j.media.2016.02.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging datasets suffer from low Signal-to-Noise\nRatio, especially at high b-values. Acquiring data at high b-values contains\nrelevant information and is now of great interest for microstructural and\nconnectomics studies. High noise levels bias the measurements due to the\nnon-Gaussian nature of the noise, which in turn can lead to a false and biased\nestimation of the diffusion parameters. Additionally, the usage of in-plane\nacceleration techniques during the acquisition leads to a spatially varying\nnoise distribution, which depends on the parallel acceleration method\nimplemented on the scanner. This paper proposes a novel diffusion MRI denoising\ntechnique that can be used on all existing data, without adding to the scanning\ntime. We first apply a statistical framework to convert the noise to Gaussian\ndistributed noise, effectively removing the bias. We then introduce a spatially\nand angular adaptive denoising technique, the Non Local Spatial and Angular\nMatching (NLSAM) algorithm. Each volume is first decomposed in small 4D\noverlapping patches to capture the structure of the diffusion data and a\ndictionary of atoms is learned on those patches. A local sparse decomposition\nis then found by bounding the reconstruction error with the local noise\nvariance. We compare against three other state-of-the-art denoising methods and\nshow quantitative local and connectivity results on a synthetic phantom and on\nan in-vivo high resolution dataset. Overall, our method restores perceptual\ninformation, removes the noise bias in common diffusion metrics, restores the\nextracted peaks coherence and improves reproducibility of tractography. Our\nwork paves the way for higher spatial resolution acquisition of diffusion MRI\ndatasets, which could in turn reveal new anatomical details that are not\ndiscernible at the spatial resolution currently used by the diffusion MRI\ncommunity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 09:28:29 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["St-Jean", "Samuel", ""], ["Coup\u00e9", "Pierrick", ""], ["Descoteaux", "Maxime", ""]]}, {"id": "1606.07247", "submitter": "Sayem Mohammad Siam", "authors": "Sayem Mohammad Siam, Jahidul Adnan Sakel, and Md. Hasanul Kabir", "title": "Human Computer Interaction Using Marker Based Hand Gesture Recognition", "comments": "8 Pages, didn't submit to any conference yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Computer Interaction (HCI) has been redefined in this era. People want\nto interact with their devices in such a way that has physical significance in\nthe real world, in other words, they want ergonomic input devices. In this\npaper, we propose a new method of interaction with computing devices having a\nconsumer grade camera, that uses two colored markers (red and green) worn on\ntips of the fingers to generate desired hand gestures, and for marker detection\nand tracking we used template matching with kalman filter. We have implemented\nall the usual system commands, i.e., cursor movement, right click, left click,\ndouble click, going forward and backward, zoom in and out through different\nhand gestures. Our system can easily recognize these gestures and give\ncorresponding system commands. Our system is suitable for both desktop devices\nand devices where touch screen is not feasible like large screens or projected\nscreens.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 09:49:15 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Siam", "Sayem Mohammad", ""], ["Sakel", "Jahidul Adnan", ""], ["Kabir", "Md. Hasanul", ""]]}, {"id": "1606.07253", "submitter": "Liuhao Ge", "authors": "Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann", "title": "Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View\n  CNN to Multi-View CNNs", "comments": "9 pages, 9 figures, published at Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulated hand pose estimation plays an important role in human-computer\ninteraction. Despite the recent progress, the accuracy of existing methods is\nstill not satisfactory, partially due to the difficulty of embedded\nhigh-dimensional and non-linear regression problem. Different from the existing\ndiscriminative methods that regress for the hand pose with a single depth\nimage, we propose to first project the query depth image onto three orthogonal\nplanes and utilize these multi-view projections to regress for 2D heat-maps\nwhich estimate the joint positions on each plane. These multi-view heat-maps\nare then fused to produce final 3D hand pose estimation with learned pose\npriors. Experiments show that the proposed method largely outperforms\nstate-of-the-art on a challenging dataset. Moreover, a cross-dataset experiment\nalso demonstrates the good generalization ability of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:00:03 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 09:15:42 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 14:22:54 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Ge", "Liuhao", ""], ["Liang", "Hui", ""], ["Yuan", "Junsong", ""], ["Thalmann", "Daniel", ""]]}, {"id": "1606.07256", "submitter": "Jenny Benois-Pineau", "authors": "Philippe P\\'erez de San Roman, Jenny Benois-Pineau, Jean-Philippe\n  Domenger, Florent Paclet, Daniel Cataert, Aymar de Rugy", "title": "Saliency Driven Object recognition in egocentric videos with deep CNN", "comments": "20 pages, 8 figures, 3 tables, Submitted to the Journal of Computer\n  Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of object recognition in natural scenes has been recently\nsuccessfully addressed with Deep Convolutional Neuronal Networks giving a\nsignificant break-through in recognition scores. The computational efficiency\nof Deep CNNs as a function of their depth, allows for their use in real-time\napplications. One of the key issues here is to reduce the number of windows\nselected from images to be submitted to a Deep CNN. This is usually solved by\npreliminary segmentation and selection of specific windows, having outstanding\n\"objectiveness\" or other value of indicators of possible location of objects.\nIn this paper we propose a Deep CNN approach and the general framework for\nrecognition of objects in a real-time scenario and in an egocentric\nperspective. Here the window of interest is built on the basis of visual\nattention map computed over gaze fixations measured by a glass-worn\neye-tracker. The application of this set-up is an interactive user-friendly\nenvironment for upper-limb amputees. Vision has to help the subject to control\nhis worn neuro-prosthesis in case of a small amount of remaining muscles when\nthe EMG control becomes unefficient. The recognition results on a specifically\nrecorded corpus of 151 videos with simple geometrical objects show the mAP of\n64,6\\% and the computational time at the generalization lower than a time of a\nvisual fixation on the object-of-interest.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:10:31 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Roman", "Philippe P\u00e9rez de San", ""], ["Benois-Pineau", "Jenny", ""], ["Domenger", "Jean-Philippe", ""], ["Paclet", "Florent", ""], ["Cataert", "Daniel", ""], ["de Rugy", "Aymar", ""]]}, {"id": "1606.07285", "submitter": "Wojciech Samek", "authors": "Farhad Arbabzadah and Gr\\'egoire Montavon and Klaus-Robert M\\\"uller\n  and Wojciech Samek", "title": "Identifying individual facial expressions by deconstructing a neural\n  network", "comments": "12 pages, 7 figures, Paper accepted for GCPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of explaining predictions of psychological\nattributes such as attractiveness, happiness, confidence and intelligence from\nface photographs using deep neural networks. Since psychological attribute\ndatasets typically suffer from small sample sizes, we apply transfer learning\nwith two base models to avoid overfitting. These models were trained on an age\nand gender prediction task, respectively. Using a novel explanation method we\nextract heatmaps that highlight the parts of the image most responsible for the\nprediction. We further observe that the explanation method provides important\ninsights into the nature of features of the base model, which allow one to\nassess the aptitude of the base model for a given transfer learning task.\nFinally, we observe that the multiclass model is more feature rich than its\nbinary counterpart. The experimental evaluation is performed on the 2222 images\nfrom the 10k US faces dataset containing psychological attribute labels as well\nas on a subset of KDEF images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:24:45 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 00:41:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Arbabzadah", "Farhad", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1606.07287", "submitter": "Andrea Esuli", "authors": "Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio Falchi, Alejandro\n  Moreo Fern\\'andez", "title": "Picture It In Your Mind: Generating High Level Visual Representations\n  From Textual Descriptions", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": "10.1007/s10791-017-9318-6", "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of image search when the query is a short\ntextual description of the image the user is looking for. We choose to\nimplement the actual search process as a similarity search in a visual feature\nspace, by learning to translate a textual query into a visual representation.\nSearching in the visual feature space has the advantage that any update to the\ntranslation model does not require to reprocess the, typically huge, image\ncollection on which the search is performed. We propose Text2Vis, a neural\nnetwork that generates a visual representation, in the visual feature space of\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\noptimizes two loss functions, using a stochastic loss-selection method. A\nvisual-focused loss is aimed at learning the actual text-to-visual feature\nmapping, while a text-focused loss is aimed at modeling the higher-level\nsemantic concepts expressed in language and countering the overfit on\nnon-relevant visual components of the visual loss. We report preliminary\nresults on the MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:25:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Carrara", "Fabio", ""], ["Esuli", "Andrea", ""], ["Fagni", "Tiziano", ""], ["Falchi", "Fabrizio", ""], ["Fern\u00e1ndez", "Alejandro Moreo", ""]]}, {"id": "1606.07324", "submitter": "Aniello Raffaele Patrone", "authors": "Aniello Raffaele Patrone and Christian Valuch and Ulrich Ansorge and\n  Otmar Scherzer", "title": "Dynamical optical flow of saliency maps for predicting visual attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps are used to understand human attention and visual fixation.\nHowever, while very well established for static images, there is no general\nagreement on how to compute a saliency map of dynamic scenes. In this paper we\npropose a mathematically rigorous approach to this prob- lem, including static\nsaliency maps of each video frame for the calculation of the optical flow.\nTaking into account static saliency maps for calculating the optical flow\nallows for overcoming the aperture problem. Our ap- proach is able to explain\nhuman fixation behavior in situations which pose challenges to standard\napproaches, such as when a fixated object disappears behind an occlusion and\nreappears after several frames. In addition, we quantitatively compare our\nmodel against alternative solutions using a large eye tracking data set.\nTogether, our results suggest that assessing optical flow information across a\nseries of saliency maps gives a highly accurate and useful account of human\novert attention in dynamic scenes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 14:29:43 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Patrone", "Aniello Raffaele", ""], ["Valuch", "Christian", ""], ["Ansorge", "Ulrich", ""], ["Scherzer", "Otmar", ""]]}, {"id": "1606.07326", "submitter": "Wei Pan", "authors": "Wei Pan and Hao Dong and Yike Guo", "title": "DropNeuron: Simplifying the Structure of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning using multi-layer neural networks (NNs) architecture manifests\nsuperb power in modern machine learning systems. The trained Deep Neural\nNetworks (DNNs) are typically large. The question we would like to address is\nwhether it is possible to simplify the NN during training process to achieve a\nreasonable performance within an acceptable computational time. We presented a\nnovel approach of optimising a deep neural network through regularisation of\nnet- work architecture. We proposed regularisers which support a simple\nmechanism of dropping neurons during a network training process. The method\nsupports the construction of a simpler deep neural networks with compatible\nperformance with its simplified version. As a proof of concept, we evaluate the\nproposed method with examples including sparse linear regression, deep\nautoencoder and convolutional neural network. The valuations demonstrate\nexcellent performance.\n  The code for this work can be found in\nhttp://www.github.com/panweihit/DropNeuron\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 14:30:36 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 12:50:27 GMT"}, {"version": "v3", "created": "Sun, 3 Jul 2016 09:39:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Pan", "Wei", ""], ["Dong", "Hao", ""], ["Guo", "Yike", ""]]}, {"id": "1606.07356", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh", "title": "Analyzing the Behavior of Visual Question Answering Models", "comments": "13 pages, 20 figures; To appear in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a number of deep-learning based models have been proposed for the\ntask of Visual Question Answering (VQA). The performance of most models is\nclustered around 60-70%. In this paper we propose systematic methods to analyze\nthe behavior of these models as a first step towards recognizing their\nstrengths and weaknesses, and identifying the most fruitful directions for\nprogress. We analyze two models, one each from two major classes of VQA models\n-- with-attention and without-attention and show the similarities and\ndifferences in the behavior of these models. We also analyze the winning entry\nof the VQA Challenge 2016.\n  Our behavior analysis reveals that despite recent progress, today's VQA\nmodels are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump\nto conclusions\" (converge on a predicted answer after 'listening' to just half\nthe question), and are \"stubborn\" (do not change their answers across images).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:05:16 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 19:56:22 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.07372", "submitter": "Noah Apthorpe", "authors": "Noah J. Apthorpe, Alexander J. Riordan, Rob E. Aguilar, Jan Homann, Yi\n  Gu, David W. Tank, H. Sebastian Seung", "title": "Automatic Neuron Detection in Calcium Imaging Data Using Convolutional\n  Networks", "comments": "9 pages, 5 figures, 2 ancillary files; minor changes for camera-ready\n  version. appears in Advances in Neural Information Processing Systems 29\n  (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging is an important technique for monitoring the activity of\nthousands of neurons simultaneously. As calcium imaging datasets grow in size,\nautomated detection of individual neurons is becoming important. Here we apply\na supervised learning approach to this problem and show that convolutional\nnetworks can achieve near-human accuracy and superhuman speed. Accuracy is\nsuperior to the popular PCA/ICA method based on precision and recall relative\nto ground truth annotation by a human expert. These results suggest that\nconvolutional networks are an efficient and flexible tool for the analysis of\nlarge-scale calcium imaging data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:49:40 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 23:40:08 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Apthorpe", "Noah J.", ""], ["Riordan", "Alexander J.", ""], ["Aguilar", "Rob E.", ""], ["Homann", "Jan", ""], ["Gu", "Yi", ""], ["Tank", "David W.", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1606.07373", "submitter": "Maksim Bolonkin", "authors": "Du Tran, Maksim Bolonkin, Manohar Paluri, Lorenzo Torresani", "title": "VideoMCC: a New Benchmark for Video Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there is overall agreement that future technology for organizing,\nbrowsing and searching videos hinges on the development of methods for\nhigh-level semantic understanding of video, so far no consensus has been\nreached on the best way to train and assess models for this task. Casting video\nunderstanding as a form of action or event categorization is problematic as it\nis not fully clear what the semantic classes or abstractions in this domain\nshould be. Language has been exploited to sidestep the problem of defining\nvideo categories, by formulating video understanding as the task of captioning\nor description. However, language is highly complex, redundant and sometimes\nambiguous. Many different captions may express the same semantic concept. To\naccount for this ambiguity, quantitative evaluation of video description\nrequires sophisticated metrics, whose performance scores are typically hard to\ninterpret by humans.\n  This paper provides four contributions to this problem. First, we formulate\nVideo Multiple Choice Caption (VideoMCC) as a new well-defined task with an\neasy-to-interpret performance measure. Second, we describe a general\nsemi-automatic procedure to create benchmarks for this task. Third, we publicly\nrelease a large-scale video benchmark created with an implementation of this\nprocedure and we include a human study that assesses human performance on our\ndataset. Finally, we propose and test a varied collection of approaches on this\nbenchmark for the purpose of gaining a better understanding of the new\nchallenges posed by video comprehension.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:53:22 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 19:49:57 GMT"}, {"version": "v3", "created": "Fri, 31 Mar 2017 17:50:47 GMT"}, {"version": "v4", "created": "Fri, 14 Apr 2017 17:30:12 GMT"}, {"version": "v5", "created": "Fri, 16 Jun 2017 19:50:46 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Tran", "Du", ""], ["Bolonkin", "Maksim", ""], ["Paluri", "Manohar", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1606.07396", "submitter": "Hossein Talebi", "authors": "Hossein Talebi, Peyman Milanfar", "title": "Fast Multi-Layer Laplacian Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel, fast and practical way of enhancing images is introduced in this\npaper. Our approach builds on Laplacian operators of well-known edge-aware\nkernels, such as bilateral and nonlocal means, and extends these filter's\ncapabilities to perform more effective and fast image smoothing, sharpening and\ntone manipulation. We propose an approximation of the Laplacian, which does not\nrequire normalization of the kernel weights. Multiple Laplacians of the\naffinity weights endow our method with progressive detail decomposition of the\ninput image from fine to coarse scale. These image components are blended by a\nstructure mask, which avoids noise/artifact magnification or detail loss in the\noutput image. Contributions of the proposed method to existing image editing\ntools are: (1) Low computational and memory requirements, making it appropriate\nfor mobile device implementations (e.g. as a finish step in a camera pipeline),\n(2) A range of filtering applications from detail enhancement to denoising with\nonly a few control parameters, enabling the user to apply a combination of\nvarious (and even opposite) filtering effects.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 18:30:05 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Talebi", "Hossein", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1606.07414", "submitter": "Renato J Cintra", "authors": "T. L. T. Silveira, R. S. Oliveira, F. M. Bayer, R. J. Cintra, A.\n  Madanayake", "title": "Multiplierless 16-point DCT Approximation for Low-complexity Image and\n  Video Coding", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": null, "doi": "10.1007/s11760-016-0923-4", "report-no": null, "categories": "cs.CV cs.MM cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal 16-point approximate discrete cosine transform (DCT) is\nintroduced. The proposed transform requires neither multiplications nor\nbit-shifting operations. A fast algorithm based on matrix factorization is\nintroduced, requiring only 44 additions---the lowest arithmetic cost in\nliterature. To assess the introduced transform, computational complexity,\nsimilarity with the exact DCT, and coding performance measures are computed.\nClassical and state-of-the-art 16-point low-complexity transforms were used in\na comparative analysis. In the context of image compression, the proposed\napproximation was evaluated via PSNR and SSIM measurements, attaining the best\ncost-benefit ratio among the competitors. For video encoding, the proposed\napproximation was embedded into a HEVC reference software for direct comparison\nwith the original HEVC standard. Physically realized and tested using FPGA\nhardware, the proposed transform showed 35% and 37% improvements of area-time\nand area-time-squared VLSI metrics when compared to the best competing\ntransform in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:26:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Silveira", "T. L. T.", ""], ["Oliveira", "R. S.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Madanayake", "A.", ""]]}, {"id": "1606.07415", "submitter": "Wei-Chiu Ma", "authors": "Wei-Chiu Ma, Shenlong Wang, Marcus A. Brubaker, Sanja Fidler, Raquel\n  Urtasun", "title": "Find your Way by Observing the Sun and Other Semantic Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a robust, efficient and affordable approach to\nself-localization which does not require neither GPS nor knowledge about the\nappearance of the world. Towards this goal, we utilize freely available\ncartographic maps and derive a probabilistic model that exploits semantic cues\nin the form of sun direction, presence of an intersection, road type, speed\nlimit as well as the ego-car trajectory in order to produce very reliable\nlocalization results. Our experimental evaluation shows that our approach can\nlocalize much faster (in terms of driving time) with less computation and more\nrobustly than competing approaches, which ignore semantic information.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:28:22 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Ma", "Wei-Chiu", ""], ["Wang", "Shenlong", ""], ["Brubaker", "Marcus A.", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1606.07419", "submitter": "Pulkit Agrawal", "authors": "Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, Sergey\n  Levine", "title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics", "comments": null, "journal-ref": "NIPS 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an experiential learning paradigm for acquiring an internal\nmodel of intuitive physics. Our model is evaluated on a real-world robotic\nmanipulation task that requires displacing objects to target locations by\npoking. The robot gathered over 400 hours of experience by executing more than\n100K pokes on different objects. We propose a novel approach based on deep\nneural networks for modeling the dynamics of robot's interactions directly from\nimages, by jointly estimating forward and inverse models of dynamics. The\ninverse model objective provides supervision to construct informative visual\nfeatures, which the forward model can then predict and in turn regularize the\nfeature space for the inverse model. The interplay between these two objectives\ncreates useful, accurate models that can then be used for multi-step decision\nmaking. This formulation has the additional benefit that it is possible to\nlearn forward models in an abstract feature space and thus alleviate the need\nof predicting pixels. Our experiments show that this joint modeling approach\noutperforms alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 19:42:57 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 22:53:52 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Agrawal", "Pulkit", ""], ["Nair", "Ashvin", ""], ["Abbeel", "Pieter", ""], ["Malik", "Jitendra", ""], ["Levine", "Sergey", ""]]}, {"id": "1606.07493", "submitter": "Arjun Chandrasekaran", "authors": "Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, Mohit\n  Bansal", "title": "Sort Story: Sorting Jumbled Images and Captions into Stories", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:54:44 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 05:26:43 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 19:56:36 GMT"}, {"version": "v4", "created": "Sat, 24 Sep 2016 00:37:27 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 18:48:13 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Agrawal", "Harsh", ""], ["Chandrasekaran", "Arjun", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Bansal", "Mohit", ""]]}, {"id": "1606.07496", "submitter": "Roberto Camacho Barranco", "authors": "Roberto Camacho Barranco (1), Laura M. Rodriguez (1), Rebecca Urbina\n  (1), and M. Shahriar Hossain (1) ((1) The University of Texas at El Paso)", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "comments": "10 pages, 11 figures, \"for associated results, see\n  http://http://auto-captioning.herokuapp.com/\" \"submitted to DLRS 2016\n  workshop\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 22:04:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Barranco", "Roberto Camacho", "", "The University of Texas at El Paso"], ["Rodriguez", "Laura M.", "", "The University of Texas at El Paso"], ["Urbina", "Rebecca", "", "The University of Texas at El Paso"], ["Hossain", "M. Shahriar", "", "The University of Texas at El Paso"]]}, {"id": "1606.07509", "submitter": "Fitsum Mesadi", "authors": "Fitsum Mesadi and Tolga Tasdizen", "title": "Convex Decomposition And Efficient Shape Representation Using Deformable\n  Convex Polytopes", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposition of shapes into (approximate) convex parts is essential for\napplications such as part-based shape representation, shape matching, and\ncollision detection. In this paper, we propose a novel convex decomposition\nusing a parametric implicit shape model called Disjunctive Normal Shape Model\n(DNSM). The DNSM is formed as a union of polytopes which themselves are formed\nby intersections of halfspaces. The key idea is by deforming the polytopes,\nwhich naturally remain convex during the evolution, the polytopes capture\nconvex parts without the need to compute convexity. The major contributions of\nthis paper include a robust convex decomposition which also results in an\nefficient part-based shape representation, and a novel shape convexity measure.\nThe experimental results show the potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 23:53:09 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Mesadi", "Fitsum", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1606.07511", "submitter": "Fitsum Mesadi", "authors": "Fitsum Mesadi, Mujdat Cetin, Tolga Tasdizen", "title": "Disjunctive Normal Level Set: An Efficient Parametric Implicit Method", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Level set methods are widely used for image segmentation because of their\ncapability to handle topological changes. In this paper, we propose a novel\nparametric level set method called Disjunctive Normal Level Set (DNLS), and\napply it to both two phase (single object) and multiphase (multi-object) image\nsegmentations. The DNLS is formed by union of polytopes which themselves are\nformed by intersections of half-spaces. The proposed level set framework has\nthe following major advantages compared to other level set methods available in\nthe literature. First, segmentation using DNLS converges much faster. Second,\nthe DNLS level set function remains regular throughout its evolution. Third,\nthe proposed multiphase version of the DNLS is less sensitive to\ninitialization, and its computational cost and memory requirement remains\nalmost constant as the number of objects to be simultaneously segmented grows.\nThe experimental results show the potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 00:08:14 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Mesadi", "Fitsum", ""], ["Cetin", "Mujdat", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1606.07536", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu and Oncel Tuzel", "title": "Coupled Generative Adversarial Networks", "comments": "To be published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose coupled generative adversarial network (CoGAN) for learning a\njoint distribution of multi-domain images. In contrast to the existing\napproaches, which require tuples of corresponding images in different domains\nin the training set, CoGAN can learn a joint distribution without any tuple of\ncorresponding images. It can learn a joint distribution with just samples drawn\nfrom the marginal distributions. This is achieved by enforcing a weight-sharing\nconstraint that limits the network capacity and favors a joint distribution\nsolution over a product of marginal distributions one. We apply CoGAN to\nseveral joint distribution learning tasks, including learning a joint\ndistribution of color and depth images, and learning a joint distribution of\nface images with different attributes. For each task it successfully learns the\njoint distribution without any tuple of corresponding images. We also\ndemonstrate its applications to domain adaptation and image transformation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 01:20:06 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 17:01:49 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Tuzel", "Oncel", ""]]}, {"id": "1606.07575", "submitter": "Arash Shahriari", "authors": "Arash Shahriari", "title": "Multipartite Ranking-Selection of Low-Dimensional Instances by\n  Supervised Projection to High-Dimensional Space", "comments": "15 pages, 1 figure, 2 tables, 3 algorithms, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning of redundant or irrelevant instances of data is a key to every\nsuccessful solution for pattern recognition. In this paper, we present a novel\nranking-selection framework for low-length but highly correlated instances.\nInstead of working in the low-dimensional instance space, we learn a supervised\nprojection to high-dimensional space spanned by the number of classes in the\ndataset under study. Imposing higher distinctions via exposing the notion of\nlabels to the instances, lets to deploy one versus all ranking for each\nindividual classes and selecting quality instances via adaptive thresholding of\nthe overall scores. To prove the efficiency of our paradigm, we employ it for\nthe purpose of texture understanding which is a hard recognition challenge due\nto high similarity of texture pixels and low dimensionality of their color\nfeatures. Our experiments show considerable improvements in recognition\nperformance over other local descriptors on several publicly available\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:15:45 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Shahriari", "Arash", ""]]}, {"id": "1606.07695", "submitter": "Yong Xu", "authors": "Yong Xu, Qiang Huang, Wenwu Wang, Philip J. B. Jackson, Mark D.\n  Plumbley", "title": "Fully DNN-based Multi-label regression for audio tagging", "comments": "Submitted to DCASE2016 Workshop which is as a satellite event to the\n  2016 European Signal Processing Conference (EUSIPCO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic event detection for content analysis in most cases relies on lots of\nlabeled data. However, manually annotating data is a time-consuming task, which\nthus makes few annotated resources available so far. Unlike audio event\ndetection, automatic audio tagging, a multi-label acoustic event classification\ntask, only relies on weakly labeled data. This is highly desirable to some\npractical applications using audio analysis. In this paper we propose to use a\nfully deep neural network (DNN) framework to handle the multi-label\nclassification task in a regression way. Considering that only chunk-level\nrather than frame-level labels are available, the whole or almost whole frames\nof the chunk were fed into the DNN to perform a multi-label regression for the\nexpected tags. The fully DNN, which is regarded as an encoding function, can\nwell map the audio features sequence to a multi-tag vector. A deep pyramid\nstructure was also designed to extract more robust high-level features related\nto the target tags. Further improved methods were adopted, such as the Dropout\nand background noise aware training, to enhance its generalization capability\nfor new audio recordings in mismatched environments. Compared with the\nconventional Gaussian Mixture Model (GMM) and support vector machine (SVM)\nmethods, the proposed fully DNN-based method could well utilize the long-term\ntemporal information with the whole chunk as the input. The results show that\nour approach obtained a 15% relative improvement compared with the official\nGMM-based method of DCASE 2016 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 14:17:34 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 10:39:29 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Xu", "Yong", ""], ["Huang", "Qiang", ""], ["Wang", "Wenwu", ""], ["Jackson", "Philip J. B.", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1606.07757", "submitter": "Felix Gr\\\"un", "authors": "Felix Gr\\\"un, Christian Rupprecht, Nassir Navab, Federico Tombari", "title": "A Taxonomy and Library for Visualizing Learned Features in Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, Convolutional Neural Networks (CNN) saw a tremendous\nsurge in performance. However, understanding what a network has learned still\nproves to be a challenging task. To remedy this unsatisfactory situation, a\nnumber of groups have recently proposed different methods to visualize the\nlearned models. In this work we suggest a general taxonomy to classify and\ncompare these methods, subdividing the literature into three main categories\nand providing researchers with a terminology to base their works on.\nFurthermore, we introduce the FeatureVis library for MatConvNet: an extendable,\neasy to use open source library for visualizing CNNs. It contains\nimplementations from each of the three main classes of visualization methods\nand serves as a useful tool for an enhanced understanding of the features\nlearned by intermediate layers, as well as for the analysis of why a network\nmight fail for certain examples.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 16:40:24 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Gr\u00fcn", "Felix", ""], ["Rupprecht", "Christian", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1606.07770", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond\n  Mooney, Trevor Darrell, Kate Saenko", "title": "Captioning Images with Diverse Objects", "comments": "CVPR 2017 Camera ready version. 17 pages (8 + 9 supplement), 12\n  figures, 8 tables. Includes project page\n  http://vsubhashini.github.io/noc.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent captioning models are limited in their ability to scale and describe\nconcepts unseen in paired image-text corpora. We propose the Novel Object\nCaptioner (NOC), a deep visual semantic captioning model that can describe a\nlarge number of object categories not present in existing image-caption\ndatasets. Our model takes advantage of external sources -- labeled images from\nobject recognition datasets, and semantic knowledge extracted from unannotated\ntext. We propose minimizing a joint objective which can learn from these\ndiverse data sources and leverage distributional semantic embeddings, enabling\nthe model to generalize and describe novel objects outside of image-caption\ndatasets. We demonstrate that our model exploits semantic information to\ngenerate captions for hundreds of object categories in the ImageNet object\nrecognition dataset that are not observed in MSCOCO image-caption training\ndata, as well as many categories that are observed very rarely. Both automatic\nevaluations and human judgements show that our model considerably outperforms\nprior work in being able to describe many more categories of objects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 17:53:45 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 20:54:17 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 18:06:27 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Hendricks", "Lisa Anne", ""], ["Rohrbach", "Marcus", ""], ["Mooney", "Raymond", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1606.07827", "submitter": "Dan Xie", "authors": "Dan Xie and Tianmin Shu and Sinisa Todorovic and Song-Chun Zhu", "title": "Modeling and Inferring Human Intents and Latent Functional Objects for\n  Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about detecting functional objects and inferring human\nintentions in surveillance videos of public spaces. People in the videos are\nexpected to intentionally take shortest paths toward functional objects subject\nto obstacles, where people can satisfy certain needs (e.g., a vending machine\ncan quench thirst), by following one of three possible intent behaviors: reach\na single functional object and stop, or sequentially visit several functional\nobjects, or initially start moving toward one goal but then change the intent\nto move toward another. Since detecting functional objects in low-resolution\nsurveillance videos is typically unreliable, we call them \"dark matter\"\ncharacterized by the functionality to attract people. We formulate the\nAgent-based Lagrangian Mechanics wherein human trajectories are\nprobabilistically modeled as motions of agents in many layers of \"dark-energy\"\nfields, where each agent can select a particular force field to affect its\nmotions, and thus define the minimum-energy Dijkstra path toward the\ncorresponding source \"dark matter\". For evaluation, we compiled and annotated a\nnew dataset. The results demonstrate our effectiveness in predicting human\nintent behaviors and trajectories, and localizing functional objects, as well\nas discovering distinct functional classes of objects by clustering human\nmotion behavior in the vicinity of functional objects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 20:15:12 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Xie", "Dan", ""], ["Shu", "Tianmin", ""], ["Todorovic", "Sinisa", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1606.07830", "submitter": "Holger Roth", "authors": "Holger R. Roth, Le Lu, Amal Farag, Andrew Sohn, Ronald M. Summers", "title": "Spatial Aggregation of Holistically-Nested Networks for Automated\n  Pancreas Segmentation", "comments": "This article will be presented at MICCAI (Medical Image Computing and\n  Computer-Assisted Interventions), Athens, Greece, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Accurate automatic organ segmentation is an important yet challenging problem\nfor medical image analysis. The pancreas is an abdominal organ with very high\nanatomical variability. This inhibits traditional segmentation methods from\nachieving high accuracies, especially compared to other organs such as the\nliver, heart or kidneys. In this paper, we present a holistic learning approach\nthat integrates semantic mid-level cues of deeply-learned organ interior and\nboundary maps via robust spatial aggregation using random forest. Our method\ngenerates boundary preserving pixel-wise class labels for pancreas\nsegmentation. Quantitative evaluation is performed on CT scans of 82 patients\nin 4-fold cross-validation. We achieve a (mean $\\pm$ std. dev.) Dice Similarity\nCoefficient of 78.01% $\\pm$ 8.2% in testing which significantly outperforms the\nprevious state-of-the-art approach of 71.8% $\\pm$ 10.7% under the same\nevaluation criterion.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 20:21:59 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Farag", "Amal", ""], ["Sohn", "Andrew", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1606.07839", "submitter": "Stefan Lee", "authors": "Stefan Lee, Senthil Purushwalkam, Michael Cogswell, Viresh Ranjan,\n  David Crandall, and Dhruv Batra", "title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical perception systems exist within larger processes that include\ninteractions with users or additional components capable of evaluating the\nquality of predicted solutions. In these contexts, it is beneficial to provide\nthese oracle mechanisms with multiple highly likely hypotheses rather than a\nsingle prediction. In this work, we pose the task of producing multiple outputs\nas a learning problem over an ensemble of deep networks -- introducing a novel\nstochastic gradient descent based approach to minimize the loss with respect to\nan oracle. Our method is simple to implement, agnostic to both architecture and\nloss function, and parameter-free. Our approach achieves lower oracle error\ncompared to existing methods on a wide range of tasks and deep architectures.\nWe also show qualitatively that the diverse solutions produced often provide\ninterpretable representations of task ambiguity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 21:48:55 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 20:44:55 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 17:12:00 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Lee", "Stefan", ""], ["Purushwalkam", "Senthil", ""], ["Cogswell", "Michael", ""], ["Ranjan", "Viresh", ""], ["Crandall", "David", ""], ["Batra", "Dhruv", ""]]}, {"id": "1606.07873", "submitter": "Jacob Walker", "authors": "Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert", "title": "An Uncertain Future: Forecasting from Static Images using Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a given scene, humans can often easily predict a set of immediate future\nevents that might happen. However, generalized pixel-level anticipation in\ncomputer vision systems is difficult because machine learning struggles with\nthe ambiguity inherent in predicting the future. In this paper, we focus on\npredicting the dense trajectory of pixels in a scene, specifically what will\nmove in the scene, where it will travel, and how it will deform over the course\nof one second. We propose a conditional variational autoencoder as a solution\nto this problem. In this framework, direct inference from the image shapes the\ndistribution of possible trajectories, while latent variables encode any\nnecessary information that is not available in the image. We show that our\nmethod is able to successfully predict events in a wide variety of scenes and\ncan produce multiple different predictions when the future is ambiguous. Our\nalgorithm is trained on thousands of diverse, realistic videos and requires\nabsolutely no human labeling. In addition to non-semantic action prediction, we\nfind that our method learns a representation that is applicable to semantic\nvision tasks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 05:56:46 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Walker", "Jacob", ""], ["Doersch", "Carl", ""], ["Gupta", "Abhinav", ""], ["Hebert", "Martial", ""]]}, {"id": "1606.07921", "submitter": "Gonzalo Vaca-Castano", "authors": "Gonzalo Vaca-Castano", "title": "Finding the Topic of a Set of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the problem of determining the topic that a set of\nimages is describing, where every topic is represented as a set of words.\nDifferent from other problems like tag assignment or similar, a) we assume\nmultiple images are used as input instead of single image, b) Input images are\ntypically not visually related, c) Input images are not necessarily\nsemantically close, and d) Output word space is unconstrained. In our proposed\nsolution, visual information of each query image is used to retrieve similar\nimages with text labels (tags) from an image database. We consider a scenario\nwhere the tags are very noisy and diverse, given that they were obtained by\nimplicit crowd-sourcing in a database of 1 million images and over seventy\nseven thousand tags. The words or tags associated to each query are processed\njointly in a word selection algorithm using random walks that allows to refine\nthe search topic, rejecting words that are not part of the topic and produce a\nset of words that fairly describe the topic. Experiments on a dataset of 300\ntopics, with up to twenty images per topic, show that our algorithm performs\nbetter than the proposed baseline for any number of query images. We also\npresent a new Conditional Random Field (CRF) word mapping algorithm that\npreserves the semantic similarity of the mapped words, increasing the\nperformance of the results over the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 15:06:27 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Vaca-Castano", "Gonzalo", ""]]}, {"id": "1606.07968", "submitter": "Hernan Vargas", "authors": "Hernan Dario Vargas Cardona, Mauricio A. Alvarez, Alvaro A. Orozco", "title": "Generalized Wishart processes for interpolation over diffusion tensor\n  fields", "comments": "8 pages, 3 figures, 15 subfigures", "journal-ref": "Advances in Visual Computing, Volume 9475 of the series Lecture\n  Notes in Computer Science pp 499-508, year 2015", "doi": "10.1007/978-3-319-27863-6_46", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Magnetic Resonance Imaging (dMRI) is a non-invasive tool for\nwatching the microstructure of fibrous nerve and muscle tissue. From dMRI, it\nis possible to estimate 2-rank diffusion tensors imaging (DTI) fields, that are\nwidely used in clinical applications: tissue segmentation, fiber tractography,\nbrain atlas construction, brain conductivity models, among others. Due to\nhardware limitations of MRI scanners, DTI has the difficult compromise between\nspatial resolution and signal noise ratio (SNR) during acquisition. For this\nreason, the data are often acquired with very low resolution. To enhance DTI\ndata resolution, interpolation provides an interesting software solution. The\naim of this work is to develop a methodology for DTI interpolation that enhance\nthe spatial resolution of DTI fields. We assume that a DTI field follows a\nrecently introduced stochastic process known as a generalized Wishart process\n(GWP), which we use as a prior over the diffusion tensor field. For posterior\ninference, we use Markov Chain Monte Carlo methods. We perform experiments in\ntoy and real data. Results of GWP outperform other methods in the literature,\nwhen compared in different validation protocols.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 21:03:24 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Cardona", "Hernan Dario Vargas", ""], ["Alvarez", "Mauricio A.", ""], ["Orozco", "Alvaro A.", ""]]}, {"id": "1606.07970", "submitter": "Hernan Vargas", "authors": "Hernan Dario Vargas Cardona, Mauricio A. Alvarez, Alvaro A. Orozco", "title": "A Tucker decomposition process for probabilistic modeling of diffusion\n  magnetic resonance imaging", "comments": "9 pages, 7 figures, 31 subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) is an emerging medical technique\nused for describing water diffusion in an organic tissue. Typically, rank-2\ntensors quantify this diffusion. From this quantification, it is possible to\ncalculate relevant scalar measures (i.e. fractional anisotropy and mean\ndiffusivity) employed in clinical diagnosis of neurological diseases.\nNonetheless, 2nd-order tensors fail to represent complex tissue structures like\ncrossing fibers. To overcome this limitation, several researchers proposed a\ndiffusion representation with higher order tensors (HOT), specifically 4th and\n6th orders. However, the current acquisition protocols of dMRI data allow\nimages with a spatial resolution between 1 $mm^3$ and 2 $mm^3$. This voxel size\nis much smaller than tissue structures. Therefore, several clinical procedures\nderived from dMRI may be inaccurate. Interpolation has been used to enhance\nresolution of dMRI in a tensorial space. Most interpolation methods are valid\nonly for rank-2 tensors and a generalization for HOT data is missing. In this\nwork, we propose a novel stochastic process called Tucker decomposition process\n(TDP) for performing HOT data interpolation. Our model is based on the Tucker\ndecomposition and Gaussian processes as parameters of the TDP. We test the TDP\nin 2nd, 4th and 6th rank HOT fields. For rank-2 tensors, we compare against\ndirect interpolation, log-Euclidean approach and Generalized Wishart processes.\nFor rank-4 and rank-6 tensors we compare against direct interpolation. Results\nobtained show that TDP interpolates accurately the HOT fields and generalizes\nto any rank.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 21:28:08 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Cardona", "Hernan Dario Vargas", ""], ["Alvarez", "Mauricio A.", ""], ["Orozco", "Alvaro A.", ""]]}, {"id": "1606.08008", "submitter": "Liangjia Zhu", "authors": "Liangjia Zhu, Peter Karasev, Ivan Kolesov, Romeil Sandhu, and Allen\n  Tannenbaum", "title": "Interactive Image Segmentation From A Feedback Control Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation is a fundamental problem in computational vision and\nmedical imaging. Designing a generic, automated method that works for various\nobjects and imaging modalities is a formidable task. Instead of proposing a new\nspecific segmentation algorithm, we present a general design principle on how\nto integrate user interactions from the perspective of feedback control theory.\nImpulsive control and Lyapunov stability analysis are employed to design and\nanalyze an interactive segmentation system. Then stabilization conditions are\nderived to guide algorithm design. Finally, the effectiveness and robustness of\nproposed method are demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 08:27:23 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 05:15:44 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Zhu", "Liangjia", ""], ["Karasev", "Peter", ""], ["Kolesov", "Ivan", ""], ["Sandhu", "Romeil", ""], ["Tannenbaum", "Allen", ""]]}, {"id": "1606.08051", "submitter": "Amir Ahooye Atashin", "authors": "Amir Ahooye Atashin, Kamaledin Ghiasi-Shirazi, Ahad Harati", "title": "Training LDCRF model on unsegmented sequences using Connectionist\n  Temporal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems such as speech recognition, gesture\nrecognition, and handwriting recognition are concerned with simultaneous\nsegmentation and labeling of sequence data. Latent-dynamic conditional random\nfield (LDCRF) is a well-known discriminative method that has been successfully\nused for this task. However, LDCRF can only be trained with pre-segmented data\nsequences in which the label of each frame is available apriori. In the realm\nof neural networks, the invention of connectionist temporal classification\n(CTC) made it possible to train recurrent neural networks on unsegmented\nsequences with great success. In this paper, we use CTC to train an LDCRF model\non unsegmented sequences. Experimental results on two gesture recognition tasks\nshow that the proposed method outperforms LDCRFs, hidden Markov models, and\nconditional random fields.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 16:26:19 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 14:23:06 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 11:57:06 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Atashin", "Amir Ahooye", ""], ["Ghiasi-Shirazi", "Kamaledin", ""], ["Harati", "Ahad", ""]]}, {"id": "1606.08078", "submitter": "Nicolas Jaccard", "authors": "Nicolas Jaccard, Thomas W. Rogers, Edward J. Morton, Lewis D. Griffin", "title": "Detection of concealed cars in complex cargo X-ray imagery using Deep\n  Learning", "comments": "Submission to Journal of X-ray Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive inspection systems based on X-ray radiography techniques are\nroutinely used at transport hubs to ensure the conformity of cargo content with\nthe supplied shipping manifest. As trade volumes increase and regulations\nbecome more stringent, manual inspection by trained operators is less and less\nviable due to low throughput. Machine vision techniques can assist operators in\ntheir task by automating parts of the inspection workflow. Since cars are\nroutinely involved in trafficking, export fraud, and tax evasion schemes, they\nrepresent an attractive target for automated detection and flagging for\nsubsequent inspection by operators. In this contribution, we describe a method\nfor the detection of cars in X-ray cargo images based on trained-from-scratch\nConvolutional Neural Networks. By introducing an oversampling scheme that\nsuitably addresses the low number of car images available for training, we\nachieved 100% car image classification rate for a false positive rate of\n1-in-454. Cars that were partially or completely obscured by other goods, a\nmodus operandi frequently adopted by criminals, were correctly detected. We\nbelieve that this level of performance suggests that the method is suitable for\ndeployment in the field. It is expected that the generic object detection\nworkflow described can be extended to other object classes given the\navailability of suitable training data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 19:45:22 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 11:54:18 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Jaccard", "Nicolas", ""], ["Rogers", "Thomas W.", ""], ["Morton", "Edward J.", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "1606.08282", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Marco F. Duarte and Benjamin Marlin", "title": "Out-of-Sample Extension for Dimensionality Reduction of Noisy Time\n  Series", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2735189", "report-no": null, "categories": "stat.ML cs.CG cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an out-of-sample extension framework for a global\nmanifold learning algorithm (Isomap) that uses temporal information in\nout-of-sample points in order to make the embedding more robust to noise and\nartifacts. Given a set of noise-free training data and its embedding, the\nproposed framework extends the embedding for a noisy time series. This is\nachieved by adding a spatio-temporal compactness term to the optimization\nobjective of the embedding. To the best of our knowledge, this is the first\nmethod for out-of-sample extension of manifold embeddings that leverages timing\ninformation available for the extension set. Experimental results demonstrate\nthat our out-of-sample extension algorithm renders a more robust and accurate\nembedding of sequentially ordered image data in the presence of various noise\nand artifacts when compared to other timing-aware embeddings. Additionally, we\nshow that an out-of-sample extension framework based on the proposed algorithm\noutperforms the state of the art in eye-gaze estimation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:03:40 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 20:13:17 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 01:37:40 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Duarte", "Marco F.", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1606.08288", "submitter": "Cristina Gallego", "authors": "Cristina Gallego-Ortiz and Anne L. Martel", "title": "Interpreting extracted rules from ensemble of trees: Application to\n  computer-aided diagnosis of breast MRI", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High predictive performance and ease of use and interpretability are\nimportant requirements for the applicability of a computer-aided diagnosis\n(CAD) to human reading studies. We propose a CAD system specifically designed\nto be more comprehensible to the radiologist reviewing screening breast MRI\nstudies. Multiparametric imaging features are combined to produce a CAD system\nfor differentiating cancerous and non-cancerous lesions. The complete system\nuses a rule-extraction algorithm to present lesion classification results in an\neasy to understand graph visualization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 14:35:09 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Gallego-Ortiz", "Cristina", ""], ["Martel", "Anne L.", ""]]}, {"id": "1606.08315", "submitter": "Yi\\u{g}it Oktar", "authors": "Yigit Oktar", "title": "Depth Estimation from Single Image using Sparse Representations", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.5059.0323", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is an interesting and challenging problem as there\nis no analytic mapping known between an intensity image and its depth map.\nRecently there has been a lot of data accumulated through depth-sensing\ncameras, in parallel to that researchers started to tackle this task using\nvarious learning algorithms. In this paper, a deep sparse coding method is\nproposed for monocular depth estimation along with an approach for\ndeterministic dictionary initialization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 15:23:04 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Oktar", "Yigit", ""]]}, {"id": "1606.08390", "submitter": "Armand Joulin", "authors": "Allan Jabri, Armand Joulin, Laurens van der Maaten", "title": "Revisiting Visual Question Answering Baselines", "comments": "European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) is an interesting learning setting for\nevaluating the abilities and shortcomings of current systems for image\nunderstanding. Many of the recently proposed VQA systems include attention or\nmemory mechanisms designed to support \"reasoning\". For multiple-choice VQA,\nnearly all of these systems train a multi-class classifier on image and\nquestion features to predict an answer. This paper questions the value of these\ncommon practices and develops a simple alternative model based on binary\nclassification. Instead of treating answers as competing choices, our model\nreceives the answer as input and predicts whether or not an\nimage-question-answer triplet is correct. We evaluate our model on the Visual7W\nTelling and the VQA Real Multiple Choice tasks, and find that even simple\nversions of our model perform competitively. Our best model achieves\nstate-of-the-art performance on the Visual7W Telling task and compares\nsurprisingly well with the most complex systems proposed for the VQA Real\nMultiple Choice task. We explore variants of the model and study its\ntransferability between both datasets. We also present an error analysis of our\nmodel that suggests a key problem of current VQA systems lies in the lack of\nvisual grounding of concepts that occur in the questions and answers. Overall,\nour results suggest that the performance of current VQA systems is not\nsignificantly better than that of systems designed to exploit dataset biases.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 18:07:58 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 21:26:06 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Jabri", "Allan", ""], ["Joulin", "Armand", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1606.08571", "submitter": "Yang Lu", "authors": "Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu", "title": "Alternating Back-Propagation for Generator Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternating back-propagation algorithm for learning\nthe generator network model. The model is a non-linear generalization of factor\nanalysis. In this model, the mapping from the continuous latent factors to the\nobserved signal is parametrized by a convolutional neural network. The\nalternating back-propagation algorithm iterates the following two steps: (1)\nInferential back-propagation, which infers the latent factors by Langevin\ndynamics or gradient descent. (2) Learning back-propagation, which updates the\nparameters given the inferred latent factors by gradient descent. The gradient\ncomputations in both steps are powered by back-propagation, and they share most\nof their code in common. We show that the alternating back-propagation\nalgorithm can learn realistic generator models of natural images, video\nsequences, and sounds. Moreover, it can also be used to learn from incomplete\nor indirect training data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 06:46:05 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 15:11:00 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 04:38:01 GMT"}, {"version": "v4", "created": "Tue, 6 Dec 2016 04:04:19 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Han", "Tian", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1606.08572", "submitter": "Bo Zhao", "authors": "Bo Zhao and Xiao Wu and Jiashi Feng and Qiang Peng and Shuicheng Yan", "title": "Diversified Visual Attention Networks for Fine-Grained Object\n  Classification", "comments": null, "journal-ref": "Published in: IEEE Transactions on Multimedia ( Volume: 19, Issue:\n  6, June 2017 )", "doi": "10.1109/TMM.2017.2648498", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained object classification is a challenging task due to the subtle\ninter-class difference and large intra-class variation. Recently, visual\nattention models have been applied to automatically localize the discriminative\nregions of an image for better capturing critical difference and demonstrated\npromising performance. However, without consideration of the diversity in\nattention process, most of existing attention models perform poorly in\nclassifying fine-grained objects. In this paper, we propose a diversified\nvisual attention network (DVAN) to address the problems of fine-grained object\nclassification, which substan- tially relieves the dependency on\nstrongly-supervised information for learning to localize discriminative regions\ncompared with attentionless models. More importantly, DVAN explicitly pursues\nthe diversity of attention and is able to gather discriminative information to\nthe maximal extent. Multiple attention canvases are generated to extract\nconvolutional features for attention. An LSTM recurrent unit is employed to\nlearn the attentiveness and discrimination of attention canvases. The proposed\nDVAN has the ability to attend the object from coarse to fine granularity, and\na dynamic internal representation for classification is built up by\nincrementally combining the information from different locations and scales of\nthe image. Extensive experiments con- ducted on CUB-2011, Stanford Dogs and\nStanford Cars datasets have demonstrated that the proposed diversified visual\nattention networks achieve competitive performance compared to the state-\nof-the-art approaches, without using any prior knowledge, user interaction or\nexternal resource in training or testing.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 06:47:41 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:36:27 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhao", "Bo", ""], ["Wu", "Xiao", ""], ["Feng", "Jiashi", ""], ["Peng", "Qiang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1606.08694", "submitter": "Martin Alain PhD", "authors": "Martin Alain, Christine Guillemot, Dominique Thoreau, Philippe\n  Guillotel", "title": "Scalable image coding based on epitomes", "comments": "Preprint submitted to IEEE Trans. on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel scheme for scalable image coding based on\nthe concept of epitome. An epitome can be seen as a factorized representation\nof an image. Focusing on spatial scalability, the enhancement layer of the\nproposed scheme contains only the epitome of the input image. The pixels of the\nenhancement layer not contained in the epitome are then restored using two\napproaches inspired from local learning-based super-resolution methods. In the\nfirst method, a locally linear embedding model is learned on base layer patches\nand then applied to the corresponding epitome patches to reconstruct the\nenhancement layer. The second approach learns linear mappings between pairs of\nco-located base layer and epitome patches. Experiments have shown that\nsignificant improvement of the rate-distortion performances can be achieved\ncompared to an SHVC reference.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:37:41 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Alain", "Martin", ""], ["Guillemot", "Christine", ""], ["Thoreau", "Dominique", ""], ["Guillotel", "Philippe", ""]]}, {"id": "1606.08805", "submitter": "Mario Valerio Giuffrida", "authors": "Mario Valerio Giuffrida and Sotirios A. Tsaftaris", "title": "Theta-RBM: Unfactored Gated Restricted Boltzmann Machine for\n  Rotation-Invariant Representations", "comments": "9 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning invariant representations is a critical task in computer vision. In\nthis paper, we propose the Theta-Restricted Boltzmann Machine ({\\theta}-RBM in\nshort), which builds upon the original RBM formulation and injects the notion\nof rotation-invariance during the learning procedure. In contrast to previous\napproaches, we do not transform the training set with all possible rotations.\nInstead, we rotate the gradient filters when they are computed during the\nContrastive Divergence algorithm. We formulate our model as an unfactored gated\nBoltzmann machine, where another input layer is used to modulate the input\nvisible layer to drive the optimisation procedure. Among our contributions is a\nmathematical proof that demonstrates that {\\theta}-RBM is able to learn\nrotation-invariant features according to a recently proposed invariance\nmeasure. Our method reaches an invariance score of ~90% on mnist-rot dataset,\nwhich is the highest result compared with the baseline methods and the current\nstate of the art in transformation-invariant feature learning in RBM. Using an\nSVM classifier, we also showed that our network learns discriminative features\nas well, obtaining ~10% of testing error.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:02:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 09:57:08 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Giuffrida", "Mario Valerio", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1606.08921", "submitter": "Chunhua Shen", "authors": "Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang", "title": "Image Restoration Using Convolutional Auto-encoders with Symmetric Skip\n  Connections", "comments": "17 pages. A journal extension of the version at arXiv:1603.09056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration, including image denoising, super resolution, inpainting,\nand so on, is a well-studied problem in computer vision and image processing,\nas well as a test bed for low-level image modeling algorithms. In this work, we\npropose a very deep fully convolutional auto-encoder network for image\nrestoration, which is a encoding-decoding framework with symmetric\nconvolutional-deconvolutional layers. In other words, the network is composed\nof multiple layers of convolution and de-convolution operators, learning\nend-to-end mappings from corrupted images to the original ones. The\nconvolutional layers capture the abstraction of image contents while\neliminating corruptions. Deconvolutional layers have the capability to upsample\nthe feature maps and recover the image details. To deal with the problem that\ndeeper networks tend to be more difficult to train, we propose to symmetrically\nlink convolutional and deconvolutional layers with skip-layer connections, with\nwhich the training converges much faster and attains better results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 00:32:28 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 04:12:33 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2016 08:50:51 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Mao", "Xiao-Jiao", ""], ["Shen", "Chunhua", ""], ["Yang", "Yu-Bin", ""]]}, {"id": "1606.08998", "submitter": "Ernest C. H. Cheung", "authors": "Ernest Cheung, Tsan Kwong Wong, Aniket Bera, Xiaogang Wang, and Dinesh\n  Manocha", "title": "LCrowdV: Generating Labeled Videos for Simulation-based Crowd Behavior\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel procedural framework to generate an arbitrary number of\nlabeled crowd videos (LCrowdV). The resulting crowd video datasets are used to\ndesign accurate algorithms or training models for crowded scene understanding.\nOur overall approach is composed of two components: a procedural simulation\nframework for generating crowd movements and behaviors, and a procedural\nrendering framework to generate different videos or images. Each video or image\nis automatically labeled based on the environment, number of pedestrians,\ndensity, behavior, flow, lighting conditions, viewpoint, noise, etc.\nFurthermore, we can increase the realism by combining synthetically-generated\nbehaviors with real-world background videos. We demonstrate the benefits of\nLCrowdV over prior lableled crowd datasets by improving the accuracy of\npedestrian detection and crowd behavior classification algorithms. LCrowdV\nwould be released on the WWW.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 08:30:44 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 05:33:48 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Cheung", "Ernest", ""], ["Wong", "Tsan Kwong", ""], ["Bera", "Aniket", ""], ["Wang", "Xiaogang", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1606.08999", "submitter": "Yin-Hsi Kuo", "authors": "Yin-Hsi Kuo and Winston H. Hsu", "title": "De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile\n  Visual Search", "comments": "Accepted for publication in IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the prevalence of mobile devices, mobile search becomes a more\nconvenient way than desktop search. Different from the traditional desktop\nsearch, mobile visual search needs more consideration for the limited resources\non mobile devices (e.g., bandwidth, computing power, and memory consumption).\nThe state-of-the-art approaches show that bag-of-words (BoW) model is robust\nfor image and video retrieval; however, the large vocabulary tree might not be\nable to be loaded on the mobile device. We observe that recent works mainly\nfocus on designing compact feature representations on mobile devices for\nbandwidth-limited network (e.g., 3G) and directly adopt feature matching on\nremote servers (cloud). However, the compact (binary) representation might fail\nto retrieve target objects (images, videos). Based on the hashed binary codes,\nwe propose a de-hashing process that reconstructs BoW by leveraging the\ncomputing power of remote servers. To mitigate the information loss from binary\ncodes, we further utilize contextual information (e.g., GPS) to reconstruct a\ncontext-aware BoW for better retrieval results. Experiment results show that\nthe proposed method can achieve competitive retrieval accuracy as BoW while\nonly transmitting few bits from mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 08:33:28 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Kuo", "Yin-Hsi", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1606.09002", "submitter": "Cong Yao", "authors": "Cong Yao, Xiang Bai, Nong Sang, Xinyu Zhou, Shuchang Zhou and Zhimin\n  Cao", "title": "Scene Text Detection via Holistic, Multi-Channel Prediction", "comments": "10 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, scene text detection has become an active research topic in\ncomputer vision and document analysis, because of its great importance and\nsignificant challenge. However, vast majority of the existing methods detect\ntext within local regions, typically through extracting character, word or line\nlevel candidates followed by candidate aggregation and false positive\nelimination, which potentially exclude the effect of wide-scope and long-range\ncontextual cues in the scene. To take full advantage of the rich information\navailable in the whole natural image, we propose to localize text in a holistic\nmanner, by casting scene text detection as a semantic segmentation problem. The\nproposed algorithm directly runs on full images and produces global, pixel-wise\nprediction maps, in which detections are subsequently formed. To better make\nuse of the properties of text, three types of information regarding text\nregion, individual characters and their relationship are estimated, with a\nsingle Fully Convolutional Network (FCN) model. With such predictions of text\nproperties, the proposed algorithm can simultaneously handle horizontal,\nmulti-oriented and curved text in real-world natural images. The experiments on\nstandard benchmarks, including ICDAR 2013, ICDAR 2015 and MSRA-TD500,\ndemonstrate that the proposed algorithm substantially outperforms previous\nstate-of-the-art approaches. Moreover, we report the first baseline result on\nthe recently-released, large-scale dataset COCO-Text.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 08:45:17 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 11:22:49 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Yao", "Cong", ""], ["Bai", "Xiang", ""], ["Sang", "Nong", ""], ["Zhou", "Xinyu", ""], ["Zhou", "Shuchang", ""], ["Cao", "Zhimin", ""]]}, {"id": "1606.09029", "submitter": "Ksenia Konyushkova", "authors": "Ksenia Konyushkova, Raphael Sznitman and Pascal Fua", "title": "Geometry in Active Learning for Binary and Multi-class Image\n  Segmentation", "comments": "Extension of our previous paper arXiv:1508.04955", "journal-ref": "Published in \"Computer Vision and Image Understanding\" journal,\n  1077-3142, 2019", "doi": "10.1016/j.cviu.2019.01.007", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an active learning approach to image segmentation that exploits\ngeometric priors to speed up and streamline the annotation process. It can be\napplied for both background-foreground and multi-class segmentation tasks in 2D\nimages and 3D image volumes. Our approach combines geometric smoothness priors\nin the image space with more traditional uncertainty measures to estimate which\npixels or voxels are the most informative, and thus should to be annotated\nnext. For multi-class settings, we additionally introduce two novel criteria\nfor uncertainty. In the 3D case, we use the resulting uncertainty measure to\nselect voxels lying on a planar patch, which makes batch annotation much more\nconvenient for the end user compared to the setting where voxels are randomly\ndistributed in a volume. The planar patch is found using a branch-and-bound\nalgorithm that looks for a 2D patch in a 3D volume where the most informative\ninstances are located. We evaluate our approach on Electron Microscopy and\nMagnetic Resonance image volumes, as well as on regular images of horses and\nfaces. We demonstrate a substantial performance increase over other approaches\nthanks to the use of geometric priors.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 10:17:32 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 10:23:26 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 17:51:39 GMT"}, {"version": "v4", "created": "Thu, 4 Apr 2019 08:28:29 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Konyushkova", "Ksenia", ""], ["Sznitman", "Raphael", ""], ["Fua", "Pascal", ""]]}, {"id": "1606.09072", "submitter": "Farnoud Kazemzadeh", "authors": "Farnoud Kazemzadeh and Alexander Wong", "title": "Resolution- and throughput-enhanced spectroscopy using high-throughput\n  computational slit", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": "10.1364/OL.41.004352", "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a fundamental tradeoff between spectral resolution and the\nefficiency or throughput for all optical spectrometers. The primary factors\naffecting the spectral resolution and throughput of an optical spectrometer are\nthe size of the entrance aperture and the optical power of the focusing\nelement. Thus far collective optimization of the above mentioned has proven\ndifficult. Here, we introduce the concept of high-throughput computational\nslits (HTCS), a numerical technique for improving both the effective spectral\nresolution and efficiency of a spectrometer. The proposed HTCS approach was\nexperimentally validated using an optical spectrometer configured with a 200 um\nentrance aperture, test, and a 50 um entrance aperture, control, demonstrating\nimprovements in spectral resolution of the spectrum by ~ 50% over the control\nspectral resolution and improvements in efficiency of > 2 times over the\nefficiency of the largest entrance aperture used in the study while producing\nhighly accurate spectra.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 13:07:24 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 04:20:52 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Kazemzadeh", "Farnoud", ""], ["Wong", "Alexander", ""]]}, {"id": "1606.09118", "submitter": "Robert Amelard", "authors": "Robert Amelard, David A Clausi, Alexander Wong", "title": "A spectral-spatial fusion model for robust blood pulse waveform\n  extraction in photoplethysmographic imaging", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoplethysmographic imaging is a camera-based solution for non-contact\ncardiovascular monitoring from a distance. This technology enables monitoring\nin situations where contact-based devices may be problematic or infeasible,\nsuch as ambulatory, sleep, and multi-individual monitoring. However, extracting\nthe blood pulse waveform signal is challenging due to the unknown mixture of\nrelevant (pulsatile) and irrelevant pixels in the scene. Here, we design and\nimplement a signal fusion framework, FusionPPG, for extracting a blood pulse\nwaveform signal with strong temporal fidelity from a scene without requiring\nanatomical priors (e.g., facial tracking). The extraction problem is posed as a\nBayesian least squares fusion problem, and solved using a novel probabilistic\npulsatility model that incorporates both physiologically derived spectral and\nspatial waveform priors to identify pulsatility characteristics in the scene.\nExperimental results show statistically significantly improvements compared to\nthe FaceMeanPPG method ($p<0.001$) and DistancePPG ($p<0.001$) methods. Heart\nrates predicted using FusionPPG correlated strongly with ground truth\nmeasurements ($r^2=0.9952$). FusionPPG was the only method able to assess\ncardiac arrhythmia via temporal analysis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 14:31:09 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Amelard", "Robert", ""], ["Clausi", "David A", ""], ["Wong", "Alexander", ""]]}, {"id": "1606.09163", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka and Giampiero Salvi", "title": "Optimising The Input Window Alignment in CD-DNN Based Phoneme\n  Recognition for Low Latency Processing", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic analysis on the performance of a phonetic recogniser\nwhen the window of input features is not symmetric with respect to the current\nframe. The recogniser is based on Context Dependent Deep Neural Networks\n(CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the\nlatency of the system by reducing the number of future feature frames required\nto estimate the current output. Our tests performed on the TIMIT database show\nthat the performance does not degrade when the input window is shifted up to 5\nframes in the past compared to common practice (no future frame). This\ncorresponds to improving the latency by 50 ms in our settings. Our tests also\nshow that the best results are not obtained with the symmetric window commonly\nemployed, but with an asymmetric window with eight past and two future context\nframes, although this observation should be confirmed on other data sets. The\nreduction in latency suggested by our results is critical for specific\napplications such as real-time lip synchronisation for tele-presence, but may\nalso be beneficial in general applications to improve the lag in human-machine\nspoken interaction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 15:51:44 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1606.09187", "submitter": "Wojciech Samek", "authors": "Jing Yu Koh and Wojciech Samek and Klaus-Robert M\\\"uller and Alexander\n  Binder", "title": "Object Boundary Detection and Classification with Image-level Labels", "comments": "12 pages, 2 figures, accepted for GCPR 2017 - 39th German Conference\n  on Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic boundary and edge detection aims at simultaneously detecting object\nedge pixels in images and assigning class labels to them. Systematic training\nof predictors for this task requires the labeling of edges in images which is a\nparticularly tedious task. We propose a novel strategy for solving this task,\nwhen pixel-level annotations are not available, performing it in an almost\nzero-shot manner by relying on conventional whole image neural net classifiers\nthat were trained using large bounding boxes. Our method performs the following\ntwo steps at test time. Firstly it predicts the class labels by applying the\ntrained whole image network to the test images. Secondly, it computes\npixel-wise scores from the obtained predictions by applying backprop gradients\nas well as recent visualization algorithms such as deconvolution and layer-wise\nrelevance propagation. We show that high pixel-wise scores are indicative for\nthe location of semantic boundaries, which suggests that the semantic boundary\nproblem can be approached without using edge labels during the training phase.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 17:08:58 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 17:13:43 GMT"}, {"version": "v3", "created": "Sun, 25 Jun 2017 12:50:55 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Koh", "Jing Yu", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Binder", "Alexander", ""]]}, {"id": "1606.09239", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan,\n  Eric P. Xing", "title": "Learning Concept Taxonomies from Multi-modal Data", "comments": "To appear in ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of automatically building hypernym taxonomies from\ntextual and visual data. Previous works in taxonomy induction generally ignore\nthe increasingly prominent visual data, which encode important perceptual\nsemantics. Instead, we propose a probabilistic model for taxonomy induction by\njointly leveraging text and images. To avoid hand-crafted feature engineering,\nwe design end-to-end features based on distributed representations of images\nand words. The model is discriminatively trained given a small set of existing\nontologies and is capable of building full taxonomies from scratch for a\ncollection of unseen conceptual label items with associated images. We evaluate\nour model and features on the WordNet hierarchies, where our system outperforms\nprevious approaches by a large gap.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 19:52:53 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Zhang", "Hao", ""], ["Hu", "Zhiting", ""], ["Deng", "Yuntian", ""], ["Sachan", "Mrinmaya", ""], ["Yan", "Zhicheng", ""], ["Xing", "Eric P.", ""]]}, {"id": "1606.09264", "submitter": "Xingjie Wei", "authors": "Xingjie Wei and David Stillwell", "title": "How smart does your profile image look? Estimating intelligence from\n  social network profile images", "comments": null, "journal-ref": null, "doi": "10.1145/3018661.3018663", "report-no": null, "categories": "cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profile images on social networks are users' opportunity to present\nthemselves and to affect how others judge them. We examine what Facebook images\nsay about users' perceived and measured intelligence. 1,122 Facebook users\ncompleted a matrices intelligence test and shared their current Facebook\nprofile image. Strangers also rated the images for perceived intelligence. We\nuse automatically extracted image features to predict both measured and\nperceived intelligence. Intelligence estimation from images is a difficult task\neven for humans, but experimental results show that human accuracy can be\nequalled using computing methods. We report the image features that predict\nboth measured and perceived intelligence, and highlight misleading features\nsuch as \"smiling\" and \"wearing glasses\" that are correlated with perceived but\nnot measured intelligence. Our results give insights into inaccurate\nstereotyping from profile images and also have implications for privacy,\nespecially since in most social networks profile images are public by default.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:03:55 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 18:01:13 GMT"}, {"version": "v3", "created": "Sun, 11 Dec 2016 21:16:17 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Wei", "Xingjie", ""], ["Stillwell", "David", ""]]}, {"id": "1606.09281", "submitter": "Lucas Mentch", "authors": "Duy Hoang Thai and Lucas Mentch", "title": "Multiphase Segmentation For Simultaneously Homogeneous and Textural\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation remains an important problem in image processing. For\nhomogeneous (piecewise smooth) images, a number of important models have been\ndeveloped and refined over the past several decades. However, these models\noften fail when applied to the substantially larger class of natural images\nthat simultaneously contain regions of both texture and homogeneity. This work\nintroduces a bi-level constrained minimization model for simultaneous\nmultiphase segmentation of images containing both homogeneous and textural\nregions. We develop novel norms defined in different functional Banach spaces\nfor the segmentation which results in a non-convex minimization. Finally, we\ndevelop a generalized notion of segmentation delving into approximation theory\nand demonstrating that a more refined decomposition of these images results in\nmultiple meaningful components. Both theoretical results and demonstrations on\nnatural images are provided.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:52:10 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Thai", "Duy Hoang", ""], ["Mentch", "Lucas", ""]]}, {"id": "1606.09282", "submitter": "Zhizhong Li", "authors": "Zhizhong Li, Derek Hoiem", "title": "Learning without Forgetting", "comments": "Conference version appears in ECCV 2016; updated with journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building a unified vision system or gradually adding new capabilities to\na system, the usual assumption is that training data for all tasks is always\navailable. However, as the number of tasks grows, storing and retraining on\nsuch data becomes infeasible. A new problem arises where we add new\ncapabilities to a Convolutional Neural Network (CNN), but the training data for\nits existing capabilities are unavailable. We propose our Learning without\nForgetting method, which uses only new task data to train the network while\npreserving the original capabilities. Our method performs favorably compared to\ncommonly used feature extraction and fine-tuning adaption techniques and\nperforms similarly to multitask learning that uses original task data we assume\nunavailable. A more surprising observation is that Learning without Forgetting\nmay be able to replace fine-tuning with similar old and new task datasets for\nimproved new task performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:54:04 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 22:12:43 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 22:32:30 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Li", "Zhizhong", ""], ["Hoiem", "Derek", ""]]}, {"id": "1606.09349", "submitter": "Yuzhong Xie", "authors": "Zhong Ji, Yuzhong Xie, Yanwei Pang, Lei Chen, Zhongfei Zhang", "title": "Zero-Shot Learning with Multi-Battery Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) extends the conventional image classification\ntechnique to a more challenging situation where the test image categories are\nnot seen in the training samples. Most studies on ZSL utilize side information\nsuch as attributes or word vectors to bridge the relations between the seen\nclasses and the unseen classes. However, existing approaches on ZSL typically\nexploit a shared space for each type of side information independently, which\ncannot make full use of the complementary knowledge of different types of side\ninformation. To this end, this paper presents an MBFA-ZSL approach to embed\ndifferent types of side information as well as the visual feature into one\nshared space. Specifically, we first develop an algorithm named Multi-Battery\nFactor Analysis (MBFA) to build a unified semantic space, and then employ\nmultiple types of side information in it to achieve the ZSL. The close-form\nsolution makes MBFA-ZSL simple to implement and efficient to run on large\ndatasets. Extensive experiments on the popular AwA, CUB, and SUN datasets show\nits significant superiority over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 05:32:37 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Ji", "Zhong", ""], ["Xie", "Yuzhong", ""], ["Pang", "Yanwei", ""], ["Chen", "Lei", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1606.09367", "submitter": "Sepehr Valipour", "authors": "Sepehr Valipour, Mennatullah Siam, Eleni Stroulia, Martin Jagersand", "title": "Parking Stall Vacancy Indicator System Based on Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parking management systems, and vacancy-indication services in particular,\ncan play a valuable role in reducing traffic and energy waste in large cities.\nVisual detection methods represent a cost-effective option, since they can take\nadvantage of hardware usually already available in many parking lots, namely\ncameras. However, visual detection methods can be fragile and not easily\ngeneralizable. In this paper, we present a robust detection algorithm based on\ndeep convolutional neural networks. We implemented and tested our algorithm on\na large baseline dataset, and also on a set of image feeds from actual cameras\nalready installed in parking lots. We have developed a fully functional system,\nfrom server-side image analysis to front-end user interface, to demonstrate the\npracticality of our method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 06:57:11 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Valipour", "Sepehr", ""], ["Siam", "Mennatullah", ""], ["Stroulia", "Eleni", ""], ["Jagersand", "Martin", ""]]}, {"id": "1606.09433", "submitter": "Diederik Paul Moeys", "authors": "Diederik Paul Moeys, Federico Corradi, Emmett Kerr, Philip Vance,\n  Gautham Das, Daniel Neil, Dermot Kerr, Tobi Delbruck", "title": "Steering a Predator Robot using a Mixed Frame/Event-Driven Convolutional\n  Neural Network", "comments": "Paper presented at the conference: Second International Conference on\n  Event-Based Control, Communication and Signal Processing (EBCCSP) 2016, At\n  Krakow, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the application of a Convolutional Neural Network (CNN)\nin the context of a predator/prey scenario. The CNN is trained and run on data\nfrom a Dynamic and Active Pixel Sensor (DAVIS) mounted on a Summit XL robot\n(the predator), which follows another one (the prey). The CNN is driven by both\nconventional image frames and dynamic vision sensor \"frames\" that consist of a\nconstant number of DAVIS ON and OFF events. The network is thus \"data driven\"\nat a sample rate proportional to the scene activity, so the effective sample\nrate varies from 15 Hz to 240 Hz depending on the robot speeds. The network\ngenerates four outputs: steer right, left, center and non-visible. After\noff-line training on labeled data, the network is imported on the on-board\nSummit XL robot which runs jAER and receives steering directions in real time.\nSuccessful results on closed-loop trials, with accuracies up to 87% or 92%\n(depending on evaluation criteria) are reported. Although the proposed approach\ndiscards the precise DAVIS event timing, it offers the significant advantage of\ncompatibility with conventional deep learning technology without giving up the\nadvantage of data-driven computing.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 11:17:00 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Moeys", "Diederik Paul", ""], ["Corradi", "Federico", ""], ["Kerr", "Emmett", ""], ["Vance", "Philip", ""], ["Das", "Gautham", ""], ["Neil", "Daniel", ""], ["Kerr", "Dermot", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1606.09518", "submitter": "Benjamin Irving", "authors": "Benjamin Irving", "title": "maskSLIC: Regional Superpixel Generation with Application to Local\n  Pathology Characterisation in Medical Images", "comments": "The article has been submitted to IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Supervoxel methods such as Simple Linear Iterative Clustering (SLIC) are an\neffective technique for partitioning an image or volume into locally similar\nregions, and are a common building block for the development of detection,\nsegmentation and analysis methods. We introduce maskSLIC an extension of SLIC\nto create supervoxels within regions-of-interest, and demonstrate,on examples\nfrom 2-dimensions to 4-dimensions, that maskSLIC overcomes issues that affect\nSLIC within an irregular mask. We highlight the benefits of this method through\nexamples, and show that it is able to better represent underlying tumour\nsubregions and achieves significantly better results than SLIC on the BRATS\n2013 brain tumour challenge data (p=0.001) - outperforming SLIC on 18/20 scans.\nFinally, we show an application of this method for the analysis of functional\ntumour subregions and demonstrate that it is more effective than voxel\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 14:46:41 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 16:32:07 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Irving", "Benjamin", ""]]}, {"id": "1606.09549", "submitter": "Jack Valmadre", "authors": "Luca Bertinetto, Jack Valmadre, Jo\\~ao F. Henriques, Andrea Vedaldi,\n  Philip H. S. Torr", "title": "Fully-Convolutional Siamese Networks for Object Tracking", "comments": "The first two authors contributed equally, and are listed in\n  alphabetical order. Code available at\n  http://www.robots.ox.ac.uk/~luca/siamese-fc.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of arbitrary object tracking has traditionally been tackled by\nlearning a model of the object's appearance exclusively online, using as sole\ntraining data the video itself. Despite the success of these methods, their\nonline-only approach inherently limits the richness of the model they can\nlearn. Recently, several attempts have been made to exploit the expressive\npower of deep convolutional networks. However, when the object to track is not\nknown beforehand, it is necessary to perform Stochastic Gradient Descent online\nto adapt the weights of the network, severely compromising the speed of the\nsystem. In this paper we equip a basic tracking algorithm with a novel\nfully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset\nfor object detection in video. Our tracker operates at frame-rates beyond\nreal-time and, despite its extreme simplicity, achieves state-of-the-art\nperformance in multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 16:00:43 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 11:48:29 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Bertinetto", "Luca", ""], ["Valmadre", "Jack", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Vedaldi", "Andrea", ""], ["Torr", "Philip H. S.", ""]]}]