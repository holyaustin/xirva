[{"id": "1810.00072", "submitter": "David Zeng", "authors": "David Y Zeng, Jamil Shaikh, Dwight G Nishimura, Shreyas S Vasanawala,\n  Joseph Y Cheng", "title": "Deep Residual Network for Off-Resonance Artifact Correction with\n  Application to Pediatric Body Magnetic Resonance Angiography with 3D Cones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Off-resonance artifact correction by deep-learning, to facilitate\nrapid pediatric body imaging with a scan time efficient 3D cones trajectory.\nMethods: A residual convolutional neural network to correct off-resonance\nartifacts (Off-ResNet) was trained with a prospective study of 30 pediatric\nmagnetic resonance angiography exams. Each exam acquired a short-readout scan\n(1.18 ms +- 0.38) and a long-readout scan (3.35 ms +- 0.74) at 3T.\nShort-readout scans, with longer scan times but negligible off-resonance\nblurring, were used as reference images and augmented with additional\noff-resonance for supervised training examples. Long-readout scans, with\ngreater off-resonance artifacts but shorter scan time, were corrected by\nautofocus and Off-ResNet and compared to short-readout scans by normalized\nroot-mean-square error (NRMSE), structural similarity index (SSIM), and peak\nsignal-to-noise ratio (PSNR). Scans were also compared by scoring on eight\nanatomical features by two radiologists, using analysis of variance with\npost-hoc Tukey's test. Reader agreement was determined with intraclass\ncorrelation. Results: Long-readout scans were on average 59.3% shorter than\nshort-readout scans. Images from Off-ResNet had superior NRMSE, SSIM, and PSNR\ncompared to uncorrected images across +-1kHz off-resonance (P<0.01). The\nproposed method had superior NRMSE over -677Hz to +1kHz and superior SSIM and\nPSNR over +-1kHz compared to autofocus (P<0.01). Radiologic scoring\ndemonstrated that long-readout scans corrected with Off-ResNet were\nnon-inferior to short-readout scans (P<0.01). Conclusion: The proposed method\ncan correct off-resonance artifacts from rapid long-readout 3D cones scans to a\nnon-inferior image quality compared to diagnostically standard short-readout\nscans.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 20:17:23 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Zeng", "David Y", ""], ["Shaikh", "Jamil", ""], ["Nishimura", "Dwight G", ""], ["Vasanawala", "Shreyas S", ""], ["Cheng", "Joseph Y", ""]]}, {"id": "1810.00091", "submitter": "Kun Wan", "authors": "Kun Wan, Boyuan Feng, Lingwei Xie, Yufei Ding", "title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized\n  Dropout", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently convolutional neural networks (CNNs) achieve great accuracy in\nvisual recognition tasks. DenseNet becomes one of the most popular CNN models\ndue to its effectiveness in feature-reuse. However, like other CNN models,\nDenseNets also face overfitting problem if not severer. Existing dropout method\ncan be applied but not as effective due to the introduced nonlinear\nconnections. In particular, the property of feature-reuse in DenseNet will be\nimpeded, and the dropout effect will be weakened by the spatial correlation\ninside feature maps. To address these problems, we craft the design of a\nspecialized dropout method from three aspects, dropout location, dropout\ngranularity, and dropout probability. The insights attained here could\npotentially be applied as a general approach for boosting the accuracy of other\nCNN models with similar nonlinear connections. Experimental results show that\nDenseNets with our specialized dropout method yield better accuracy compared to\nvanilla DenseNet and state-of-the-art CNN models, and such accuracy boost\nincreases with the model depth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 21:42:38 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wan", "Kun", ""], ["Feng", "Boyuan", ""], ["Xie", "Lingwei", ""], ["Ding", "Yufei", ""]]}, {"id": "1810.00107", "submitter": "Xin Li", "authors": "Celong Liu and Xin Li", "title": "Superimposition-guided Facial Reconstruction from Skull", "comments": "14 pages; 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algorithm to perform facial reconstruction from a given\nskull. This technique has forensic application in helping the identification of\nskeletal remains when other information is unavailable. Unlike most existing\nstrategies that directly reconstruct the face from the skull, we utilize a\ndatabase of portrait photos to create many face candidates, then perform a\nsuperimposition to get a well matched face, and then revise it according to the\nsuperimposition. To support this pipeline, we build an effective autoencoder\nfor image-based facial reconstruction, and a generative model for constrained\nface inpainting. Our experiments have demonstrated that the proposed pipeline\nis stable and accurate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 22:24:07 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Celong", ""], ["Li", "Xin", ""]]}, {"id": "1810.00108", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Georgios\n  Tzimiropoulos, Maja Pantic", "title": "Audio-Visual Speech Recognition With A Hybrid CTC/Attention Architecture", "comments": "Accepted to IEEE SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in speech recognition rely either on connectionist temporal\nclassification (CTC) or sequence-to-sequence models for character-level\nrecognition. CTC assumes conditional independence of individual characters,\nwhereas attention-based models can provide nonsequential alignments. Therefore,\nwe could use a CTC loss in combination with an attention-based model in order\nto force monotonic alignments and at the same time get rid of the conditional\nindependence assumption. In this paper, we use the recently proposed hybrid\nCTC/attention architecture for audio-visual recognition of speech in-the-wild.\nTo the best of our knowledge, this is the first time that such a hybrid\narchitecture architecture is used for audio-visual recognition of speech. We\nuse the LRS2 database and show that the proposed audio-visual model leads to an\n1.3% absolute decrease in word error rate over the audio-only model and\nachieves the new state-of-the-art performance on LRS2 database (7% word error\nrate). We also observe that the audio-visual model significantly outperforms\nthe audio-based model (up to 32.9% absolute improvement in word error rate) for\nseveral different types of noise as the signal-to-noise ratio decreases.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 22:41:59 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Petridis", "Stavros", ""], ["Stafylakis", "Themos", ""], ["Ma", "Pingchuan", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1810.00111", "submitter": "Riddhish Bhalodia", "authors": "Riddhish Bhalodia, Shireen Y. Elhabian, Ladislav Kavan, and Ross T.\n  Whitaker", "title": "DeepSSM: A Deep Learning Framework for Statistical Shape Modeling from\n  Raw Images", "comments": "Accepted to ShapeMI MICCAI 2018 (oral): Workshop on Shape in Medical\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape modeling is an important tool to characterize variation in\nanatomical morphology. Typical shapes of interest are measured using 3D imaging\nand a subsequent pipeline of registration, segmentation, and some extraction of\nshape features or projections onto some lower-dimensional shape space, which\nfacilitates subsequent statistical analysis. Many methods for constructing\ncompact shape representations have been proposed, but are often impractical due\nto the sequence of image preprocessing operations, which involve significant\nparameter tuning, manual delineation, and/or quality control by the users. We\npropose DeepSSM: a deep learning approach to extract a low-dimensional shape\nrepresentation directly from 3D images, requiring virtually no parameter tuning\nor user assistance. DeepSSM uses a convolutional neural network (CNN) that\nsimultaneously localizes the biological structure of interest, establishes\ncorrespondences, and projects these points onto a low-dimensional shape\nrepresentation in the form of PCA loadings within a point distribution model.\nTo overcome the challenge of the limited availability of training images, we\npresent a novel data augmentation procedure that uses existing correspondences\non a relatively small set of processed images with shape statistics to create\nplausible training samples with known shape parameters. Hence, we leverage the\nlimited CT/MRI scans (40-50) into thousands of images needed to train a CNN.\nAfter the training, the CNN automatically produces accurate low-dimensional\nshape representations for unseen images. We validate DeepSSM for three\ndifferent applications pertaining to modeling pediatric cranial CT for\ncharacterization of metopic craniosynostosis, femur CT scans identifying\nmorphologic deformities of the hip due to femoroacetabular impingement, and\nleft atrium MRI scans for atrial fibrillation recurrence prediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 22:53:49 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bhalodia", "Riddhish", ""], ["Elhabian", "Shireen Y.", ""], ["Kavan", "Ladislav", ""], ["Whitaker", "Ross T.", ""]]}, {"id": "1810.00119", "submitter": "Hossein Kashiani", "authors": "Hossein Kashiani, Shahriar B. Shokouhi", "title": "Visual Object Tracking based on Adaptive Siamese and Motion Estimation\n  Network", "comments": "28 pages, 1 algorithm, 7 figures, 2 table, Submitted to Elsevier,\n  Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural network (CNN) has attracted much attention in\ndifferent areas of computer vision, due to its powerful abstract feature\nrepresentation. Visual object tracking is one of the interesting and important\nareas in computer vision that achieves remarkable improvements in recent years.\nIn this work, we aim to improve both the motion and observation models in\nvisual object tracking by leveraging representation power of CNNs. To this end,\na motion estimation network (named MEN) is utilized to seek the most likely\nlocations of the target and prepare a further clue in addition to the previous\ntarget position. Hence the motion estimation would be enhanced by generating a\nsmall number of candidates near two plausible positions. The generated\ncandidates are then fed into a trained Siamese network to detect the most\nprobable candidate. Each candidate is compared to an adaptable buffer, which is\nupdated under a predefined condition. To take into account the target\nappearance changes, a weighting CNN (called WCNN) adaptively assigns weights to\nthe final similarity scores of the Siamese network using sequence-specific\ninformation. Evaluation results on well-known benchmark datasets (OTB100, OTB50\nand OTB2013) prove that the proposed tracker outperforms the state-of-the-art\ncompetitors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 00:34:36 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Kashiani", "Hossein", ""], ["Shokouhi", "Shahriar B.", ""]]}, {"id": "1810.00128", "submitter": "Jacky Chow", "authors": "Jacky C.K. Chow, Ivan Detchev, Kathleen Ang, Kristian Morin, Karthik\n  Mahadevan, Nicholas Louie", "title": "Robot Vision: Calibration of Wide-Angle Lens Cameras Using Collinearity\n  Condition and K-Nearest Neighbour Regression", "comments": "ISPRS TC I Mid-term Symposium \"Innovative Sensing - From Sensors to\n  Methods and Applications\", 10-12 October 2018. Karlsruhe, Germany", "journal-ref": "The International Archives of the Photogrammetry, Remote Sensing\n  and Spatial Information Sciences, Volume XLII-1, 2018, pp. 93-99", "doi": "10.5194/isprs-archives-XLII-1-93-2018", "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual perception is regularly used by humans and robots for navigation. By\neither implicitly or explicitly mapping the environment, ego-motion can be\ndetermined and a path of actions can be planned. The process of mapping and\nnavigation are delicately intertwined; therefore, improving one can often lead\nto an improvement of the other. Both processes are sensitive to the interior\norientation parameters of the camera system and mathematically modelling these\nsystematic errors can often improve the precision and accuracy of the overall\nsolution. This paper presents an automatic camera calibration method suitable\nfor any lens, without having prior knowledge about the sensor. Statistical\ninference is performed to map the environment and localize the camera\nsimultaneously. K-nearest neighbour regression is used to model the geometric\ndistortions of the images. A normal-angle lens Nikon camera and wide-angle lens\nGoPro camera were calibrated using the proposed method, as well as the\nconventional bundle adjustment with self-calibration method (for comparison).\nResults showed that the mapping error was reduced from an average of 14.9 mm to\n1.2 mm (i.e. a 92% improvement) and 66.6 mm to 1.5 mm (i.e. a 98% improvement)\nusing the proposed method for the Nikon and GoPro cameras, respectively. In\ncontrast, the conventional approach achieved an average 3D error of 0.9 mm\n(i.e. 94% improvement) and 3.3 mm (i.e. 95% improvement) for the Nikon and\nGoPro cameras, respectively. Thus, the proposed method performs well\nirrespective of the lens/sensor used: it yields results that are comparable to\nthe conventional approach for normal-angle lens cameras, and it has the\nadditional benefit of improving calibration results for wide-angle lens\ncameras.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 01:16:27 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Chow", "Jacky C. K.", ""], ["Detchev", "Ivan", ""], ["Ang", "Kathleen", ""], ["Morin", "Kristian", ""], ["Mahadevan", "Karthik", ""], ["Louie", "Nicholas", ""]]}, {"id": "1810.00136", "submitter": "Yash Bhalgat", "authors": "Yash Bhalgat", "title": "FusedLSTM: Fusing frame-level and video-level features for Content-based\n  Video Relevance Prediction", "comments": "Submission report for the ACMMM CBVRP challenge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes two of my best performing approaches on the\nContent-based Video Relevance Prediction challenge. In the FusedLSTM based\napproach, the inception-pool3 and the C3D-pool5 features are combined using an\nLSTM and a dense layer to form embeddings with the objective to minimize the\ntriplet loss function. In the second approach, an Online Kernel Similarity\nLearning method is proposed to learn a non-linear similarity measure to adhere\nthe relevance training data. The last section gives a complete comparison of\nall the approaches implemented during this challenge, including the one\npresented in the baseline paper.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 02:22:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bhalgat", "Yash", ""]]}, {"id": "1810.00138", "submitter": "Jacky Chow", "authors": "Jacky C.K. Chow, Derek Lichti, Kathleen Ang, Gregor Kuntze, Gulshan\n  Sharma, and Janet Ronsky", "title": "Modelling Errors in X-ray Fluoroscopic Imaging Systems Using\n  Photogrammetric Bundle Adjustment With a Data-Driven Self-Calibration\n  Approach", "comments": "ISPRS TC I Mid-term Symposium \"Innovative Sensing - From Sensors to\n  Methods and Applications\", 10-12 October 2018. Karlsruhe, Germany", "journal-ref": "The International Archives of the Photogrammetry, Remote Sensing\n  and Spatial Information Sciences, Volume XLII-1, 2018", "doi": "10.5194/isprs-archives-XLII-1-101-2018", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  X-ray imaging is a fundamental tool of routine clinical diagnosis.\nFluoroscopic imaging can further acquire X-ray images at video frame rates,\nthus enabling non-invasive in-vivo motion studies of joints, gastrointestinal\ntract, etc. For both the qualitative and quantitative analysis of static and\ndynamic X-ray images, the data should be free of systematic biases. Besides\nprecise fabrication of hardware, software-based calibration solutions are\ncommonly used for modelling the distortions. In this primary research study, a\nrobust photogrammetric bundle adjustment was used to model the projective\ngeometry of two fluoroscopic X-ray imaging systems. However, instead of relying\non an expert photogrammetrist's knowledge and judgement to decide on a\nparametric model for describing the systematic errors, a self-tuning\ndata-driven approach is used to model the complex non-linear distortion profile\nof the sensors. Quality control from the experiment showed that 0.06 mm to 0.09\nmm 3D reconstruction accuracy was achievable post-calibration using merely 15\nX-ray images. As part of the bundle adjustment, the location of the virtual\nfluoroscopic system relative to the target field can also be spatially resected\nwith an RMSE between 3.10 mm and 3.31 mm.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 03:17:15 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 06:11:05 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Chow", "Jacky C. K.", ""], ["Lichti", "Derek", ""], ["Ang", "Kathleen", ""], ["Kuntze", "Gregor", ""], ["Sharma", "Gulshan", ""], ["Ronsky", "Janet", ""]]}, {"id": "1810.00162", "submitter": "Evgenii Zheltonozhskii", "authors": "Chaim Baskin, Natan Liss, Yoav Chai, Evgenii Zheltonozhskii, Eli\n  Schwartz, Raja Giryes, Avi Mendelson, Alexander M. Bronstein", "title": "NICE: Noise Injection and Clamping Estimation for Neural Network\n  Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNN) are very popular in many fields including\ncomputer vision, speech recognition, natural language processing, to name a\nfew. Though deep learning leads to groundbreaking performance in these domains,\nthe networks used are very demanding computationally and are far from real-time\neven on a GPU, which is not power efficient and therefore does not suit low\npower systems such as mobile devices. To overcome this challenge, some\nsolutions have been proposed for quantizing the weights and activations of\nthese networks, which accelerate the runtime significantly. Yet, this\nacceleration comes at the cost of a larger error. The \\uniqname method proposed\nin this work trains quantized neural networks by noise injection and a learned\nclamping, which improve the accuracy. This leads to state-of-the-art results on\nvarious regression and classification tasks, e.g., ImageNet classification with\narchitectures such as ResNet-18/34/50 with low as 3-bit weights and\nactivations. We implement the proposed solution on an FPGA to demonstrate its\napplicability for low power real-time applications. The implementation of the\npaper is available at https://github.com/Lancer555/NICE\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 06:56:33 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 20:07:32 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Baskin", "Chaim", ""], ["Liss", "Natan", ""], ["Chai", "Yoav", ""], ["Zheltonozhskii", "Evgenii", ""], ["Schwartz", "Eli", ""], ["Giryes", "Raja", ""], ["Mendelson", "Avi", ""], ["Bronstein", "Alexander M.", ""]]}, {"id": "1810.00207", "submitter": "Yongyi Tang", "authors": "Yongyi Tang, Xing Zhang, Jingwen Wang, Shaoxiang Chen, Lin Ma and\n  Yu-Gang Jiang", "title": "Non-local NetVLAD Encoding for Video Classification", "comments": "ECCV2018 workshop on YouTube-8M Large-Scale Video Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our solution for the 2$^\\text{nd}$ YouTube-8M video\nunderstanding challenge organized by Google AI. Unlike the video recognition\nbenchmarks, such as Kinetics and Moments, the YouTube-8M challenge provides\npre-extracted visual and audio features instead of raw videos. In this\nchallenge, the submitted model is restricted to 1GB, which encourages\nparticipants focus on constructing one powerful single model rather than\nincorporating of the results from a bunch of models. Our system fuses six\ndifferent sub-models into one single computational graph, which are categorized\ninto three families. More specifically, the most effective family is the model\nwith non-local operations following the NetVLAD encoding. The other two family\nmodels are Soft-BoF and GRU, respectively. In order to further boost single\nmodels performance, the model parameters of different checkpoints are averaged.\nExperimental results demonstrate that our proposed system can effectively\nperform the video classification task, achieving 0.88763 on the public test set\nand 0.88704 on the private set in terms of GAP@20, respectively. We finally\nranked at the fourth place in the YouTube-8M video understanding challenge.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 13:07:38 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Tang", "Yongyi", ""], ["Zhang", "Xing", ""], ["Wang", "Jingwen", ""], ["Chen", "Shaoxiang", ""], ["Ma", "Lin", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1810.00216", "submitter": "Alejandro Frery", "authors": "D\\'ebora Chan and Andrea Rey and Juliana Gambini and Alejandro C.\n  Frery", "title": "Parameter Estimation for the Single-Look $\\mathcal{G}^0$ Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical properties of Synthetic Aperture Radar (SAR) image texture\nreveals useful target characteristics. It is well-known that these images are\naffected by speckle, and prone to contamination as double bounce and corner\nreflectors. The $\\mathcal{G}^0$ distribution is flexible enough to model\ndifferent degrees of texture in speckled data. It is indexed by three\nparameters: $\\alpha$, related to the texture, $\\gamma$, a scale parameter, and\n$L$, the number of looks which is related to the signal-to-noise ratio. Quality\nestimation of $\\alpha$ is essential due to its immediate interpretability. In\nthis article, we compare the behavior of a number of parameter estimation\ntechniques in the noisiest case, namely single look data. We evaluate them\nusing Monte Carlo methods for non-contaminated and contaminated data,\nconsidering convergence rate, bias, mean squared error (MSE) and computational\ncost. The results are verified with simulated and actual SAR images.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 14:31:09 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Chan", "D\u00e9bora", ""], ["Rey", "Andrea", ""], ["Gambini", "Juliana", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1810.00236", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, Daniel Borders, Richard Chen, Gregory N. McKay, Kevan\n  J. Salimian, Alexander Baras, Nicholas J. Durr", "title": "Deep Adversarial Training for Multi-Organ Nuclei Segmentation in\n  Histopathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclei segmentation is a fundamental task that is critical for various\ncomputational pathology applications including nuclei morphology analysis, cell\ntype classification, and cancer grading. Conventional vision-based methods for\nnuclei segmentation struggle in challenging cases and deep learning approaches\nhave proven to be more robust and generalizable. However, CNNs require large\namounts of labeled histopathology data. Moreover, conventional CNN-based\napproaches lack structured prediction capabilities which are required to\ndistinguish overlapping and clumped nuclei. Here, we present an approach to\nnuclei segmentation that overcomes these challenges by utilizing a conditional\ngenerative adversarial network (cGAN) trained with synthetic and real data. We\ngenerate a large dataset of H&E training images with perfect nuclei\nsegmentation labels using an unpaired GAN framework. This synthetic data along\nwith real histopathology data from six different organs are used to train a\nconditional GAN with spectral normalization and gradient penalty for nuclei\nsegmentation. This adversarial regression framework enforces higher order\nconsistency when compared to conventional CNN models. We demonstrate that this\nnuclei segmentation approach generalizes across different organs, sites,\npatients and disease states, and outperforms conventional approaches,\nespecially in isolating individual and overlapping nuclei.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 17:06:48 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 20:59:31 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Mahmood", "Faisal", ""], ["Borders", "Daniel", ""], ["Chen", "Richard", ""], ["McKay", "Gregory N.", ""], ["Salimian", "Kevan J.", ""], ["Baras", "Alexander", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1810.00258", "submitter": "Orestis Georgiou", "authors": "Michele Iodice, William Frier, James Wilcox, Ben Long, Orestis\n  Georgiou", "title": "Pulsed Schlieren Imaging of Ultrasonic Haptics and Levitation using\n  Phased Arrays", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasonic acoustic fields have recently been used to generate haptic effects\non the human skin as well as to levitate small sub-wavelength size particles.\nSchlieren imaging and background-oriented schlieren techniques can be used for\nacoustic wave pattern and beam shape visualization. These techniques exploit\nvariations in the refractive index of a propagation medium by applying\nrefractive optics or cross-correlation algorithms of photographs of illuminated\nbackground patterns. Here both background-oriented and traditional schlieren\nsystems are used to visualize the regions of the acoustic power involved in\ncreating dynamic haptic sensations and dynamic levitation traps. We demonstrate\nfor the first time the application of back-ground-oriented schlieren for\nimaging ultrasonic fields in air. We detail our imaging apparatus and present\nimproved algorithms used to visualize these phenomena that we have produced\nusing multiple phased arrays. Moreover, to improve imaging, we leverage an\nelectronically controlled, high-output LED which is pulsed in synchrony with\nthe ultrasonic carrier frequency.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 19:42:25 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Iodice", "Michele", ""], ["Frier", "William", ""], ["Wilcox", "James", ""], ["Long", "Ben", ""], ["Georgiou", "Orestis", ""]]}, {"id": "1810.00302", "submitter": "Shanshan Wang", "authors": "Shanshan Wang, Ziwen Ke, Huitao Cheng, Sen Jia, Ying Leslie, Hairong\n  Zheng, Dong Liang", "title": "DIMENSION: Dynamic MR Imaging with Both K-space and Spatial Prior\n  Knowledge Obtained via Multi-Supervised Network Training", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic MR image reconstruction from incomplete k-space data has generated\ngreat research interest due to its capability in reducing scan time.\nNevertheless, the reconstruction problem is still challenging due to its\nill-posed nature. Most existing methods either suffer from long iterative\nreconstruction time or explore limited prior knowledge. This paper proposes a\ndynamic MR imaging method with both k-space and spatial prior knowledge\nintegrated via multi-supervised network training, dubbed as DIMENSION.\nSpecifically, the DIMENSION architecture consists of a frequential prior\nnetwork for updating the k-space with its network prediction and a spatial\nprior network for capturing image structures and details. Furthermore, a\nmultisupervised network training technique is developed to constrain the\nfrequency domain information and reconstruction results at different levels.\nThe comparisons with classical k-t FOCUSS, k-t SLR, L+S and the\nstate-of-the-art CNN-based method on in vivo datasets show our method can\nachieve improved reconstruction results in shorter time.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 03:01:14 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 01:33:18 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 14:13:19 GMT"}, {"version": "v4", "created": "Tue, 6 Nov 2018 04:59:23 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Wang", "Shanshan", ""], ["Ke", "Ziwen", ""], ["Cheng", "Huitao", ""], ["Jia", "Sen", ""], ["Leslie", "Ying", ""], ["Zheng", "Hairong", ""], ["Liang", "Dong", ""]]}, {"id": "1810.00304", "submitter": "Zichuan Liu", "authors": "Zichuan Liu, Guosheng Lin, Wang Ling Goh, Fayao Liu, Chunhua Shen and\n  Xiaokang Yang", "title": "Correlation Propagation Networks for Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel hybrid method for scene text detection\nnamely Correlation Propagation Network (CPN). It is an end-to-end trainable\nframework engined by advanced Convolutional Neural Networks. Our CPN predicts\ntext objects according to both top-down observations and the bottom-up cues.\nMultiple candidate boxes are assembled by a spatial communication mechanism\ncall Correlation Propagation (CP). The extracted spatial features by CNN are\nregarded as node features in a latticed graph and Correlation Propagation\nalgorithm runs distributively on each node to update the hypothesis of\ncorresponding object centers. The CP process can flexibly handle scale-varying\nand rotated text objects without using predefined bounding box templates.\nBenefit from its distributive nature, CPN is computationally efficient and\nenjoys a high level of parallelism. Moreover, we introduce deformable\nconvolution to the backbone network to enhance the adaptability to long texts.\nThe evaluation on public benchmarks shows that the proposed method achieves\nstate-of-art performance, and it significantly outperforms the existing methods\nfor handling multi-scale and multi-oriented text objects with much lower\ncomputation cost.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 03:14:41 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Zichuan", ""], ["Lin", "Guosheng", ""], ["Goh", "Wang Ling", ""], ["Liu", "Fayao", ""], ["Shen", "Chunhua", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1810.00308", "submitter": "Mohamed El Amine Elforaici", "authors": "Mohamed El Amine Elforaici, Ismail Chaaraoui, Wassim Bouachir, Youssef\n  Ouakrim and Neila Mezghani", "title": "Posture recognition using an RGB-D camera : exploring 3D body modeling\n  and deep learning approaches", "comments": "IEEE Life Sciences Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of RGB-D sensors offered new possibilities for addressing\ncomplex artificial vision problems efficiently. Human posture recognition is\namong these computer vision problems, with a wide range of applications such as\nambient assisted living and intelligent health care systems. In this context,\nour paper presents novel methods and ideas to design automatic posture\nrecognition systems using an RGB-D camera. More specifically, we introduce two\nsupervised methods to learn and recognize human postures using the main types\nof visual data provided by an RGB-D camera. The first method is based on\nconvolutional features extracted from 2D images. Convolutional Neural Networks\n(CNNs) are trained to recognize human postures using transfer learning on RGB\nand depth images. Secondly, we propose to model the posture using the body\njoint configuration in the 3D space. Posture recognition is then performed\nthrough SVM classification of 3D skeleton-based features. To evaluate the\nproposed methods, we created a challenging posture recognition dataset with a\nconsiderable variability regarding the acquisition conditions. The experimental\nresults demonstrated comparable performances and high precision for both\nmethods in recognizing human postures, with a slight superiority for the\nCNN-based method when applied on depth images. Moreover, the two approaches\ndemonstrated a high robustness to several perturbation factors, such as scale\nand orientation change.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 03:50:09 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 00:07:11 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Elforaici", "Mohamed El Amine", ""], ["Chaaraoui", "Ismail", ""], ["Bouachir", "Wassim", ""], ["Ouakrim", "Youssef", ""], ["Mezghani", "Neila", ""]]}, {"id": "1810.00319", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Kevin Murphy, Jiyan Pan, Joseph Roth, Florian Schroff,\n  Andrew Gallagher", "title": "Modeling Uncertainty with Hedged Instance Embedding", "comments": "15 pages, 11 figures, updated version of ICLR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance embeddings are an efficient and versatile image representation that\nfacilitates applications like recognition, verification, retrieval, and\nclustering. Many metric learning methods represent the input as a single point\nin the embedding space. Often the distance between points is used as a proxy\nfor match confidence. However, this can fail to represent uncertainty arising\nwhen the input is ambiguous, e.g., due to occlusion or blurriness. This work\naddresses this issue and explicitly models the uncertainty by hedging the\nlocation of each input in the embedding space. We introduce the hedged instance\nembedding (HIB) in which embeddings are modeled as random variables and the\nmodel is trained under the variational information bottleneck principle.\nEmpirical results on our new N-digit MNIST dataset show that our method leads\nto the desired behavior of hedging its bets across the embedding space upon\nencountering ambiguous inputs. This results in improved performance for image\nmatching and classification tasks, more structure in the learned embedding\nspace, and an ability to compute a per-exemplar uncertainty measure that is\ncorrelated with downstream performance.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 04:51:27 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 17:26:22 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 15:41:25 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 23:46:55 GMT"}, {"version": "v5", "created": "Wed, 7 Aug 2019 06:32:15 GMT"}, {"version": "v6", "created": "Tue, 27 Aug 2019 00:31:41 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Oh", "Seong Joon", ""], ["Murphy", "Kevin", ""], ["Pan", "Jiyan", ""], ["Roth", "Joseph", ""], ["Schroff", "Florian", ""], ["Gallagher", "Andrew", ""]]}, {"id": "1810.00327", "submitter": "Amirhossein Dadashzadeh", "authors": "Amirhossein Dadashzadeh and Alireza Tavakoli Targhi", "title": "Multi-Level Contextual Network for Biomedical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable image segmentation is an essential part of biomedical\nimage analysis. In this paper, we consider the problem of biomedical image\nsegmentation using deep convolutional neural networks. We propose a new\nend-to-end network architecture that effectively integrates local and global\ncontextual patterns of histologic primitives to obtain a more reliable\nsegmentation result. Specifically, we introduce a deep fully convolution\nresidual network with a new skip connection strategy to control the contextual\ninformation passed forward. Moreover, our trained model is also computationally\ninexpensive due to its small number of network parameters. We evaluate our\nmethod on two public datasets for epithelium segmentation and tubule\nsegmentation tasks. Our experimental results show that the proposed method\nprovides a fast and effective way of producing a pixel-wise dense prediction of\nbiomedical images.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 06:45:16 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Dadashzadeh", "Amirhossein", ""], ["Targhi", "Alireza Tavakoli", ""]]}, {"id": "1810.00345", "submitter": "Yuhu Shan", "authors": "Yuhu Shan, Wen Feng Lu, Chee Meng Chew", "title": "Pixel and Feature Level Based Domain Adaption for Object Detection in\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating large scale datasets to train modern convolutional neural networks\nis prohibitively expensive and time-consuming for many real tasks. One\nalternative is to train the model on labeled synthetic datasets and apply it in\nthe real scenes. However, this straightforward method often fails to generalize\nwell mainly due to the domain bias between the synthetic and real datasets.\nMany unsupervised domain adaptation (UDA) methods are introduced to address\nthis problem but most of them only focus on the simple classification task. In\nthis paper, we present a novel UDA model to solve the more complex object\ndetection problem in the context of autonomous driving. Our model integrates\nboth pixel level and feature level based transformtions to fulfill the cross\ndomain detection task and can be further trained end-to-end to pursue better\nperformance. We employ objectives of the generative adversarial network and the\ncycle consistency loss for image translation in the pixel space. To address the\npotential semantic inconsistency problem, we propose region proposal based\nfeature adversarial training to preserve the semantics of our target objects as\nwell as further minimize the domain shifts. Extensive experiments are conducted\non several different datasets, and the results demonstrate the robustness and\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 09:26:40 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 08:19:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Shan", "Yuhu", ""], ["Lu", "Wen Feng", ""], ["Chew", "Chee Meng", ""]]}, {"id": "1810.00360", "submitter": "Dawood Al Chanti", "authors": "Dawood Al Chanti and Alice Caplier", "title": "Improving Bag-of-Visual-Words Towards Effective Facial Expressive Image\n  Classification", "comments": "8 pages, 6 figures, Volume 5: VISAPPm year 2018,\n  publisher=SciTePress, organization=INSTICC, isbn=978-989-758-290-5", "journal-ref": "In Proceedings of the 13th International Joint Conference on\n  Computer Vision, Imaging and Computer Graphics Theory and Applications -\n  Volume 5: VISAPP, ISBN 978-989-758-290-5, pages 145-152, year=2018", "doi": "10.5220/0006537601450152", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bag-of-Visual-Words (BoVW) approach has been widely used in the recent years\nfor image classification purposes. However, the limitations regarding optimal\nfeature selection, clustering technique, the lack of spatial organization of\nthe data and the weighting of visual words are crucial. These factors affect\nthe stability of the model and reduce performance. We propose to develop an\nalgorithm based on BoVW for facial expression analysis which goes beyond those\nlimitations. Thus the visual codebook is built by using k-Means++ method to\navoid poor clustering. To exploit reliable low level features, we search for\nthe best feature detector that avoids locating a large number of keypoints\nwhich do not contribute to the classification process. Then, we propose to\ncompute the relative conjunction matrix in order to preserve the spatial order\nof the data by coding the relationships among visual words. In addition, a\nweighting scheme that reflects how important a visual word is with respect to a\ngiven image is introduced. We speed up the learning process by using histogram\nintersection kernel by Support Vector Machine to learn a discriminative\nclassifier. The efficiency of the proposed algorithm is compared with standard\nbag of visual words method and with bag of visual words method with spatial\npyramid. Extensive experiments on the CK+, the MMI and the JAFFE databases show\ngood average recognition rates. Likewise, the ability to recognize spontaneous\nand non-basic expressive states is investigated using the DynEmo database.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 11:28:24 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Chanti", "Dawood Al", ""], ["Caplier", "Alice", ""]]}, {"id": "1810.00362", "submitter": "Dawood Al Chanti", "authors": "Dawood Al Chanti and Alice Caplier", "title": "Spontaneous Facial Expression Recognition using Sparse Representation", "comments": "11 pages, 9 figures, VISAPP 2017, publisher=SciTePress,\n  organization=INSTICC, isbn=978-989-758-226-4, Proceedings of the 12th\n  International Joint Conference on Computer Vision, Imaging and Computer\n  Graphics Theory and Applications - Volume 5: VISAPP, (VISIGRAPP 2017)}, 2017", "journal-ref": null, "doi": "10.5220/0006118000640074", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facial expression is the most natural means for human beings to communicate\ntheir emotions. Most facial expression analysis studies consider the case of\nacted expressions. Spontaneous facial expression recognition is significantly\nmore challenging since each person has a different way to react to a given\nemotion. We consider the problem of recognizing spontaneous facial expression\nby learning discriminative dictionaries for sparse representation. Facial\nimages are represented as a sparse linear combination of prototype atoms via\nOrthogonal Matching Pursuit algorithm. Sparse codes are then used to train an\nSVM classifier dedicated to the recognition task. The dictionary that\nsparsifies the facial images (feature points with the same class labels should\nhave similar sparse codes) is crucial for robust classification. Learning\nsparsifying dictionaries heavily relies on the initialization process of the\ndictionary. To improve the performance of dictionaries, a random face feature\ndescriptor based on the Random Projection concept is developed. The\neffectiveness of the proposed method is evaluated through several experiments\non the spontaneous facial expressions DynEmo database. It is also estimated on\nthe well-known acted facial expressions JAFFE database for a purpose of\ncomparison with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 11:38:34 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Chanti", "Dawood Al", ""], ["Caplier", "Alice", ""]]}, {"id": "1810.00396", "submitter": "Dmitry Podviaznikov", "authors": "Roman Khudorozhkov, Dmitry Podvyaznikov", "title": "Benchmarks of ResNet Architecture for Atrial Fibrillation Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we apply variations of ResNet architecture to the task of atrial\nfibrillation classification. Variations differ in number of filter after first\nconvolution, ResNet block layout, number of filters in block convolutions and\nnumber of ResNet blocks between downsampling operations. We have found a range\nof model size in which models with quite different configurations show similar\nperformance. It is likely that overall number of parameters plays dominant role\nin model performance. However, configuration parameters like layout have values\nthat constantly lead to better results, which allows to suggest that these\nparameters should be defined and fixed in the first place, while others may be\nvaried in a reasonable range to satisfy any existing constraints.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:09:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Khudorozhkov", "Roman", ""], ["Podvyaznikov", "Dmitry", ""]]}, {"id": "1810.00403", "submitter": "Ido Zachevsky", "authors": "Ido Zachevsky and Yehoshua Y. Zeevi", "title": "Modelling local phase of images and textures with applications in phase\n  denoising and phase retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fourier magnitude has been studied extensively, but less effort has been\ndevoted to the Fourier phase, despite its well-established importance in image\nrepresentation. Global phase was shown to be more important for image\nrepresentation than the magnitude, whereas local phase, exhibited in Gabor\nfilters, has been used for analysis purposes in detecting image contours and\nedges. Neither global nor local phase has been modelled in closed form,\nsuitable for Bayesian estimation.\n  In this work, we analyze the local phase of textured images and propose a\nlocal (Markovian) model for local phase coefficients. This model is\nGaussian-mixture-based, learned from the graph representation of images, based\non their complex wavelet decomposition. We demonstrate the applicability of the\nmodel in restoration of images with noisy local phase and in image retrieval,\nwhere we show superior performance to the well-known hybrid input-output (HIO)\nmethod. We also provide a framework for application of the model in a general\nsetup of image processing.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:31:28 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Zachevsky", "Ido", ""], ["Zeevi", "Yehoshua Y.", ""]]}, {"id": "1810.00415", "submitter": "Robert Williams", "authors": "Robert Max Williams, Roman V. Yampolskiy", "title": "Optical Illusions Images Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human vision is capable of performing many tasks not optimized for in its\nlong evolution. Reading text and identifying artificial objects such as road\nsigns are both tasks that mammalian brains never encountered in the wild but\nare very easy for us to perform. However, humans have discovered many very\nspecific tricks that cause us to misjudge color, size, alignment and movement\nof what we are looking at. A better understanding of these phenomenon could\nreveal insights into how human perception achieves these feats. In this paper\nwe present a dataset of 6725 illusion images gathered from two websites, and a\nsmaller dataset of 500 hand-picked images. We will discuss the process of\ncollecting this data, models trained on it, and the work that needs to be done\nto make it of value to computer vision researchers.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 16:14:48 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 15:21:43 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Williams", "Robert Max", ""], ["Yampolskiy", "Roman V.", ""]]}, {"id": "1810.00434", "submitter": "Huizi Mao", "authors": "Huizi Mao, Taeyoung Kong, William J. Dally", "title": "CaTDet: Cascaded Tracked Detector for Efficient Object Detection from\n  Video", "comments": "Accepted to SysML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects in a video is a compute-intensive task. In this paper we\npropose CaTDet, a system to speedup object detection by leveraging the temporal\ncorrelation in video. CaTDet consists of two DNN models that form a cascaded\ndetector, and an additional tracker to predict regions of interests based on\nhistoric detections. We also propose a new metric, mean Delay(mD), which is\ndesigned for latency-critical video applications. Experiments on the KITTI\ndataset show that CaTDet reduces operation count by 5.1-8.7x with the same mean\nAverage Precision(mAP) as the single-model Faster R-CNN detector and incurs\nadditional delay of 0.3 frame. On CityPersons dataset, CaTDet achieves 13.0x\nreduction in operations with 0.8% mAP loss.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 17:59:42 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 18:57:18 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Mao", "Huizi", ""], ["Kong", "Taeyoung", ""], ["Dally", "William J.", ""]]}, {"id": "1810.00457", "submitter": "Alberto Pretto", "authors": "Ciro Potena, Raghav Khanna, Juan Nieto, Roland Siegwart, Daniele\n  Nardi, and Alberto Pretto", "title": "AgriColMap: Aerial-Ground Collaborative 3D Mapping for Precision Farming", "comments": "Published in IEEE Robotics and Automation Letters, 2019", "journal-ref": "IEEE Robotics and Automation Letters, Vol: 4, Issue: 2, April\n  2019, pages 1085-1092", "doi": "10.1109/LRA.2019.2894468", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of aerial survey capabilities of Unmanned Aerial Vehicles\nwith targeted intervention abilities of agricultural Unmanned Ground Vehicles\ncan significantly improve the effectiveness of robotic systems applied to\nprecision agriculture. In this context, building and updating a common map of\nthe field is an essential but challenging task. The maps built using robots of\ndifferent types show differences in size, resolution and scale, the associated\ngeolocation data may be inaccurate and biased, while the repetitiveness of both\nvisual appearance and geometric structures found within agricultural contexts\nrender classical map merging techniques ineffective. In this paper we propose\nAgriColMap, a novel map registration pipeline that leverages a grid-based\nmultimodal environment representation which includes a vegetation index map and\na Digital Surface Model. We cast the data association problem between maps\nbuilt from UAVs and UGVs as a multimodal, large displacement dense optical flow\nestimation. The dominant, coherent flows, selected using a voting scheme, are\nused as point-to-point correspondences to infer a preliminary non-rigid\nalignment between the maps. A final refinement is then performed, by exploiting\nonly meaningful parts of the registered maps. We evaluate our system using real\nworld data for 3 fields with different crop species. The results show that our\nmethod outperforms several state of the art map registration and matching\ntechniques by a large margin, and has a higher tolerance to large initial\nmisalignments. We release an implementation of the proposed approach along with\nthe acquired datasets with this paper.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 20:10:52 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 10:26:36 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Potena", "Ciro", ""], ["Khanna", "Raghav", ""], ["Nieto", "Juan", ""], ["Siegwart", "Roland", ""], ["Nardi", "Daniele", ""], ["Pretto", "Alberto", ""]]}, {"id": "1810.00461", "submitter": "Priyanka Mandikal", "authors": "Priyanka Mandikal, Navaneet K L, R. Venkatesh Babu", "title": "3D-PSRNet: Part Segmented 3D Point Cloud Reconstruction From a Single\n  Image", "comments": "Accepted at ECCV Workshop 2018. Codes are available at\n  https://github.com/val-iisc/3d-psrnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mechanism to reconstruct part annotated 3D point clouds of\nobjects given just a single input image. We demonstrate that jointly training\nfor both reconstruction and segmentation leads to improved performance in both\nthe tasks, when compared to training for each task individually. The key idea\nis to propagate information from each task so as to aid the other during the\ntraining procedure. Towards this end, we introduce a location-aware\nsegmentation loss in the training regime. We empirically show the effectiveness\nof the proposed loss in generating more faithful part reconstructions while\nalso improving segmentation accuracy. We thoroughly evaluate the proposed\napproach on different object categories from the ShapeNet dataset to obtain\nimproved results in reconstruction as well as segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 20:36:58 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Mandikal", "Priyanka", ""], ["L", "Navaneet K", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1810.00475", "submitter": "Riddhish Bhalodia", "authors": "Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris,\n  Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, and\n  Shireen Elhabian", "title": "Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation", "comments": "Presented at Computing in Cardiology (CinC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left atrium shape has been shown to be an independent predictor of recurrence\nafter atrial fibrillation (AF) ablation. Shape-based representation is\nimperative to such an estimation process, where correspondence-based\nrepresentation offers the most flexibility and ease-of-computation for\npopulation-level shape statistics. Nonetheless, population-level shape\nrepresentations in the form of image segmentation and correspondence models\nderived from cardiac MRI require significant human resources with sufficient\nanatomy-specific expertise. In this paper, we propose a machine learning\napproach that uses deep networks to estimate AF recurrence by predicting shape\ndescriptors directly from MRI images, with NO image pre-processing involved. We\nalso propose a novel data augmentation scheme to effectively train a deep\nnetwork in a limited training data setting. We compare this new method of\nestimating shape descriptors from images with the state-of-the-art\ncorrespondence-based shape modeling that requires image segmentation and\ncorrespondence optimization. Results show that the proposed method and the\ncurrent state-of-the-art produce statistically similar outcomes on AF\nrecurrence, eliminating the need for expensive pre-processing pipelines and\nassociated human labor.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 22:10:28 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bhalodia", "Riddhish", ""], ["Goparaju", "Anupama", ""], ["Sodergren", "Tim", ""], ["Morris", "Alan", ""], ["Kholmovski", "Evgueni", ""], ["Marrouche", "Nassir", ""], ["Cates", "Joshua", ""], ["Whitaker", "Ross", ""], ["Elhabian", "Shireen", ""]]}, {"id": "1810.00482", "submitter": "Annie Xie", "authors": "Annie Xie, Avi Singh, Sergey Levine, Chelsea Finn", "title": "Few-Shot Goal Inference for Visuomotor Learning and Planning", "comments": "Videos available at https://sites.google.com/view/few-shot-goals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning and planning methods require an objective or reward\nfunction that encodes the desired behavior. Yet, in practice, there is a wide\nrange of scenarios where an objective is difficult to provide programmatically,\nsuch as tasks with visual observations involving unknown object positions or\ndeformable objects. In these cases, prior methods use engineered\nproblem-specific solutions, e.g., by instrumenting the environment with\nadditional sensors to measure a proxy for the objective. Such solutions require\na significant engineering effort on a per-task basis, and make it impractical\nfor robots to continuously learn complex skills outside of laboratory settings.\nWe aim to find a more general and scalable solution for specifying goals for\nrobot learning in unconstrained environments. To that end, we formulate the\nfew-shot objective learning problem, where the goal is to learn a task\nobjective from only a few example images of successful end states for that\ntask. We propose a simple solution to this problem: meta-learn a classifier\nthat can recognize new goals from a few examples. We show how this approach can\nbe used with both model-free reinforcement learning and visual model-based\nplanning and show results in three domains: rope manipulation from images in\nsimulation, visual navigation in a simulated 3D environment, and object\narrangement into user-specified configurations on a real robot.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 22:57:58 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Xie", "Annie", ""], ["Singh", "Avi", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1810.00495", "submitter": "Qiang Zhang", "authors": "Qiang Zhang, Qiangqiang Yuan, Jie Li, Xinxin Liu, Huanfeng Shen,\n  Liangpei Zhang", "title": "Hybrid Noise Removal in Hyperspectral Imagery With a Spatial-Spectral\n  Gradient Network", "comments": "Accept by IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2019.2912909", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of hybrid noise in hyperspectral images (HSIs) severely\ndegrades the data quality, reduces the interpretation accuracy of HSIs, and\nrestricts the subsequent HSIs applications. In this paper, the spatial-spectral\ngradient network (SSGN) is presented for mixed noise removal in HSIs. The\nproposed method employs a spatial-spectral gradient learning strategy, in\nconsideration of the unique spatial structure directionality of sparse noise\nand spectral differences with additional complementary information for better\nextracting intrinsic and deep features of HSIs. Based on a fully cascaded\nmulti-scale convolutional network, SSGN can simultaneously deal with the\ndifferent types of noise in different HSIs or spectra by the use of the same\nmodel. The simulated and real-data experiments undertaken in this study\nconfirmed that the proposed SSGN performs better at mixed noise removal than\nthe other state-of-the-art HSI denoising algorithms, in evaluation indices,\nvisual assessments, and time consumption.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 00:52:34 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 04:15:39 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 08:02:41 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhang", "Qiang", ""], ["Yuan", "Qiangqiang", ""], ["Li", "Jie", ""], ["Liu", "Xinxin", ""], ["Shen", "Huanfeng", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1810.00500", "submitter": "Jong Chul Ye", "authors": "Yoseob Han and Jong Chul Ye", "title": "One Network to Solve All ROIs: Deep Learning CT for Any ROI using\n  Differentiated Backprojection", "comments": "Accepted by Medical Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography for region-of-interest (ROI) reconstruction has\nadvantages of reducing X-ray radiation dose and using a small detector.\nHowever, standard analytic reconstruction methods suffer from severe cupping\nartifacts, and existing model-based iterative reconstruction methods require\nextensive computations. Recently, we proposed a deep neural network to learn\nthe cupping artifact, but the network is not well generalized for different\nROIs due to the singularities in the corrupted images. Therefore, there is an\nincreasing demand for a neural network that works well for any ROI sizes. In\nthis paper, two types of neural networks are designed. The first type learns\nROI size-specific cupping artifacts from the analytic reconstruction images,\nwhereas the second type network is to learn to invert the finite Hilbert\ntransform from the truncated differentiated backprojection (DBP) data. Their\ngeneralizability for any ROI sizes is then examined. Experimental results show\nthat the new type of neural network significantly outperforms the existing\niterative methods for any ROI size in spite of significantly reduced run-time\ncomplexity. Since the proposed method consistently surpasses existing methods\nfor any ROIs, it can be used as a general CT reconstruction engine for many\npractical applications without compromising possible detector truncation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 01:51:33 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 04:26:34 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Han", "Yoseob", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1810.00518", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Cha Zhang, Diana Marculescu", "title": "Layer-compensated Pruning for Resource-constrained Convolutional Neural\n  Networks", "comments": "11 pages, 8 figures, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource-efficient convolution neural networks enable not only the\nintelligence on edge devices but also opportunities in system-level\noptimization such as scheduling. In this work, we aim to improve the\nperformance of resource-constrained filter pruning by merging two sub-problems\ncommonly considered, i.e., (i) how many filters to prune for each layer and\n(ii) which filters to prune given a per-layer pruning budget, into a global\nfilter ranking problem. Our framework entails a novel algorithm, dubbed\nlayer-compensated pruning, where meta-learning is involved to determine better\nsolutions. We show empirically that the proposed algorithm is superior to prior\nart in both effectiveness and efficiency. Specifically, we reduce the accuracy\ngap between the pruned and original networks from 0.9% to 0.7% with 8x\nreduction in time needed for meta-learning, i.e., from 1 hour down to 7\nminutes. To this end, we demonstrate the effectiveness of our algorithm using\nResNet and MobileNetV2 networks under CIFAR-10, ImageNet, and Bird-200\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 03:41:25 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 02:36:01 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Zhang", "Cha", ""], ["Marculescu", "Diana", ""]]}, {"id": "1810.00523", "submitter": "Soheil Esmaeilzadeh", "authors": "Soheil Esmaeilzadeh, Dimitrios Ioannis Belivanis, Kilian M. Pohl, and\n  Ehsan Adeli", "title": "End-To-End Alzheimer's Disease Diagnosis and Biomarker Identification", "comments": null, "journal-ref": "Machine Learning in Medical Imaging. MLMI 2018. Lecture Notes in\n  Computer Science, vol 11046. Springer, Cham", "doi": "10.1007/978-3-030-00919-9_39", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As shown in computer vision, the power of deep learning lies in automatically\nlearning relevant and powerful features for any perdition task, which is made\npossible through end-to-end architectures. However, deep learning approaches\napplied for classifying medical images do not adhere to this architecture as\nthey rely on several pre- and post-processing steps. This shortcoming can be\nexplained by the relatively small number of available labeled subjects, the\nhigh dimensionality of neuroimaging data, and difficulties in interpreting the\nresults of deep learning methods. In this paper, we propose a simple 3D\nConvolutional Neural Networks and exploit its model parameters to tailor the\nend-to-end architecture for the diagnosis of Alzheimer's disease (AD). Our\nmodel can diagnose AD with an accuracy of 94.1\\% on the popular ADNI dataset\nusing only MRI data, which outperforms the previous state-of-the-art. Based on\nthe learned model, we identify the disease biomarkers, the results of which\nwere in accordance with the literature. We further transfer the learned model\nto diagnose mild cognitive impairment (MCI), the prodromal stage of AD, which\nyield better results compared to other methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 04:01:09 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Esmaeilzadeh", "Soheil", ""], ["Belivanis", "Dimitrios Ioannis", ""], ["Pohl", "Kilian M.", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1810.00530", "submitter": "Juhan Bae", "authors": "Sebastian Kmiec, Juhan Bae, Ruijian An", "title": "Learnable Pooling Methods for Video Classification", "comments": "Presented at Youtube 8M ECCV18 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce modifications to state-of-the-art approaches to aggregating\nlocal video descriptors by using attention mechanisms and function\napproximations. Rather than using ensembles of existing architectures, we\nprovide an insight on creating new architectures. We demonstrate our solutions\nin the \"The 2nd YouTube-8M Video Understanding Challenge\", by using frame-level\nvideo and audio descriptors. We obtain testing accuracy similar to the state of\nthe art, while meeting budget constraints, and touch upon strategies to improve\nthe state of the art. Model implementations are available in\nhttps://github.com/pomonam/LearnablePoolingMethods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 05:02:47 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Kmiec", "Sebastian", ""], ["Bae", "Juhan", ""], ["An", "Ruijian", ""]]}, {"id": "1810.00551", "submitter": "Hazrat Ali", "authors": "Talha Iqbal, Hazrat Ali", "title": "Generative Adversarial Network for Medical Images (MI-GAN)", "comments": "Journal of Medical Systems", "journal-ref": "Med Syst (2018) 42: 231", "doi": "10.1007/s10916-018-1072-9", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms produces state-of-the-art results for different\nmachine learning and computer vision tasks. To perform well on a given task,\nthese algorithms require large dataset for training. However, deep learning\nalgorithms lack generalization and suffer from over-fitting whenever trained on\nsmall dataset, especially when one is dealing with medical images. For\nsupervised image analysis in medical imaging, having image data along with\ntheir corresponding annotated ground-truths is costly as well as time consuming\nsince annotations of the data is done by medical experts manually. In this\npaper, we propose a new Generative Adversarial Network for Medical Imaging\n(MI-GAN). The MI-GAN generates synthetic medical images and their segmented\nmasks, which can then be used for the application of supervised analysis of\nmedical images. Particularly, we present MI-GAN for synthesis of retinal\nimages. The proposed method generates precise segmented images better than the\nexisting techniques. The proposed model achieves a dice coefficient of 0.837 on\nSTARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance\non both the datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 06:59:37 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Iqbal", "Talha", ""], ["Ali", "Hazrat", ""]]}, {"id": "1810.00589", "submitter": "Yi Zhou", "authors": "Yi Zhou, Yue Bai, Shuvra S. Bhattacharyya, Heikki Huttunen", "title": "Elastic Neural Networks for Classification", "comments": "2019 IEEE International Conference on Artificial Intelligence\n  Circuits and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a framework for improving the performance of any deep\nneural network that may suffer from vanishing gradients. To address the\nvanishing gradient issue, we study a framework, where we insert an intermediate\noutput branch after each layer in the computational graph and use the\ncorresponding prediction loss for feeding the gradient to the early layers. The\nframework - which we name Elastic network - is tested with several well-known\nnetworks on CIFAR10 and CIFAR100 datasets, and the experimental results show\nthat the proposed framework improves the accuracy on both shallow networks\n(e.g., MobileNet) and deep convolutional neural networks (e.g., DenseNet). We\nalso identify the types of networks where the framework does not improve the\nperformance and discuss the reasons. Finally, as a side product, the\ncomputational complexity of the resulting networks can be adjusted in an\nelastic manner by selecting the output branch according to current\ncomputational budget.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 09:15:42 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 08:41:32 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 08:20:48 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 06:26:05 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Zhou", "Yi", ""], ["Bai", "Yue", ""], ["Bhattacharyya", "Shuvra S.", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1810.00599", "submitter": "Hongfa Zhao", "authors": "Zhenzhou Shao, Hongfa Zhao, Jiexin Xie, Ying Qu, Yong Guan and Jindong\n  Tan", "title": "Unsupervised Trajectory Segmentation and Promoting of Multi-Modal\n  Surgical Demonstrations", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the efficiency of surgical trajectory segmentation for robot\nlearning in robot-assisted minimally invasive surgery, this paper presents a\nfast unsupervised method using video and kinematic data, followed by a\npromoting procedure to address the over-segmentation issue. Unsupervised deep\nlearning network, stacking convolutional auto-encoder, is employed to extract\nmore discriminative features from videos in an effective way. To further\nimprove the accuracy of segmentation, on one hand, wavelet transform is used to\nfilter out the noises existed in the features from video and kinematic data. On\nthe other hand, the segmentation result is promoted by identifying the adjacent\nsegments with no state transition based on the predefined similarity\nmeasurements. Extensive experiments on a public dataset JIGSAWS show that our\nmethod achieves much higher accuracy of segmentation than state-of-the-art\nmethods in the shorter time.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 09:59:53 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Shao", "Zhenzhou", ""], ["Zhao", "Hongfa", ""], ["Xie", "Jiexin", ""], ["Qu", "Ying", ""], ["Guan", "Yong", ""], ["Tan", "Jindong", ""]]}, {"id": "1810.00602", "submitter": "Shruti Tople", "authors": "Karan Grover, Shruti Tople, Shweta Shinde, Ranjita Bhagwan and\n  Ramachandran Ramjee", "title": "Privado: Practical and Secure DNN Inference with Enclaves", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers are extending support for trusted hardware primitives such as\nIntel SGX. Simultaneously, the field of deep learning is seeing enormous\ninnovation as well as an increase in adoption. In this paper, we ask a timely\nquestion: \"Can third-party cloud services use Intel SGX enclaves to provide\npractical, yet secure DNN Inference-as-a-service?\" We first demonstrate that\nDNN models executing inside enclaves are vulnerable to access pattern based\nattacks. We show that by simply observing access patterns, an attacker can\nclassify encrypted inputs with 97% and 71% attack accuracy for MNIST and\nCIFAR10 datasets on models trained to achieve 99% and 79% original accuracy\nrespectively. This motivates the need for PRIVADO, a system we have designed\nfor secure, easy-to-use, and performance efficient inference-as-a-service.\nPRIVADO is input-oblivious: it transforms any deep learning framework that is\nwritten in C/C++ to be free of input-dependent access patterns thus eliminating\nthe leakage. PRIVADO is fully-automated and has a low TCB: with zero developer\neffort, given an ONNX description of a model, it generates compact and\nenclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO\nincurs low performance overhead: we use PRIVADO with Torch framework and show\nits overhead to be 17.18% on average on 11 different contemporary neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 10:13:42 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 15:03:14 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Grover", "Karan", ""], ["Tople", "Shruti", ""], ["Shinde", "Shweta", ""], ["Bhagwan", "Ranjita", ""], ["Ramjee", "Ramachandran", ""]]}, {"id": "1810.00609", "submitter": "Anbumani Subramanian", "authors": "Adithya Subramanian, Anbumani Subramanian", "title": "One-Click Annotation with Guided Hierarchical Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The increase in data collection has made data annotation an interesting and\nvaluable task in the contemporary world. This paper presents a new methodology\nfor quickly annotating data using click-supervision and hierarchical object\ndetection. The proposed work is semi-automatic in nature where the task of\nannotations is split between the human and a neural network. We show that our\nimproved method of annotation reduces the time, cost and mental stress on a\nhuman annotator. The research also highlights how our method performs better\nthan the current approach in different circumstances such as variation in\nnumber of objects, object size and different datasets. Our approach also\nproposes a new method of using object detectors making it suitable for data\nannotation task. The experiment conducted on PASCAL VOC dataset revealed that\nannotation created from our approach achieves a mAP of 0.995 and a recall of\n0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean\naverage precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES\ndataset. The proposed framework is 3-4 times faster as compared to the standard\nannotation method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 10:41:35 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Subramanian", "Adithya", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "1810.00689", "submitter": "Cheolkon Jung", "authors": "Inyong Yun, Cheolkon Jung, Xinran Wang, Alfred O Hero, and Joongkyu\n  Kim", "title": "Part-Level Convolutional Neural Networks for Pedestrian Detection Using\n  Saliency and Boundary Box Alignment", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrians in videos have a wide range of appearances such as body poses,\nocclusions, and complex backgrounds, and there exists the proposal shift\nproblem in pedestrian detection that causes the loss of body parts such as head\nand legs. To address it, we propose part-level convolutional neural networks\n(CNN) for pedestrian detection using saliency and boundary box alignment in\nthis paper. The proposed network consists of two sub-networks: detection and\nalignment. We use saliency in the detection sub-network to remove false\npositives such as lamp posts and trees. We adopt bounding box alignment on\ndetection proposals in the alignment sub-network to address the proposal shift\nproblem. First, we combine FCN and CAM to extract deep features for pedestrian\ndetection. Then, we perform part-level CNN to recall the lost body parts.\nExperimental results on various datasets demonstrate that the proposed method\nremarkably improves accuracy in pedestrian detection and outperforms existing\nstate-of-the-arts in terms of log average miss rate at false position per image\n(FPPI).\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 13:00:30 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Yun", "Inyong", ""], ["Jung", "Cheolkon", ""], ["Wang", "Xinran", ""], ["Hero", "Alfred O", ""], ["Kim", "Joongkyu", ""]]}, {"id": "1810.00729", "submitter": "Thomas Sch\\\"ops", "authors": "Thomas Sch\\\"ops, Torsten Sattler, Marc Pollefeys", "title": "SurfelMeshing: Online Surfel-Based Mesh Reconstruction", "comments": "Version accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2947048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of mesh reconstruction from live RGB-D video, assuming\na calibrated camera and poses provided externally (e.g., by a SLAM system). In\ncontrast to most existing approaches, we do not fuse depth measurements in a\nvolume but in a dense surfel cloud. We asynchronously (re)triangulate the\nsmoothed surfels to reconstruct a surface mesh. This novel approach enables to\nmaintain a dense surface representation of the scene during SLAM which can\nquickly adapt to loop closures. This is possible by deforming the surfel cloud\nand asynchronously remeshing the surface where necessary. The surfel-based\nrepresentation also naturally supports strongly varying scan resolution. In\nparticular, it reconstructs colors at the input camera's resolution. Moreover,\nin contrast to many volumetric approaches, ours can reconstruct thin objects\nsince objects do not need to enclose a volume. We demonstrate our approach in a\nnumber of experiments, showing that it produces reconstructions that are\ncompetitive with the state-of-the-art, and we discuss its advantages and\nlimitations. The algorithm (excluding loop closure functionality) is available\nas open source at https://github.com/puzzlepaint/surfelmeshing .\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:41:50 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 17:37:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Sch\u00f6ps", "Thomas", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1810.00736", "submitter": "Luigi Celona", "authors": "Simone Bianco, Remi Cadene, Luigi Celona, Paolo Napoletano", "title": "Benchmark Analysis of Representative Deep Neural Network Architectures", "comments": "Will appear in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2877890", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an in-depth analysis of the majority of the deep neural\nnetworks (DNNs) proposed in the state of the art for image recognition. For\neach DNN multiple performance indices are observed, such as recognition\naccuracy, model complexity, computational complexity, memory usage, and\ninference time. The behavior of such performance indices and some combinations\nof them are analyzed and discussed. To measure the indices we experiment the\nuse of DNNs on two different computer architectures, a workstation equipped\nwith a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson\nTX1 board. This experimentation allows a direct comparison between DNNs running\non machines with very different computational capacity. This study is useful\nfor researchers to have a complete view of what solutions have been explored so\nfar and in which research directions are worth exploring in the future; and for\npractitioners to select the DNN architecture(s) that better fit the resource\nconstraints of practical deployments and applications. To complete this work,\nall the DNNs, as well as the software used for the analysis, are available\nonline.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:48:18 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 15:39:24 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Bianco", "Simone", ""], ["Cadene", "Remi", ""], ["Celona", "Luigi", ""], ["Napoletano", "Paolo", ""]]}, {"id": "1810.00740", "submitter": "Chuanbiao Song", "authors": "Chuanbiao Song and Kun He and Liwei Wang and John E. Hopcroft", "title": "Improving the Generalization of Adversarial Training with Domain\n  Adaptation", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By injecting adversarial examples into training data, adversarial training is\npromising for improving the robustness of deep learning models. However, most\nexisting adversarial training approaches are based on a specific type of\nadversarial attack. It may not provide sufficiently representative samples from\nthe adversarial domain, leading to a weak generalization ability on adversarial\nexamples from other attacks. Moreover, during the adversarial training,\nadversarial perturbations on inputs are usually crafted by fast single-step\nadversaries so as to scale to large datasets. This work is mainly focused on\nthe adversarial training yet efficient FGSM adversary. In this scenario, it is\ndifficult to train a model with great generalization due to the lack of\nrepresentative adversarial samples, aka the samples are unable to accurately\nreflect the adversarial domain. To alleviate this problem, we propose a novel\nAdversarial Training with Domain Adaptation (ATDA) method. Our intuition is to\nregard the adversarial training on FGSM adversary as a domain adaption task\nwith limited number of target domain samples. The main idea is to learn a\nrepresentation that is semantically meaningful and domain invariant on the\nclean domain as well as the adversarial domain. Empirical evaluations on\nFashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly\nimprove the generalization of adversarial training and the smoothness of the\nlearned models, and outperforms state-of-the-art methods on standard benchmark\ndatasets. To show the transfer ability of our method, we also extend ATDA to\nthe adversarial training on iterative attacks such as PGD-Adversial Training\n(PAT) and the defense performance is improved considerably.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:52:08 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 09:00:02 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2018 13:29:39 GMT"}, {"version": "v4", "created": "Mon, 10 Dec 2018 08:43:35 GMT"}, {"version": "v5", "created": "Thu, 17 Jan 2019 05:13:22 GMT"}, {"version": "v6", "created": "Mon, 11 Mar 2019 11:22:56 GMT"}, {"version": "v7", "created": "Fri, 15 Mar 2019 08:37:29 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Song", "Chuanbiao", ""], ["He", "Kun", ""], ["Wang", "Liwei", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1810.00746", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Mario Fritz, Bernt Schiele", "title": "Bayesian Prediction of Future Street Scenes using Synthetic Likelihoods", "comments": "To appear in ICLR 2019. arXiv admin note: text overlap with\n  arXiv:1806.06939", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous agents to successfully operate in the real world, the ability\nto anticipate future scene states is a key competence. In real-world scenarios,\nfuture states become increasingly uncertain and multi-modal, particularly on\nlong time horizons. Dropout based Bayesian inference provides a computationally\ntractable, theoretically well grounded approach to learn likely\nhypotheses/models to deal with uncertain futures and make predictions that\ncorrespond well to observations -- are well calibrated. However, it turns out\nthat such approaches fall short to capture complex real-world scenes, even\nfalling behind in accuracy when compared to the plain deterministic approaches.\nThis is because the used log-likelihood estimate discourages diversity. In this\nwork, we propose a novel Bayesian formulation for anticipating future scene\nstates which leverages synthetic likelihoods that encourage the learning of\ndiverse models to accurately capture the multi-modal nature of future scene\nstates. We show that our approach achieves accurate state-of-the-art\npredictions and calibrated probabilities through extensive experiments for\nscene anticipation on Cityscapes dataset. Moreover, we show that our approach\ngeneralizes across diverse tasks such as digit generation and precipitation\nforecasting.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 15:02:54 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 14:19:54 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 08:09:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1810.00797", "submitter": "Bo Jiang", "authors": "Bo Jiang, Doudou Lin, Jin Tang", "title": "Graph Diffusion-Embedding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel graph diffusion-embedding networks (GDEN) for graph\nstructured data. GDEN is motivated by our closed-form formulation on\nregularized feature diffusion on graph. GDEN integrates both regularized\nfeature diffusion and low-dimensional embedding simultaneously in a unified\nnetwork model. Moreover, based on GDEN, we can naturally deal with structured\ndata with multiple graph structures. Experiments on semi-supervised learning\ntasks on several benchmark datasets demonstrate the better performance of the\nproposed GDEN when comparing with the traditional GCN models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 16:27:55 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Jiang", "Bo", ""], ["Lin", "Doudou", ""], ["Tang", "Jin", ""]]}, {"id": "1810.00818", "submitter": "Max Schwarz", "authors": "Max Schwarz, Anton Milan, Arul Selvam Periyasamy, Sven Behnke", "title": "RGB-D Object Detection and Semantic Segmentation for Autonomous\n  Manipulation in Clutter", "comments": null, "journal-ref": "International Journal of Robotics Research 37(4-5): 437-451 (2018)", "doi": "10.1177/0278364917713117", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robotic manipulation in clutter is challenging. A large variety of\nobjects must be perceived in complex scenes, where they are partially occluded\nand embedded among many distractors, often in restricted spaces. To tackle\nthese challenges, we developed a deep-learning approach that combines object\ndetection and semantic segmentation. The manipulation scenes are captured with\nRGB-D cameras, for which we developed a depth fusion method. Employing\npretrained features makes learning from small annotated robotic data sets\npossible. We evaluate our approach on two challenging data sets: one captured\nfor the Amazon Picking Challenge 2016, where our team NimbRo came in second in\nthe Stowing and third in the Picking task, and one captured in\ndisaster-response scenarios. The experiments show that object detection and\nsemantic segmentation complement each other and can be combined to yield\nreliable object perception.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 16:55:53 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Schwarz", "Max", ""], ["Milan", "Anton", ""], ["Periyasamy", "Arul Selvam", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.00826", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka", "title": "How Powerful are Graph Neural Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are an effective framework for representation\nlearning of graphs. GNNs follow a neighborhood aggregation scheme, where the\nrepresentation vector of a node is computed by recursively aggregating and\ntransforming representation vectors of its neighboring nodes. Many GNN variants\nhave been proposed and have achieved state-of-the-art results on both node and\ngraph classification tasks. However, despite GNNs revolutionizing graph\nrepresentation learning, there is limited understanding of their\nrepresentational properties and limitations. Here, we present a theoretical\nframework for analyzing the expressive power of GNNs to capture different graph\nstructures. Our results characterize the discriminative power of popular GNN\nvariants, such as Graph Convolutional Networks and GraphSAGE, and show that\nthey cannot learn to distinguish certain simple graph structures. We then\ndevelop a simple architecture that is provably the most expressive among the\nclass of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism\ntest. We empirically validate our theoretical findings on a number of graph\nclassification benchmarks, and demonstrate that our model achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 17:11:31 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 07:44:16 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 19:15:54 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Xu", "Keyulu", ""], ["Hu", "Weihua", ""], ["Leskovec", "Jure", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1810.00850", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof A. Bertram, Robert Klopfleisch, and Andreas\n  Maier", "title": "Augmented Mitotic Cell Count using Field Of Interest Proposal", "comments": "6 pages, submitted to BVM 2019 (bvm-workshop.org)", "journal-ref": "Bildverarbeitung f\\\"ur die Medizin 2019, pp. 321-326", "doi": "10.1007/978-3-658-25326-4_71", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological prognostication of neoplasia including most tumor grading\nsystems are based upon a number of criteria. Probably the most important is the\nnumber of mitotic figures which are most commonly determined as the mitotic\ncount (MC), i.e. number of mitotic figures within 10 consecutive high power\nfields. Often the area with the highest mitotic activity is to be selected for\nthe MC. However, since mitotic activity is not known in advance, an arbitrary\nchoice of this region is considered one important cause for high variability in\nthe prognostication and grading.\n  In this work, we present an algorithmic approach that first calculates a\nmitotic cell map based upon a deep convolutional network. This map is in a\nsecond step used to construct a mitotic activity estimate. Lastly, we select\nthe image segment representing the size of ten high power fields with the\noverall highest mitotic activity as a region proposal for an expert MC\ndetermination. We evaluate the approach using a dataset of 32 completely\nannotated whole slide images, where 22 were used for training of the network\nand 10 for test. We find a correlation of r=0.936 in mitotic count estimate.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 17:40:54 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1810.00871", "submitter": "Fakrul Islam Tushar", "authors": "Fakrul Islam Tushar", "title": "Automatic Skin Lesion Segmentation Using GrabCut in HSV Colour Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion segmentation is one of the first steps towards automatic\nComputer-Aided Diagnosis of skin cancer. Vast variety in the appearance of the\nskin lesion makes this task very challenging. The contribution of this paper is\nto apply a power foreground extraction technique called GrabCut for automatic\nskin lesion segmentation with minimal human interaction in HSV color space.\nPreprocessing was performed for removing the outer black border. Jaccard Index\nwas measured to evaluate the performance of the segmentation method. On\naverage, 0.71 Jaccard Index was achieved on 1000 images from ISIC challenge\n2017 Training Dataset.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 01:30:43 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Tushar", "Fakrul Islam", ""]]}, {"id": "1810.00912", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh", "title": "Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition", "comments": "18 pages, 10 figures, Oral Presentation in Conference on Robot\n  Learning (CoRL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an open-world setting, it is inevitable that an intelligent agent (e.g., a\nrobot) will encounter visual objects, attributes or relationships it does not\nrecognize. In this work, we develop an agent empowered with visual curiosity,\ni.e. the ability to ask questions to an Oracle (e.g., human) about the contents\nin images (e.g., What is the object on the left side of the red cube?) and\nbuild visual recognition model based on the answers received (e.g., Cylinder).\nIn order to do this, the agent must (1) understand what it recognizes and what\nit does not, (2) formulate a valid, unambiguous and informative language query\n(a question) to ask the Oracle, (3) derive the parameters of visual classifiers\nfrom the Oracle response and (4) leverage the updated visual classifiers to ask\nmore clarified questions. Specifically, we propose a novel framework and\nformulate the learning of visual curiosity as a reinforcement learning problem.\nIn this framework, all components of our agent, visual recognition module (to\nsee), question generation policy (to ask), answer digestion module (to\nunderstand) and graph memory module (to memorize), are learned entirely\nend-to-end to maximize the reward derived from the scene graph obtained by the\nagent as a consequence of the dialog with the Oracle. Importantly, the question\ngeneration policy is disentangled from the visual recognition system and\nspecifics of the environment. Consequently, we demonstrate a sort of double\ngeneralization. Our question generation policy generalizes to new environments\nand a new pair of eyes, i.e., new visual system. Trained on a synthetic\ndataset, our results show that our agent learns new visual concepts\nsignificantly faster than several heuristic baselines, even when tested on\nsynthetic environments with novel objects, as well as in a realistic\nenvironment.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:37:05 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Yang", "Jianwei", ""], ["Lu", "Jiasen", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1810.00953", "submitter": "Adam Oberman", "authors": "Chris Finlay, Adam Oberman, Bilal Abbasi", "title": "Improved robustness to adversarial examples using Lipschitz\n  regularization of the loss", "comments": "Merged with arXiv:1808.09540", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We augment adversarial training (AT) with worst case adversarial training\n(WCAT) which improves adversarial robustness by 11% over the current\nstate-of-the-art result in the $\\ell_2$ norm on CIFAR-10. We obtain verifiable\naverage case and worst case robustness guarantees, based on the expected and\nmaximum values of the norm of the gradient of the loss. We interpret\nadversarial training as Total Variation Regularization, which is a fundamental\ntool in mathematical image processing, and WCAT as Lipschitz regularization.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 20:02:00 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 16:08:46 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 16:01:04 GMT"}, {"version": "v4", "created": "Fri, 13 Sep 2019 14:56:57 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Finlay", "Chris", ""], ["Oberman", "Adam", ""], ["Abbasi", "Bilal", ""]]}, {"id": "1810.00965", "submitter": "Rados{\\l}aw Kycia", "authors": "R. A. Kycia, Z. Tabor", "title": "Natural measures of alignment", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural coordinate system will be proposed. In this coordinate system\nalignment procedure of a device and a detector can be easily performed. This\napproach is generalization of previous specific formulas in the field of\ncalibration and provide top level description of the procedure. A basic example\napplication to linac therapy plan is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 20:37:07 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Kycia", "R. A.", ""], ["Tabor", "Z.", ""]]}, {"id": "1810.00986", "submitter": "Janne Mustaniemi", "authors": "Janne Mustaniemi, Juho Kannala, Simo S\\\"arkk\\\"a, Jiri Matas, Janne\n  Heikkil\\\"a", "title": "Gyroscope-Aided Motion Deblurring with Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deblurring method that incorporates gyroscope measurements into\na convolutional neural network (CNN). With the help of such measurements, it\ncan handle extremely strong and spatially-variant motion blur. At the same\ntime, the image data is used to overcome the limitations of gyro-based blur\nestimation. To train our network, we also introduce a novel way of generating\nrealistic training data using the gyroscope. The evaluation shows a clear\nimprovement in visual quality over the state-of-the-art while achieving\nreal-time performance. Furthermore, the method is shown to improve the\nperformance of existing feature detectors and descriptors against the motion\nblur.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 21:32:59 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 13:44:09 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Mustaniemi", "Janne", ""], ["Kannala", "Juho", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Matas", "Jiri", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1810.01008", "submitter": "Martin Loncaric", "authors": "Martin Loncaric and Bowei Liu and Ryan Weber", "title": "Learning Hash Codes via Hamming Distance Targets", "comments": "8 pages, overhaul of our previous submission Convolutional Hashing\n  for Automated Scene Matching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful new loss function and training scheme for learning\nbinary hash codes with any differentiable model and similarity function. Our\nloss function improves over prior methods by using log likelihood loss on top\nof an accurate approximation for the probability that two inputs fall within a\nHamming distance target. Our novel training scheme obtains a good estimate of\nthe true gradient by better sampling inputs and evaluating loss terms between\nall pairs of inputs in each minibatch. To fully leverage the resulting hashes,\nwe use multi-indexing. We demonstrate that these techniques provide large\nimprovements to a similarity search tasks. We report the best results to date\non competitive information retrieval tasks for ImageNet and SIFT 1M, improving\nMAP from 73% to 84% and reducing query cost by a factor of 2-8, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 23:03:27 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Loncaric", "Martin", ""], ["Liu", "Bowei", ""], ["Weber", "Ryan", ""]]}, {"id": "1810.01011", "submitter": "Shing Yan Loo", "authors": "Shing Yan Loo, Ali Jahani Amiri, Syamsiah Mashohor, Sai Hong Tang,\n  Hong Zhang", "title": "CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using\n  Single-Image Depth Prediction", "comments": "6 pages, 5 figures, submitted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable feature correspondence between frames is a critical step in visual\nodometry (VO) and visual simultaneous localization and mapping (V-SLAM)\nalgorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct\nvisual odometry (SVO) has two main advantages that lead to state-of-the-art\nframe rate camera motion estimation: direct pixel correspondence and efficient\nimplementation of probabilistic mapping method. This paper improves the SVO\nmapping by initializing the mean and the variance of the depth at a feature\nlocation according to the depth prediction from a single-image depth prediction\nnetwork. By significantly reducing the depth uncertainty of the initialized map\npoint (i.e., small variance centred about the depth prediction), the benefits\nare twofold: reliable feature correspondence between views and fast convergence\nto the true depth in order to create new map points. We evaluate our method\nwith two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The\nexperimental results indicate that the improved SVO mapping results in\nincreased robustness and camera tracking accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 23:20:16 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Loo", "Shing Yan", ""], ["Amiri", "Ali Jahani", ""], ["Mashohor", "Syamsiah", ""], ["Tang", "Sai Hong", ""], ["Zhang", "Hong", ""]]}, {"id": "1810.01032", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Yang Liu, Bo Li", "title": "Reinforcement Learning with Perturbed Rewards", "comments": "AAAI 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that reinforcement learning (RL) models are\nvulnerable in various noisy scenarios. For instance, the observed reward\nchannel is often subject to noise in practice (e.g., when rewards are collected\nthrough sensors), and is therefore not credible. In addition, for applications\nsuch as robotics, a deep reinforcement learning (DRL) algorithm can be\nmanipulated to produce arbitrary errors by receiving corrupted rewards. In this\npaper, we consider noisy RL problems with perturbed rewards, which can be\napproximated with a confusion matrix. We develop a robust RL framework that\nenables agents to learn in noisy environments where only perturbed rewards are\nobserved. Our solution framework builds on existing RL/DRL algorithms and\nfirstly addresses the biased noisy reward setting without any assumptions on\nthe true distribution (e.g., zero-mean Gaussian noise as made in previous\nworks). The core ideas of our solution include estimating a reward confusion\nmatrix and defining a set of unbiased surrogate rewards. We prove the\nconvergence and sample complexity of our approach. Extensive experiments on\ndifferent DRL platforms show that trained policies based on our estimated\nsurrogate reward can achieve higher expected rewards, and converge faster than\nexisting baselines. For instance, the state-of-the-art PPO algorithm is able to\nobtain 84.6% and 80.8% improvements on average score for five Atari games, with\nerror rates as 10% and 30% respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 01:43:45 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 15:47:23 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 22:19:26 GMT"}, {"version": "v4", "created": "Sat, 1 Feb 2020 21:15:52 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Jingkang", ""], ["Liu", "Yang", ""], ["Li", "Bo", ""]]}, {"id": "1810.01069", "submitter": "Zhengyi Luo", "authors": "Zhengyi Luo, Austin Small, Liam Dugan, Stephen Lane", "title": "Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing\n  Power Devices", "comments": "Accepted to The 11th International Conference on Machine Vision (ICMV\n  2018). Project site: https://zhengyiluo.github.io/projects/cloudchaser/", "journal-ref": null, "doi": "10.1117/12.2523087", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Internet of Things(IoT) devices, mobile phones, and robotic systems are often\ndenied the power of deep learning algorithms due to their limited computing\npower. However, to provide time-critical services such as emergency response,\nhome assistance, surveillance, etc, these devices often need real-time analysis\nof their camera data. This paper strives to offer a viable approach to\nintegrate high-performance deep learning-based computer vision algorithms with\nlow-resource and low-power devices by leveraging the computing power of the\ncloud. By offloading the computation work to the cloud, no dedicated hardware\nis needed to enable deep neural networks on existing low computing power\ndevices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the\npower of using cloud computing to perform real-time vision tasks. Furthermore,\nto reduce latency and improve real-time performance, compression algorithms are\nproposed and evaluated for streaming real-time video frames to the cloud.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 05:08:12 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 22:00:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Luo", "Zhengyi", ""], ["Small", "Austin", ""], ["Dugan", "Liam", ""], ["Lane", "Stephen", ""]]}, {"id": "1810.01074", "submitter": "Chakkrit Termritthikun", "authors": "Chakkrit Termritthikun, Surachet Kanprachar, Paisarn Muneesawang", "title": "NU-LiteNet: Mobile Landmark Recognition using Convolutional Neural\n  Networks", "comments": "6 pages, 7 figures, this paper presented to NVIDIA's GPU Technology\n  Conference (GTC 2017), San Jose McEnery Convention Center, San Jose, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The growth of high-performance mobile devices has resulted in more research\ninto on-device image recognition. The research problems are the latency and\naccuracy of automatic recognition, which remains obstacles to its real-world\nusage. Although the recently developed deep neural networks can achieve\naccuracy comparable to that of a human user, some of them still lack the\nnecessary latency. This paper describes the development of the architecture of\na new convolutional neural network model, NU-LiteNet. For this, SqueezeNet was\ndeveloped to reduce the model size to a degree suitable for smartphones. The\nmodel size of NU-LiteNet is therefore 2.6 times smaller than that of\nSqueezeNet. The recognition accuracy of NU-LiteNet also compared favorably with\nother recently developed deep neural networks, when experiments were conducted\non two standard landmark databases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 05:27:22 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Termritthikun", "Chakkrit", ""], ["Kanprachar", "Surachet", ""], ["Muneesawang", "Paisarn", ""]]}, {"id": "1810.01091", "submitter": "Sinem Aslan", "authors": "Sinem Aslan, Sebastiano Vascon, Marcello Pelillo", "title": "Ancient Coin Classification Using Graph Transduction Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the type of an ancient coin requires theoretical expertise and\nyears of experience in the field of numismatics. Our goal in this work is\nautomatizing this time consuming and demanding task by a visual classification\nframework. Specifically, we propose to model ancient coin image classification\nusing Graph Transduction Games (GTG). GTG casts the classification problem as a\nnon-cooperative game where the players (the coin images) decide their\nstrategies (class labels) according to the choices made by the others, which\nresults with a global consensus at the final labeling. Experiments are\nconducted on the only publicly available dataset which is composed of 180\nimages of 60 types of Roman coins. We demonstrate that our approach outperforms\nthe literature work on the same dataset with the classification accuracy of\n73.6% and 87.3% when there are one and two images per class in the training\nset, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 07:00:46 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Aslan", "Sinem", ""], ["Vascon", "Sebastiano", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1810.01104", "submitter": "Yang Zhong", "authors": "Yang Zhong, Vladimir Li, Ryuzo Okada, Atsuto Maki", "title": "Target Aware Network Adaptation for Efficient Representation Learning", "comments": "Accepted by the ECCV'18 Workshops (2nd International Workshop on\n  Compact and Efficient Feature Representation and Learning in Computer Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automatic network adaptation method that finds a\nConvNet structure well-suited to a given target task, e.g., image\nclassification, for efficiency as well as accuracy in transfer learning. We\ncall the concept target-aware transfer learning. Given only small-scale labeled\ndata, and starting from an ImageNet pre-trained network, we exploit a scheme of\nremoving its potential redundancy for the target task through iterative\noperations of filter-wise pruning and network optimization. The basic\nmotivation is that compact networks are on one hand more efficient and should\nalso be more tolerant, being less complex, against the risk of overfitting\nwhich would hinder the generalization of learned representations in the context\nof transfer learning. Further, unlike existing methods involving network\nsimplification, we also let the scheme identify redundant portions across the\nentire network, which automatically results in a network structure adapted to\nthe task at hand. We achieve this with a few novel ideas: (i) cumulative sum of\nactivation statistics for each layer, and (ii) a priority evaluation of pruning\nacross multiple layers. Experimental results by the method on five datasets\n(Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable\naccuracies over the related state-of-the-art techniques while enhancing the\ncomputational and storage efficiency of the transferred model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:01:48 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Zhong", "Yang", ""], ["Li", "Vladimir", ""], ["Okada", "Ryuzo", ""], ["Maki", "Atsuto", ""]]}, {"id": "1810.01108", "submitter": "Subhajit Chaudhury", "authors": "Subhajit Chaudhury, Daiki Kimura, Asim Munawar and Ryuki Tachibana", "title": "Injective State-Image Mapping facilitates Visual Adversarial Imitation\n  Learning", "comments": "Updated the paper to match with version accepted at IEEE MMSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing use of virtual autonomous agents in applications like games and\nentertainment demands better control policies for natural-looking movements and\nactions. Unlike the conventional approach of hard-coding motion routines, we\npropose a deep learning method for obtaining control policies by directly\nmimicking raw video demonstrations. Previous methods in this domain rely on\nextracting low-dimensional features from expert videos followed by a separate\nhand-crafted reward estimation step. We propose an imitation learning framework\nthat reduces the dependence on hand-engineered reward functions by jointly\nlearning the feature extraction and reward estimation steps using Generative\nAdversarial Networks (GANs). Our main contribution in this paper is to show\nthat under injective mapping between low-level joint state (angles and\nvelocities) trajectories and corresponding raw video stream, performing\nadversarial imitation learning on video demonstrations is equivalent to\nlearning from the state trajectories. Experimental results show that the\nproposed adversarial learning method from raw videos produces a similar\nperformance to state-of-the-art imitation learning techniques while frequently\noutperforming existing hand-crafted video imitation methods. Furthermore, we\nshow that our method can learn action policies by imitating video\ndemonstrations on YouTube with similar performance to learned agents from true\nreward signals. Please see the supplementary video submission at\nhttps://ibm.biz/BdzzNA.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:22:41 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 09:32:10 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Chaudhury", "Subhajit", ""], ["Kimura", "Daiki", ""], ["Munawar", "Asim", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1810.01109", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim\n  Hartley, Luc Van Gool", "title": "AI Benchmark: Running Deep Neural Networks on Android Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, the computational power of mobile devices such as\nsmartphones and tablets has grown dramatically, reaching the level of desktop\ncomputers available not long ago. While standard smartphone apps are no longer\na problem for them, there is still a group of tasks that can easily challenge\neven high-end devices, namely running artificial intelligence algorithms. In\nthis paper, we present a study of the current state of deep learning in the\nAndroid ecosystem and describe available frameworks, programming models and the\nlimitations of running AI on smartphones. We give an overview of the hardware\nacceleration resources available on four main mobile chipset platforms:\nQualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the\nreal-world performance results of different mobile SoCs collected with AI\nBenchmark that are covering all main existing hardware configurations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:24:09 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 08:09:24 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Chou", "William", ""], ["Wang", "Ke", ""], ["Wu", "Max", ""], ["Hartley", "Tim", ""], ["Van Gool", "Luc", ""]]}, {"id": "1810.01118", "submitter": "Giorgio Patrini", "authors": "Giorgio Patrini, Rianne van den Berg, Patrick Forr\\'e, Marcello\n  Carioni, Samarth Bhargav, Max Welling, Tim Genewein, Frank Nielsen", "title": "Sinkhorn AutoEncoders", "comments": "Accepted for oral presentation at UAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport offers an alternative to maximum likelihood for learning\ngenerative autoencoding models. We show that minimizing the p-Wasserstein\ndistance between the generator and the true data distribution is equivalent to\nthe unconstrained min-min optimization of the p-Wasserstein distance between\nthe encoder aggregated posterior and the prior in latent space, plus a\nreconstruction error. We also identify the role of its trade-off hyperparameter\nas the capacity of the generator: its Lipschitz constant. Moreover, we prove\nthat optimizing the encoder over any class of universal approximators, such as\ndeterministic neural networks, is enough to come arbitrarily close to the\noptimum. We therefore advertise this framework, which holds for any metric\nspace and prior, as a sweet-spot of current generative autoencoding objectives.\nWe then introduce the Sinkhorn auto-encoder (SAE), which approximates and\nminimizes the p-Wasserstein distance in latent space via backprogation through\nthe Sinkhorn algorithm. SAE directly works on samples, i.e. it models the\naggregated posterior as an implicit distribution, with no need for a\nreparameterization trick for gradients estimations. SAE is thus able to work\nwith different metric spaces and priors with minimal adaptations. We\ndemonstrate the flexibility of SAE on latent spaces with different geometries\nand priors and compare with other methods on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:43:08 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 07:21:35 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 02:04:33 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Patrini", "Giorgio", ""], ["Berg", "Rianne van den", ""], ["Forr\u00e9", "Patrick", ""], ["Carioni", "Marcello", ""], ["Bhargav", "Samarth", ""], ["Welling", "Max", ""], ["Genewein", "Tim", ""], ["Nielsen", "Frank", ""]]}, {"id": "1810.01140", "submitter": "Alexandre Araujo", "authors": "Alexandre Araujo, Benjamin Negrevergne, Yann Chevaleyre, Jamal Atif", "title": "Training compact deep learning models for video classification using\n  circulant matrices", "comments": "The 2nd Workshop on YouTube-8M Large-Scale Video Understanding, ECCV\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In real world scenarios, model accuracy is hardly the only factor to\nconsider. Large models consume more memory and are computationally more\nintensive, which makes them difficult to train and to deploy, especially on\nmobile devices. In this paper, we build on recent results at the crossroads of\nLinear Algebra and Deep Learning which demonstrate how imposing a structure on\nlarge weight matrices can be used to reduce the size of the model. We propose\nvery compact models for video classification based on state-of-the-art network\narchitectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then\nconduct thorough experiments using the large YouTube-8M video classification\ndataset. As we will show, the circulant DBoF embedding achieves an excellent\ntrade-off between size and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 09:45:15 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 08:40:40 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Araujo", "Alexandre", ""], ["Negrevergne", "Benjamin", ""], ["Chevaleyre", "Yann", ""], ["Atif", "Jamal", ""]]}, {"id": "1810.01151", "submitter": "Francis Engelmann", "authors": "Francis Engelmann, Theodora Kontogianni, Jonas Schult, Bastian Leibe", "title": "Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-11015-4_29", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep learning architecture which addresses the\nproblem of 3D semantic segmentation of unstructured point clouds. Compared to\nprevious work, we introduce grouping techniques which define point\nneighborhoods in the initial world space and the learned feature space.\nNeighborhoods are important as they allow to compute local or global point\nfeatures depending on the spatial extend of the neighborhood. Additionally, we\nincorporate dedicated loss functions to further structure the learned point\nfeature space: the pairwise distance loss and the centroid loss. We show how to\napply these mechanisms to the task of 3D semantic segmentation of point clouds\nand report state-of-the-art performance on indoor and outdoor datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 10:06:13 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 15:15:08 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Engelmann", "Francis", ""], ["Kontogianni", "Theodora", ""], ["Schult", "Jonas", ""], ["Leibe", "Bastian", ""]]}, {"id": "1810.01152", "submitter": "Pingbo Pan", "authors": "Pingbo Pan, Yan Yan, Tianbao Yang, Yi Yang", "title": "Learning Discriminators as Energy Networks in Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for structured prediction via adversarial\nlearning. Existing adversarial learning methods involve two separate networks,\ni.e., the structured prediction models and the discriminative models, in the\ntraining. The information captured by discriminative models complements that in\nthe structured prediction models, but few existing researches have studied on\nutilizing such information to improve structured prediction models at the\ninference stage. In this work, we propose to refine the predictions of\nstructured prediction models by effectively integrating discriminative models\ninto the prediction. Discriminative models are treated as energy-based models.\nSimilar to the adversarial learning, discriminative models are trained to\nestimate scores which measure the quality of predicted outputs, while\nstructured prediction models are trained to predict contrastive outputs with\nmaximal energy scores. In this way, the gradient vanishing problem is\nameliorated, and thus we are able to perform inference by following the ascent\ngradient directions of discriminative models to refine structured prediction\nmodels. The proposed method is able to handle a range of tasks, e.g.,\nmulti-label classification and image segmentation. Empirical results on these\ntwo tasks validate the effectiveness of our learning method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 10:06:32 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Pan", "Pingbo", ""], ["Yan", "Yan", ""], ["Yang", "Tianbao", ""], ["Yang", "Yi", ""]]}, {"id": "1810.01163", "submitter": "Bharath Bhushan Damodaran", "authors": "Bharath Bhushan Damodaran, R\\'emi Flamary, Viven Seguy, Nicolas Courty", "title": "An Entropic Optimal Transport Loss for Learning Deep Neural Networks\n  under Label Noise in Remote Sensing Images", "comments": "Under Consideration at Computer Vision and Image Understanding", "journal-ref": "Computer Vision and Image Understanding, Volume 191, 2020, 102863,\n  ISSN 1077-3142", "doi": "10.1016/j.cviu.2019.102863", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have established as a powerful tool for large scale\nsupervised classification tasks. The state-of-the-art performances of deep\nneural networks are conditioned to the availability of large number of\naccurately labeled samples. In practice, collecting large scale accurately\nlabeled datasets is a challenging and tedious task in most scenarios of remote\nsensing image analysis, thus cheap surrogate procedures are employed to label\nthe dataset. Training deep neural networks on such datasets with inaccurate\nlabels easily overfits to the noisy training labels and degrades the\nperformance of the classification tasks drastically. To mitigate this effect,\nwe propose an original solution with entropic optimal transportation. It allows\nto learn in an end-to-end fashion deep neural networks that are, to some\nextent, robust to inaccurately labeled samples. We empirically demonstrate on\nseveral remote sensing datasets, where both scene and pixel-based hyperspectral\nimages are considered for classification. Our method proves to be highly\ntolerant to significant amounts of label noise and achieves favorable results\nagainst state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 10:31:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Damodaran", "Bharath Bhushan", ""], ["Flamary", "R\u00e9mi", ""], ["Seguy", "Viven", ""], ["Courty", "Nicolas", ""]]}, {"id": "1810.01185", "submitter": "Alexandru Constantin Serban", "authors": "Alexandru Constantin Serban, Erik Poll, Joost Visser", "title": "Adversarial Examples - A Complete Characterisation of the Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete characterisation of the phenomenon of adversarial\nexamples - inputs intentionally crafted to fool machine learning models. We aim\nto cover all the important concerns in this field of study: (1) the conjectures\non the existence of adversarial examples, (2) the security, safety and\nrobustness implications, (3) the methods used to generate and (4) protect\nagainst adversarial examples and (5) the ability of adversarial examples to\ntransfer between different machine learning models. We provide ample background\ninformation in an effort to make this document self-contained. Therefore, this\ndocument can be used as survey, tutorial or as a catalog of attacks and\ndefences using adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 11:54:51 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 21:48:42 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Serban", "Alexandru Constantin", ""], ["Poll", "Erik", ""], ["Visser", "Joost", ""]]}, {"id": "1810.01193", "submitter": "Sebastiano Vascon Mr", "authors": "Sebastiano Vascon, Ylenia Parin, Eis Annavini, Mattia D'Andola, Davide\n  Zoccolan, Marcello Pelillo", "title": "Characterization of Visual Object Representations in Rat Primary Visual\n  Cortex", "comments": "Accepted at Brain Driven Computer Vision Workshop at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most animal species, quick and reliable identification of visual objects\nis critical for survival. This applies also to rodents, which, in recent years,\nhave become increasingly popular models of visual functions. For this reason in\nthis work we analyzed how various properties of visual objects are represented\nin rat primary visual cortex (V1). The analysis has been carried out through\nsupervised (classification) and unsupervised (clustering) learning methods. We\nassessed quantitatively the discrimination capabilities of V1 neurons by\ndemonstrating how photometric properties (luminosity and object position in the\nscene) can be derived directly from the neuronal responses.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 12:08:32 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Vascon", "Sebastiano", ""], ["Parin", "Ylenia", ""], ["Annavini", "Eis", ""], ["D'Andola", "Mattia", ""], ["Zoccolan", "Davide", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1810.01256", "submitter": "Yang Chen", "authors": "Guanxiong Zeng, Yang Chen, Bo Cui, Shan Yu", "title": "Continual Learning of Context-dependent Processing in Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1038/s42256-019-0080-x", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 09:45:08 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 15:36:51 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 13:38:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zeng", "Guanxiong", ""], ["Chen", "Yang", ""], ["Cui", "Bo", ""], ["Yu", "Shan", ""]]}, {"id": "1810.01271", "submitter": "Jing Zhang", "authors": "Jing Zhang and Yonggong Ren", "title": "Marrying Tracking with ELM: A Metric Constraint Guided Multiple Feature\n  Fusion Method", "comments": "arXiv admin note: substantial text overlap with arXiv:1807.10211", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Tracking is one important problem in computer vision and surveillance\nsystem. The existing models mainly exploit the single-view feature (i.e. color,\ntexture, shape) to solve the problem, failing to describe the objects\ncomprehensively. In this paper, we solve the problem from multi-view\nperspective by leveraging multi-view complementary and latent information, so\nas to be robust to the partial occlusion and background clutter especially when\nthe objects are similar to the target, meanwhile addressing tracking drift.\nHowever, one big problem is that multi-view fusion strategy can inevitably\nresult tracking into non-efficiency. To this end, we propose to marry ELM\n(Extreme learning machine) to multi-view fusion to train the global hidden\noutput weight, to effectively exploit the local information from each view.\nFollowing this principle, we propose a novel method to obtain the optimal\nsample as the target object, which avoids tracking drift resulting from noisy\nsamples. Our method is evaluated over 12 challenge image sequences challenged\nwith different attributes including illumination, occlusion, deformation, etc.,\nwhich demonstrates better performance than several state-of-the-art methods in\nterms of effectiveness and robustness.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 13:43:12 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 13:16:22 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zhang", "Jing", ""], ["Ren", "Yonggong", ""]]}, {"id": "1810.01325", "submitter": "Sandra Aigner", "authors": "Sandra Aigner and Marco K\\\"orner", "title": "FutureGAN: Anticipating the Future Frames of Video Sequences using\n  Spatio-Temporal 3d Convolutions in Progressively Growing GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new encoder-decoder GAN model, FutureGAN, that predicts future\nframes of a video sequence conditioned on a sequence of past frames. During\ntraining, the networks solely receive the raw pixel values as an input, without\nrelying on additional constraints or dataset specific conditions. To capture\nboth the spatial and temporal components of a video sequence, spatio-temporal\n3d convolutions are used in all encoder and decoder modules. Further, we\nutilize concepts of the existing progressively growing GAN (PGGAN) that\nachieves high-quality results on generating high-resolution single images. The\nFutureGAN model extends this concept to the complex task of video prediction.\nWe conducted experiments on three different datasets, MovingMNIST, KTH Action,\nand Cityscapes. Our results show that the model learned representations to\ntransform the information of an input sequence into a plausible future sequence\neffectively for all three datasets. The main advantage of the FutureGAN\nframework is that it is applicable to various different datasets without\nadditional changes, whilst achieving stable results that are competitive to the\nstate-of-the-art in video prediction. Our code is available at\nhttps://github.com/TUM-LMF/FutureGAN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 15:30:25 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 16:14:45 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Aigner", "Sandra", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1810.01365", "submitter": "Neil Houlsby", "authors": "Ting Chen, Mario Lucic, Neil Houlsby, Sylvain Gelly", "title": "On Self Modulation for Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Generative Adversarial Networks (GANs) is notoriously challenging.\nWe propose and study an architectural modification, self-modulation, which\nimproves GAN performance across different data sets, architectures, losses,\nregularizers, and hyperparameter settings. Intuitively, self-modulation allows\nthe intermediate feature maps of a generator to change as a function of the\ninput noise vector. While reminiscent of other conditioning techniques, it\nrequires no labeled data. In a large-scale empirical study we observe a\nrelative decrease of $5\\%-35\\%$ in FID. Furthermore, all else being equal,\nadding this modification to the generator leads to improved performance in\n$124/144$ ($86\\%$) of the studied settings. Self-modulation is a simple\narchitectural change that requires no additional parameter tuning, which\nsuggests that it can be applied readily to any GAN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 16:50:28 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 07:20:50 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Chen", "Ting", ""], ["Lucic", "Mario", ""], ["Houlsby", "Neil", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1810.01367", "submitter": "Ricky T. Q. Chen", "authors": "Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever,\n  David Duvenaud", "title": "FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative\n  Models", "comments": "8 Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising class of generative models maps points from a simple distribution\nto a complex distribution through an invertible neural network.\nLikelihood-based training of these models requires restricting their\narchitectures to allow cheap computation of Jacobian determinants.\nAlternatively, the Jacobian trace can be used if the transformation is\nspecified by an ordinary differential equation. In this paper, we use\nHutchinson's trace estimator to give a scalable unbiased estimate of the\nlog-density. The result is a continuous-time invertible generative model with\nunbiased density estimation and one-pass sampling, while allowing unrestricted\nneural network architectures. We demonstrate our approach on high-dimensional\ndensity estimation, image generation, and variational inference, achieving the\nstate-of-the-art among exact likelihood methods with efficient sampling.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 16:56:37 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 15:28:48 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 17:56:45 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Grathwohl", "Will", ""], ["Chen", "Ricky T. Q.", ""], ["Bettencourt", "Jesse", ""], ["Sutskever", "Ilya", ""], ["Duvenaud", "David", ""]]}, {"id": "1810.01369", "submitter": "Wendong Mao", "authors": "Wendong Mao, Mingjie Wang, Jun Zhou, Minglun Gong", "title": "Semi-dense Stereo Matching using Dual CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust solution for semi-dense stereo matching is presented. It utilizes\ntwo CNN models for computing stereo matching cost and performing\nconfidence-based filtering, respectively. Compared to existing CNNs-based\nmatching cost generation approaches, our method feeds additional global\ninformation into the network so that the learned model can better handle\nchallenging cases, such as lighting changes and lack of textures. Through\nutilizing non-parametric transforms, our method is also more self-reliant than\nmost existing semi-dense stereo approaches, which rely highly on the adjustment\nof parameters. The experimental results based on Middlebury Stereo dataset\ndemonstrate that the proposed approach outperforms the state-of-the-art\nsemi-dense stereo approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 16:57:58 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Mao", "Wendong", ""], ["Wang", "Mingjie", ""], ["Zhou", "Jun", ""], ["Gong", "Minglun", ""]]}, {"id": "1810.01373", "submitter": "Mingjie Wang", "authors": "Mingjie Wang, Jun Zhou, Wendong Mao, Minglun Gong", "title": "Multi-scale Convolution Aggregation and Stochastic Feature Reuse for\n  DenseNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolution Neural Networks (CNNs) obtained huge success in\nnumerous vision tasks. In particular, DenseNets have demonstrated that feature\nreuse via dense skip connections can effectively alleviate the difficulty of\ntraining very deep networks and that reusing features generated by the initial\nlayers in all subsequent layers has strong impact on performance. To feed even\nricher information into the network, a novel adaptive Multi-scale Convolution\nAggregation module is presented in this paper. Composed of layers for\nmulti-scale convolutions, trainable cross-scale aggregation, maxout, and\nconcatenation, this module is highly non-linear and can boost the accuracy of\nDenseNet while using much fewer parameters. In addition, due to high model\ncomplexity, the network with extremely dense feature reuse is prone to\noverfitting. To address this problem, a regularization method named Stochastic\nFeature Reuse is also presented. Through randomly dropping a set of feature\nmaps to be reused for each mini-batch during the training phase, this\nregularization method reduces training costs and prevents co-adaptation.\nExperimental results on CIFAR-10, CIFAR-100 and SVHN benchmarks demonstrated\nthe effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:07:35 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Wang", "Mingjie", ""], ["Zhou", "Jun", ""], ["Mao", "Wendong", ""], ["Gong", "Minglun", ""]]}, {"id": "1810.01406", "submitter": "Ke Li", "authors": "Ke Li, Shichong Peng, Jitendra Malik", "title": "Super-Resolution via Conditional Implicit Maximum Likelihood Estimation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution (SISR) is a canonical problem with diverse\napplications. Leading methods like SRGAN produce images that contain various\nartifacts, such as high-frequency noise, hallucinated colours and shape\ndistortions, which adversely affect the realism of the result. In this paper,\nwe propose an alternative approach based on an extension of the method of\nImplicit Maximum Likelihood Estimation (IMLE). We demonstrate greater\neffectiveness at noise reduction and preservation of the original colours and\nshapes, yielding more realistic super-resolved images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:58:02 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Malik", "Jitendra", ""]]}, {"id": "1810.01423", "submitter": "J\\'er\\'emy Lebreton", "authors": "Roland Brochard, J\\'er\\'emy Lebreton, Cyril Robin, Keyvan Kanani,\n  Gr\\'egory Jonniaux, Aurore Masson, Noela Despr\\'e, Ahmad Berjaoui", "title": "Scientific image rendering for space scenes with the SurRender software", "comments": "11 pages, 10 figures, 69th International Astronautical Congress\n  (IAC), Bremen, Germany, 1-5 October 2018,\n  https://www.airbus.com/space/space-exploration/SurRenderSoftware.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spacecraft autonomy can be enhanced by vision-based navigation (VBN)\ntechniques. Applications range from manoeuvers around Solar System objects and\nlanding on planetary surfaces, to in-orbit servicing or space debris removal.\nThe development and validation of VBN algorithms relies on the availability of\nphysically accurate relevant images. Yet archival data from past missions can\nrarely serve this purpose and acquiring new data is often costly. The SurRender\nsoftware is an image simulator that addresses the challenges of realistic image\nrendering, with high representativeness for space scenes. Images are rendered\nby raytracing, which implements the physical principles of geometrical light\npropagation, in physical units. A macroscopic instrument model and scene\nobjects reflectance functions are used. SurRender is specially optimized for\nspace scenes, with huge distances between objects and scenes up to Solar System\nsize. Raytracing conveniently tackles some important effects for VBN\nalgorithms: image quality, eclipses, secondary illumination, subpixel limb\nimaging, etc. A simulation is easily setup (in MATLAB, Python, and more) by\nspecifying the position of the bodies (camera, Sun, planets, satellites) over\ntime, 3D shapes and material surface properties. SurRender comes with its own\nmodelling tool enabling to go beyond existing models for shapes, materials and\nsensors (projection, temporal sampling, electronics, etc.). It is natively\ndesigned to simulate different kinds of sensors (visible, LIDAR, etc.). Tools\nare available for manipulating huge datasets to store albedo maps and digital\nelevation models, or for procedural (fractal) texturing that generates\nhigh-quality images for a large range of observing distances (from millions of\nkm to touchdown). We illustrate SurRender performances with a selection of case\nstudies, placing particular emphasis on a 900-km Moon flyby simulation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 18:00:02 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Brochard", "Roland", ""], ["Lebreton", "J\u00e9r\u00e9my", ""], ["Robin", "Cyril", ""], ["Kanani", "Keyvan", ""], ["Jonniaux", "Gr\u00e9gory", ""], ["Masson", "Aurore", ""], ["Despr\u00e9", "Noela", ""], ["Berjaoui", "Ahmad", ""]]}, {"id": "1810.01455", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "Representation Flow for Action Recognition", "comments": "CVPR 2019", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a convolutional layer inspired by optical flow\nalgorithms to learn motion representations. Our representation flow layer is a\nfully-differentiable layer designed to capture the `flow' of any representation\nchannel within a convolutional neural network for action recognition. Its\nparameters for iterative flow optimization are learned in an end-to-end fashion\ntogether with the other CNN model parameters, maximizing the action recognition\nperformance. Furthermore, we newly introduce the concept of learning `flow of\nflow' representations by stacking multiple representation flow layers. We\nconducted extensive experimental evaluations, confirming its advantages over\nprevious recognition models using traditional optical flows in both\ncomputational speed and performance. Code/models available here:\nhttps://piergiaj.github.io/rep-flow-site/\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 18:57:45 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 16:50:19 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 18:51:22 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1810.01483", "submitter": "Jo\\~ao Caldeira", "authors": "Jo\\~ao Caldeira, W. L. Kimmy Wu, Brian Nord, Camille Avestruz,\n  Shubhendu Trivedi, Kyle T. Story", "title": "DeepCMB: Lensing Reconstruction of the Cosmic Microwave Background with\n  Deep Neural Networks", "comments": "19 pages; LaTeX; 12 figures; changes to match published version", "journal-ref": "Astronomy and Computing 28 100307 (2019)", "doi": "10.1016/j.ascom.2019.100307", "report-no": "FERMILAB-PUB-18-515-A-CD", "categories": "astro-ph.CO cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation cosmic microwave background (CMB) experiments will have lower\nnoise and therefore increased sensitivity, enabling improved constraints on\nfundamental physics parameters such as the sum of neutrino masses and the\ntensor-to-scalar ratio r. Achieving competitive constraints on these parameters\nrequires high signal-to-noise extraction of the projected gravitational\npotential from the CMB maps. Standard methods for reconstructing the lensing\npotential employ the quadratic estimator (QE). However, the QE performs\nsuboptimally at the low noise levels expected in upcoming experiments. Other\nmethods, like maximum likelihood estimators (MLE), are under active\ndevelopment. In this work, we demonstrate reconstruction of the CMB lensing\npotential with deep convolutional neural networks (CNN) - ie, a ResUNet. The\nnetwork is trained and tested on simulated data, and otherwise has no physical\nparametrization related to the physical processes of the CMB and gravitational\nlensing. We show that, over a wide range of angular scales, ResUNets recover\nthe input gravitational potential with a higher signal-to-noise ratio than the\nQE method, reaching levels comparable to analytic approximations of MLE\nmethods. We demonstrate that the network outputs quantifiably different lensing\nmaps when given input CMB maps generated with different cosmologies. We also\nshow we can use the reconstructed lensing map for cosmological parameter\nestimation. This application of CNN provides a few innovations at the\nintersection of cosmology and machine learning. First, while training and\nregressing on images, we predict a continuous-variable field rather than\ndiscrete classes. Second, we are able to establish uncertainty measures for the\nnetwork output that are analogous to standard methods. We expect this approach\nto excel in capturing hard-to-model non-Gaussian astrophysical foreground and\nnoise contributions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:04:07 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 14:56:24 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 22:33:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Caldeira", "Jo\u00e3o", ""], ["Wu", "W. L. Kimmy", ""], ["Nord", "Brian", ""], ["Avestruz", "Camille", ""], ["Trivedi", "Shubhendu", ""], ["Story", "Kyle T.", ""]]}, {"id": "1810.01544", "submitter": "Jungseock Joo", "authors": "Jungseock Joo, Zachary C. Steinert-Threlkeld", "title": "Image as Data: Automated Visual Content Analysis for Political Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image data provide unique information about political events, actors, and\ntheir interactions which are difficult to measure from or not available in text\ndata. This article introduces a new class of automated methods based on\ncomputer vision and deep learning which can automatically analyze visual\ncontent data. Scholars have already recognized the importance of visual data\nand a variety of large visual datasets have become available. The lack of\nscalable analytic methods, however, has prevented from incorporating large\nscale image data in political analysis. This article aims to offer an in-depth\noverview of automated methods for visual content analysis and explains their\nusages and implementations. We further elaborate on how these methods and\nresults can be validated and interpreted. We then discuss how these methods can\ncontribute to the study of political communication, identity and politics,\ndevelopment, and conflict, by enabling a new set of research questions at\nscale.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 00:11:55 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Joo", "Jungseock", ""], ["Steinert-Threlkeld", "Zachary C.", ""]]}, {"id": "1810.01562", "submitter": "Irwandi Hipiny", "authors": "Silvia Joseph, Irwandi Hipiny, Hamimah Ujir", "title": "Performance Evaluation of SIFT Descriptor against Common Image\n  Deformations on Iban Plaited Mat Motifs", "comments": "14th International Borneo Research Council Conference, 6 to 8 August\n  2018, UNIMAS, Sarawak", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Borneo indigenous communities are blessed with rich craft heritage. One such\nexamples is the Iban's plaited mat craft. There have been many efforts by\nUNESCO and the Sarawak Government to preserve and promote the craft. One such\nmethod is by developing a mobile app capable of recognising the different mat\nmotifs. As a first step towards this aim, we presents a novel image dataset\nconsisting of seven mat motif classes. Each class possesses a unique variation\nof chevrons, diagonal shapes, symmetrical, repetitive, geometric and non\ngeometric patterns. In this study, the performance of the Scale invariant\nfeature transform (SIFT) descriptor is evaluated against five common image\ndeformations, i.e., zoom and rotation, viewpoint, image blur, JPEG compression\nand illumination. Using our dataset, SIFT performed favourably with test\nsequences belonging to Illumination changes, Viewpoint changes, JPEG\ncompression and Zoom and Rotation. However, it did not performed well with\nImage blur test sequences with an average of 1.61 percents retained pairwise\nmatching after blurring with a Gaussian kernel of 8.0 radius.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 01:43:12 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Joseph", "Silvia", ""], ["Hipiny", "Irwandi", ""], ["Ujir", "Hamimah", ""]]}, {"id": "1810.01564", "submitter": "Irwandi Hipiny", "authors": "Faustine John, Irwandi Hipiny, Hamimah Ujir and Mohd Shahrizal Sunar", "title": "Assessing Performance of Aerobic Routines using Background Subtraction\n  and Intersected Image Region", "comments": "Presented at The International UNIMAS STEM Engineering Conference\n  2018 (ENCON2018). Accepted for publication in MATEC Web of Conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is recommended for a novice to engage a trained and experience person,\ni.e., a coach before starting an unfamiliar aerobic or weight routine. The\ncoach's task is to provide real-time feedbacks to ensure that the routine is\nperformed in a correct manner. This greatly reduces the risk of injury and\nmaximise physical gains. We present a simple image similarity measure based on\nintersected image region to assess a subject's performance of an aerobic\nroutine. The method is implemented inside an Augmented Reality (AR) desktop app\nthat employs a single RGB camera to capture still images of the subject as he\nor she progresses through the routine. The background-subtracted body pose\nimage is compared against the exemplar body pose image (i.e., AR template) at\nspecific intervals. Based on a limited dataset, our pose matching function is\nreported to have an accuracy of 93.67%.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 02:04:15 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["John", "Faustine", ""], ["Hipiny", "Irwandi", ""], ["Ujir", "Hamimah", ""], ["Sunar", "Mohd Shahrizal", ""]]}, {"id": "1810.01575", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Guandao Yang, Aditya Prakash, Qiuren Fang, Hanqing\n  Jiang, Bharath Hariharan, Serge Belongie", "title": "Deep Fundamental Matrix Estimation without Correspondences", "comments": "ECCV 2018, Geometry Meets Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating fundamental matrices is a classic problem in computer vision.\nTraditional methods rely heavily on the correctness of estimated key-point\ncorrespondences, which can be noisy and unreliable. As a result, it is\ndifficult for these methods to handle image pairs with large occlusion or\nsignificantly different camera poses. In this paper, we propose novel neural\nnetwork architectures to estimate fundamental matrices in an end-to-end manner\nwithout relying on point correspondences. New modules and layers are introduced\nin order to preserve mathematical properties of the fundamental matrix as a\nhomogeneous rank-2 matrix with seven degrees of freedom. We analyze performance\nof the proposed models using various metrics on the KITTI dataset, and show\nthat they achieve competitive performance with traditional methods without the\nneed for extracting correspondences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 03:59:15 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Poursaeed", "Omid", ""], ["Yang", "Guandao", ""], ["Prakash", "Aditya", ""], ["Fang", "Qiuren", ""], ["Jiang", "Hanqing", ""], ["Hariharan", "Bharath", ""], ["Belongie", "Serge", ""]]}, {"id": "1810.01604", "submitter": "Duanshun Li Dr.", "authors": "Duanshun Li and Chen Feng", "title": "Primitive Fitting Using Deep Boundary Aware Geometric Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify and fit geometric primitives (e.g., planes, spheres, cylinders,\ncones) in a noisy point cloud is a challenging yet beneficial task for fields\nsuch as robotics and reverse engineering. As a multi-model multi-instance\nfitting problem, it has been tackled with different approaches including\nRANSAC, which however often fit inferior models in practice with noisy inputs\nof cluttered scenes. Inspired by the corresponding human recognition process,\nand benefiting from the recent advancements in image semantic segmentation\nusing deep neural networks, we propose BAGSFit as a new framework addressing\nthis problem. Firstly, through a fully convolutional neural network, the input\npoint cloud is point-wisely segmented into multiple classes divided by jointly\ndetected instance boundaries without any geometric fitting. Thus, segments can\nserve as primitive hypotheses with a probability estimation of associating\nprimitive classes. Finally, all hypotheses are sent through a geometric\nverification to correct any misclassification by fitting primitives\nrespectively. We performed training using simulated range images and tested it\nwith both simulated and real-world point clouds. Quantitative and qualitative\nexperiments demonstrated the superiority of BAGSFit.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 06:50:51 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Li", "Duanshun", ""], ["Feng", "Chen", ""]]}, {"id": "1810.01616", "submitter": "Sungeun Hong", "authors": "Sungeun Hong, Wonjin Jung, Ilsang Woo, Seung Wook Kim", "title": "Cascaded Pyramid Network for 3D Human Pose Estimation Challenge", "comments": "Accepted to ECCV Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, there has been a growing interest in human pose\nestimation. Although much work has been done on 2D pose estimation, 3D pose\nestimation has still been relatively studied less. In this paper, we propose a\ntop-bottom based two-stage 3D estimation framework. GloabalNet and RefineNet in\nour 2D pose estimation process enable us to find occluded or invisible 2D\njoints while 2D-to-3D pose estimator composed of residual blocks is used to\nlift 2D joints to 3D joints effectively. The proposed method achieves promising\nresults with mean per joint position error at 42.39 on the validation dataset\non `3D Human Pose Estimation within the ECCV 2018 PoseTrack Challenge.'\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 07:58:18 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Hong", "Sungeun", ""], ["Jung", "Wonjin", ""], ["Woo", "Ilsang", ""], ["Kim", "Seung Wook", ""]]}, {"id": "1810.01620", "submitter": "Wendi Xu", "authors": "Wendi Xu, Ming Zhang", "title": "Towards WARSHIP: Combining Components of Brain-Inspired Computing of RSH\n  for Image Super Resolution", "comments": "2018 5th IEEE International Conference on Cloud Computing and\n  Intelligence Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution of deep learning shows that some algorithmic tricks are more\ndurable , while others are not. To the best of our knowledge, we firstly\nsummarize 5 more durable and complete deep learning components for vision, that\nis, WARSHIP. Moreover, we give a biological overview of WARSHIP, emphasizing\nbrain-inspired computing of WARSHIP. As a step towards WARSHIP, our case study\nof image super resolution combines 3 components of RSH to deploy a CNN model of\nWARSHIP-XZNet, which performs a happy medium between speed and performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 08:10:03 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Xu", "Wendi", ""], ["Zhang", "Ming", ""]]}, {"id": "1810.01621", "submitter": "Bilwaj Gaonkar", "authors": "Bilwaj Gaonkar, Matthew Edwards, Alex Bui, Matthew Brown, Luke\n  Macyszyn", "title": "Extreme Augmentation : Can deep learning based medical image\n  segmentation be trained using a single manually delineated scan?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yes, it can. Data augmentation is perhaps the oldest preprocessing step in\ncomputer vision literature. Almost every computer vision model trained on\nimaging data uses some form of augmentation. In this paper, we use the\ninter-vertebral disk segmentation task alongside a deep residual U-Net as the\nlearning model, to explore the effectiveness of augmentation. In the extreme,\nwe observed that a model trained on patches extracted from just one scan, with\neach patch augmented 50 times; achieved a Dice score of 0.73 in a validation\nset of 40 cases. Qualitative evaluation indicated a clinically usable\nsegmentation algorithm, which appropriately segments regions of interest,\nalongside limited false positive specks. When the initial patches are extracted\nfrom nine scans the average Dice coefficient jumps to 0.86 and most of the\nfalse positives disappear. While this still falls short of state-of-the-art\ndeep learning based segmentation of discs reported in literature, qualitative\nexamination reveals that it does yield segmentation, which can be amended by\nexpert clinicians with minimal effort to generate additional data for training\nimproved deep models. Extreme augmentation of training data, should thus be\nconstrued as a strategy for training deep learning based algorithms, when very\nlittle manually annotated data is available to work with. Models trained with\nextreme augmentation can then be used to accelerate the generation of manually\nlabelled data. Hence, we show that extreme augmentation can be a valuable tool\nin addressing scaling up small imaging data sets to address medical image\nsegmentation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 08:10:35 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 18:51:29 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 22:45:17 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Gaonkar", "Bilwaj", ""], ["Edwards", "Matthew", ""], ["Bui", "Alex", ""], ["Brown", "Matthew", ""], ["Macyszyn", "Luke", ""]]}, {"id": "1810.01622", "submitter": "Wendi Xu", "authors": "Wendi Xu, Ming Zhang", "title": "Theory of Generative Deep Learning : Probe Landscape of Empirical Error\n  via Norm Based Capacity Control", "comments": "2018 5th IEEE International Conference on Cloud Computing and\n  Intelligence Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its remarkable empirical success as a highly competitive branch of\nartificial intelligence, deep learning is often blamed for its widely known low\ninterpretation and lack of firm and rigorous mathematical foundation. However,\nmost theoretical endeavor is devoted in discriminative deep learning case,\nwhose complementary part is generative deep learning. To the best of our\nknowledge, we firstly highlight landscape of empirical error in generative case\nto complete the full picture through exquisite design of image super resolution\nunder norm based capacity control. Our theoretical advance in interpretation of\nthe training dynamic is achieved from both mathematical and biological sides.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 08:10:51 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Xu", "Wendi", ""], ["Zhang", "Ming", ""]]}, {"id": "1810.01638", "submitter": "Yibo Yang", "authors": "Huan Li, Yibo Yang, Dongmin Chen, Zhouchen Lin", "title": "Optimization Algorithm Inspired Deep Neural Network Structure Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been one of the dominant machine learning\napproaches in recent years. Several new network structures are proposed and\nhave better performance than the traditional feedforward neural network\nstructure. Representative ones include the skip connection structure in ResNet\nand the dense connection structure in DenseNet. However, it still lacks a\nunified guidance for the neural network structure design. In this paper, we\npropose the hypothesis that the neural network structure design can be inspired\nby optimization algorithms and a faster optimization algorithm may lead to a\nbetter neural network structure. Specifically, we prove that the propagation in\nthe feedforward neural network with the same linear transformation in different\nlayers is equivalent to minimizing some function using the gradient descent\nalgorithm. Based on this observation, we replace the gradient descent algorithm\nwith the heavy ball algorithm and Nesterov's accelerated gradient descent\nalgorithm, which are faster and inspire us to design new and better network\nstructures. ResNet and DenseNet can be considered as two special cases of our\nframework. Numerical experiments on CIFAR-10, CIFAR-100 and ImageNet verify the\nadvantage of our optimization algorithm inspired structures over ResNet and\nDenseNet.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 08:59:41 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Li", "Huan", ""], ["Yang", "Yibo", ""], ["Chen", "Dongmin", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1810.01641", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Radu Timofte, Thang Van Vu, Tung Minh Luu, Trung X\n  Pham, Cao Van Nguyen, Yongwoo Kim, Jae-Seok Choi, Munchurl Kim, Jie Huang,\n  Jiewen Ran, Chen Xing, Xingguang Zhou, Pengfei Zhu, Mingrui Geng, Yawei Li,\n  Eirikur Agustsson, Shuhang Gu, Luc Van Gool, Etienne de Stoutz, Nikolay\n  Kobyshev, Kehui Nie, Yan Zhao, Gen Li, Tong Tong, Qinquan Gao, Liu Hanwen,\n  Pablo Navarrete Michelini, Zhu Dan, Hu Fengshuo, Zheng Hui, Xiumei Wang,\n  Lirui Deng, Rang Meng, Jinghui Qin, Yukai Shi, Wushao Wen, Liang Lin,\n  Ruicheng Feng, Shixiang Wu, Chao Dong, Yu Qiao, Subeesh Vasu, Nimisha Thekke\n  Madam, Praveen Kandula, A. N. Rajagopalan, Jie Liu, Cheolkon Jung", "title": "PIRM Challenge on Perceptual Image Enhancement on Smartphones: Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the first challenge on efficient perceptual image\nenhancement with the focus on deploying deep learning models on smartphones.\nThe challenge consisted of two tracks. In the first one, participants were\nsolving the classical image super-resolution problem with a bicubic downscaling\nfactor of 4. The second track was aimed at real-world photo enhancement, and\nthe goal was to map low-quality photos from the iPhone 3GS device to the same\nphotos captured with a DSLR camera. The target metric used in this challenge\ncombined the runtime, PSNR scores and solutions' perceptual results measured in\nthe user study. To ensure the efficiency of the submitted models, we\nadditionally measured their runtime and memory requirements on Android\nsmartphones. The proposed solutions significantly improved baseline results\ndefining the state-of-the-art for image enhancement on smartphones.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 09:07:28 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Van Vu", "Thang", ""], ["Luu", "Tung Minh", ""], ["Pham", "Trung X", ""], ["Van Nguyen", "Cao", ""], ["Kim", "Yongwoo", ""], ["Choi", "Jae-Seok", ""], ["Kim", "Munchurl", ""], ["Huang", "Jie", ""], ["Ran", "Jiewen", ""], ["Xing", "Chen", ""], ["Zhou", "Xingguang", ""], ["Zhu", "Pengfei", ""], ["Geng", "Mingrui", ""], ["Li", "Yawei", ""], ["Agustsson", "Eirikur", ""], ["Gu", "Shuhang", ""], ["Van Gool", "Luc", ""], ["de Stoutz", "Etienne", ""], ["Kobyshev", "Nikolay", ""], ["Nie", "Kehui", ""], ["Zhao", "Yan", ""], ["Li", "Gen", ""], ["Tong", "Tong", ""], ["Gao", "Qinquan", ""], ["Hanwen", "Liu", ""], ["Michelini", "Pablo Navarrete", ""], ["Dan", "Zhu", ""], ["Fengshuo", "Hu", ""], ["Hui", "Zheng", ""], ["Wang", "Xiumei", ""], ["Deng", "Lirui", ""], ["Meng", "Rang", ""], ["Qin", "Jinghui", ""], ["Shi", "Yukai", ""], ["Wen", "Wushao", ""], ["Lin", "Liang", ""], ["Feng", "Ruicheng", ""], ["Wu", "Shixiang", ""], ["Dong", "Chao", ""], ["Qiao", "Yu", ""], ["Vasu", "Subeesh", ""], ["Madam", "Nimisha Thekke", ""], ["Kandula", "Praveen", ""], ["Rajagopalan", "A. N.", ""], ["Liu", "Jie", ""], ["Jung", "Cheolkon", ""]]}, {"id": "1810.01665", "submitter": "Lukas Hoyer", "authors": "Lukas Hoyer, Christoph Steup, Sanaz Mostaghim", "title": "A Robot Localization Framework Using CNNs for Object Detection and Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External localization is an essential part for the indoor operation of small\nor cost-efficient robots, as they are used, for example, in swarm robotics. We\nintroduce a two-stage localization and instance identification framework for\narbitrary robots based on convolutional neural networks. Object detection is\nperformed on an external camera image of the operation zone providing robot\nbounding boxes for an identification and orientation estimation convolutional\nneural network. Additionally, we propose a process to generate the necessary\ntraining data. The framework was evaluated with 3 different robot types and\nvarious identification patterns. We have analyzed the main framework\nhyperparameters providing recommendations for the framework operation settings.\nWe achieved up to 98% mAP@IOU0.5 and only 1.6{\\deg} orientation error, running\nwith a frame rate of 50 Hz on a GPU.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 09:58:21 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Hoyer", "Lukas", ""], ["Steup", "Christoph", ""], ["Mostaghim", "Sanaz", ""]]}, {"id": "1810.01732", "submitter": "Yung-Hsiang Lu", "authors": "Sergei Alyamkin, Matthew Ardi, Achille Brighton, Alexander C. Berg,\n  Yiran Chen, Hsin-Pai Cheng, Bo Chen, Zichen Fan, Chen Feng, Bo Fu, Kent\n  Gauen, Jongkook Go, Alexander Goncharenko, Xuyang Guo, Hong Hanh Nguyen,\n  Andrew Howard, Yuanjun Huang, Donghyun Kang, Jaeyoun Kim, Alexander\n  Kondratyev, Seungjae Lee, Suwoong Lee, Junhyeok Lee, Zhiyu Liang, Xin Liu,\n  Juzheng Liu, Zichao Li, Yang Lu, Yung-Hsiang Lu, Deeptanshu Malik, Eunbyung\n  Park, Denis Repin, Tao Sheng, Liang Shen, Fei Sun, David Svitov, George K.\n  Thiruvathukal, Baiwu Zhang, Jingchi Zhang, Xiaopeng Zhang, Shaojie Zhuo", "title": "2018 Low-Power Image Recognition Challenge", "comments": "13 pages, workshop in 2018 CVPR, competition, low-power, image\n  recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Low-Power Image Recognition Challenge (LPIRC,\nhttps://rebootingcomputing.ieee.org/lpirc) is an annual competition started in\n2015. The competition identifies the best technologies that can classify and\ndetect objects in images efficiently (short execution time and low energy\nconsumption) and accurately (high precision). Over the four years, the winners'\nscores have improved more than 24 times. As computer vision is widely used in\nmany battery-powered systems (such as drones and mobile phones), the need for\nlow-power computer vision will become increasingly important. This paper\nsummarizes LPIRC 2018 by describing the three different tracks and the winners'\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:34:52 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Alyamkin", "Sergei", ""], ["Ardi", "Matthew", ""], ["Brighton", "Achille", ""], ["Berg", "Alexander C.", ""], ["Chen", "Yiran", ""], ["Cheng", "Hsin-Pai", ""], ["Chen", "Bo", ""], ["Fan", "Zichen", ""], ["Feng", "Chen", ""], ["Fu", "Bo", ""], ["Gauen", "Kent", ""], ["Go", "Jongkook", ""], ["Goncharenko", "Alexander", ""], ["Guo", "Xuyang", ""], ["Nguyen", "Hong Hanh", ""], ["Howard", "Andrew", ""], ["Huang", "Yuanjun", ""], ["Kang", "Donghyun", ""], ["Kim", "Jaeyoun", ""], ["Kondratyev", "Alexander", ""], ["Lee", "Seungjae", ""], ["Lee", "Suwoong", ""], ["Lee", "Junhyeok", ""], ["Liang", "Zhiyu", ""], ["Liu", "Xin", ""], ["Liu", "Juzheng", ""], ["Li", "Zichao", ""], ["Lu", "Yang", ""], ["Lu", "Yung-Hsiang", ""], ["Malik", "Deeptanshu", ""], ["Park", "Eunbyung", ""], ["Repin", "Denis", ""], ["Sheng", "Tao", ""], ["Shen", "Liang", ""], ["Sun", "Fei", ""], ["Svitov", "David", ""], ["Thiruvathukal", "George K.", ""], ["Zhang", "Baiwu", ""], ["Zhang", "Jingchi", ""], ["Zhang", "Xiaopeng", ""], ["Zhuo", "Shaojie", ""]]}, {"id": "1810.01733", "submitter": "Alessio Tonioni", "authors": "Alessio Tonioni and Eugenio Serra and Luigi Di Stefano", "title": "A deep learning pipeline for product recognition on store shelves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of grocery products in store shelves poses peculiar challenges.\nFirstly, the task mandates the recognition of an extremely high number of\ndifferent items, in the order of several thousands for medium-small shops, with\nmany of them featuring small inter and intra class variability. Then, available\nproduct databases usually include just one or a few studio-quality images per\nproduct (referred to herein as reference images), whilst at test time\nrecognition is performed on pictures displaying a portion of a shelf containing\nseveral products and taken in the store by cheap cameras (referred to as query\nimages). Moreover, as the items on sale in a store as well as their appearance\nchange frequently over time, a practical recognition system should handle\nseamlessly new products/packages. Inspired by recent advances in object\ndetection and image retrieval, we propose to leverage on state of the art\nobject detectors based on deep learning to obtain an initial productagnostic\nitem detection. Then, we pursue product recognition through a similarity search\nbetween global descriptors computed on reference and cropped query images. To\nmaximize performance, we learn an ad-hoc global descriptor by a CNN trained on\nreference images based on an image embedding loss. Our system is\ncomputationally expensive at training time but can perform recognition rapidly\nand accurately at test time.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:36:26 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 16:18:40 GMT"}, {"version": "v3", "created": "Sun, 27 Jan 2019 08:52:17 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Tonioni", "Alessio", ""], ["Serra", "Eugenio", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1810.01771", "submitter": "Elham Saraee", "authors": "Elham Saraee, Mona Jalal, Margrit Betke", "title": "SAVOIAS: A Diverse, Multi-Category Visual Complexity Dataset", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual complexity identifies the level of intricacy and details in an image\nor the level of difficulty to describe the image. It is an important concept in\na variety of areas such as cognitive psychology, computer vision and\nvisualization, and advertisement. Yet, efforts to create large, downloadable\nimage datasets with diverse content and unbiased groundtruthing are lacking. In\nthis work, we introduce Savoias, a visual complexity dataset that compromises\nof more than 1,400 images from seven image categories relevant to the above\nresearch areas, namely Scenes, Advertisements, Visualization and infographics,\nObjects, Interior design, Art, and Suprematism. The images in each category\nportray diverse characteristics including various low-level and high-level\nfeatures, objects, backgrounds, textures and patterns, text, and graphics. The\nground truth for Savoias is obtained by crowdsourcing more than 37,000 pairwise\ncomparisons of images using the forced-choice methodology and with more than\n1,600 contributors. The resulting relative scores are then converted to\nabsolute visual complexity scores using the Bradley-Terry method and matrix\ncompletion. When applying five state-of-the-art algorithms to analyze the\nvisual complexity of the images in the Savoias dataset, we found that the\nscores obtained from these baseline tools only correlate well with crowdsourced\nlabels for abstract patterns in the Suprematism category (Pearson correlation\nr=0.84). For the other categories, in particular, the objects and advertisement\ncategories, low correlation coefficients were revealed (r=0.3 and 0.56,\nrespectively). These findings suggest that (1) state-of-the-art approaches are\nmostly insufficient and (2) Savoias enables category-specific method\ndevelopment, which is likely to improve the impact of visual complexity\nanalysis on specific application areas, including computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 14:34:37 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Saraee", "Elham", ""], ["Jalal", "Mona", ""], ["Betke", "Margrit", ""]]}, {"id": "1810.01829", "submitter": "Masayuki Tanaka", "authors": "Masayuki Tanaka", "title": "Weighted Sigmoid Gate Unit for an Activation Function of Deep Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An activation function has crucial role in a deep neural network.\n  A simple rectified linear unit (ReLU) are widely used for the activation\nfunction.\n  In this paper, a weighted sigmoid gate unit (WiG) is proposed as the\nactivation function.\n  The proposed WiG consists of a multiplication of inputs and the weighted\nsigmoid gate.\n  It is shown that the WiG includes the ReLU and same activation functions as a\nspecial case.\n  Many activation functions have been proposed to overcome the performance of\nthe ReLU.\n  In the literature, the performance is mainly evaluated with an object\nrecognition task.\n  The proposed WiG is evaluated with the object recognition task and the image\nrestoration task.\n  Then, the expeirmental comparisons demonstrate the proposed WiG overcomes the\nexisting activation functions including the ReLU.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:26:24 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Tanaka", "Masayuki", ""]]}, {"id": "1810.01831", "submitter": "Aiwen Jiang", "authors": "Kangfu Mei, Aiwen Jiang, Juncheng Li, Jihua Ye, Mingwen Wang", "title": "An Effective Single-Image Super-Resolution Model Using\n  Squeeze-and-Excitation Networks", "comments": "12 pages, accepted by ICONIP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on single-image super-resolution are concentrated on improving\nperformance through enhancing spatial encoding between convolutional layers. In\nthis paper, we focus on modeling the correlations between channels of\nconvolutional features. We present an effective deep residual network based on\nsqueeze-and-excitation blocks (SEBlock) to reconstruct high-resolution (HR)\nimage from low-resolution (LR) image. SEBlock is used to adaptively recalibrate\nchannel-wise feature mappings. Further, short connections between each SEBlock\nare used to remedy information loss. Extensive experiments show that our model\ncan achieve the state-of-the-art performance and get finer texture details.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:29:37 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Mei", "Kangfu", ""], ["Jiang", "Aiwen", ""], ["Li", "Juncheng", ""], ["Ye", "Jihua", ""], ["Wang", "Mingwen", ""]]}, {"id": "1810.01835", "submitter": "Lex Fridman", "authors": "Lex Fridman", "title": "Human-Centered Autonomous Vehicle Systems: Principles of Effective\n  Shared Autonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective, enjoyable, and safe autonomous vehicles is a lot harder\nthan has historically been considered. The reason is that, simply put, an\nautonomous vehicle must interact with human beings. This interaction is not a\nrobotics problem nor a machine learning problem nor a psychology problem nor an\neconomics problem nor a policy problem. It is all of these problems put into\none. It challenges our assumptions about the limitations of human beings at\ntheir worst and the capabilities of artificial intelligence systems at their\nbest. This work proposes a set of principles for designing and building\nautonomous vehicles in a human-centered way that does not run away from the\ncomplexity of human nature but instead embraces it. We describe our development\nof the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study\nof implementing these principles in practice.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:36:22 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Fridman", "Lex", ""]]}, {"id": "1810.01845", "submitter": "Dafni Antotsiou", "authors": "Dafni Antotsiou, Guillermo Garcia-Hernando, Tae-Kyun Kim", "title": "Task-Oriented Hand Motion Retargeting for Dexterous Manipulation\n  Imitation", "comments": "ECCV 2018 workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human hand actions are quite complex, especially when they involve object\nmanipulation, mainly due to the high dimensionality of the hand and the vast\naction space that entails. Imitating those actions with dexterous hand models\ninvolves different important and challenging steps: acquiring human hand\ninformation, retargeting it to a hand model, and learning a policy from\nacquired data. In this work, we capture the hand information by using a\nstate-of-the-art hand pose estimator. We tackle the retargeting problem from\nthe hand pose to a 29 DoF hand model by combining inverse kinematics and PSO\nwith a task objective optimisation. This objective encourages the virtual hand\nto accomplish the manipulation task, relieving the effect of the estimator's\nnoise and the domain gap. Our approach leads to a better success rate in the\ngrasping task compared to our inverse kinematics baseline, allowing us to\nrecord successful human demonstrations. Furthermore, we used these\ndemonstrations to learn a policy network using generative adversarial imitation\nlearning (GAIL) that is able to autonomously grasp an object in the virtual\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 17:14:05 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Antotsiou", "Dafni", ""], ["Garcia-Hernando", "Guillermo", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1810.01849", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai, Rares Ambrus, Adrien Gaidon", "title": "SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation", "comments": "6 pages, 5 figures, 2 tables, ICRA 2019 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent techniques in self-supervised monocular depth estimation are\napproaching the performance of supervised methods, but operate in low\nresolution only. We show that high resolution is key towards high-fidelity\nself-supervised monocular depth prediction. Inspired by recent deep learning\nmethods for Single-Image Super-Resolution, we propose a sub-pixel convolutional\nlayer extension for depth super-resolution that accurately synthesizes\nhigh-resolution disparities from their corresponding low-resolution\nconvolutional features. In addition, we introduce a differentiable\nflip-augmentation layer that accurately fuses predictions from the image and\nits horizontally flipped version, reducing the effect of left and right shadow\nregions generated in the disparity map due to occlusions. Both contributions\nprovide significant performance gains over the state-of-the-art in\nself-supervised depth and pose estimation on the public KITTI benchmark. A\nvideo of our approach can be found at https://youtu.be/jKNgBeBMx0I.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 17:24:06 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Pillai", "Sudeep", ""], ["Ambrus", "Rares", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1810.01868", "submitter": "{\\L}ukasz Maziarka", "authors": "{\\L}ukasz Maziarka, Marek \\'Smieja, Aleksandra Nowak, Jacek Tabor,\n  {\\L}ukasz Struski, Przemys{\\l}aw Spurek", "title": "Set Aggregation Network as a Trainable Pooling Layer", "comments": "ICONIP 2019", "journal-ref": "Neural Information Processing. ICONIP 2019", "doi": "10.1007/978-3-030-36711-4_35", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global pooling, such as max- or sum-pooling, is one of the key ingredients in\ndeep neural networks used for processing images, texts, graphs and other types\nof structured data. Based on the recent DeepSets architecture proposed by\nZaheer et al. (NIPS 2017), we introduce a Set Aggregation Network (SAN) as an\nalternative global pooling layer. In contrast to typical pooling operators, SAN\nallows to embed a given set of features to a vector representation of arbitrary\nsize. We show that by adjusting the size of embedding, SAN is capable of\npreserving the whole information from the input. In experiments, we demonstrate\nthat replacing global pooling layer by SAN leads to the improvement of\nclassification accuracy. Moreover, it is less prone to overfitting and can be\nused as a regularizer.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:20:13 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 08:44:25 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 10:25:02 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Maziarka", "\u0141ukasz", ""], ["\u015amieja", "Marek", ""], ["Nowak", "Aleksandra", ""], ["Tabor", "Jacek", ""], ["Struski", "\u0141ukasz", ""], ["Spurek", "Przemys\u0142aw", ""]]}, {"id": "1810.01898", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey, Snehasis Mukherjee", "title": "A Multi-Face Challenging Dataset for Robust Face Recognition", "comments": "15th International Conference on Control, Automation, Robotics and\n  Vision (ICARCV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition in images is an active area of interest among the computer\nvision researchers. However, recognizing human face in an unconstrained\nenvironment, is a relatively less-explored area of research. Multiple face\nrecognition in unconstrained environment is a challenging task, due to the\nvariation of view-point, scale, pose, illumination and expression of the face\nimages. Partial occlusion of faces makes the recognition task even more\nchallenging. The contribution of this paper is two-folds: introducing a\nchallenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition\nin unconstrained environment and evaluating the performance of state-of-the-art\nhand-designed and deep learning based face descriptors on the dataset. The\nproposed IIITS MFace dataset contains faces with challenges like pose\nvariation, occlusion, mask, spectacle, expressions, change of illumination,\netc. We experiment with several state-of-the-art face descriptors, including\nrecent deep learning based face descriptors like VGGFace, and compare with the\nexisting benchmark face datasets. Results of the experiments clearly show that\nthe difficulty level of the proposed dataset is much higher compared to the\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 07:04:59 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 01:03:01 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1810.01928", "submitter": "Mauricio Orbes Arteaga", "authors": "Mauricio Orbes Arteaga, Lauge S{\\o}rensen, M. Jorge Cardoso, Marc\n  Modat, Sebastien Ourselin, Stefan Sommer, Mads Nielsen, Christian Igel,\n  Akshay Pai", "title": "PADDIT: Probabilistic Augmentation of Data using Diffeomorphic Image\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For proper generalization performance of convolutional neural networks (CNNs)\nin medical image segmentation, the learnt features should be invariant under\nparticular non-linear shape variations of the input. To induce invariance in\nCNNs to such transformations, we propose Probabilistic Augmentation of Data\nusing Diffeomorphic Image Transformation (PADDIT) -- a systematic framework for\ngenerating realistic transformations that can be used to augment data for\ntraining CNNs. We show that CNNs trained with PADDIT outperforms CNNs trained\nwithout augmentation and with generic augmentation in segmenting white matter\nhyperintensities from T1 and FLAIR brain MRI scans.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 19:39:29 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 13:47:47 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Arteaga", "Mauricio Orbes", ""], ["S\u00f8rensen", "Lauge", ""], ["Cardoso", "M. Jorge", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""], ["Sommer", "Stefan", ""], ["Nielsen", "Mads", ""], ["Igel", "Christian", ""], ["Pai", "Akshay", ""]]}, {"id": "1810.01967", "submitter": "Mohammad Golbabaee", "authors": "Mohammad Golbabaee, Zhouye Chen, Yves Wiaux, Mike Davies", "title": "CoverBLIP: accelerated and scalable iterative matched-filtering for\n  Magnetic Resonance Fingerprint reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are\nbottlenecked by the heavy computations of a matched-filtering step due to the\ngrowing size and complexity of the fingerprint dictionaries in multi-parametric\nquantitative MRI applications. We address this shortcoming by arranging\ndictionary atoms in the form of cover tree structures and adopt the\ncorresponding fast approximate nearest neighbour searches to accelerate\nmatched-filtering. For datasets belonging to smooth low-dimensional manifolds\ncover trees offer search complexities logarithmic in terms of data population.\nWith this motivation we propose an iterative reconstruction algorithm, named\nCoverBLIP, to address large-size MRF problems where the fingerprint dictionary\ni.e. discrete manifold of Bloch responses, encodes several intrinsic NMR\nparameters. We study different forms of convergence for this algorithm and we\nshow that provided with a notion of embedding, the inexact and non-convex\niterations of CoverBLIP linearly convergence toward a near-global solution with\nthe same order of accuracy as using exact brute-force searches. Our further\nexaminations on both synthetic and real-world datasets and using different\nsampling strategies, indicates between 2 to 3 orders of magnitude reduction in\ntotal search computations. Cover trees are robust against the\ncurse-of-dimensionality and therefore CoverBLIP provides a notion of\nscalability -- a consistent gain in time-accuracy performance-- for searching\nhigh-dimensional atoms which may not be easily preprocessed (i.e. for\ndimensionality reduction) due to the increasing degrees of non-linearities\nappearing in the emerging multi-parametric MRF dictionaries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 20:52:49 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Golbabaee", "Mohammad", ""], ["Chen", "Zhouye", ""], ["Wiaux", "Yves", ""], ["Davies", "Mike", ""]]}, {"id": "1810.01987", "submitter": "Varun Murali", "authors": "Amado Antonini, Winter Guerra, Varun Murali, Thomas Sayre-McCord, and\n  Sertac Karaman", "title": "The Blackbird Dataset: A large-scale dataset for UAV perception in\n  aggressive flight", "comments": "Accepted to appear at ISER 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale,\naggressive indoor flight dataset collected using a custom-built quadrotor\nplatform for use in evaluation of agile perception.Inspired by the potential of\nfuture high-speed fully-autonomous drone racing, the Blackbird dataset contains\nover 10 hours of flight data from 168 flights over 17 flight trajectories and 5\nenvironments at velocities up to $7.0ms^-1$. Each flight includes sensor data\nfrom 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz\nIMU, $\\sim190Hz$ motor speed sensors, and 360Hz millimeter-accurate motion\ncapture ground truth. Camera images for each flight were photorealistically\nrendered using FlightGoggles across a variety of environments to facilitate\neasy experimentation of high performance perception algorithms. The dataset is\navailable for download at http://blackbird-dataset.mit.edu/\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 21:54:11 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Antonini", "Amado", ""], ["Guerra", "Winter", ""], ["Murali", "Varun", ""], ["Sayre-McCord", "Thomas", ""], ["Karaman", "Sertac", ""]]}, {"id": "1810.02001", "submitter": "Muhammad Kamran Janjua", "authors": "Ignazio Gallo, Alessandro Calefati, Shah Nawaz, Muhammad Kamran Janjua", "title": "Image and Encoded Text Fusion for Multi-Modal Classification", "comments": "Accepted to DICTA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal approaches employ data from multiple input streams such as\ntextual and visual domains. Deep neural networks have been successfully\nemployed for these approaches. In this paper, we present a novel multi-modal\napproach that fuses images and text descriptions to improve multi-modal\nclassification performance in real-world scenarios. The proposed approach\nembeds an encoded text onto an image to obtain an information-enriched image.\nTo learn feature representations of resulting images, standard Convolutional\nNeural Networks (CNNs) are employed for the classification task. We demonstrate\nhow a CNN based pipeline can be used to learn representations of the novel\nfusion approach. We compare our approach with individual sources on two\nlarge-scale multi-modal classification datasets while obtaining encouraging\nresults. Furthermore, we evaluate our approach against two famous multi-modal\nstrategies namely early fusion and late fusion.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 23:11:39 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Gallo", "Ignazio", ""], ["Calefati", "Alessandro", ""], ["Nawaz", "Shah", ""], ["Janjua", "Muhammad Kamran", ""]]}, {"id": "1810.02010", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Chia-Lin Yu, Matthew Halpern, Hasan Genc, Shiao-Li Tsao\n  and Vijay Janapa Reddi", "title": "Domain Specific Approximation for Object Detection", "comments": "6 pages, 6 figures. Published in IEEE Micro, vol. 38, no. 1, pp.\n  31-40, January/February 2018", "journal-ref": "T. Chin, C. Yu, M. Halpern, H. Genc, S. Tsao and V. J. Reddi,\n  \"Domain-Specific Approximation for Object Detection,\" in IEEE Micro, vol. 38,\n  no. 1, pp. 31-40, January/February 2018", "doi": "10.1109/MM.2018.112130335", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in object detection in advanced driver assistance\nsystems and autonomous robots and vehicles. To enable such innovative systems,\nwe need faster object detection. In this work, we investigate the trade-off\nbetween accuracy and speed with domain-specific approximations, i.e.\ncategory-aware image size scaling and proposals scaling, for two\nstate-of-the-art deep learning-based object detection meta-architectures. We\nstudy the effectiveness of applying approximation both statically and\ndynamically to understand the potential and the applicability of them. By\nconducting experiments on the ImageNet VID dataset, we show that\ndomain-specific approximation has great potential to improve the speed of the\nsystem without deteriorating the accuracy of object detectors, i.e. up to 7.5x\nspeedup for dynamic domain-specific approximation. To this end, we present our\ninsights toward harvesting domain-specific approximation as well as devise a\nproof-of-concept runtime, AutoFocus, that exploits dynamic domain-specific\napproximation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 00:25:02 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Yu", "Chia-Lin", ""], ["Halpern", "Matthew", ""], ["Genc", "Hasan", ""], ["Tsao", "Shiao-Li", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1810.02020", "submitter": "Ghouthi Boukli Hacene Gbh", "authors": "Ghouthi Boukli Hacene, Vincent Gripon, Nicolas Farrugia, Matthieu\n  Arzel, Michel Jezequel", "title": "Transfer Incremental Learning using Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods have reached state of the art performances,\nrelying on large quantity of available data and computational power. Such\nmethods still remain highly inappropriate when facing a major open machine\nlearning problem, which consists of learning incrementally new classes and\nexamples over time. Combining the outstanding performances of Deep Neural\nNetworks (DNNs) with the flexibility of incremental learning techniques is a\npromising venue of research. In this contribution, we introduce Transfer\nIncremental Learning using Data Augmentation (TILDA). TILDA is based on\npre-trained DNNs as feature extractor, robust selection of feature vectors in\nsubspaces using a nearest-class-mean based technique, majority votes and data\naugmentation at both the training and the prediction stages. Experiments on\nchallenging vision datasets demonstrate the ability of the proposed method for\nlow complexity incremental learning, while achieving significantly better\naccuracy than existing incremental counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 01:38:02 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hacene", "Ghouthi Boukli", ""], ["Gripon", "Vincent", ""], ["Farrugia", "Nicolas", ""], ["Arzel", "Matthieu", ""], ["Jezequel", "Michel", ""]]}, {"id": "1810.02068", "submitter": "Cheng Fu", "authors": "Cheng Fu, Shilin Zhu, Hao Su, Ching-En Lee, Jishen Zhao", "title": "Towards Fast and Energy-Efficient Binarized Neural Network Inference on\n  FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN\nby using a single bit (-1/+1) for network parameters and intermediate\nrepresentations, which has greatly reduced the off-chip data transfer and\nstorage overhead. However, a large amount of computation redundancy still\nexists in BNN inference. By analyzing local properties of images and the\nlearned BNN kernel weights, we observe an average of $\\sim$78% input similarity\nand $\\sim$59% weight similarity among weight kernels, measured by our proposed\nmetric in common network architectures. Thus there does exist redundancy that\ncan be exploited to further reduce the amount of on-chip computations.\n  Motivated by the observation, in this paper, we proposed two types of fast\nand energy-efficient architectures for BNN inference. We also provide analysis\nand insights to pick the better strategy of these two for different datasets\nand network models. By reusing the results from previous computation, much\ncycles for data buffer access and computations can be skipped. By experiments,\nwe demonstrate that 80% of the computation and 40% of the buffer access can be\nskipped by exploiting BNN similarity. Thus, our design can achieve 17%\nreduction in total power consumption, 54% reduction in on-chip power\nconsumption and 2.4$\\times$ maximum speedup, compared to the baseline without\napplying our reuse technique. Our design also shows 1.9$\\times$ more\narea-efficiency compared to state-of-the-art BNN inference design. We believe\nour deployment of BNN on FPGA leads to a promising future of running deep\nlearning models on mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 06:29:59 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Fu", "Cheng", ""], ["Zhu", "Shilin", ""], ["Su", "Hao", ""], ["Lee", "Ching-En", ""], ["Zhao", "Jishen", ""]]}, {"id": "1810.02074", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Charan Reddy, Prabir Kumar Biswas", "title": "Unsupervised Adversarial Visual Level Domain Adaptation for Learning\n  Video Object Detectors from Images", "comments": "* First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based object detectors require thousands of diversified\nbounding box and class annotated examples. Though image object detectors have\nshown rapid progress in recent years with the release of multiple large-scale\nstatic image datasets, object detection on videos still remains an open problem\ndue to scarcity of annotated video frames. Having a robust video object\ndetector is an essential component for video understanding and curating\nlarge-scale automated annotations in videos. Domain difference between images\nand videos makes the transferability of image object detectors to videos\nsub-optimal. The most common solution is to use weakly supervised annotations\nwhere a video frame has to be tagged for presence/absence of object categories.\nThis still takes up manual effort. In this paper we take a step forward by\nadapting the concept of unsupervised adversarial image-to-image translation to\nperturb static high quality images to be visually indistinguishable from a set\nof video frames. We assume the presence of a fully annotated static image\ndataset and an unannotated video dataset. Object detector is trained on\nadversarially transformed image dataset using the annotations of the original\ndataset. Experiments on Youtube-Objects and Youtube-Objects-Subset datasets\nwith two contemporary baseline object detectors reveal that such unsupervised\npixel level domain adaptation boosts the generalization performance on video\nframes compared to direct application of original image object detector. Also,\nwe achieve competitive performance compared to recent baselines of weakly\nsupervised methods. This paper can be seen as an application of image\ntranslation for cross domain object detection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 07:07:29 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Lahiri", "Avisek", ""], ["Reddy", "Charan", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1810.02076", "submitter": "Changhao Chen", "authors": "Changhao Chen, Yishu Miao, Chris Xiaoxuan Lu, Phil Blunsom, Andrew\n  Markham, Niki Trigoni", "title": "Transferring Physical Motion Between Domains for Neural Inertial\n  Tracking", "comments": "NIPS 2018 workshop on Modeling the Physical World: Perception,\n  Learning, and Control. A complete version will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inertial information processing plays a pivotal role in ego-motion awareness\nfor mobile agents, as inertial measurements are entirely egocentric and not\nenvironment dependent. However, they are affected greatly by changes in sensor\nplacement/orientation or motion dynamics, and it is infeasible to collect\nlabelled data from every domain. To overcome the challenges of domain\nadaptation on long sensory sequences, we propose a novel framework that\nextracts domain-invariant features of raw sequences from arbitrary domains, and\ntransforms to new domains without any paired data. Through the experiments, we\ndemonstrate that it is able to efficiently and effectively convert the raw\nsequence from a new unlabelled target domain into an accurate inertial\ntrajectory, benefiting from the physical motion knowledge transferred from the\nlabelled source domain. We also conduct real-world experiments to show our\nframework can reconstruct physically meaningful trajectories from raw IMU\nmeasurements obtained with a standard mobile phone in various attachments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 07:12:47 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Chen", "Changhao", ""], ["Miao", "Yishu", ""], ["Lu", "Chris Xiaoxuan", ""], ["Blunsom", "Phil", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1810.02113", "submitter": "Maayan Frid-Adar", "authors": "Maayan Frid-Adar, Avi Ben-Cohen, Rula Amer, Hayit Greenspan", "title": "Improving the Segmentation of Anatomical Structures in Chest Radiographs\n  using U-Net with an ImageNet Pre-trained Encoder", "comments": "Presented at the First International Workshop on Thoracic Image\n  Analysis (TIA), MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00946-5_17", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of anatomical structures in chest radiographs is\nessential for many computer-aided diagnosis tasks. In this paper we investigate\nthe latest fully-convolutional architectures for the task of multi-class\nsegmentation of the lungs field, heart and clavicles in a chest radiograph. In\naddition, we explore the influence of using different loss functions in the\ntraining process of a neural network for semantic segmentation. We evaluate all\nmodels on a common benchmark of 247 X-ray images from the JSRT database and\nground-truth segmentation masks from the SCR dataset. Our best performing\narchitecture, is a modified U-Net that benefits from pre-trained encoder\nweights. This model outperformed the current state-of-the-art methods tested on\nthe same benchmark, with Jaccard overlap scores of 96.1% for lung fields, 90.6%\nfor heart and 85.5% for clavicles.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 09:18:42 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Frid-Adar", "Maayan", ""], ["Ben-Cohen", "Avi", ""], ["Amer", "Rula", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1810.02126", "submitter": "Youssef Tamaazousti", "authors": "Julien Girard, Youssef Tamaazousti, Herv\\'e Le Borgne, C\\'eline\n  Hudelot", "title": "Learning Finer-class Networks for Universal Representations", "comments": "British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world visual recognition use-cases can not directly benefit from\nstate-of-the-art CNN-based approaches because of the lack of many annotated\ndata. The usual approach to deal with this is to transfer a representation\npre-learned on a large annotated source-task onto a target-task of interest.\nThis raises the question of how well the original representation is\n\"universal\", that is to say directly adapted to many different target-tasks. To\nimprove such universality, the state-of-the-art consists in training networks\non a diversified source problem, that is modified either by adding generic or\nspecific categories to the initial set of categories. In this vein, we proposed\na method that exploits finer-classes than the most specific ones existing, for\nwhich no annotation is available. We rely on unsupervised learning and a\nbottom-up split and merge strategy. We show that our method learns more\nuniversal representations than state-of-the-art, leading to significantly\nbetter results on 10 target-tasks from multiple domains, using several network\narchitectures, either alone or combined with networks learned at a coarser\nsemantic level.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 09:58:34 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Girard", "Julien", ""], ["Tamaazousti", "Youssef", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "1810.02201", "submitter": "Giacomo Tarroni", "authors": "Giacomo Tarroni, Ozan Oktay, Matthew Sinclair, Wenjia Bai, Andreas\n  Schuh, Hideaki Suzuki, Antonio de Marvao, Declan O'Regan, Stuart Cook, Daniel\n  Rueckert", "title": "A Comprehensive Approach for Learning-based Fully-Automated Inter-slice\n  Motion Correction for Short-Axis Cine Cardiac MR Image Stacks", "comments": "Accepted for publication at MICCAI 2018. arXiv admin note: text\n  overlap with arXiv:1803.09354", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the clinical routine, short axis (SA) cine cardiac MR (CMR) image stacks\nare acquired during multiple subsequent breath-holds. If the patient cannot\nconsistently hold the breath at the same position, the acquired image stack\nwill be affected by inter-slice respiratory motion and will not correctly\nrepresent the cardiac volume, introducing potential errors in the following\nanalyses and visualisations. We propose an approach to automatically correct\ninter-slice respiratory motion in SA CMR image stacks. Our approach makes use\nof probabilistic segmentation maps (PSMs) of the left ventricular (LV) cavity\ngenerated with decision forests. PSMs are generated for each slice of the SA\nstack and rigidly registered in-plane to a target PSM. If long axis (LA) images\nare available, PSMs are generated for them and combined to create the target\nPSM; if not, the target PSM is produced from the same stack using a 3D model\ntrained from motion-free stacks. The proposed approach was tested on a dataset\nof SA stacks acquired from 24 healthy subjects (for which anatomical 3D cardiac\nimages were also available as reference) and compared to two techniques which\nuse LA intensity images and LA segmentations as targets, respectively. The\nresults show the accuracy and robustness of the proposed approach in motion\ncompensation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 11:40:07 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Tarroni", "Giacomo", ""], ["Oktay", "Ozan", ""], ["Sinclair", "Matthew", ""], ["Bai", "Wenjia", ""], ["Schuh", "Andreas", ""], ["Suzuki", "Hideaki", ""], ["de Marvao", "Antonio", ""], ["O'Regan", "Declan", ""], ["Cook", "Stuart", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1810.02244", "submitter": "Christopher Morris", "authors": "Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton,\n  Jan Eric Lenssen, Gaurav Rattan, Martin Grohe", "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks", "comments": "Extended version with proofs, accepted at AAAI 2019, added units of\n  measurement of QM9 dataset into appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, graph neural networks (GNNs) have emerged as a powerful\nneural architecture to learn vector representations of nodes and graphs in a\nsupervised, end-to-end fashion. Up to now, GNNs have only been evaluated\nempirically---showing promising results. The following work investigates GNNs\nfrom a theoretical point of view and relates them to the $1$-dimensional\nWeisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have\nthe same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic\n(sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on\nthis, we propose a generalization of GNNs, so-called $k$-dimensional GNNs\n($k$-GNNs), which can take higher-order graph structures at multiple scales\ninto account. These higher-order structures play an essential role in the\ncharacterization of social networks and molecule graphs. Our experimental\nevaluation confirms our theoretical findings as well as confirms that\nhigher-order information is useful in the task of graph classification and\nregression.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 14:31:57 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 12:52:37 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 15:55:24 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Morris", "Christopher", ""], ["Ritzert", "Martin", ""], ["Fey", "Matthias", ""], ["Hamilton", "William L.", ""], ["Lenssen", "Jan Eric", ""], ["Rattan", "Gaurav", ""], ["Grohe", "Martin", ""]]}, {"id": "1810.02274", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Anton Raichuk, Rapha\\\"el Marinier, Damien Vincent,\n  Marc Pollefeys, Timothy Lillicrap, Sylvain Gelly", "title": "Episodic Curiosity through Reachability", "comments": "Accepted to ICLR 2019. Code at\n  https://github.com/google-research/episodic-curiosity/. Videos at\n  https://sites.google.com/view/episodic-curiosity/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewards are sparse in the real world and most of today's reinforcement\nlearning algorithms struggle with such sparsity. One solution to this problem\nis to allow the agent to create rewards for itself - thus making rewards dense\nand more suitable for learning. In particular, inspired by curious behaviour in\nanimals, observing something novel could be rewarded with a bonus. Such bonus\nis summed up with the real task reward - making it possible for RL algorithms\nto learn from the combined reward. We propose a new curiosity method which uses\nepisodic memory to form the novelty bonus. To determine the bonus, the current\nobservation is compared with the observations in memory. Crucially, the\ncomparison is done based on how many environment steps it takes to reach the\ncurrent observation from those in memory - which incorporates rich information\nabout environment dynamics. This allows us to overcome the known \"couch-potato\"\nissues of prior work - when the agent finds a way to instantly gratify itself\nby exploiting actions which lead to hardly predictable consequences. We test\nour approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In\nnavigational tasks from ViZDoom and DMLab, our agent outperforms the\nstate-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our\ncuriosity module learns locomotion out of the first-person-view curiosity only.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:24:06 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 17:39:39 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 17:02:58 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 13:10:33 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 17:54:03 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Savinov", "Nikolay", ""], ["Raichuk", "Anton", ""], ["Marinier", "Rapha\u00ebl", ""], ["Vincent", "Damien", ""], ["Pollefeys", "Marc", ""], ["Lillicrap", "Timothy", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1810.02277", "submitter": "Sanne Van Velzen", "authors": "Sanne G.M. van Velzen, Majd Zreik, Nikolas Lessmann, Max A. Viergever,\n  Pim A. de Jong, Helena M. Verkooijen, Ivana I\\v{s}gum", "title": "Direct Prediction of Cardiovascular Mortality from Low-dose Chest CT\n  using Deep Learning", "comments": "This work has been submitted to SPIE 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease (CVD) is a leading cause of death in the lung cancer\nscreening population. Chest CT scans made in lung cancer screening are suitable\nfor identification of participants at risk of CVD. Existing methods analyzing\nCT images from lung cancer screening for prediction of CVD events or mortality\nuse engineered features extracted from the images combined with patient\ninformation. In this work we propose a method that automatically predicts\n5-year cardiovascular mortality directly from chest CT scans without the need\nfor hand-crafting image features. A set of 1,583 participants of the National\nLung Screening Trial was included (1,188 survivors, 395 non-survivors).\nLow-dose chest CT images acquired at baseline were analyzed and the follow-up\ntime was 5 years. To limit the analysis to the heart region, the heart was\nfirst localized by our previously developed algorithm for organ localization\nexploiting convolutional neural networks. Thereafter, a convolutional\nautoencoder was used to encode the identified heart region. Finally, based on\nthe extracted encodings subjects were classified into survivors or\nnon-survivors using a support vector machine classifier. The performance of the\nmethod was assessed in eight cross-validation experiments with 1,433 images\nused for training, 50 for validation and 100 for testing. The method achieved a\nperformance with an area under the ROC curve of 0.72. The results demonstrate\nthat prediction of cardiovascular mortality directly from low-dose screening\nchest CT scans, without hand-crafted features, is feasible, allowing\nidentification of subjects at risk of fatal CVD events.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:33:14 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["van Velzen", "Sanne G. M.", ""], ["Zreik", "Majd", ""], ["Lessmann", "Nikolas", ""], ["Viergever", "Max A.", ""], ["de Jong", "Pim A.", ""], ["Verkooijen", "Helena M.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1810.02283", "submitter": "Aiwen Jiang", "authors": "Kangfu Mei, Aiwen Jiang, Juncheng Li, Mingwen Wang", "title": "Progressive Feature Fusion Network for Realistic Image Dehazing", "comments": "14 pages, 7 figures, 1 tables, accepted by ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image dehazing is a challenging ill-posed restoration problem. Various\nprior-based and learning-based methods have been proposed. Most of them follow\na classic atmospheric scattering model which is an elegant simplified physical\nmodel based on the assumption of single-scattering and homogeneous atmospheric\nmedium. The formulation of haze in realistic environment is more complicated.\nIn this paper, we propose to take its essential mechanism as \"black box\", and\nfocus on learning an input-adaptive trainable end-to-end dehazing model. An\nU-Net like encoder-decoder deep network via progressive feature fusions has\nbeen proposed to directly learn highly nonlinear transformation function from\nobserved hazy image to haze-free ground-truth. The proposed network is\nevaluated on two public image dehazing benchmarks. The experiments demonstrate\nthat it can achieve superior performance when compared with popular\nstate-of-the-art methods. With efficient GPU memory usage, it can\nsatisfactorily recover ultra high definition hazed image up to 4K resolution,\nwhich is unaffordable by many deep learning based dehazing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:56:22 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Mei", "Kangfu", ""], ["Jiang", "Aiwen", ""], ["Li", "Juncheng", ""], ["Wang", "Mingwen", ""]]}, {"id": "1810.02320", "submitter": "Ehsan Farahbakhsh", "authors": "Ehsan Farahbakhsh, Rohitash Chandra, Hugo K. H. Olierook, Richard\n  Scalzo, Chris Clark, Steven M. Reddy, R. Dietmar Muller", "title": "Computer vision-based framework for extracting geological lineaments\n  from optical remote sensing data", "comments": "17 pages, 10 figures, 2 tables", "journal-ref": null, "doi": "10.1080/01431161.2019.1674462", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extraction of geological lineaments from digital satellite data is a\nfundamental application in remote sensing. The location of geological\nlineaments such as faults and dykes are of interest for a range of\napplications, particularly because of their association with hydrothermal\nmineralization. Although a wide range of applications have utilized computer\nvision techniques, a standard workflow for application of these techniques to\nmineral exploration is lacking. We present a framework for extracting\ngeological lineaments using computer vision techniques which is a combination\nof edge detection and line extraction algorithms for extracting geological\nlineaments using optical remote sensing data. It features ancillary computer\nvision techniques for reducing data dimensionality, removing noise and\nenhancing the expression of lineaments. We test the proposed framework on\nLandsat 8 data of a mineral-rich portion of the Gascoyne Province in Western\nAustralia using different dimension reduction techniques and convolutional\nfilters. To validate the results, the extracted lineaments are compared to our\nmanual photointerpretation and geologically mapped structures by the Geological\nSurvey of Western Australia (GSWA). The results show that the best correlation\nbetween our extracted geological lineaments and the GSWA geological lineament\nmap is achieved by applying a minimum noise fraction transformation and a\nLaplacian filter. Application of a directional filter instead shows a stronger\ncorrelation with the output of our manual photointerpretation and known sites\nof hydrothermal mineralization. Hence, our framework using either filter can be\nused for mineral prospectivity mapping in other regions where faults are\nexposed and observable in optical remote sensing data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:09:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Farahbakhsh", "Ehsan", ""], ["Chandra", "Rohitash", ""], ["Olierook", "Hugo K. H.", ""], ["Scalzo", "Richard", ""], ["Clark", "Chris", ""], ["Reddy", "Steven M.", ""], ["Muller", "R. Dietmar", ""]]}, {"id": "1810.02334", "submitter": "Kyle Hsu", "authors": "Kyle Hsu and Sergey Levine and Chelsea Finn", "title": "Unsupervised Learning via Meta-Learning", "comments": "ICLR 2019 camera-ready. 24 pages, 2 figures, links to code available\n  at https://sites.google.com/view/unsupervised-via-meta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of unsupervised learning is to acquire representations from\nunlabeled data or experience that can be used for more effective learning of\ndownstream tasks from modest amounts of labeled data. Many prior unsupervised\nlearning works aim to do so by developing proxy objectives based on\nreconstruction, disentanglement, prediction, and other metrics. Instead, we\ndevelop an unsupervised meta-learning method that explicitly optimizes for the\nability to learn a variety of tasks from small amounts of data. To do so, we\nconstruct tasks from unlabeled data in an automatic way and run meta-learning\nover the constructed tasks. Surprisingly, we find that, when integrated with\nmeta-learning, relatively simple task construction mechanisms, such as\nclustering embeddings, lead to good performance on a variety of downstream,\nhuman-specified tasks. Our experiments across four image datasets indicate that\nour unsupervised meta-learning approach acquires a learning algorithm without\nany labeled data that is applicable to a wide range of downstream\nclassification tasks, improving upon the embedding learned by four prior\nunsupervised learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:29:17 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 23:39:52 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 23:57:36 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 20:47:45 GMT"}, {"version": "v5", "created": "Fri, 7 Dec 2018 20:38:03 GMT"}, {"version": "v6", "created": "Thu, 21 Mar 2019 23:43:47 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Hsu", "Kyle", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1810.02338", "submitter": "Kexin Yi", "authors": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli,\n  Joshua B. Tenenbaum", "title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language\n  Understanding", "comments": "NeurIPS 2018 (spotlight). The first two authors contributed equally\n  to this work. Project page: http://nsvqa.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We marry two powerful ideas: deep representation learning for visual\nrecognition and language understanding, and symbolic program execution for\nreasoning. Our neural-symbolic visual question answering (NS-VQA) system first\nrecovers a structural scene representation from the image and a program trace\nfrom the question. It then executes the program on the scene representation to\nobtain an answer. Incorporating symbolic structure as prior knowledge offers\nthree unique advantages. First, executing programs on a symbolic space is more\nrobust to long program traces; our model can solve complex reasoning tasks\nbetter, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model\nis more data- and memory-efficient: it performs well after learning on a small\nnumber of training data; it can also encode an image into a compact\nrepresentation, requiring less storage than existing methods for offline\nquestion answering. Third, symbolic program execution offers full transparency\nto the reasoning process; we are thus able to interpret and diagnose each\nexecution step.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:38:50 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 23:07:12 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Yi", "Kexin", ""], ["Wu", "Jiajun", ""], ["Gan", "Chuang", ""], ["Torralba", "Antonio", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1810.02340", "submitter": "Namhoon Lee", "authors": "Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning large neural networks while maintaining their performance is often\ndesirable due to the reduced space and time complexity. In existing methods,\npruning is done within an iterative optimization procedure with either\nheuristically designed pruning schedules or additional hyperparameters,\nundermining their utility. In this work, we present a new approach that prunes\na given network once at initialization prior to training. To achieve this, we\nintroduce a saliency criterion based on connection sensitivity that identifies\nstructurally important connections in the network for the given task. This\neliminates the need for both pretraining and the complex pruning schedule while\nmaking it robust to architecture variations. After pruning, the sparse network\nis trained in the standard way. Our method obtains extremely sparse networks\nwith virtually the same accuracy as the reference network on the MNIST,\nCIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to\nvarious architectures including convolutional, residual and recurrent networks.\nUnlike existing methods, our approach enables us to demonstrate that the\nretained connections are indeed relevant to the given task.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:39:58 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 07:45:29 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Lee", "Namhoon", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1810.02344", "submitter": "Jan-Martin O. Steitz", "authors": "Jan-Martin O. Steitz, Faraz Saeedan, Stefan Roth", "title": "Multi-view X-ray R-CNN", "comments": "To appear at the 40th German Conference on Pattern Recognition (GCPR)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the detection of prohibited objects in carry-on luggage as a\npart of avionic security screening, we develop a CNN-based object detection\napproach for multi-view X-ray image data. Our contributions are two-fold.\nFirst, we introduce a novel multi-view pooling layer to perform a 3D\naggregation of 2D CNN-features extracted from each view. To that end, our\npooling layer exploits the known geometry of the imaging system to ensure\ngeometric consistency of the feature aggregation. Second, we introduce an\nend-to-end trainable multi-view detection pipeline based on Faster R-CNN, which\nderives the region proposals and performs the final classification in 3D using\nthese aggregated multi-view features. Our approach shows significant accuracy\ngains compared to single-view detection while even being more efficient than\nperforming single-view detection in each view.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:48:54 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Steitz", "Jan-Martin O.", ""], ["Saeedan", "Faraz", ""], ["Roth", "Stefan", ""]]}, {"id": "1810.02358", "submitter": "Hyeonwoo Noh", "authors": "Hyeonwoo Noh, Taehoon Kim, Jonghwan Mun, Bohyung Han", "title": "Transfer Learning via Unsupervised Task Discovery for Visual Question\n  Answering", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to leverage off-the-shelf visual and linguistic data to cope\nwith out-of-vocabulary answers in visual question answering task. Existing\nlarge-scale visual datasets with annotations such as image class labels,\nbounding boxes and region descriptions are good sources for learning rich and\ndiverse visual concepts. However, it is not straightforward how the visual\nconcepts can be captured and transferred to visual question answering models\ndue to missing link between question dependent answering models and visual data\nwithout question. We tackle this problem in two steps: 1) learning a task\nconditional visual classifier, which is capable of solving diverse\nquestion-specific visual recognition tasks, based on unsupervised task\ndiscovery and 2) transferring the task conditional visual classifier to visual\nquestion answering models. Specifically, we employ linguistic knowledge sources\nsuch as structured lexical database (e.g. WordNet) and visual descriptions for\nunsupervised task discovery, and transfer a learned task conditional visual\nclassifier as an answering unit in a visual question answering model. We\nempirically show that the proposed algorithm generalizes to out-of-vocabulary\nanswers successfully using the knowledge transferred from the visual dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 19:48:38 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 11:50:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["Kim", "Taehoon", ""], ["Mun", "Jonghwan", ""], ["Han", "Bohyung", ""]]}, {"id": "1810.02401", "submitter": "Ghada Zamzmi", "authors": "Ghada Zamzmi, Gabriel Ruiz, Matthew Shreve, Dmitry Goldgof, Rangachar\n  Kasturi, and Sudeep Sarkar", "title": "A method to Suppress Facial Expression in Posed and Spontaneous Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of suppressing facial expressions in videos because\nexpressions can hinder the retrieval of important information in applications\nsuch as face recognition. To achieve this, we present an optical strain\nsuppression method that removes any facial expression without requiring\ntraining for a specific expression. For each frame in a video, an optical\nstrain map that provides the strain magnitude value at each pixel is generated;\nthis strain map is then utilized to neutralize the expression by replacing\npixels of high strain values with pixels from a reference face frame.\nExperimental results of testing the method on various expressions namely\nhappiness, sadness, and anger for two publicly available data sets (i.e.,\nBU-4DFE and AM-FED) show the ability of our method in suppressing facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 19:44:08 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Zamzmi", "Ghada", ""], ["Ruiz", "Gabriel", ""], ["Shreve", "Matthew", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1810.02419", "submitter": "Zhiwu Huang", "authors": "Dinesh Acharya, Zhiwu Huang, Danda Pani Paudel, Luc Van Gool", "title": "Towards High Resolution Video Generation with Progressive Growing of\n  Sliced Wasserstein GANs", "comments": "Master Thesis from ETH Zurich, May 22, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extension of image generation to video generation turns out to be a very\ndifficult task, since the temporal dimension of videos introduces an extra\nchallenge during the generation process. Besides, due to the limitation of\nmemory and training stability, the generation becomes increasingly challenging\nwith the increase of the resolution/duration of videos. In this work, we\nexploit the idea of progressive growing of Generative Adversarial Networks\n(GANs) for higher resolution video generation. In particular, we begin to\nproduce video samples of low-resolution and short-duration, and then\nprogressively increase both resolution and duration alone (or jointly) by\nadding new spatiotemporal convolutional layers to the current networks.\nStarting from the learning on a very raw-level spatial appearance and temporal\nmovement of the video distribution, the proposed progressive method learns\nspatiotemporal information incrementally to generate higher resolution videos.\nFurthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to\nimprove the distribution learning on the video data of high-dimension and\nmixed-spatiotemporal distribution. SWGAN loss replaces the distance between\njoint distributions by that of one-dimensional marginal distributions, making\nthe loss easier to compute. We evaluate the proposed model on our collected\nface video dataset of 10,900 videos to generate photorealistic face videos of\n256x256x32 resolution. In addition, our model also reaches a record inception\nscore of 14.57 in unsupervised action recognition dataset UCF-101.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 20:41:48 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 15:57:41 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Acharya", "Dinesh", ""], ["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "1810.02426", "submitter": "Md Amirul Islam", "authors": "Mahmoud Kalash, Md Amirul Islam, Neil D. B. Bruce", "title": "Relative Saliency and Ranking: Models, Metrics, Data, and Benchmarks", "comments": "Accepted to Transaction on Pattern Analysis and Machine Intelligence.\n  arXiv admin note: substantial text overlap with arXiv:1803.05082", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection is a problem that has been considered in detail and\n\\textcolor{black}{many solutions have been proposed}. In this paper, we argue\nthat work to date has addressed a problem that is relatively ill-posed.\nSpecifically, there is not universal agreement about what constitutes a salient\nobject when multiple observers are queried. This implies that some objects are\nmore likely to be judged salient than others, and implies a relative rank\nexists on salient objects. Initially, we present a novel deep learning solution\nbased on a hierarchical representation of relative saliency and stage-wise\nrefinement. Further to this, we present data, analysis and baseline benchmark\nresults towards addressing the problem of salient object ranking. Methods for\nderiving suitable ranked salient object instances are presented, along with\nmetrics suitable to measuring algorithm performance. In addition, we show how a\nderived dataset can be successively refined to provide cleaned results that\ncorrelate well with pristine ground truth in its characteristics and value for\ntraining and testing models. Finally, we provide a comparison among prevailing\nalgorithms that address salient object ranking or detection to establish\ninitial baselines providing a basis for comparison with future efforts\naddressing this problem. \\textcolor{black}{The source code and data are\npublicly available via our project page:}\n\\textrm{\\href{https://ryersonvisionlab.github.io/cocosalrank.html}{ryersonvisionlab.github.io/cocosalrank}}\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 03:18:20 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 02:20:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kalash", "Mahmoud", ""], ["Islam", "Md Amirul", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1810.02443", "submitter": "Tong He", "authors": "Tong He, Yang Hu", "title": "FashionNet: Personalized Outfit Recommendation with Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of fashion-focused social networks and online shopping,\nintelligent fashion recommendation is now in great need. We design algorithms\nwhich automatically suggest users outfits (e.g. a shirt, together with a skirt\nand a pair of high-heel shoes), that fit their personal fashion preferences.\nRecommending sets, each of which is composed of multiple interacted items, is\nrelatively new to recommender systems, which usually recommend individual items\nto users. We explore the use of deep networks for this challenging task. Our\nsystem, dubbed FashionNet, consists of two components, a feature network for\nfeature extraction and a matching network for compatibility computation. The\nformer is achieved through a deep convolutional network. And for the latter, we\nadopt a multi-layer fully-connected network structure. We design and compare\nthree alternative architectures for FashionNet. To achieve personalized\nrecommendation, we develop a two-stage training strategy, which uses the\nfine-tuning technique to transfer a general compatibility model to a model that\nembeds personal preference. Experiments on a large scale data set collected\nfrom a popular fashion-focused social network validate the effectiveness of the\nproposed networks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 22:26:24 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["He", "Tong", ""], ["Hu", "Yang", ""]]}, {"id": "1810.02492", "submitter": "Ashnil Kumar", "authors": "Ashnil Kumar, Michael Fulham, Dagan Feng, and Jinman Kim", "title": "Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer", "comments": "Source code is available from https://github.com/ashnilkumar/colearn\n  . The paper has been accepted for publication in IEEE Transactions on Medical\n  Imaging. The final published version of the manuscript can be accessed from\n  the IEEE. The paper contains 21 pages (14 main paper, 7 supplementary), 16\n  images (8 main paper, 8 supplementary), and 3 tables", "journal-ref": null, "doi": "10.1109/TMI.2019.2923601", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of multi-modality positron emission tomography and computed\ntomography (PET-CT) images for computer aided diagnosis applications requires\ncombining the sensitivity of PET to detect abnormal regions with anatomical\nlocalization from CT. Current methods for PET-CT image analysis either process\nthe modalities separately or fuse information from each modality based on\nknowledge about the image analysis task. These methods generally do not\nconsider the spatially varying visual characteristics that encode different\ninformation across the different modalities, which have different priorities at\ndifferent locations. For example, a high abnormal PET uptake in the lungs is\nmore meaningful for tumor detection than physiological PET uptake in the heart.\nOur aim is to improve fusion of the complementary information in multi-modality\nPET-CT with a new supervised convolutional neural network (CNN) that learns to\nfuse complementary information for multi-modality medical image analysis. Our\nCNN first encodes modality-specific features and then uses them to derive a\nspatially varying fusion map that quantifies the relative importance of each\nmodality's features across different spatial locations. These fusion maps are\nthen multiplied with the modality-specific feature maps to obtain a\nrepresentation of the complementary multi-modality information at different\nlocations, which can then be used for image analysis. We evaluated the ability\nof our CNN to detect and segment multiple regions with different fusion\nrequirements using a dataset of PET-CT images of lung cancer. We compared our\nmethod to baseline techniques for multi-modality image fusion and segmentation.\nOur findings show that our CNN had a significantly higher foreground detection\naccuracy (99.29%, p < 0.05) than the fusion baselines and a significantly\nhigher Dice score (63.85%) than recent PET-CT tumor segmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 02:09:54 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 08:33:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Feng", "Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "1810.02495", "submitter": "Dhinaharan Nagamalai", "authors": "Jelena Vasiljevi\\'c, Ivica Milosavljevi\\'c, Vladimir Krsti\\'c,\n  Nata\\v{s}a Zivi\\'c, Lazar Berbakov, Luka Lopu\\v{s}ina, Dhinaharan Nagamalai\n  and Milutin Cerovi\\'c", "title": "Medical Images Analysis in Cancer Diagnostic", "comments": "11 pages, 6 figures, 11th International Conference on Security and\n  its Applications (CNSA 2018), Zurich, Switzerland,", "journal-ref": "Computer Science & Information Technology (CS & IT), 2018", "doi": "10.5121/csit.2018.80107", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows results of computer analysis of images in the purpose of\nfinding differences between medical images in order of their classifications in\nterms of separation malign tissue from a normal and benign tissue. The\ndiagnostics of malign tissue is of the crucial importance in medicine.\nTherefore, ascertainment of the correlation between multifractals parameters\nand \"chaotic\" cells could be of the great appliance. This paper shows the\napplication of multifractal analysis for additional help in cancer diagnosis,\nas well as diminishing. of the subjective factor and error probability\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 02:38:46 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Vasiljevi\u0107", "Jelena", ""], ["Milosavljevi\u0107", "Ivica", ""], ["Krsti\u0107", "Vladimir", ""], ["Zivi\u0107", "Nata\u0161a", ""], ["Berbakov", "Lazar", ""], ["Lopu\u0161ina", "Luka", ""], ["Nagamalai", "Dhinaharan", ""], ["Cerovi\u0107", "Milutin", ""]]}, {"id": "1810.02513", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Samuel Schulter, Manmohan Chandraker", "title": "Learning To Simulate", "comments": "Published at International Conference on Learning Representations\n  (ICLR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is a useful tool in situations where training data for machine\nlearning models is costly to annotate or even hard to acquire. In this work, we\npropose a reinforcement learning-based method for automatically adjusting the\nparameters of any (non-differentiable) simulator, thereby controlling the\ndistribution of synthesized data in order to maximize the accuracy of a model\ntrained on that data. In contrast to prior art that hand-crafts these\nsimulation parameters or adjusts only parts of the available parameters, our\napproach fully controls the simulator with the actual underlying goal of\nmaximizing accuracy, rather than mimicking the real data distribution or\nrandomly generating a large volume of data. We find that our approach (i)\nquickly converges to the optimal simulation parameters in controlled\nexperiments and (ii) can indeed discover good sets of parameters for an image\nrendering simulator in actual computer vision applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 04:11:25 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 03:15:27 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Schulter", "Samuel", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1810.02569", "submitter": "Nicolas Gonthier", "authors": "Nicolas Gonthier, Yann Gousseau, Said Ladjal, Olivier Bonfait", "title": "Weakly Supervised Object Detection in Artworks", "comments": "Accepted at ECCV 2018 Workshop Computer Vision for Art Analysis -\n  VISART 2018 14 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-11012-3_53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for the weakly supervised detection of objects in\npaintings. At training time, only image-level annotations are needed. This,\ncombined with the efficiency of our multiple-instance learning method, enables\none to learn new classes on-the-fly from globally annotated databases, avoiding\nthe tedious task of manually marking objects. We show on several databases that\ndropping the instance-level annotations only yields mild performance losses. We\nalso introduce a new database, IconArt, on which we perform detection\nexperiments on classes that could not be learned on photographs, such as Jesus\nChild or Saint Sebastian. To the best of our knowledge, these are the first\nexperiments dealing with the automatic (and in our case weakly supervised)\ndetection of iconographic elements in paintings. We believe that such a method\nis of great benefit for helping art historians to explore large digital\ndatabases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 08:48:53 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Gonthier", "Nicolas", ""], ["Gousseau", "Yann", ""], ["Ladjal", "Said", ""], ["Bonfait", "Olivier", ""]]}, {"id": "1810.02575", "submitter": "Dengxin Dai", "authors": "Dengxin Dai and Luc Van Gool", "title": "Dark Model Adaptation: Semantic Image Segmentation from Daytime to\n  Nighttime", "comments": "Accepted to International Conference on Intelligent Transportation\n  Systems (ITSC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of semantic image segmentation of nighttime\nscenes. Although considerable progress has been made in semantic image\nsegmentation, it is mainly related to daytime scenarios. This paper proposes a\nnovel method to progressive adapt the semantic models trained on daytime\nscenes, along with large-scale annotations therein, to nighttime scenes via the\nbridge of twilight time -- the time between dawn and sunrise, or between sunset\nand dusk. The goal of the method is to alleviate the cost of human annotation\nfor nighttime images by transferring knowledge from standard daytime\nconditions. In addition to the method, a new dataset of road scenes is\ncompiled; it consists of 35,000 images ranging from daytime to twilight time\nand to nighttime. Also, a subset of the nighttime images are densely annotated\nfor method evaluation. Our experiments show that our method is effective for\nmodel adaptation from daytime scenes to nighttime scenes, without using extra\nhuman annotation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 09:09:38 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1810.02583", "submitter": "Evelyn Chee", "authors": "Evelyn Chee, Zhenzhou Wu", "title": "AIRNet: Self-Supervised Affine Registration for 3D Medical Images using\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a self-supervised learning method for affine image\nregistration on 3D medical images. Unlike optimisation-based methods, our\naffine image registration network (AIRNet) is designed to directly estimate the\ntransformation parameters between two input images without using any metric,\nwhich represents the quality of the registration, as the optimising function.\nBut since it is costly to manually identify the transformation parameters\nbetween any two images, we leverage the abundance of cheap unlabelled data to\ngenerate a synthetic dataset for the training of the model. Additionally, the\nstructure of AIRNet enables us to learn the discriminative features of the\nimages which are useful for registration purpose. Our proposed method was\nevaluated on magnetic resonance images of the axial view of human brain and\ncompared with the performance of a conventional image registration method.\nExperiments demonstrate that our approach achieves better overall performance\non registration of images from different patients and modalities with 100x\nspeed-up in execution time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 09:37:24 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 01:34:35 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chee", "Evelyn", ""], ["Wu", "Zhenzhou", ""]]}, {"id": "1810.02607", "submitter": "Daiki Kimura", "authors": "Minori Narita, Daiki Kimura, Ryuki Tachibana", "title": "Spatially-weighted Anomaly Detection", "comments": "4 pages, SSII 2018 (original paper was written in Japanese)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of anomaly detection methods have been proposed recently, and\napplied to a wide variety of fields including medical screening and production\nquality checking. Some methods have utilized images, and, in some cases, a part\nof the anomaly images is known beforehand. However, this kind of information is\ndismissed by previous methods, because the methods can only utilize a normal\npattern. Moreover, the previous methods suffer a decrease in accuracy due to\nnegative effects from surrounding noises. In this study, we propose a\nspatially-weighted anomaly detection method (SPADE) that utilizes all of the\nknown patterns and lessens the vulnerability to ambient noises by applying\nGrad-CAM, which is the visualization method of a CNN. We evaluated our method\nquantitatively using two datasets, the MNIST dataset with noise and a dataset\nbased on a brief screening test for dementia.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 11:04:06 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Narita", "Minori", ""], ["Kimura", "Daiki", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1810.02643", "submitter": "Ravimal Bandara", "authors": "M.Z.F.Amara, R.Bandara, Thushari Silva", "title": "SLIC Based Digital Image Enlargement", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low resolution image enhancement is a classical computer vision problem.\nSelecting the best method to reconstruct an image to a higher resolution with\nthe limited data available in the low-resolution image is quite a challenge. A\nmajor drawback from the existing enlargement techniques is the introduction of\ncolor bleeding while interpolating pixels over the edges that separate distinct\ncolors in an image. The color bleeding causes to accentuate the edges with new\ncolors as a result of blending multiple colors over adjacent regions. This\npaper proposes a novel approach to mitigate the color bleeding by segmenting\nthe homogeneous color regions of the image using Simple Linear Iterative\nClustering (SLIC) and applying a higher order interpolation technique\nseparately on the isolated segments. The interpolation at the boundaries of\neach of the isolated segments is handled by using a morphological operation.\nThe approach is evaluated by comparing against several frequently used image\nenlargement methods such as bilinear and bicubic interpolation by means of Peak\nSignal-to-Noise-Ratio (PSNR) value. The results obtained exhibit that the\nproposed method outperforms the baseline methods by means of PSNR and also\nmitigates the color bleeding at the edges which improves the overall\nappearance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 12:19:21 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Amara", "M. Z. F.", ""], ["Bandara", "R.", ""], ["Silva", "Thushari", ""]]}, {"id": "1810.02648", "submitter": "Marc Habermann", "authors": "Marc Habermann and Weipeng Xu and Michael Zollhoefer and Gerard\n  Pons-Moll and Christian Theobalt", "title": "LiveCap: Real-time Human Performance Capture from Monocular Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first real-time human performance capture approach that\nreconstructs dense, space-time coherent deforming geometry of entire humans in\ngeneral everyday clothing from just a single RGB video. We propose a novel\ntwo-stage analysis-by-synthesis optimization whose formulation and\nimplementation are designed for high performance. In the first stage, a skinned\ntemplate model is jointly fitted to background subtracted input video, 2D and\n3D skeleton joint positions found using a deep neural network, and a set of\nsparse facial landmark detections. In the second stage, dense non-rigid 3D\ndeformations of skin and even loose apparel are captured based on a novel\nreal-time capable algorithm for non-rigid tracking using dense photometric and\nsilhouette constraints. Our novel energy formulation leverages automatically\nidentified material regions on the template to model the differing non-rigid\ndeformation behavior of skin and apparel. The two resulting non-linear\noptimization problems per-frame are solved with specially-tailored\ndata-parallel Gauss-Newton solvers. In order to achieve real-time performance\nof over 25Hz, we design a pipelined parallel architecture using the CPU and two\ncommodity GPUs. Our method is the first real-time monocular approach for\nfull-body performance capture. Our method yields comparable accuracy with\noff-line performance capture techniques, while being orders of magnitude\nfaster.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 12:39:37 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 14:29:54 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 15:37:49 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Habermann", "Marc", ""], ["Xu", "Weipeng", ""], ["Zollhoefer", "Michael", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "1810.02683", "submitter": "Anders Eklund", "authors": "Xuan Gu and Hans Knutsson and Markus Nilsson and Anders Eklund", "title": "Generating Diffusion MRI scalar maps from T1 weighted images using\n  generative adversarial networks", "comments": null, "journal-ref": "Scandinavian Conference on Image Analysis, 2019", "doi": "10.1007/978-3-030-20205-7_40", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diffusion magnetic resonance imaging (diffusion MRI) is a non-invasive\nmicrostructure assessment technique. Scalar measures, such as FA (fractional\nanisotropy) and MD (mean diffusivity), quantifying micro-structural tissue\nproperties can be obtained using diffusion models and data processing\npipelines. However, it is costly and time consuming to collect high quality\ndiffusion data. Here, we therefore demonstrate how Generative Adversarial\nNetworks (GANs) can be used to generate synthetic diffusion scalar measures\nfrom structural T1-weighted images in a single optimized step. Specifically, we\ntrain the popular CycleGAN model to learn to map a T1 image to FA or MD, and\nvice versa. As an application, we show that synthetic FA images can be used as\na target for non-linear registration, to correct for geometric distortions\ncommon in diffusion MRI.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:53:28 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 14:23:25 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 13:09:40 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Gu", "Xuan", ""], ["Knutsson", "Hans", ""], ["Nilsson", "Markus", ""], ["Eklund", "Anders", ""]]}, {"id": "1810.02695", "submitter": "Xinjing Cheng", "authors": "Xinjing Cheng, Peng Wang and Ruigang Yang", "title": "Learning Depth with Convolutional Spatial Propagation Network", "comments": "v1.2: add some exps v1.1: fixed some mistakes, v1: 17 pages, 12\n  figures. arXiv admin note: substantial text overlap with arXiv:1808.00150", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth prediction is one of the fundamental problems in computer vision. In\nthis paper, we propose a simple yet effective convolutional spatial propagation\nnetwork (CSPN) to learn the affinity matrix for various depth estimation tasks.\nSpecifically, it is an efficient linear propagation model, in which the\npropagation is performed with a manner of recurrent convolutional operation,\nand the affinity among neighboring pixels is learned through a deep\nconvolutional neural network (CNN). We can append this module to any output\nfrom a state-of-the-art (SOTA) depth estimation networks to improve their\nperformances. In practice, we further extend CSPN in two aspects: 1) take\nsparse depth map as additional input, which is useful for the task of depth\ncompletion; 2) similar to commonly used 3D convolution operation in CNNs, we\npropose 3D CSPN to handle features with one additional dimension, which is\neffective in the task of stereo matching using 3D cost volume. For the tasks of\nsparse to dense, a.k.a depth completion. We experimented the proposed CPSN\nconjunct algorithms over the popular NYU v2 and KITTI datasets, where we show\nthat our proposed algorithms not only produce high quality (e.g., 30% more\nreduction in depth error), but also run faster (e.g., 2 to 5x faster) than\nprevious SOTA spatial propagation network. We also evaluated our stereo\nmatching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on\nboth the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the\neffectiveness of the proposed module. The code of CSPN proposed in this work\nwill be released at https://github.com/XinJCheng/CSPN.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 13:32:29 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 08:03:48 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 03:29:01 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Cheng", "Xinjing", ""], ["Wang", "Peng", ""], ["Yang", "Ruigang", ""]]}, {"id": "1810.02726", "submitter": "Saman Parvaneh", "authors": "Saman Parvaneh, Jonathan Rubin, Ali Samadani, Gajendra Katuwal", "title": "Automatic Detection of Arousals during Sleep using Multiple\n  Physiological Signals", "comments": "Computing in Cardiology 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual scoring of arousals during sleep routinely conducted by sleep\nexperts is a challenging task warranting an automatic approach. This paper\npresents an algorithm for automatic detection of arousals during sleep. Using\nthe Physionet/CinC Challenge dataset, an 80-20% subject-level split was\nperformed to create in-house training and test sets, respectively. The data for\neach subject in the training set was split to 30-second epochs with no overlap.\nA total of 428 features from EEG, EMG, EOG, airflow, and SaO2 in each epoch\nwere extracted and used for creating subject-specific models based on an\nensemble of bagged classification trees, resulting in 943 models. For marking\narousal and non-arousal regions in the test set, the data in the test set was\nsplit to 30-second epochs with 50% overlaps. The average of arousal\nprobabilities from different patient-specific models was assigned to each\n30-second epoch and then a sample-wise probability vector with the same length\nas test data was created for model evaluation. Using the PhysioNet/CinC\nChallenge 2018 scoring criteria, AUPRCs of 0.25 and 0.21 were achieved for the\nin-house test and blind test sets, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:50:55 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Parvaneh", "Saman", ""], ["Rubin", "Jonathan", ""], ["Samadani", "Ali", ""], ["Katuwal", "Gajendra", ""]]}, {"id": "1810.02766", "submitter": "J\\\"org Wagner", "authors": "J\\\"org Wagner, Volker Fischer, Michael Herman and Sven Behnke", "title": "Hierarchical Recurrent Filtering for Fully Convolutional DenseNets", "comments": "In Proceedings of 26th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Bruges,\n  Belgium, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a robust representation of the environment is a crucial ability of\nlearning agents. Deep learning based methods have greatly improved perception\nsystems but still fail in challenging situations. These failures are often not\nsolvable on the basis of a single image. In this work, we present a\nparameter-efficient temporal filtering concept which extends an existing\nsingle-frame segmentation model to work with multiple frames. The resulting\nrecurrent architecture temporally filters representations on all abstraction\nlevels in a hierarchical manner, while decoupling temporal dependencies from\nscene representation. Using a synthetic dataset, we show the ability of our\nmodel to cope with data perturbations and highlight the importance of recurrent\nand hierarchical filtering.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 15:54:46 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 16:35:14 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wagner", "J\u00f6rg", ""], ["Fischer", "Volker", ""], ["Herman", "Michael", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.02786", "submitter": "Yueru Chen", "authors": "C.-C. Jay Kuo, Min Zhang, Siyang Li, Jiali Duan and Yueru Chen", "title": "Interpretable Convolutional Neural Networks via Feedforward Design", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model parameters of convolutional neural networks (CNNs) are determined\nby backpropagation (BP). In this work, we propose an interpretable feedforward\n(FF) design without any BP as a reference. The FF design adopts a data-centric\napproach. It derives network parameters of the current layer based on data\nstatistics from the output of the previous layer in a one-pass manner. To\nconstruct convolutional layers, we develop a new signal transform, called the\nSaab (Subspace Approximation with Adjusted Bias) transform. It is a variant of\nthe principal component analysis (PCA) with an added bias vector to annihilate\nactivation's nonlinearity. Multiple Saab transforms in cascade yield multiple\nconvolutional layers. As to fully-connected (FC) layers, we construct them\nusing a cascade of multi-stage linear least squared regressors (LSRs). The\nclassification and robustness (against adversarial attacks) performances of BP-\nand FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are\ncompared. Finally, we comment on the relationship between BP and FF designs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 16:44:49 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 06:20:48 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Kuo", "C. -C. Jay", ""], ["Zhang", "Min", ""], ["Li", "Siyang", ""], ["Duan", "Jiali", ""], ["Chen", "Yueru", ""]]}, {"id": "1810.02797", "submitter": "Shiv Ram Dubey", "authors": "S H Shabbeer Basha, Soumen Ghosh, Kancharagunta Kishan Babu, Shiv Ram\n  Dubey, Viswanath Pulabaigari, Snehasis Mukherjee", "title": "RCCNet: An Efficient Convolutional Neural Network for Histological\n  Routine Colon Cancer Nuclei Classification", "comments": "Published in ICARCV 2018", "journal-ref": null, "doi": "10.1109/ICARCV.2018.8581147", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and precise classification of histological cell nuclei is of utmost\nimportance due to its potential applications in the field of medical image\nanalysis. It would facilitate the medical practitioners to better understand\nand explore various factors for cancer treatment. The classification of\nhistological cell nuclei is a challenging task due to the cellular\nheterogeneity. This paper proposes an efficient Convolutional Neural Network\n(CNN) based architecture for classification of histological routine colon\ncancer nuclei named as RCCNet. The main objective of this network is to keep\nthe CNN model as simple as possible. The proposed RCCNet model consists of only\n1,512,868 learnable parameters which are significantly less compared to the\npopular CNN models such as AlexNet, CIFARVGG, GoogLeNet, and WRN. The\nexperiments are conducted over publicly available routine colon cancer\nhistological dataset \"CRCHistoPhenotypes\". The results of the proposed RCCNet\nmodel are compared with five state-of-the-art CNN models in terms of the\naccuracy, weighted average F1 score and training time. The proposed method has\nachieved a classification accuracy of 80.61% and 0.7887 weighted average F1\nscore. The proposed RCCNet is more efficient and generalized terms of the\ntraining time and data over-fitting, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 07:18:58 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 12:09:31 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 05:19:12 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Basha", "S H Shabbeer", ""], ["Ghosh", "Soumen", ""], ["Babu", "Kancharagunta Kishan", ""], ["Dubey", "Shiv Ram", ""], ["Pulabaigari", "Viswanath", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1810.02833", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh and Sharan Agrawal", "title": "CanvasGAN: A simple baseline for text to image generation by\n  incrementally patching a canvas", "comments": "CVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new recurrent generative model for generating images from text\ncaptions while attending on specific parts of text captions. Our model creates\nimages by incrementally adding patches on a \"canvas\" while attending on words\nfrom text caption at each timestep. Finally, the canvas is passed through an\nupscaling network to generate images. We also introduce a new method for\ngenerating visual-semantic sentence embeddings based on self-attention over\ntext. We compare our model's generated images with those generated Reed et.\nal.'s model and show that our model is a stronger baseline for text to image\ngeneration tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:04:07 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Singh", "Amanpreet", ""], ["Agrawal", "Sharan", ""]]}, {"id": "1810.02835", "submitter": "Leandro Marcomini", "authors": "L. A. Marcomini, A. L. Cunha", "title": "A Comparison between Background Modelling Methods for Vehicle\n  Segmentation in Highway Traffic Videos", "comments": "12 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to compare the performance of three\nbackground-modeling algorithms in segmenting and detecting vehicles in highway\ntraffic videos. All algorithms are available in OpenCV and were all coded in\nPython. We analyzed seven videos, totaling 2 hours of recording. To compare the\nalgorithms, we created 35 ground-truth images, five from each video, and we\nused three different metrics: accuracy rate, precision rate, and processing\ntime. By using accuracy and precision, we aim to identify how well the\nalgorithms perform in detection and segmentation, while using the processing\ntime to evaluate the impact on the computational system. Results indicate that\nall three algorithms had more than 90% of precision rate, while obtaining an\naverage of 80% on accuracy. The algorithm with the lowest impact on processing\ntime allowed the computation of 60 frames per second.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:07:40 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Marcomini", "L. A.", ""], ["Cunha", "A. L.", ""]]}, {"id": "1810.02845", "submitter": "Salvator Lombardo", "authors": "Jun Han, Salvator Lombardo, Christopher Schroers, Stephan Mandt", "title": "Deep Generative Video Compression", "comments": "Accepted at NeurIPS 2019, 15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of deep generative models for image compression has led to\nimpressive performance gains over classical codecs while neural video\ncompression is still in its infancy. Here, we propose an end-to-end, deep\ngenerative modeling approach to compress temporal sequences with a focus on\nvideo. Our approach builds upon variational autoencoder (VAE) models for\nsequential data and combines them with recent work on neural image compression.\nThe approach jointly learns to transform the original sequence into a\nlower-dimensional representation as well as to discretize and entropy code this\nrepresentation according to predictions of the sequential VAE. Rate-distortion\nevaluations on small videos from public data sets with varying complexity and\ndiversity show that our model yields competitive results when trained on\ngeneric video content. Extreme compression performance is achieved when\ntraining the model on specialized content.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:42:02 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 22:48:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Han", "Jun", ""], ["Lombardo", "Salvator", ""], ["Schroers", "Christopher", ""], ["Mandt", "Stephan", ""]]}, {"id": "1810.02862", "submitter": "Zheng Liu", "authors": "Zheng Liu, Botao Xiao, Muhammad Alrabeiah, Keyan Wang, Jun Chen", "title": "Generic Model-Agnostic Convolutional Neural Network for Single Image\n  Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze and smog are among the most common environmental factors impacting image\nquality and, therefore, image analysis. This paper proposes an end-to-end\ngenerative method for image dehazing. It is based on designing a fully\nconvolutional neural network to recognize haze structures in input images and\nrestore clear, haze-free images. The proposed method is agnostic in the sense\nthat it does not explore the atmosphere scattering model. Somewhat\nsurprisingly, it achieves superior performance relative to all existing\nstate-of-the-art methods for image dehazing even on SOTS outdoor images, which\nare synthesized using the atmosphere scattering model.\n  Project detail and code can be found here:\nhttps://github.com/Seanforfun/GMAN_Net_Haze_Removal\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 19:35:28 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 15:48:24 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liu", "Zheng", ""], ["Xiao", "Botao", ""], ["Alrabeiah", "Muhammad", ""], ["Wang", "Keyan", ""], ["Chen", "Jun", ""]]}, {"id": "1810.02897", "submitter": "Ye Zhu PhD", "authors": "Ye Zhu, Kai Ming Ting, Mark Carman, Maia Angelova", "title": "CDF Transform-and-Shift: An effective way to deal with datasets of\n  inhomogeneous cluster densities", "comments": "Pattern Recognition (2021)", "journal-ref": null, "doi": "10.1016/j.patcog.2021.107977", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inhomogeneous cluster densities has been a long-standing issue\nfor distance-based and density-based algorithms in clustering and anomaly\ndetection. These algorithms implicitly assume that all clusters have\napproximately the same density. As a result, they often exhibit a bias towards\ndense clusters in the presence of sparse clusters. Many remedies have been\nsuggested; yet, we show that they are partial solutions which do not address\nthe issue satisfactorily. To match the implicit assumption, we propose to\ntransform a given dataset such that the transformed clusters have approximately\nthe same density while all regions of locally low density become globally low\ndensity -- homogenising cluster density while preserving the cluster structure\nof the dataset. We show that this can be achieved by using a new\nmulti-dimensional Cumulative Distribution Function in a transform-and-shift\nmethod. The method can be applied to every dataset, before the dataset is used\nin many existing algorithms to match their implicit assumption without\nalgorithmic modification. We show that the proposed method performs better than\nexisting remedies.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 22:32:51 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 08:53:59 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 04:27:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhu", "Ye", ""], ["Ting", "Kai Ming", ""], ["Carman", "Mark", ""], ["Angelova", "Maia", ""]]}, {"id": "1810.02936", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang\n  and Hongsheng Li", "title": "FD-GAN: Pose-guided Feature Distilling GAN for Robust Person\n  Re-identification", "comments": "Accepted in Proceedings of 32nd Conference on Neural Information\n  Processing Systems (NeurIPS 2018). Code available:\n  https://github.com/yxgeee/FD-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (reID) is an important task that requires to\nretrieve a person's images from an image dataset, given one image of the person\nof interest. For learning robust person features, the pose variation of person\nimages is one of the key challenges. Existing works targeting the problem\neither perform human alignment, or learn human-region-based representations.\nExtra pose information and computational cost is generally required for\ninference. To solve this issue, a Feature Distilling Generative Adversarial\nNetwork (FD-GAN) is proposed for learning identity-related and pose-unrelated\nrepresentations. It is a novel framework based on a Siamese structure with\nmultiple novel discriminators on human poses and identities. In addition to the\ndiscriminators, a novel same-pose loss is also integrated, which requires\nappearance of a same person's generated images to be similar. After learning\npose-unrelated person features with pose guidance, no auxiliary pose\ninformation and additional computational cost is required during testing. Our\nproposed FD-GAN achieves state-of-the-art performance on three person reID\ndatasets, which demonstrates that the effectiveness and robust feature\ndistilling capability of the proposed FD-GAN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 05:17:18 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 14:38:17 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Ge", "Yixiao", ""], ["Li", "Zhuowan", ""], ["Zhao", "Haiyu", ""], ["Yin", "Guojun", ""], ["Yi", "Shuai", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1810.02981", "submitter": "Ruslan Dautov", "authors": "Artur Kuzin, Artur Fattakhov, Ilya Kibardin, Vladimir Iglovikov,\n  Ruslan Dautov", "title": "Camera Model Identification Using Convolutional Neural Networks", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source camera identification is the process of determining which camera or\nmodel has been used to capture an image. In the recent years, there has been a\nrapid growth of research interest in the domain of forensics. In the current\nwork, we describe our Deep Learning approach to the camera detection task of 10\ncameras as a part of the Camera Model Identification Challenge hosted by\nKaggle.com where our team finished 2nd out of 582 teams with the accuracy on\nthe unseen data of 98%. We used aggressive data augmentations that allowed a\nmodel to stay robust against transformations. A number of experiments are\ncarried out on datasets collected by organizers and scraped from the web.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 11:11:31 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 07:47:43 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Kuzin", "Artur", ""], ["Fattakhov", "Artur", ""], ["Kibardin", "Ilya", ""], ["Iglovikov", "Vladimir", ""], ["Dautov", "Ruslan", ""]]}, {"id": "1810.02994", "submitter": "Yiming Wu", "authors": "Yiming Wu, Wei Ji, Xi Li, Gang Wang, Jianwei Yin and Fei Wu", "title": "Context-Aware Deep Spatio-Temporal Network for Hand Pose Estimation from\n  Depth Images", "comments": "IEEE Transactions On Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2018.2873733", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As a fundamental and challenging problem in computer vision, hand pose\nestimation aims to estimate the hand joint locations from depth images.\nTypically, the problem is modeled as learning a mapping function from images to\nhand joint coordinates in a data-driven manner. In this paper, we propose\nContext-Aware Deep Spatio-Temporal Network (CADSTN), a novel method to jointly\nmodel the spatio-temporal properties for hand pose estimation. Our proposed\nnetwork is able to learn the representations of the spatial information and the\ntemporal structure from the image sequences. Moreover, by adopting adaptive\nfusion method, the model is capable of dynamically weighting different\npredictions to lay emphasis on sufficient context. Our method is examined on\ntwo common benchmarks, the experimental results demonstrate that our proposed\napproach achieves the best or the second-best performance with state-of-the-art\nmethods and runs in 60fps.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 12:25:50 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wu", "Yiming", ""], ["Ji", "Wei", ""], ["Li", "Xi", ""], ["Wang", "Gang", ""], ["Yin", "Jianwei", ""], ["Wu", "Fei", ""]]}, {"id": "1810.03043", "submitter": "Frederik Ebert", "authors": "Frederik Ebert, Sudeep Dasari, Alex X. Lee, Sergey Levine and Chelsea\n  Finn", "title": "Robustness via Retrying: Closed-Loop Robotic Manipulation with\n  Self-Supervised Learning", "comments": "accepted at the Conference on Robot Learning (CoRL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction is an appealing objective for self-supervised learning of\nbehavioral skills, particularly for autonomous robots. However, effectively\nutilizing predictive models for control, especially with raw image inputs,\nposes a number of major challenges. How should the predictions be used? What\nhappens when they are inaccurate? In this paper, we tackle these questions by\nproposing a method for learning robotic skills from raw image observations,\nusing only autonomously collected experience. We show that even an imperfect\nmodel can complete complex tasks if it can continuously retry, but this\nrequires the model to not lose track of the objective (e.g., the object of\ninterest). To enable a robot to continuously retry a task, we devise a\nself-supervised algorithm for learning image registration, which can keep track\nof objects of interest for the duration of the trial. We demonstrate that this\nidea can be combined with a video-prediction based controller to enable complex\nbehaviors to be learned from scratch using only raw visual inputs, including\ngrasping, repositioning objects, and non-prehensile manipulation. Our\nreal-world experiments demonstrate that a model trained with 160 robot hours of\nautonomously collected, unlabeled data is able to successfully perform complex\nmanipulation tasks with a wide range of objects not seen during training.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 19:51:46 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ebert", "Frederik", ""], ["Dasari", "Sudeep", ""], ["Lee", "Alex X.", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1810.03051", "submitter": "Praneeth Narayanamurthy", "authors": "Praneeth Narayanamurthy and Vahid Daneshpajooh and Namrata Vaswani", "title": "Provable Subspace Tracking from Missing Data and Matrix Completion", "comments": "Writing changes; includes a detailed discussion of noise analysis;\n  contains discussion for Matrix Completion; Accepted to IEEE Transactions on\n  Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing (Volume: 67 , Issue: 16 ,\n  Aug, 15 2019)", "doi": "10.1109/TSP.2019.2924595", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of subspace tracking in the presence of missing data\n(ST-miss). In recent work, we studied a related problem called robust ST. In\nthis work, we show that a simple modification of our robust ST solution also\nprovably solves ST-miss and robust ST-miss. To our knowledge, our result is the\nfirst `complete' guarantee for ST-miss. This means that we can prove that under\nassumptions on only the algorithm inputs, the output subspace estimates are\nclose to the true data subspaces at all times. Our guarantees hold under mild\nand easily interpretable assumptions, and allow the underlying subspace to\nchange with time in a piecewise constant fashion. In contrast, all existing\nguarantees for ST are partial results and assume a fixed unknown subspace.\nExtensive numerical experiments are shown to back up our theoretical claims.\nFinally, our solution can be interpreted as a provably correct mini-batch and\nmemory-efficient solution to low-rank Matrix Completion (MC).\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 20:54:25 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 04:49:34 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Narayanamurthy", "Praneeth", ""], ["Daneshpajooh", "Vahid", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1810.03065", "submitter": "Fabian Manhardt", "authors": "Fabian Manhardt and Wadim Kehl and Nassir Navab and Federico Tombari", "title": "Deep Model-Based 6D Pose Refinement in RGB", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for model-based 6D pose refinement in color data.\nBuilding on the established idea of contour-based pose tracking, we teach a\ndeep neural network to predict a translational and rotational update. At the\ncore, we propose a new visual loss that drives the pose update by aligning\nobject contours, thus avoiding the definition of any explicit appearance model.\nIn contrast to previous work our method is correspondence-free,\nsegmentation-free, can handle occlusion and is agnostic to geometrical symmetry\nas well as visual ambiguities. Additionally, we observe a strong robustness\ntowards rough initialization. The approach can run in real-time and produces\npose accuracies that come close to 3D ICP without the need for depth data.\nFurthermore, our networks are trained from purely synthetic data and will be\npublished together with the refinement code to ensure reproducibility.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 01:22:57 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Manhardt", "Fabian", ""], ["Kehl", "Wadim", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1810.03075", "submitter": "Yao Xue", "authors": "Yao Xue, Gilbert Bigras, Judith Hugh, Nilanjan Ray", "title": "Training Convolutional Neural Networks and Compressed Sensing End-to-End\n  for Microscopy Cell Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated cell detection and localization from microscopy images are\nsignificant tasks in biomedical research and clinical practice. In this paper,\nwe design a new cell detection and localization algorithm that combines deep\nconvolutional neural network (CNN) and compressed sensing (CS) or sparse coding\n(SC) for end-to-end training. We also derive, for the first time, a\nbackpropagation rule, which is applicable to train any algorithm that\nimplements a sparse code recovery layer. The key observation behind our\nalgorithm is that cell detection task is a point object detection task in\ncomputer vision, where the cell centers (i.e., point objects) occupy only a\ntiny fraction of the total number of pixels in an image. Thus, we can apply\ncompressed sensing (or, equivalently sparse coding) to compactly represent a\nvariable number of cells in a projected space. Then, CNN regresses this\ncompressed vector from the input microscopy image. Thanks to the SC/CS recovery\nalgorithm (L1 optimization) that can recover sparse cell locations from the\noutput of CNN. We train this entire processing pipeline end-to-end and\ndemonstrate that end-to-end training provides accuracy improvements over a\ntraining paradigm that treats CNN and CS-recovery layers separately. Our\nalgorithm design also takes into account a form of ensemble average of trained\nmodels naturally to further boost accuracy of cell detection. We have validated\nour algorithm on benchmark datasets and achieved excellent performances.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 02:34:54 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Xue", "Yao", ""], ["Bigras", "Gilbert", ""], ["Hugh", "Judith", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1810.03077", "submitter": "Sudharshan Suresh", "authors": "Sudharshan Suresh, Nathaniel Chodosh, Montiel Abello", "title": "DeepGeo: Photo Localization with Deep Neural Network", "comments": "7 pages, 9 figures. Pre-print after submission to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the task of determining the geographical location of\nan image, a pertinent problem in learning and computer vision. This research\nwas inspired from playing GeoGuessr, a game that tests a humans' ability to\nlocalize themselves using just images of their surroundings. In particular, we\nwish to investigate how geographical, ecological and man-made features\ngeneralize for random location prediction. This is framed as a classification\nproblem: given images sampled from the USA, the most-probable state among 50 is\npredicted. Previous work uses models extensively trained on large, unfiltered\nonline datasets that are primed towards specific locations. To this end, we\ncreate (and open-source) the 50States10K dataset - with 0.5 million Google\nStreet View images of the country. A deep neural network based on the ResNet\narchitecture is trained, and four different strategies of incorporating\nlow-level cardinality information are presented. This model achieves an\naccuracy 20 times better than chance on a test dataset, which rises to 71.87%\nwhen taking the best of top-5 guesses. The network also beats human subjects in\n4 out of 5 rounds of GeoGuessr.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 03:10:57 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Suresh", "Sudharshan", ""], ["Chodosh", "Nathaniel", ""], ["Abello", "Montiel", ""]]}, {"id": "1810.03105", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Licheng Jiao, Kaiwen Zhou, James Cheng, Yan Ren, Yufei\n  Jin", "title": "ASVRG: Accelerated Proximal SVRG", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an accelerated proximal stochastic variance reduced\ngradient (ASVRG) method, in which we design a simple and effective momentum\nacceleration trick. Unlike most existing accelerated stochastic variance\nreduction methods such as Katyusha, ASVRG has only one additional variable and\none momentum parameter. Thus, ASVRG is much simpler than those methods, and has\nmuch lower per-iteration complexity. We prove that ASVRG achieves the best\nknown oracle complexities for both strongly convex and non-strongly convex\nobjectives. In addition, we extend ASVRG to mini-batch and non-smooth settings.\nWe also empirically verify our theoretical results and show that the\nperformance of ASVRG is comparable with, and sometimes even better than that of\nthe state-of-the-art stochastic methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 08:43:05 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 17:38:34 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Shang", "Fanhua", ""], ["Jiao", "Licheng", ""], ["Zhou", "Kaiwen", ""], ["Cheng", "James", ""], ["Ren", "Yan", ""], ["Jin", "Yufei", ""]]}, {"id": "1810.03143", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Robbert W. van Hamersvelt and Max A. Viergever\n  and Tim Leiner and Ivana I\\v{s}gum", "title": "Coronary Artery Centerline Extraction in Cardiac CT Angiography Using a\n  CNN-Based Orientation Classifier", "comments": "Accepted in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2018.10.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery centerline extraction in cardiac CT angiography (CCTA) images\nis a prerequisite for evaluation of stenoses and atherosclerotic plaque. We\npropose an algorithm that extracts coronary artery centerlines in CCTA using a\nconvolutional neural network (CNN).\n  A 3D dilated CNN is trained to predict the most likely direction and radius\nof an artery at any given point in a CCTA image based on a local image patch.\nStarting from a single seed point placed manually or automatically anywhere in\na coronary artery, a tracker follows the vessel centerline in two directions\nusing the predictions of the CNN. Tracking is terminated when no direction can\nbe identified with high certainty.\n  The CNN was trained using 32 manually annotated centerlines in a training set\nconsisting of 8 CCTA images provided in the MICCAI 2008 Coronary Artery\nTracking Challenge (CAT08). Evaluation using 24 test images of the CAT08\nchallenge showed that extracted centerlines had an average overlap of 93.7%\nwith 96 manually annotated reference centerlines. Extracted centerline points\nwere highly accurate, with an average distance of 0.21 mm to reference\ncenterline points. In a second test set consisting of 50 CCTA scans, 5,448\nmarkers in the coronary arteries were used as seed points to extract single\ncenterlines. This showed strong correspondence between extracted centerlines\nand manually placed markers. In a third test set containing 36 CCTA scans,\nfully automatic seeding and centerline extraction led to extraction of on\naverage 92% of clinically relevant coronary artery segments.\n  The proposed method is able to accurately and efficiently determine the\ndirection and radius of coronary arteries. The method can be trained with\nlimited training data, and once trained allows fast automatic or interactive\nextraction of coronary artery trees from CCTA images.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 13:45:51 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 13:20:35 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["van Hamersvelt", "Robbert W.", ""], ["Viergever", "Max A.", ""], ["Leiner", "Tim", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1810.03155", "submitter": "JuanLuis GonzalezBello", "authors": "Juan Luis Gonzalez, Muhammad Sarmad, Hyunjoo J.Lee, Munchurl Kim", "title": "Finding Correspondences for Optical Flow and Disparity Estimations using\n  a Sub-pixel Convolution-based Encoder-Decoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) have recently shown promising\nresults in low-level computer vision problems such as optical flow and\ndisparity estimation, but still, have much room to further improve their\nperformance. In this paper, we propose a novel sub-pixel convolution-based\nencoder-decoder network for optical flow and disparity estimations, which can\nextend FlowNetS and DispNet by replacing the deconvolution layers with\nsup-pixel convolution blocks. By using sub-pixel refinement and estimation on\nthe decoder stages instead of deconvolution, we can significantly improve the\nestimation accuracy for optical flow and disparity, even with reduced numbers\nof parameters. We show a supervised end-to-end training of our proposed\nnetworks for optical flow and disparity estimations, and an unsupervised\nend-to-end training for monocular depth and pose estimations. In order to\nverify the effectiveness of our proposed networks, we perform intensive\nexperiments for (i) optical flow and disparity estimations, and (ii) monocular\ndepth and pose estimations. Throughout the extensive experiments, our proposed\nnetworks outperform the baselines such as FlowNetS and DispNet in terms of\nestimation accuracy and training times.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 14:41:37 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Gonzalez", "Juan Luis", ""], ["Sarmad", "Muhammad", ""], ["Lee", "Hyunjoo J.", ""], ["Kim", "Munchurl", ""]]}, {"id": "1810.03173", "submitter": "Saed Moradi", "authors": "Saed Moradi, Payman Moallem, Mohamad Farzan Sabahi", "title": "Fast and Robust Small Infrared Target Detection Using Absolute\n  Directional Mean Difference Algorithm", "comments": "The Final version (Accepted in Signal Processing journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared small target detection in an infrared search and track (IRST) system\nis a challenging task. This situation becomes more complicated when high\ngray-intensity structural backgrounds appear in the field of view (FoV) of the\ninfrared seeker. While the majority of the infrared small target detection\nalgorithms neglect directional information, in this paper, a directional\napproach is presented to suppress structural backgrounds and develop a more\neffective detection algorithm. To this end, a similar concept to the average\nabsolute gray difference (AAGD) is utilized to construct a novel directional\nsmall target detection algorithm called absolute directional mean difference\n(ADMD). Also, an efficient implementation procedure is presented for the\nproposed algorithm. The proposed algorithm effectively enhances the target area\nand eliminates background clutter. Simulation results on real infrared images\nprove the significant effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 16:16:42 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 17:14:17 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 16:28:58 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 03:44:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Moradi", "Saed", ""], ["Moallem", "Payman", ""], ["Sabahi", "Mohamad Farzan", ""]]}, {"id": "1810.03213", "submitter": "Mason Swofford", "authors": "Mason Swofford", "title": "Image Completion on CIFAR-10", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This project performed image completion on CIFAR-10, a dataset of 60,000\n32x32 RGB images, using three different neural network architectures: fully\nconvolutional networks, convolutional networks with fully connected layers, and\nencoder-decoder convolutional networks. The highest performing model was a deep\nfully convolutional network, which was able to achieve a mean squared error of\n.015 when comparing the original image pixel values with the predicted pixel\nvalues. As well, this network was able to output in-painted images which\nappeared real to the human eye.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 21:44:32 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Swofford", "Mason", ""]]}, {"id": "1810.03234", "submitter": "Rickard Br\\\"uel Gabrielsson", "authors": "Rickard Br\\\"uel Gabrielsson and Gunnar Carlsson", "title": "Exposition and Interpretation of the Topology of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN's) are powerful and widely used tools.\nHowever, their interpretability is far from ideal. One such shortcoming is the\ndifficulty of deducing a network's ability to generalize to unseen data. We use\ntopological data analysis to show that the information encoded in the weights\nof a CNN can be organized in terms of a topological data model and demonstrate\nhow such information can be interpreted and utilized. We show that the weights\nof convolutional layers at depths from 1 through 13 learn simple global\nstructures. We also demonstrate the change of the simple structures over the\ncourse of training. In particular, we define and analyze the spaces of spatial\nfilters of convolutional layers and show the recurrence, among all networks,\ndepths, and during training, of a simple circle consisting of rotating edges,\nas well as a less recurring unanticipated complex circle that combines lines,\nedges, and non-linear patterns. We also demonstrate that topological structure\ncorrelates with a network's ability to generalize to unseen data and that\ntopological information can be used to improve a network's performance. We\ntrain over a thousand CNN's on MNIST, CIFAR-10, SVHN, and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 00:34:25 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:21:07 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 04:24:28 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Gabrielsson", "Rickard Br\u00fcel", ""], ["Carlsson", "Gunnar", ""]]}, {"id": "1810.03237", "submitter": "Stephen James", "authors": "Stephen James, Michael Bloesch, Andrew J. Davison", "title": "Task-Embedded Control Networks for Few-Shot Imitation Learning", "comments": "Published at the Conference on Robot Learning (CoRL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much like humans, robots should have the ability to leverage knowledge from\npreviously learned tasks in order to learn new tasks quickly in new and\nunfamiliar environments. Despite this, most robot learning approaches have\nfocused on learning a single task, from scratch, with a limited notion of\ngeneralisation, and no way of leveraging the knowledge to learn other tasks\nmore efficiently. One possible solution is meta-learning, but many of the\nrelated approaches are limited in their ability to scale to a large number of\ntasks and to learn further tasks without forgetting previously learned ones.\nWith this in mind, we introduce Task-Embedded Control Networks, which employ\nideas from metric learning in order to create a task embedding that can be used\nby a robot to learn new tasks from one or more demonstrations. In the area of\nvisually-guided manipulation, we present simulation results in which we surpass\nthe performance of a state-of-the-art method when using only visual information\nfrom each demonstration. Additionally, we demonstrate that our approach can\nalso be used in conjunction with domain randomisation to train our few-shot\nlearning ability in simulation and then deploy in the real world without any\nadditional training. Once deployed, the robot can learn new tasks from a single\nreal-world demonstration.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 00:57:24 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["James", "Stephen", ""], ["Bloesch", "Michael", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1810.03241", "submitter": "Victor Stamatescu", "authors": "Victor Stamatescu and Mark D. McDonnell", "title": "Diagnosing Convolutional Neural Networks using their Spectral Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are a class of artificial neural\nnetworks whose computational blocks use convolution, together with other linear\nand non-linear operations, to perform classification or regression. This paper\nexplores the spectral response of CNNs and its potential use in diagnosing\nproblems with their training. We measure the gain of CNNs trained for image\nclassification on ImageNet and observe that the best models are also the most\nsensitive to perturbations of their input. Further, we perform experiments on\nMNIST and CIFAR-10 to find that the gain rises as the network learns and then\nsaturates as the network converges. Moreover, we find that strong gain\nfluctuations can point to overfitting and learning problems caused by a poor\nchoice of learning rate. We argue that the gain of CNNs can act as a diagnostic\ntool and potential replacement for the validation loss when hold-out validation\ndata are not available.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 01:27:34 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Stamatescu", "Victor", ""], ["McDonnell", "Mark D.", ""]]}, {"id": "1810.03243", "submitter": "Changsheng Lu", "authors": "Changsheng Lu, Siyu Xia, Ming Shao, Yun Fu", "title": "Arc-support Line Segments Revisited: An Efficient and High-quality\n  Ellipse Detection", "comments": "IEEE Transactions on Image Processing; The paper has been revised to\n  a more appropriate title \"Arc-support Line Segments Revisited: An Efficient\n  and High-quality Ellipse Detection\"; In addition, please contacts us if you\n  want a higher quality pdf due to the limited uploading size of files in arXiv\n  where the resolution of figures might be not very high", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years many ellipse detection algorithms spring up and are studied\nbroadly, while the critical issue of detecting ellipses accurately and\nefficiently in real-world images remains a challenge. In this paper, we propose\na valuable industry-oriented ellipse detector by arc-support line segments,\nwhich simultaneously reaches high detection accuracy and efficiency. To\nsimplify the complicated curves in an image while retaining the general\nproperties including convexity and polarity, the arc-support line segments are\nextracted, which grounds the successful detection of ellipses. The arc-support\ngroups are formed by iteratively and robustly linking the arc-support line\nsegments that latently belong to a common ellipse. Afterward, two complementary\napproaches, namely, locally selecting the arc-support group with higher\nsaliency and globally searching all the valid paired groups, are adopted to fit\nthe initial ellipses in a fast way. Then, the ellipse candidate set can be\nformulated by hierarchical clustering of 5D parameter space of initial\nellipses. Finally, the salient ellipse candidates are selected and refined as\ndetections subject to the stringent and effective verification. Extensive\nexperiments on three public datasets are implemented and our method achieves\nthe best F-measure scores compared to the state-of-the-art methods. The source\ncode is available at\nhttps://github.com/AlanLuSun/High-quality-ellipse-detection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 01:41:44 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 16:41:50 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 08:28:32 GMT"}, {"version": "v4", "created": "Sat, 9 Mar 2019 03:15:01 GMT"}, {"version": "v5", "created": "Sun, 4 Aug 2019 14:49:23 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lu", "Changsheng", ""], ["Xia", "Siyu", ""], ["Shao", "Ming", ""], ["Fu", "Yun", ""]]}, {"id": "1810.03254", "submitter": "Xi Cheng", "authors": "Xi Cheng, Xiang Li, Jian Yang", "title": "Triple Attention Mixed Link Network for Single Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Single image super resolution is of great importance as a low-level computer\nvision task. Recent approaches with deep convolutional neural networks have\nachieved im-pressive performance. However, existing architectures have\nlimitations due to the less sophisticated structure along with less strong\nrepresentational power. In this work, to significantly enhance the feature\nrepresentation, we proposed Triple Attention mixed link Network (TAN) which\nconsists of 1) three different aspects (i.e., kernel, spatial and channel) of\nattention mechanisms and 2) fu-sion of both powerful residual and dense\nconnections (i.e., mixed link). Specifically, the network with multi kernel\nlearns multi hierarchical representations under different receptive fields. The\noutput features are recalibrated by the effective kernel and channel attentions\nand feed into next layer partly residual and partly dense, which filters the\ninformation and enable the network to learn more powerful representations. The\nfeatures finally pass through the spatial attention in the reconstruction\nnetwork which generates a fusion of local and global information, let the\nnetwork restore more details and improves the quality of reconstructed images.\nThanks to the diverse feature recalibrations and the advanced information flow\ntopology, our proposed model is strong enough to per-form against the\nstate-of-the-art methods on the bench-mark evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 03:04:07 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Cheng", "Xi", ""], ["Li", "Xiang", ""], ["Yang", "Jian", ""]]}, {"id": "1810.03270", "submitter": "Boyi Yang", "authors": "Boyi Yang, Marina Piccinelli, Gaetano Esposito, Tianli Han, Yasir\n  Bouchi, Bill Gogas, Don Giddens, Habib Samady, Alessandro Veneziani", "title": "Patient-Specific 3D Volumetric Reconstruction of Bioresorbable Stents: A\n  Method to Generate 3D Geometries for Computational Analysis of Coronaries\n  Treated with Bioresorbable Stents", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As experts continue to debate the optimal surgery practice for coronary\ndisease - percutaneous coronary intervention (PCI) or coronary aortic bypass\ngraft (CABG) - computational tools may provide a quantitative assessment of\neach option. Computational fluid dynamics (CFD) has been used to assess the\ninterplay between hemodynamics and stent struts; it is of particular interest\nin Bioresorbable Vascular Stents (BVS), since their thicker struts may result\nin impacted flow patterns and possible pathological consequences. Many proofs\nof concept are presented in the literature; however, a practical method for\nextracting patient-specific stented coronary artery geometries from images over\na large number of patients remains an open problem.\n  This work provides a possible pipeline for the reconstruction of the BVS.\nUsing Optical Coherence Tomographies (OCT) and Invasive Coronary Angiographies\n(ICA), we can reconstruct the 3D geometry of deployed BVS in vivo. We\nillustrate the stent reconstruction process: (i) automatic strut detection,\n(ii) identification of stent components, (iii) 3D registration of stent\ncurvature, and (iv) final stent volume reconstruction. The methodology is\ndesigned for use on clinical OCT images, as opposed to approaches that relied\non a small number of virtually deployed stents.\n  The proposed reconstruction process is validated with a virtual phantom\nstent, providing quantitative assessment of the methodology, and with selected\nclinical cases, confirming feasibility. Using multimodality image analysis, we\nobtain reliable reconstructions within a reasonable timeframe. This work is the\nfirst step toward a fully automated reconstruction and simulation procedure\naiming at an extensive quantitative analysis of the impact of BVS struts on\nhemodynamics via CFD in clinical trials, going beyond the proof-of-concept\nstage.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 04:58:49 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Yang", "Boyi", ""], ["Piccinelli", "Marina", ""], ["Esposito", "Gaetano", ""], ["Han", "Tianli", ""], ["Bouchi", "Yasir", ""], ["Gogas", "Bill", ""], ["Giddens", "Don", ""], ["Samady", "Habib", ""], ["Veneziani", "Alessandro", ""]]}, {"id": "1810.03272", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Chunhua Shen, Ian Reid", "title": "Light-Weight RefineNet for Real-Time Semantic Segmentation", "comments": "Models are available here:\n  https://github.com/drsleep/light-weight-refinenet, BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an important task of effective and efficient semantic image\nsegmentation. In particular, we adapt a powerful semantic segmentation\narchitecture, called RefineNet, into the more compact one, suitable even for\ntasks requiring real-time performance on high-resolution inputs. To this end,\nwe identify computationally expensive blocks in the original setup, and propose\ntwo modifications aimed to decrease the number of parameters and floating point\noperations. By doing that, we achieve more than twofold model reduction, while\nkeeping the performance levels almost intact. Our fastest model undergoes a\nsignificant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on\n512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL\nVOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7%\nmean iou on the same dataset. Alternatively, we showcase that our approach is\neasily mixable with light-weight classification networks: we attain 79.2% mean\niou on PASCAL VOC using a model that contains only 3.3M parameters and performs\nonly 9.3B floating point operations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 05:18:46 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1810.03275", "submitter": "Clemens Schiffer", "authors": "Clemens Schiffer", "title": "TV-regularized CT Reconstruction and Metal Artifact Reduction Using\n  Inequality Constraints with Preconditioning", "comments": "Master's Thesis, as submitted at the University of Graz", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Total variation(TV) regularization is applied to X-Ray computed\ntomography(CT) in an effort to reduce metal artifacts. Tikhonov regularization\nwith $L^2$ data fidelity term and total variation regularization is augmented\nin this novel model by inequality constraints on sinogram data affected by\nmetal to model errors caused by metal. The formulated problem is discretized\nand solved using the Chambolle-Pock algorithm. Faster convergence is achieved\nusing preconditioning in a Douglas-Rachford spitting method as well as Advanced\nDirection Method of Multipliers(ADMM). The methods are applied to real and\nsynthetic data demonstrating feasibility of the model to reduce metal\nartifacts. Technical details of CT data used and its processing are given in\nthe appendix.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 05:43:44 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Schiffer", "Clemens", ""]]}, {"id": "1810.03286", "submitter": "Yuxiao Yan", "authors": "Tongtong Zhao, Yuxiao Yan, Jinjia Peng, Zetian Mi, Xianping Fu", "title": "Guiding Intelligent Surveillance System by learning-by-synthesis gaze\n  estimation", "comments": "Submit to the journal of Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel learning-by-synthesis method for estimating gaze\ndirection of an automated intelligent surveillance system. Recently, progress\nin learning-by-synthesis has proposed training models on synthetic images,\nwhich can effectively reduce the cost of manpower and material resources.\nHowever, learning from synthetic images still fails to achieve the desired\nperformance compared to naturalistic images due to the different distribution\nof synthetic images. In an attempt to address this issue, previous method is to\nimprove the realism of synthetic images by learning a model. However, the\ndisadvantage of the method is that the distortion has not been improved and the\nauthenticity level is unstable. To solve this problem, we put forward a new\nstructure to improve synthetic images, via the reference to the idea of style\ntransformation, through which we can efficiently reduce the distortion of\npictures and minimize the need of real data annotation. We estimate that this\nenables generation of highly realistic images, which we demonstrate both\nqualitatively and with a user study. We quantitatively evaluate the generated\nimages by training models for gaze estimation. We show a significant\nimprovement over using synthetic images, and achieve state-of-the-art results\non various datasets including MPIIGaze dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:02:06 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zhao", "Tongtong", ""], ["Yan", "Yuxiao", ""], ["Peng", "Jinjia", ""], ["Mi", "Zetian", ""], ["Fu", "Xianping", ""]]}, {"id": "1810.03292", "submitter": "Julius Adebayo", "authors": "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz\n  Hardt, Been Kim", "title": "Sanity Checks for Saliency Maps", "comments": "Updating Guided Backprop experiments due to bug. The results and\n  conclusions remain the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:27:11 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 03:39:34 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 13:40:14 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Adebayo", "Julius", ""], ["Gilmer", "Justin", ""], ["Muelly", "Michael", ""], ["Goodfellow", "Ian", ""], ["Hardt", "Moritz", ""], ["Kim", "Been", ""]]}, {"id": "1810.03307", "submitter": "Julius Adebayo", "authors": "Julius Adebayo, Justin Gilmer, Ian Goodfellow, Been Kim", "title": "Local Explanation Methods for Deep Neural Networks Lack Sensitivity to\n  Parameter Values", "comments": "Workshop Track International Conference on Learning Representations\n  (ICLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the output of a complicated machine learning model like a deep\nneural network (DNN) is a central challenge in machine learning. Several\nproposed local explanation methods address this issue by identifying what\ndimensions of a single input are most responsible for a DNN's output. The goal\nof this work is to assess the sensitivity of local explanations to DNN\nparameter values. Somewhat surprisingly, we find that DNNs with\nrandomly-initialized weights produce explanations that are both visually and\nquantitatively similar to those produced by DNNs with learned weights. Our\nconjecture is that this phenomenon occurs because these explanations are\ndominated by the lower level features of a DNN, and that a DNN's architecture\nprovides a strong prior which significantly affects the representations learned\nat these lower layers. NOTE: This work is now subsumed by our recent\nmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we\nexpand on findings and address concerns raised in Sundararajan et. al. (2018).\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 08:18:14 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Adebayo", "Julius", ""], ["Gilmer", "Justin", ""], ["Goodfellow", "Ian", ""], ["Kim", "Been", ""]]}, {"id": "1810.03360", "submitter": "Fernando Alonso-Fernandez", "authors": "Fernando Alonso-Fernandez, Josef Bigun", "title": "A Survey on Periocular Biometrics Research", "comments": "Published in Pattern Recognition Letters", "journal-ref": "Pattern Recognition Letters, Special Issue An Insight on Eye\n  Biometrics, vol. 82, part 2, pp. 92-105, October 2016, ISSN: 0167-8655", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periocular refers to the facial region in the vicinity of the eye, including\neyelids, lashes and eyebrows. While face and irises have been extensively\nstudied, the periocular region has emerged as a promising trait for\nunconstrained biometrics, following demands for increased robustness of face or\niris systems. With a surprisingly high discrimination ability, this region can\nbe easily obtained with existing setups for face and iris, and the requirement\nof user cooperation can be relaxed, thus facilitating the interaction with\nbiometric systems. It is also available over a wide range of distances even\nwhen the iris texture cannot be reliably obtained (low resolution) or under\npartial face occlusion (close distances). Here, we review the state of the art\nin periocular biometrics research. A number of aspects are described,\nincluding: i) existing databases, ii) algorithms for periocular detection\nand/or segmentation, iii) features employed for recognition, iv) identification\nof the most discriminative regions of the periocular area, v) comparison with\niris and face modalities, vi) soft-biometrics (gender/ethnicity\nclassification), and vii) impact of gender transformation and plastic surgery\non the recognition accuracy. This work is expected to provide an insight of the\nmost relevant issues in periocular biometrics, giving a comprehensive coverage\nof the existing literature and current state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 10:15:15 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Alonso-Fernandez", "Fernando", ""], ["Bigun", "Josef", ""]]}, {"id": "1810.03382", "submitter": "Declan O'Regan", "authors": "Ghalib A. Bello, Timothy J.W. Dawes, Jinming Duan, Carlo Biffi,\n  Antonio de Marvao, Luke S.G.E. Howard, J. Simon R. Gibbs, Martin R. Wilkins,\n  Stuart A. Cook, Daniel Rueckert, and Declan P. O'Regan", "title": "Deep learning cardiac motion analysis for human survival prediction", "comments": null, "journal-ref": "Nature Machine Intelligence, 1, 95-104 (2019)", "doi": "10.1038/s42256-019-0019-2", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motion analysis is used in computer vision to understand the behaviour of\nmoving objects in sequences of images. Optimising the interpretation of dynamic\nbiological systems requires accurate and precise motion tracking as well as\nefficient representations of high-dimensional motion trajectories so that these\ncan be used for prediction tasks. Here we use image sequences of the heart,\nacquired using cardiac magnetic resonance imaging, to create time-resolved\nthree-dimensional segmentations using a fully convolutional network trained on\nanatomical shape priors. This dense motion model formed the input to a\nsupervised denoising autoencoder (4Dsurvival), which is a hybrid network\nconsisting of an autoencoder that learns a task-specific latent code\nrepresentation trained on observed outcome data, yielding a latent\nrepresentation optimised for survival prediction. To handle right-censored\nsurvival outcomes, our network used a Cox partial likelihood loss function. In\na study of 302 patients the predictive accuracy (quantified by Harrell's\nC-index) was significantly higher (p < .0001) for our model C=0.73 (95$\\%$ CI:\n0.68 - 0.78) than the human benchmark of C=0.59 (95$\\%$ CI: 0.53 - 0.65). This\nwork demonstrates how a complex computer vision task using high-dimensional\nmedical image data can efficiently predict human survival.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 11:34:38 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Bello", "Ghalib A.", ""], ["Dawes", "Timothy J. W.", ""], ["Duan", "Jinming", ""], ["Biffi", "Carlo", ""], ["de Marvao", "Antonio", ""], ["Howard", "Luke S. G. E.", ""], ["Gibbs", "J. Simon R.", ""], ["Wilkins", "Martin R.", ""], ["Cook", "Stuart A.", ""], ["Rueckert", "Daniel", ""], ["O'Regan", "Declan P.", ""]]}, {"id": "1810.03402", "submitter": "Di Hu", "authors": "Di Hu, Feiping Nie, Xuelong Li", "title": "Deep LDA Hashing", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional supervised hashing methods based on classification do not\nentirely meet the requirements of hashing technique, but Linear Discriminant\nAnalysis (LDA) does. In this paper, we propose to perform a revised LDA\nobjective over deep networks to learn efficient hashing codes in a truly\nend-to-end fashion. However, the complicated eigenvalue decomposition within\neach mini-batch in every epoch has to be faced with when simply optimizing the\ndeep network w.r.t. the LDA objective. In this work, the revised LDA objective\nis transformed into a simple least square problem, which naturally overcomes\nthe intractable problems and can be easily solved by the off-the-shelf\noptimizer. Such deep extension can also overcome the weakness of LDA Hashing in\nthe limited linear projection and feature learning. Amounts of experiments are\nconducted on three benchmark datasets. The proposed Deep LDA Hashing shows\nnearly 70 points improvement over the conventional one on the CIFAR-10 dataset.\nIt also beats several state-of-the-art methods on various metrics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:27:35 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Hu", "Di", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1810.03410", "submitter": "Arul Selvam Periyasamy", "authors": "Arul Selvam Periyasamy, Max Schwarz, and Sven Behnke", "title": "Robust 6D Object Pose Estimation in Cluttered Scenes using Semantic\n  Segmentation and Pose Regression Networks", "comments": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS), Madrid, Spain, to appear October 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose estimation is a crucial prerequisite for robots to perform\nautonomous manipulation in clutter. Real-world bin-picking settings such as\nwarehouses present additional challenges, e.g., new objects are added\nconstantly. Most of the existing object pose estimation methods assume that 3D\nmodels of the objects is available beforehand. We present a pipeline that\nrequires minimal human intervention and circumvents the reliance on the\navailability of 3D models by a fast data acquisition method and a synthetic\ndata generation procedure. This work builds on previous work on semantic\nsegmentation of cluttered bin-picking scenes to isolate individual objects in\nclutter. An additional network is trained on synthetic scenes to estimate\nobject poses from a cropped object-centered encoding extracted from the\nsegmentation results. The proposed method is evaluated on a synthetic\nvalidation dataset and cluttered real-world scenes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:48:02 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Periyasamy", "Arul Selvam", ""], ["Schwarz", "Max", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.03414", "submitter": "Di Hu", "authors": "Di Hu, Feiping Nie, Xuelong Li", "title": "Dense Multimodal Fusion for Hierarchically Joint Representation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple modalities can provide more valuable information than single one by\ndescribing the same contents in various ways. Hence, it is highly expected to\nlearn effective joint representation by fusing the features of different\nmodalities. However, previous methods mainly focus on fusing the shallow\nfeatures or high-level representations generated by unimodal deep networks,\nwhich only capture part of the hierarchical correlations across modalities. In\nthis paper, we propose to densely integrate the representations by greedily\nstacking multiple shared layers between different modality-specific networks,\nwhich is named as Dense Multimodal Fusion (DMF). The joint representations in\ndifferent shared layers can capture the correlations in different levels, and\nthe connection between shared layers also provides an efficient way to learn\nthe dependence among hierarchical correlations. These two properties jointly\ncontribute to the multiple learning paths in DMF, which results in faster\nconvergence, lower training loss, and better performance. We evaluate our model\non three typical multimodal learning tasks, including audiovisual speech\nrecognition, cross-modal retrieval, and multimodal classification. The\nnoticeable performance in the experiments demonstrates that our model can learn\nmore effective joint representation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:52:36 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Hu", "Di", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1810.03422", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, Yael Balbastre, Parashkev Nachev, John Ashburner", "title": "MRI Super-Resolution using Multi-Channel Total Variation", "comments": null, "journal-ref": "MIUA 2018. Communications in Computer and Information Science, vol\n  894", "doi": "10.1007/978-3-030-00928-1_97", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generative model for super-resolution in routine\nclinical magnetic resonance images (MRI), of arbitrary orientation and\ncontrast. The model recasts the recovery of high resolution images as an\ninverse problem, in which a forward model simulates the slice-select profile of\nthe MR scanner. The paper introduces a prior based on multi-channel total\nvariation for MRI super-resolution. Bias-variance trade-off is handled by\nestimating hyper-parameters from the low resolution input scans. The model was\nvalidated on a large database of brain images. The validation showed that the\nmodel can improve brain segmentation, that it can recover anatomical\ninformation between images of different MR contrasts, and that it generalises\nwell to the large variability present in MR images of different subjects. The\nimplementation is freely available at https://github.com/brudfors/spm_superres\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 13:14:28 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 15:08:37 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 09:22:03 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 10:17:10 GMT"}, {"version": "v5", "created": "Thu, 18 Oct 2018 13:44:22 GMT"}, {"version": "v6", "created": "Mon, 9 Sep 2019 16:42:17 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Brudfors", "Mikael", ""], ["Balbastre", "Yael", ""], ["Nachev", "Parashkev", ""], ["Ashburner", "John", ""]]}, {"id": "1810.03436", "submitter": "Christian Reul", "authors": "Christian Reul, Uwe Springmann, Christoph Wick, and Frank Puppe", "title": "State of the Art Optical Character Recognition of 19th Century Fraktur\n  Scripts using Open Source Engines", "comments": "Submitted to DHd 2019 (https://dhd2019.org/) which demands a...\n  creative... submission format. Consequently, some captions might look weird\n  and some links aren't clickable. Extended version with more technical details\n  and some fixes to follow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we evaluate Optical Character Recognition (OCR) of 19th century\nFraktur scripts without book-specific training using mixed models, i.e. models\ntrained to recognize a variety of fonts and typesets from previously unseen\nsources. We describe the training process leading to strong mixed OCR models\nand compare them to freely available models of the popular open source engines\nOCRopus and Tesseract as well as the commercial state of the art system ABBYY.\nFor evaluation, we use a varied collection of unseen data from books, journals,\nand a dictionary from the 19th century. The experiments show that training\nmixed models with real data is superior to training with synthetic data and\nthat the novel OCR engine Calamari outperforms the other engines considerably,\non average reducing ABBYYs character error rate (CER) by over 70%, resulting in\nan average CER below 1%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 13:32:06 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Reul", "Christian", ""], ["Springmann", "Uwe", ""], ["Wick", "Christoph", ""], ["Puppe", "Frank", ""]]}, {"id": "1810.03505", "submitter": "Luke Darlow", "authors": "Luke N. Darlow, Elliot J. Crowley, Antreas Antoniou, Amos J. Storkey", "title": "CINIC-10 is not ImageNet or CIFAR-10", "comments": "Dataset compilation, 9 pages, 11 figures, technical report", "journal-ref": null, "doi": null, "report-no": "EDI-INF-ANC-1802", "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this brief technical report we introduce the CINIC-10 dataset as a plug-in\nextended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with\nimages selected and downsampled from the ImageNet database. We present the\napproach to compiling the dataset, illustrate the example images for different\nclasses, give pixel distributions for each part of the repository, and give\nsome standard benchmarks for well known models. Details for download, usage,\nand compilation can be found in the associated github repository.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 21:20:09 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Darlow", "Luke N.", ""], ["Crowley", "Elliot J.", ""], ["Antoniou", "Antreas", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1810.03522", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb,\n  Erik Goodman and Wolfgang Banzhaf", "title": "NSGA-Net: Neural Architecture Search using Multi-Objective Genetic\n  Algorithm", "comments": "GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces NSGA-Net -- an evolutionary approach for neural\narchitecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a\nprocedure considering multiple and conflicting objectives, (2) an efficient\nprocedure balancing exploration and exploitation of the space of potential\nneural network architectures, and (3) a procedure finding a diverse set of\ntrade-off network architectures achieved in a single run. NSGA-Net is a\npopulation-based search algorithm that explores a space of potential neural\nnetwork architectures in three steps, namely, a population initialization step\nthat is based on prior-knowledge from hand-crafted architectures, an\nexploration step comprising crossover and mutation of architectures, and\nfinally an exploitation step that utilizes the hidden useful knowledge stored\nin the entire history of evaluated neural architectures in the form of a\nBayesian Network. Experimental results suggest that combining the dual\nobjectives of minimizing an error metric and computational complexity, as\nmeasured by FLOPs, allows NSGA-Net to find competitive neural architectures.\nMoreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with\nother state-of-the-art NAS methods while using orders of magnitude less\ncomputational resources. These results are encouraging and shows the promise to\nfurther use of EC methods in various deep-learning paradigms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 15:14:33 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 23:07:16 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lu", "Zhichao", ""], ["Whalen", "Ian", ""], ["Boddeti", "Vishnu", ""], ["Dhebar", "Yashesh", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""], ["Banzhaf", "Wolfgang", ""]]}, {"id": "1810.03523", "submitter": "Xian Wei", "authors": "Xian Wei, Hao Shen, Martin Kleinsteuber", "title": "Trace Quotient with Sparsity Priors for Learning Low Dimensional Image\n  Representations", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of learning appropriate low dimensional image\nrepresentations. We propose a generic algorithmic framework, which leverages\ntwo classic representation learning paradigms, i.e., sparse representation and\nthe trace quotient criterion. The former is a well-known powerful tool to\nidentify underlying self-explanatory factors of data, while the latter is known\nfor disentangling underlying low dimensional discriminative factors in data.\nOur developed solutions disentangle sparse representations of images by\nemploying the trace quotient criterion. We construct a unified cost function,\ncoined as the SPARse LOW dimensional representation (SparLow) function, for\njointly learning both a sparsifying dictionary and a dimensionality reduction\ntransformation. The SparLow function is widely applicable for developing\nvarious algorithms in three classic machine learning scenarios, namely,\nunsupervised, supervised, and semi-supervised learning. In order to develop\nefficient joint learning algorithms for maximizing the SparLow function, we\ndeploy a framework of sparse coding with appropriate convex priors to ensure\nthe sparse representations to be locally differentiable. Moreover, we develop\nan efficient geometric conjugate gradient algorithm to maximize the SparLow\nfunction on its underlying Riemannian manifold. Performance of the proposed\nSparLow algorithmic framework is investigated on several image processing\ntasks, such as 3D data visualization, face/digit recognition, and object/scene\ncategorization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 15:19:13 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Wei", "Xian", ""], ["Shen", "Hao", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1810.03570", "submitter": "Clint Sebastian", "authors": "Clint Sebastian, Bas Boom, Thijs van Lankveld, Egor Bondarev, Peter\n  H.N. De With", "title": "Bootstrapped CNNs for Building Segmentation on RGB-D Aerial Imagery", "comments": "Published at ISPRS Annals of the Photogrammetry, Remote Sensing and\n  Spatial Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection of buildings and other objects from aerial images has various\napplications in urban planning and map making. Automated building detection\nfrom aerial imagery is a challenging task, as it is prone to varying lighting\nconditions, shadows and occlusions. Convolutional Neural Networks (CNNs) are\nrobust against some of these variations, although they fail to distinguish easy\nand difficult examples. We train a detection algorithm from RGB-D images to\nobtain a segmented mask by using the CNN architecture DenseNet.First, we\nimprove the performance of the model by applying a statistical re-sampling\ntechnique called Bootstrapping and demonstrate that more informative examples\nare retained. Second, the proposed method outperforms the non-bootstrapped\nversion by utilizing only one-sixth of the original training data and it\nobtains a precision-recall break-even of 95.10% on our aerial imagery dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 16:41:36 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Sebastian", "Clint", ""], ["Boom", "Bas", ""], ["van Lankveld", "Thijs", ""], ["Bondarev", "Egor", ""], ["De With", "Peter H. N.", ""]]}, {"id": "1810.03599", "submitter": "Xue Bin Peng", "authors": "Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, Sergey\n  Levine", "title": "SFV: Reinforcement Learning of Physical Skills from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven character animation based on motion capture can produce highly\nnaturalistic behaviors and, when combined with physics simulation, can provide\nfor natural procedural responses to physical perturbations, environmental\nchanges, and morphological discrepancies. Motion capture remains the most\npopular source of motion data, but collecting mocap data typically requires\nheavily instrumented environments and actors. In this paper, we propose a\nmethod that enables physically simulated characters to learn skills from videos\n(SFV). Our approach, based on deep pose estimation and deep reinforcement\nlearning, allows data-driven animation to leverage the abundance of publicly\navailable video clips from the web, such as those from YouTube. This has the\npotential to enable fast and easy design of character controllers simply by\nquerying for video recordings of the desired behavior. The resulting\ncontrollers are robust to perturbations, can be adapted to new settings, can\nperform basic object interactions, and can be retargeted to new morphologies\nvia reinforcement learning. We further demonstrate that our method can predict\npotential human motions from still images, by forward simulation of learned\ncontrollers initialized from the observed pose. Our framework is able to learn\na broad range of dynamic skills, including locomotion, acrobatics, and martial\narts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 17:55:39 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 17:15:34 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Peng", "Xue Bin", ""], ["Kanazawa", "Angjoo", ""], ["Malik", "Jitendra", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1810.03649", "submitter": "Sainandan Ramakrishnan", "authors": "Sainandan Ramakrishnan, Aishwarya Agrawal, Stefan Lee", "title": "Overcoming Language Priors in Visual Question Answering with Adversarial\n  Regularization", "comments": "NIPS 2018. 11 pages ( with references ), 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Visual Question Answering (VQA) models have been shown to rely heavily\non superficial correlations between question and answer words learned during\ntraining such as overwhelmingly reporting the type of room as kitchen or the\nsport being played as tennis, irrespective of the image. Most alarmingly, this\nshortcoming is often not well reflected during evaluation because the same\nstrong priors exist in test distributions; however, a VQA system that fails to\nground questions in image content would likely perform poorly in real-world\nsettings. In this work, we present a novel regularization scheme for VQA that\nreduces this effect. We introduce a question-only model that takes as input the\nquestion encoding from the VQA model and must leverage language biases in order\nto succeed. We then pose training as an adversarial game between the VQA model\nand this question-only adversary -- discouraging the VQA model from capturing\nlanguage biases in its question encoding. Further,we leverage this\nquestion-only model to estimate the increase in model confidence after\nconsidering the image, which we maximize explicitly to encourage visual\ngrounding. Our approach is a model agnostic training procedure and simple to\nimplement. We show empirically that it can improve performance significantly on\na bias-sensitive split of the VQA dataset for multiple base models -- achieving\nstate-of-the-art on this task. Further, on standard VQA tasks, our approach\nshows significantly less drop in accuracy compared to existing bias-reducing\nVQA models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 18:29:05 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 20:51:44 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Ramakrishnan", "Sainandan", ""], ["Agrawal", "Aishwarya", ""], ["Lee", "Stefan", ""]]}, {"id": "1810.03654", "submitter": "Yang Wang", "authors": "Yang Wang, Zhenheng Yang, Peng Wang, Yi Yang, Chenxu Luo and Wei Xu", "title": "Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo\n  Videos", "comments": "Submitted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning depth and optical flow via deep neural networks by watching videos\nhas made significant progress recently. In this paper, we jointly solve the two\ntasks by exploiting the underlying geometric rules within stereo videos.\nSpecifically, given two consecutive stereo image pairs from a video, we first\nestimate depth, camera ego-motion and optical flow from three neural networks.\nThen the whole scene is decomposed into moving foreground and static background\nby compar- ing the estimated optical flow and rigid flow derived from the depth\nand ego-motion. We propose a novel consistency loss to let the optical flow\nlearn from the more accurate rigid flow in static regions. We also design a\nrigid alignment module which helps refine ego-motion estimation by using the\nestimated depth and optical flow. Experiments on the KITTI dataset show that\nour results significantly outperform other state-of- the-art algorithms. Source\ncodes can be found at https: //github.com/baidu-research/UnDepthflow\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 18:49:46 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Wang", "Yang", ""], ["Yang", "Zhenheng", ""], ["Wang", "Peng", ""], ["Yang", "Yi", ""], ["Luo", "Chenxu", ""], ["Xu", "Wei", ""]]}, {"id": "1810.03707", "submitter": "Mahdi Rad", "authors": "Mahdi Rad and Markus Oberweger and Vincent Lepetit", "title": "Domain Transfer for 3D Pose Estimation from Color Images without Manual\n  Annotations", "comments": "ACCV 2018 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel learning method for 3D pose estimation from color\nimages. While acquiring annotations for color images is a difficult task, our\napproach circumvents this problem by learning a mapping from paired color and\ndepth images captured with an RGB-D camera. We jointly learn the pose from\nsynthetic depth images that are easy to generate, and learn to align these\nsynthetic depth images with the real depth images. We show our approach for the\ntask of 3D hand pose estimation and 3D object pose estimation, both from color\nimages only. Our method achieves performances comparable to state-of-the-art\nmethods on popular benchmark datasets, without requiring any annotations for\nthe color images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 21:24:39 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 10:22:05 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Rad", "Mahdi", ""], ["Oberweger", "Markus", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1810.03716", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Saliency Prediction in the Deep Learning Era: Successes, Limitations,\n  and Future Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency models have enjoyed a big leap in performance in recent\nyears, thanks to advances in deep learning and large scale annotated data.\nDespite enormous effort and huge breakthroughs, however, models still fall\nshort in reaching human-level accuracy. In this work, I explore the landscape\nof the field emphasizing on new deep saliency models, benchmarks, and datasets.\nA large number of image and video saliency models are reviewed and compared\nover two image benchmarks and two large scale video datasets. Further, I\nidentify factors that contribute to the gap between models and humans and\ndiscuss remaining issues that need to be addressed to build the next generation\nof more powerful saliency models. Some specific questions that are addressed\ninclude: in what ways current models fail, how to remedy them, what can be\nlearned from cognitive studies of attention, how explicit saliency judgments\nrelate to fixations, how to conduct fair model comparison, and what are the\nemerging applications of saliency models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 21:50:27 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 18:35:17 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 23:29:41 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "1810.03728", "submitter": "Emilien Dupont", "authors": "Emilien Dupont, Suhas Suresha", "title": "Probabilistic Semantic Inpainting with Pixel Constrained CNNs", "comments": "AISTATS camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic inpainting is the task of inferring missing pixels in an image given\nsurrounding pixels and high level image semantics. Most semantic inpainting\nalgorithms are deterministic: given an image with missing regions, a single\ninpainted image is generated. However, there are often several plausible\ninpaintings for a given missing region. In this paper, we propose a method to\nperform probabilistic semantic inpainting by building a model, based on\nPixelCNNs, that learns a distribution of images conditioned on a subset of\nvisible pixels. Experiments on the MNIST and CelebA datasets show that our\nmethod produces diverse and realistic inpaintings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 22:19:08 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 22:38:27 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Dupont", "Emilien", ""], ["Suresha", "Suhas", ""]]}, {"id": "1810.03737", "submitter": "Siddhant Ranade", "authors": "Siddhant Ranade, Srikumar Ramalingam", "title": "Novel Single View Constraints for Manhattan 3D Line Reconstruction", "comments": "Accepted at 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel and exact method to reconstruct line-based 3D\nstructure from a single image using Manhattan world assumption. This problem is\na distinctly unsolved problem because there can be multiple 3D reconstructions\nfrom a single image. Thus, we are often forced to look for priors like\nManhattan world assumption and common scene structures. In addition to the\nstandard orthogonality, perspective projection, and parallelism constraints, we\ninvestigate a few novel constraints based on the physical realizability of the\n3D scene structure. We treat the line segments in the image to be part of a\ngraph similar to straws and connectors game, where the goal is to back-project\nthe line segments in 3D space and while ensuring that some of these 3D line\nsegments connect with each other (i.e., truly intersect in 3D space) to form\nthe 3D structure. We consider three sets of novel constraints while solving the\nreconstruction: (1) constraints on a series of Manhattan line intersections\nthat form cycles, but are not all physically realizable, (2) constraints on\ntrue and false intersections in the case of nearby lines lying on the same\nManhattan plane, and (3) constraints from the intersections on boundary and\nnon-boundary line segments. The reconstruction is achieved using mixed integer\nlinear programming (MILP), and we show compelling results on real images. Along\nwith this paper, we will release a challenging Single View Line Reconstruction\ndataset with ground truth 3D line models for research purposes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 22:51:50 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Ranade", "Siddhant", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1810.03744", "submitter": "Marcelo Prates", "authors": "Felipe Zilio, Marcelo Prates, Luis Lamb", "title": "Neural Networks Models for Analyzing Magic: the Gathering Cards", "comments": "10 pages, 1 figure, 9 tables. Accepted at ICONIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, games of all kinds have often been the subject of study in\nscientific works of Computer Science, including the field of machine learning.\nBy using machine learning techniques and applying them to a game with defined\nrules or a structured dataset, it's possible to learn and improve on the\nalready existing techniques and methods to tackle new challenges and solve\nproblems that are out of the ordinary. The already existing work on card games\ntends to focus on gameplay and card mechanics. This work aims to apply neural\nnetworks models, including Convolutional Neural Networks and Recurrent Neural\nNetworks, in order to analyze Magic: the Gathering cards, both in terms of card\ntext and illustrations; the card images and texts are used to train the\nnetworks in order to be able to classify them into multiple categories. The\nultimate goal was to develop a methodology that could generate card text\nmatching it to an input image, which was attained by relating the prediction\nvalues of the images and generated text across the different categories.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 23:25:18 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Zilio", "Felipe", ""], ["Prates", "Marcelo", ""], ["Lamb", "Luis", ""]]}, {"id": "1810.03745", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Paul Peppard, Emmanuel\n  Mignot, Helge Bjarup Dissing Sorensen", "title": "Deep residual networks for automatic sleep stage classification of raw\n  polysomnographic waveforms", "comments": null, "journal-ref": "2018 40th Annual International Conference of the IEEE Engineering\n  in Medicine and Biology Society (EMBC), Honolulu, HI, 2018, pp. 1-4", "doi": "10.1109/EMBC.2018.8513080", "report-no": null, "categories": "cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an automatic sleep stage classification algorithm based on\ndeep residual neural networks and raw polysomnogram signals. Briefly, the raw\ndata is passed through 50 convolutional layers before subsequent classification\ninto one of five sleep stages. Three model configurations were trained on 1850\npolysomnogram recordings and subsequently tested on 230 independent recordings.\nOur best performing model yielded an accuracy of 84.1% and a Cohen's kappa of\n0.746, improving on previous reported results by other groups also using only\nraw polysomnogram data. Most errors were made on non-REM stage 1 and 3\ndecisions, errors likely resulting from the definition of these stages. Further\ntesting on independent cohorts is needed to verify performance for clinical\nuse.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 23:28:33 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Peppard", "Paul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge Bjarup Dissing", ""]]}, {"id": "1810.03756", "submitter": "Kuan-Hui Lee", "authors": "Kuan-Hui Lee, German Ros, Jie Li, Adrien Gaidon", "title": "SPIGAN: Privileged Adversarial Learning from Simulation", "comments": "Accepted by ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning for Computer Vision depends mainly on the source of\nsupervision.Photo-realistic simulators can generate large-scale automatically\nlabeled syntheticdata, but introduce a domain gap negatively impacting\nperformance. We propose anew unsupervised domain adaptation algorithm, called\nSPIGAN, relying on Sim-ulator Privileged Information (PI) and Generative\nAdversarial Networks (GAN).We use internal data from the simulator as PI during\nthe training of a target tasknetwork. We experimentally evaluate our approach\non semantic segmentation. Wetrain the networks on real-world Cityscapes and\nVistas datasets, using only unla-beled real-world images and synthetic labeled\ndata with z-buffer (depth) PI fromthe SYNTHIA dataset. Our method improves over\nno adaptation and state-of-the-art unsupervised domain adaptation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 00:17:24 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 00:17:45 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 06:33:11 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Lee", "Kuan-Hui", ""], ["Ros", "German", ""], ["Li", "Jie", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1810.03758", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Rigas Kouskouridas, Tae-Kyun Kim, Federico Tombari,\n  Kostas Bekris, Bertram Drost, Thibault Groueix, Krzysztof Walas, Vincent\n  Lepetit, Ales Leonardis, Carsten Steger, Frank Michel, Caner Sahin, Carsten\n  Rother, Jiri Matas", "title": "A Summary of the 4th International Workshop on Recovering 6D Object Pose", "comments": "In: Computer Vision - ECCV 2018 Workshops - Munich, Germany,\n  September 8-9 and 14, 2018, Proceedings", "journal-ref": null, "doi": "10.1007/978-3-030-11009-3_36", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document summarizes the 4th International Workshop on Recovering 6D\nObject Pose which was organized in conjunction with ECCV 2018 in Munich. The\nworkshop featured four invited talks, oral and poster presentations of accepted\nworkshop papers, and an introduction of the BOP benchmark for 6D object pose\nestimation. The workshop was attended by 100+ people working on relevant topics\nin both academia and industry who shared up-to-date advances and discussed open\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 00:31:59 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hodan", "Tomas", ""], ["Kouskouridas", "Rigas", ""], ["Kim", "Tae-Kyun", ""], ["Tombari", "Federico", ""], ["Bekris", "Kostas", ""], ["Drost", "Bertram", ""], ["Groueix", "Thibault", ""], ["Walas", "Krzysztof", ""], ["Lepetit", "Vincent", ""], ["Leonardis", "Ales", ""], ["Steger", "Carsten", ""], ["Michel", "Frank", ""], ["Sahin", "Caner", ""], ["Rother", "Carsten", ""], ["Matas", "Jiri", ""]]}, {"id": "1810.03767", "submitter": "Shuai Yang", "authors": "Shuai Yang, Jiaying Liu, Wenhan Yang, Zongming Guo", "title": "Context-Aware Text-Based Binary Image Stylization and Synthesis", "comments": "Accepted by IEEE Trans. on Image Processing. Project page:\n  http://www.icst.pku.edu.cn/struct/Projects/UTS.html", "journal-ref": null, "doi": "10.1109/TIP.2018.2873064", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new framework for the stylization of text-based\nbinary images. First, our method stylizes the stroke-based geometric shape like\ntext, symbols and icons in the target binary image based on an input style\nimage. Second, the composition of the stylized geometric shape and a background\nimage is explored. To accomplish the task, we propose legibility-preserving\nstructure and texture transfer algorithms, which progressively narrow the\nvisual differences between the binary image and the style image. The\nstylization is then followed by a context-aware layout design algorithm, where\ncues for both seamlessness and aesthetics are employed to determine the optimal\nlayout of the shape in the background. Given the layout, the binary image is\nseamlessly embedded into the background by texture synthesis under a\ncontext-aware boundary constraint. According to the contents of binary images,\nour method can be applied to many fields. We show that the proposed method is\ncapable of addressing the unsupervised text stylization problem and is superior\nto state-of-the-art style transfer methods in automatic artistic typography\ncreation. Besides, extensive experiments on various tasks, such as\nvisual-textual presentation synthesis, icon/symbol rendering and\nstructure-guided image inpainting, demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 01:35:46 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Yang", "Shuai", ""], ["Liu", "Jiaying", ""], ["Yang", "Wenhan", ""], ["Guo", "Zongming", ""]]}, {"id": "1810.03774", "submitter": "Shafeeq Elanattil Mr", "authors": "Shafeeq Elanattil, Peyman Moghadam, Simon Denman, Sridha Sridharan,\n  Clinton Fookes", "title": "Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction", "comments": "Accepted in DICTA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method which can track and 3D reconstruct the non-rigid\nsurface motion of human performance using a moving RGB-D camera. 3D\nreconstruction of marker-less human performance is a challenging problem due to\nthe large range of articulated motions and considerable non-rigid deformations.\nCurrent approaches use local optimization for tracking. These methods need many\niterations to converge and may get stuck in local minima during sudden\narticulated movements. We propose a puppet model-based tracking approach using\nskeleton prior, which provides a better initialization for tracking articulated\nmovements. The proposed approach uses an aligned puppet model to estimate\ncorrect correspondences for human performance capture. We also contribute a\nsynthetic dataset which provides ground truth locations for frame-by-frame\ngeometry and skeleton joints of human subjects. Experimental results show that\nour approach is more robust when faced with sudden articulated motions, and\nprovides better 3D reconstruction compared to the existing state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 02:11:03 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Elanattil", "Shafeeq", ""], ["Moghadam", "Peyman", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1810.03783", "submitter": "Tao Zhuo", "authors": "Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong, Mohan Kankanhalli", "title": "Unsupervised Online Video Object Segmentation with Motion Property\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised video object segmentation aims to automatically segment moving\nobjects over an unconstrained video without any user annotation. So far, only\nfew unsupervised online methods have been reported in literature and their\nperformance is still far from satisfactory, because the complementary\ninformation from future frames cannot be processed under online setting. To\nsolve this challenging problem, in this paper, we propose a novel Unsupervised\nOnline Video Object Segmentation (UOVOS) framework by construing the motion\nproperty to mean moving in concurrence with a generic object for segmented\nregions. By incorporating salient motion detection and object proposal, a\npixel-wise fusion strategy is developed to effectively remove detection noise\nsuch as dynamic background and stationary objects. Furthermore, by leveraging\nthe obtained segmentation from immediately preceding frames, a forward\npropagation algorithm is employed to deal with unreliable motion detection and\nobject proposals. Experimental results on several benchmark datasets\ndemonstrate the efficacy of the proposed method. Compared to the\nstate-of-the-art unsupervised online segmentation algorithms, the proposed\nmethod achieves an absolute gain of 6.2%. Moreover, our method achieves better\nperformance than the best unsupervised offline algorithm on the DAVIS-2016\nbenchmark dataset. Our code is available on the project website:\nhttps://github.com/visiontao/uovos.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 02:48:18 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 03:12:29 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhuo", "Tao", ""], ["Cheng", "Zhiyong", ""], ["Zhang", "Peng", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1810.03790", "submitter": "Ruiqi Cheng", "authors": "Ruiqi Cheng, Kaiwei Wang, Longqing Lin and Kailun Yang", "title": "Visual Localization of Key Positions for Visually Impaired People", "comments": "This paper has been accepted by International Conference on Pattern\n  Recognition (ICPR) 2018", "journal-ref": null, "doi": "10.1109/ICPR.2018.8545141", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the off-the-shelf navigational assistance devices, the localization\nprecision is limited to the signal error of global navigation satellite system\n(GNSS). During travelling outdoors, the inaccurately localization perplexes\nvisually impaired people, especially at key positions, such as gates, bus\nstations or intersections. The visual localization is a feasible approach to\nimproving the positioning precision of assistive devices. Using multiple image\ndescriptors, the paper proposes a robust and efficient visual localization\nalgorithm, which takes advantage of priori GNSS signals and multi-modal images\nto achieve the accurate localization of key positions. In the experiments, we\nimplement the approach on the wearable system and test the performance of\nvisual localization under practical scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 03:12:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Cheng", "Ruiqi", ""], ["Wang", "Kaiwei", ""], ["Lin", "Longqing", ""], ["Yang", "Kailun", ""]]}, {"id": "1810.03821", "submitter": "Wei Li", "authors": "Wei Li, Zehuan Yuan, Xiangzhong Fang and Changhu Wang", "title": "Knowing Where to Look? Analysis on Attention of Visual Question\n  Answering System", "comments": "ECCV SiVL Workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have been widely used in Visual Question Answering (VQA)\nsolutions due to their capacity to model deep cross-domain interactions.\nAnalyzing attention maps offers us a perspective to find out limitations of\ncurrent VQA systems and an opportunity to further improve them. In this paper,\nwe select two state-of-the-art VQA approaches with attention mechanisms to\nstudy their robustness and disadvantages by visualizing and analyzing their\nestimated attention maps. We find that both methods are sensitive to features,\nand simultaneously, they perform badly for counting and multi-object related\nquestions. We believe that the findings and analytical method will help\nresearchers identify crucial challenges on the way to improve their own VQA\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 05:51:08 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Li", "Wei", ""], ["Yuan", "Zehuan", ""], ["Fang", "Xiangzhong", ""], ["Wang", "Changhu", ""]]}, {"id": "1810.03851", "submitter": "Yibing Song", "authors": "Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang", "title": "Deep Attentive Tracking via Reciprocative Learning", "comments": "In NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention, derived from cognitive neuroscience, facilitates human\nperception on the most pertinent subset of the sensory data. Recently,\nsignificant efforts have been made to exploit attention schemes to advance\ncomputer vision systems. For visual tracking, it is often challenging to track\ntarget objects undergoing large appearance changes. Attention maps facilitate\nvisual tracking by selectively paying attention to temporal robust features.\nExisting tracking-by-detection approaches mainly use additional attention\nmodules to generate feature weights as the classifiers are not equipped with\nsuch mechanisms. In this paper, we propose a reciprocative learning algorithm\nto exploit visual attention for training deep classifiers. The proposed\nalgorithm consists of feed-forward and backward operations to generate\nattention maps, which serve as regularization terms coupled with the original\nclassification loss function for training. The deep classifier learns to attend\nto the regions of target objects robust to appearance changes. Extensive\nexperiments on large-scale benchmark datasets show that the proposed attentive\ntracking method performs favorably against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 08:25:49 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 06:46:50 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Pu", "Shi", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Zhang", "Honggang", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1810.03867", "submitter": "J\\\"org Wagner", "authors": "J\\\"org Wagner, Volker Fischer, Michael Herman, Sven Behnke", "title": "Functionally Modular and Interpretable Temporal Filtering for Robust\n  Segmentation", "comments": "In Proceedings of 29th British Machine Vision Conference (BMVC),\n  Newcastle upon Tyne, UK, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of autonomous systems heavily relies on their ability to\ngenerate a robust representation of the environment. Deep neural networks have\ngreatly improved vision-based perception systems but still fail in challenging\nsituations, e.g. sensor outages or heavy weather. These failures are often\nintroduced by data-inherent perturbations, which significantly reduce the\ninformation provided to the perception system. We propose a functionally\nmodularized temporal filter, which stabilizes an abstract feature\nrepresentation of a single-frame segmentation model using information of\nprevious time steps. Our filter module splits the filter task into multiple\nless complex and more interpretable subtasks. The basic structure of the filter\nis inspired by a Bayes estimator consisting of a prediction and an update step.\nTo make the prediction more transparent, we implement it using a geometric\nprojection and estimate its parameters. This additionally enables the\ndecomposition of the filter task into static representation filtering and\nlow-dimensional motion filtering. Our model can cope with missing frames and is\ntrainable in an end-to-end fashion. Using photorealistic, synthetic video data,\nwe show the ability of the proposed architecture to overcome data-inherent\nperturbations. The experiments especially highlight advantages introduced by an\ninterpretable and explicit filter module.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 09:13:36 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 16:42:20 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wagner", "J\u00f6rg", ""], ["Fischer", "Volker", ""], ["Herman", "Michael", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.03871", "submitter": "Mina Rezaei", "authors": "Mina Rezaei, Haojin Yang, Christoph Meinel", "title": "Conditional Generative Refinement Adversarial Networks for Unbalanced\n  Medical Image Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generative adversarial architecture to mitigate imbalance\ndata problem in medical image semantic segmentation where the majority of\npixels belongs to a healthy region and few belong to lesion or non-health\nregion. A model trained with imbalanced data tends to bias toward healthy data\nwhich is not desired in clinical applications and predicted outputs by these\nnetworks have high precision and low sensitivity. We propose a new conditional\ngenerative refinement network with three components: a generative, a\ndiscriminative, and a refinement network to mitigate unbalanced data problem\nthrough ensemble learning. The generative network learns to a segment at the\npixel level by getting feedback from the discriminative network according to\nthe true positive and true negative maps. On the other hand, the refinement\nnetwork learns to predict the false positive and the false negative masks\nproduced by the generative network that has significant value, especially in\nmedical application. The final semantic segmentation masks are then composed by\nthe output of the three networks. The proposed architecture shows\nstate-of-the-art results on LiTS-2017 for liver lesion segmentation, and two\nmicroscopic cell segmentation datasets MDA231, PhC-HeLa. We have achieved\ncompetitive results on BraTS-2017 for brain tumour segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 09:17:47 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Rezaei", "Mina", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1810.03908", "submitter": "Ravimal Bandara", "authors": "Ravimal Bandara", "title": "Image Segmentation using Unsupervised Watershed Algorithm with an\n  Over-segmentation Reduction Technique", "comments": "Source code related to this technical report is available at\n  www.codeproject.com/Articles/751744/Image-Segmentation-using-Unsupervised-Watershed-Al", "journal-ref": null, "doi": "10.13140/RG.2.2.35055.28323", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning an image into meaningful\nsegments. The meaning of the segments is subjective due to the definition of\nhomogeneity is varied based on the users perspective hence the automation of\nthe segmentation is challenging. Watershed is a popular segmentation technique\nwhich assumes topographic map in an image, with the brightness of each pixel\nrepresenting its height, and finds the lines that run along the tops of ridges.\nThe results from the algorithm typically suffer from over segmentation due to\nthe lack of knowledge of the objects being classified. This paper presents an\napproach to reduce the over segmentation of watershed algorithm by assuming\nthat the different adjacent segments of an object have similar color\ndistribution. The approach demonstrates an improvement over conventional\nwatershed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 11:02:00 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Bandara", "Ravimal", ""]]}, {"id": "1810.03946", "submitter": "Berton Huang", "authors": "Xiaobo Huang", "title": "Convolutional Neural Networks In Convolution", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Currently, increasingly deeper neural networks have been applied to improve\ntheir accuracy. In contrast, We propose a novel wider Convolutional Neural\nNetworks (CNN) architecture, motivated by the Multi-column Deep Neural Networks\nand the Network In Network(NIN), aiming for higher accuracy without input data\ntransmutation. In our architecture, namely \"CNN In Convolution\"(CNNIC), a small\nCNN, instead of the original generalized liner model(GLM) based filters, is\nconvoluted as kernel on the original image, serving as feature extracting layer\nof this networks. And further classifications are then carried out by a global\naverage pooling layer and a softmax layer. Dropout and orthonormal\ninitialization are applied to overcome training difficulties including slow\nconvergence and over-fitting. Persuasive classification performance is\ndemonstrated on MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 12:59:12 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Huang", "Xiaobo", ""]]}, {"id": "1810.03956", "submitter": "Guillaume Caron", "authors": "Guillaume Caron, Mounya Belghiti, Anthony Dessaux", "title": "3D model silhouette-based tracking in depth images for puppet suit\n  dynamic video-mapping", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-mapping is the process of coherent video-projection of images,\nanimations or movies on static objects or buildings for shows. This paper\nfocuses on the dynamic video-mapping of the suit of a puppet being moved by its\npuppeteer on the theater stage. This may allow changing the costume dynamically\nand simulate light interaction and more.\n  Contrary to common video-mapping, the image warping cannot be done once,\noffline, before the show. It must be done in real-time, and considering a\nnon-flat projection surface, so that the video-projected suit always maps\nperfectly the puppet, automatically.\n  Hence, we propose a new visual tracking method of articulated object, for the\npuppet tracking, exploiting the silhouette of a 3D model of it, in the depth\nimages of a Kinect v2. Then, considering the precise calibration between the\nlatter and the video-projector, that we propose, coherent dynamic video-mapping\nis made possible as the presented results show.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 13:28:48 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Caron", "Guillaume", ""], ["Belghiti", "Mounya", ""], ["Dessaux", "Anthony", ""]]}, {"id": "1810.03962", "submitter": "Umar Asif", "authors": "Umar Asif, Jianbin Tang, and Stefan Harrer", "title": "Densely Supervised Grasp Detector (DSGD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Densely Supervised Grasp Detector (DSGD), a deep learning\nframework which combines CNN structures with layer-wise feature fusion and\nproduces grasps and their confidence scores at different levels of the image\nhierarchy (i.e., global-, region-, and pixel-levels). % Specifically, at the\nglobal-level, DSGD uses the entire image information to predict a grasp. At the\nregion-level, DSGD uses a region proposal network to identify salient regions\nin the image and predicts a grasp for each salient region. At the pixel-level,\nDSGD uses a fully convolutional network and predicts a grasp and its confidence\nat every pixel. % During inference, DSGD selects the most confident grasp as\nthe output. This selection from hierarchically generated grasp candidates\novercomes limitations of the individual models. % DSGD outperforms\nstate-of-the-art methods on the Cornell grasp dataset in terms of grasp\naccuracy. % Evaluation on a multi-object dataset and real-world robotic\ngrasping experiments show that DSGD produces highly stable grasps on a set of\nunseen objects in new environments. It achieves 97% grasp detection accuracy\nand 90% robotic grasping success rate with real-time inference speed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 04:35:08 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:09:38 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Asif", "Umar", ""], ["Tang", "Jianbin", ""], ["Harrer", "Stefan", ""]]}, {"id": "1810.03963", "submitter": "Jiawei Mo", "authors": "Jiawei Mo, Junaed Sattar", "title": "DSVO: Direct Stereo Visual Odometry", "comments": "Rewritten to \"Extending Monocular Visual Odometry to Stereo Camera\n  Systems by Scale Optimization\" arXiv:1905.12723", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to stereo visual odometry without stereo\nmatching. It is particularly robust in scenes of repetitive high-frequency\ntextures. Referred to as DSVO (Direct Stereo Visual Odometry), it operates\ndirectly on pixel intensities, without any explicit feature matching, and is\nthus efficient and more accurate than the state-of-the-art\nstereo-matching-based methods. It applies a semi-direct monocular visual\nodometry running on one camera of the stereo pair, tracking the camera pose and\nmapping the environment simultaneously; the other camera is used to optimize\nthe scale of monocular visual odometry. We evaluate DSVO in a number of\nchallenging scenes to evaluate its performance and present comparisons with the\nstate-of-the-art stereo visual odometry algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:56:57 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 14:52:24 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mo", "Jiawei", ""], ["Sattar", "Junaed", ""]]}, {"id": "1810.03964", "submitter": "Alhabib Abbas", "authors": "Mohammad Jubran, Alhabib Abbas, Aaron Chadha and Yiannis Andreopoulos", "title": "Rate-Accuracy Trade-Off In Video Classification With Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced video classification systems decode video frames to derive the\nnecessary texture and motion representations for ingestion and analysis by\nspatio-temporal deep convolutional neural networks (CNNs). However, when\nconsidering visual Internet-of-Things applications, surveillance systems and\nsemantic crawlers of large video repositories, the video capture and the\nCNN-based semantic analysis parts do not tend to be co-located. This\nnecessitates the transport of compressed video over networks and incurs\nsignificant overhead in bandwidth and energy consumption, thereby significantly\nundermining the deployment potential of such systems. In this paper, we\ninvestigate the trade-off between the encoding bitrate and the achievable\naccuracy of CNN-based video classification models that directly ingest\nAVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video\nbitstreams and applying complex optical flow calculations prior to CNN\nprocessing, we only retain motion vector and select texture information at\nsignificantly-reduced bitrates and apply no additional processing prior to CNN\ningestion. Based on three CNN architectures and two action recognition\ndatasets, we achieve 11%-94% saving in bitrate with marginal effect on\nclassification accuracy. A model-based selection between multiple CNNs\nincreases these savings further, to the point where, if up to 7% loss of\naccuracy can be tolerated, video classification can take place with as little\nas 3 kbps for the transport of the required compressed video information to the\nsystem implementing the CNN models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:33:43 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 13:08:19 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Jubran", "Mohammad", ""], ["Abbas", "Alhabib", ""], ["Chadha", "Aaron", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1810.03965", "submitter": "Aniket Bera", "authors": "Aniket Bera and Dinesh Manocha", "title": "Interactive Surveillance Technologies for Dense Crowds", "comments": "Presented at AAAI FSS-18: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an algorithm for realtime anomaly detection in low to medium\ndensity crowd videos using trajectory-level behavior learning. Our formulation\ncombines online tracking algorithms from computer vision, non-linear pedestrian\nmotion models from crowd simulation, and Bayesian learning techniques to\nautomatically compute the trajectory-level pedestrian behaviors for each agent\nin the video. These learned behaviors are used to segment the trajectories and\nmotions of different pedestrians or agents and detect anomalies. We demonstrate\nthe interactive performance on the PETS ARENA dataset as well as indoor and\noutdoor crowd video benchmarks consisting of tens of human agents. We also\ndiscuss the implications of recent public policy and law enforcement issues\nrelating to surveillance and our research.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 20:18:25 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1810.03966", "submitter": "Yang Gao", "authors": "Yang Gao, Swarup Chandra, Zhuoyi Wang, Latifur Khan", "title": "Adaptive Image Stream Classification via Convolutional Neural Network\n  with Intrinsic Similarity Metrics", "comments": "10 pages; KDD'18 Deep Learning Day, August 2018, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing data classification over a stream of continuously occurring\ninstances, a key challenge is to develop an open-world classifier that\nanticipates instances from an unknown class. Studies addressing this problem,\ntypically called novel class detection, have considered classification methods\nthat reactively adapt to such changes along the stream. Importantly, they rely\non the property of cohesion and separation among instances in feature space.\nInstances belonging to the same class are assumed to be closer to each other\n(cohesion) than those belonging to different classes (separation).\nUnfortunately, this assumption may not have large support when dealing with\nhigh dimensional data such as images. In this paper, we address this key\nchallenge by proposing a semisupervised multi-task learning framework called\nCSIM which aims to intrinsically search for a latent space suitable for\ndetecting labels of instances from both known and unknown classes.\nParticularly, we utilize a convolution neural network layer that aids in the\nlearning of a latent feature space suitable for novel class detection. We\nempirically measure the performance of CSIM over multiple realworld image\ndatasets and demonstrate its superiority by comparing its performance with\nexisting semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:27:26 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Gao", "Yang", ""], ["Chandra", "Swarup", ""], ["Wang", "Zhuoyi", ""], ["Khan", "Latifur", ""]]}, {"id": "1810.03967", "submitter": "Mhafuzul Islam", "authors": "Mhafuzul Islam, Mahsrur Chowdhury, Hongda Li, Hongxin Hu", "title": "Vision-based Navigation of Autonomous Vehicle in Roadway Environments\n  with Unexpected Hazards", "comments": "17 pages, 12 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based navigation of autonomous vehicles primarily depends on the Deep\nNeural Network (DNN) based systems in which the controller obtains input from\nsensors/detectors, such as cameras and produces a vehicle control output, such\nas a steering wheel angle to navigate the vehicle safely in a roadway traffic\nenvironment. Typically, these DNN-based systems of the autonomous vehicle are\ntrained through supervised learning; however, recent studies show that a\ntrained DNN-based system can be compromised by perturbation or adversarial\ninputs. Similarly, this perturbation can be introduced into the DNN-based\nsystems of autonomous vehicle by unexpected roadway hazards, such as debris and\nroadblocks. In this study, we first introduce a roadway hazardous environment\n(both intentional and unintentional roadway hazards) that can compromise the\nDNN-based navigational system of an autonomous vehicle, and produces an\nincorrect steering wheel angle, which can cause crashes resulting in fatality\nand injury. Then, we develop a DNN-based autonomous vehicle driving system\nusing object detection and semantic segmentation to mitigate the adverse effect\nof this type of hazardous environment, which helps the autonomous vehicle to\nnavigate safely around such hazards. We find that our developed DNN-based\nautonomous vehicle driving system including hazardous object detection and\nsemantic segmentation improves the navigational ability of an autonomous\nvehicle to avoid a potential hazard by 21% compared to the traditional\nDNN-based autonomous vehicle driving system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 02:08:21 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 15:59:24 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 17:31:59 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Islam", "Mhafuzul", ""], ["Chowdhury", "Mahsrur", ""], ["Li", "Hongda", ""], ["Hu", "Hongxin", ""]]}, {"id": "1810.03968", "submitter": "Steffen Bruns", "authors": "Steffen Bruns, Jelmer M. Wolterink, Robbert W. van Hamersvelt, Majd\n  Zreik, Tim Leiner, Ivana I\\v{s}gum", "title": "Improving Myocardium Segmentation in Cardiac CT Angiography using\n  Spectral Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the left ventricle myocardium in cardiac CT\nangiography (CCTA) is essential for e.g. the assessment of myocardial\nperfusion. Automatic deep learning methods for segmentation in CCTA might\nsuffer from differences in contrast-agent attenuation between training and test\ndata due to non-standardized contrast administration protocols and varying\ncardiac output. We propose augmentation of the training data with virtual\nmono-energetic reconstructions from a spectral CT scanner which show different\nattenuation levels of the contrast agent. We compare this to an augmentation by\nlinear scaling of all intensity values, and combine both types of augmentation.\nWe train a 3D fully convolutional network (FCN) with 10 conventional CCTA\nimages and corresponding virtual mono-energetic reconstructions acquired on a\nspectral CT scanner, and evaluate on 40 CCTA scans acquired on a conventional\nCT scanner. We show that training with data augmentation using virtual\nmono-energetic images improves upon training with only conventional images\n(Dice similarity coefficient (DSC) 0.895 $\\pm$ 0.039 vs. 0.846 $\\pm$ 0.125). In\ncomparison, training with data augmentation using linear scaling improves the\nDSC to 0.890 $\\pm$ 0.039. Moreover, combining the results of both augmentation\nmethods leads to a DSC of 0.901 $\\pm$ 0.036, showing that both augmentations\nlead to different local improvements of the segmentations. Our results indicate\nthat virtual mono-energetic images improve the generalization of an FCN used\nfor myocardium segmentation in CCTA images.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:34:42 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 08:46:09 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Bruns", "Steffen", ""], ["Wolterink", "Jelmer M.", ""], ["van Hamersvelt", "Robbert W.", ""], ["Zreik", "Majd", ""], ["Leiner", "Tim", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1810.03969", "submitter": "Nicolo' Savioli", "authors": "Nicol\\'o Savioli, Miguel Silva Vieira, Pablo Lamata, Giovanni Montana", "title": "A Generative Adversarial Model for Right Ventricle Segmentation", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The clinical management of several cardiovascular conditions, such as\npulmonary hypertension, require the assessment of the right ventricular (RV)\nfunction. This work addresses the fully automatic and robust access to one of\nthe key RV biomarkers, its ejection fraction, from the gold standard imaging\nmodality, MRI. The problem becomes the accurate segmentation of the RV blood\npool from cine MRI sequences. This work proposes a solution based on Fully\nConvolutional Neural Networks (FCNN), where our first contribution is the\noptimal combination of three concepts (the convolution Gated Recurrent Units\n(GRU), the Generative Adversarial Networks (GAN), and the L1 loss function)\nthat achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff\nDistance respectively with respect to the baseline FCNN. This improvement is\nthen doubled by our second contribution, the ROI-GAN, that sets two GANs to\ncooperate working at two fields of view of the image, its full resolution and\nthe region of interest (ROI). Our rationale here is to better guide the FCNN\nlearning by combining global (full resolution) and local Region Of Interest\n(ROI) features. The study is conducted in a large in-house dataset of $\\sim$\n23.000 segmented MRI slices, and its generality is verified in a publicly\navailable dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 09:52:10 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Savioli", "Nicol\u00f3", ""], ["Vieira", "Miguel Silva", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1810.03970", "submitter": "Alexander Prange", "authors": "Alexander Prange, Michael Barz, Daniel Sonntag", "title": "A categorisation and implementation of digital pen features for\n  behaviour characterisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a categorisation and implementation of digital ink\nfeatures for behaviour characterisation. Based on four feature sets taken from\nliterature, we provide a categorisation in different classes of syntactic and\nsemantic features. We implemented a publicly available framework to calculate\nthese features and show its deployment in the use case of analysing cognitive\nassessments performed using a digital pen.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:24:20 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Prange", "Alexander", ""], ["Barz", "Michael", ""], ["Sonntag", "Daniel", ""]]}, {"id": "1810.03973", "submitter": "Zeqing Fu", "authors": "Zeqing Fu, Wei Hu, Zongming Guo", "title": "Local Frequency Interpretation and Non-Local Self-Similarity on Graph\n  for Point Cloud Inpainting", "comments": "11 pages, 11 figures, submitted to IEEE Transactions on Image\n  Processing at 2018.09.04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D scanning devices and depth sensors mature, point clouds have attracted\nincreasing attention as a format for 3D object representation, with\napplications in various fields such as tele-presence, navigation and heritage\nreconstruction. However, point clouds usually exhibit holes of missing data,\nmainly due to the limitation of acquisition techniques and complicated\nstructure. Further, point clouds are defined on irregular non-Euclidean\ndomains, which is challenging to address especially with conventional signal\nprocessing tools. Hence, leveraging on recent advances in graph signal\nprocessing, we propose an efficient point cloud inpainting method, exploiting\nboth the local smoothness and the non-local self-similarity in point clouds.\nSpecifically, we first propose a frequency interpretation in graph nodal\ndomain, based on which we introduce the local graph-signal smoothness prior in\norder to describe the local smoothness of point clouds. Secondly, we explore\nthe characteristics of non-local self-similarity, by globally searching for the\nmost similar area to the missing region. The similarity metric between two\nareas is defined based on the direct component and the anisotropic graph total\nvariation of normals in each area. Finally, we formulate the hole-filling step\nas an optimization problem based on the selected most similar area and\nregularized by the graph-signal smoothness prior. Besides, we propose\nvoxelization and automatic hole detection methods for the point cloud prior to\ninpainting. Experimental results show that the proposed approach outperforms\nfour competing methods significantly, both in objective and subjective quality.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 13:08:49 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Fu", "Zeqing", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""]]}, {"id": "1810.03977", "submitter": "Dinesh Kumar Amara", "authors": "Amara Dinesh Kumar, Vinayakumar R, Soman KP", "title": "DeepImageSpam: Deep Learning based Image Spam Detection", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hackers and spammers are employing innovative and novel techniques to deceive\nnovice and even knowledgeable internet users. Image spam is one of such\ntechnique where the spammer varies and changes some portion of the image such\nthat it is indistinguishable from the original image fooling the users. This\npaper proposes a deep learning based approach for image spam detection using\nthe convolutional neural networks which uses a dataset with 810 natural images\nand 928 spam images for classification achieving an accuracy of 91.7%\noutperforming the existing image processing and machine learning techniques\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 09:35:01 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Kumar", "Amara Dinesh", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "1810.03979", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "Extended Bit-Plane Compression for Convolutional Neural Network\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the tremendous success of convolutional neural networks in image\nclassification, object detection, speech recognition, etc., there is now rising\ndemand for deployment of these compute-intensive ML models on tightly power\nconstrained embedded and mobile systems at low cost as well as for pushing the\nthroughput in data centers. This has triggered a wave of research towards\nspecialized hardware accelerators. Their performance is often constrained by\nI/O bandwidth and the energy consumption is dominated by I/O transfers to\noff-chip memory. We introduce and evaluate a novel, hardware-friendly\ncompression scheme for the feature maps present within convolutional neural\nnetworks. We show that an average compression ratio of 4.4x relative to\nuncompressed data and a gain of 60% over existing method can be achieved for\nResNet-34 with a compression block requiring <300 bit of sequential cells and\nminimal combinational logic.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 21:02:53 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1810.03982", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Paul Hand", "title": "Deep Decoder: Concise Image Representations from Untrained\n  Non-convolutional Networks", "comments": "International Conference on Learning Representations 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks, in particular convolutional neural networks, have\nbecome highly effective tools for compressing images and solving inverse\nproblems including denoising, inpainting, and reconstruction from few and noisy\nmeasurements. This success can be attributed in part to their ability to\nrepresent and generate natural images well. Contrary to classical tools such as\nwavelets, image-generating deep neural networks have a large number of\nparameters---typically a multiple of their output dimension---and need to be\ntrained on large datasets. In this paper, we propose an untrained simple image\nmodel, called the deep decoder, which is a deep neural network that can\ngenerate natural images from very few weight parameters. The deep decoder has a\nsimple architecture with no convolutions and fewer weight parameters than the\noutput dimensionality. This underparameterization enables the deep decoder to\ncompress images into a concise set of network weights, which we show is on par\nwith wavelet-based thresholding. Further, underparameterization provides a\nbarrier to overfitting, allowing the deep decoder to have state-of-the-art\nperformance for denoising. The deep decoder is simple in the sense that each\nlayer has an identical structure that consists of only one upsampling unit,\npixel-wise linear combination of channels, ReLU activation, and channelwise\nnormalization. This simplicity makes the network amenable to theoretical\nanalysis, and it sheds light on the aspects of neural networks that enable them\nto form effective signal representations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:07:07 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 22:13:19 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Heckel", "Reinhard", ""], ["Hand", "Paul", ""]]}, {"id": "1810.03986", "submitter": "Yuhan Shen", "authors": "Yu-Han Shen, Ke-Xin He and Wei-Qiang Zhang", "title": "SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level\n  Attention Mechanism for Home Activity Monitoring", "comments": "6 pages, accepted by ISSPIT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for home activity monitoring. We\ndemonstrate our model on dataset of Detection and Classification of Acoustic\nScenes and Events (DCASE) 2018 Challenge Task 5. This task aims to classify\nmulti-channel audios into one of the provided pre-defined classes. All of these\nclasses are daily activities performed in a home environment. To tackle this\ntask, we propose a gated convolutional neural network with segment-level\nattention mechanism (SAM-GCNN). The proposed framework is a convolutional model\nwith two auxiliary modules: a gated convolutional neural network and a\nsegment-level attention mechanism. Furthermore, we adopted model ensemble to\nenhance the capability of generalization of our model. We evaluated our work on\nthe development dataset of DCASE 2018 Task 5 and achieved competitive\nperformance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%,\ncompared with the convolutional baseline system.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 14:55:32 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 09:42:40 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Shen", "Yu-Han", ""], ["He", "Ke-Xin", ""], ["Zhang", "Wei-Qiang", ""]]}, {"id": "1810.03987", "submitter": "Anupama Goparaju", "authors": "Anupama Goparaju, Ibolya Csecs, Alan Morris, Evgueni Kholmovski,\n  Nassir Marrouche, Ross Whitaker, and Shireen Elhabian", "title": "On the Evaluation and Validation of Off-the-shelf Statistical Shape\n  Modeling Tools: A Clinical Application", "comments": "To Appear: ShapeMI Workshop: Workshop on Shape in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape modeling (SSM) has proven useful in many areas of biology\nand medicine as a new generation of morphometric approaches for the\nquantitative analysis of anatomical shapes. Recently, the increased\navailability of high-resolution in vivo images of anatomy has led to the\ndevelopment and distribution of open-source computational tools to model\nanatomical shapes and their variability within populations with unprecedented\ndetail and statistical power. Nonetheless, there is little work on the\nevaluation and validation of such tools as related to clinical applications\nthat rely on morphometric quantifications for treatment planning. To address\nthis lack of validation, we systematically assess the outcome of widely used\noff-the-shelf SSM tools, namely ShapeWorks, SPHARM-PDM, and Deformetrica, in\nthe context of designing closure devices for left atrium appendage (LAA) in\natrial fibrillation (AF) patients to prevent stroke, where an incomplete LAA\nclosure may be worse than no closure. This study is motivated by the potential\nrole of SSM in the geometric design of closure devices, which could be informed\nby population-level statistics, and patient-specific device selection, which is\ndriven by anatomical measurements that could be automated by relating\npatient-level anatomy to population-level morphometrics. Hence, understanding\nthe consequences of different SSM tools for the final analysis is critical for\nthe careful choice of the tool to be deployed in real clinical scenarios.\nResults demonstrate that estimated measurements from ShapeWorks model are more\nconsistent compared to models from Deformetrica and SPHARM-PDM. Furthermore,\nShapeWorks and Deformetrica shape models capture clinically relevant\npopulation-level variability compared to SPHARM-PDM models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 15:37:40 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Goparaju", "Anupama", ""], ["Csecs", "Ibolya", ""], ["Morris", "Alan", ""], ["Kholmovski", "Evgueni", ""], ["Marrouche", "Nassir", ""], ["Whitaker", "Ross", ""], ["Elhabian", "Shireen", ""]]}, {"id": "1810.03988", "submitter": "Lin Li", "authors": "Chengyao Du, Jingling Yuan, Jiansheng Dong, Lin Li, Mincheng Chen and\n  Tao Li", "title": "GPU based Parallel Optimization for Real Time Panoramic Video Stitching", "comments": "under review for Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoramic video is a sort of video recorded at the same point of view to\nrecord the full scene. With the development of video surveillance and the\nrequirement for 3D converged video surveillance in smart cities, CPU and GPU\nare required to possess strong processing abilities to make panoramic video.\nThe traditional panoramic products depend on post processing, which results in\nhigh power consumption, low stability and unsatisfying performance in real\ntime. In order to solve these problems,we propose a real-time panoramic video\nstitching framework.The framework we propose mainly consists of three\nalgorithms, LORB image feature extraction algorithm, feature point matching\nalgorithm based on LSH and GPU parallel video stitching algorithm based on\nCUDA.The experiment results show that the algorithm mentioned can improve the\nperformance in the stages of feature extraction of images stitching and\nmatching, the running speed of which is 11 times than that of the traditional\nORB algorithm and 639 times than that of the traditional SIFT algorithm. Based\non analyzing the GPU resources occupancy rate of each resolution image\nstitching, we further propose a stream parallel strategy to maximize the\nutilization of GPU resources. Compared with the L-ORB algorithm, the efficiency\nof this strategy is improved by 1.6-2.5 times, and it can make full use of GPU\nresources. The performance of the system accomplished in the paper is 29.2\ntimes than that of the former embedded one, while the power dissipation is\nreduced to 10W.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 04:18:17 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 07:56:26 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Du", "Chengyao", ""], ["Yuan", "Jingling", ""], ["Dong", "Jiansheng", ""], ["Li", "Lin", ""], ["Chen", "Mincheng", ""], ["Li", "Tao", ""]]}, {"id": "1810.03989", "submitter": "Lin Li", "authors": "Zhongwei Xie, Lin Li, Xian Zhong, Luo Zhong", "title": "Image-to-Video Person Re-Identification by Reusing Cross-modal\n  Embeddings", "comments": "under review for Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-video person re-identification identifies a target person by a probe\nimage from quantities of pedestrian videos captured by non-overlapping cameras.\nDespite the great progress achieved,it's still challenging to match in the\nmultimodal scenario,i.e. between image and video. Currently,state-of-the-art\napproaches mainly focus on the task-specific data,neglecting the extra\ninformation on the different but related tasks. In this paper,we propose an\nend-to-end neural network framework for image-to-video person reidentification\nby leveraging cross-modal embeddings learned from extra information.Concretely\nspeaking,cross-modal embeddings from image captioning and video captioning\nmodels are reused to help learned features be projected into a coordinated\nspace,where similarity can be directly computed. Besides,training steps from\nfixed model reuse approach are integrated into our framework,which can\nincorporate beneficial information and eventually make the target networks\nindependent of existing models. Apart from that,our proposed framework resorts\nto CNNs and LSTMs for extracting visual and spatiotemporal features,and\ncombines the strengths of identification and verification model to improve the\ndiscriminative ability of the learned feature. The experimental results\ndemonstrate the effectiveness of our framework on narrowing down the gap\nbetween heterogeneous data and obtaining observable improvement in\nimage-to-video person re-identification.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 04:19:49 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 07:58:48 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Xie", "Zhongwei", ""], ["Li", "Lin", ""], ["Zhong", "Xian", ""], ["Zhong", "Luo", ""]]}, {"id": "1810.03999", "submitter": "Dufan Wu", "authors": "Dufan Wu, Kyungsang Kim, and Quanzheng Li", "title": "Computationally Efficient Deep Neural Network for Computed Tomography\n  Image Reconstruction", "comments": "33 pages, 14 figures, accepted by Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13627", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-neural-network-based image reconstruction has demonstrated promising\nperformance in medical imaging for under-sampled and low-dose scenarios.\nHowever, it requires large amount of memory and extensive time for the\ntraining. It is especially challenging to train the reconstruction networks for\nthree-dimensional computed tomography (CT) because of the high resolution of CT\nimages. The purpose of this work is to reduce the memory and time consumption\nof the training of the reconstruction networks for CT to make it practical for\ncurrent hardware, while maintaining the quality of the reconstructed images.\n  We unrolled the proximal gradient descent algorithm for iterative image\nreconstruction to finite iterations and replaced the terms related to the\npenalty function with trainable convolutional neural networks (CNN). The\nnetwork was trained greedily iteration by iteration in the image-domain on\npatches, which requires reasonable amount of memory and time on mainstream\ngraphics processing unit (GPU). To overcome the local-minimum problem caused by\ngreedy learning, we used deep UNet as the CNN and incorporated separable\nquadratic surrogate with ordered subsets for data fidelity, so that the\nsolution could escape from easy local minimums and achieve better image\nquality.\n  The proposed method achieved comparable image quality with state-of-the-art\nneural network for CT image reconstruction on 2D sparse-view and limited-angle\nproblems on the low-dose CT challenge dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:26:39 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 15:50:20 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 03:25:59 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Wu", "Dufan", ""], ["Kim", "Kyungsang", ""], ["Li", "Quanzheng", ""]]}, {"id": "1810.04002", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu,\n  Thomas Huang, and Humphrey Shi", "title": "Decoupled Classification Refinement: Hard False Positive Suppression for\n  Object Detection", "comments": "under review. arXiv admin note: substantial text overlap with\n  arXiv:1803.06799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze failure cases of state-of-the-art detectors and\nobserve that most hard false positives result from classification instead of\nlocalization and they have a large negative impact on the performance of object\ndetectors. We conjecture there are three factors: (1) Shared feature\nrepresentation is not optimal due to the mismatched goals of feature learning\nfor classification and localization; (2) multi-task learning helps, yet\noptimization of the multi-task loss may result in sub-optimal for individual\ntasks; (3) large receptive field for different scales leads to redundant\ncontext information for small objects. We demonstrate the potential of detector\nclassification power by a simple, effective, and widely-applicable Decoupled\nClassification Refinement (DCR) network. In particular, DCR places a separate\nclassification network in parallel with the localization network (base\ndetector). With ROI Pooling placed on the early stage of the classification\nnetwork, we enforce an adaptive receptive field in DCR. During training, DCR\nsamples hard false positives from the base detector and trains a strong\nclassifier to refine classification results. During testing, DCR refines all\nboxes from the base detector. Experiments show competitive results on PASCAL\nVOC and COCO without any bells and whistles. Our codes are available at:\nhttps://github.com/bowenc0221/Decoupled-Classification-Refinement.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 19:34:20 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 04:04:10 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Cheng", "Bowen", ""], ["Wei", "Yunchao", ""], ["Feris", "Rogerio", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Huang", "Thomas", ""], ["Shi", "Humphrey", ""]]}, {"id": "1810.04008", "submitter": "Dmitrii Lachinov", "authors": "Dmitry Lachinov, Evgeny Vasiliev, Vadim Turlapov", "title": "Glioma Segmentation with Cascaded Unet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MRI analysis takes central position in brain tumor diagnosis and treatment,\nthus it's precise evaluation is crucially important. However, it's 3D nature\nimposes several challenges, so the analysis is often performed on 2D\nprojections that reduces the complexity, but increases bias. On the other hand,\ntime consuming 3D evaluation, like, segmentation, is able to provide precise\nestimation of a number of valuable spatial characteristics, giving us\nunderstanding about the course of the disease.\\newline Recent studies, focusing\non the segmentation task, report superior performance of Deep Learning methods\ncompared to classical computer vision algorithms. But still, it remains a\nchallenging problem. In this paper we present deep cascaded approach for\nautomatic brain tumor segmentation. Similar to recent methods for object\ndetection, our implementation is based on neural networks; we propose\nmodifications to the 3D UNet architecture and augmentation strategy to\nefficiently handle multimodal MRI input, besides this we introduce approach to\nenhance segmentation quality with context obtained from models of the same\ntopology operating on downscaled data. We evaluate presented approach on BraTS\n2018 dataset and discuss results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:07:28 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Lachinov", "Dmitry", ""], ["Vasiliev", "Evgeny", ""], ["Turlapov", "Vadim", ""]]}, {"id": "1810.04012", "submitter": "Risheng Liu", "authors": "Risheng Liu, Long Ma, Yiyang Wang, Lei Zhang", "title": "Learning Converged Propagations with Deep Prior Ensemble for Image\n  Enhancement", "comments": "This paper has been accepted in the IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2875568", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancing visual qualities of images plays very important roles in various\nvision and learning applications. In the past few years, both knowledge-driven\nmaximum a posterior (MAP) with prior modelings and fully data-dependent\nconvolutional neural network (CNN) techniques have been investigated to address\nspecific enhancement tasks. In this paper, by exploiting the advantages of\nthese two types of mechanisms within a complementary propagation perspective,\nwe propose a unified framework, named deep prior ensemble (DPE), for solving\nvarious image enhancement tasks. Specifically, we first establish the basic\npropagation scheme based on the fundamental image modeling cues and then\nintroduce residual CNNs to help predicting the propagation direction at each\nstage. By designing prior projections to perform feedback control, we\ntheoretically prove that even with experience-inspired CNNs, DPE is definitely\nconverged and the output will always satisfy our fundamental task constraints.\nThe main advantage against conventional optimization-based MAP approaches is\nthat our descent directions are learned from collected training data, thus are\nmuch more robust to unwanted local minimums. While, compared with existing CNN\ntype networks, which are often designed in heuristic manners without\ntheoretical guarantees, DPE is able to gain advantages from rich task cues\ninvestigated on the bases of domain knowledges. Therefore, DPE actually\nprovides a generic ensemble methodology to integrate both knowledge and\ndata-based cues for different image enhancement tasks. More importantly, our\ntheoretical investigations verify that the feedforward propagations of DPE are\nproperly controlled toward our desired solution. Experimental results\ndemonstrate that the proposed DPE outperforms state-of-the-arts on a variety of\nimage enhancement tasks in terms of both quantitative measure and visual\nperception quality.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:15:01 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Liu", "Risheng", ""], ["Ma", "Long", ""], ["Wang", "Yiyang", ""], ["Zhang", "Lei", ""]]}, {"id": "1810.04017", "submitter": "Grzegorz Chlebus", "authors": "Hans Meine, Grzegorz Chlebus, Mohsen Ghafoorian, Itaru Endo, Andrea\n  Schenk", "title": "Comparison of U-net-based Convolutional Neural Networks for Liver\n  Segmentation in CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches for liver segmentation in CT have been proposed: Besides\nstatistical shape models, which played a major role in this research area,\nnovel approaches on the basis of convolutional neural networks have been\nintroduced recently. Using a set of 219 liver CT datasets with reference\nsegmentations from liver surgery planning, we evaluate the performance of\nseveral neural network classifiers based on 2D and 3D U-net architectures. An\ninteresting observation is that slice-wise approaches perform surprisingly\nwell, with mean and median Dice coefficients above 0.97, and may be preferable\nover 3D approaches given current hardware and software limitations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:20:57 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Meine", "Hans", ""], ["Chlebus", "Grzegorz", ""], ["Ghafoorian", "Mohsen", ""], ["Endo", "Itaru", ""], ["Schenk", "Andrea", ""]]}, {"id": "1810.04020", "submitter": "Md Zakir Hossain", "authors": "Md. Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, Hamid Laga", "title": "A Comprehensive Survey of Deep Learning for Image Captioning", "comments": "36 Pages, Accepted as a Journal Paper in ACM Computing Surveys\n  (October 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating a description of an image is called image captioning. Image\ncaptioning requires to recognize the important objects, their attributes and\ntheir relationships in an image. It also needs to generate syntactically and\nsemantically correct sentences. Deep learning-based techniques are capable of\nhandling the complexities and challenges of image captioning. In this survey\npaper, we aim to present a comprehensive review of existing deep learning-based\nimage captioning techniques. We discuss the foundation of the techniques to\nanalyze their performances, strengths and limitations. We also discuss the\ndatasets and the evaluation metrics popularly used in deep learning based\nautomatic image captioning.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 16:31:52 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 04:55:06 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Hossain", "Md. Zakir", ""], ["Sohel", "Ferdous", ""], ["Shiratuddin", "Mohd Fairuz", ""], ["Laga", "Hamid", ""]]}, {"id": "1810.04021", "submitter": "Neslisah Torosdagli", "authors": "Neslisah Torosdagli, Denise K. Liberton, Payal Verma, Murat Sincan,\n  Janice S. Lee, and Ulas Bagci", "title": "Deep Geodesic Learning for Segmentation and Anatomical Landmarking", "comments": "14 pages, 12 Figures, IEEE Transactions on Medical Imaging 2018,\n  TMI-2018-0898.R1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep learning framework for anatomy\nsegmentation and automatic landmark- ing. Specifically, we focus on the\nchallenging problem of mandible segmentation from cone-beam computed tomography\n(CBCT) scans and identification of 9 anatomical landmarks of the mandible on\nthe geodesic space. The overall approach employs three inter-related steps. In\nstep 1, we propose a deep neu- ral network architecture with carefully designed\nregularization, and network hyper-parameters to perform image segmentation\nwithout the need for data augmentation and complex post- processing refinement.\nIn step 2, we formulate the landmark localization problem directly on the\ngeodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose\nto use a long short-term memory (LSTM) network to identify closely- spaced\nlandmarks, which is rather difficult to obtain using other standard detection\nnetworks. The proposed fully automated method showed superior efficacy compared\nto the state-of-the- art mandible segmentation and landmarking approaches in\ncraniofacial anomalies and diseased states. We used a very challenging CBCT\ndataset of 50 patients with a high-degree of craniomaxillofacial (CMF)\nvariability that is realistic in clinical practice. Complementary to the\nquantitative analysis, the qualitative visual inspection was conducted for\ndistinct CBCT scans from 250 patients with high anatomical variability. We have\nalso shown feasibility of the proposed work in an independent dataset from\nMICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.\nLastly, we present an in-depth analysis of the proposed deep networks with\nrespect to the choice of hyper-parameters such as pooling and activation\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 17:37:39 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Torosdagli", "Neslisah", ""], ["Liberton", "Denise K.", ""], ["Verma", "Payal", ""], ["Sincan", "Murat", ""], ["Lee", "Janice S.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1810.04028", "submitter": "Hao Zhang", "authors": "Hao Zhang, Jianwei Ma", "title": "Hartley Spectral Pooling for Deep Learning", "comments": "5 pages, 6 figures, letter", "journal-ref": "CSIAM Transactions on Applied Mathematics, 2020, 1(3):518-529", "doi": "10.4208/csiam-am.2020", "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most convolution neural networks (CNNs), downsampling hidden layers is\nadopted for increasing computation efficiency and the receptive field size.\nSuch operation is commonly so-called pooling. Maximation and averaging over\nsliding windows (max/average pooling), and plain downsampling in the form of\nstrided convolution are popular pooling methods. Since the pooling is a lossy\nprocedure, a motivation of our work is to design a new pooling approach for\nless lossy in the dimensionality reduction. Inspired by the Fourier spectral\npooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform\nbased spectral pooling method in CNNs. Compared with FSP, the proposed spectral\npooling avoids the use of complex arithmetic for frequency representation and\nreduces the computation. Spectral pooling preserves more structure features for\nnetwork's discriminability than max and average pooling. We empirically show\nthat Hartley spectral pooling gives rise to the convergence of training CNNs on\nMNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 06:57:01 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 20:05:06 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhang", "Hao", ""], ["Ma", "Jianwei", ""]]}, {"id": "1810.04029", "submitter": "Sang Jun Lee", "authors": "Sang Jun Lee, Sang Woo Kim, Wookyong Kwon, Gyogwon Koo, Jong Pil Yun", "title": "Selective Distillation of Weakly Annotated GTD for Vision-based Slab\n  Identification System", "comments": "10 pages, 12 figures, submitted to a journal", "journal-ref": "IEEE Access 7 (2019) 23177-23186", "doi": "10.1109/ACCESS.2019.2899109", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm for recognizing slab identification numbers\nin factory scenes. In the development of a deep-learning based system, manual\nlabeling to make ground truth data (GTD) is an important but expensive task.\nFurthermore, the quality of GTD is closely related to the performance of a\nsupervised learning algorithm. To reduce manual work in the labeling process,\nwe generated weakly annotated GTD by marking only character centroids. Whereas\nbounding-boxes for characters require at least a drag-and-drop operation or two\nclicks to annotate a character location, the weakly annotated GTD requires a\nsingle click to record a character location. The main contribution of this\npaper is on selective distillation to improve the quality of the weakly\nannotated GTD. Because manual GTD are usually generated by many people, it may\ncontain personal bias or human error. To address this problem, the information\nin manual GTD is integrated and refined by selective distillation. In the\nprocess of selective distillation, a fully convolutional network is trained\nusing the weakly annotated GTD, and its prediction maps are selectively used to\nrevise locations and boundaries of semantic regions of characters in the\ninitial GTD. The modified GTD are used in the main training stage, and a\npost-processing is conducted to retrieve text information. Experiments were\nthoroughly conducted on actual industry data collected at a steelmaking factory\nto demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:32:45 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:31:57 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Lee", "Sang Jun", ""], ["Kim", "Sang Woo", ""], ["Kwon", "Wookyong", ""], ["Koo", "Gyogwon", ""], ["Yun", "Jong Pil", ""]]}, {"id": "1810.04039", "submitter": "Mason Swofford", "authors": "Mason Swofford, John Peruzzi, and Marynel V\\'azquez", "title": "Conversational Group Detection With Deep Convolutional Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Detection of interacting and conversational groups from images has\napplications in video surveillance and social robotics. In this paper we build\non prior attempts to find conversational groups by detection of social\ngathering spaces called o-spaces used to assign people to groups. As our\ncontributions to the task, we are the first paper to incorporate features\nextracted from the room layout image, and the first to incorporate a deep\nnetwork to generate an image representation of the proposed o-spaces.\nSpecifically, this novel network builds on the PointNet architecture which\nallows unordered inputs of variable sizes. We present accuracies which\ndemonstrate the ability to rival and sometimes outperform the best models, but\ndue to a data imbalance issue we do not yet outperform existing models in our\ntest results.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 20:27:26 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 16:52:03 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Swofford", "Mason", ""], ["Peruzzi", "John", ""], ["V\u00e1zquez", "Marynel", ""]]}, {"id": "1810.04041", "submitter": "Ancheng Lin", "authors": "Ancheng Lin, Jun Li, Zhenyuan Ma", "title": "On Learning and Learned Data Representation by Capsule Networks", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 50808-50822, 2019", "doi": "10.1109/ACCESS.2019.2911622", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the following: 1) how the routing affects the\nCapsNet model fitting; 2) how the representation using capsules helps discover\nglobal structures in data distribution, and; 3) how the learned data\nrepresentation adapts and generalizes to new tasks. Our investigation yielded\nthe results some of which have been mentioned in the original paper of CapsNet,\nthey are: 1) the routing operation determines the certainty with which a layer\nof capsules pass information to the layer above and the appropriate level of\ncertainty is related to the model fitness; 2) in a designed experiment using\ndata with a known 2D structure, capsule representations enable a more\nmeaningful 2D manifold embedding than neurons do in a standard convolutional\nneural network (CNN), and; 3) compared with neurons of the standard CNN,\ncapsules of successive layers are less coupled and more adaptive to new data\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 03:14:53 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 02:24:29 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 03:22:39 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lin", "Ancheng", ""], ["Li", "Jun", ""], ["Ma", "Zhenyuan", ""]]}, {"id": "1810.04047", "submitter": "Samvit Jain", "authors": "Samvit Jain and Joseph E. Gonzalez", "title": "Inter-BMV: Interpolation with Block Motion Vectors for Fast Semantic\n  Segmentation on Video", "comments": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:1803.07742", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Models optimized for accuracy on single images are often prohibitively slow\nto run on each frame in a video. Recent work exploits the use of optical flow\nto warp image features forward from select keyframes, as a means to conserve\ncomputation on video. This approach, however, achieves only limited speedup,\neven when optimized, due to the accuracy degradation introduced by repeated\nforward warping, and the inference cost of optical flow estimation. To address\nthese problems, we propose a new scheme that propagates features using the\nblock motion vectors (BMV) present in compressed video (e.g. H.264 codecs),\ninstead of optical flow, and bi-directionally warps and fuses features from\nenclosing keyframes to capture scene context on each video frame. Our\ntechnique, interpolation-BMV, enables us to accurately estimate the features of\nintermediate frames, while keeping inference costs low. We evaluate our system\non the CamVid and Cityscapes datasets, comparing to both a strong single-frame\nbaseline and related work. We find that we are able to substantially accelerate\nsegmentation on video, achieving near real-time frame rates (20+ frames per\nsecond) on large images (e.g. 960 x 720 pixels), while maintaining competitive\naccuracy. This represents an improvement of almost 6x over the single-frame\nbaseline and 2.5x over the fastest prior work.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 06:22:30 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Jain", "Samvit", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1810.04093", "submitter": "Pierluigi Zama Ramirez", "authors": "Pierluigi Zama Ramirez, Matteo Poggi, Fabio Tosi, Stefano Mattoccia,\n  Luigi Di Stefano", "title": "Geometry meets semantics for semi-supervised monocular depth estimation", "comments": "16 pages, Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from a single image represents a very exciting challenge in\ncomputer vision. While other image-based depth sensing techniques leverage on\nthe geometry between different viewpoints (e.g., stereo or structure from\nmotion), the lack of these cues within a single image renders ill-posed the\nmonocular depth estimation task. For inference, state-of-the-art\nencoder-decoder architectures for monocular depth estimation rely on effective\nfeature representations learned at training time. For unsupervised training of\nthese models, geometry has been effectively exploited by suitable images\nwarping losses computed from views acquired by a stereo rig or a moving camera.\nIn this paper, we make a further step forward showing that learning semantic\ninformation from images enables to improve effectively monocular depth\nestimation as well. In particular, by leveraging on semantically labeled images\ntogether with unsupervised signals gained by geometry through an image warping\nloss, we propose a deep learning approach aimed at joint semantic segmentation\nand depth estimation. Our overall learning framework is semi-supervised, as we\ndeploy groundtruth data only in the semantic domain. At training time, our\nnetwork learns a common feature representation for both tasks and a novel\ncross-task loss function is proposed. The experimental findings show how,\njointly tackling depth prediction and semantic segmentation, allows to improve\ndepth estimation accuracy. In particular, on the KITTI dataset our network\noutperforms state-of-the-art methods for monocular depth estimation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 15:55:11 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 14:39:37 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Ramirez", "Pierluigi Zama", ""], ["Poggi", "Matteo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1810.04101", "submitter": "Loris Bazzani", "authors": "Loris Bazzani and Tobias Domhan and Felix Hieber", "title": "Image Captioning as Neural Machine Translation Task in SOCKEYE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is an interdisciplinary research problem that stands between\ncomputer vision and natural language processing. The task is to generate a\ntextual description of the content of an image. The typical model used for\nimage captioning is an encoder-decoder deep network, where the encoder captures\nthe essence of an image while the decoder is responsible for generating a\nsentence describing the image. Attention mechanisms can be used to\nautomatically focus the decoder on parts of the image which are relevant to\npredict the next word. In this paper, we explore different decoders and\nattentional models popular in neural machine translation, namely attentional\nrecurrent neural networks, self-attentional transformers, and\nfully-convolutional networks, which represent the current state of the art of\nneural machine translation. The image captioning module is available as part of\nSOCKEYE at https://github.com/awslabs/sockeye which tutorial can be found at\nhttps://awslabs.github.io/sockeye/image_captioning.html .\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:16:48 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 08:52:22 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 14:27:17 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bazzani", "Loris", ""], ["Domhan", "Tobias", ""], ["Hieber", "Felix", ""]]}, {"id": "1810.04108", "submitter": "Yeqi Liu", "authors": "Yeqi Liu, Yingyi Chen, Huihui Yu, Xiaomin Fang, Chuanyang Gong", "title": "Real time expert system for anomaly detection of aerators based on\n  computer vision technology and existing surveillance cameras", "comments": "17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerators are essential and crucial auxiliary devices in intensive culture,\nespecially in industrial culture in China. The traditional methods cannot\naccurately detect abnormal condition of aerators in time. Surveillance cameras\nare widely used as visual perception modules of the Internet of Things, and\nthen using these widely existing surveillance cameras to realize real-time\nanomaly detection of aerators is a cost-free and easy-to-promote method.\nHowever, it is difficult to develop such an expert system due to some technical\nand applied challenges, e.g., illumination, occlusion, complex background, etc.\nTo tackle these aforementioned challenges, we propose a real-time expert system\nbased on computer vision technology and existing surveillance cameras for\nanomaly detection of aerators, which consists of two modules, i.e., object\nregion detection and working state detection. First, it is difficult to detect\nthe working state for some small object regions in whole images, and the time\ncomplexity of global feature comparison is also high, so we present an object\nregion detection method based on the region proposal idea. Moreover, we propose\na novel algorithm called reference frame Kanade-Lucas-Tomasi (RF-KLT) algorithm\nfor motion feature extraction in fixed regions. Then, we present a dimension\nreduction method of time series for establishing a feature dataset with obvious\nboundaries between classes. Finally, we use machine learning algorithms to\nbuild the feature classifier. The experimental results in both the actual video\ndataset and the augmented video dataset show that the accuracy for detecting\nobject region and working state of aerators is 100% and 99.9% respectively, and\nthe detection speed is 77-333 frames per second (FPS) according to the\ndifferent types of surveillance cameras.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:28:14 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 01:47:25 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 15:34:59 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Liu", "Yeqi", ""], ["Chen", "Yingyi", ""], ["Yu", "Huihui", ""], ["Fang", "Xiaomin", ""], ["Gong", "Chuanyang", ""]]}, {"id": "1810.04144", "submitter": "Dinesh Kumar Amara", "authors": "Amara Dinesh Kumar, Koti Naga Renu Chebrolu, Vinayakumar R, Soman KP", "title": "A Brief Survey on Autonomous Vehicle Possible Attacks, Exploits and\n  Vulnerabilities", "comments": "5 Pages,1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advanced driver assistance systems are advancing at a rapid pace and all\nmajor companies started investing in developing the autonomous vehicles. But\nthe security and reliability is still uncertain and debatable. Imagine that a\nvehicle is compromised by the attackers and then what they can do. An attacker\ncan control brake, accelerate and even steering which can lead to catastrophic\nconsequences. This paper gives a very short and brief overview of most of the\npossible attacks on autonomous vehicle software and hardware and their\npotential implications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 09:31:14 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Kumar", "Amara Dinesh", ""], ["Chebrolu", "Koti Naga Renu", ""], ["R", "Vinayakumar", ""], ["KP", "Soman", ""]]}, {"id": "1810.04158", "submitter": "Benjamin Planche", "authors": "Benjamin Planche, Sergey Zakharov, Ziyan Wu, Andreas Hutter, Harald\n  Kosch, Slobodan Ilic", "title": "Seeing Beyond Appearance - Mapping Real Images into Geometrical Domains\n  for Unsupervised CAD-based Recognition", "comments": "paper + supplementary material; previous work: \"Keep it Unreal:\n  Bridging the Realism Gap for 2.5D Recognition with Geometry Priors Only\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks are dominating the field of computer\nvision, one usually does not have access to the large amount of domain-relevant\ndata needed for their training. It thus became common to use available\nsynthetic samples along domain adaptation schemes to prepare algorithms for the\ntarget domain. Tackling this problem from a different angle, we introduce a\npipeline to map unseen target samples into the synthetic domain used to train\ntask-specific methods. Denoising the data and retaining only the features these\nrecognition algorithms are familiar with, our solution greatly improves their\nperformance. As this mapping is easier to learn than the opposite one (ie to\nlearn to generate realistic features to augment the source samples), we\ndemonstrate how our whole solution can be trained purely on augmented synthetic\ndata, and still perform better than methods trained with domain-relevant\ninformation (eg real images or realistic textures for the 3D models). Applying\nour approach to object recognition from texture-less CAD data, we present a\ncustom generative network which fully utilizes the purely geometrical\ninformation to learn robust features and achieve a more refined mapping for\nunseen color images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 17:59:16 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Planche", "Benjamin", ""], ["Zakharov", "Sergey", ""], ["Wu", "Ziyan", ""], ["Hutter", "Andreas", ""], ["Kosch", "Harald", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1810.04231", "submitter": "Kun Wan", "authors": "Kun Wan, Boyuan Feng, Shu Yang, Yufei Ding", "title": "Penetrating the Fog: the Path to Efficient CNN Models", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand to deploy convolutional neural networks (CNNs) on\nmobile platforms, the sparse kernel approach was proposed, which could save\nmore parameters than the standard convolution while maintaining accuracy.\nHowever, despite the great potential, no prior research has pointed out how to\ncraft an sparse kernel design with such potential (i.e., effective design), and\nall prior works just adopt simple combinations of existing sparse kernels such\nas group convolution. Meanwhile due to the large design space it is also\nimpossible to try all combinations of existing sparse kernels. In this paper,\nwe are the first in the field to consider how to craft an effective sparse\nkernel design by eliminating the large design space. Specifically, we present a\nsparse kernel scheme to illustrate how to reduce the space from three aspects.\nFirst, in terms of composition we remove designs composed of repeated layers.\nSecond, to remove designs with large accuracy degradation, we find an unified\nproperty named information field behind various sparse kernel designs, which\ncould directly indicate the final accuracy. Last, we remove designs in two\ncases where a better parameter efficiency could be achieved. Additionally, we\nprovide detailed efficiency analysis on the final four designs in our scheme.\nExperimental results validate the idea of our scheme by showing that our scheme\nis able to find designs which are more efficient in using parameters and\ncomputation with similar or higher accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 20:16:29 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 00:40:37 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Wan", "Kun", ""], ["Feng", "Boyuan", ""], ["Yang", "Shu", ""], ["Ding", "Yufei", ""]]}, {"id": "1810.04246", "submitter": "Mohammed Jabi", "authors": "Mohammed Jabi, Marco Pedersoli, Amar Mitiche and Ismail Ben Ayed", "title": "Deep clustering: On the link between discriminative models and K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of recent deep clustering studies, discriminative models\ndominate the literature and report the most competitive performances. These\nmodels learn a deep discriminative neural network classifier in which the\nlabels are latent. Typically, they use multinomial logistic regression\nposteriors and parameter regularization, as is very common in supervised\nlearning. It is generally acknowledged that discriminative objective functions\n(e.g., those based on the mutual information or the KL divergence) are more\nflexible than generative approaches (e.g., K-means) in the sense that they make\nfewer assumptions about the data distributions and, typically, yield much\nbetter unsupervised deep learning results. On the surface, several recent\ndiscriminative models may seem unrelated to K-means. This study shows that\nthese models are, in fact, equivalent to K-means under mild conditions and\ncommon posterior models and parameter regularization. We prove that, for the\ncommonly used logistic regression posteriors, maximizing the $L_2$ regularized\nmutual information via an approximate alternating direction method (ADM) is\nequivalent to a soft and regularized K-means loss. Our theoretical analysis not\nonly connects directly several recent state-of-the-art discriminative models to\nK-means, but also leads to a new soft and regularized deep K-means algorithm,\nwhich yields competitive performance on several image clustering benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 21:17:09 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 23:28:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Jabi", "Mohammed", ""], ["Pedersoli", "Marco", ""], ["Mitiche", "Amar", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1810.04250", "submitter": "Akash Kumar", "authors": "Sourya Dipta Das and Akash Kumar", "title": "Bird Species Classification using Transfer Learning with Multistage\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bird species classification has received more and more attention in the field\nof computer vision, for its promising applications in biology and environmental\nstudies. Recognizing bird species is difficult due to the challenges of\ndiscriminative region localization and fine-grained feature learning. In this\npaper, we have introduced a Transfer learning based method with multistage\ntraining. We have used both Pre-Trained Mask-RCNN and an ensemble model\nconsisting of Inception Nets (InceptionV3 & InceptionResNetV2 ) to get\nlocalization and species of the bird from the images respectively. Our final\nmodel achieves an F1 score of 0.5567 or 55.67 % on the dataset provided in CVIP\n2018 Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 21:29:08 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 16:30:29 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Kumar", "Akash", ""]]}, {"id": "1810.04260", "submitter": "Vishwesh Nath", "authors": "Vishwesh Nath, Prasanna Parvathaneni, Colin B. Hansen, Allison E.\n  Hainline, Camilo Bermudez, Samuel Remedios, Justin A. Blaber, Kurt G.\n  Schilling, Ilwoo Lyu, Vaibhav Janve, Yurui Gao, Iwona Stepniewska, Baxter P.\n  Rogers, Allen T. Newton, L. Taylor Davis, Jeff Luci, Adam W. Anderson,\n  Bennett A. Landman", "title": "Inter-Scanner Harmonization of High Angular Resolution DW-MRI using Null\n  Space Deep Learning", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (DW-MRI) allows for\nnon-invasive imaging of the local fiber architecture of the human brain at a\nmillimetric scale. Multiple classical approaches have been proposed to detect\nboth single (e.g., tensors) and multiple (e.g., constrained spherical\ndeconvolution, CSD) fiber population orientations per voxel. However, existing\ntechniques generally exhibit low reproducibility across MRI scanners. Herein,\nwe propose a data-driven tech-nique using a neural network design which\nexploits two categories of data. First, training data were acquired on three\nsquirrel monkey brains using ex-vivo DW-MRI and histology of the brain. Second,\nrepeated scans of human subjects were acquired on two different scanners to\naugment the learning of the network pro-posed. To use these data, we propose a\nnew network architecture, the null space deep network (NSDN), to simultaneously\nlearn on traditional observed/truth pairs (e.g., MRI-histology voxels) along\nwith repeated observations without a known truth (e.g., scan-rescan MRI). The\nNSDN was tested on twenty percent of the histology voxels that were kept\ncompletely blind to the network. NSDN significantly improved absolute\nperformance relative to histology by 3.87% over CSD and 1.42% over a recently\nproposed deep neural network approach. More-over, it improved reproducibility\non the paired data by 21.19% over CSD and 10.09% over a recently proposed deep\napproach. Finally, NSDN improved gen-eralizability of the model to a third in\nvivo human scanner (which was not used in training) by 16.08% over CSD and\n10.41% over a recently proposed deep learn-ing approach. This work suggests\nthat data-driven approaches for local fiber re-construction are more\nreproducible, informative and precise and offers a novel, practical method for\ndetermining these models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 21:52:10 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Nath", "Vishwesh", ""], ["Parvathaneni", "Prasanna", ""], ["Hansen", "Colin B.", ""], ["Hainline", "Allison E.", ""], ["Bermudez", "Camilo", ""], ["Remedios", "Samuel", ""], ["Blaber", "Justin A.", ""], ["Schilling", "Kurt G.", ""], ["Lyu", "Ilwoo", ""], ["Janve", "Vaibhav", ""], ["Gao", "Yurui", ""], ["Stepniewska", "Iwona", ""], ["Rogers", "Baxter P.", ""], ["Newton", "Allen T.", ""], ["Davis", "L. Taylor", ""], ["Luci", "Jeff", ""], ["Anderson", "Adam W.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1810.04261", "submitter": "Ruiqi Gao", "authors": "Ying Nian Wu, Ruiqi Gao, Tian Han, Song-Chun Zhu", "title": "A Tale of Three Probabilistic Families: Discriminative, Descriptive and\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pattern theory of Grenander is a mathematical framework where patterns\nare represented by probability models on random variables of algebraic\nstructures. In this paper, we review three families of probability models,\nnamely, the discriminative models, the descriptive models, and the generative\nmodels. A discriminative model is in the form of a classifier. It specifies the\nconditional probability of the class label given the input signal. A\ndescriptive model specifies the probability distribution of the signal, based\non an energy function defined on the signal. A generative model assumes that\nthe signal is generated by some latent variables via a transformation. We shall\nreview these models within a common framework and explore their connections. We\nshall also review the recent developments that take advantage of the high\napproximation capacities of deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 21:54:54 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 00:33:15 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Wu", "Ying Nian", ""], ["Gao", "Ruiqi", ""], ["Han", "Tian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1810.04274", "submitter": "Mariano Cabezas", "authors": "Mariano Cabezas, Sergi Valverde, Sandra Gonz\\'alez-Vill\\`a, Albert\n  Cl\\'erigues, Mostafa Salem, Kaisar Kushibar, Jose Bernal, Arnau Oliver, and\n  Xavier Llad\\'o", "title": "Survival prediction using ensemble tumor segmentation and transfer\n  learning", "comments": "Submitted to the BRATS2018 MICCAI challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting tumors and their subregions is a challenging task as demonstrated\nby the annual BraTS challenge. Moreover, predicting the survival of the patient\nusing mainly imaging features, while being a desirable outcome to evaluate the\ntreatment of the patient, it is also a difficult task. In this paper, we\npresent a cascaded pipeline to segment the tumor and its subregions and then we\nuse these results and other clinical features together with image features\ncoming from a pretrained VGG-16 network to predict the survival of the patient.\nPreliminary results with the training and validation dataset show a promising\nstart in terms of segmentation, while the prediction values could be improved\nwith further testing on the feature extraction part of the network.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 09:55:09 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Cabezas", "Mariano", ""], ["Valverde", "Sergi", ""], ["Gonz\u00e1lez-Vill\u00e0", "Sandra", ""], ["Cl\u00e9rigues", "Albert", ""], ["Salem", "Mostafa", ""], ["Kushibar", "Kaisar", ""], ["Bernal", "Jose", ""], ["Oliver", "Arnau", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1810.04320", "submitter": "Oliver Woodford", "authors": "Oliver J. Woodford", "title": "Least Squares Normalized Cross Correlation", "comments": "18 pages. Submitted for re-review to TPAMI on July 6th 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct methods are widely used for alignment of models to images, due to\ntheir accuracy, since they minimize errors in the domain of measurement noise.\nThey have leveraged least squares minimizations, for simple, efficient,\nvariational optimization, since the seminal 1981 work of Lucas & Kanade, and\nnormalized cross correlation (NCC), for robustness to intensity variations,\nsince at least 1972. Despite the complementary benefits of these two well known\nmethods, they have not been effectively combined to address local variations in\nintensity. Many ad-hoc NCC frameworks, sub-optimal least squares methods and\nimage transformation approaches have thus been proposed instead, each with\ntheir own limitations. This work shows that a least squares optimization of NCC\nwithout approximation is not only possible, but straightforward and efficient.\nA robust, locally normalized formulation is introduced to mitigate local\nintensity variations and partial occlusions. Finally, sparse features with\noriented patches are proposed for further efficiency. The resulting framework\nis simple to implement, computationally efficient and robust to local intensity\nvariations. It is evaluated on the image alignment problem, showing\nimprovements in both convergence rate and computation time over existing\nlighting invariant methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 00:55:19 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 15:40:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Woodford", "Oliver J.", ""]]}, {"id": "1810.04377", "submitter": "Lili Huang", "authors": "Lili Huang, Jiefeng Peng, Ruimao Zhang, Guanbin Li, Liang Lin", "title": "Learning Deep Representations for Semantic Image Parsing: a\n  Comprehensive Overview", "comments": null, "journal-ref": null, "doi": "10.1007/s11704-018-7195-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image parsing, which refers to the process of decomposing images\ninto semantic regions and constructing the structure representation of the\ninput, has recently aroused widespread interest in the field of computer\nvision. The recent application of deep representation learning has driven this\nfield into a new stage of development. In this paper, we summarize three\naspects of the progress of research on semantic image parsing, i.e.,\ncategory-level semantic segmentation, instance-level semantic segmentation, and\nbeyond segmentation. Specifically, we first review the general frameworks for\neach task and introduce the relevant variants. The advantages and limitations\nof each method are also discussed. Moreover, we present a comprehensive\ncomparison of different benchmark datasets and evaluation metrics. Finally, we\nexplore the future trends and challenges of semantic image parsing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 05:29:47 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Huang", "Lili", ""], ["Peng", "Jiefeng", ""], ["Zhang", "Ruimao", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "1810.04409", "submitter": "Suiyi Ling", "authors": "Suiyi Ling, Jes\\'us Guti\\'errez, Gu Ke and Patrick Le Callet", "title": "Prediction of the Influence of Navigation Scan-path on Perceived Quality\n  of Free-Viewpoint Videos", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-Viewpoint Video (FVV) systems allow the viewers to freely change the\nviewpoints of the scene. In such systems, view synthesis and compression are\nthe two main sources of artifacts influencing the perceived quality. To assess\nthis influence, quality evaluation studies are often carried out using\nconventional displays and generating predefined navigation trajectories\nmimicking the possible movement of the viewers when exploring the content.\nNevertheless, as different trajectories may lead to different conclusions in\nterms of visual quality when benchmarking the performance of the systems,\nmethods to identify critical trajectories are needed. This paper aims at\nexploring the impact of exploration trajectories (defined as Hypothetical\nRendering Trajectories: HRT) on perceived quality of FVV subjectively and\nobjectively, providing two main contributions. Firstly, a subjective assessment\ntest including different HRTs was carried out and analyzed. The results\ndemonstrate and quantify the influence of HRT in the perceived quality.\nSecondly, we propose a new objective video quality assessment measure to\nobjectively predict the impact of HRT. This measure, based on Sketch-Token\nrepresentation, models how the categories of the contours change spatially and\ntemporally from a higher semantic level. Performance in comparison with\nexisting quality metrics for FVV, highlight promising results for automatic\ndetection of most critical HRTs for the benchmark of immersive systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 08:12:06 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Ling", "Suiyi", ""], ["Guti\u00e9rrez", "Jes\u00fas", ""], ["Ke", "Gu", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1810.04452", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Ahmed Imtiaz Humayun, Samiul Alam, Rashed Mohammad\n  Doha, Manash Kumar Mandal, Tahsin Reasat and Fuad Rahman", "title": "AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:59:28 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Humayun", "Ahmed Imtiaz", ""], ["Alam", "Samiul", ""], ["Doha", "Rashed Mohammad", ""], ["Mandal", "Manash Kumar", ""], ["Reasat", "Tahsin", ""], ["Rahman", "Fuad", ""]]}, {"id": "1810.04456", "submitter": "Zhaohui Che", "authors": "Zhaohui Che, Ali Borji, Guangtao Zhai, Xiongkuo Min", "title": "Invariance Analysis of Saliency Models versus Human Gaze During Scene\n  Free Viewing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of current studies on human gaze and saliency modeling have used\nhigh-quality stimuli. In real world, however, captured images undergo various\ntypes of distortions during the whole acquisition, transmission, and displaying\nchain. Some distortion types include motion blur, lighting variations and\nrotation. Despite few efforts, influences of ubiquitous distortions on visual\nattention and saliency models have not been systematically investigated. In\nthis paper, we first create a large-scale database including eye movements of\n10 observers over 1900 images degraded by 19 types of distortions. Second, by\nanalyzing eye movements and saliency models, we find that: a) observers look at\ndifferent locations over distorted versus original images, and b) performances\nof saliency models are drastically hindered over distorted images, with the\nmaximum performance drop belonging to Rotation and Shearing distortions.\nFinally, we investigate the effectiveness of different distortions when serving\nas data augmentation transformations. Experimental results verify that some\nuseful data augmentation transformations which preserve human gaze of reference\nimages can improve deep saliency models against distortions, while some invalid\ntransformations which severely change human gaze will degrade the performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 11:10:28 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Che", "Zhaohui", ""], ["Borji", "Ali", ""], ["Zhai", "Guangtao", ""], ["Min", "Xiongkuo", ""]]}, {"id": "1810.04461", "submitter": "Daniele De Gregorio", "authors": "Daniele De Gregorio, Gianluca Palli and Luigi Di Stefano", "title": "Let's take a Walk on Superpixels Graphs: Deformable Linear Objects\n  Segmentation and Model Estimation", "comments": "Accepted as Oral to ACCV 2018, Perth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While robotic manipulation of rigid objects is quite straightforward, coping\nwith deformable objects is an open issue. More specifically, tasks like tying a\nknot, wiring a connector or even surgical suturing deal with the domain of\nDeformable Linear Objects (DLOs). In particular the detection of a DLO is a\nnon-trivial problem especially under clutter and occlusions (as well as\nself-occlusions). The pose estimation of a DLO results into the identification\nof its parameters related to a designed model, e.g. a basis spline. It follows\nthat the stand-alone segmentation of a DLO might not be sufficient to conduct a\nfull manipulation task. This is why we propose a novel framework able to\nperform both a semantic segmentation and b-spline modeling of multiple\ndeformable linear objects simultaneously without strict requirements about\nenvironment (i.e. the background). The core algorithm is based on biased random\nwalks over the Region Adiacency Graph built on a superpixel oversegmentation of\nthe source image. The algorithm is initialized by a Convolutional Neural\nNetworks that detects the DLO's endcaps. An open source implementation of the\nproposed approach is also provided to easy the reproduction of the whole\ndetection pipeline along with a novel cables dataset in order to encourage\nfurther experiments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 11:31:14 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["De Gregorio", "Daniele", ""], ["Palli", "Gianluca", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1810.04511", "submitter": "Lili Meng", "authors": "Lili Meng, Bo Zhao, Bo Chang, Gao Huang, Wei Sun, Frederich Tung,\n  Leonid Sigal", "title": "Interpretable Spatio-temporal Attention for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the observation that humans are able to process videos\nefficiently by only paying attention where and when it is needed, we propose an\ninterpretable and easy plug-in spatial-temporal attention mechanism for video\naction recognition. For spatial attention, we learn a saliency mask to allow\nthe model to focus on the most salient parts of the feature maps. For temporal\nattention, we employ a convolutional LSTM based attention mechanism to identify\nthe most relevant frames from an input video. Further, we propose a set of\nregularizers to ensure that our attention mechanism attends to coherent regions\nin space and time. Our model not only improves video action recognition\naccuracy, but also localizes discriminative regions both spatially and\ntemporally, despite being trained in a weakly-supervised manner with only\nclassification labels (no bounding box labels or time frame temporal labels).\nWe evaluate our approach on several public video action recognition datasets\nwith ablation studies. Furthermore, we quantitatively and qualitatively\nevaluate our model's ability to localize discriminative regions spatially and\ncritical frames temporally. Experimental results demonstrate the efficacy of\nour approach, showing superior or comparable accuracy with the state-of-the-art\nmethods while increasing model interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 04:23:35 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 03:09:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Meng", "Lili", ""], ["Zhao", "Bo", ""], ["Chang", "Bo", ""], ["Huang", "Gao", ""], ["Sun", "Wei", ""], ["Tung", "Frederich", ""], ["Sigal", "Leonid", ""]]}, {"id": "1810.04604", "submitter": "Vuong M. Ngo", "authors": "Sven Helmer and Vuong M. Ngo", "title": "A Similarity Measure for Weaving Patterns in Textiles", "comments": "10 papes, will be published in SIGIR 2015", "journal-ref": "SIGIR 2015", "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for measuring the similarity between weaving\npatterns that can provide similarity-based search functionality for textile\narchives. We represent textile structures using hypergraphs and extract\nmultisets of k-neighborhoods from these graphs. The resulting multisets are\nthen compared using Jaccard coefficients, Hamming distances, and cosine\nmeasures. We evaluate the different variants of our similarity measure\nexperimentally, showing that it can be implemented efficiently and illustrating\nits quality using it to cluster and query a data set containing more than a\nthousand textile samples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:50:03 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Helmer", "Sven", ""], ["Ngo", "Vuong M.", ""]]}, {"id": "1810.04622", "submitter": "Elliot J. Crowley", "authors": "Elliot J. Crowley, Jack Turner, Amos Storkey, Michael O'Boyle", "title": "A Closer Look at Structured Pruning for Neural Network Compression", "comments": "Preprint. First two authors contributed equally. Paper title has\n  changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Structured pruning is a popular method for compressing a neural network:\ngiven a large trained network, one alternates between removing channel\nconnections and fine-tuning; reducing the overall width of the network.\nHowever, the efficacy of structured pruning has largely evaded scrutiny. In\nthis paper, we examine ResNets and DenseNets obtained through structured\npruning-and-tuning and make two interesting observations: (i) reduced\nnetworks---smaller versions of the original network trained from\nscratch---consistently outperform pruned networks; (ii) if one takes the\narchitecture of a pruned network and then trains it from scratch it is\nsignificantly more competitive. Furthermore, these architectures are easy to\napproximate: we can prune once and obtain a family of new, scalable network\narchitectures that can simply be trained from scratch. Finally, we compare the\ninference speed of reduced and pruned networks on hardware, and show that\nreduced networks are significantly faster. Code is available at\nhttps://github.com/BayesWatch/pytorch-prunes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:30:02 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 14:40:12 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 14:23:14 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Crowley", "Elliot J.", ""], ["Turner", "Jack", ""], ["Storkey", "Amos", ""], ["O'Boyle", "Michael", ""]]}, {"id": "1810.04637", "submitter": "Fakrul Islam Tushar", "authors": "Md. Kamrul Hasan, Fakrul Islam Tushar", "title": "Quantification of Trabeculae Inside the Heart from MRI Using Fractal\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left ventricular non-compaction (LVNC) is a rare cardiomyopathy (CMP) that\nshould be considered as a possible diagnosis because of its potential\ncomplications which are heart failure, ventricular arrhythmias, and embolic\nevents. For analysis cardiac functionality, extracting information from the\nLeft ventricular (LV) is already a broad field of Medical Imaging. Different\nalgorithms and strategies ranging that is semiautomated or automated has\nalready been developed to get useful information from such a critical structure\nof heart. Trabeculae in the heart undergoes difference changes like solid from\nspongy. Due to failure of this process left ventricle non-compaction occurred.\nIn this project, we will demonstrate the fractal dimension (FD) and manual\nsegmentation of the Magnetic Resonance Imaging (MRI) of the heart that quantify\namount of trabeculae inside the heart. The greater the value of fractal\ndimension inside the heart indicates the greater complex pattern of the\ntrabeculae in the heart.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:05:48 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 17:00:09 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Tushar", "Fakrul Islam", ""]]}, {"id": "1810.04652", "submitter": "Huy Nguyen", "authors": "Eric Dodds, Huy Nguyen, Simao Herdade, Jack Culpepper, Andrew Kae,\n  Pierre Garrigues", "title": "Learning Embeddings for Product Visual Search with Triplet Loss and\n  Online Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose learning an embedding function for content-based\nimage retrieval within the e-commerce domain using the triplet loss and an\nonline sampling method that constructs triplets from within a minibatch. We\ncompare our method to several strong baselines as well as recent works on the\nDeepFashion and Stanford Online Product datasets. Our approach significantly\noutperforms the state-of-the-art on the DeepFashion dataset. With a\nmodification to favor sampling minibatches from a single product category, the\nsame approach demonstrates competitive results when compared to the\nstate-of-the-art for the Stanford Online Products dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 17:19:08 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Dodds", "Eric", ""], ["Nguyen", "Huy", ""], ["Herdade", "Simao", ""], ["Culpepper", "Jack", ""], ["Kae", "Andrew", ""], ["Garrigues", "Pierre", ""]]}, {"id": "1810.04703", "submitter": "Yinghao Huang", "authors": "Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J. Black, Otmar\n  Hilliges, Gerard Pons-Moll", "title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse\n  Inertial Measurements in Real Time", "comments": "SIGGRAPH Asia 2018. First two authors contributed equally to this\n  work. Project page: http://dip.is.tue.mpg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a novel deep neural network capable of reconstructing human\nfull body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on\nthe user's body. In doing so, we address several difficult challenges. First,\nthe problem is severely under-constrained as multiple pose parameters produce\nthe same IMU orientations. Second, capturing IMU data in conjunction with\nground-truth poses is expensive and difficult to do in many target application\nscenarios (e.g., outdoors). Third, modeling temporal dependencies through\nnon-linear optimization has proven effective in prior work but makes real-time\nprediction infeasible. To address this important limitation, we learn the\ntemporal pose priors using deep learning. To learn from sufficient data, we\nsynthesize IMU data from motion capture datasets. A bi-directional RNN\narchitecture leverages past and future information that is available at\ntraining time. At test time, we deploy the network in a sliding window fashion,\nretaining real time capabilities. To evaluate our method, we recorded DIP-IMU,\na dataset consisting of $10$ subjects wearing 17 IMUs for validation in $64$\nsequences with $330\\,000$ time instants; this constitutes the largest IMU\ndataset publicly available. We quantitatively evaluate our approach on multiple\ndatasets and show results from a real-time implementation. DIP-IMU and the code\nare available for research purposes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 18:45:55 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Huang", "Yinghao", ""], ["Kaufmann", "Manuel", ""], ["Aksan", "Emre", ""], ["Black", "Michael J.", ""], ["Hilliges", "Otmar", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1810.04752", "submitter": "T. Hoang Ngan Le", "authors": "T. Hoang Ngan Le, Raajitha Gummadi, Marios Savvides", "title": "Deep Recurrent Level Set for Segmenting Brain Tumors", "comments": null, "journal-ref": "booktitle=\"Medical Image Computing and Computer Assisted\n  Intervention -- MICCAI 2018\", year=\"2018\", publisher=\"Springer International\n  Publishing\",", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Level Set (VLS) has been a widely used method in medical\nsegmentation. However, segmentation accuracy in the VLS method dramatically\ndecreases when dealing with intervening factors such as lighting, shadows,\ncolors, etc. Additionally, results are quite sensitive to initial settings and\nare highly dependent on the number of iterations. In order to address these\nlimitations, the proposed method incorporates VLS into deep learning by\ndefining a novel end-to-end trainable model called as Deep Recurrent Level Set\n(DRLS). The proposed DRLS consists of three layers, i.e, Convolutional layers,\nDeconvolutional layers with skip connections and LevelSet layers. Brain tumor\nsegmentation is taken as an instant to illustrate the performance of the\nproposed DRLS. Convolutional layer learns visual representation of brain tumor\nat different scales. Since brain tumors occupy a small portion of the image,\ndeconvolutional layers are designed with skip connections to obtain a high\nquality feature map. Level-Set Layer drives the contour towards the brain\ntumor. In each step, the Convolutional Layer is fed with the LevelSet map to\nobtain a brain tumor feature map. This in turn serves as input for the LevelSet\nlayer in the next step. The experimental results have been obtained on\nBRATS2013, BRATS2015 and BRATS2017 datasets. The proposed DRLS model improves\nboth computational time and segmentation accuracy when compared to the the\nclassic VLS-based method. Additionally, a fully end-to-end system DRLS achieves\nstate-of-the-art segmentation on brain tumors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 21:33:13 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Le", "T. Hoang Ngan", ""], ["Gummadi", "Raajitha", ""], ["Savvides", "Marios", ""]]}, {"id": "1810.04871", "submitter": "Homanga Bharadhwaj", "authors": "Homanga Bharadhwaj, Zihan Wang, Yoshua Bengio, Liam Paull", "title": "A Data-Efficient Framework for Training and Sim-to-Real Transfer of\n  Navigation Policies", "comments": "Under review in ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective visuomotor policies for robots purely from data is\nchallenging, but also appealing since a learning-based system should not\nrequire manual tuning or calibration. In the case of a robot operating in a\nreal environment the training process can be costly, time-consuming, and even\ndangerous since failures are common at the start of training. For this reason,\nit is desirable to be able to leverage \\textit{simulation} and\n\\textit{off-policy} data to the extent possible to train the robot. In this\nwork, we introduce a robust framework that plans in simulation and transfers\nwell to the real environment. Our model incorporates a gradient-descent based\nplanning module, which, given the initial image and goal image, encodes the\nimages to a lower dimensional latent state and plans a trajectory to reach the\ngoal. The model, consisting of the encoder and planner modules, is trained\nthrough a meta-learning strategy in simulation first. We subsequently perform\nadversarial domain transfer on the encoder by using a bank of unlabelled but\nrandom images from the simulation and real environments to enable the encoder\nto map images from the real and simulated environments to a similarly\ndistributed latent representation. By fine tuning the entire model (encoder +\nplanner) with far fewer real world expert demonstrations, we show successful\nplanning performances in different navigation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 07:22:54 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Bharadhwaj", "Homanga", ""], ["Wang", "Zihan", ""], ["Bengio", "Yoshua", ""], ["Paull", "Liam", ""]]}, {"id": "1810.04873", "submitter": "Yucheng Wang", "authors": "Yucheng Wang and Jialiang Shen and Jian Zhang", "title": "Deep Bi-Dense Networks for Image Super-Resolution", "comments": "DICTA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Deep Bi-Dense Networks (DBDN) for single image\nsuper-resolution. Our approach extends previous intra-block dense connection\napproaches by including novel inter-block dense connections. In this way,\nfeature information propagates from a single dense block to all subsequent\nblocks, instead of to a single successor. To build a DBDN, we firstly construct\nintra-dense blocks, which extract and compress abundant local features via\ndensely connected convolutional layers and compression layers for further\nfeature learning. Then, we use an inter-block dense net to connect intra-dense\nblocks, which allow each intra-dense block propagates its own local features to\nall successors. Additionally, our bi-dense construction connects each block to\nthe output, alleviating the vanishing gradient problems in training. The\nevaluation of our proposed method on five benchmark datasets shows that our\nDBDN outperforms the state of the art in SISR with a moderate number of network\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 07:34:39 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Wang", "Yucheng", ""], ["Shen", "Jialiang", ""], ["Zhang", "Jian", ""]]}, {"id": "1810.04879", "submitter": "Sao Mai Nguyen", "authors": "Maxime Devanne (LIFL), Sao Mai Nguyen (Lab-STICC, IMT Atlantique)", "title": "Generating Shared Latent Variables for Robots to Imitate Human Movements\n  and Understand their Physical Limitations", "comments": null, "journal-ref": "Computer Vision -- ECCV 2018 Workshops", "doi": "10.1007/978-3-030-11012-3_15", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assistive robotics and particularly robot coaches may be very helpful for\nrehabilitation healthcare. In this context, we propose a method based on\nGaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a\nphysiotherapist, a robot coach and a patient. Our model is able to map visual\nhuman body features to robot data in order to facilitate the robot learning and\nimitation. In addition , we propose to extend the model to adapt robots'\nunderstanding to patient's physical limitations during the assessment of\nrehabilitation exercises. Experimental evaluation demonstrates promising\nresults for both robot imitation and model adaptation according to the\npatients' limitations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:01:08 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 22:09:34 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Devanne", "Maxime", "", "LIFL"], ["Nguyen", "Sao Mai", "", "Lab-STICC, IMT Atlantique"]]}, {"id": "1810.04881", "submitter": "Stefano Bromuri Dr", "authors": "R.L. Curier, T.J.A. De Jong, Katharina Strauch, Katharina Cramer,\n  Natalie Rosenski, Clara Schartner, M. Debusschere, and Hannah Ziemons, Deniz\n  Iren and Stefano Bromuri", "title": "Monitoring spatial sustainable development: Semi-automated analysis of\n  satellite and aerial images for energy transition and sustainability\n  indicators", "comments": "This document provides the reader with an overview of the various\n  datasets which will be used throughout the project. The collection of\n  satellite and aerial images as well as auxiliary information such as the\n  location of buildings and roofs which is required to train, test and validate\n  the machine learning algorithm that is being developed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Solar panels are installed by a large and growing number of households due to\nthe convenience of having cheap and renewable energy to power house appliances.\nIn contrast to other energy sources solar installations are distributed very\ndecentralized and spread over hundred-thousands of locations. On a global level\nmore than 25% of solar photovoltaic (PV) installations were decentralized. The\neffect of the quick energy transition from a carbon based economy to a green\neconomy is though still very difficult to quantify. As a matter of fact the\nquick adoption of solar panels by households is difficult to track, with local\nregistries that miss a large number of the newly built solar panels. This makes\nthe task of assessing the impact of renewable energies an impossible task.\nAlthough models of the output of a region exist, they are often black box\nestimations. This project's aim is twofold: First automate the process to\nextract the location of solar panels from aerial or satellite images and\nsecond, produce a map of solar panels along with statistics on the number of\nsolar panels. Further, this project takes place in a wider framework which\ninvestigates how official statistics can benefit from new digital data sources.\nAt project completion, a method for detecting solar panels from aerial images\nvia machine learning will be developed and the methodology initially developed\nfor BE, DE and NL will be standardized for application to other EU countries.\nIn practice, machine learning techniques are used to identify solar panels in\nsatellite and aerial images for the province of Limburg (NL), Flanders (BE) and\nNorth Rhine-Westphalia (DE).\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:05:29 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Curier", "R. L.", ""], ["De Jong", "T. J. A.", ""], ["Strauch", "Katharina", ""], ["Cramer", "Katharina", ""], ["Rosenski", "Natalie", ""], ["Schartner", "Clara", ""], ["Debusschere", "M.", ""], ["Ziemons", "Hannah", ""], ["Iren", "Deniz", ""], ["Bromuri", "Stefano", ""]]}, {"id": "1810.04891", "submitter": "Lan Hu", "authors": "Lan Hu, Yuchen Cao, Peng Wu, Laurent Kneip", "title": "Dense Object Reconstruction from RGBD Images with Embedded Deep Shape\n  Representations", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most problems involving simultaneous localization and mapping can nowadays be\nsolved using one of two fundamentally different approaches. The traditional\napproach is given by a least-squares objective, which minimizes many local\nphotometric or geometric residuals over explicitly parametrized structure and\ncamera parameters. Unmodeled effects violating the lambertian surface\nassumption or geometric invariances of individual residuals are encountered\nthrough statistical averaging or the addition of robust kernels and smoothness\nterms. Aiming at more accurate measurement models and the inclusion of\nhigher-order shape priors, the community more recently shifted its attention to\ndeep end-to-end models for solving geometric localization and mapping problems.\nHowever, at test-time, these feed-forward models ignore the more traditional\ngeometric or photometric consistency terms, thus leading to a low ability to\nrecover fine details and potentially complete failure in corner case scenarios.\nWith an application to dense object modeling from RGBD images, our work aims at\ntaking the best of both worlds by embedding modern higher-order object shape\npriors into classical iterative residual minimization objectives. We\ndemonstrate a general ability to improve mapping accuracy with respect to each\nmodality alone, and present a successful application to real data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:26:33 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Hu", "Lan", ""], ["Cao", "Yuchen", ""], ["Wu", "Peng", ""], ["Kneip", "Laurent", ""]]}, {"id": "1810.04898", "submitter": "David Robben", "authors": "David Robben, Paul Suetens", "title": "Perfusion parameter estimation using neural networks and data\n  augmentation", "comments": "Presented at the MICCAI 2018 SWITCH workshop (16 September 2018,\n  Granada, Spain)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perfusion imaging plays a crucial role in acute stroke diagnosis and\ntreatment decision making. Current perfusion analysis relies on deconvolution\nof the measured signals, an operation that is mathematically ill-conditioned\nand requires strong regularization. We propose a neural network and a data\naugmentation approach to predict perfusion parameters directly from the native\nmeasurements. A comparison on simulated CT Perfusion data shows that the neural\nnetwork provides better estimations for both CBF and Tmax than a state of the\nart deconvolution method, and this over a wide range of noise levels. The\nproposed data augmentation enables to achieve these results with less than 100\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:36:30 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Robben", "David", ""], ["Suetens", "Paul", ""]]}, {"id": "1810.04903", "submitter": "Fatma BenSaid", "authors": "Fatma BenSaid and Adel M. Alimi", "title": "MOANOFS: Multi-Objective Automated Negotiation based Online Feature\n  Selection System for Big Data Classification", "comments": "15 pages, 8 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Selection (FS) plays an important role in learning and classification\ntasks. The object of FS is to select the relevant and non-redundant features.\nConsidering the huge amount number of features in real-world applications, FS\nmethods using batch learning technique can't resolve big data problem\nespecially when data arrive sequentially. In this paper, we propose an online\nfeature selection system which resolves this problem. More specifically, we\ntreat the problem of online supervised feature selection for binary\nclassification as a decision-making problem. A philosophical vision to this\nproblem leads to a hybridization between two important domains: feature\nselection using online learning technique (OFS) and automated negotiation (AN).\nThe proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation\nbased Online Feature Selection) uses two levels of decision. In the first\nlevel, from n learners (or OFS methods), we decide which are the k trustful\nones (with high confidence or trust value). These elected k learners will\nparticipate in the second level. In this level, we integrate our proposed\nMultilateral Automated Negotiation based OFS (MANOFS) method to decide finally\nwhich is the best solution or which are relevant features. We show that MOANOFS\nsystem is applicable to different domains successfully and achieves high\naccuracy with several real-world applications.\n  Index Terms: Feature selection, online learning, multi-objective automated\nnegotiation, trust, classification, big data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:41:30 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 15:19:17 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["BenSaid", "Fatma", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1810.04927", "submitter": "Xuesong Niu", "authors": "Xuesong Niu, Hu Han, Shiguang Shan, Xilin Chen", "title": "VIPL-HR: A Multi-modal Database for Pulse Estimation from\n  Less-constrained Face Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart rate (HR) is an important physiological signal that reflects the\nphysical and emotional activities of humans. Traditional HR measurements are\nmainly based on contact monitors, which are inconvenient and may cause\ndiscomfort for the subjects. Recently, methods have been proposed for remote HR\nestimation from face videos. However, most of the existing methods focus on\nwell-controlled scenarios, their generalization ability into less-constrained\nscenarios are not known. At the same time, lacking large-scale databases has\nlimited the use of deep representation learning methods in remote HR\nestimation. In this paper, we introduce a large-scale multi-modal HR database\n(named as VIPL-HR), which contains 2,378 visible light videos (VIS) and 752\nnear-infrared (NIR) videos of 107 subjects. Our VIPL-HR database also contains\nvarious variations such as head movements, illumination variations, and\nacquisition device changes. We also learn a deep HR estimator (named as\nRhythmNet) with the proposed spatial-temporal representation, which achieves\npromising results on both the public-domain and our VIPL-HR HR estimation\ndatabases. We would like to put the VIPL-HR database into the public domain.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 09:32:27 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 04:57:32 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Niu", "Xuesong", ""], ["Han", "Hu", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1810.04937", "submitter": "Hafez Farazi", "authors": "Niloofar Azizi, Hafez Farazi, and Sven Behnke", "title": "Location Dependency in Video Prediction", "comments": "International Conference on Artificial Neural Networks. Springer,\n  Cham, 2018", "journal-ref": "International Conference on Artificial Neural Networks. Springer,\n  Cham, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are used to address many computer vision\nproblems, including video prediction. The task of video prediction requires\nanalyzing the video frames, temporally and spatially, and constructing a model\nof how the environment evolves. Convolutional neural networks are spatially\ninvariant, though, which prevents them from modeling location-dependent\npatterns. In this work, the authors propose location-biased convolutional\nlayers to overcome this limitation. The effectiveness of location bias is\nevaluated on two architectures: Video Ladder Network (VLN) and Convolutional\nredictive Gating Pyramid (Conv-PGP). The results indicate that encoding\nlocation-dependent features is crucial for the task of video prediction. Our\nproposed methods significantly outperform spatially invariant models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 10:01:00 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 12:11:00 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Azizi", "Niloofar", ""], ["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.04941", "submitter": "Hafez Farazi", "authors": "Hafez Farazi and Sven Behnke", "title": "Online Visual Robot Tracking and Identification using Deep LSTM Networks", "comments": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), Vancouver, Canada, 2017. IROS RoboCup Best Paper Award", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Vancouver, Canada, 2017", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative robots working on a common task are necessary for many\napplications. One of the challenges for achieving collaboration in a team of\nrobots is mutual tracking and identification. We present a novel pipeline for\nonline visionbased detection, tracking and identification of robots with a\nknown and identical appearance. Our method runs in realtime on the limited\nhardware of the observer robot. Unlike previous works addressing robot tracking\nand identification, we use a data-driven approach based on recurrent neural\nnetworks to learn relations between sequential inputs and outputs. We formulate\nthe data association problem as multiple classification problems. A deep LSTM\nnetwork was trained on a simulated dataset and fine-tuned on small set of real\ndata. Experiments on two challenging datasets, one synthetic and one real,\nwhich include long-term occlusions, show promising results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 10:20:52 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 12:04:43 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.04954", "submitter": "He Wang", "authors": "He Wang and Carol O'Sullivan", "title": "Globally Continuous and Non-Markovian Activity Analysis from Videos", "comments": "Preprint of our ECCV 2016 spotlight paper", "journal-ref": "Wang H., O'Sullivan C. (2016) Globally Continuous and\n  Non-Markovian Crowd Activity Analysis from Videos. In ECCV 2016. Lecture\n  Notes in Computer Science, vol 9909. Springer", "doi": "10.1007/978-3-319-46454-1_32", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically recognizing activities in video is a classic problem in vision\nand helps to understand behaviors, describe scenes and detect anomalies. We\npropose an unsupervised method for such purposes. Given video data, we discover\nrecurring activity patterns that appear, peak, wane and disappear over time. By\nusing non-parametric Bayesian methods, we learn coupled spatial and temporal\npatterns with minimum prior knowledge. To model the temporal changes of\npatterns, previous works compute Markovian progressions or locally continuous\nmotifs whereas we model time in a globally continuous and non-Markovian way.\nVisually, the patterns depict flows of major activities. Temporally, each\npattern has its own unique appearance-disappearance cycles. To compute compact\npattern representations, we also propose a hybrid sampling method. By combining\nthese patterns with detailed environment information, we interpret the\nsemantics of activities and report anomalies. Also, our method fits data better\nand detects anomalies that were difficult to detect previously.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 11:15:17 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Wang", "He", ""], ["O'Sullivan", "Carol", ""]]}, {"id": "1810.04991", "submitter": "Xiaoming Yu", "authors": "Xiaoming Yu, Xing Cai, Zhenqiang Ying, Thomas Li, and Ge Li", "title": "SingleGAN: Image-to-Image Translation by a Single-Generator Network\n  using Multiple Generative Adversarial Learning", "comments": "Accepted in ACCV 2018. Code is available at\n  https://github.com/Xiaoming-Yu/SingleGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation is a burgeoning field in computer vision where the goal is\nto learn the mapping between an input image and an output image. However, most\nrecent methods require multiple generators for modeling different domain\nmappings, which are inefficient and ineffective on some multi-domain image\ntranslation tasks. In this paper, we propose a novel method, SingleGAN, to\nperform multi-domain image-to-image translations with a single generator. We\nintroduce the domain code to explicitly control the different generative tasks\nand integrate multiple optimization goals to ensure the translation.\nExperimental results on several unpaired datasets show superior performance of\nour model in translation between two domains. Besides, we explore variants of\nSingleGAN for different tasks, including one-to-many domain translation,\nmany-to-many domain translation and one-to-one domain translation with\nmultimodality. The extended experiments show the universality and extensibility\nof our model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 13:01:40 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 15:46:37 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yu", "Xiaoming", ""], ["Cai", "Xing", ""], ["Ying", "Zhenqiang", ""], ["Li", "Thomas", ""], ["Li", "Ge", ""]]}, {"id": "1810.05016", "submitter": "Roberto J. L\\'opez-Sastre", "authors": "Carlos Herranz-Perdiguero and Roberto J. L\\'opez-Sastre", "title": "ISA$^2$: Intelligent Speed Adaptation from Appearance", "comments": "IROS 2018 Workshop: 10th Planning, Perception and Navigation for\n  Intelligent Vehicles (PPNIV'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new problem named Intelligent Speed Adaptation\nfrom Appearance (ISA$^2$). Technically, the goal of an ISA$^2$ model is to\npredict for a given image of a driving scenario the proper speed of the\nvehicle. Note this problem is different from predicting the actual speed of the\nvehicle. It defines a novel regression problem where the appearance information\nhas to be directly mapped to get a prediction for the speed at which the\nvehicle should go, taking into account the traffic situation. First, we release\na novel dataset for the new problem, where multiple driving video sequences,\nwith the annotated adequate speed per frame, are provided. We then introduce\ntwo deep learning based ISA$^2$ models, which are trained to perform the final\nregression of the proper speed given a test image. We end with a thorough\nexperimental validation where the results show the level of difficulty of the\nproposed task. The dataset and the proposed models will all be made publicly\navailable to encourage much needed further research on this problem.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 13:43:36 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Herranz-Perdiguero", "Carlos", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1810.05017", "submitter": "Tom Paine", "authors": "Tom Le Paine, Sergio G\\'omez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf\n  Aytar, Tobias Pfaff, Matt W. Hoffman, Gabriel Barth-Maron, Serkan Cabi, David\n  Budden, Nando de Freitas", "title": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are experts at high-fidelity imitation -- closely mimicking a\ndemonstration, often in one attempt. Humans use this ability to quickly solve a\ntask instance, and to bootstrap learning of new tasks. Achieving these\nabilities in autonomous agents is an open problem. In this paper, we introduce\nan off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn\nboth (i) policies for high-fidelity one-shot imitation of diverse novel skills,\nand (ii) policies that enable the agent to solve tasks more efficiently than\nthe demonstrators. MetaMimic relies on the principle of storing all experiences\nin a memory and replaying these to learn massive deep neural network policies\nby off-policy RL. This paper introduces, to the best of our knowledge, the\nlargest existing neural networks for deep RL and shows that larger networks\nwith normalization are needed to achieve one-shot high-fidelity imitation on a\nchallenging manipulation task. The results also show that both types of policy\ncan be learned from vision, in spite of the task rewards being sparse, and\nwithout access to demonstrator actions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 13:46:18 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Paine", "Tom Le", ""], ["Colmenarejo", "Sergio G\u00f3mez", ""], ["Wang", "Ziyu", ""], ["Reed", "Scott", ""], ["Aytar", "Yusuf", ""], ["Pfaff", "Tobias", ""], ["Hoffman", "Matt W.", ""], ["Barth-Maron", "Gabriel", ""], ["Cabi", "Serkan", ""], ["Budden", "David", ""], ["de Freitas", "Nando", ""]]}, {"id": "1810.05052", "submitter": "Chunwei Tian", "authors": "Chunwei Tian, Yong Xu, Lunke Fei and Ke Yan", "title": "Deep Learning for Image Denoising: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the proposal of big data analysis and Graphic Processing Unit (GPU),\nthe deep learning technology has received a great deal of attention and has\nbeen widely applied in the field of imaging processing. In this paper, we have\nan aim to completely review and summarize the deep learning technologies for\nimage denoising proposed in recent years. Morever, we systematically analyze\nthe conventional machine learning methods for image denoising. Finally, we\npoint out some research directions for the deep learning technologies in image\ndenoising.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 14:43:43 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Tian", "Chunwei", ""], ["Xu", "Yong", ""], ["Fei", "Lunke", ""], ["Yan", "Ke", ""]]}, {"id": "1810.05077", "submitter": "Abdullah Alchihabi", "authors": "Abdullah Alchihabi, Omer Ekmekci, Baran B. Kivilcim, Sharlene D.\n  Newman, Fatos T. Yarman Vural", "title": "On the Brain Networks of Complex Problem Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex problem solving is a high level cognitive process which has been\nthoroughly studied over the last decade. The Tower of London (TOL) is a task\nthat has been widely used to study problem-solving. In this study, we aim to\nexplore the underlying cognitive network dynamics among anatomical regions of\ncomplex problem solving and its sub-phases, namely planning and execution. A\nnew brain network construction model establishing dynamic functional brain\nnetworks using fMRI is proposed. The first step of the model is a preprocessing\npipeline that manages to decrease the spatial redundancy while increasing the\ntemporal resolution of the fMRI recordings. Then, dynamic brain networks are\nestimated using artificial neural networks. The network properties of the\nestimated brain networks are studied in order to identify regions of interest,\nsuch as hubs and subgroups of densely connected brain regions. The major\nsimilarities and dissimilarities of the network structure of planning and\nexecution phases are highlighted. Our findings show the hubs and clusters of\ndensely interconnected regions during both subtasks. It is observed that there\nare more hubs during the planning phase compared to the execution phase, and\nthe clusters are more strongly connected during planning compared to execution.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 09:22:21 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Alchihabi", "Abdullah", ""], ["Ekmekci", "Omer", ""], ["Kivilcim", "Baran B.", ""], ["Newman", "Sharlene D.", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1810.05080", "submitter": "Vandit Gajjar J", "authors": "Hiren Galiyawala, Kenil Shah, Vandit Gajjar, Mehul S. Raval", "title": "Person Retrieval in Surveillance Video using Height, Color and Gender", "comments": "6 Pages, 6 Figures, Accepted to Semantic Person Retrieval in\n  Surveillance Using Soft Biometrics challenge in Conjunction with AVSS-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A person is commonly described by attributes like height, build, cloth color,\ncloth type, and gender. Such attributes are known as soft biometrics. They\nbridge the semantic gap between human description and person retrieval in\nsurveillance video. The paper proposes a deep learning-based linear filtering\napproach for person retrieval using height, cloth color, and gender. The\nproposed approach uses Mask R-CNN for pixel-wise person segmentation. It\nremoves background clutter and provides precise boundary around the person.\nColor and gender models are fine-tuned using AlexNet and the algorithm is\ntested on SoftBioSearch dataset. It achieves good accuracy for person retrieval\nusing the semantic query in challenging conditions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 07:21:23 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Galiyawala", "Hiren", ""], ["Shah", "Kenil", ""], ["Gajjar", "Vandit", ""], ["Raval", "Mehul S.", ""]]}, {"id": "1810.05107", "submitter": "Sukhad Anand Mr", "authors": "Sukhad Anand, Saksham Gupta, Vaibhav Darbari, Shivam Kohli", "title": "Crack-pot: Autonomous Road Crack and Pothole Detection", "comments": "Submitted at DICTA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of self-driving cars and autonomous robots, it is imperative\nto detect road impairments like cracks and potholes and to perform necessary\nevading maneuvers to ensure fluid journey for on-board passengers or equipment.\nWe propose a fully autonomous robust real-time road crack and pothole detection\nalgorithm which can be deployed on any GPU based conventional processing boards\nwith an associated camera. The approach is based on a deep neural net\narchitecture which detects cracks and potholes using texture and spatial\nfeatures. We also propose pre-processing methods which ensure real-time\nperformance. The novelty of the approach lies in using texture- based features\nto differentiate between crack surfaces and sound roads. The approach performs\nwell in large viewpoint changes, background noise, shadows, and occlusion. The\nefficacy of the system is shown on standard road crack datasets.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 18:46:27 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Anand", "Sukhad", ""], ["Gupta", "Saksham", ""], ["Darbari", "Vaibhav", ""], ["Kohli", "Shivam", ""]]}, {"id": "1810.05162", "submitter": "Ruizhi Deng", "authors": "Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, and Dawn\n  Song", "title": "Characterizing Adversarial Examples Based on Spatial Consistency\n  Information for Semantic Segmentation", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been widely applied in various recognition\ntasks. However, recently DNNs have been shown to be vulnerable against\nadversarial examples, which can mislead DNNs to make arbitrary incorrect\npredictions. While adversarial examples are well studied in classification\ntasks, other learning problems may have different properties. For instance,\nsemantic segmentation requires additional components such as dilated\nconvolutions and multiscale processing. In this paper, we aim to characterize\nadversarial examples based on spatial context information in semantic\nsegmentation. We observe that spatial consistency information can be\npotentially leveraged to detect adversarial examples robustly even when a\nstrong adaptive attacker has access to the model and detection strategies. We\nalso show that adversarial examples based on attacks considered within the\npaper barely transfer among models, even though transferability is common in\nclassification. Our observations shed new light on developing adversarial\nattacks and defenses to better understand the vulnerabilities of DNNs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:03:44 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Xiao", "Chaowei", ""], ["Deng", "Ruizhi", ""], ["Li", "Bo", ""], ["Yu", "Fisher", ""], ["Liu", "Mingyan", ""], ["Song", "Dawn", ""]]}, {"id": "1810.05186", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, James Cheng, Yuanyuan Liu, Zhi-Quan Luo, Zhouchen Lin", "title": "Bilinear Factor Matrix Norm Minimization for Robust PCA: Algorithms and\n  Applications", "comments": "29 pages, 19 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  40(9): 2066-2080, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heavy-tailed distributions of corrupted outliers and singular values of\nall channels in low-level vision have proven effective priors for many\napplications such as background modeling, photometric stereo and image\nalignment. And they can be well modeled by a hyper-Laplacian. However, the use\nof such distributions generally leads to challenging non-convex, non-smooth and\nnon-Lipschitz problems, and makes existing algorithms very slow for large-scale\napplications. Together with the analytic solutions to lp-norm minimization with\ntwo specific values of p, i.e., p=1/2 and p=2/3, we propose two novel bilinear\nfactor matrix norm minimization models for robust principal component analysis.\nWe first define the double nuclear norm and Frobenius/nuclear hybrid norm\npenalties, and then prove that they are in essence the Schatten-1/2 and 2/3\nquasi-norms, respectively, which lead to much more tractable and scalable\nLipschitz optimization problems. Our experimental analysis shows that both our\nmethods yield more accurate solutions than original Schatten quasi-norm\nminimization, even when the number of observations is very limited. Finally, we\napply our penalties to various low-level vision problems, e.g., text removal,\nmoving object detection, image alignment and inpainting, and show that our\nmethods usually outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 18:06:27 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Shang", "Fanhua", ""], ["Cheng", "James", ""], ["Liu", "Yuanyuan", ""], ["Luo", "Zhi-Quan", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1810.05206", "submitter": "Dawei Yang", "authors": "Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, Mingyan Liu", "title": "MeshAdv: Adversarial Meshes for Visual Recognition", "comments": "Published in IEEE CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly expressive models such as deep neural networks (DNNs) have been widely\napplied to various applications. However, recent studies show that DNNs are\nvulnerable to adversarial examples, which are carefully crafted inputs aiming\nto mislead the predictions. Currently, the majority of these studies have\nfocused on perturbation added to image pixels, while such manipulation is not\nphysically realistic. Some works have tried to overcome this limitation by\nattaching printable 2D patches or painting patterns onto surfaces, but can be\npotentially defended because 3D shape features are intact. In this paper, we\npropose meshAdv to generate \"adversarial 3D meshes\" from objects that have rich\nshape features but minimal textural variation. To manipulate the shape or\ntexture of the objects, we make use of a differentiable renderer to compute\naccurate shading on the shape and propagate the gradient. Extensive experiments\nshow that the generated 3D meshes are effective in attacking both classifiers\nand object detectors. We evaluate the attack under different viewpoints. In\naddition, we design a pipeline to perform black-box attack on a photorealistic\nrenderer with unknown rendering parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 19:01:10 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 19:43:54 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Xiao", "Chaowei", ""], ["Yang", "Dawei", ""], ["Li", "Bo", ""], ["Deng", "Jia", ""], ["Liu", "Mingyan", ""]]}, {"id": "1810.05220", "submitter": "Saad Nadeem", "authors": "Shreeraj Jadhav, Saad Nadeem and Arie Kaufman", "title": "FeatureLego: Volume Exploration Using Exhaustive Clustering of\n  Super-Voxels", "comments": "IEEE Transactions on Visualization and Computer Graphics, 2018 (12\n  pages, 11 figures). Supplementary video demonstrating FeatureLego can be\n  found here: https://www.youtube.com/watch?v=y_a3VnACXfE", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (Volume:\n  25, Issue: 9, Pages: 2725 - 2737, Sept. 1 2019)", "doi": "10.1109/TVCG.2018.2856744", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a volume exploration framework, FeatureLego, that uses a novel\nvoxel clustering approach for efficient selection of semantic features. We\npartition the input volume into a set of compact super-voxels that represent\nthe finest selection granularity. We then perform an exhaustive clustering of\nthese super-voxels using a graph-based clustering method. Unlike the prevalent\nbrute-force parameter sampling approaches, we propose an efficient algorithm to\nperform this exhaustive clustering. By computing an exhaustive set of clusters,\nwe aim to capture as many boundaries as possible and ensure that the user has\nsufficient options for efficiently selecting semantically relevant features.\nFurthermore, we merge all the computed clusters into a single tree of\nmeta-clusters that can be used for hierarchical exploration. We implement an\nintuitive user-interface to interactively explore volumes using our clustering\napproach. Finally, we show the effectiveness of our framework on multiple\nreal-world datasets of different modalities.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 19:40:25 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 12:30:23 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jadhav", "Shreeraj", ""], ["Nadeem", "Saad", ""], ["Kaufman", "Arie", ""]]}, {"id": "1810.05270", "submitter": "Zhuang Liu", "authors": "Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell", "title": "Rethinking the Value of Network Pruning", "comments": "ICLR 2019. Significant revisions from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is widely used for reducing the heavy inference cost of deep\nmodels in low-resource settings. A typical pruning algorithm is a three-stage\npipeline, i.e., training (a large model), pruning and fine-tuning. During\npruning, according to a certain criterion, redundant weights are pruned and\nimportant weights are kept to best preserve the accuracy. In this work, we make\nseveral surprising observations which contradict common beliefs. For all\nstate-of-the-art structured pruning algorithms we examined, fine-tuning a\npruned model only gives comparable or worse performance than training that\nmodel with randomly initialized weights. For pruning algorithms which assume a\npredefined target network architecture, one can get rid of the full pipeline\nand directly train the target network from scratch. Our observations are\nconsistent for multiple network architectures, datasets, and tasks, which imply\nthat: 1) training a large, over-parameterized model is often not necessary to\nobtain an efficient final model, 2) learned \"important\" weights of the large\nmodel are typically not useful for the small pruned model, 3) the pruned\narchitecture itself, rather than a set of inherited \"important\" weights, is\nmore crucial to the efficiency in the final model, which suggests that in some\ncases pruning can be useful as an architecture search paradigm. Our results\nsuggest the need for more careful baseline evaluations in future research on\nstructured pruning methods. We also compare with the \"Lottery Ticket\nHypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate,\nthe \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not\nbring improvement over random initialization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 22:15:28 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 05:58:11 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Liu", "Zhuang", ""], ["Sun", "Mingjie", ""], ["Zhou", "Tinghui", ""], ["Huang", "Gao", ""], ["Darrell", "Trevor", ""]]}, {"id": "1810.05331", "submitter": "Xitong Gao", "authors": "Xitong Gao and Yiren Zhao and {\\L}ukasz Dudziak and Robert Mullins and\n  Cheng-zhong Xu", "title": "Dynamic Channel Pruning: Feature Boosting and Suppression", "comments": "14 pages, 5 figures, 4 tables, published as a conference paper at\n  ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Making deep convolutional neural networks more accurate typically comes at\nthe cost of increased computational and memory resources. In this paper, we\nreduce this cost by exploiting the fact that the importance of features\ncomputed by convolutional layers is highly input-dependent, and propose feature\nboosting and suppression (FBS), a new method to predictively amplify salient\nconvolutional channels and skip unimportant ones at run-time. FBS introduces\nsmall auxiliary connections to existing convolutional layers. In contrast to\nchannel pruning methods which permanently remove channels, it preserves the\nfull network structures and accelerates convolution by dynamically skipping\nunimportant input and output channels. FBS-augmented networks are trained with\nconventional stochastic gradient descent, making it readily available for many\nstate-of-the-art CNNs. We compare FBS to a range of existing channel pruning\nand dynamic execution schemes and demonstrate large improvements on ImageNet\nclassification. Experiments show that FBS can respectively provide $5\\times$\nand $2\\times$ savings in compute on VGG-16 and ResNet-18, both with less than\n$0.6\\%$ top-5 accuracy loss.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 03:00:59 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 05:25:48 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gao", "Xitong", ""], ["Zhao", "Yiren", ""], ["Dudziak", "\u0141ukasz", ""], ["Mullins", "Robert", ""], ["Xu", "Cheng-zhong", ""]]}, {"id": "1810.05340", "submitter": "Zhong Li", "authors": "Zhong Li, Minye Wu, Wangyiteng Zhou, Jingyi Yu", "title": "4D Human Body Correspondences from Panoramic Depth Maps", "comments": "10 pages, 12 figures, CVPR 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of affordable 3D full body reconstruction systems has given\nrise to free-viewpoint video (FVV) of human shapes. Most existing solutions\nproduce temporally uncorrelated point clouds or meshes with unknown\npoint/vertex correspondences. Individually compressing each frame is\nineffective and still yields to ultra-large data sizes. We present an\nend-to-end deep learning scheme to establish dense shape correspondences and\nsubsequently compress the data. Our approach uses sparse set of \"panoramic\"\ndepth maps or PDMs, each emulating an inward-viewing concentric mosaics. We\nthen develop a learning-based technique to learn pixel-wise feature descriptors\non PDMs. The results are fed into an autoencoder-based network for compression.\nComprehensive experiments demonstrate our solution is robust and effective on\nboth public and our newly captured datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 03:33:20 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Li", "Zhong", ""], ["Wu", "Minye", ""], ["Zhou", "Wangyiteng", ""], ["Yu", "Jingyi", ""]]}, {"id": "1810.05358", "submitter": "Seungjoon Yang", "authors": "Hyunjoong Cho, Jinhyeok Jang, Chanhyeok Lee, and Seungjoon Yang", "title": "Efficient architecture for deep neural networks with heterogeneous\n  sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a neural network that consists of nodes with heterogeneous\nsensitivity. Each node in a network is assigned a variable that determines the\nsensitivity with which it learns to perform a given task. The network is\ntrained by a constrained optimization that maximizes the sparsity of the\nsensitivity variables while ensuring the network's performance. As a result,\nthe network learns to perform a given task using only a small number of\nsensitive nodes. Insensitive nodes, the nodes with zero sensitivity, can be\nremoved from a trained network to obtain a computationally efficient network.\nRemoving zero-sensitivity nodes has no effect on the network's performance\nbecause the network has already been trained to perform the task without them.\nThe regularization parameter used to solve the optimization problem is found\nsimultaneously during the training of networks. To validate our approach, we\ndesign networks with computationally efficient architectures for various tasks\nsuch as autoregression, object recognition, facial expression recognition, and\nobject detection using various datasets. In our experiments, the networks\ndesigned by the proposed method provide the same or higher performance but with\nfar less computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 05:09:12 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 02:09:06 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 02:40:15 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Cho", "Hyunjoong", ""], ["Jang", "Jinhyeok", ""], ["Lee", "Chanhyeok", ""], ["Yang", "Seungjoon", ""]]}, {"id": "1810.05361", "submitter": "Hadi Kazemi", "authors": "Hadi Kazemi, Fariborz Taherkhani, Nasser M. Nasrabadi", "title": "Unsupervised Facial Geometry Learning for Sketch to Photo Synthesis", "comments": "Published as a conference paper in BIOSIG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face sketch-photo synthesis is a critical application in law enforcement and\ndigital entertainment industry where the goal is to learn the mapping between a\nface sketch image and its corresponding photo-realistic image. However, the\nlimited number of paired sketch-photo training data usually prevents the\ncurrent frameworks to learn a robust mapping between the geometry of sketches\nand their matching photo-realistic images. Consequently, in this work, we\npresent an approach for learning to synthesize a photo-realistic image from a\nface sketch in an unsupervised fashion. In contrast to current unsupervised\nimage-to-image translation techniques, our framework leverages a novel\nperceptual discriminator to learn the geometry of human face. Learning facial\nprior information empowers the network to remove the geometrical artifacts in\nthe face sketch. We demonstrate that a simultaneous optimization of the face\nphoto generator network, employing the proposed perceptual discriminator in\ncombination with a texture-wise discriminator, results in a significant\nimprovement in quality and recognition rate of the synthesized photos. We\nevaluate the proposed network by conducting extensive experiments on multiple\nbaseline sketch-photo datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 05:22:56 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Kazemi", "Hadi", ""], ["Taherkhani", "Fariborz", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1810.05367", "submitter": "Peng Gao", "authors": "Ke Song, Chun Yuan, Peng Gao, Yunxu Sun", "title": "FPGA-based Acceleration System for Visual Tracking", "comments": "Accepted by IEEE 14th International Conference on Solid-State and\n  Integrated Circuit Technology (ICSICT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is one of the most important application areas of computer\nvision. At present, most algorithms are mainly implemented on PCs, and it is\ndifficult to ensure real-time performance when applied in the real scenario. In\norder to improve the tracking speed and reduce the overall power consumption of\nvisual tracking, this paper proposes a real-time visual tracking algorithm\nbased on DSST(Discriminative Scale Space Tracking) approach. We implement a\nhardware system on Xilinx XC7K325T FPGA platform based on our proposed visual\ntracking algorithm. Our hardware system can run at more than 153 frames per\nsecond. In order to reduce the resource occupation, our system adopts the batch\nprocessing method in the feature extraction module. In the filter processing\nmodule, the FFT IP core is time-division multiplexed. Therefore, our hardware\nsystem utilizes LUTs and storage blocks of 33% and 40%, respectively. Test\nresults show that the proposed visual tracking hardware system has excellent\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 05:46:05 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 00:31:32 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Song", "Ke", ""], ["Yuan", "Chun", ""], ["Gao", "Peng", ""], ["Sun", "Yunxu", ""]]}, {"id": "1810.05394", "submitter": "Meenakshi Sarkar", "authors": "Meenakshi Sarkar, Debasish Ghose", "title": "Sequential Learning of Movement Prediction in Dynamic Environments using\n  LSTM Autoencoder", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting movement of objects while the action of learning agent interacts\nwith the dynamics of the scene still remains a key challenge in robotics. We\npropose a multi-layer Long Short Term Memory (LSTM) autoendocer network that\npredicts future frames for a robot navigating in a dynamic environment with\nmoving obstacles. The autoencoder network is composed of a state and action\nconditioned decoder network that reconstructs the future frames of video,\nconditioned on the action taken by the agent. The input image frames are first\ntransformed into low dimensional feature vectors with a pre-trained encoder\nnetwork and then reconstructed with the LSTM autoencoder network to generate\nthe future frames. A virtual environment, based on the OpenAi-Gym framework for\nrobotics, is used to gather training data and test the proposed network. The\ninitial experiments show promising results indicating that these predicted\nframes can be used by an appropriate reinforcement learning framework in future\nto navigate around dynamic obstacles.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:11:13 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Sarkar", "Meenakshi", ""], ["Ghose", "Debasish", ""]]}, {"id": "1810.05396", "submitter": "Xu Cao", "authors": "Xu Cao and Katashi Nagao", "title": "Point Cloud Colorization Based on Densely Annotated 3D Shape Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces DensePoint, a densely sampled and annotated point cloud\ndataset containing over 10,000 single objects across 16 categories, by merging\ndifferent kind of information from two existing datasets. Each point cloud in\nDensePoint contains 40,000 points, and each point is associated with two sorts\nof information: RGB value and part annotation. In addition, we propose a method\nfor point cloud colorization by utilizing Generative Adversarial Networks\n(GANs). The network makes it possible to generate colours for point clouds of\nsingle objects by only giving the point cloud itself. Experiments on DensePoint\nshow that there exist clear boundaries in point clouds between different parts\nof an object, suggesting that the proposed network is able to generate\nreasonably good colours. Our dataset is publicly available on the project page.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:18:24 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Cao", "Xu", ""], ["Nagao", "Katashi", ""]]}, {"id": "1810.05399", "submitter": "Xiaodong Kuang", "authors": "Xiaodong Kuang, Xiubao Sui, Chengwei Liu, Yuan Liu, Qian Chen, Guohua\n  Gu", "title": "Thermal Infrared Colorization via Conditional Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming a thermal infrared image into a realistic RGB image is a\nchallenging task. In this paper we propose a deep learning method to bridge\nthis gap. We propose learning the transformation mapping using a coarse-to-fine\ngenerator that preserves the details. Since the standard mean squared loss\ncannot penalize the distance between colorized and ground truth images well, we\npropose a composite loss function that combines content, adversarial,\nperceptual and total variation losses. The content loss is used to recover\nglobal image information while the latter three losses are used to synthesize\nlocal realistic textures. Quantitative and qualitative experiments demonstrate\nthat our approach significantly outperforms existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:21:04 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 03:11:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Kuang", "Xiaodong", ""], ["Sui", "Xiubao", ""], ["Liu", "Chengwei", ""], ["Liu", "Yuan", ""], ["Chen", "Qian", ""], ["Gu", "Guohua", ""]]}, {"id": "1810.05401", "submitter": "Andreas Maier", "authors": "Andreas Maier, Christopher Syben, Tobias Lasser, Christian Riess", "title": "A Gentle Introduction to Deep Learning in Medical Image Processing", "comments": "Accepted by Journal of Medical Physics; Final Version after review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper tries to give a gentle introduction to deep learning in medical\nimage processing, proceeding from theoretical foundations to applications. We\nfirst discuss general reasons for the popularity of deep learning, including\nseveral major breakthroughs in computer science. Next, we start reviewing the\nfundamental basics of the perceptron and neural networks, along with some\nfundamental theory that is often omitted. Doing so allows us to understand the\nreasons for the rise of deep learning in many application domains. Obviously\nmedical image processing is one of these areas which has been largely affected\nby this rapid progress, in particular in image detection and recognition, image\nsegmentation, image registration, and computer-aided diagnosis. There are also\nrecent trends in physical simulation, modelling, and reconstruction that have\nled to astonishing results. Yet, some of these approaches neglect prior\nknowledge and hence bear the risk of producing implausible results. These\napparent weaknesses highlight current limitations of deep learning. However, we\nalso briefly discuss promising approaches that might be able to resolve these\nproblems in the future.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:27:53 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 13:37:23 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Maier", "Andreas", ""], ["Syben", "Christopher", ""], ["Lasser", "Tobias", ""], ["Riess", "Christian", ""]]}, {"id": "1810.05420", "submitter": "Florian Jug", "authors": "Tim-Oliver Buchholz, Mareike Jordan, Gaia Pigino, Florian Jug", "title": "Cryo-CARE: Content-Aware Image Restoration for Cryo-Transmission\n  Electron Microscopy Data", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. This version fixed flipped graph labels in Figure 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple approaches to use deep learning for image restoration have recently\nbeen proposed. Training such approaches requires well registered pairs of high\nand low quality images. While this is easily achievable for many imaging\nmodalities, e.g. fluorescence light microscopy, for others it is not.\nCryo-transmission electron microscopy (cryo-TEM) could profoundly benefit from\nimproved denoising methods, unfortunately it is one of the latter. Here we show\nhow recent advances in network training for image restoration tasks, i.e.\ndenoising, can be applied to cryo-TEM data. We describe our proposed method and\nshow how it can be applied to single cryo-TEM projections and whole\ncryo-tomographic image volumes. Our proposed restoration method dramatically\nincreases contrast in cryo-TEM images, which improves the interpretability of\nthe acquired data. Furthermore we show that automated downstream processing on\nrestored image data, demonstrated on a dense segmentation task, leads to\nimproved results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 09:15:06 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 18:49:50 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Buchholz", "Tim-Oliver", ""], ["Jordan", "Mareike", ""], ["Pigino", "Gaia", ""], ["Jug", "Florian", ""]]}, {"id": "1810.05423", "submitter": "Ismail Elezi", "authors": "Ismail Elezi, Lukas Tuggener, Marcello Pelillo, Thilo Stadelmann", "title": "DeepScores and Deep Watershed Detection: current state and open issues", "comments": "Published on WORMS workshop (ISMIR affiliated workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an overview of our current Optical Music Recognition (OMR)\nresearch. We recently released the OMR dataset \\emph{DeepScores} as well as the\nobject detection method \\emph{Deep Watershed Detector}. We are currently taking\nsome additional steps to improve both of them. Here we summarize current and\nfuture efforts, aimed at improving usefulness on real-world task and tackling\nextreme class imbalance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 09:16:42 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Elezi", "Ismail", ""], ["Tuggener", "Lukas", ""], ["Pelillo", "Marcello", ""], ["Stadelmann", "Thilo", ""]]}, {"id": "1810.05424", "submitter": "Alessio Tonioni", "authors": "Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia and Luigi\n  Di Stefano", "title": "Real-time self-adaptive deep stereo", "comments": "Accepted at CVPR2019 as oral presentation. Code Available\n  https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks trained end-to-end are the\nstate-of-the-art methods to regress dense disparity maps from stereo pairs.\nThese models, however, suffer from a notable decrease in accuracy when exposed\nto scenarios significantly different from the training set, e.g., real vs\nsynthetic images, etc.). We argue that it is extremely unlikely to gather\nenough samples to achieve effective training/tuning in any target domain, thus\nmaking this setup impractical for many applications. Instead, we propose to\nperform unsupervised and continuous online adaptation of a deep stereo network,\nwhich allows for preserving its accuracy in any environment. However, this\nstrategy is extremely computationally demanding and thus prevents real-time\ninference. We address this issue introducing a new lightweight, yet effective,\ndeep stereo architecture, Modularly ADaptive Network (MADNet) and developing a\nModular ADaptation (MAD) algorithm, which independently trains sub-portions of\nthe network. By deploying MADNet together with MAD we introduce the first\nreal-time self-adaptive deep stereo system enabling competitive performance on\nheterogeneous datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 09:17:53 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 09:57:29 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Tonioni", "Alessio", ""], ["Tosi", "Fabio", ""], ["Poggi", "Matteo", ""], ["Mattoccia", "Stefano", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1810.05438", "submitter": "Dong Gong", "authors": "Dong Gong, Mingkui Tan, Qinfeng Shi, Anton van den Hengel, Yanning\n  Zhang", "title": "MPTV: Matching Pursuit Based Total Variation Minimization for Image\n  Deconvolution", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2875352", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total variation (TV) regularization has proven effective for a range of\ncomputer vision tasks through its preferential weighting of sharp image edges.\nExisting TV-based methods, however, often suffer from the over-smoothing issue\nand solution bias caused by the homogeneous penalization. In this paper, we\nconsider addressing these issues by applying inhomogeneous regularization on\ndifferent image components. We formulate the inhomogeneous TV minimization\nproblem as a convex quadratic constrained linear programming problem. Relying\non this new model, we propose a matching pursuit based total variation\nminimization method (MPTV), specifically for image deconvolution. The proposed\nMPTV method is essentially a cutting-plane method, which iteratively activates\na subset of nonzero image gradients, and then solves a subproblem focusing on\nthose activated gradients only. Compared to existing methods, MPTV is less\nsensitive to the choice of the trade-off parameter between data fitting and\nregularization. Moreover, the inhomogeneity of MPTV alleviates the\nover-smoothing and ringing artifacts, and improves the robustness to errors in\nblur kernel. Extensive experiments on different tasks demonstrate the\nsuperiority of the proposed method over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 10:07:05 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Gong", "Dong", ""], ["Tan", "Mingkui", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Zhang", "Yanning", ""]]}, {"id": "1810.05444", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina", "title": "Cats or CAT scans: transfer learning from natural or medical image\n  source datasets?", "comments": "Accepted to Current Opinion in Biomedical Engineering", "journal-ref": null, "doi": "10.1016/j.cobme.2018.12.005", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a widely used strategy in medical image analysis.\nInstead of only training a network with a limited amount of data from the\ntarget task of interest, we can first train the network with other, potentially\nlarger source datasets, creating a more robust model. The source datasets do\nnot have to be related to the target task. For a classification task in lung CT\nimages, we could use both head CT images, or images of cats, as the source.\nWhile head CT images appear more similar to lung CT images, the number and\ndiversity of cat images might lead to a better model overall. In this survey we\nreview a number of papers that have performed similar comparisons. Although the\nanswer to which strategy is best seems to be \"it depends\", we discuss a number\nof research directions we need to take as a community, to gain more\nunderstanding of this topic.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 10:35:21 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 08:45:08 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Cheplygina", "Veronika", ""]]}, {"id": "1810.05456", "submitter": "Yonggen Ling", "authors": "Yonggen Ling and Linchao Bao and Zequn Jie and Fengming Zhu and Ziyang\n  Li and Shanmin Tang and Yongsheng Liu and Wei Liu and Tong Zhang", "title": "Modeling Varying Camera-IMU Time Offset in Optimization-Based\n  Visual-Inertial Odometry", "comments": "European Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining cameras and inertial measurement units (IMUs) has been proven\neffective in motion tracking, as these two sensing modalities offer\ncomplementary characteristics that are suitable for fusion. While most works\nfocus on global-shutter cameras and synchronized sensor measurements,\nconsumer-grade devices are mostly equipped with rolling-shutter cameras and\nsuffer from imperfect sensor synchronization. In this work, we propose a\nnonlinear optimization-based monocular visual inertial odometry (VIO) with\nvarying camera-IMU time offset modeled as an unknown variable. Our approach is\nable to handle the rolling-shutter effects and imperfect sensor synchronization\nin a unified way. Additionally, we introduce an efficient algorithm based on\ndynamic programming and red-black tree to speed up IMU integration over\nvariable-length time intervals during the optimization. An uncertainty-aware\ninitialization is also presented to launch the VIO robustly. Comparisons with\nstate-of-the-art methods on the Euroc dataset and mobile phone data are shown\nto validate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 11:35:27 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Ling", "Yonggen", ""], ["Bao", "Linchao", ""], ["Jie", "Zequn", ""], ["Zhu", "Fengming", ""], ["Li", "Ziyang", ""], ["Tang", "Shanmin", ""], ["Liu", "Yongsheng", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1810.05552", "submitter": "Yanting Pei", "authors": "Yanting Pei, Yaping Huang, Qi Zou, Hao Zang, Xingyuan Zhang, Song Wang", "title": "Effects of Image Degradations to CNN-based Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just like many other topics in computer vision, image classification has\nachieved significant progress recently by using deep-learning neural networks,\nespecially the Convolutional Neural Networks (CNN). Most of the existing works\nare focused on classifying very clear natural images, evidenced by the widely\nused image databases such as Caltech-256, PASCAL VOCs and ImageNet. However, in\nmany real applications, the acquired images may contain certain degradations\nthat lead to various kinds of blurring, noise, and distortions. One important\nand interesting problem is the effect of such degradations to the performance\nof CNN-based image classification. More specifically, we wonder whether\nimage-classification performance drops with each kind of degradation, whether\nthis drop can be avoided by including degraded images into training, and\nwhether existing computer vision algorithms that attempt to remove such\ndegradations can help improve the image-classification performance. In this\npaper, we empirically study this problem for four kinds of degraded images --\nhazy images, underwater images, motion-blurred images and fish-eye images. For\nthis study, we synthesize a large number of such degraded images by applying\nrespective physical models to the clear natural images and collect a new hazy\nimage dataset from the Internet. We expect this work can draw more interests\nfrom the community to study the classification of degraded images.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 14:38:52 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Pei", "Yanting", ""], ["Huang", "Yaping", ""], ["Zou", "Qi", ""], ["Zang", "Hao", ""], ["Zhang", "Xingyuan", ""], ["Wang", "Song", ""]]}, {"id": "1810.05591", "submitter": "Yongbin Sun", "authors": "Yongbin Sun, Yue Wang, Ziwei Liu, Joshua E. Siegel, Sanjay E. Sarma", "title": "PointGrow: Autoregressively Learned Point Cloud Generation with\n  Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating 3D point clouds is challenging yet highly desired. This work\npresents a novel autoregressive model, PointGrow, which can generate diverse\nand realistic point cloud samples from scratch or conditioned on semantic\ncontexts. This model operates recurrently, with each point sampled according to\na conditional distribution given its previously-generated points, allowing\ninter-point correlations to be well-exploited and 3D shape generative processes\nto be better interpreted. Since point cloud object shapes are typically encoded\nby long-range dependencies, we augment our model with dedicated self-attention\nmodules to capture such relations. Extensive evaluations show that PointGrow\nachieves satisfying performance on both unconditional and conditional point\ncloud generation tasks, with respect to realism and diversity. Several\nimportant applications, such as unsupervised feature learning and shape\narithmetic operations, are also demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 16:08:32 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 17:21:09 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 18:17:41 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Sun", "Yongbin", ""], ["Wang", "Yue", ""], ["Liu", "Ziwei", ""], ["Siegel", "Joshua E.", ""], ["Sarma", "Sanjay E.", ""]]}, {"id": "1810.05642", "submitter": "Robert Krajewski", "authors": "Robert Krajewski, Julian Bock, Laurent Kloeker and Lutz Eckstein", "title": "The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories\n  on German Highways for Validation of Highly Automated Driving Systems", "comments": "IEEE International Conference on Intelligent Transportation Systems\n  (ITSC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario-based testing for the safety validation of highly automated vehicles\nis a promising approach that is being examined in research and industry. This\napproach heavily relies on data from real-world scenarios to derive the\nnecessary scenario information for testing. Measurement data should be\ncollected at a reasonable effort, contain naturalistic behavior of road users\nand include all data relevant for a description of the identified scenarios in\nsufficient quality. However, the current measurement methods fail to meet at\nleast one of the requirements. Thus, we propose a novel method to measure data\nfrom an aerial perspective for scenario-based validation fulfilling the\nmentioned requirements. Furthermore, we provide a large-scale naturalistic\nvehicle trajectory dataset from German highways called highD. We evaluate the\ndata in terms of quantity, variety and contained scenarios. Our dataset\nconsists of 16.5 hours of measurements from six locations with 110 000\nvehicles, a total driven distance of 45 000 km and 5600 recorded complete lane\nchanges. The highD dataset is available online at: http://www.highD-dataset.com\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 22:47:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Krajewski", "Robert", ""], ["Bock", "Julian", ""], ["Kloeker", "Laurent", ""], ["Eckstein", "Lutz", ""]]}, {"id": "1810.05670", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Ikuo Hoshi, Tomoyoshi Shimobaba, Takashi Kakue, Tomoyoshi Ito", "title": "Computational ghost imaging using a field-programmable gate array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AR cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational ghost imaging is a promising technique for single-pixel imaging\nbecause it is robust to disturbance and can be operated over broad wavelength\nbands, unlike common cameras. However, one disadvantage of this method is that\nit has a long calculation time for image reconstruction. In this paper, we have\ndesigned a dedicated calculation circuit that accelerated the process of\ncomputational ghost imaging. We implemented this circuit by using a\nfield-programmable gate array, which reduced the calculation time for the\ncircuit compared to a CPU. The dedicated circuit reconstructs images at a frame\nrate of 300 Hz.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 21:32:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Hoshi", "Ikuo", ""], ["Shimobaba", "Tomoyoshi", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1810.05680", "submitter": "Ali Borji", "authors": "Ali Borji, Hamed R. Tavakoli, Zoya Bylinskii", "title": "Bottom-up Attention, Models of", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.03716", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review, we examine the recent progress in saliency prediction and\nproposed several avenues for future research. In spite of tremendous efforts\nand huge progress, there is still room for improvement in terms finer-grained\nanalysis of deep saliency models, evaluation measures, datasets, annotation\nmethods, cognitive studies, and new applications. This chapter will appear in\nEncyclopedia of Computational Neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:58:35 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 01:14:11 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 03:26:50 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Borji", "Ali", ""], ["Tavakoli", "Hamed R.", ""], ["Bylinskii", "Zoya", ""]]}, {"id": "1810.05716", "submitter": "Yanting Pei", "authors": "Yanting Pei, Yaping Huang, Qi Zou, Yuhang Lu and Song Wang", "title": "Does Haze Removal Help CNN-based Image Classification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hazy images are common in real scenarios and many dehazing methods have been\ndeveloped to automatically remove the haze from images. Typically, the goal of\nimage dehazing is to produce clearer images from which human vision can better\nidentify the object and structural details present in the images. When the\nground-truth haze-free image is available for a hazy image, quantitative\nevaluation of image dehazing is usually based on objective metrics, such as\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). However, in\nmany applications, large-scale images are collected not for visual examination\nby human. Instead, they are used for many high-level vision tasks, such as\nautomatic classification, recognition and categorization. One fundamental\nproblem here is whether various dehazing methods can produce clearer images\nthat can help improve the performance of the high-level tasks. In this paper,\nwe empirically study this problem in the important task of image classification\nby using both synthetic and real hazy image datasets. From the experimental\nresults, we find that the existing image-dehazing methods cannot improve much\nthe image-classification performance and sometimes even reduce the\nimage-classification performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 20:46:29 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Pei", "Yanting", ""], ["Huang", "Yaping", ""], ["Zou", "Qi", ""], ["Lu", "Yuhang", ""], ["Wang", "Song", ""]]}, {"id": "1810.05723", "submitter": "Ron Banner", "authors": "Ron Banner, Yury Nahshan, Elad Hoffer and Daniel Soudry", "title": "Post-training 4-bit quantization of convolution networks for\n  rapid-deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks require significant memory bandwidth and\nstorage for intermediate computations, apart from substantial computing\nresources. Neural network quantization has significant benefits in reducing the\namount of intermediate results, but it often requires the full datasets and\ntime-consuming fine tuning to recover the accuracy lost after quantization.\nThis paper introduces the first practical 4-bit post training quantization\napproach: it does not involve training the quantized model (fine-tuning), nor\nit requires the availability of the full dataset. We target the quantization of\nboth activations and weights and suggest three complementary methods for\nminimizing quantization error at the tensor level, two of whom obtain a\nclosed-form analytical solution. Combining these methods, our approach achieves\naccuracy that is just a few percents less the state-of-the-art baseline across\na wide range of convolutional models. The source code to replicate all\nexperiments is available on GitHub:\n\\url{https://github.com/submission2019/cnn-quantization}.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 15:10:44 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 07:23:56 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 08:45:02 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Banner", "Ron", ""], ["Nahshan", "Yury", ""], ["Hoffer", "Elad", ""], ["Soudry", "Daniel", ""]]}, {"id": "1810.05724", "submitter": "Andrej Junginger", "authors": "Andrej Junginger, Markus Hanselmann, Thilo Strauss, Sebastian Boblest,\n  Jens Buchner, Holger Ulmer", "title": "Unpaired High-Resolution and Scalable Style Transfer Using Generative\n  Adversarial Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have proven their capabilities by outperforming many other\napproaches on regression or classification tasks on various kinds of data.\nOther astonishing results have been achieved using neural nets as data\ngenerators, especially in settings of generative adversarial networks (GANs).\nOne special application is the field of image domain translations. Here, the\ngoal is to take an image with a certain style (e.g. a photography) and\ntransform it into another one (e.g. a painting). If such a task is performed\nfor unpaired training examples, the corresponding GAN setting is complex, the\nneural networks are large, and this leads to a high peak memory consumption\nduring, both, training and evaluation phase. This sets a limit to the highest\nprocessable image size. We address this issue by the idea of not processing the\nwhole image at once, but to train and evaluate the domain translation on the\nlevel of overlapping image subsamples. This new approach not only enables us to\ntranslate high-resolution images that otherwise cannot be processed by the\nneural network at once, but also allows us to work with comparably small neural\nnetworks and with limited hardware resources. Additionally, the number of\nimages required for the training process is significantly reduced. We present\nhigh-quality results on images with a total resolution of up to over 50\nmegapixels and emonstrate that our method helps to preserve local image details\nwhile it also keeps global consistency.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 07:02:47 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Junginger", "Andrej", ""], ["Hanselmann", "Markus", ""], ["Strauss", "Thilo", ""], ["Boblest", "Sebastian", ""], ["Buchner", "Jens", ""], ["Ulmer", "Holger", ""]]}, {"id": "1810.05725", "submitter": "Dhinaharan Nagamalai", "authors": "Marija Prokopijevi\\'c, Aleksandar Stan\\v{c}i\\'c, Jelena Vasiljevi\\'c,\n  \\v{Z}eljko Stojkovi\\'c, Goran Dimi\\'c, Jelena Sopta, Dalibor Risti\\'c and\n  Dhinaharan Nagamalai", "title": "Neural Network based classification of bone metastasis by primary\n  cacinoma", "comments": "13 pages, 9 figures", "journal-ref": "Computer Science & Information Technology (CS & IT), 7th\n  International Conference on Information Technology Convergence and Services\n  (ITCSE 2018), Vienna, Austria, May 26~27, 2018", "doi": "10.5121/csit.2018.80707", "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks have been known for a long time as a tool for different types\nof classification, but only just in the last decade they have showed their\nentire power. Along with appearing of hardware that is capable to support\ndemanding matrix operations and parallel algorithms, the neural network, as a\nuniversal function approximation framework, turns out to be the most successful\nclassification method widely used in all fields of science. On the other side,\nmultifractal (MF) approach is an efficient way for quantitative description of\ncomplex structures [1] such as metastatic carcinoma, which recommends this\nmethod as an accurate tool for medical diagnostics. The only part that is\nmissing is classification method. The goal of this research is to describe and\napply a feed-forward neural network as an auxiliary diagnostic method for\nclassification of multifractal parameters in order to determine primary cancer.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 23:44:59 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Prokopijevi\u0107", "Marija", ""], ["Stan\u010di\u0107", "Aleksandar", ""], ["Vasiljevi\u0107", "Jelena", ""], ["Stojkovi\u0107", "\u017deljko", ""], ["Dimi\u0107", "Goran", ""], ["Sopta", "Jelena", ""], ["Risti\u0107", "Dalibor", ""], ["Nagamalai", "Dhinaharan", ""]]}, {"id": "1810.05726", "submitter": "Alex Olsen", "authors": "Alex Olsen, Dmitry A. Konovalov, Bronson Philippa, Peter Ridd, Jake C.\n  Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James\n  Whinney, Brendan Calvert, Mostafa Rahimi Azghadi and Ronald D. White", "title": "DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning", "comments": "14 pages, 8 figures, 4 tables", "journal-ref": "Sci.Rep. 9, 2058 (2019)", "doi": "10.1038/s41598-018-38343-3", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic weed control has seen increased research of late with its potential\nfor boosting productivity in agriculture. Majority of works focus on developing\nrobotics for croplands, ignoring the weed management problems facing rangeland\nstock farmers. Perhaps the greatest obstacle to widespread uptake of robotic\nweed control is the robust classification of weed species in their natural\nenvironment. The unparalleled successes of deep learning make it an ideal\ncandidate for recognising various weed species in the complex rangeland\nenvironment. This work contributes the first large, public, multiclass image\ndataset of weed species from the Australian rangelands; allowing for the\ndevelopment of robust classification methods to make robotic weed control\nviable. The DeepWeeds dataset consists of 17,509 labelled images of eight\nnationally significant weed species native to eight locations across northern\nAustralia. This paper presents a baseline for classification performance on the\ndataset using the benchmark deep learning models, Inception-v3 and ResNet-50.\nThese models achieved an average classification accuracy of 95.1% and 95.7%,\nrespectively. We also demonstrate real time performance of the ResNet-50\narchitecture, with an average inference time of 53.4 ms per image. These strong\nresults bode well for future field implementation of robotic weed control\nmethods in the Australian rangelands.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 05:53:26 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:49:49 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 11:20:57 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Olsen", "Alex", ""], ["Konovalov", "Dmitry A.", ""], ["Philippa", "Bronson", ""], ["Ridd", "Peter", ""], ["Wood", "Jake C.", ""], ["Johns", "Jamie", ""], ["Banks", "Wesley", ""], ["Girgenti", "Benjamin", ""], ["Kenny", "Owen", ""], ["Whinney", "James", ""], ["Calvert", "Brendan", ""], ["Azghadi", "Mostafa Rahimi", ""], ["White", "Ronald D.", ""]]}, {"id": "1810.05727", "submitter": "Julia Noothout", "authors": "Julia M. H. Noothout, Bob D. de Vos, Jelmer M. Wolterink, Ivana Isgum", "title": "Automatic Segmentation of Thoracic Aorta Segments in Low-Dose Chest CT", "comments": null, "journal-ref": "SPIE Medical Imaging, 2018, vol. 10574, pp. 105741S", "doi": "10.1117/12.2293114", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological analysis and identification of pathologies in the aorta are\nimportant for cardiovascular diagnosis and risk assessment in patients. Manual\nannotation is time-consuming and cumbersome in CT scans acquired without\ncontrast enhancement and with low radiation dose. Hence, we propose an\nautomatic method to segment the ascending aorta, the aortic arch and the\nthoracic descending aorta in low-dose chest CT without contrast enhancement.\nSegmentation was performed using a dilated convolutional neural network (CNN),\nwith a receptive field of 131X131 voxels, that classified voxels in axial,\ncoronal and sagittal image slices. To obtain a final segmentation, the obtained\nprobabilities of the three planes were averaged per class, and voxels were\nsubsequently assigned to the class with the highest class probability. Two-fold\ncross-validation experiments were performed where ten scans were used to train\nthe network and another ten to evaluate the performance. Dice coefficients of\n0.83, 0.86 and 0.88, and Average Symmetrical Surface Distances (ASSDs) of 2.44,\n1.56 and 1.87 mm were obtained for the ascending aorta, the aortic arch, and\nthe descending aorta, respectively. The results indicate that the proposed\nmethod could be used in large-scale studies analyzing the anatomical location\nof pathology and morphology of the thoracic aorta.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 13:31:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Noothout", "Julia M. H.", ""], ["de Vos", "Bob D.", ""], ["Wolterink", "Jelmer M.", ""], ["Isgum", "Ivana", ""]]}, {"id": "1810.05729", "submitter": "Teresa Finisterra Ara\\'ujo", "authors": "Teresa Ara\\'ujo, Guilherme Aresta, Adrian Galdran, Pedro Costa, Ana\n  Maria Mendon\\c{c}a, and Aur\\'elio Campilho", "title": "UOLO - automatic object detection and segmentation in biomedical images", "comments": "Publised on DLMIA 2018. Licensed under the Creative Commons\n  CC-BY-NC-ND 4.0 license: http://creativecommons.org/licenses/by-nc-nd/4.0/", "journal-ref": "4th International Workshop, DLMIA 2018, and 8th International\n  Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain,\n  September 20, 2018, Proceedings. 165-173", "doi": "10.1007/978-3-030-00889-5_19", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose UOLO, a novel framework for the simultaneous detection and\nsegmentation of structures of interest in medical images. UOLO consists of an\nobject segmentation module which intermediate abstract representations are\nprocessed and used as input for object detection. The resulting system is\noptimized simultaneously for detecting a class of objects and segmenting an\noptionally different class of structures. UOLO is trained on a set of bounding\nboxes enclosing the objects to detect, as well as pixel-wise segmentation\ninformation, when available. A new loss function is devised, taking into\naccount whether a reference segmentation is accessible for each training image,\nin order to suitably backpropagate the error. We validate UOLO on the task of\nsimultaneous optic disc (OD) detection, fovea detection, and OD segmentation\nfrom retinal images, achieving state-of-the-art performance on public datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 13:53:13 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Ara\u00fajo", "Teresa", ""], ["Aresta", "Guilherme", ""], ["Galdran", "Adrian", ""], ["Costa", "Pedro", ""], ["Mendon\u00e7a", "Ana Maria", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1810.05730", "submitter": "Miao Cheng", "authors": "Miao Cheng, Ah Chung Tsoi", "title": "CRH: A Simple Benchmark Approach to Continuous Hashing", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/GlobalSIP.2015.7418363", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, the distinctive advancement of handling huge data promotes\nthe evolution of ubiquitous computing and analysis technologies. With the\nconstantly upward system burden and computational complexity, adaptive coding\nhas been a fascinating topic for pattern analysis, with outstanding\nperformance. In this work, a continuous hashing method, termed continuous\nrandom hashing (CRH), is proposed to encode sequential data stream, while\nignorance of previously hashing knowledge is possible. Instead, a random\nselection idea is adopted to adaptively approximate the differential encoding\npatterns of data stream, e.g., streaming media, and iteration is avoided for\nstepwise learning. Experimental results demonstrate our method is able to\nprovide outstanding performance, as a benchmark approach to continuous hashing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 14:18:00 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Cheng", "Miao", ""], ["Tsoi", "Ah Chung", ""]]}, {"id": "1810.05731", "submitter": "Saifuddin Hitawala", "authors": "Saifuddin Hitawala, Yao Li, Xian Wang, Dongyang Yang", "title": "Image Super-Resolution Using VDSR-ResNeXt and SRCGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, many Super Resolution techniques have been developed\nusing deep learning. Among those, generative adversarial networks (GAN) and\nvery deep convolutional networks (VDSR) have shown promising results in terms\nof HR image quality and computational speed. In this paper, we propose two\napproaches based on these two algorithms: VDSR-ResNeXt, which is a deep\nmulti-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN,\nwhich is a conditional GAN that explicitly passes class labels as input to the\nGAN. The two methods were implemented on common SR benchmark datasets for both\nquantitative and qualitative assessment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 19:20:15 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Hitawala", "Saifuddin", ""], ["Li", "Yao", ""], ["Wang", "Xian", ""], ["Yang", "Dongyang", ""]]}, {"id": "1810.05732", "submitter": "Amir Gholami", "authors": "Amir Gholami and Shashank Subramanian and Varun Shenoy and Naveen\n  Himthani and Xiangyu Yue and Sicheng Zhao and Peter Jin and George Biros and\n  Kurt Keutzer", "title": "A Novel Domain Adaptation Framework for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a segmentation framework that uses deep neural networks and\nintroduce two innovations. First, we describe a biophysics-based domain\nadaptation method. Second, we propose an automatic method to segment white and\ngray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding\nour first innovation, we use a domain adaptation framework that combines a\nnovel multispecies biophysical tumor growth model with a generative adversarial\nmodel to create realistic looking synthetic multimodal MR images with known\nsegmentation. Regarding our second innovation, we propose an automatic approach\nto enrich available segmentation data by computing the segmentation for healthy\ntissues. This segmentation, which is done using diffeomorphic image\nregistration between the BraTS training data and a set of prelabeled atlases,\nprovides more information for training and reduces the class imbalance problem.\nOur overall approach is not specific to any particular neural network and can\nbe used in conjunction with existing solutions. We demonstrate the performance\nimprovement using a 2D U-Net for the BraTS'18 segmentation challenge. Our\nbiophysics based domain adaptation achieves better results, as compared to the\nexisting state-of-the-art GAN model used to create synthetic data for training.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 04:03:30 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Gholami", "Amir", ""], ["Subramanian", "Shashank", ""], ["Shenoy", "Varun", ""], ["Himthani", "Naveen", ""], ["Yue", "Xiangyu", ""], ["Zhao", "Sicheng", ""], ["Jin", "Peter", ""], ["Biros", "George", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1810.05733", "submitter": "Shubham Kumar", "authors": "Shubham Kumar, Abhijit Guha Roy, Ping Wu, Sailesh Conjeti, R. S.\n  Anand, Jian Wang, Igor Yakushev, Stefan F\\\"orster, Markus Schwaiger,\n  Sung-Cheng Huang, Axel Rominger, Chuantao Zuo, Kuangyu Shi", "title": "Learning Optimal Deep Projection of $^{18}$F-FDG PET Imaging for Early\n  Differential Diagnosis of Parkinsonian Syndromes", "comments": "8 pages, 3 figures, conference, MICCAI DLMIA, 2018", "journal-ref": "Kumar, Shubham, et al. DLMIA, Springer, Cham, 2018. 227-235", "doi": "10.1007/978-3-030-00889-5_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several diseases of parkinsonian syndromes present similar symptoms at early\nstage and no objective widely used diagnostic methods have been approved until\nnow. Positron emission tomography (PET) with $^{18}$F-FDG was shown to be able\nto assess early neuronal dysfunction of synucleinopathies and tauopathies.\nTensor factorization (TF) based approaches have been applied to identify\ncharacteristic metabolic patterns for differential diagnosis. However, these\nconventional dimension-reduction strategies assume linear or multi-linear\nrelationships inside data, and are therefore insufficient to distinguish\nnonlinear metabolic differences between various parkinsonian syndromes. In this\npaper, we propose a Deep Projection Neural Network (DPNN) to identify\ncharacteristic metabolic pattern for early differential diagnosis of\nparkinsonian syndromes. We draw our inspiration from the existing TF methods.\nThe network consists of a (i) compression part: which uses a deep network to\nlearn optimal 2D projections of 3D scans, and a (ii) classification part: which\nmaps the 2D projections to labels. The compression part can be pre-trained\nusing surplus unlabelled datasets. Also, as the classification part operates on\nthese 2D projections, it can be trained end-to-end effectively with limited\nlabelled data, in contrast to 3D approaches. We show that DPNN is more\neffective in comparison to existing state-of-the-art and plausible baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 15:41:26 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Kumar", "Shubham", ""], ["Roy", "Abhijit Guha", ""], ["Wu", "Ping", ""], ["Conjeti", "Sailesh", ""], ["Anand", "R. S.", ""], ["Wang", "Jian", ""], ["Yakushev", "Igor", ""], ["F\u00f6rster", "Stefan", ""], ["Schwaiger", "Markus", ""], ["Huang", "Sung-Cheng", ""], ["Rominger", "Axel", ""], ["Zuo", "Chuantao", ""], ["Shi", "Kuangyu", ""]]}, {"id": "1810.05735", "submitter": "Shubham Kumar", "authors": "Shubham Kumar, Sailesh Conjeti, Abhijit Guha Roy, Christian Wachinger,\n  Nassir Navab", "title": "InfiNet: Fully Convolutional Networks for Infant Brain MRI Segmentation", "comments": "4 pages, 3 figures, conference, IEEE ISBI, 2018", "journal-ref": "Kumar, Shubham, et al. ISBI, IEEE (2018)(pp. 145-148)", "doi": "10.1109/ISBI.2018.8363542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, parameter-efficient and practical fully convolutional\nneural network architecture, termed InfiNet, aimed at voxel-wise semantic\nsegmentation of infant brain MRI images at iso-intense stage, which can be\neasily extended for other segmentation tasks involving multi-modalities.\nInfiNet consists of double encoder arms for T1 and T2 input scans that feed\ninto a joint-decoder arm that terminates in the classification layer. The\nnovelty of InfiNet lies in the manner in which the decoder upsamples lower\nresolution input feature map(s) from multiple encoder arms. Specifically, the\npooled indices computed in the max-pooling layers of each of the encoder blocks\nare related to the corresponding decoder block to perform non-linear\nlearning-free upsampling. The sparse maps are concatenated with intermediate\nencoder representations (skip connections) and convolved with trainable filters\nto produce dense feature maps. InfiNet is trained end-to-end to optimize for\nthe Generalized Dice Loss, which is well-suited for high class imbalance.\nInfiNet achieves the whole-volume segmentation in under 50 seconds and we\ndemonstrate competitive performance against multiple state-of-the art deep\narchitectures and their multi-modal variants.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 16:05:00 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Kumar", "Shubham", ""], ["Conjeti", "Sailesh", ""], ["Roy", "Abhijit Guha", ""], ["Wachinger", "Christian", ""], ["Navab", "Nassir", ""]]}, {"id": "1810.05749", "submitter": "Chris Zhang", "authors": "Chris Zhang, Mengye Ren, Raquel Urtasun", "title": "Graph HyperNetworks for Neural Architecture Search", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) automatically finds the best task-specific\nneural network topology, outperforming many manual architecture designs.\nHowever, it can be prohibitively expensive as the search requires training\nthousands of different networks, while each can last for hours. In this work,\nwe propose the Graph HyperNetwork (GHN) to amortize the search cost: given an\narchitecture, it directly generates the weights by running inference on a graph\nneural network. GHNs model the topology of an architecture and therefore can\npredict network performance more accurately than regular hypernetworks and\npremature early stopping. To perform NAS, we randomly sample architectures and\nuse the validation accuracy of networks with GHN generated weights as the\nsurrogate search signal. GHNs are fast -- they can search nearly 10 times\nfaster than other random search methods on CIFAR-10 and ImageNet. GHNs can be\nfurther extended to the anytime prediction setting, where they have found\nnetworks with better speed-accuracy tradeoff than the state-of-the-art manual\ndesigns.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 22:21:05 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 04:03:03 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 18:01:04 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Zhang", "Chris", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1810.05778", "submitter": "Sorour Mohajerani", "authors": "Sorour Mohajerani, Parvaneh Saeedi", "title": "CPNet: A Context Preserver Convolutional Neural Network for Detecting\n  Shadows in Single RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of shadow regions in an image is a difficult task due to\nthe lack of prior information about the illumination source and the dynamic of\nthe scene objects. To address this problem, in this paper, a deep-learning\nbased segmentation method is proposed that identifies shadow regions at the\npixel-level in a single RGB image. We exploit a novel Convolutional Neural\nNetwork (CNN) architecture to identify and extract shadow features in an\nend-to-end manner. This network preserves learned contexts during the training\nand observes the entire image to detect global and local shadow patterns\nsimultaneously. The proposed method is evaluated on two publicly available\ndatasets of SBU and UCF. We have improved the state-of-the-art Balanced Error\nRate (BER) on these datasets by 22\\% and 14\\%, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 01:26:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Mohajerani", "Sorour", ""], ["Saeedi", "Parvaneh", ""]]}, {"id": "1810.05780", "submitter": "Enric Corona", "authors": "Enric Corona, Kaustav Kundu, Sanja Fidler", "title": "Pose Estimation for Objects with Rotational Symmetry", "comments": "Accepted at IROS 2018. More details available at\n  http://www.cs.utoronto.ca/~ecorona/symmetry_pose_estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation is a widely explored problem, enabling many robotic tasks\nsuch as grasping and manipulation. In this paper, we tackle the problem of pose\nestimation for objects that exhibit rotational symmetry, which are common in\nman-made and industrial environments. In particular, our aim is to infer poses\nfor objects not seen at training time, but for which their 3D CAD models are\navailable at test time. Previous work has tackled this problem by learning to\ncompare captured views of real objects with the rendered views of their 3D CAD\nmodels, by embedding them in a joint latent space using neural networks. We\nshow that sidestepping the issue of symmetry in this scenario during training\nleads to poor performance at test time. We propose a model that reasons about\nrotational symmetry during training by having access to only a small set of\nsymmetry-labeled objects, whereby exploiting a large collection of unlabeled\nCAD models. We demonstrate that our approach significantly outperforms a\nnaively trained neural network on a new pose dataset containing images of tools\nand hardware.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 01:40:15 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Corona", "Enric", ""], ["Kundu", "Kaustav", ""], ["Fidler", "Sanja", ""]]}, {"id": "1810.05782", "submitter": "Sorour Mohajerani", "authors": "Sorour Mohajerani, Thomas A. Krammer, Parvaneh Saeedi", "title": "Cloud Detection Algorithm for Remote Sensing Images Using Fully\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep-learning based framework for addressing the\nproblem of accurate cloud detection in remote sensing images. This framework\nbenefits from a Fully Convolutional Neural Network (FCN), which is capable of\npixel-level labeling of cloud regions in a Landsat 8 image. Also, a\ngradient-based identification approach is proposed to identify and exclude\nregions of snow/ice in the ground truths of the training set. We show that\nusing the hybrid of the two methods (threshold-based and deep-learning)\nimproves the performance of the cloud identification process without the need\nto manually correct automatically generated ground truths. In average the\nJaccard index and recall measure are improved by 4.36% and 3.62%, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 01:53:49 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Mohajerani", "Sorour", ""], ["Krammer", "Thomas A.", ""], ["Saeedi", "Parvaneh", ""]]}, {"id": "1810.05786", "submitter": "Hai Wang", "authors": "Hai Wang and Jason D. Williams and SingBing Kang", "title": "Learning to Globally Edit Images with Textual Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how we can globally edit images using textual instructions: given a\nsource image and a textual instruction for the edit, generate a new image\ntransformed under this instruction. To tackle this novel problem, we develop\nthree different trainable models based on RNN and Generative Adversarial\nNetwork (GAN). The models (bucket, filter bank, and end-to-end) differ in how\nmuch expert knowledge is encoded, with the most general version being purely\nend-to-end. To train these systems, we use Amazon Mechanical Turk to collect\ntextual descriptions for around 2000 image pairs sampled from several datasets.\nExperimental results evaluated on our dataset validate our approaches. In\naddition, given that the filter bank model is a good compromise between\ngenerality and performance, we investigate it further by replacing RNN with\nGraph RNN, and show that Graph RNN improves performance. To the best of our\nknowledge, this is the first computational photography work on global image\nediting that is purely based on free-form textual instructions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 02:14:15 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wang", "Hai", ""], ["Williams", "Jason D.", ""], ["Kang", "SingBing", ""]]}, {"id": "1810.05801", "submitter": "Zhiwei Li", "authors": "Zhiwei Li, Huanfeng Shen, Qing Cheng, Yuhao Liu, Shucheng You, Zongyi\n  He", "title": "Deep learning based cloud detection for medium and high resolution\n  remote sensing images of different sensors", "comments": "This manuscript has been accepted for publication in ISPRS Journal of\n  Photogrammetry and Remote Sensing, vol. 150, pp.197-212, 2019.\n  (https://doi.org/10.1016/j.isprsjprs.2019.02.017)", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 150,\n  pp.197-212, 2019", "doi": "10.1016/j.isprsjprs.2019.02.017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud detection is an important preprocessing step for the precise\napplication of optical satellite imagery. In this paper, we propose a deep\nlearning based cloud detection method named multi-scale convolutional feature\nfusion (MSCFF) for remote sensing images of different sensors. In the network\narchitecture of MSCFF, the symmetric encoder-decoder module, which provides\nboth local and global context by densifying feature maps with trainable\nconvolutional filter banks, is utilized to extract multi-scale and high-level\nspatial features. The feature maps of multiple scales are then up-sampled and\nconcatenated, and a novel multi-scale feature fusion module is designed to fuse\nthe features of different scales for the output. The two output feature maps of\nthe network are cloud and cloud shadow maps, which are in turn fed to binary\nclassifiers outside the model to obtain the final cloud and cloud shadow mask.\nThe MSCFF method was validated on hundreds of globally distributed optical\nsatellite images, with spatial resolutions ranging from 0.5 to 50 m, including\nLandsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and\ncollected high-resolution images exported from Google Earth. The experimental\nresults show that MSCFF achieves a higher accuracy than the traditional\nrule-based cloud detection methods and the state-of-the-art deep learning\nmodels, especially in bright surface covered areas. The effectiveness of MSCFF\nmeans that it has great promise for the practical application of cloud\ndetection for multiple types of medium and high-resolution remote sensing\nimages. Our established global high-resolution cloud detection validation\ndataset has been made available online.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 05:58:47 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 01:34:20 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 02:49:17 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Li", "Zhiwei", ""], ["Shen", "Huanfeng", ""], ["Cheng", "Qing", ""], ["Liu", "Yuhao", ""], ["You", "Shucheng", ""], ["He", "Zongyi", ""]]}, {"id": "1810.05810", "submitter": "Peng Gao", "authors": "Yipeng Ma, Chun Yuan, Peng Gao, Fei Wang", "title": "Efficient Multi-level Correlating for Visual Tracking", "comments": "Accepted by ACCV'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF) based tracking algorithms have demonstrated favorable\nperformance recently. Nevertheless, the top performance trackers always employ\ncomplicated optimization methods which constraint their real-time applications.\nHow to accelerate the tracking speed while retaining the tracking accuracy is a\nsignificant issue. In this paper, we propose a multi-level CF-based tracking\napproach named MLCFT which further explores the potential capacity of CF with\ntwo-stage detection: primal detection and oriented re-detection. The cascaded\ndetection scheme is simple but competent to prevent model drift and accelerate\nthe speed. An effective fusion method based on relative entropy is introduced\nto combine the complementary features extracted from deep and shallow layers of\nconvolutional neural networks (CNN). Moreover, a novel online model update\nstrategy is utilized in our tracker, which enhances the tracking performance\nfurther. Experimental results demonstrate that our proposed approach\noutperforms the most state-of-the-art trackers while tracking at speed of\nexceeded 16 frames per second on challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 06:58:46 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ma", "Yipeng", ""], ["Yuan", "Chun", ""], ["Gao", "Peng", ""], ["Wang", "Fei", ""]]}, {"id": "1810.05835", "submitter": "Manuel Soriano-Trigueros", "authors": "N. Atienza, L.M. Escudero, M.J. Jimenez, M. Soriano-Trigueros", "title": "Characterising epithelial tissues using persistent entropy", "comments": "12 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1007/978-3-030-10828-1_14", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply persistent entropy, a novel topological statistic,\nfor characterization of images of epithelial tissues. We have found out that\npersistent entropy is able to summarize topological and geometric information\nencoded by \\alpha-complexes and persistent homology. After using some\nstatistical tests, we can guarantee the existence of significant differences in\nthe studied tissues.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 09:43:14 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Atienza", "N.", ""], ["Escudero", "L. M.", ""], ["Jimenez", "M. J.", ""], ["Soriano-Trigueros", "M.", ""]]}, {"id": "1810.05852", "submitter": "Pierluigi Zama Ramirez", "authors": "Pierluigi Zama Ramirez, Alessio Tonioni, Luigi Di Stefano", "title": "Exploiting Semantics in Adversarial Training for Image-Level Domain\n  Adaptation", "comments": "6 pages, Accepted to IPAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance achievable by modern deep learning approaches are directly\nrelated to the amount of data used at training time. Unfortunately, the\nannotation process is notoriously tedious and expensive, especially for\npixel-wise tasks like semantic segmentation. Recent works have proposed to rely\non synthetically generated imagery to ease the training set creation. However,\nmodels trained on these kind of data usually under-perform on real images due\nto the well known issue of domain shift. We address this problem by learning a\ndomain-to-domain image translation GAN to shrink the gap between real and\nsynthetic images. Peculiarly to our method, we introduce semantic constraints\ninto the generation process to both avoid artifacts and guide the synthesis. To\nprove the effectiveness of our proposal, we show how a semantic segmentation\nCNN trained on images from the synthetic GTA dataset adapted by our method can\nimprove performance by more than 16% mIoU with respect to the same model\ntrained on synthetic images.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 12:20:44 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Ramirez", "Pierluigi Zama", ""], ["Tonioni", "Alessio", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1810.05863", "submitter": "Yuanxin Wu", "authors": "Qi Cai, Yuanxin Wu, Lilian Zhang and Peike Zhang", "title": "Equivalent Constraints for Two-View Geometry: Pose Solution/Pure\n  Rotation Identification and 3D Reconstruction", "comments": "15 pages, 13 figures", "journal-ref": "International Journal of Computer Vision, 2019", "doi": "10.1007/s11263-018-1136-9", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-view relative pose estimation and structure reconstruction is a classical\nproblem in computer vision. The typical methods usually employ the singular\nvalue decomposition of the essential matrix to get multiple solutions of the\nrelative pose, from which the right solution is picked out by reconstructing\nthe three-dimension (3D) feature points and imposing the constraint of positive\ndepth. This paper revisits the two-view geometry problem and discovers that the\ntwo-view imaging geometry is equivalently governed by a Pair of new Pose-Only\n(PPO) constraints: the same-side constraint and the intersection constraint.\nFrom the perspective of solving equation, the complete pose solutions of the\nessential matrix are explicitly derived and we rigorously prove that the\norientation part of the pose can still be recovered in the case of pure\nrotation. The PPO constraints are simplified and formulated in the form of\ninequalities to directly identify the right pose solution with no need of 3D\nreconstruction and the 3D reconstruction can be analytically achieved from the\nidentified right pose. Furthermore, the intersection inequality also enables a\nrobust criterion for pure rotation identification. Experiment results validate\nthe correctness of analyses and the robustness of the derived pose\nsolution/pure rotation identification and analytical 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 13:51:15 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Cai", "Qi", ""], ["Wu", "Yuanxin", ""], ["Zhang", "Lilian", ""], ["Zhang", "Peike", ""]]}, {"id": "1810.05866", "submitter": "Fan Yang", "authors": "Fan Yang, Ke Yan, Shijian Lu, Huizhu Jia, Xiaodong Xie, Wen Gao", "title": "Attention Driven Person Re-identification", "comments": "Accepted in the Pattern Recognition (PR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is a challenging task due to arbitrary human\npose variations, background clutters, etc. It has been studied extensively in\nrecent years, but the multifarious local and global features are still not\nfully exploited by either ignoring the interplay between whole-body images and\nbody-part images or missing in-depth examination of specific body-part images.\nIn this paper, we propose a novel attention-driven multi-branch network that\nlearns robust and discriminative human representation from global whole-body\nimages and local body-part images simultaneously. Within each branch, an\nintra-attention network is designed to search for informative and\ndiscriminative regions within the whole-body or body-part images, where\nattention is elegantly decomposed into spatial-wise attention and channel-wise\nattention for effective and efficient learning. In addition, a novel\ninter-attention module is designed which fuses the output of intra-attention\nnetworks adaptively for optimal person ReID. The proposed technique has been\nevaluated over three widely used datasets CUHK03, Market-1501 and\nDukeMTMC-ReID, and experiments demonstrate its superior robustness and\neffectiveness as compared with the state of the arts.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 14:25:47 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Yang", "Fan", ""], ["Yan", "Ke", ""], ["Lu", "Shijian", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""], ["Gao", "Wen", ""]]}, {"id": "1810.05874", "submitter": "Petteri Teikari", "authors": "Petteri Teikari, Raymond P. Najjar, Leopold Schmetterer, Dan Milea", "title": "Embedded deep learning in ophthalmology: Making ophthalmic imaging\n  smarter", "comments": "This work has been submitted to \"Therapeutic Advances in\n  Ophthalmology\" for possible publication 17 pages, 5 figures", "journal-ref": "Therapeutic advances in ophthalmology 11 (2019): 2515841419827172", "doi": "10.1177/2515841419827172", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has recently gained high interest in ophthalmology, due to its\nability to detect clinically significant features for diagnosis and prognosis.\nDespite these significant advances, little is known about the ability of\nvarious deep learning systems to be embedded within ophthalmic imaging devices,\nallowing automated image acquisition. In this work, we will review the existing\nand future directions for \"active acquisition\" embedded deep learning, leading\nto as high quality images with little intervention by the human operator. In\nclinical practice, the improved image quality should translate into more robust\ndeep learning-based clinical diagnostics. Embedded deep learning will be\nenabled by the constantly improving hardware performance with low cost. We will\nbriefly review possible computation methods in larger clinical systems.\nBriefly, they can be included in a three-layer framework composed of edge, fog\nand cloud layers, the former being performed at a device-level. Improved edge\nlayer performance via \"active acquisition\" serves as an automatic data curation\noperator translating to better quality data in electronic health records\n(EHRs), as well as on the cloud layer, for improved deep learning-based\nclinical data mining.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 15:20:32 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 09:17:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Teikari", "Petteri", ""], ["Najjar", "Raymond P.", ""], ["Schmetterer", "Leopold", ""], ["Milea", "Dan", ""]]}, {"id": "1810.05919", "submitter": "Si Lu", "authors": "Si Lu", "title": "No-reference Image Denoising Quality Assessment", "comments": "17 pages, 41 figures, accepted by Computer Vision Conference (CVC)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of image denoising methods are available now. However, the\nperformance of a denoising algorithm often depends on individual input noisy\nimages as well as its parameter setting. In this paper, we present a\nno-reference image denoising quality assessment method that can be used to\nselect for an input noisy image the right denoising algorithm with the optimal\nparameter setting. This is a challenging task as no ground truth is available.\nThis paper presents a data-driven approach to learn to predict image denoising\nquality. Our method is based on the observation that while individual existing\nquality metrics and denoising models alone cannot robustly rank denoising\nresults, they often complement each other. We accordingly design denoising\nquality features based on these existing metrics and models and then use Random\nForests Regression to aggregate them into a more powerful unified metric. Our\nexperiments on images with various types and levels of noise show that our\nno-reference denoising quality assessment method significantly outperforms the\nstate-of-the-art quality metrics. This paper also provides a method that\nleverages our quality assessment method to automatically tune the parameter\nsettings of a denoising algorithm for an input noisy image to produce an\noptimal denoising result.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 19:47:09 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Lu", "Si", ""]]}, {"id": "1810.05922", "submitter": "Shervan Fekri-Ershad", "authors": "Shervan Fekri-Ershad", "title": "Porosity Amount Estimation in Stones Based on Combination of One\n  Dimensional Local Binary Patterns and Image Normalization Technique", "comments": "16 pages, in Farsi. 9 Figures, Computing Science Journal, Vol. 9,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since now, many approaches has been proposed for surface defect detection\nbased on image texture analysis techniques. One of the efficient texture\nanalysis operations is local binary patterns which provides good accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 20:14:00 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Fekri-Ershad", "Shervan", ""]]}, {"id": "1810.05943", "submitter": "Yulei Qin", "authors": "Yulei Qin, Juan Wen, Hao Zheng, Xiaolin Huang, Jie Yang, Ning Song,\n  Yue-Min Zhu, Lingqian Wu, Guang-Zhong Yang", "title": "Varifocal-Net: A Chromosome Classification Approach using Deep\n  Convolutional Networks", "comments": "This paper is accepted to IEEE TMI for future publication. 13 pages,\n  11 figures, 9 tables", "journal-ref": null, "doi": "10.1109/TMI.2019.2905841", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromosome classification is critical for karyotyping in abnormality\ndiagnosis. To expedite the diagnosis, we present a novel method named\nVarifocal-Net for simultaneous classification of chromosome's type and polarity\nusing deep convolutional networks. The approach consists of one global-scale\nnetwork (G-Net) and one local-scale network (L-Net). It follows three stages.\nThe first stage is to learn both global and local features. We extract global\nfeatures and detect finer local regions via the G-Net. By proposing a varifocal\nmechanism, we zoom into local parts and extract local features via the L-Net.\nResidual learning and multi-task learning strategies are utilized to promote\nhigh-level feature extraction. The detection of discriminative local parts is\nfulfilled by a localization subnet of the G-Net, whose training process\ninvolves both supervised and weakly-supervised learning. The second stage is to\nbuild two multi-layer perceptron classifiers that exploit features of both two\nscales to boost classification performance. The third stage is to introduce a\ndispatch strategy of assigning each chromosome to a type within each patient\ncase, by utilizing the domain knowledge of karyotyping. Evaluation results from\n1909 karyotyping cases showed that the proposed Varifocal-Net achieved the\nhighest accuracy per patient case (%) 99.2 for both type and polarity tasks. It\noutperformed state-of-the-art methods, demonstrating the effectiveness of our\nvarifocal mechanism, multi-scale feature ensemble, and dispatch strategy. The\nproposed method has been applied to assist practical karyotype diagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 23:39:39 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 14:02:01 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 03:55:05 GMT"}, {"version": "v4", "created": "Wed, 20 Mar 2019 05:20:11 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Qin", "Yulei", ""], ["Wen", "Juan", ""], ["Zheng", "Hao", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Song", "Ning", ""], ["Zhu", "Yue-Min", ""], ["Wu", "Lingqian", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1810.05952", "submitter": "Jialin Chen", "authors": "Yixiong Liang, Zhihong Tang, Meng Yan, Jialin Chen, Qing Liu, Yao\n  Xiang", "title": "Comparison-Based Convolutional Neural Networks for Cervical Cell/Clumps\n  Detection in the Limited Data Scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection of cervical cancer cells or cell clumps has the potential\nto significantly reduce error rate and increase productivity in cervical cancer\nscreening. However, most traditional methods rely on the success of accurate\ncell segmentation and discriminative hand-crafted features extraction. Recently\nthere are emerging deep learning-based methods which train convolutional neural\nnetworks (CNN) to classify image patches, but they are computationally\nexpensive. In this paper we propose an efficient CNN-based object detection\nmethods for cervical cancer cells/clumps detection. Specifically, we utilize\nthe state-of-the-art two-stage object detection method, the Faster-RCNN with\nFeature Pyramid Network (FPN) as the baseline and propose a novel comparison\ndetector to deal with the limited data problem. The key idea is that classify\nthe proposals by comparing with the reference samples of each category in\nobject detection. In addition, we propose to learn the reference samples of the\nbackground from data instead of manually choosing them by some heuristic rules.\nExperimental results show that the proposed Comparison Detector yields\nsignificant improvement on the small dataset, achieving a mean Average\nPrecision (mAP) of 26.3% and an Average Recall (AR) of 35.7%, both improving\nabout 20 points compared to the baseline. Moreover, Comparison Detector\nimproved AR by 4.6 points and achieved marginally better performance in terms\nof mAP compared with baseline model when training on the medium dataset. Our\nmethod is promising for the development of automation-assisted cervical cancer\nscreening systems. Code is available at\nhttps://github.com/kuku-sichuan/ComparisonDetector.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 02:12:12 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 03:13:14 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 15:08:17 GMT"}, {"version": "v4", "created": "Mon, 11 Mar 2019 10:35:27 GMT"}, {"version": "v5", "created": "Mon, 23 Dec 2019 07:35:20 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liang", "Yixiong", ""], ["Tang", "Zhihong", ""], ["Yan", "Meng", ""], ["Chen", "Jialin", ""], ["Liu", "Qing", ""], ["Xiang", "Yao", ""]]}, {"id": "1810.05964", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Perceptual Image Quality Assessment through Spectral Analysis of Error\n  Representations", "comments": "23 pages, 6 figures, 4 tables", "journal-ref": "Signal Processing: Image Communication, Volume 70, 2019, Pages\n  37-46,ISSN 0923-5965", "doi": "10.1016/j.image.2018.09.005", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the statistics of error signals to assess the\nperceived quality of images. Specifically, we focus on the magnitude spectrum\nof error images obtained from the difference of reference and distorted images.\nAnalyzing spectral statistics over grayscale images partially models\ninterference in spatial harmonic distortion exhibited by the visual system but\nit overlooks color information, selective and hierarchical nature of visual\nsystem. To overcome these shortcomings, we introduce an image quality\nassessment algorithm based on the Spectral Understanding of Multi-scale and\nMulti-channel Error Representations, denoted as SUMMER. We validate the quality\nassessment performance over 3 databases with around 30 distortion types. These\ndistortion types are grouped into 7 main categories as compression artifact,\nimage noise, color artifact, communication error, blur, global and local\ndistortions. In total, we benchmark the performance of 17 algorithms along with\nthe proposed algorithm using 5 performance metrics that measure linearity,\nmonotonicity, accuracy, and consistency. In addition to experiments with\nstandard performance metrics, we analyze the distribution of objective and\nsubjective scores with histogram difference metrics and scatter plots.\nMoreover, we analyze the classification performance of quality assessment\nalgorithms along with their statistical significance tests. Based on our\nexperiments, SUMMER significantly outperforms majority of the compared methods\nin all benchmark categories\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 04:13:56 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:49:25 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1810.05977", "submitter": "Tao Zhou", "authors": "Tao Zhou, Chen Fang, Zhaowen Wang, Jimei Yang, Byungmoon Kim, Zhili\n  Chen, Jonathan Brandt, Demetri Terzopoulos", "title": "Learning to Sketch with Deep Q Networks and Demonstrated Strokes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doodling is a useful and common intelligent skill that people can learn and\nmaster. In this work, we propose a two-stage learning framework to teach a\nmachine to doodle in a simulated painting environment via Stroke Demonstration\nand deep Q-learning (SDQ). The developed system, Doodle-SDQ, generates a\nsequence of pen actions to reproduce a reference drawing and mimics the\nbehavior of human painters. In the first stage, it learns to draw simple\nstrokes by imitating in supervised fashion from a set of strokeaction pairs\ncollected from artist paintings. In the second stage, it is challenged to draw\nreal and more complex doodles without ground truth actions; thus, it is trained\nwith Qlearning. Our experiments confirm that (1) doodling can be learned\nwithout direct stepby- step action supervision and (2) pretraining with stroke\ndemonstration via supervised learning is important to improve performance. We\nfurther show that Doodle-SDQ is effective at producing plausible drawings in\ndifferent media types, including sketch and watercolor.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 05:25:49 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhou", "Tao", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["Yang", "Jimei", ""], ["Kim", "Byungmoon", ""], ["Chen", "Zhili", ""], ["Brandt", "Jonathan", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1810.05989", "submitter": "Ophir Gozes", "authors": "Ophir Gozes, Hayit Greenspan", "title": "Lung Structures Enhancement in Chest Radiographs via CT based FCNN\n  Training", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-00946-5_16", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of overlapping anatomical structures appearing in chest\nradiographs can reduce the performance of lung pathology detection by automated\nalgorithms (CAD) as well as the human reader. In this paper, we present a deep\nlearning based image processing technique for enhancing the contrast of soft\nlung structures in chest radiographs using Fully Convolutional Neural Networks\n(FCNN). Two 2D FCNN architectures were trained to accomplish the task: The\nfirst performs 2D lung segmentation which is used for normalization of the lung\narea. The second FCNN is trained to extract lung structures. To create the\ntraining images, we employed Simulated X-Ray or Digitally Reconstructed\nRadiographs (DRR) derived from 516 scans belonging to the LIDC-IDRI dataset. By\nfirst segmenting the lungs in the CT domain, we are able to create a dataset of\n2D lung masks to be used for training the segmentation FCNN. For training the\nextraction FCNN, we create DRR images of only voxels belonging to the 3D lung\nsegmentation which we call \"Lung X-ray\" and use them as target images. Once the\nlung structures are extracted, the original image can be enhanced by fusing the\noriginal input x-ray and the synthesized \"Lung X-ray\". We show that our\nenhancement technique is applicable to real x-ray data, and display our results\non the recently released NIH Chest X-Ray-14 dataset. We see promising results\nwhen training a DenseNet-121 based architecture to work directly on the lung\nenhanced X-ray images.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 08:04:43 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Gozes", "Ophir", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1810.06055", "submitter": "Ruzhang Zhao", "authors": "Ruzhang Zhao, Yajun Fang, Berthold K.P. Horn", "title": "A Simple Change Comparison Method for Image Sequences Based on\n  Uncertainty Coefficient", "comments": "5 pages, 5 figures, 2 tables, accepted as a conference paper at IEEE\n  UV 2018, Boston, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For identification of change information in image sequences, most studies\nfocus on change detection in one image sequence, while few studies have\nconsidered the change level comparison between two different image sequences.\nMoreover, most studies require the detection of image information in details,\nfor example, object detection. Based on Uncertainty Coefficient(UC), this paper\nproposes an innovative method CCUC for change comparison between two image\nsequences. The proposed method is computationally efficient and simple to\nimplement. The change comparison stems from video monitoring system. The\nlimited number of provided screens and a large number of monitoring cameras\nrequire the videos or image sequences ordered by change level. We demonstrate\nthis new method by applying it on two publicly available image sequences. The\nresults are able to show the method can distinguish the different change level\nfor sequences.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 16:28:59 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhao", "Ruzhang", ""], ["Fang", "Yajun", ""], ["Horn", "Berthold K. P.", ""]]}, {"id": "1810.06058", "submitter": "Ling Zhang", "authors": "Haoming Lin, Yuyang Hu, Siping Chen, Jianhua Yao, Ling Zhang", "title": "Fine-Grained Classification of Cervical Cells Using Morphological and\n  Appearance Based Convolutional Neural Networks", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification of cervical cells into different abnormality\nlevels is of great clinical importance but remains very challenging. Contrary\nto traditional classification methods that rely on hand-crafted or engineered\nfeatures, convolution neural network (CNN) can classify cervical cells based on\nautomatically learned deep features. However, CNN in previous studies do not\ninvolve cell morphological information, and it is unknown whether morphological\nfeatures can be directly modeled by CNN to classify cervical cells. This paper\npresents a CNN-based method that combines cell image appearance with cell\nmorphology for classification of cervical cells in Pap smear. The training\ncervical cell dataset consists of adaptively re-sampled image patches coarsely\ncentered on the nuclei. Several CNN models (AlexNet, GoogleNet, ResNet and\nDenseNet) pre-trained on ImageNet dataset were fine-tuned on the cervical\ndataset for comparison. The proposed method is evaluated on the Herlev cervical\ndataset by five-fold cross-validation at patient level splitting. Results show\nthat by adding cytoplasm and nucleus masks as raw morphological information\ninto appearance-based CNN learning, higher classification accuracies can be\nachieved in general. Among the four CNN models, GoogleNet fed with both\nmorphological and appearance information obtains the highest classification\naccuracies of 94.5% for 2-class classification task and 64.5% for 7-class\nclassification task. Our method demonstrates that combining cervical cell\nmorphology with appearance information can provide improved classification\nperformance, which is clinically important for early diagnosis of cervical\ndysplastic changes.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 16:48:32 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Lin", "Haoming", ""], ["Hu", "Yuyang", ""], ["Chen", "Siping", ""], ["Yao", "Jianhua", ""], ["Zhang", "Ling", ""]]}, {"id": "1810.06071", "submitter": "Sarfaraz Hussein", "authors": "Ismail Irmakci, Sarfaraz Hussein, Aydogan Savran, Rita R. Kalyani,\n  David Reiter, Chee W. Chia, Kenneth W. Fishbein, Richard G. Spencer, Luigi\n  Ferrucci and Ulas Bagci", "title": "A Novel Extension to Fuzzy Connectivity for Body Composition Analysis:\n  Applications in Thigh, Brain, and Whole Body Tissue Segmentation", "comments": "In press for IEEE Transactions on Biomedical Engineering (TBME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is the non-invasive modality of choice for\nbody tissue composition analysis due to its excellent soft tissue contrast and\nlack of ionizing radiation. However, quantification of body composition\nrequires an accurate segmentation of fat, muscle and other tissues from MR\nimages, which remains a challenging goal due to the intensity overlap between\nthem. In this study, we propose a fully automated, data-driven image\nsegmentation platform that addresses multiple difficulties in segmenting MR\nimages such as varying inhomogeneity, non-standardness, and noise, while\nproducing high-quality definition of different tissues. In contrast to most\napproaches in the literature, we perform segmentation operation by combining\nthree different MRI contrasts and a novel segmentation tool which takes into\naccount variability in the data. The proposed system, based on a novel affinity\ndefinition within the fuzzy connectivity (FC) image segmentation family,\nprevents the need for user intervention and reparametrization of the\nsegmentation algorithms. In order to make the whole system fully automated, we\nadapt an affinity propagation clustering algorithm to roughly identify tissue\nregions and image background. We perform a thorough evaluation of the proposed\nalgorithm's individual steps as well as comparison with several approaches from\nthe literature for the main application of muscle/fat separation. Furthermore,\nwhole-body tissue composition and brain tissue delineation were conducted to\nshow the generalization ability of the proposed system. This new automated\nplatform outperforms other state-of-the-art segmentation approaches both in\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 17:53:44 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Irmakci", "Ismail", ""], ["Hussein", "Sarfaraz", ""], ["Savran", "Aydogan", ""], ["Kalyani", "Rita R.", ""], ["Reiter", "David", ""], ["Chia", "Chee W.", ""], ["Fishbein", "Kenneth W.", ""], ["Spencer", "Richard G.", ""], ["Ferrucci", "Luigi", ""], ["Bagci", "Ulas", ""]]}, {"id": "1810.06125", "submitter": "Zhenheng Yang", "authors": "Chenxu Luo, Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, Ram Nevatia,\n  Alan Yuille", "title": "Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D\n  Holistic Understanding", "comments": "Chenxu Luo, Zhenheng Yang, and Peng Wang contributed equally, TPAMI\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to estimate 3D geometry in a single frame and optical flow from\nconsecutive frames by watching unlabeled videos via deep convolutional network\nhas made significant progress recently. Current state-of-the-art (SoTA) methods\ntreat the two tasks independently. One typical assumption of the existing depth\nestimation methods is that the scenes contain no independent moving objects.\nwhile object moving could be easily modeled using optical flow. In this paper,\nwe propose to address the two tasks as a whole, i.e. to jointly understand\nper-pixel 3D geometry and motion. This eliminates the need of static scene\nassumption and enforces the inherent geometrical consistency during the\nlearning process, yielding significantly improved results for both tasks. We\ncall our method as \"Every Pixel Counts++\" or \"EPC++\". Specifically, during\ntraining, given two consecutive frames from a video, we adopt three parallel\nnetworks to predict the camera motion (MotionNet), dense depth map (DepthNet),\nand per-pixel optical flow between two frames (OptFlowNet) respectively. The\nthree types of information are fed into a holistic 3D motion parser (HMP), and\nper-pixel 3D motion of both rigid background and moving objects are\ndisentangled and recovered. Comprehensive experiments were conducted on\ndatasets with different scenes, including driving scenario (KITTI 2012 and\nKITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic\nanimation (MPI Sintel dataset). Performance on the five tasks of depth\nestimation, optical flow estimation, odometry, moving object segmentation and\nscene flow estimation shows that our approach outperforms other SoTA methods.\nCode will be available at: https://github.com/chenxuluo/EPC.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 23:21:05 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 01:56:18 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Luo", "Chenxu", ""], ["Yang", "Zhenheng", ""], ["Wang", "Peng", ""], ["Wang", "Yang", ""], ["Xu", "Wei", ""], ["Nevatia", "Ram", ""], ["Yuille", "Alan", ""]]}, {"id": "1810.06169", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "Traffic Signs in the Wild: Highlights from the IEEE Video and Image\n  Processing Cup 2017 Student Competition [SP Competitions]", "comments": "11 pages, 5 figures", "journal-ref": "IEEE Signal Processing Magazine, vol. 35, no. 2, pp. 154-161,\n  March 2018", "doi": "10.1109/MSP.2017.2783449", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and reliable traffic sign detection is necessary to bring autonomous\nvehicles onto our roads. State-of-the-art algorithms successfully perform\ntraffic sign detection over existing databases that mostly lack severe\nchallenging conditions. VIP Cup 2017 competition focused on detecting such\ntraffic signs under challenging conditions. To facilitate such task and\ncompetition, we introduced a video dataset denoted as CURE-TSD that includes a\nvariety of challenging conditions. The goal of this challenge was to implement\ntraffic sign detection algorithms that can robustly perform under such\nchallenging conditions. In this article, we share an overview of the VIP Cup\n2017 experience including competition setup, teams, technical approaches,\nparticipation statistics, and competition experience through finalist teams\nmembers' and organizers' eyes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 03:40:48 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:44:58 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1810.06178", "submitter": "Jingyun Xiao", "authors": "Jingyun Xiao", "title": "3D Feature Pyramid Attention Module for Robust Visual Speech Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual speech recognition is the task to decode the speech content from a\nvideo based on visual information, especially the movements of lips. It is also\nreferenced as lipreading. Motivated by two problems existing in lipreading,\nwords with similar pronunciation and the variation of word duration, we propose\na novel 3D Feature Pyramid Attention (3D-FPA) module to jointly improve the\nrepresentation power of features in both the spatial and temporal domains.\nSpecifically, the input features are downsampled for 3 times in both the\nspatial and temporal dimensions to construct spatiotemporal feature pyramids.\nThen high-level features are upsampled and combined with low-level features,\nfinally generating a pixel-level soft attention mask to be multiplied with the\ninput features.It enhances the discriminative power of features and exploits\nthe temporal multi-scale information while decoding the visual speeches. Also,\nthis module provides a new method to construct and utilize temporal pyramid\nstructures in video analysis tasks. The field of temporal featrue pyramids are\nstill under exploring compared to the plentiful works on spatial feature\npyramids for image analysis tasks. To validate the effectiveness and\nadaptability of our proposed module, we embed the module in a sentence-level\nlipreading model, LipNet, with the result of 3.6% absolute decrease in word\nerror rate, and a word-level model, with the result of 1.4% absolute\nimprovement in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 04:38:23 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 04:03:01 GMT"}, {"version": "v3", "created": "Sat, 17 Nov 2018 05:02:39 GMT"}, {"version": "v4", "created": "Fri, 11 Jan 2019 11:57:12 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Xiao", "Jingyun", ""]]}, {"id": "1810.06208", "submitter": "Xingyuan Bu", "authors": "Yuan Gao, Xingyuan Bu, Yang Hu, Hui Shen, Ti Bai, Xubin Li and Shilei\n  Wen", "title": "Solution for Large-Scale Hierarchical Object Detection Datasets with\n  Incomplete Annotation and Data Imbalance", "comments": "5 pages, 4 figures, ECCV 2018 Open Images workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report demonstrates our solution for the Open Images 2018 Challenge.\nBased on our detailed analysis on the Open Images Datasets (OID), it is found\nthat there are four typical features: large-scale, hierarchical tag system,\nsevere annotation incompleteness and data imbalance. Considering these\ncharacteristics, an amount of strategies are employed, including SNIPER, soft\nsampling, class-aware sampling (CAS), hierarchical non-maximum suppression\n(HNMS) and so on. In virtue of these effective strategies, and further using\nthe powerful SENet154 armed with feature pyramid module and deformable ROIalign\nas the backbone, our best single model could achieve a mAP of 56.9%. After a\nfurther ensemble with 9 models, the final mAP is boosted to 62.2% in the public\nleaderboard (ranked the 2nd place) and 58.6% in the private leaderboard (ranked\nthe 3rd place, slightly inferior to the 1st place by only 0.04 point).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 07:40:53 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Gao", "Yuan", ""], ["Bu", "Xingyuan", ""], ["Hu", "Yang", ""], ["Shen", "Hui", ""], ["Bai", "Ti", ""], ["Li", "Xubin", ""], ["Wen", "Shilei", ""]]}, {"id": "1810.06219", "submitter": "Philipp Blandfort", "authors": "Tushar Karayil, Philipp Blandfort, J\\\"orn Hees, Andreas Dengel", "title": "The Focus-Aspect-Polarity Model for Predicting Subjective Noun\n  Attributes in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective visual interpretation is a challenging yet important topic in\ncomputer vision. Many approaches reduce this problem to the prediction of\nadjective- or attribute-labels from images. However, most of these do not take\nattribute semantics into account, or only process the image in a holistic\nmanner. Furthermore, there is a lack of relevant datasets with fine-grained\nsubjective labels. In this paper, we propose the Focus-Aspect-Polarity model to\nstructure the process of capturing subjectivity in image processing, and\nintroduce a novel dataset following this way of modeling. We run experiments on\nthis dataset to compare several deep learning methods and find that\nincorporating context information based on tensor multiplication in several\ncases outperforms the default way of information fusion (concatenation).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:14:38 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Karayil", "Tushar", ""], ["Blandfort", "Philipp", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "1810.06221", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh, and Afzel\n  Noore", "title": "Supervised COSMOS Autoencoder: Learning Beyond the Euclidean Loss!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders are unsupervised deep learning models used for learning\nrepresentations. In literature, autoencoders have shown to perform well on a\nvariety of tasks spread across multiple domains, thereby establishing\nwidespread applicability. Typically, an autoencoder is trained to generate a\nmodel that minimizes the reconstruction error between the input and the\nreconstructed output, computed in terms of the Euclidean distance. While this\ncan be useful for applications related to unsupervised reconstruction, it may\nnot be optimal for classification. In this paper, we propose a novel Supervised\nCOSMOS Autoencoder which utilizes a multi-objective loss function to learn\nrepresentations that simultaneously encode the (i) \"similarity\" between the\ninput and reconstructed vectors in terms of their direction, (ii)\n\"distribution\" of pixel values of the reconstruction with respect to the input\nsample, while also incorporating (iii) \"discriminability\" in the feature\nlearning pipeline. The proposed autoencoder model incorporates a Cosine\nsimilarity and Mahalanobis distance based loss function, along with supervision\nvia Mutual Information based loss. Detailed analysis of each component of the\nproposed model motivates its applicability for feature learning in different\nclassification tasks. The efficacy of Supervised COSMOS autoencoder is\ndemonstrated via extensive experimental evaluations on different image\ndatasets. The proposed model outperforms existing algorithms on MNIST,\nCIFAR-10, and SVHN databases. It also yields state-of-the-art results on\nCelebA, LFWA, Adience, and IJB-A databases for attribute prediction and face\nrecognition, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:19:38 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""]]}, {"id": "1810.06228", "submitter": "Yi Zhang", "authors": "Peng Bao, Wenjun Xia, Kang Yang, Jiliu Zhou, and Yi Zhang", "title": "Sparse-View CT Reconstruction via Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional dictionary learning based CT reconstruction methods are\npatch-based and the features learned with these methods often contain shifted\nversions of the same features. To deal with these problems, the convolutional\nsparse coding (CSC) has been proposed and introduced into various applications.\nIn this paper, inspired by the successful applications of CSC in the field of\nsignal processing, we propose a novel sparse-view CT reconstruction method\nbased on CSC with gradient regularization on feature maps. By directly working\non whole image, which need not to divide the image into overlapped patches like\ndictionary learning based methods, the proposed method can maintain more\ndetails and avoid the artifacts caused by patch aggregation. Experimental\nresults demonstrate that the proposed method has better performance than\nseveral existing algorithms in both qualitative and quantitative aspects.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:50:25 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bao", "Peng", ""], ["Xia", "Wenjun", ""], ["Yang", "Kang", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "1810.06231", "submitter": "Sameera Ramasinghe Mr.", "authors": "Sameera Ramasinghe, C.D. Athuralya, Salman Khan", "title": "A Context-aware Capsule Network for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed Capsule Network is a brain inspired architecture that\nbrings a new paradigm to deep learning by modelling input domain variations\nthrough vector based representations. Despite being a seminal contribution,\nCapsNet does not explicitly model structured relationships between the detected\nentities and among the capsule features for related inputs. Motivated by the\nworking of cortical network in human visual system, we seek to resolve CapsNet\nlimitations by proposing several intuitive modifications to the CapsNet\narchitecture. We introduce, (1) a novel routing weight initialization\ntechnique, (2) an improved CapsNet design that exploits semantic relationships\nbetween the primary capsule activations using a densely connected Conditional\nRandom Field and (3) a Cholesky transformation based correlation module to\nlearn a general priority scheme. Our proposed design allows CapsNet to scale\nbetter to more complex problems, such as the multi-label classification task,\nwhere semantically related categories co-exist with various interdependencies.\nWe present theoretical bases for our extensions and demonstrate significant\nimprovements on ADE20K scene dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 09:02:54 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 04:58:27 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Athuralya", "C. D.", ""], ["Khan", "Salman", ""]]}, {"id": "1810.06268", "submitter": "Mohammad Mahdi Haji-Esmaeili", "authors": "Mohammad Mahdi Haji-Esmaeili, Gholamali Montazer", "title": "Playing for Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the relative depth of a scene is a significant step towards\nunderstanding the general structure of the depicted scenery, the relations of\nentities in the scene and their interactions. When faced with the task of\nestimating depth without the use of Stereo images, we are dependent on the\navailability of large-scale depth datasets and high-capacity models to capture\nthe intrinsic nature of depth. Unfortunately, creating datasets of depth images\nis not a trivial task as the requirements for the camera mainly limits us to\nareas where we can provide the necessities for the camera to work.\n  In this work, we present a new depth dataset captured from Video Games in an\neasy and reproducible way. The nature of open-world video games gives us the\nability to capture high-quality depth maps in the wild without the\nconstrictions of Stereo cameras. Experiments on this dataset shows that using\nsuch synthetic datasets increases the accuracy of Monocular Depth Estimation in\nthe wild where other approaches usually fail to generalize.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 10:54:19 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Haji-Esmaeili", "Mohammad Mahdi", ""], ["Montazer", "Gholamali", ""]]}, {"id": "1810.06282", "submitter": "Aiga Suzuki", "authors": "Aiga Suzuki, Hidenori Sakanashi, Shoji Kido, Hayaru Shouno", "title": "Feature Representation Analysis of Deep Convolutional Neural Network\n  using Two-stage Feature Transfer -An Application for Diffuse Lung Disease\n  Classification-", "comments": "Preprint of the journal article to be published in IPSJ TOM-51.\n  Notice for the use of this material The copyright of this material is\n  retained by the Information Processing Society of Japan (IPSJ). This material\n  is published on this web site with the agreement of the author (s) and the\n  IPSJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a machine learning technique designed to improve\ngeneralization performance by using pre-trained parameters obtained from other\nlearning tasks. For image recognition tasks, many previous studies have\nreported that, when transfer learning is applied to deep neural networks,\nperformance improves, despite having limited training data. This paper proposes\na two-stage feature transfer learning method focusing on the recognition of\ntextural medical images. During the proposed method, a model is successively\ntrained with massive amounts of natural images, some textural images, and the\ntarget images. We applied this method to the classification task of textural\nX-ray computed tomography images of diffuse lung diseases. In our experiment,\nthe two-stage feature transfer achieves the best performance compared to a\nfrom-scratch learning and a conventional single-stage feature transfer. We also\ninvestigated the robustness of the target dataset, based on size. Two-stage\nfeature transfer shows better robustness than the other two learning methods.\nMoreover, we analyzed the feature representations obtained from DLDs imagery\ninputs for each feature transfer models using a visualization method. We showed\nthat the two-stage feature transfer obtains both edge and textural features of\nDLDs, which does not occur in conventional single-stage feature transfer\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 11:35:34 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Suzuki", "Aiga", ""], ["Sakanashi", "Hidenori", ""], ["Kido", "Shoji", ""], ["Shouno", "Hayaru", ""]]}, {"id": "1810.06323", "submitter": "Mehmet Yamac", "authors": "Aysen Degerli, Sinem Aslan, Mehmet Yamac, Bulent Sankur, Moncef\n  Gabbouj", "title": "Compressively Sensed Image Recognition", "comments": "6 pages, submitted/accepted, EUVIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive Sensing (CS) theory asserts that sparse signal reconstruction is\npossible from a small number of linear measurements. Although CS enables\nlow-cost linear sampling, it requires non-linear and costly reconstruction.\nRecent literature works show that compressive image classification is possible\nin CS domain without reconstruction of the signal. In this work, we introduce a\nDCT base method that extracts binary discriminative features directly from CS\nmeasurements. These CS measurements can be obtained by using (i) a random or a\npseudo-random measurement matrix, or (ii) a measurement matrix whose elements\nare learned from the training data to optimize the given classification task.\nWe further introduce feature fusion by concatenating Bag of Words (BoW)\nrepresentation of our binary features with one of the two state-of-the-art\nCNN-based feature vectors. We show that our fused feature outperforms the\nstate-of-the-art in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 12:55:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Degerli", "Aysen", ""], ["Aslan", "Sinem", ""], ["Yamac", "Mehmet", ""], ["Sankur", "Bulent", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1810.06327", "submitter": "Jinsong Zhang", "authors": "Jinsong Zhang, Rodrigo Verschae, Shohei Nobuhara and Jean-Fran\\c{c}ois\n  Lalonde", "title": "Deep Photovoltaic Nowcasting", "comments": "28 pages, 10 figure, 4 tables, preprint accepted to Solar Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the short-term power output of a photovoltaic panel is an\nimportant task for the efficient management of smart grids. Short-term\nforecasting at the minute scale, also known as nowcasting, can benefit from sky\nimages captured by regular cameras and installed close to the solar panel.\nHowever, estimating the weather conditions from these images---sun intensity,\ncloud appearance and movement, etc.---is a very challenging task that the\ncommunity has yet to solve with traditional computer vision techniques. In this\nwork, we propose to learn the relationship between sky appearance and the\nfuture photovoltaic power output using deep learning. We train several variants\nof convolutional neural networks which take historical photovoltaic power\nvalues and sky images as input and estimate photovoltaic power in a very short\nterm future. In particular, we compare three different architectures based on:\na multi-layer perceptron (MLP), a convolutional neural network (CNN), and a\nlong short term memory (LSTM) module. We evaluate our approach quantitatively\non a dataset of photovoltaic power values and corresponding images gathered in\nKyoto, Japan. Our experiments reveal that the MLP network, already used\nsimilarly in previous work, achieves an RMSE skill score of 7% over the\ncommonly-used persistence baseline on the 1-minute future photovoltaic power\nprediction task. Our CNN-based network improves upon this with a 12% skill\nscore. In contrast, our LSTM-based model, which can learn the temporal\ndependencies in the data, achieves a 21% RMSE skill score, thus outperforming\nall other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 13:00:31 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhang", "Jinsong", ""], ["Verschae", "Rodrigo", ""], ["Nobuhara", "Shohei", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1810.06411", "submitter": "Hafez Farazi", "authors": "Hafez Farazi and Sven Behnke", "title": "Real-Time Visual Tracking and Identification for a Team of Homogeneous\n  Humanoid Robots", "comments": "20th RoboCup International Symposium, Leipzig, Germany, 2016", "journal-ref": "20th RoboCup International Symposium, Leipzig, Germany, 2016", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of a team of humanoid robots to collaborate in completing a task is\nan increasingly important field of research. One of the challenges in achieving\ncollaboration, is mutual identification and tracking of the robots. This work\npresents a real-time vision-based approach to the detection and tracking of\nrobots of known appearance, based on the images captured by a stationary robot.\nA Histogram of Oriented Gradients descriptor is used to detect the robots and\nthe robot headings are estimated by a multiclass classifier. The tracked robots\nreport their own heading estimate from magnetometer readings. For tracking, a\ncost function based on position and heading is applied to each of the\ntracklets, and a globally optimal labeling of the detected robots is found\nusing the Hungarian algorithm. The complete identification and tracking system\nwas tested using two igus Humanoid Open Platform robots on a soccer field. We\nexpect that a similar system can be used with other humanoid robots, such as\nNao and DARwIn-OP\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:45:29 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 12:22:34 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "1810.06415", "submitter": "Amal Lahiani", "authors": "Amal Lahiani, Jacob Gildenblat, Irina Klaman, Shadi Albarqouni, Nassir\n  Navab, Eldad Klaiman", "title": "Virtualization of tissue staining in digital pathology using an\n  unsupervised deep learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological evaluation of tissue samples is a key practice in patient\ndiagnosis and drug development, especially in oncology. Historically,\nHematoxylin and Eosin (H&E) has been used by pathologists as a gold standard\nstaining. However, in many cases, various target specific stains, including\nimmunohistochemistry (IHC), are needed in order to highlight specific\nstructures in the tissue. As tissue is scarce and staining procedures are\ntedious, it would be beneficial to generate images of stained tissue virtually.\nVirtual staining could also generate in-silico multiplexing of different stains\non the same tissue segment. In this paper, we present a sample application that\ngenerates FAP-CK virtual IHC images from Ki67-CD8 real IHC images using an\nunsupervised deep learning approach based on CycleGAN. We also propose a method\nto deal with tiling artifacts caused by normalization layers and we validate\nour approach by comparing the results of tissue analysis algorithms for virtual\nand real images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:45:53 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Lahiani", "Amal", ""], ["Gildenblat", "Jacob", ""], ["Klaman", "Irina", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""], ["Klaiman", "Eldad", ""]]}, {"id": "1810.06453", "submitter": "Xiaole Zhao", "authors": "Xiaole Zhao, Yulun Zhang, Tao Zhang, Xueming Zou", "title": "Channel Splitting Network for Single MR Image Super-Resolution", "comments": "13 pages, 11 figures and 4 tables", "journal-ref": null, "doi": "10.1109/TIP.2019.2921882", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution magnetic resonance (MR) imaging is desirable in many clinical\napplications due to its contribution to more accurate subsequent analyses and\nearly clinical diagnoses. Single image super resolution (SISR) is an effective\nand cost efficient alternative technique to improve the spatial resolution of\nMR images. In the past few years, SISR methods based on deep learning\ntechniques, especially convolutional neural networks (CNNs), have achieved\nstate-of-the-art performance on natural images. However, the information is\ngradually weakened and training becomes increasingly difficult as the network\ndeepens. The problem is more serious for medical images because lacking high\nquality and effective training samples makes deep models prone to underfitting\nor overfitting. Nevertheless, many current models treat the hierarchical\nfeatures on different channels equivalently, which is not helpful for the\nmodels to deal with the hierarchical features discriminatively and targetedly.\nTo this end, we present a novel channel splitting network (CSN) to ease the\nrepresentational burden of deep models. The proposed CSN model divides the\nhierarchical features into two branches, i.e., residual branch and dense\nbranch, with different information transmissions. The residual branch is able\nto promote feature reuse, while the dense branch is beneficial to the\nexploration of new features. Besides, we also adopt the merge-and-run mapping\nto facilitate information integration between different branches. Extensive\nexperiments on various MR images, including proton density (PD), T1 and T2\nimages, show that the proposed CSN model achieves superior performance over\nother state-of-the-art SISR methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:15:16 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:05:22 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 05:57:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhao", "Xiaole", ""], ["Zhang", "Yulun", ""], ["Zhang", "Tao", ""], ["Zou", "Xueming", ""]]}, {"id": "1810.06455", "submitter": "Anders Eklund", "authors": "David Abramian and Anders Eklund", "title": "Refacing: reconstructing anonymized facial features using GANs", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging, 2019", "doi": "10.1109/ISBI.2019.8759515", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anonymization of medical images is necessary for protecting the identity of\nthe test subjects, and is therefore an essential step in data sharing. However,\nrecent developments in deep learning may raise the bar on the amount of\ndistortion that needs to be applied to guarantee anonymity. To test such\npossibilities, we have applied the novel CycleGAN unsupervised image-to-image\ntranslation framework on sagittal slices of T1 MR images, in order to\nreconstruct facial features from anonymized data. We applied the CycleGAN\nframework on both face-blurred and face-removed images. Our results show that\nface blurring may not provide adequate protection against malicious attempts at\nidentifying the subjects, while face removal provides more robust\nanonymization, but is still partially reversible.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:19:25 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 12:33:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Abramian", "David", ""], ["Eklund", "Anders", ""]]}, {"id": "1810.06464", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Ghassan AlRegib", "title": "CSV: Image Quality Assessment Based on Color, Structure, and Visual\n  System", "comments": "31 pages, 9 figures, 7 tables", "journal-ref": "Signal Processing: Image Communication, Volume 48, 2016, Pages\n  92-103, ISSN 0923-5965", "doi": "10.1016/j.image.2016.08.008", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a full-reference image quality estimator based on color,\nstructure, and visual system characteristics denoted as CSV. In contrast to the\nmajority of existing methods, we quantify perceptual color degradations rather\nthan absolute pixel-wise changes. We use the CIEDE2000 color difference\nformulation to quantify low-level color degradations and the Earth Mover's\nDistance between color name descriptors to measure significant color\ndegradations. In addition to the perceptual color difference, CSV also contains\nstructural and perceptual differences. Structural feature maps are obtained by\nmean subtraction and divisive normalization, and perceptual feature maps are\nobtained from contrast sensitivity formulations of retinal ganglion cells. The\nproposed quality estimator CSV is tested on the LIVE, the Multiply Distorted\nLIVE, and the TID 2013 databases, and it is always among the top two performing\nquality estimators in terms of at least ranking, monotonic behavior or\nlinearity.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:33:14 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:36:27 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1810.06470", "submitter": "Numan Khurshid", "authors": "Mohbat Tharani, Numan Khurshid, and Murtaza Taj", "title": "Unsupervised Deep Features for Remote Sensing Image Matching via\n  Discriminator Network", "comments": "13 Pages, 7 Figures", "journal-ref": null, "doi": "10.1109/TGRS.2019.2951820", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of deep perceptual networks brought about a paradigm shift in\nmachine vision and image perception. Image apprehension lately carried out by\nhand-crafted features in the latent space have been replaced by deep features\nacquired from supervised networks for improved understanding. However, such\ndeep networks require strict supervision with a substantial amount of the\nlabeled data for authentic training process. These methods perform poorly in\ndomains lacking labeled data especially in case of remote sensing image\nretrieval. Resolving this, we propose an unsupervised encoder-decoder feature\nfor remote sensing image matching (RSIM). Moreover, we replace the conventional\ndistance metrics with a deep discriminator network to identify the similarity\nof the image pairs. To the best of our knowledge, discriminator network has\nnever been used before for solving RSIM problem. Results have been validated\nwith two publicly available benchmark remote sensing image datasets. The\ntechnique has also been investigated for content-based remote sensing image\nretrieval (CBRSIR); one of the widely used applications of RSIM. Results\ndemonstrate that our technique supersedes the state-of-the-art methods used for\nunsupervised image matching with mean average precision (mAP) of 81%, and image\nretrieval with an overall improvement in mAP score of about 12%.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:39:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Tharani", "Mohbat", ""], ["Khurshid", "Numan", ""], ["Taj", "Murtaza", ""]]}, {"id": "1810.06498", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Hyeonsoo Moon, Shunxing Bao, Albert Assad,\n  Tamara K. Moyo, Michael R. Savona, Richard G. Abramson, Bennett A. Landman", "title": "SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth", "comments": "IEEE Transactions on Medical Imaging (TMI)", "journal-ref": "\"Synseg-net: Synthetic segmentation without target modality ground\n  truth.\" IEEE transactions on medical imaging 38, no. 4 (2018): 1016-1025", "doi": "10.1109/TMI.2018.2876633", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key limitation of deep convolutional neural networks (DCNN) based image\nsegmentation methods is the lack of generalizability. Manually traced training\nimages are typically required when segmenting organs in a new imaging modality\nor from distinct disease cohort. The manual efforts can be alleviated if the\nmanually traced images in one imaging modality (e.g., MRI) are able to train a\nsegmentation network for another imaging modality (e.g., CT). In this paper, we\npropose an end-to-end synthetic segmentation network (SynSeg-Net) to train a\nsegmentation network for a target imaging modality without having manual\nlabels. SynSeg-Net is trained by using (1) unpaired intensity images from\nsource and target modalities, and (2) manual labels only from source modality.\nSynSeg-Net is enabled by the recent advances of cycle generative adversarial\nnetworks (CycleGAN) and DCNN. We evaluate the performance of the SynSeg-Net on\ntwo experiments: (1) MRI to CT splenomegaly synthetic segmentation for\nabdominal images, and (2) CT to MRI total intracranial volume synthetic\nsegmentation (TICV) for brain images. The proposed end-to-end approach achieved\nsuperior performance to two stage methods. Moreover, the SynSeg-Net achieved\ncomparable performance to the traditional segmentation network using target\nmodality labels in certain scenarios. The source code of SynSeg-Net is publicly\navailable (https://github.com/MASILab/SynSeg-Net).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 16:23:08 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 18:45:26 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Moon", "Hyeonsoo", ""], ["Bao", "Shunxing", ""], ["Assad", "Albert", ""], ["Moyo", "Tamara K.", ""], ["Savona", "Michael R.", ""], ["Abramson", "Richard G.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1810.06514", "submitter": "Anpei Chen", "authors": "Anpei Chen, Minye Wu, Yingliang Zhang, Nianyi Li, Jie Lu, Shenghua\n  Gao, and Jingyi Yu", "title": "Deep Surface Light Fields", "comments": null, "journal-ref": null, "doi": "10.1145/3203192", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A surface light field represents the radiance of rays originating from any\npoints on the surface in any directions. Traditional approaches require\nultra-dense sampling to ensure the rendering quality. In this paper, we present\na novel neural network based technique called deep surface light field or DSLF\nto use only moderate sampling for high fidelity rendering. DSLF automatically\nfills in the missing data by leveraging different sampling patterns across the\nvertices and at the same time eliminates redundancies due to the network's\nprediction capability. For real data, we address the image registration problem\nas well as conduct texture-aware remeshing for aligning texture edges with\nvertices to avoid blurring. Comprehensive experiments show that DSLF can\nfurther achieve high data compression ratio while facilitating real-time\nrendering on the GPU.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 16:56:58 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chen", "Anpei", ""], ["Wu", "Minye", ""], ["Zhang", "Yingliang", ""], ["Li", "Nianyi", ""], ["Lu", "Jie", ""], ["Gao", "Shenghua", ""], ["Yu", "Jingyi", ""]]}, {"id": "1810.06543", "submitter": "Roozbeh Mottaghi", "authors": "Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi", "title": "Visual Semantic Navigation using Scene Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do humans navigate to target objects in novel scenes? Do we use the\nsemantic/functional priors we have built over years to efficiently search and\nnavigate? For example, to search for mugs, we search cabinets near the coffee\nmachine and for fruits we try the fridge. In this work, we focus on\nincorporating semantic priors in the task of semantic navigation. We propose to\nuse Graph Convolutional Networks for incorporating the prior knowledge into a\ndeep reinforcement learning framework. The agent uses the features from the\nknowledge graph to predict the actions. For evaluation, we use the AI2-THOR\nframework. Our experiments show how semantic knowledge improves performance\nsignificantly. More importantly, we show improvement in generalization to\nunseen scenes and/or objects. The supplementary video can be accessed at the\nfollowing link: https://youtu.be/otKjuO805dE .\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 17:45:02 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Yang", "Wei", ""], ["Wang", "Xiaolong", ""], ["Farhadi", "Ali", ""], ["Gupta", "Abhinav", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "1810.06544", "submitter": "Nicholas Rhinehart", "authors": "Nicholas Rhinehart, Rowan McAllister, Sergey Levine", "title": "Deep Imitative Models for Flexible Inference, Planning, and Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation Learning (IL) is an appealing approach to learn desirable\nautonomous behavior. However, directing IL to achieve arbitrary goals is\ndifficult. In contrast, planning-based algorithms use dynamics models and\nreward functions to achieve goals. Yet, reward functions that evoke desirable\nbehavior are often difficult to specify. In this paper, we propose Imitative\nModels to combine the benefits of IL and goal-directed planning. Imitative\nModels are probabilistic predictive models of desirable behavior able to plan\ninterpretable expert-like trajectories to achieve specified goals. We derive\nfamilies of flexible goal objectives, including constrained goal regions,\nunconstrained goal sets, and energy-based goals. We show that our method can\nuse these objectives to successfully direct behavior. Our method substantially\noutperforms six IL approaches and a planning-based approach in a dynamic\nsimulated autonomous driving task, and is efficiently learned from expert\ndemonstrations without online data collection. We also show our approach is\nrobust to poorly specified goals, such as goals on the wrong side of the road.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 17:51:03 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 20:07:49 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 19:48:56 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 00:13:58 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Rhinehart", "Nicholas", ""], ["McAllister", "Rowan", ""], ["Levine", "Sergey", ""]]}, {"id": "1810.06553", "submitter": "Javier Marin", "authors": "Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia\n  Salvador, Yusuf Aytar, Ingmar Weber, Antonio Torralba", "title": "Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking\n  Recipes and Food Images", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Recipe1M+, a new large-scale, structured corpus\nof over one million cooking recipes and 13 million food images. As the largest\npublicly available collection of recipe data, Recipe1M+ affords the ability to\ntrain high-capacity modelson aligned, multimodal data. Using these data, we\ntrain a neural network to learn a joint embedding of recipes and images that\nyields impressive results on an image-recipe retrieval task. Moreover, we\ndemonstrate that regularization via the addition of a high-level classification\nobjective both improves retrieval performance to rival that of humans and\nenables semantic vector arithmetic. We postulate that these embeddings will\nprovide a basis for further exploration of the Recipe1M+ dataset and food and\ncooking in general. Code, data and models are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 20:51:41 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 20:25:22 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Marin", "Javier", ""], ["Biswas", "Aritro", ""], ["Ofli", "Ferda", ""], ["Hynes", "Nicholas", ""], ["Salvador", "Amaia", ""], ["Aytar", "Yusuf", ""], ["Weber", "Ingmar", ""], ["Torralba", "Antonio", ""]]}, {"id": "1810.06611", "submitter": "Aydogan Ozcan", "authors": "Tairan Liu, Kevin de Haan, Yair Rivenson, Zhensong Wei, Xin Zeng, Yibo\n  Zhang, Aydogan Ozcan", "title": "Deep learning-based super-resolution in coherent imaging systems", "comments": "18 pages, 9 figures, 3 tables", "journal-ref": "Scientific Reports (2019)", "doi": "10.1038/s41598-019-40554-1", "report-no": null, "categories": "cs.CV cs.LG physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework based on a generative adversarial\nnetwork (GAN) to perform super-resolution in coherent imaging systems. We\ndemonstrate that this framework can enhance the resolution of both pixel\nsize-limited and diffraction-limited coherent imaging systems. We\nexperimentally validated the capabilities of this deep learning-based coherent\nimaging approach by super-resolving complex images acquired using a lensfree\non-chip holographic microscope, the resolution of which was pixel size-limited.\nUsing the same GAN-based approach, we also improved the resolution of a\nlens-based holographic imaging system that was limited in resolution by the\nnumerical aperture of its objective lens. This deep learning-based\nsuper-resolution framework can be broadly applied to enhance the\nspace-bandwidth product of coherent imaging systems using image data and\nconvolutional neural networks, and provides a rapid, non-iterative method for\nsolving inverse image reconstruction or enhancement problems in optics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:55:26 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Liu", "Tairan", ""], ["de Haan", "Kevin", ""], ["Rivenson", "Yair", ""], ["Wei", "Zhensong", ""], ["Zeng", "Xin", ""], ["Zhang", "Yibo", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1810.06612", "submitter": "Tejas Sudharshan Mathai", "authors": "Tejas Sudharshan Mathai, Kira Lathrop, and John Galeotti", "title": "Learning to Segment Corneal Tissue Interfaces in OCT Images", "comments": "Accepted to ISBI 2019. 5 pages. First version received by IEEE ISBI\n  on 21st Sept 2018. This work has been submitted to the IEEE for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and repeatable delineation of corneal tissue interfaces is necessary\nfor surgical planning during anterior segment interventions, such as\nKeratoplasty. Designing an approach to identify interfaces, which generalizes\nto datasets acquired from different Optical Coherence Tomographic (OCT)\nscanners, is paramount. In this paper, we present a Convolutional Neural\nNetwork (CNN) based framework called CorNet that can accurately segment three\ncorneal interfaces across datasets obtained with different scan settings from\ndifferent OCT scanners. Extensive validation of the approach was conducted\nacross all imaged datasets. To the best of our knowledge, this is the first\ndeep learning based approach to segment both anterior and posterior corneal\ntissue interfaces. Our errors are 2x lower than non-proprietary\nstate-of-the-art corneal tissue interface segmentation algorithms, which\ninclude image analysis-based and deep learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:56:07 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 01:09:59 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 21:43:45 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 17:28:34 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Mathai", "Tejas Sudharshan", ""], ["Lathrop", "Kira", ""], ["Galeotti", "John", ""]]}, {"id": "1810.06621", "submitter": "Karim Armanious", "authors": "Karim Armanious, Youssef Mecky, Sergios Gatidis, Bin Yang", "title": "Adversarial Inpainting of Medical Image Modalities", "comments": "To be submitted to ICASSP 2019", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682677", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous factors could lead to partial deteriorations of medical images. For\nexample, metallic implants will lead to localized perturbations in MRI scans.\nThis will affect further post-processing tasks such as attenuation correction\nin PET/MRI or radiation therapy planning. In this work, we propose the\ninpainting of medical images via Generative Adversarial Networks (GANs). The\nproposed framework incorporates two patch-based discriminator networks with\nadditional style and perceptual losses for the inpainting of missing\ninformation in realistically detailed and contextually consistent manner. The\nproposed framework outperformed other natural image inpainting techniques both\nqualitatively and quantitatively on two different medical modalities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 19:14:16 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Armanious", "Karim", ""], ["Mecky", "Youssef", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "1810.06631", "submitter": "Dogancan Temel", "authors": "D. Temel and M. Prabhushankar and G. AlRegib", "title": "UNIQUE: Unsupervised Image Quality Estimation", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": "D. Temel, M. Prabhushankar and G. AlRegib, \"UNIQUE: Unsupervised\n  Image Quality Estimation,\" in IEEE Signal Processing Letters, vol. 23, no.\n  10, pp. 1414-1418, Oct. 2016", "doi": "10.1109/LSP.2016.2601119", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we estimate perceived image quality using sparse\nrepresentations obtained from generic image databases through an unsupervised\nlearning approach. A color space transformation, a mean subtraction, and a\nwhitening operation are used to enhance descriptiveness of images by reducing\nspatial redundancy; a linear decoder is used to obtain sparse representations;\nand a thresholding stage is used to formulate suppression mechanisms in a\nvisual system. A linear decoder is trained with 7 GB worth of data, which\ncorresponds to 100,000 8x8 image patches randomly obtained from nearly 1,000\nimages in the ImageNet 2013 database. A patch-wise training approach is\npreferred to maintain local information. The proposed quality estimator UNIQUE\nis tested on the LIVE, the Multiply Distorted LIVE, and the TID 2013 databases\nand compared with thirteen quality estimators. Experimental results show that\nUNIQUE is generally a top performing quality estimator in terms of accuracy,\nconsistency, linearity, and monotonic behavior.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 19:36:34 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:31:22 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Temel", "D.", ""], ["Prabhushankar", "M.", ""], ["AlRegib", "G.", ""]]}, {"id": "1810.06635", "submitter": "Anoop Toffy", "authors": "Maharajan Chellapriyadharshini, Anoop Toffy, Srinivasa Raghavan K. M.,\n  V Ramasubramanian", "title": "Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model\n  Refinement for a Low Resource Indian Language", "comments": null, "journal-ref": "Proc. Interspeech 2018", "doi": "10.21437/Interspeech.2018-2486", "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of efficient acoustic-model refinement (continuous\nretraining) using semi-supervised and active learning for a low resource Indian\nlanguage, wherein the low resource constraints are having i) a small labeled\ncorpus from which to train a baseline `seed' acoustic model and ii) a large\ntraining corpus without orthographic labeling or from which to perform a data\nselection for manual labeling at low costs. The proposed semi-supervised\nlearning decodes the unlabeled large training corpus using the seed model and\nthrough various protocols, selects the decoded utterances with high reliability\nusing confidence levels (that correlate to the WER of the decoded utterances)\nand iterative bootstrapping. The proposed active learning protocol uses\nconfidence level based metric to select the decoded utterances from the large\nunlabeled corpus for further labeling. The semi-supervised learning protocols\ncan offer a WER reduction, from a poorly trained seed model, by as much as 50%\nof the best WER-reduction realizable from the seed model's WER, if the large\ncorpus were labeled and used for acoustic-model training. The active learning\nprotocols allow that only 60% of the entire training corpus be manually\nlabeled, to reach the same performance as the entire data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 07:23:42 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Chellapriyadharshini", "Maharajan", ""], ["Toffy", "Anoop", ""], ["M.", "Srinivasa Raghavan K.", ""], ["Ramasubramanian", "V", ""]]}, {"id": "1810.06679", "submitter": "Ren Yang", "authors": "Jiaxin Lu, Mai Xu, Ren Yang, Zulin Wang", "title": "Understanding and Predicting the Memorability of Outdoor Natural Scenes", "comments": "arXiv admin note: some text overlap with arXiv:1808.08754", "journal-ref": "IEEE Transactions on Image Processing, 2020", "doi": "10.1109/TIP.2020.2975957", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memorability measures how easily an image is to be memorized after glancing,\nwhich may contribute to designing magazine covers, tourism publicity materials,\nand so forth. Recent works have shed light on the visual features that make\ngeneric images, object images or face photographs memorable. However, these\nmethods are not able to effectively predict the memorability of outdoor natural\nscene images. To overcome this shortcoming of previous works, in this paper, we\nprovide an attempt to answer: \"what exactly makes outdoor natural scenes\nmemorable\". To this end, we first establish a large-scale outdoor natural scene\nimage memorability (LNSIM) database, containing 2,632 outdoor natural scene\nimages with their ground truth memorability scores and the multi-label scene\ncategory annotations. Then, similar to previous works, we mine our database to\ninvestigate how low-, middle- and high-level handcrafted features affect the\nmemorability of outdoor natural scenes. In particular, we find that the\nhigh-level feature of scene category is rather correlated with outdoor natural\nscene memorability, and the deep features learnt by deep neural network (DNN)\nare also effective in predicting the memorability scores. Moreover, combining\nthe deep features with the category feature can further boost the performance\nof memorability prediction. Therefore, we propose an end-to-end DNN based\noutdoor natural scene memorability (DeepNSM) predictor, which takes advantage\nof the learned category-related features. Then, the experimental results\nvalidate the effectiveness of our DeepNSM model, exceeding the state-of-the-art\nmethods. Finally, we try to understand the reason of the good performance for\nour DeepNSM model, and also study the cases that our DeepNSM model succeeds or\nfails to accurately predict the memorability of outdoor natural scenes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 09:25:07 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 01:42:31 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 08:48:57 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 10:24:20 GMT"}, {"version": "v5", "created": "Sat, 29 Feb 2020 10:19:37 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lu", "Jiaxin", ""], ["Xu", "Mai", ""], ["Yang", "Ren", ""], ["Wang", "Zulin", ""]]}, {"id": "1810.06693", "submitter": "Guang Yang A", "authors": "Jin Zhu and Guang Yang and Pietro Lio", "title": "Lesion Focused Super-Resolution", "comments": "4 pages, 2 figures, 1 table, Accepted as Oral Presentation by the\n  SPIE Medical Imaging Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) for image enhancement has great importance in medical\nimage applications. Broadly speaking, there are two types of SR, one requires\nmultiple low resolution (LR) images from different views of the same object to\nbe reconstructed to the high resolution (HR) output, and the other one relies\non the learning from a large amount of training datasets, i.e., LR-HR pairs. In\nreal clinical environment, acquiring images from multi-views is expensive and\nsometimes infeasible. In this paper, we present a novel Generative Adversarial\nNetworks (GAN) based learning framework to achieve SR from its LR version. By\nperforming simulation based studies on the Multimodal Brain Tumor Segmentation\nChallenge (BraTS) datasets, we demonstrate the efficacy of our method in\napplication of brain tumor MRI enhancement. Compared to bilinear interpolation\nand other state-of-the-art SR methods, our model is lesion focused, which is\nnot only resulted in better perceptual image quality without blurring, but also\nmore efficient and directly benefit for the following clinical tasks, e.g.,\nlesion detection and abnormality enhancement. Therefore, we can envisage the\napplication of our SR method to boost image spatial resolution while\nmaintaining crucial diagnostic information for further clinical tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 21:02:24 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Zhu", "Jin", ""], ["Yang", "Guang", ""], ["Lio", "Pietro", ""]]}, {"id": "1810.06766", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Mostafa El-Khamy, Jungwon Lee", "title": "DN-ResNet: Efficient Deep Residual Network for Image Denoising", "comments": null, "journal-ref": "Asian Conference of Computer Vision 2018", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep learning approach to blind denoising of images without complete\nknowledge of the noise statistics is considered. We propose DN-ResNet, which is\na deep convolutional neural network (CNN) consisting of several residual blocks\n(ResBlocks). With cascade training, DN-ResNet is more accurate and more\ncomputationally efficient than the state of art denoising networks. An\nedge-aware loss function is further utilized in training DN-ResNet, so that the\ndenoising results have better perceptive quality compared to conventional loss\nfunction. Next, we introduce the depthwise separable DN-ResNet (DS-DN-ResNet)\nutilizing the proposed Depthwise Seperable ResBlock (DS-ResBlock) instead of\nstandard ResBlock, which has much less computational cost. DS-DN-ResNet is\nincrementally evolved by replacing the ResBlocks in DN-ResNet by DS-ResBlocks\nstage by stage. As a result, high accuracy and good computational efficiency\nare achieved concurrently. Whereas previous state of art deep learning methods\nfocused on denoising either Gaussian or Poisson corrupted images, we consider\ndenoising images having the more practical Poisson with additive Gaussian noise\nas well. The results show that DN-ResNets are more efficient, robust, and\nperform better denoising than current state of art deep learning methods, as\nwell as the popular variants of the BM3D algorithm, in cases of blind and\nnon-blind denoising of images corrupted with Poisson, Gaussian or\nPoisson-Gaussian noise. Our network also works well for other image enhancement\ntask such as compressed image restoration.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 00:33:09 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Ren", "Haoyu", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1810.06767", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Tom Drummond, Ian Reid, and Gustavo Carneiro", "title": "Approximate Fisher Information Matrix to Characterise the Training of\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel methodology for characterising the\nperformance of deep learning networks (ResNets and DenseNet) with respect to\ntraining convergence and generalisation as a function of mini-batch size and\nlearning rate for image classification. This methodology is based on novel\nmeasurements derived from the eigenvalues of the approximate Fisher information\nmatrix, which can be efficiently computed even for high capacity deep models.\nOur proposed measurements can help practitioners to monitor and control the\ntraining process (by actively tuning the mini-batch size and learning rate) to\nallow for good training convergence and generalisation. Furthermore, the\nproposed measurements also allow us to show that it is possible to optimise the\ntraining process with a new dynamic sampling training approach that\ncontinuously and automatically change the mini-batch size and learning rate\nduring the training process. Finally, we show that the proposed dynamic\nsampling training approach has a faster training time and a competitive\nclassification accuracy compared to the current state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 00:37:03 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Liao", "Zhibin", ""], ["Drummond", "Tom", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1810.06797", "submitter": "Dongdong Zeng", "authors": "Dongdong Zeng, Ming Zhu, Hang Yang", "title": "A Robust Local Binary Similarity Pattern for Foreground Object Detection", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and fast extraction of the foreground object is one of the most\nsignificant issues to be solved due to its important meaning for object\ntracking and recognition in video surveillance. Although many foreground object\ndetection methods have been proposed in the recent past, it is still regarded\nas a tough problem due to illumination variations and dynamic backgrounds\nchallenges. In this paper, we propose a robust foreground object detection\nmethod with two aspects of contributions. First, we propose a robust texture\noperator named Robust Local Binary Similarity Pattern (RLBSP), which shows\nstrong robustness to illumination variations and dynamic backgrounds. Second, a\ncombination of color and texture features are used to characterize pixel\nrepresentations, which compensate each other to make full use of their own\nadvantages. Comprehensive experiments evaluated on the CDnet 2012 dataset\ndemonstrate that the proposed method performs favorably against\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 03:30:15 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 09:50:40 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Zeng", "Dongdong", ""], ["Zhu", "Ming", ""], ["Yang", "Hang", ""]]}, {"id": "1810.06827", "submitter": "Sameera Ramasinghe Mr.", "authors": "Sameera Ramasinghe, Jathushan Rajasegaran, Vinoj Jayasundara, Kanchana\n  Ranasinghe, Ranga Rodrigo, Ajith A. Pasqual", "title": "Combined Static and Motion Features for Deep-Networks Based Activity\n  Recognition in Videos", "comments": null, "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology\n  (2017)", "doi": "10.1109/TCSVT.2017.2760858", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition in videos in a deep-learning setting---or\notherwise---uses both static and pre-computed motion components. The method of\ncombining the two components, whilst keeping the burden on the deep network\nless, still remains uninvestigated. Moreover, it is not clear what the level of\ncontribution of individual components is, and how to control the contribution.\nIn this work, we use a combination of CNN-generated static features and motion\nfeatures in the form of motion tubes. We propose three schemas for combining\nstatic and motion components: based on a variance ratio, principal components,\nand Cholesky decomposition. The Cholesky decomposition based method allows the\ncontrol of contributions. The ratio given by variance analysis of static and\nmotion features match well with the experimental optimal ratio used in the\nCholesky decomposition based method. The resulting activity recognition system\nis better or on par with existing state-of-the-art when tested with three\npopular datasets. The findings also enable us to characterize a dataset with\nrespect to its richness in motion information.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 06:04:35 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Rajasegaran", "Jathushan", ""], ["Jayasundara", "Vinoj", ""], ["Ranasinghe", "Kanchana", ""], ["Rodrigo", "Ranga", ""], ["Pasqual", "Ajith A.", ""]]}, {"id": "1810.06859", "submitter": "Hong Chen", "authors": "Hong Chen, Yifei Huang, Hideki Nakayama", "title": "Semantic Aware Attention Based Deep Object Co-segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object co-segmentation is the task of segmenting the same objects from\nmultiple images. In this paper, we propose the Attention Based Object\nCo-Segmentation for object co-segmentation that utilize a novel attention\nmechanism in the bottleneck layer of deep neural network for the selection of\nsemantically related features. Furthermore, we take the benefit of attention\nlearner and propose an algorithm to segment multi-input images in linear time\ncomplexity. Experiment results demonstrate that our model achieves state of the\nart performance on multiple datasets, with a significant reduction of\ncomputational time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 07:57:04 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Chen", "Hong", ""], ["Huang", "Yifei", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1810.06877", "submitter": "Kele Xu", "authors": "Kele Xu, Haibo Mi, Dawei Feng, Huaimin Wang, Chuan Chen, Zibin Zheng,\n  Xu Lan", "title": "Collaborative Deep Learning Across Multiple Data Centers", "comments": "Submitted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valuable training data is often owned by independent organizations and\nlocated in multiple data centers. Most deep learning approaches require to\ncentralize the multi-datacenter data for performance purpose. In practice,\nhowever, it is often infeasible to transfer all data to a centralized data\ncenter due to not only bandwidth limitation but also the constraints of privacy\nregulations. Model averaging is a conventional choice for data parallelized\ntraining, but its ineffectiveness is claimed by previous studies as deep neural\nnetworks are often non-convex. In this paper, we argue that model averaging can\nbe effective in the decentralized environment by using two strategies, namely,\nthe cyclical learning rate and the increased number of epochs for local model\ntraining. With the two strategies, we show that model averaging can provide\ncompetitive performance in the decentralized mode compared to the\ndata-centralized one. In a practical environment with multiple data centers, we\nconduct extensive experiments using state-of-the-art deep network architectures\non different types of data. Results demonstrate the effectiveness and\nrobustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 08:33:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Xu", "Kele", ""], ["Mi", "Haibo", ""], ["Feng", "Dawei", ""], ["Wang", "Huaimin", ""], ["Chen", "Chuan", ""], ["Zheng", "Zibin", ""], ["Lan", "Xu", ""]]}, {"id": "1810.06889", "submitter": "Vincent Andrearczyk", "authors": "Vincent Andrearczyk and Adrien Depeursinge", "title": "Rotational 3D Texture Classification Using Group Equivariant CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) traditionally encode translation\nequivariance via the convolution operation. Generalization to other\ntransformations has recently received attraction to encode the knowledge of the\ndata geometry in group convolution operations. Equivariance to rotation is\nparticularly important for 3D image analysis due to the large diversity of\npossible pattern orientations. 3D texture is a particularly important cue for\nthe analysis of medical images such as CT and MRI scans as it describes\ndifferent types of tissues and lesions. In this paper, we evaluate the use of\n3D group equivariant CNNs accounting for the simplified group of right-angle\nrotations to classify 3D synthetic textures from a publicly available dataset.\nThe results validate the importance of rotation equivariance in a controlled\nsetup and yet motivate the use of a finer coverage of orientations in order to\nobtain equivariance to realistic rotations present in 3D textures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 09:22:36 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Andrearczyk", "Vincent", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "1810.06892", "submitter": "Aiga Suzuki", "authors": "Aiga Suzuki, Hayaru Shouno", "title": "A Generative Model of Textures Using Hierarchical Probabilistic\n  Principal Component Analysis", "comments": "6 pages, 9 figures; A proceeding of PDPTA'17 accepted as an oral\n  presentation", "journal-ref": "Proc. of the 2017 Intl. Conference on Parallel and Distributed\n  Processing Techniques and Applications (PDPTA'17), CSREA Press, pp.333-338,\n  (2017)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling of textures in natural images is an important task to make a\nmicroscopic model of natural images. Portilla and Simoncelli proposed a\ngenerative texture model, which is based on the mechanism of visual systems in\nbrains, with a set of texture features and a feature matching. On the other\nhand, the texture features, used in Portillas' model, have redundancy between\nits components came from typical natural textures. In this paper, we propose a\ncontracted texture model which provides a dimension reduction for the\nPortillas' feature. This model is based on a hierarchical principal components\nanalysis using known group structure of the feature. In the experiment, we\nreveal effective dimensions to describe texture is fewer than the original\ndescription. Moreover, we also demonstrate how well the textures can be\nsynthesized from the contracted texture representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 09:24:19 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Suzuki", "Aiga", ""], ["Shouno", "Hayaru", ""]]}, {"id": "1810.06933", "submitter": "Dennis Eschweiler", "authors": "Dennis Eschweiler, Thiago V. Spina, Rohan C. Choudhury, Elliot\n  Meyerowitz, Alexandre Cunha, Johannes Stegmaier", "title": "CNN-based Preprocessing to Optimize Watershed-based Cell Segmentation in\n  3D Confocal Microscopy Images", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantitative analysis of cellular membranes helps understanding\ndevelopmental processes at the cellular level. Particularly 3D microscopic\nimage data offers valuable insights into cell dynamics, but error-free\nautomatic segmentation remains challenging due to the huge amount of data\ngenerated and strong variations in image intensities. In this paper, we propose\na new 3D segmentation approach which combines the discriminative power of\nconvolutional neural networks (CNNs) for preprocessing and investigates the\nperformance of three watershed-based postprocessing strategies (WS), which are\nwell suited to segment object shapes, even when supplied with vague seed and\nboundary constraints. To leverage the full potential of the watershed\nalgorithm, the multi-instance segmentation problem is initially interpreted as\nthree-class semantic segmentation problem, which in turn is well-suited for the\napplication of CNNs. Using manually annotated 3D confocal microscopy images of\nArabidopsis thaliana, we show the superior performance of the proposed method\ncompared to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 11:30:54 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Eschweiler", "Dennis", ""], ["Spina", "Thiago V.", ""], ["Choudhury", "Rohan C.", ""], ["Meyerowitz", "Elliot", ""], ["Cunha", "Alexandre", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "1810.06935", "submitter": "Yue Lu", "authors": "Yue Lu, Yun Zhou, Zhuqing Jiang, Xiaoqiang Guo, Zixuan Yang", "title": "Channel Attention and Multi-level Features Fusion for Single Image\n  Super-Resolution", "comments": "4 pages, 3 figures, Accepted as an oral presentation at VCIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated superior performance\nin super-resolution (SR). However, most CNN-based SR methods neglect the\ndifferent importance among feature channels or fail to take full advantage of\nthe hierarchical features. To address these issues, this paper presents a novel\nrecursive unit. Firstly, at the beginning of each unit, we adopt a compact\nchannel attention mechanism to adaptively recalibrate the channel importance of\ninput features. Then, the multi-level features, rather than only deep-level\nfeatures, are extracted and fused. Additionally, we find that it will force our\nmodel to learn more details by using the learnable upsampling method (i.e.,\ntransposed convolution) only on residual branch (instead of using it both on\nresidual branch and identity branch) while using the bicubic interpolation on\nthe other branch. Analytic experiments show that our method achieves\ncompetitive results compared with the state-of-the-art methods and maintains\nfaster speed as well.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 11:33:09 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Lu", "Yue", ""], ["Zhou", "Yun", ""], ["Jiang", "Zhuqing", ""], ["Guo", "Xiaoqiang", ""], ["Yang", "Zixuan", ""]]}, {"id": "1810.06936", "submitter": "Alberto Garcia-Garcia", "authors": "Pablo Martinez-Gonzalez, Sergiu Oprea, Alberto Garcia-Garcia, Alvaro\n  Jover-Alvarez, Sergio Orts-Escolano, Jose Garcia-Rodriguez", "title": "UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation", "comments": "Published in Virtual Reality journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven algorithms have surpassed traditional techniques in almost every\naspect in robotic vision problems. Such algorithms need vast amounts of quality\ndata to be able to work properly after their training process. Gathering and\nannotating that sheer amount of data in the real world is a time-consuming and\nerror-prone task. Those problems limit scale and quality. Synthetic data\ngeneration has become increasingly popular since it is faster to generate and\nautomatic to annotate. However, most of the current datasets and environments\nlack realism, interactions, and details from the real world. UnrealROX is an\nenvironment built over Unreal Engine 4 which aims to reduce that reality gap by\nleveraging hyperrealistic indoor scenes that are explored by robot agents which\nalso interact with objects in a visually realistic manner in that simulated\nworld. Photorealistic scenes and robots are rendered by Unreal Engine into a\nvirtual reality headset which captures gaze so that a human operator can move\nthe robot and use controllers for the robotic hands; scene information is\ndumped on a per-frame basis so that it can be reproduced offline to generate\nraw data and ground truth annotations. This virtual reality environment enables\nrobotic vision researchers to generate realistic and visually plausible data\nwith full ground truth for a wide variety of problems such as class and\ninstance semantic segmentation, object detection, depth estimation, visual\ngrasping, and navigation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 11:43:50 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 17:58:02 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Martinez-Gonzalez", "Pablo", ""], ["Oprea", "Sergiu", ""], ["Garcia-Garcia", "Alberto", ""], ["Jover-Alvarez", "Alvaro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1810.06951", "submitter": "Weilin Huang", "authors": "Weifeng Ge and Weilin Huang and Dengke Dong and Matthew R. Scott", "title": "Deep Metric Learning with Hierarchical Triplet Loss", "comments": "Published in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical triplet loss (HTL) capable of automatically\ncollecting informative training samples (triplets) via a defined hierarchical\ntree that encodes global context information. This allows us to cope with the\nmain limitation of random sampling in training a conventional triplet loss,\nwhich is a central issue for deep metric learning. Our main contributions are\ntwo-fold. (i) we construct a hierarchical class-level tree where neighboring\nclasses are merged recursively. The hierarchical structure naturally captures\nthe intrinsic data distribution over the whole database. (ii) we formulate the\nproblem of triplet collection by introducing a new violate margin, which is\ncomputed dynamically based on the designed hierarchical tree. This allows it to\nautomatically select meaningful hard samples with the guide of global context.\nIt encourages the model to learn more discriminative features from visual\nsimilar classes, leading to faster convergence and better performance. Our\nmethod is evaluated on the tasks of image retrieval and face recognition, where\nit outperforms the standard triplet loss substantially by 1%-18%. It achieves\nnew state-of-the-art performance on a number of benchmarks, with much fewer\nlearning iterations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 12:23:32 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Ge", "Weifeng", ""], ["Huang", "Weilin", ""], ["Dong", "Dengke", ""], ["Scott", "Matthew R.", ""]]}, {"id": "1810.06990", "submitter": "Shuang Yang", "authors": "Shuang Yang, Yuanhang Zhang, Dalu Feng, Mingmin Yang, Chenhao Wang,\n  Jingyun Xiao, Keyu Long, Shiguang Shan, Xilin Chen", "title": "LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading\n  in the Wild", "comments": "IEEE FG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale datasets have successively proven their fundamental importance in\nseveral research fields, especially for early progress in some emerging topics.\nIn this paper, we focus on the problem of visual speech recognition, also known\nas lipreading, which has received increasing interest in recent years. We\npresent a naturally-distributed large-scale benchmark for lip reading in the\nwild, named LRW-1000, which contains 1,000 classes with 718,018 samples from\nmore than 2,000 individual speakers. Each class corresponds to the syllables of\na Mandarin word composed of one or several Chinese characters. To the best of\nour knowledge, it is currently the largest word-level lipreading dataset and\nalso the only public large-scale Mandarin lipreading dataset. This dataset aims\nat covering a \"natural\" variability over different speech modes and imaging\nconditions to incorporate challenges encountered in practical applications. It\nhas shown a large variation in this benchmark in several aspects, including the\nnumber of samples in each class, video resolution, lighting conditions, and\nspeakers' attributes such as pose, age, gender, and make-up. Besides providing\na detailed description of the dataset and its collection pipeline, we evaluate\nseveral typical popular lipreading methods and perform a thorough analysis of\nthe results from several aspects. The results demonstrate the consistency and\nchallenges of our dataset, which may open up some new promising directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:39:08 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 08:58:07 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 16:20:52 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 12:41:16 GMT"}, {"version": "v5", "created": "Wed, 27 Feb 2019 12:45:01 GMT"}, {"version": "v6", "created": "Wed, 24 Apr 2019 00:40:56 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Yang", "Shuang", ""], ["Zhang", "Yuanhang", ""], ["Feng", "Dalu", ""], ["Yang", "Mingmin", ""], ["Wang", "Chenhao", ""], ["Xiao", "Jingyun", ""], ["Long", "Keyu", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1810.06996", "submitter": "Xing Fan", "authors": "Xing Fan, Hao Luo, Xuan Zhang, Lingxiao He, Chi Zhang, Wei Jiang", "title": "SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and\n  Partial Person Re-Identification", "comments": "accepted by ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holistic person re-identification (ReID) has received extensive study in the\npast few years and achieves impressive progress. However, persons are often\noccluded by obstacles or other persons in practical scenarios, which makes\npartial person re-identification non-trivial. In this paper, we propose a\nspatial-channel parallelism network (SCPNet) in which each channel in the ReID\nfeature pays attention to a given spatial part of the body. The spatial-channel\ncorresponding relationship supervises the network to learn discriminative\nfeature for both holistic and partial person re-identification. The single\nmodel trained on four holistic ReID datasets achieves competitive accuracy on\nthese four datasets, as well as outperforms the state-of-the-art methods on two\npartial ReID datasets without training.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:52:08 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Fan", "Xing", ""], ["Luo", "Hao", ""], ["Zhang", "Xuan", ""], ["He", "Lingxiao", ""], ["Zhang", "Chi", ""], ["Jiang", "Wei", ""]]}, {"id": "1810.07003", "submitter": "Jose Dolz", "authors": "Jose Dolz, Ismail Ben Ayed, Christian Desrosiers", "title": "Dense Multi-path U-Net for Ischemic Stroke Lesion Segmentation in\n  Multiple Image Modalities", "comments": "Submitted to the MICCAI BrainLes proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineating infarcted tissue in ischemic stroke lesions is crucial to\ndetermine the extend of damage and optimal treatment for this life-threatening\ncondition. However, this problem remains challenging due to high variability of\nischemic strokes' location and shape. Recently, fully-convolutional neural\nnetworks (CNN), in particular those based on U-Net, have led to improved\nperformances for this task. In this work, we propose a novel architecture that\nimproves standard U-Net based methods in three important ways. First, instead\nof combining the available image modalities at the input, each of them is\nprocessed in a different path to better exploit their unique information.\nMoreover, the network is densely-connected (i.e., each layer is connected to\nall following layers), both within each path and across different paths,\nsimilar to HyperDenseNet. This gives our model the freedom to learn the scale\nat which modalities should be processed and combined. Finally, inspired by the\nInception architecture, we improve standard U-Net modules by extending\ninception modules with two convolutional blocks with dilated convolutions of\ndifferent scale. This helps handling the variability in lesion sizes. We split\nthe 93 stroke datasets into training and validation sets containing 83 and 9\nexamples respectively. Our network was trained on a NVidia TITAN XP GPU with 16\nGBs RAM, using ADAM as optimizer and a learning rate of 1$\\times$10$^{-5}$\nduring 200 epochs. Training took around 5 hours and segmentation of a whole\nvolume took between 0.2 and 2 seconds, as average. The performance on the test\nset obtained by our method is compared to several baselines, to demonstrate the\neffectiveness of our architecture, and to a state-of-art architecture that\nemploys factorized dilated convolutions, i.e., ERFNet.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:59:11 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1810.07037", "submitter": "Muhammad Kamran Janjua", "authors": "Muhammad Kamran Janjua, Shah Nawaz, Alessandro Calefati, Ignazio Gallo", "title": "Learning Inward Scaled Hypersphere Embedding: Exploring Projections in\n  Higher Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the current dimensionality reduction or retrieval techniques rely\non embedding the learned feature representations onto a computable metric\nspace. Once the learned features are mapped, a distance metric aids the\nbridging of gaps between similar instances. Since the scaled projection is not\nexploited in these methods, discriminative embedding onto a hyperspace becomes\na challenge. In this paper, we propose to inwardly scale feature\nrepresentations in proportional to projecting them onto a hypersphere manifold\nfor discriminative analysis. We further propose a novel, yet simpler,\nconvolutional neural network based architecture and extensively evaluate the\nproposed methodology in the context of classification and retrieval tasks\nobtaining results comparable to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:30:45 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Janjua", "Muhammad Kamran", ""], ["Nawaz", "Shah", ""], ["Calefati", "Alessandro", ""], ["Gallo", "Ignazio", ""]]}, {"id": "1810.07050", "submitter": "Chiou Ting Hsu", "authors": "Zi-Yi Ke and Chiou-Ting Hsu", "title": "Generating Self-Guided Dense Annotations for Weakly Supervised Semantic\n  Segmentation", "comments": "16 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning semantic segmentation models under image-level supervision is far\nmore challenging than under fully supervised setting. Without knowing the exact\npixel-label correspondence, most weakly-supervised methods rely on external\nmodels to infer pseudo pixel-level labels for training semantic segmentation\nmodels. In this paper, we aim to develop a single neural network without\nresorting to any external models. We propose a novel self-guided strategy to\nfully utilize features learned across multiple levels to progressively generate\nthe dense pseudo labels. First, we use high-level features as class-specific\nlocalization maps to roughly locate the classes. Next, we propose an\naffinity-guided method to encourage each localization map to be consistent with\ntheir intermediate level features. Third, we adopt the training image itself as\nguidance and propose a self-guided refinement to further transfer the image's\ninherent structure into the maps. Finally, we derive pseudo pixel-level labels\nfrom these localization maps and use the pseudo labels as ground truth to train\nthe semantic segmentation model. Our proposed self-guided strategy is a unified\nframework, which is built on a single network and alternatively updates the\nfeature representation and refines localization maps during the training\nprocedure. Experimental results on PASCAL VOC 2012 segmentation benchmark\ndemonstrate that our method outperforms other weakly-supervised methods under\nthe same setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:49:18 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Ke", "Zi-Yi", ""], ["Hsu", "Chiou-Ting", ""]]}, {"id": "1810.07052", "submitter": "Yigitcan Kaya", "authors": "Yigitcan Kaya, Sanghyun Hong, Tudor Dumitras", "title": "Shallow-Deep Networks: Understanding and Mitigating Network Overthinking", "comments": "Accepted to ICML2019. Source code here: www.shallowdeep.network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize a prevalent weakness of deep neural networks\n(DNNs)---overthinking---which occurs when a DNN can reach correct predictions\nbefore its final layer. Overthinking is computationally wasteful, and it can\nalso be destructive when, by the final layer, a correct prediction changes into\na misclassification. Understanding overthinking requires studying how each\nprediction evolves during a DNN's forward pass, which conventionally is opaque.\nFor prediction transparency, we propose the Shallow-Deep Network (SDN), a\ngeneric modification to off-the-shelf DNNs that introduces internal\nclassifiers. We apply SDN to four modern architectures, trained on three image\nclassification tasks, to characterize the overthinking problem. We show that\nSDNs can mitigate the wasteful effect of overthinking with confidence-based\nearly exits, which reduce the average inference cost by more than 50% and\npreserve the accuracy. We also find that the destructive effect occurs for 50%\nof misclassifications on natural inputs and that it can be induced,\nadversarially, with a recent backdooring attack. To mitigate this effect, we\npropose a new confusion metric to quantify the internal disagreements that will\nlikely lead to misclassifications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:51:13 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 23:34:31 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 00:49:52 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Kaya", "Yigitcan", ""], ["Hong", "Sanghyun", ""], ["Dumitras", "Tudor", ""]]}, {"id": "1810.07075", "submitter": "Shaofeng Yuan", "authors": "Yujiao Tang, Feng Yang, Shaofeng Yuan, Chang'an Zhan", "title": "A Multi-stage Framework with Context Information Fusion Structure for\n  Skin Lesion Segmentation", "comments": "4 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer-aided diagnosis (CAD) systems can highly improve the reliability\nand efficiency of melanoma recognition. As a crucial step of CAD, skin lesion\nsegmentation has the unsatisfactory accuracy in existing methods due to large\nvariability in lesion appearance and artifacts. In this work, we propose a\nframework employing multi-stage UNets (MS-UNet) in the auto-context scheme to\nsegment skin lesion accurately end-to-end. We apply two approaches to boost the\nperformance of MS-UNet. First, UNet is coupled with a context information\nfusion structure (CIFS) to integrate the low-level and context information in\nthe multi-scale feature space. Second, to alleviate the gradient vanishing\nproblem, we use deep supervision mechanism through supervising MS-UNet by\nminimizing a weighted Jaccard distance loss function. Four out of five commonly\nused performance metrics, including Jaccard index and Dice coefficient, show\nthat our approach outperforms the state-ofthe-art deep learning based methods\non the ISBI 2016 Skin Lesion Challenge dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 15:26:30 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Tang", "Yujiao", ""], ["Yang", "Feng", ""], ["Yuan", "Shaofeng", ""], ["Zhan", "Chang'an", ""]]}, {"id": "1810.07088", "submitter": "Shaofeng Yuan", "authors": "Yunan Wu, Feng Yang, Ying Liu, Xuefan Zha, Shaofeng Yuan", "title": "A Comparison of 1-D and 2-D Deep Convolutional Neural Networks in ECG\n  Classification", "comments": "4 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective detection of arrhythmia is an important task in the remote\nmonitoring of electrocardiogram (ECG). The traditional ECG recognition depends\non the judgment of the clinicians' experience, but the results suffer from the\nprobability of human error due to the fatigue. To solve this problem, an ECG\nsignal classification method based on the images is presented to classify ECG\nsignals into normal and abnormal beats by using two-dimensional convolutional\nneural networks (2D-CNNs). First, we compare the accuracy and robustness\nbetween one-dimensional ECG signal input method and two-dimensional image input\nmethod in AlexNet network. Then, in order to alleviate the overfitting problem\nin two-dimensional network, we initialize AlexNet-like network with weights\ntrained on ImageNet, to fit the training ECG images and fine-tune the model,\nand to further improve the accuracy and robustness of ECG classification. The\nperformance evaluated on the MIT-BIH arrhythmia database demonstrates that the\nproposed method can achieve the accuracy of 98% and maintain high accuracy\nwithin SNR range from 20 dB to 35 dB. The experiment shows that the 2D-CNNs\ninitialized with AlexNet weights performs better than one-dimensional signal\nmethod without a large-scale dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 15:40:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Wu", "Yunan", ""], ["Yang", "Feng", ""], ["Liu", "Ying", ""], ["Zha", "Xuefan", ""], ["Yuan", "Shaofeng", ""]]}, {"id": "1810.07097", "submitter": "Mohammad Shokri", "authors": "Mohammad Shokri, Ahad Harati, Kimya Taba", "title": "Salient Object Detection in Video using Deep Non-Local Neural Networks", "comments": "Submitted to Journal of Visual Communication and Image Representation", "journal-ref": null, "doi": "10.1016/j.jvcir.2020.102769", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of salient objects in image and video is of great importance in\nmany computer vision applications. In spite of the fact that the state of the\nart in saliency detection for still images has been changed substantially over\nthe last few years, there have been few improvements in video saliency\ndetection. This paper investigates the use of recently introduced non-local\nneural networks in video salient object detection. Non-local neural networks\nare applied to capture global dependencies and hence determine the salient\nobjects. The effect of non-local operations is studied separately on static and\ndynamic saliency detection in order to exploit both appearance and motion\nfeatures. A novel deep non-local neural network architecture is introduced for\nvideo salient object detection and tested on two well-known datasets DAVIS and\nFBMS. The experimental results show that the proposed algorithm outperforms\nstate-of-the-art video saliency detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 15:55:57 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Shokri", "Mohammad", ""], ["Harati", "Ahad", ""], ["Taba", "Kimya", ""]]}, {"id": "1810.07121", "submitter": "Lerrel Pinto Mr", "authors": "Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, Abhinav Gupta", "title": "Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data\n  for Imitation", "comments": "10 pages, CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen an emergence of data-driven approaches in\nrobotics. However, most existing efforts and datasets are either in simulation\nor focus on a single task in isolation such as grasping, pushing or poking. In\norder to make progress and capture the space of manipulation, we would need to\ncollect a large-scale dataset of diverse tasks such as pouring, opening\nbottles, stacking objects etc. But how does one collect such a dataset? In this\npaper, we present the largest available robotic-demonstration dataset (MIME)\nthat contains 8260 human-robot demonstrations over 20 different robotic tasks\n(https://sites.google.com/view/mimedataset). These tasks range from the simple\ntask of pushing objects to the difficult task of stacking household objects.\nOur dataset consists of videos of human demonstrations and kinesthetic\ntrajectories of robot demonstrations. We also propose to use this dataset for\nthe task of mapping 3rd person video features to robot trajectories.\nFurthermore, we present two different approaches using this dataset and\nevaluate the predicted robot trajectories against ground-truth trajectories. We\nhope our dataset inspires research in multiple areas including visual\nimitation, trajectory prediction, and multi-task robotic learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 16:27:43 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sharma", "Pratyusha", ""], ["Mohan", "Lekha", ""], ["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1810.07122", "submitter": "Arturo Gomez Chavez", "authors": "Arturo Gomez Chavez, Christian A. Mueller, Tobias Doernbach, Davide\n  Chiarella, Andreas Birk", "title": "Robust Gesture-Based Communication for Underwater Human-Robot\n  Interaction in the context of Search and Rescue Diver Missions", "comments": "Workshop on Human-Aiding Robotics. International Conference on\n  Intelligent Robots and Systems 2018 (IROS)", "journal-ref": "Journal of Marine Science and Engineering. Vol. 7 (2019)", "doi": "10.3390/jmse7010016", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust gesture-based communication pipeline for divers to\ninstruct an Autonomous Underwater Vehicle (AUV) to assist them in performing\nhigh-risk tasks and helping in case of emergency. A gesture communication\nlanguage (CADDIAN) is developed, based on consolidated and standardized diver\ngestures, including an alphabet, syntax and semantics, ensuring a logical\nconsistency. A hierarchical classification approach is introduced for hand\ngesture recognition based on stereo imagery and multi-descriptor aggregation to\nspecifically cope with underwater image artifacts, e.g. light backscatter or\ncolor attenuation. Once the classification task is finished, a syntax check is\nperformed to filter out invalid command sequences sent by the diver or\ngenerated by errors in the classifier. Throughout this process, the diver\nreceives constant feedback from an underwater tablet to acknowledge or abort\nthe mission at any time. The objective is to prevent the AUV from executing\nunnecessary, infeasible or potentially harmful motions. Experimental results\nunder different environmental conditions in archaeological exploration and\nbridge inspection applications show that the system performs well in the field.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 16:32:30 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Chavez", "Arturo Gomez", ""], ["Mueller", "Christian A.", ""], ["Doernbach", "Tobias", ""], ["Chiarella", "Davide", ""], ["Birk", "Andreas", ""]]}, {"id": "1810.07212", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Hexiang Hu, Fei Sha", "title": "Cross-Modal and Hierarchical Modeling of Video and Text", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data and text data are composed of information at multiple\ngranularities. A video can describe a complex scene that is composed of\nmultiple clips or shots, where each depicts a semantically coherent event or\naction. Similarly, a paragraph may contain sentences with different topics,\nwhich collectively conveys a coherent message or story. In this paper, we\ninvestigate the modeling techniques for such hierarchical sequential data where\nthere are correspondences across multiple modalities. Specifically, we\nintroduce hierarchical sequence embedding (HSE), a generic model for embedding\nsequential data of different modalities into hierarchically semantic spaces,\nwith either explicit or implicit correspondence information. We perform\nempirical studies on large-scale video and paragraph retrieval datasets and\ndemonstrated superior performance by the proposed methods. Furthermore, we\nexamine the effectiveness of our learned embeddings when applied to downstream\ntasks. We show its utility in zero-shot action recognition and video\ncaptioning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 18:07:47 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Zhang", "Bowen", ""], ["Hu", "Hexiang", ""], ["Sha", "Fei", ""]]}, {"id": "1810.07218", "submitter": "Mengye Ren", "authors": "Mengye Ren, Renjie Liao, Ethan Fetaya, Richard S. Zemel", "title": "Incremental Few-Shot Learning with Attention Attractor Networks", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning classifiers are often trained to recognize a set of\npre-defined classes. However, in many applications, it is often desirable to\nhave the flexibility of learning additional concepts, with limited data and\nwithout re-training on the full training set. This paper addresses this\nproblem, incremental few-shot learning, where a regular classification network\nhas already been trained to recognize a set of base classes, and several extra\nnovel classes are being considered, each with only a few labeled examples.\nAfter learning the novel classes, the model is then evaluated on the overall\nclassification performance on both base and novel classes. To this end, we\npropose a meta-learning model, the Attention Attractor Network, which\nregularizes the learning of novel classes. In each episode, we train a set of\nnew weights to recognize novel classes until they converge, and we show that\nthe technique of recurrent back-propagation can back-propagate through the\noptimization process and facilitate the learning of these parameters. We\ndemonstrate that the learned attractor network can help recognize novel classes\nwhile remembering old classes without the need to review the original training\nset, outperforming various baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 18:25:17 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 15:35:29 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 21:08:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ren", "Mengye", ""], ["Liao", "Renjie", ""], ["Fetaya", "Ethan", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1810.07230", "submitter": "Mebin Jose Vi", "authors": "V.I Mebin Jose, D.J Binoj", "title": "Hybrid Feature Based SLAM Prototype", "comments": "7 pages,1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of data innovation as of late and the expanded limit, has\npermitted the acquaintance of artificial vision connected with SLAM, offering\nascend to what is known as Visual SLAM. The objective of this paper is to build\nup a route framework dependent on Visual SLAM to get a robot to a fundamental\nand new condition, have the capacity to set and make a three-dimensional guide\nthereof, utilizing just as sources of info recording your way with a stereo\nvision camera. The consequence of this analysis is that the framework Visual\nSLAM together with the combination of Fast SLAM (combination of kalman with\nparticulate filter and SIFT) perceive and recognize characteristic points in\nimages so adequately exact and unambiguous. This framework uses MATLAB, since\nits adaptability and comfort for performing a wide range of tests. The program\nhas been tested by inserting a prerecorded video input with a camera stereo in\nwhich a course is done by an office environment. The algorithm initially\nlocates points of interest in a stereo frame captured by the camera. These will\nbe located in 3D and they associate an identification descriptor. In the next\nframe, the camera likewise identified points of interest and it will be\ncompared which of them have been previously detected by comparing their\ndescriptors. This process is known as \"data association\" and its successful\ncompletion is fundamental to the SLAM algorithm. The position data of the robot\nand points interest stored in data structures known as \"particles\" that evolve\nindependently. Its management is very important for the proper functioning of\nthe algorithm Fast SLAM. The results are found to be satisfactory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 18:54:06 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 11:17:21 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Jose", "V. I Mebin", ""], ["Binoj", "D. J", ""]]}, {"id": "1810.07251", "submitter": "Nelly Elsayed", "authors": "Nelly Elsayed, Anthony S. Maida, Magdy Bayoumi", "title": "Reduced-Gate Convolutional LSTM Using Predictive Coding for\n  Spatiotemporal Prediction", "comments": "A novel rgcLSTM model for spatiotemporal prediction. This version\n  contains the full description and detailed empirical study of the rgcLSTM\n  architecture. 28 pages, 12 figures, 20 tables", "journal-ref": null, "doi": "10.1111/coin.12277", "report-no": "COIN12277", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal sequence prediction is an important problem in deep learning.\nWe study next-frame(s) video prediction using a deep-learning-based predictive\ncoding framework that uses convolutional, long short-term memory (convLSTM)\nmodules. We introduce a novel reduced-gate convolutional LSTM(rgcLSTM)\narchitecture that requires a significantly lower parameter budget than a\ncomparable convLSTM. By using a single multi-function gate, our reduced-gate\nmodel achieves equal or better next-frame(s) prediction accuracy than the\noriginal convolutional LSTM while using a smaller parameter budget, thereby\nreducing training time and memory requirements. We tested our reduced gate\nmodules within a predictive coding architecture on the moving MNIST and KITTI\ndatasets. We found that our reduced-gate model has a significant reduction of\napproximately 40 percent of the total number of training parameters and a 25\npercent reduction in elapsed training time in comparison with the standard\nconvolutional LSTM model. The performance accuracy of the new model was also\nimproved. This makes our model more attractive for hardware implementation,\nespecially on small devices. We also explored a space of twenty different gated\narchitectures to get insight into how our rgcLSTM fit into that space.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 19:55:51 GMT"}, {"version": "v10", "created": "Wed, 23 Oct 2019 03:30:11 GMT"}, {"version": "v11", "created": "Sun, 22 Dec 2019 21:44:41 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 20:36:59 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 06:08:08 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 01:52:15 GMT"}, {"version": "v5", "created": "Sun, 9 Dec 2018 03:11:44 GMT"}, {"version": "v6", "created": "Tue, 11 Dec 2018 01:57:45 GMT"}, {"version": "v7", "created": "Fri, 28 Dec 2018 01:26:57 GMT"}, {"version": "v8", "created": "Thu, 10 Jan 2019 19:05:49 GMT"}, {"version": "v9", "created": "Fri, 15 Mar 2019 19:21:00 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Elsayed", "Nelly", ""], ["Maida", "Anthony S.", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "1810.07322", "submitter": "Zhuwei Qin", "authors": "Zhuwei Qin, Fuxun Yu, Chenchen Liu, Xiang Chen", "title": "Functionality-Oriented Convolutional Filter Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sophisticated structure of Convolutional Neural Network (CNN) allows for\noutstanding performance, but at the cost of intensive computation. As\nsignificant redundancies inevitably present in such a structure, many works\nhave been proposed to prune the convolutional filters for computation cost\nreduction. Although extremely effective, most works are based only on\nquantitative characteristics of the convolutional filters, and highly overlook\nthe qualitative interpretation of individual filter's specific functionality.\nIn this work, we interpreted the functionality and redundancy of the\nconvolutional filters from different perspectives, and proposed a\nfunctionality-oriented filter pruning method. With extensive experiment\nresults, we proved the convolutional filters' qualitative significance\nregardless of magnitude, demonstrated significant neural network redundancy due\nto repetitive filter functions, and analyzed the filter functionality defection\nunder inappropriate retraining process. Such an interpretable pruning approach\nnot only offers outstanding computation cost optimization over previous filter\npruning methods, but also interprets filter pruning process.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 20:39:47 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 03:24:06 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Qin", "Zhuwei", ""], ["Yu", "Fuxun", ""], ["Liu", "Chenchen", ""], ["Chen", "Xiang", ""]]}, {"id": "1810.07378", "submitter": "Tianyun Zhang", "authors": "Shaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Kaidi Xu, Yunfei\n  Yang, Fuxun Yu, Jian Tang, Makan Fardad, Sijia Liu, Xiang Chen, Xue Lin,\n  Yanzhi Wang", "title": "Progressive Weight Pruning of Deep Neural Networks using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) although achieving human-level performance in\nmany domains, have very large model size that hinders their broader\napplications on edge computing devices. Extensive research work have been\nconducted on DNN model compression or pruning. However, most of the previous\nwork took heuristic approaches. This work proposes a progressive weight pruning\napproach based on ADMM (Alternating Direction Method of Multipliers), a\npowerful technique to deal with non-convex optimization problems with\npotentially combinatorial constraints. Motivated by dynamic programming, the\nproposed method reaches extremely high pruning rate by using partial prunings\nwith moderate pruning rates. Therefore, it resolves the accuracy degradation\nand long convergence time problems when pursuing extremely high pruning ratios.\nIt achieves up to 34 times pruning rate for ImageNet dataset and 167 times\npruning rate for MNIST dataset, significantly higher than those reached by the\nliterature work. Under the same number of epochs, the proposed method also\nachieves faster convergence and higher compression rates. The codes and pruned\nDNN models are released in the link bit.ly/2zxdlss\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 03:51:38 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 16:41:06 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ye", "Shaokai", ""], ["Zhang", "Tianyun", ""], ["Zhang", "Kaiqi", ""], ["Li", "Jiayu", ""], ["Xu", "Kaidi", ""], ["Yang", "Yunfei", ""], ["Yu", "Fuxun", ""], ["Tang", "Jian", ""], ["Fardad", "Makan", ""], ["Liu", "Sijia", ""], ["Chen", "Xiang", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1810.07399", "submitter": "He Lingxiao", "authors": "Lingxiao He, Zhenan Sun, Yuhao Zhu and Yunbo Wang", "title": "Recognizing Partial Biometric Patterns", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric recognition on partial captured targets is challenging, where only\nseveral partial observations of objects are available for matching. In this\narea, deep learning based methods are widely applied to match these partial\ncaptured objects caused by occlusions, variations of postures or just partial\nout of view in person re-identification and partial face recognition. However,\nmost current methods are not able to identify an individual in case that some\nparts of the object are not obtainable, while the rest are specialized to\ncertain constrained scenarios. To this end, we propose a robust general\nframework for arbitrary biometric matching scenarios without the limitations of\nalignment as well as the size of inputs. We introduce a feature post-processing\nstep to handle the feature maps from FCN and a dictionary learning based\nSpatial Feature Reconstruction (SFR) to match different sized feature maps in\nthis work. Moreover, the batch hard triplet loss function is applied to\noptimize the model. The applicability and effectiveness of the proposed method\nare demonstrated by the results from experiments on three person\nre-identification datasets (Market1501, CUHK03, DukeMTMC-reID), two partial\nperson datasets (Partial REID and Partial iLIDS) and two partial face datasets\n(CASIA-NIR-Distance and Partial LFW), on which state-of-the-art performance is\nensured in comparison with several state-of-the-art approaches. The code is\nreleased online and can be found on the website:\nhttps://github.com/lingxiao-he/Partial-Person-ReID.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 06:56:45 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["He", "Lingxiao", ""], ["Sun", "Zhenan", ""], ["Zhu", "Yuhao", ""], ["Wang", "Yunbo", ""]]}, {"id": "1810.07420", "submitter": "Roberto J. L\\'opez-Sastre", "authors": "Marcos Baptista-R\\'ios, Roberto J. L\\'opez-Sastre, Franciso Javier\n  Acevedo-Rodr\\'iguez and Saturnino Maldonado-Basc\\'on", "title": "Embarrassingly Simple Model for Early Action Proposal", "comments": "Published in the Anticipating Human Behavior Workshop, ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early action proposal consists in generating high quality candidate temporal\nsegments that are likely to contain an action in a video stream, as soon as\nthey happen. Many sophisticated approaches have been proposed for the action\nproposal problem but from the off-line perspective. On the contrary, we focus\non the on-line version of the problem, proposing a simple classifier-based\nmodel, using standard 3D CNNs, that performs significantly better than the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 08:09:09 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 07:43:00 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Baptista-R\u00edos", "Marcos", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""], ["Acevedo-Rodr\u00edguez", "Franciso Javier", ""], ["Maldonado-Basc\u00f3n", "Saturnino", ""]]}, {"id": "1810.07430", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Marco Loog, Wilbert Bartels, Adri\\\"enne M. Mendrik", "title": "Learning an MR acquisition-invariant representation using Siamese neural\n  networks", "comments": "3 figures, submitted to International Symposium on Biomedical Imaging\n  2019", "journal-ref": "16th IEEE International Symposium on Biomedical Imaging (ISBI),\n  Venice, 2019, pp. 364-367", "doi": "10.1109/ISBI.2019.8759281", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization of voxelwise classifiers is hampered by differences between\nMRI-scanners, e.g. different acquisition protocols and field strengths. To\naddress this limitation, we propose a Siamese neural network (MRAI-NET) that\nextracts acquisition-invariant feature vectors. These can consequently be used\nby task-specific methods, such as voxelwise classifiers for tissue\nsegmentation. MRAI-NET is tested on both simulated and real patient data.\nExperiments show that MRAI-NET outperforms voxelwise classifiers trained on the\nsource or target scanner data when a small number of labeled samples is\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 08:37:09 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""], ["Bartels", "Wilbert", ""], ["Mendrik", "Adri\u00ebnne M.", ""]]}, {"id": "1810.07433", "submitter": "Silas {\\O}rting", "authors": "Silas Nyboe {\\O}rting, Jens Petersen, Laura H. Thomsen, Mathilde M. W.\n  Wille, Marleen de Bruijne", "title": "Learning to quantify emphysema extent: What labels do we need?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate assessment of pulmonary emphysema is crucial to assess disease\nseverity and subtype, to monitor disease progression and to predict lung cancer\nrisk. However, visual assessment is time-consuming and subject to substantial\ninter-rater variability and standard densitometry approaches to quantify\nemphysema remain inferior to visual scoring. We explore if machine learning\nmethods that learn from a large dataset of visually assessed CT scans can\nprovide accurate estimates of emphysema extent. We further investigate if\nmachine learning algorithms that learn from a scoring of emphysema extent can\noutperform algorithms that learn only from a scoring of emphysema presence. We\ncompare four Multiple Instance Learning classifiers that are trained on\nemphysema presence labels, and five Learning with Label Proportions classifiers\nthat are trained on emphysema extent labels. We evaluate performance on 600\nlow-dose CT scans from the Danish Lung Cancer Screening Trial and find that\nlearning from emphysema presence labels, which are much easier to obtain, gives\nequally good performance to learning from emphysema extent labels. The best\nclassifiers achieve intra-class correlation coefficients around 0.90 and\naverage overall agreement with raters of 78% and 79% on six emphysema extent\nclasses versus inter-rater agreement of 83%.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 08:48:27 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["\u00d8rting", "Silas Nyboe", ""], ["Petersen", "Jens", ""], ["Thomsen", "Laura H.", ""], ["Wille", "Mathilde M. W.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1810.07491", "submitter": "Paul Maergner", "authors": "Paul Maergner, Vinaychandran Pondenkandath, Michele Alberti, Marcus\n  Liwicki, Kaspar Riesen, Rolf Ingold, Andreas Fischer", "title": "Offline Signature Verification by Combining Graph Edit Distance and\n  Triplet Networks", "comments": null, "journal-ref": "Structural, Syntactic, and Statistical Pattern Recognition. S+SSPR\n  2018. Lecture Notes in Computer Science, vol 11004. Springer, Cham", "doi": "10.1007/978-3-319-97785-0_45", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric authentication by means of handwritten signatures is a challenging\npattern recognition task, which aims to infer a writer model from only a\nhandful of genuine signatures. In order to make it more difficult for a forger\nto attack the verification system, a promising strategy is to combine different\nwriter models. In this work, we propose to complement a recent structural\napproach to offline signature verification based on graph edit distance with a\nstatistical approach based on metric learning with deep neural networks. On the\nMCYT and GPDS benchmark datasets, we demonstrate that combining the structural\nand statistical models leads to significant improvements in performance,\nprofiting from their complementary properties.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 11:55:04 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Maergner", "Paul", ""], ["Pondenkandath", "Vinaychandran", ""], ["Alberti", "Michele", ""], ["Liwicki", "Marcus", ""], ["Riesen", "Kaspar", ""], ["Ingold", "Rolf", ""], ["Fischer", "Andreas", ""]]}, {"id": "1810.07500", "submitter": "Ivo Matteo Baltruschat", "authors": "Ivo M. Baltruschat and Leonhard Steinmeister and Harald Ittrich and\n  Gerhard Adam and Hannes Nickisch and Axel Saalbach and Jens von Berg and\n  Michael Grass and Tobias Knopp", "title": "When does Bone Suppression and Lung Field Segmentation Improve Chest\n  X-Ray Disease Classification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiography is the most common clinical examination type. To improve\nthe quality of patient care and to reduce workload, methods for automatic\npathology classification have been developed. In this contribution we\ninvestigate the usefulness of two advanced image pre-processing techniques,\ninitially developed for image reading by radiologists, for the performance of\nDeep Learning methods. First, we use bone suppression, an algorithm to\nartificially remove the rib cage. Secondly, we employ an automatic lung field\ndetection to crop the image to the lung area. Furthermore, we consider the\ncombination of both in the context of an ensemble approach. In a five-times\nre-sampling scheme, we use Receiver Operating Characteristic (ROC) statistics\nto evaluate the effect of the pre-processing approaches. Using a Convolutional\nNeural Network (CNN), optimized for X-ray analysis, we achieve a good\nperformance with respect to all pathologies on average. Superior results are\nobtained for selected pathologies when using pre-processing, i.e. for mass the\narea under the ROC curve increased by 9.95%. The ensemble with pre-processed\ntrained models yields the best overall results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 12:30:08 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Baltruschat", "Ivo M.", ""], ["Steinmeister", "Leonhard", ""], ["Ittrich", "Harald", ""], ["Adam", "Gerhard", ""], ["Nickisch", "Hannes", ""], ["Saalbach", "Axel", ""], ["von Berg", "Jens", ""], ["Grass", "Michael", ""], ["Knopp", "Tobias", ""]]}, {"id": "1810.07535", "submitter": "Xiaochun Liu", "authors": "Xiaochun Liu, Ib\\'on Guill\\'en, Marco La Manna, Ji Hyun Nam, Syed Azer\n  Reza, Toan Huu Le, Diego Gutierrez, Adrian Jarabo, Andreas Velten", "title": "Virtual Wave Optics for Non-Line-of-Sight Imaging", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Line-of-Sight (NLOS) imaging allows to observe objects partially or fully\noccluded from direct view, by analyzing indirect diffuse reflections off a\nsecondary, relay surface. Despite its many potential applications, existing\nmethods lack practical usability due to several shared limitations, including\nthe assumption of single scattering only, lack of occlusions, and Lambertian\nreflectance. We lift these limitations by transforming the NLOS problem into a\nvirtual Line-Of-Sight (LOS) one. Since imaging information cannot be recovered\nfrom the irradiance arriving at the relay surface, we introduce the concept of\nthe phasor field, a mathematical construct representing a fast variation in\nirradiance. We show that NLOS light transport can be modeled as the propagation\nof a phasor field wave, which can be solved accurately by the\nRayleigh-Sommerfeld diffraction integral. We demonstrate for the first time\nNLOS reconstruction of complex scenes with strong multiply scattered and\nambient light, arbitrary materials, large depth range, and occlusions. Our\nmethod handles these challenging cases without explicitly developing a light\ntransport model. By leveraging existing fast algorithms, we outperform existing\nmethods in terms of execution speed, computational complexity, and memory use.\nWe believe that our approach will help unlock the potential of NLOS imaging,\nand the development of novel applications not restricted to lab conditions. For\nexample, we demonstrate both refocusing and transient NLOS videos of\nreal-world, complex scenes with large depth.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 13:38:37 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 14:01:27 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Liu", "Xiaochun", ""], ["Guill\u00e9n", "Ib\u00f3n", ""], ["La Manna", "Marco", ""], ["Nam", "Ji Hyun", ""], ["Reza", "Syed Azer", ""], ["Le", "Toan Huu", ""], ["Gutierrez", "Diego", ""], ["Jarabo", "Adrian", ""], ["Velten", "Andreas", ""]]}, {"id": "1810.07538", "submitter": "Nikolas Hesse", "authors": "Nikolas Hesse, Sergi Pujades, Michael J. Black, Michael Arens, Ulrich\n  G. Hofmann, A. Sebastian Schroeder", "title": "Learning and Tracking the 3D Body Shape of Freely Moving Infants from\n  RGB-D sequences", "comments": "12 pages, supplemental video at https://youtu.be/aahF1xGurmM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models of the human body surface are generally learned from\nthousands of high-quality 3D scans in predefined poses to cover the wide\nvariety of human body shapes and articulations. Acquisition of such data\nrequires expensive equipment, calibration procedures, and is limited to\ncooperative subjects who can understand and follow instructions, such as\nadults. We present a method for learning a statistical 3D Skinned Multi-Infant\nLinear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely\nmoving infants. Quantitative experiments show that SMIL faithfully represents\nthe RGB-D data and properly factorizes the shape and pose of the infants. To\ndemonstrate the applicability of SMIL, we fit the model to RGB-D sequences of\nfreely moving infants and show, with a case study, that our method captures\nenough motion detail for General Movements Assessment (GMA), a method used in\nclinical practice for early detection of neurodevelopmental disorders in\ninfants. SMIL provides a new tool for analyzing infant shape and movement and\nis a step towards an automated system for GMA.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 13:41:10 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Hesse", "Nikolas", ""], ["Pujades", "Sergi", ""], ["Black", "Michael J.", ""], ["Arens", "Michael", ""], ["Hofmann", "Ulrich G.", ""], ["Schroeder", "A. Sebastian", ""]]}, {"id": "1810.07599", "submitter": "Zhifeng Li", "authors": "Yitong Wang, Dihong Gong, Zheng Zhou, Xing Ji, Hao Wang, Zhifeng Li,\n  Wei Liu and Tong Zhang", "title": "Orthogonal Deep Features Decomposition for Age-Invariant Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 15:08:11 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Wang", "Yitong", ""], ["Gong", "Dihong", ""], ["Zhou", "Zheng", ""], ["Ji", "Xing", ""], ["Wang", "Hao", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1810.07610", "submitter": "Artur Jordao", "authors": "Artur Jordao, Ricardo Kloss, Fernando Yamada, William Robson Schwartz", "title": "Pruning Deep Neural Networks using Partial Least Squares", "comments": null, "journal-ref": "British Machine Vision Conference Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern pattern recognition methods are based on convolutional networks since\nthey are able to learn complex patterns that benefit the classification.\nHowever, convolutional networks are computationally expensive and require a\nconsiderable amount of memory, which limits their deployment on low-power and\nresource-constrained systems. To handle these problems, recent approaches have\nproposed pruning strategies that find and remove unimportant neurons (i.e.,\nfilters) in these networks. Despite achieving remarkable results, existing\npruning approaches are ineffective since the accuracy of the original network\nis degraded. In this work, we propose a novel approach to efficiently remove\nfilters from convolutional networks. Our approach estimates the filter\nimportance based on its relationship with the class label on a low-dimensional\nspace. This relationship is computed using Partial Least Squares (PLS) and\nVariable Importance in Projection (VIP). Our method is able to reduce up to 67%\nof the floating point operations (FLOPs) without penalizing the network\naccuracy. With a negligible drop in accuracy, we can reduce up to 90% of FLOPs.\nAdditionally, sometimes the method is even able to improve the accuracy\ncompared to original, unpruned, network. We show that employing PLS+VIP as the\ncriterion for detecting the filters to be removed is better than recent feature\nselection techniques, which have been employed by state-of-the-art pruning\nmethods. Finally, we show that the proposed method achieves the highest FLOPs\nreduction and the smallest drop in accuracy when compared to state-of-the-art\npruning approaches. Codes are available at:\nhttps://github.com/arturjordao/PruningNeuralNetworks\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 15:24:21 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 23:06:19 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 14:33:14 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Jordao", "Artur", ""], ["Kloss", "Ricardo", ""], ["Yamada", "Fernando", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1810.07733", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Chen Jiang, Steven Lu, Laura Petrich, Mahmoud Gamal,\n  Mohamed Elhoseiny, Martin Jagersand", "title": "Video Object Segmentation using Teacher-Student Adaptation in a Human\n  Robot Interaction (HRI) Setting", "comments": "Accepted in ICRA'19, https://msiam.github.io/ivos/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video object segmentation is an essential task in robot manipulation to\nfacilitate grasping and learning affordances. Incremental learning is important\nfor robotics in unstructured environments, since the total number of objects\nand their variations can be intractable. Inspired by the children learning\nprocess, human robot interaction (HRI) can be utilized to teach robots about\nthe world guided by humans similar to how children learn from a parent or a\nteacher. A human teacher can show potential objects of interest to the robot,\nwhich is able to self adapt to the teaching signal without providing manual\nsegmentation labels. We propose a novel teacher-student learning paradigm to\nteach robots about their surrounding environment. A two-stream motion and\nappearance \"teacher\" network provides pseudo-labels to adapt an appearance\n\"student\" network. The student network is able to segment the newly learned\nobjects in other scenes, whether they are static or in motion. We also\nintroduce a carefully designed dataset that serves the proposed HRI setup,\ndenoted as (I)nteractive (V)ideo (O)bject (S)egmentation. Our IVOS dataset\ncontains teaching videos of different objects, and manipulation tasks. Unlike\nprevious datasets, IVOS provides manipulation tasks sequences with segmentation\nannotation along with the waypoints for the robot trajectories. It also\nprovides segmentation annotation for the different transformations such as\ntranslation, scale, planar rotation, and out-of-plane rotation. Our proposed\nadaptation method outperforms the state-of-the-art on DAVIS and FBMS with 6.8%\nand 1.2% in F-measure respectively. It improves over the baseline on IVOS\ndataset with 46.1% and 25.9% in mIoU.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 18:42:53 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 21:39:43 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 17:35:33 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 21:18:32 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Siam", "Mennatullah", ""], ["Jiang", "Chen", ""], ["Lu", "Steven", ""], ["Petrich", "Laura", ""], ["Gamal", "Mahmoud", ""], ["Elhoseiny", "Mohamed", ""], ["Jagersand", "Martin", ""]]}, {"id": "1810.07746", "submitter": "Evan Yu", "authors": "Evan M. Yu and Mert R. Sabuncu", "title": "A Convolutional Autoencoder Approach to Learn Volumetric Shape\n  Representations for Brain Structures", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel machine learning strategy for studying neuroanatomical\nshape variation. Our model works with volumetric binary segmentation images,\nand requires no pre-processing such as the extraction of surface points or a\nmesh. The learned shape descriptor is invariant to affine transformations,\nincluding shifts, rotations and scaling. Thanks to the adopted autoencoder\nframework, inter-subject differences are automatically enhanced in the learned\nrepresentation, while intra-subject variances are minimized. Our experimental\nresults on a shape retrieval task showed that the proposed representation\noutperforms a state-of-the-art benchmark for brain structures extracted from\nMRI scans.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 19:34:59 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Yu", "Evan M.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1810.07810", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang", "title": "LadderNet: Multi-path networks based on U-Net for medical image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-Net has been providing state-of-the-art performance in many medical image\nsegmentation problems. Many modifications have been proposed for U-Net, such as\nattention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net\nwith residual blocks or blocks with dense connections. However, all these\nmodifications have an encoder-decoder structure with skip connections, and the\nnumber of paths for information flow is limited. We propose LadderNet in this\npaper, which can be viewed as a chain of multiple U-Nets. Instead of only one\npair of encoder branch and decoder branch in U-Net, a LadderNet has multiple\npairs of encoder-decoder branches, and has skip connections between every pair\nof adjacent decoder and decoder branches in each level. Inspired by the success\nof ResNet and R2-UNet, we use modified residual blocks where two convolutional\nlayers in one block share the same weights. A LadderNet has more paths for\ninformation flow because of skip connections and residual blocks, and can be\nviewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to\nan ensemble of FCNs improves segmentation accuracy, while the shared weights\nwithin each residual block reduce parameter number. Semantic segmentation is\nessential for retinal disease detection. We tested LadderNet on two benchmark\ndatasets for blood vessel segmentation in retinal images, and achieved superior\nperformance over methods in the literature. The implementation is provided\n\\url{https://github.com/juntang-zhuang/LadderNet}\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 21:33:27 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 05:02:50 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 18:12:33 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 17:16:55 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Zhuang", "Juntang", ""]]}, {"id": "1810.07842", "submitter": "Nabila Abraham", "authors": "Nabila Abraham and Naimul Mefraz Khan", "title": "A Novel Focal Tversky loss function with improved Attention U-Net for\n  lesion segmentation", "comments": "submitted to 2019 IEEE International Symposium on Biomedical Imaging\n  (ISBI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalized focal loss function based on the Tversky index to\naddress the issue of data imbalance in medical image segmentation. Compared to\nthe commonly used Dice loss, our loss function achieves a better trade off\nbetween precision and recall when training on small structures such as lesions.\nTo evaluate our loss function, we improve the attention U-Net model by\nincorporating an image pyramid to preserve contextual features. We experiment\non the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and\n21.4% of the images area and improve segmentation accuracy when compared to the\nstandard U-Net by 25.7% and 3.6%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 00:07:33 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Abraham", "Nabila", ""], ["Khan", "Naimul Mefraz", ""]]}, {"id": "1810.07884", "submitter": "Guotai Wang", "authors": "Guotai Wang, Wenqi Li, Sebastien Ourselin, Tom Vercauteren", "title": "Automatic Brain Tumor Segmentation using Convolutional Neural Networks\n  with Test-Time Augmentation", "comments": "12 pages, 3 figures, MICCAI BrainLes 2018", "journal-ref": null, "doi": "10.1007/978-3-030-11726-9_6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic brain tumor segmentation plays an important role for diagnosis,\nsurgical planning and treatment assessment of brain tumors. Deep convolutional\nneural networks (CNNs) have been widely used for this task. Due to the\nrelatively small data set for training, data augmentation at training time has\nbeen commonly used for better performance of CNNs. Recent works also\ndemonstrated the usefulness of using augmentation at test time, in addition to\ntraining time, for achieving more robust predictions. We investigate how\ntest-time augmentation can improve CNNs' performance for brain tumor\nsegmentation. We used different underpinning network structures and augmented\nthe image by 3D rotation, flipping, scaling and adding random noise at both\ntraining and test time. Experiments with BraTS 2018 training and validation set\nshow that test-time augmentation helps to improve the brain tumor segmentation\naccuracy and obtain uncertainty estimation of the segmentation results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 02:55:56 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 02:02:01 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Wang", "Guotai", ""], ["Li", "Wenqi", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1810.07901", "submitter": "Vishal Keshav", "authors": "Vishal Keshav, Tej Pratap GVSL", "title": "Decoupling Semantic Context and Color Correlation with multi-class cross\n  branch regularization", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel design methodology for architecting a\nlight-weight and faster DNN architecture for vision applications. The\neffectiveness of the architecture is demonstrated on Color-Constancy use case\nan inherent block in camera and imaging pipelines. Specifically, we present a\nmulti-branch architecture that disassembles the contextual features and color\nproperties from an image, and later combines them to predict a global property\n(e.g. Global Illumination). We also propose an implicit regularization\ntechnique by designing cross-branch regularization block that enables the\nnetwork to retain high generalization accuracy. With a conservative use of best\ncomputational operators, the proposed architecture achieves state-of-the-art\naccuracy with 30X lesser model parameters and 70X faster inference time for\ncolor constancy. It is also shown that the proposed architecture is generic and\nachieves similar efficiency in other vision applications such as Low-Light\nphotography.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 05:25:13 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 10:00:15 GMT"}], "update_date": "2020-08-09", "authors_parsed": [["Keshav", "Vishal", ""], ["GVSL", "Tej Pratap", ""]]}, {"id": "1810.07911", "submitter": "Zhiding Yu", "authors": "Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, Jinsong Wang", "title": "Domain Adaptation for Semantic Segmentation via Class-Balanced\n  Self-Training", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent deep networks achieved state of the art performance on a variety of\nsemantic segmentation tasks. Despite such progress, these models often face\nchallenges in real world `wild tasks' where large difference between labeled\ntraining/source data and unseen test/target data exists. In particular, such\ndifference is often referred to as `domain gap', and could cause significantly\ndecreased performance which cannot be easily remedied by further increasing the\nrepresentation power. Unsupervised domain adaptation (UDA) seeks to overcome\nsuch problem without target domain labels. In this paper, we propose a novel\nUDA framework based on an iterative self-training procedure, where the problem\nis formulated as latent variable loss minimization, and can be solved by\nalternatively generating pseudo labels on target data and re-training the model\nwith these labels. On top of self-training, we also propose a novel\nclass-balanced self-training framework to avoid the gradual dominance of large\nclasses on pseudo-label generation, and introduce spatial priors to refine\ngenerated labels. Comprehensive experiments show that the proposed methods\nachieve state of the art semantic segmentation performance under multiple major\nUDA settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 06:20:02 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:51:52 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zou", "Yang", ""], ["Yu", "Zhiding", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Wang", "Jinsong", ""]]}, {"id": "1810.07926", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Abhinav Agarwalla, Prabir Kumar Biswas", "title": "Unsupervised Domain Adaptation for Learning Eye Gaze from a Million\n  Synthetic Images: An Adversarial Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With contemporary advancements of graphics engines, recent trend in deep\nlearning community is to train models on automatically annotated simulated\nexamples and apply on real data during test time. This alleviates the burden of\nmanual annotation. However, there is an inherent difference of distributions\nbetween images coming from graphics engine and real world. Such domain\ndifference deteriorates test time performances of models trained on synthetic\nexamples. In this paper we address this issue with unsupervised adversarial\nfeature adaptation across synthetic and real domain for the special use case of\neye gaze estimation which is an essential component for various downstream HCI\ntasks. We initially learn a gaze estimator on annotated synthetic samples\nrendered from a 3D game engine and then adapt the features of unannotated real\nsamples via a zero-sum minmax adversarial game against a domain discriminator\nfollowing the recent paradigm of generative adversarial networks. Such\nadversarial adaptation forces features of both domains to be indistinguishable\nwhich enables us to use regression models trained on synthetic domain to be\nused on real samples. On the challenging MPIIGaze real life dataset, we\noutperform recent fully supervised methods trained on manually annotated real\nsamples by appreciable margins and also achieve 13\\% more relative gain after\nadaptation compared to the current benchmark method of SimGAN\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 07:15:06 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Lahiri", "Avisek", ""], ["Agarwalla", "Abhinav", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1810.07945", "submitter": "Quoc-Tin Phan", "authors": "Quoc-Tin Phan, Giulia Boato, Francesco G. B. De Natale", "title": "Accurate and Scalable Image Clustering Based On Sparse Representation of\n  Camera Fingerprint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering images according to their acquisition devices is a well-known\nproblem in multimedia forensics, which is typically faced by means of camera\nSensor Pattern Noise (SPN). Such an issue is challenging since SPN is a\nnoise-like signal, hard to be estimated and easy to be attenuated or destroyed\nby many factors. Moreover, the high dimensionality of SPN hinders large-scale\napplications. Existing approaches are typically based on the correlation among\nSPNs in the pixel domain, which might not be able to capture intrinsic data\nstructure in union of vector subspaces. In this paper, we propose an accurate\nclustering framework, which exploits linear dependencies among SPNs in their\nintrinsic vector subspaces. Such dependencies are encoded under sparse\nrepresentations which are obtained by solving a LASSO problem with\nnon-negativity constraint. The proposed framework is highly accurate in number\nof clusters estimation and image association. Moreover, our framework is\nscalable to the number of images and robust against double JPEG compression as\nwell as the presence of outliers, owning big potential for real-world\napplications. Experimental results on Dresden and Vision database show that our\nproposed framework can adapt well to both medium-scale and large-scale\ncontexts, and outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 08:33:21 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 11:56:38 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Phan", "Quoc-Tin", ""], ["Boato", "Giulia", ""], ["De Natale", "Francesco G. B.", ""]]}, {"id": "1810.07960", "submitter": "Bolun Zheng", "authors": "Bolun Zheng, Rui Sun, Xiang Tian, Yaowu Chen", "title": "S-Net: A Scalable Convolutional Neural Network for JPEG Compression\n  Artifact Reduction", "comments": "accepted by Journal of Electronic Imaging", "journal-ref": null, "doi": "10.1117/1.JEI.27.4.043037", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have used deep residual convolutional neural networks (CNNs)\nfor JPEG compression artifact reduction. This study proposes a scalable CNN\ncalled S-Net. Our approach effectively adjusts the network scale dynamically in\na multitask system for real-time operation with little performance loss. It\noffers a simple and direct technique to evaluate the performance gains obtained\nwith increasing network depth, and it is helpful for removing redundant network\nlayers to maximize the network efficiency. We implement our architecture using\nthe Keras framework with the TensorFlow backend on an NVIDIA K80 GPU server. We\ntrain our models on the DIV2K dataset and evaluate their performance on public\nbenchmark datasets. To validate the generality and universality of the proposed\nmethod, we created and utilized a new dataset, called WIN143, for\nover-processed images evaluation. Experimental results indicate that our\nproposed approach outperforms other CNN-based methods and achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 09:21:44 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zheng", "Bolun", ""], ["Sun", "Rui", ""], ["Tian", "Xiang", ""], ["Chen", "Yaowu", ""]]}, {"id": "1810.07961", "submitter": "Pulkit Kumar", "authors": "Simmi Mourya, Sonaal Kant, Pulkit Kumar, Anubha Gupta, Ritu Gupta", "title": "LeukoNet: DCT-based CNN architecture for the classification of normal\n  versus Leukemic blasts in B-ALL Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute lymphoblastic leukemia (ALL) constitutes approximately 25% of the\npediatric cancers. In general, the task of identifying immature leukemic blasts\nfrom normal cells under the microscope is challenging because morphologically\nthe images of the two cells appear similar. In this paper, we propose a deep\nlearning framework for classifying immature leukemic blasts and normal cells.\nThe proposed model combines the Discrete Cosine Transform (DCT) domain features\nextracted via CNN with the Optical Density (OD) space features to build a\nrobust classifier. Elaborate experiments have been conducted to validate the\nproposed LeukoNet classifier.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 09:24:14 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 08:05:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Mourya", "Simmi", ""], ["Kant", "Sonaal", ""], ["Kumar", "Pulkit", ""], ["Gupta", "Anubha", ""], ["Gupta", "Ritu", ""]]}, {"id": "1810.08016", "submitter": "Mikhail Aliev", "authors": "Yulia S. Chernyshova, Mikhail A. Aliev, Ekaterina S. Gushchanskaia and\n  Alexander V. Sheshkus", "title": "Optical Font Recognition in Smartphone-Captured Images, and its\n  Applicability for ID Forgery Detection", "comments": null, "journal-ref": null, "doi": "10.1117/12.2522955", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of detecting counterfeit identity\ndocuments in images captured with smartphones. As the number of documents\ncontain special fonts, we study the applicability of convolutional neural\nnetworks (CNNs) for detection of the conformance of the fonts used with the\nones, corresponding to the government standards. Here, we use multi-task\nlearning to differentiate samples by both fonts and characters and compare the\nresulting classifier with its analogue trained for binary font classification.\nWe train neural networks for authenticity estimation of the fonts used in\nmachine-readable zones and ID numbers of the Russian national passport and test\nthem on samples of individual characters acquired from 3238 images of the\nRussian national passport. Our results show that the usage of multi-task\nlearning increases sensitivity and specificity of the classifier. Moreover, the\nresulting CNNs demonstrate high generalization ability as they correctly\nclassify fonts which were not present in the training set. We conclude that the\nproposed method is sufficient for authentication of the fonts and can be used\nas a part of the forgery detection system for images acquired with a smartphone\ncamera.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 12:44:05 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chernyshova", "Yulia S.", ""], ["Aliev", "Mikhail A.", ""], ["Gushchanskaia", "Ekaterina S.", ""], ["Sheshkus", "Alexander V.", ""]]}, {"id": "1810.08042", "submitter": "Bolun Zheng", "authors": "Bolun Zheng, Yaowu Chen, Xiang Tian, Fan Zhou, Xuesong Liu", "title": "Implicit Dual-domain Convolutional Network for Robust Color Image\n  Compression Artifact Reduction", "comments": "accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology(T-CSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2931045", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several dual-domain convolutional neural network-based methods show\noutstanding performance in reducing image compression artifacts. However, they\nsuffer from handling color images because the compression processes for\ngray-scale and color images are completely different. Moreover, these methods\ntrain a specific model for each compression quality and require multiple models\nto achieve different compression qualities. To address these problems, we\nproposed an implicit dual-domain convolutional network (IDCN) with the pixel\nposition labeling map and the quantization tables as inputs. Specifically, we\nproposed an extractor-corrector framework-based dual-domain correction unit\n(DCU) as the basic component to formulate the IDCN. A dense block was\nintroduced to improve the performance of extractor in DRU. The implicit\ndual-domain translation allows the IDCN to handle color images with the\ndiscrete cosine transform (DCT)-domain priors. A flexible version of IDCN\n(IDCN-f) was developed to handle a wide range of compression qualities.\nExperiments for both objective and subjective evaluations on benchmark datasets\nshow that IDCN is superior to the state-of-the-art methods and IDCN-f exhibits\nexcellent abilities to handle a wide range of compression qualities with little\nperformance sacrifice and demonstrates great potential for practical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:28:06 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 01:17:24 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 02:29:05 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zheng", "Bolun", ""], ["Chen", "Yaowu", ""], ["Tian", "Xiang", ""], ["Zhou", "Fan", ""], ["Liu", "Xuesong", ""]]}, {"id": "1810.08097", "submitter": "Johan \\\"Ofverstedt", "authors": "Johan \\\"Ofverstedt, Joakim Lindblad, Nata\\v{s}a Sladoje", "title": "Stochastic Distance Transform", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": "In Proceedings of the 21th international conference on Discrete\n  Geometry for Computer Imagery (DGCI), LNCS-11414, pp. 75-86, Paris, France,\n  March 2019", "doi": "10.1007/978-3-030-14085-4_7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distance transform (DT) and its many variations are ubiquitous tools for\nimage processing and analysis. In many imaging scenarios, the images of\ninterest are corrupted by noise. This has a strong negative impact on the\naccuracy of the DT, which is highly sensitive to spurious noise points. In this\nstudy, we consider images represented as discrete random sets and observe\nstatistics of DT computed on such representations. We, thus, define a\nstochastic distance transform (SDT), which has an adjustable robustness to\nnoise. Both a stochastic Monte Carlo method and a deterministic method for\ncomputing the SDT are proposed and compared. Through a series of empirical\ntests, we demonstrate that the SDT is effective not only in improving the\naccuracy of the computed distances in the presence of noise, but also in\nimproving the performance of template matching and watershed segmentation of\npartially overlapping objects, which are examples of typical applications where\nDTs are utilized.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:07:58 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["\u00d6fverstedt", "Johan", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "1810.08100", "submitter": "Lijun Wang", "authors": "Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang, Zhe Lin,\n  Chih-Yao Hsieh, Sarah Kong, Huchuan Lu", "title": "DeepLens: Shallow Depth Of Field From A Single Image", "comments": "11 pages, 15 figures, accepted by SIGGRAPH Asia 2018, low-resolution\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We aim to generate high resolution shallow depth-of-field (DoF) images from a\nsingle all-in-focus image with controllable focal distance and aperture size.\nTo achieve this, we propose a novel neural network model comprised of a depth\nprediction module, a lens blur module, and a guided upsampling module. All\nmodules are differentiable and are learned from data. To train our depth\nprediction module, we collect a dataset of 2462 RGB-D images captured by mobile\nphones with a dual-lens camera, and use existing segmentation datasets to\nimprove border prediction. We further leverage a synthetic dataset with known\ndepth to supervise the lens blur and guided upsampling modules. The\neffectiveness of our system and training strategies are verified in the\nexperiments. Our method can generate high-quality shallow DoF images at high\nresolution, and produces significantly fewer artifacts than the baselines and\nexisting solutions for single image shallow DoF synthesis. Compared with the\niPhone portrait mode, which is a state-of-the-art shallow DoF solution based on\na dual-lens depth camera, our method generates comparable results, while\nallowing for greater flexibility to choose focal points and aperture size, and\nis not limited to one capture setup.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:14:41 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Wang", "Lijun", ""], ["Shen", "Xiaohui", ""], ["Zhang", "Jianming", ""], ["Wang", "Oliver", ""], ["Lin", "Zhe", ""], ["Hsieh", "Chih-Yao", ""], ["Kong", "Sarah", ""], ["Lu", "Huchuan", ""]]}, {"id": "1810.08103", "submitter": "Peng Sun", "authors": "Peng Sun, Guang Chen, Guerdan Luke, Yi Shang", "title": "Salience Biased Loss for Object Detection in Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in remote sensing, especially in aerial images, remains a\nchallenging problem due to low image resolution, complex backgrounds, and\nvariation of scale and angles of objects in images. In current implementations,\nmulti-scale based and angle-based networks have been proposed and generate\npromising results with aerial image detection. In this paper, we propose a\nnovel loss function, called Salience Biased Loss (SBL), for deep neural\nnetworks, which uses salience information of the input image to achieve\nimproved performance for object detection. Our novel loss function treats\ntraining examples differently based on input complexity in order to avoid the\nover-contribution of easy cases in the training process. In our experiments,\nRetinaNet was trained with SBL to generate an one-stage detector,\nSBL-RetinaNet. SBL-RetinaNet is applied to the largest existing public aerial\nimage dataset, DOTA. Experimental results show our proposed loss function with\nthe RetinaNet architecture outperformed other state-of-art object detection\nmodels by at least 4.31 mAP, and RetinaNet by 2.26 mAP with the same inference\nspeed of RetinaNet.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:18:58 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Sun", "Peng", ""], ["Chen", "Guang", ""], ["Luke", "Guerdan", ""], ["Shang", "Yi", ""]]}, {"id": "1810.08126", "submitter": "Peiye Liu", "authors": "Peiye Liu, Wu Liu, Huadong Ma, Tao Mei, Mingoo Seok", "title": "KTAN: Knowledge Transfer Adversarial Network", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the large computation and storage cost of a deep convolutional\nneural network, the knowledge distillation based methods have pioneered to\ntransfer the generalization ability of a large (teacher) deep network to a\nlight-weight (student) network. However, these methods mostly focus on\ntransferring the probability distribution of the softmax layer in a teacher\nnetwork and thus neglect the intermediate representations. In this paper, we\npropose a knowledge transfer adversarial network to better train a student\nnetwork. Our technique holistically considers both intermediate representations\nand probability distributions of a teacher network. To transfer the knowledge\nof intermediate representations, we set high-level teacher feature maps as a\ntarget, toward which the student feature maps are trained. Specifically, we\narrange a Teacher-to-Student layer for enabling our framework suitable for\nvarious student structures. The intermediate representation helps the student\nnetwork better understand the transferred generalization as compared to the\nprobability distribution only. Furthermore, we infuse an adversarial learning\nprocess by employing a discriminator network, which can fully exploit the\nspatial correlation of feature maps in training a student network. The\nexperimental results demonstrate that the proposed method can significantly\nimprove the performance of a student network on both image classification and\nobject detection tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:57:02 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Liu", "Peiye", ""], ["Liu", "Wu", ""], ["Ma", "Huadong", ""], ["Mei", "Tao", ""], ["Seok", "Mingoo", ""]]}, {"id": "1810.08169", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang and Ming Jiang", "title": "Exploiting High-Level Semantics for No-Reference Image Quality\n  Assessment of Realistic Blur Images", "comments": "correct typos, e.g., \"avarage\" -> \"average\" in the figure of the\n  proposed framework", "journal-ref": "Proceedings of the 2017 ACM on Multimedia Conference", "doi": "10.1145/3123266.3123322", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To guarantee a satisfying Quality of Experience (QoE) for consumers, it is\nrequired to measure image quality efficiently and reliably. The neglect of the\nhigh-level semantic information may result in predicting a clear blue sky as\nbad quality, which is inconsistent with human perception. Therefore, in this\npaper, we tackle this problem by exploiting the high-level semantics and\npropose a novel no-reference image quality assessment method for realistic blur\nimages. Firstly, the whole image is divided into multiple overlapping patches.\nSecondly, each patch is represented by the high-level feature extracted from\nthe pre-trained deep convolutional neural network model. Thirdly, three\ndifferent kinds of statistical structures are adopted to aggregate the\ninformation from different patches, which mainly contain some common statistics\n(i.e., the mean\\&standard deviation, quantiles and moments). Finally, the\naggregated features are fed into a linear regression model to predict the image\nquality. Experiments show that, compared with low-level features, high-level\nfeatures indeed play a more critical role in resolving the aforementioned\nchallenging problem for quality estimation. Besides, the proposed method\nsignificantly outperforms the state-of-the-art methods on two realistic blur\nimage databases and achieves comparable performance on two synthetic blur image\ndatabases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 17:15:26 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "1810.08189", "submitter": "Cheng Kang Hsieh", "authors": "Cheng-Kang Hsieh, Miguel Campo, Abhinav Taliyan, Matt Nickens,\n  Mitkumar Pandya, JJ Espinoza", "title": "Convolutional Collaborative Filter Network for Video Based\n  Recommendation Systems", "comments": "8 pages, 3 figures, 1 table include ablation study. arguments /\n  results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This analysis explores the temporal sequencing of objects in a movie trailer.\nTemporal sequencing of objects in a movie trailer (e.g., a long shot of an\nobject vs intermittent short shots) can convey information about the type of\nmovie, plot of the movie, role of the main characters, and the filmmakers\ncinematographic choices. When combined with historical customer data,\nsequencing analysis can be used to improve predictions of customer behavior.\nE.g., a customer buys tickets to a new movie and maybe the customer has seen\nmovies in the past that contained similar sequences. To explore object\nsequencing in movie trailers, we propose a video convolutional network to\ncapture actions and scenes that are predictive of customers' preferences. The\nmodel learns the specific nature of sequences for different types of objects\n(e.g., cars vs faces), and the role of sequences in predicting customer future\nbehavior. We show how such a temporal-aware model outperforms simple feature\npooling methods proposed in our previous works and, importantly, demonstrate\nthe additional model explain-ability allowed by such a model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 17:57:58 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 20:43:16 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Hsieh", "Cheng-Kang", ""], ["Campo", "Miguel", ""], ["Taliyan", "Abhinav", ""], ["Nickens", "Matt", ""], ["Pandya", "Mitkumar", ""], ["Espinoza", "JJ", ""]]}, {"id": "1810.08229", "submitter": "Qiaoying Huang", "authors": "Qiaoying Huang, Dong Yang, Pengxiang Wu, Hui Qu, Jingru Yi, Dimitris\n  Metaxas", "title": "MRI Reconstruction via Cascaded Channel-wise Attention Network", "comments": "Accepted by the IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2019. Code is available now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an MRI reconstruction problem with input of k-space data at a\nvery low undersampled rate. This can practically benefit patient due to reduced\ntime of MRI scan, but it is also challenging since quality of reconstruction\nmay be compromised. Currently, deep learning based methods dominate MRI\nreconstruction over traditional approaches such as Compressed Sensing, but they\nrarely show satisfactory performance in the case of low undersampled k-space\ndata. One explanation is that these methods treat channel-wise features\nequally, which results in degraded representation ability of the neural\nnetwork. To solve this problem, we propose a new model called MRI Cascaded\nChannel-wise Attention Network (MICCAN), highlighted by three components: (i) a\nvariant of U-net with Channel-wise Attention (UCA) module, (ii) a long skip\nconnection and (iii) a combined loss. Our model is able to attend to salient\ninformation by filtering irrelevant features and also concentrate on\nhigh-frequency information by enforcing low-frequency information bypassed to\nthe final output. We conduct both quantitative evaluation and qualitative\nanalysis of our method on a cardiac dataset. The experiment shows that our\nmethod achieves very promising results in terms of three common metrics on the\nMRI reconstruction with low undersampled k-space data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 18:37:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 21:11:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Huang", "Qiaoying", ""], ["Yang", "Dong", ""], ["Wu", "Pengxiang", ""], ["Qu", "Hui", ""], ["Yi", "Jingru", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1810.08290", "submitter": "Jonathan Krause", "authors": "Paisan Raumviboonsuk, Jonathan Krause, Peranut Chotcomwongse, Rory\n  Sayres, Rajiv Raman, Kasumi Widner, Bilson J L Campana, Sonia Phene, Kornwipa\n  Hemarat, Mongkol Tadarati, Sukhum Silpa-Acha, Jirawut Limwattanayingyong,\n  Chetan Rao, Oscar Kuruvilla, Jesse Jung, Jeffrey Tan, Surapong Orprayoon,\n  Chawawat Kangwanwongpaisan, Ramase Sukulmalpaiboon, Chainarong\n  Luengchaichawang, Jitumporn Fuangkaew, Pipat Kongsap, Lamyong Chualinpha,\n  Sarawuth Saree, Srirat Kawinpanitan, Korntip Mitvongsa, Siriporn Lawanasakol,\n  Chaiyasit Thepchatri, Lalita Wongpichedchai, Greg S Corrado, Lily Peng, Dale\n  R Webster", "title": "Deep Learning vs. Human Graders for Classifying Severity Levels of\n  Diabetic Retinopathy in a Real-World Nationwide Screening Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have been used to detect diabetic retinopathy (DR)\nwith specialist-level accuracy. This study aims to validate one such algorithm\non a large-scale clinical population, and compare the algorithm performance\nwith that of human graders. 25,326 gradable retinal images of patients with\ndiabetes from the community-based, nation-wide screening program of DR in\nThailand were analyzed for DR severity and referable diabetic macular edema\n(DME). Grades adjudicated by a panel of international retinal specialists\nserved as the reference standard. Across different severity levels of DR for\ndetermining referable disease, deep learning significantly reduced the false\nnegative rate (by 23%) at the cost of slightly higher false positive rates\n(2%). Deep learning algorithms may serve as a valuable tool for DR screening.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 22:17:45 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Raumviboonsuk", "Paisan", ""], ["Krause", "Jonathan", ""], ["Chotcomwongse", "Peranut", ""], ["Sayres", "Rory", ""], ["Raman", "Rajiv", ""], ["Widner", "Kasumi", ""], ["Campana", "Bilson J L", ""], ["Phene", "Sonia", ""], ["Hemarat", "Kornwipa", ""], ["Tadarati", "Mongkol", ""], ["Silpa-Acha", "Sukhum", ""], ["Limwattanayingyong", "Jirawut", ""], ["Rao", "Chetan", ""], ["Kuruvilla", "Oscar", ""], ["Jung", "Jesse", ""], ["Tan", "Jeffrey", ""], ["Orprayoon", "Surapong", ""], ["Kangwanwongpaisan", "Chawawat", ""], ["Sukulmalpaiboon", "Ramase", ""], ["Luengchaichawang", "Chainarong", ""], ["Fuangkaew", "Jitumporn", ""], ["Kongsap", "Pipat", ""], ["Chualinpha", "Lamyong", ""], ["Saree", "Sarawuth", ""], ["Kawinpanitan", "Srirat", ""], ["Mitvongsa", "Korntip", ""], ["Lawanasakol", "Siriporn", ""], ["Thepchatri", "Chaiyasit", ""], ["Wongpichedchai", "Lalita", ""], ["Corrado", "Greg S", ""], ["Peng", "Lily", ""], ["Webster", "Dale R", ""]]}, {"id": "1810.08293", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Jinsol Lee and Ghassan AlRegib", "title": "CURE-OR: Challenging Unreal and Real Environments for Object Recognition", "comments": "8 pages, 7 figures, 4 tables", "journal-ref": "D. Temel, J. Lee, and G. AlRegib, \"CURE-OR: Challenging unreal and\n  real environments for object recognition,\" 2018 17th IEEE International\n  Conference on Machine Learning and Applications (ICMLA), Orlando, Florida,\n  USA, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a large-scale, controlled, and multi-platform\nobject recognition dataset denoted as Challenging Unreal and Real Environments\nfor Object Recognition (CURE-OR). In this dataset, there are 1,000,000 images\nof 100 objects with varying size, color, and texture that are positioned in\nfive different orientations and captured using five devices including a webcam,\na DSLR, and three smartphone cameras in real-world (real) and studio (unreal)\nenvironments. The controlled challenging conditions include underexposure,\noverexposure, blur, contrast, dirty lens, image noise, resizing, and loss of\ncolor information. We utilize CURE-OR dataset to test recognition APIs-Amazon\nRekognition and Microsoft Azure Computer Vision- and show that their\nperformance significantly degrades under challenging conditions. Moreover, we\ninvestigate the relationship between object recognition and image quality and\nshow that objective quality algorithms can estimate recognition performance\nunder certain photometric challenging conditions. The dataset is publicly\navailable at https://ghassanalregib.com/cure-or/.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 22:23:50 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:24:17 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Temel", "Dogancan", ""], ["Lee", "Jinsol", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1810.08310", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Paul Rosen", "title": "An Efficient Data Retrieval Parallel Reeb Graph Algorithm", "comments": "30 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reeb graph of a scalar function defined on a domain gives a topologically\nmeaningful summary of that domain. Reeb graphs have been shown in the past\ndecade to be of great importance in geometric processing, image processing,\ncomputer graphics, and computational topology. The demand for analyzing large\ndata sets has increased in the last decade. Hence the parallelization of\ntopological computations needs to be more fully considered. We propose a\nparallel augmented Reeb graph algorithm on triangulated meshes with and without\na boundary. That is, in addition to our parallel algorithm for computing a Reeb\ngraph, we describe a method for extracting the original manifold data from the\nReeb graph structure. We demonstrate the running time of our algorithm on\nstandard datasets. As an application, we show how our algorithm can be utilized\nin mesh segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 23:49:26 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 05:09:51 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 02:47:37 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 04:30:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hajij", "Mustafa", ""], ["Rosen", "Paul", ""]]}, {"id": "1810.08315", "submitter": "Abdullah Nazib", "authors": "Abdullah Nazib, Clinton Fookes, Dimitri Perrin", "title": "A Comparative Analysis of Registration Tools: Traditional vs Deep\n  Learning Approach on High Resolution Tissue Cleared Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image registration plays an important role in comparing images. It is\nparticularly important in analyzing medical images like CT, MRI, PET, etc. to\nquantify different biological samples, to monitor disease progression and to\nfuse different modalities to support better diagnosis. The recent emergence of\ntissue clearing protocols enable us to take images at cellular level\nresolution. Image registration tools developed for other modalities are\ncurrently unable to manage images of such extreme high resolution. The recent\npopularity of deep learning based methods in the computer vision community\njustifies a rigorous investigation of deep-learning based methods on tissue\ncleared images along with their traditional counterparts. In this paper, we\ninvestigate and compare the performance of a deep learning based registration\nmethod with traditional optimization based methods on samples from\ntissue-clearing methods. From the comparative results it is found that a\ndeep-learning based method outperforms all traditional registration tools in\nterms of registration time and has achieved promising registration accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 00:22:07 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Nazib", "Abdullah", ""], ["Fookes", "Clinton", ""], ["Perrin", "Dimitri", ""]]}, {"id": "1810.08322", "submitter": "Dae Hoon Park", "authors": "Chiu Man Ho, Dae Hoon Park, Wei Yang, Yi Chang", "title": "Sequenced-Replacement Sampling for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sequenced-replacement sampling (SRS) for training deep neural\nnetworks. The basic idea is to assign a fixed sequence index to each sample in\nthe dataset. Once a mini-batch is randomly drawn in each training iteration, we\nrefill the original dataset by successively adding samples according to their\nsequence index. Thus we carry out replacement sampling but in a batched and\nsequenced way. In a sense, SRS could be viewed as a way of performing\n\"mini-batch augmentation\". It is particularly useful for a task where we have a\nrelatively small images-per-class such as CIFAR-100. Together with a longer\nperiod of initial large learning rate, it significantly improves the\nclassification accuracy in CIFAR-100 over the current state-of-the-art results.\nOur experiments indicate that training deeper networks with SRS is less prone\nto over-fitting. In the best case, we achieve an error rate as low as 10.10%.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 00:55:47 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Ho", "Chiu Man", ""], ["Park", "Dae Hoon", ""], ["Yang", "Wei", ""], ["Chang", "Yi", ""]]}, {"id": "1810.08326", "submitter": "Zhiwu Lu", "authors": "An Zhao, Mingyu Ding, Jiechao Guan, Zhiwu Lu, Tao Xiang, and Ji-Rong\n  Wen", "title": "Domain-Invariant Projection Learning for Zero-Shot Recognition", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen object classes without any\ntraining samples, which can be regarded as a form of transfer learning from\nseen classes to unseen ones. This is made possible by learning a projection\nbetween a feature space and a semantic space (e.g. attribute space). Key to ZSL\nis thus to learn a projection function that is robust against the often large\ndomain gap between the seen and unseen classes. In this paper, we propose a\nnovel ZSL model termed domain-invariant projection learning (DIPL). Our model\nhas two novel components: (1) A domain-invariant feature self-reconstruction\ntask is introduced to the seen/unseen class data, resulting in a simple linear\nformulation that casts ZSL into a min-min optimization problem. Solving the\nproblem is non-trivial, and a novel iterative algorithm is formulated as the\nsolver, with rigorous theoretic algorithm analysis provided. (2) To further\nalign the two domains via the learned projection, shared semantic structure\namong seen and unseen classes is explored via forming superclasses in the\nsemantic space. Extensive experiments show that our model outperforms the\nstate-of-the-art alternatives by significant margins.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 01:08:05 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Zhao", "An", ""], ["Ding", "Mingyu", ""], ["Guan", "Jiechao", ""], ["Lu", "Zhiwu", ""], ["Xiang", "Tao", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1810.08329", "submitter": "Zhiwu Lu", "authors": "Aoxue Li, Zhiwu Lu, Jiechao Guan, Tao Xiang, Liwei Wang, and Ji-Rong\n  Wen", "title": "Transferrable Feature and Projection Learning with Class Hierarchy for\n  Zero-Shot Learning", "comments": "Submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to\nunseen ones so that the latter can be recognised without any training samples.\nThis is made possible by learning a projection function between a feature space\nand a semantic space (e.g. attribute space). Considering the seen and unseen\nclasses as two domains, a big domain gap often exists which challenges ZSL.\nInspired by the fact that an unseen class is not exactly `unseen' if it belongs\nto the same superclass as a seen class, we propose a novel inductive ZSL model\nthat leverages superclasses as the bridge between seen and unseen classes to\nnarrow the domain gap. Specifically, we first build a class hierarchy of\nmultiple superclass layers and a single class layer, where the superclasses are\nautomatically generated by data-driven clustering over the semantic\nrepresentations of all seen and unseen class names. We then exploit the\nsuperclasses from the class hierarchy to tackle the domain gap challenge in two\naspects: deep feature learning and projection function learning. First, to\nnarrow the domain gap in the feature space, we integrate a recurrent neural\nnetwork (RNN) defined with the superclasses into a convolutional neural network\n(CNN), in order to enforce the superclass hierarchy. Second, to further learn a\ntransferrable projection function for ZSL, a novel projection function learning\nmethod is proposed by exploiting the superclasses to align the two domains.\nImportantly, our transferrable feature and projection learning methods can be\neasily extended to a closely related task -- few-shot learning (FSL). Extensive\nexperiments show that the proposed model significantly outperforms the\nstate-of-the-art alternatives in both ZSL and FSL tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 01:21:08 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Li", "Aoxue", ""], ["Lu", "Zhiwu", ""], ["Guan", "Jiechao", ""], ["Xiang", "Tao", ""], ["Wang", "Liwei", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1810.08332", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Jiechao Guan, Aoxue Li, Tao Xiang, An Zhao, and Ji-Rong Wen", "title": "Zero and Few Shot Learning with Semantic Feature Synthesis and\n  Competitive Learning", "comments": "Submitted to IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is made possible by learning a projection function\nbetween a feature space and a semantic space (e.g.,~an attribute space). Key to\nZSL is thus to learn a projection that is robust against the often large domain\ngap between the seen and unseen class domains. In this work, this is achieved\nby unseen class data synthesis and robust projection function learning.\nSpecifically, a novel semantic data synthesis strategy is proposed, by which\nsemantic class prototypes (e.g., attribute vectors) are used to simply perturb\nseen class data for generating unseen class ones. As in any data\nsynthesis/hallucination approach, there are ambiguities and uncertainties on\nhow well the synthesised data can capture the targeted unseen class data\ndistribution. To cope with this, the second contribution of this work is a\nnovel projection learning model termed competitive bidirectional projection\nlearning (BPL) designed to best utilise the ambiguous synthesised data.\nSpecifically, we assume that each synthesised data point can belong to any\nunseen class; and the most likely two class candidates are exploited to learn a\nrobust projection function in a competitive fashion. As a third contribution,\nwe show that the proposed ZSL model can be easily extended to few-shot learning\n(FSL) by again exploiting semantic (class prototype guided) feature synthesis\nand competitive BPL. Extensive experiments show that our model achieves the\nstate-of-the-art results on both problems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 01:52:03 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Lu", "Zhiwu", ""], ["Guan", "Jiechao", ""], ["Li", "Aoxue", ""], ["Xiang", "Tao", ""], ["Zhao", "An", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1810.08338", "submitter": "Hengkai Guo", "authors": "Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen Lu, Linfu\n  Wen", "title": "Multi-Domain Pose Network for Multi-Person Pose Estimation and Tracking", "comments": "Extended abstract for the ECCV 2018 PoseTrack Workshop", "journal-ref": null, "doi": "10.1007/978-3-030-11012-3_17", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person human pose estimation and tracking in the wild is important and\nchallenging. For training a powerful model, large-scale training data are\ncrucial. While there are several datasets for human pose estimation, the best\npractice for training on multi-dataset has not been investigated. In this\npaper, we present a simple network called Multi-Domain Pose Network (MDPN) to\naddress this problem. By treating the task as multi-domain learning, our\nmethods can learn a better representation for pose prediction. Together with\nprediction heads fine-tuning and multi-branch combination, it shows significant\nimprovement over baselines and achieves the best performance on PoseTrack ECCV\n2018 Challenge without additional datasets other than MPII and COCO.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 02:51:14 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Guo", "Hengkai", ""], ["Tang", "Tang", ""], ["Luo", "Guozhong", ""], ["Chen", "Riwei", ""], ["Lu", "Yongchen", ""], ["Wen", "Linfu", ""]]}, {"id": "1810.08352", "submitter": "Han Liu", "authors": "Han Liu, Dan Zeng, Qi Tian", "title": "Super-pixel cloud detection using Hierarchical Fusion CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud detection plays a very important role in the process of remote sensing\nimages. This paper designs a super-pixel level cloud detection method based on\nconvolutional neural network (CNN) and deep forest. Firstly, remote sensing\nimages are segmented into super-pixels through the combination of SLIC and\nSEEDS. Structured forests is carried out to compute edge probability of each\npixel, based on which super-pixels are segmented more precisely. Segmented\nsuper-pixels compose a super-pixel level remote sensing database. Though cloud\ndetection is essentially a binary classification problem, our database is\nlabeled into four categories: thick cloud, cirrus cloud, building and other\nculture, to improve the generalization ability of our proposed models.\nSecondly, super-pixel level database is used to train our cloud detection\nmodels based on CNN and deep forest. Considering super-pixel level remote\nsensing images contain less semantic information compared with general object\nclassification database, we propose a Hierarchical Fusion CNN (HFCNN). It takes\nfull advantage of low-level features like color and texture information and is\nmore applicable to cloud detection task. In test phase, every super-pixel in\nremote sensing images is classified by our proposed models and then combined to\nrecover final binary mask by our proposed distance metric, which is used to\ndetermine ambiguous super-pixels. Experimental results show that, compared with\nconventional methods, HFCNN can achieve better precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 04:37:46 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Liu", "Han", ""], ["Zeng", "Dan", ""], ["Tian", "Qi", ""]]}, {"id": "1810.08375", "submitter": "Wen Wang", "authors": "Wen Wang, Yongjian Wu, Haijun Liu, Shiguang Wang, Jian Cheng", "title": "Temporal Action Detection by Joint Identification-Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action detection aims at not only recognizing action category but\nalso detecting start time and end time for each action instance in an untrimmed\nvideo. The key challenge of this task is to accurately classify the action and\ndetermine the temporal boundaries of each action instance. In temporal action\ndetection benchmark: THUMOS 2014, large variations exist in the same action\ncategory while many similarities exist in different action categories, which\nalways limit the performance of temporal action detection. To address this\nproblem, we propose to use joint Identification-Verification network to reduce\nthe intra-action variations and enlarge inter-action differences. The joint\nIdentification-Verification network is a siamese network based on 3D ConvNets,\nwhich can simultaneously predict the action categories and the similarity\nscores for the input pairs of video proposal segments. Extensive experimental\nresults on the challenging THUMOS 2014 dataset demonstrate the effectiveness of\nour proposed method compared to the existing state-of-art methods for temporal\naction detection in untrimmed videos.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 07:22:40 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Wang", "Wen", ""], ["Wu", "Yongjian", ""], ["Liu", "Haijun", ""], ["Wang", "Shiguang", ""], ["Cheng", "Jian", ""]]}, {"id": "1810.08378", "submitter": "Fengdong Sun", "authors": "Fengdong Sun, Wenhui Li", "title": "Saliency guided deep network for weakly-supervised image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised image segmentation is an important task in computer vision.\nA key problem is how to obtain high quality objects location from image-level\ncategory. Classification activation mapping is a common method which can be\nused to generate high-precise object location cues. However these location cues\nare generally very sparse and small such that they can not provide effective\ninformation for image segmentation. In this paper, we propose a saliency guided\nimage segmentation network to resolve this problem. We employ a self-attention\nsaliency method to generate subtle saliency maps, and render the location cues\ngrow as seeds by seeded region growing method to expand pixel-level labels\nextent. In the process of seeds growing, we use the saliency values to weight\nthe similarity between pixels to control the growing. Therefore saliency\ninformation could help generate discriminative object regions, and the effects\nof wrong salient pixels can be suppressed efficiently. Experimental results on\na common segmentation dataset PASCAL VOC2012 demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 07:35:40 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Sun", "Fengdong", ""], ["Li", "Wenhui", ""]]}, {"id": "1810.08393", "submitter": "Iaroslav Melekhov", "authors": "Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys,\n  Esa Rahtu, Juho Kannala", "title": "DGC-Net: Dense Geometric Correspondence Network", "comments": "Supplementary material included; Affiliation section has been changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of dense pixel correspondence estimation\nbetween two images. This problem is closely related to optical flow estimation\ntask where ConvNets (CNNs) have recently achieved significant progress. While\noptical flow methods produce very accurate results for the small pixel\ntranslation and limited appearance variation scenarios, they hardly deal with\nthe strong geometric transformations that we consider in this work. In this\npaper, we propose a coarse-to-fine CNN-based framework that can leverage the\nadvantages of optical flow approaches and extend them to the case of large\ntransformations providing dense and subpixel accurate estimates. It is trained\non synthetic transformations and demonstrates very good performance to unseen,\nrealistic, data. Further, we apply our method to the problem of relative camera\npose estimation and demonstrate that the model outperforms existing dense\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 08:28:59 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 14:54:57 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Melekhov", "Iaroslav", ""], ["Tiulpin", "Aleksei", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""]]}, {"id": "1810.08412", "submitter": "Sahin I\\c{s}{\\i}k Dr", "authors": "\\c{S}ahin I\\c{s}{\\i}k, Kemal \\\"Ozkan, \\\"Omer Nezih Gerek", "title": "CVABS: Moving Object Segmentation with Common Vector Approach for Videos", "comments": "12 Pages, 4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background modelling is a fundamental step for several real-time computer\nvision applications that requires security systems and monitoring. An accurate\nbackground model helps detecting activity of moving objects in the video. In\nthis work, we have developed a new subspace based background modelling\nalgorithm using the concept of Common Vector Approach with Gram-Schmidt\northogonalization. Once the background model that involves the common\ncharacteristic of different views corresponding to the same scene is acquired,\na smart foreground detection and background updating procedure is applied based\non dynamic control parameters. A variety of experiments is conducted on\ndifferent problem types related to dynamic backgrounds. Several types of\nmetrics are utilized as objective measures and the obtained visual results are\njudged subjectively. It was observed that the proposed method stands\nsuccessfully for all problem types reported on CDNet2014 dataset by updating\nthe background frames with a self-learning feedback mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 09:10:42 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["I\u015f\u0131k", "\u015eahin", ""], ["\u00d6zkan", "Kemal", ""], ["Gerek", "\u00d6mer Nezih", ""]]}, {"id": "1810.08425", "submitter": "Rui Zhu", "authors": "Rui Zhu, Shifeng Zhang, Xiaobo Wang, Longyin Wen, Hailin Shi, Liefeng\n  Bo, Tao Mei", "title": "ScratchDet: Training Single-Shot Object Detectors from Scratch", "comments": "CVPR2019 Oral Presentation. Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art object objectors are fine-tuned from the\noff-the-shelf networks pretrained on large-scale classification dataset\nImageNet, which incurs some additional problems: 1) The classification and\ndetection have different degrees of sensitivity to translation, resulting in\nthe learning objective bias; 2) The architecture is limited by the\nclassification network, leading to the inconvenience of modification. To cope\nwith these problems, training detectors from scratch is a feasible solution.\nHowever, the detectors trained from scratch generally perform worse than the\npretrained ones, even suffer from the convergence issue in training. In this\npaper, we explore to train object detectors from scratch robustly. By analysing\nthe previous work on optimization landscape, we find that one of the overlooked\npoints in current trained-from-scratch detector is the BatchNorm. Resorting to\nthe stable and predictable gradient brought by BatchNorm, detectors can be\ntrained from scratch stably while keeping the favourable performance\nindependent to the network architecture. Taking this advantage, we are able to\nexplore various types of networks for object detection, without suffering from\nthe poor convergence. By extensive experiments and analyses on downsampling\nfactor, we propose the Root-ResNet backbone network, which makes full use of\nthe information from original images. Our ScratchDet achieves the\nstate-of-the-art accuracy on PASCAL VOC 2007, 2012 and MS COCO among all the\ntrain-from-scratch detectors and even performs better than several one-stage\npretrained methods. Codes will be made publicly available at\nhttps://github.com/KimSoybean/ScratchDet.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 09:57:50 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 04:27:27 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 14:30:25 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 03:33:25 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zhu", "Rui", ""], ["Zhang", "Shifeng", ""], ["Wang", "Xiaobo", ""], ["Wen", "Longyin", ""], ["Shi", "Hailin", ""], ["Bo", "Liefeng", ""], ["Mei", "Tao", ""]]}, {"id": "1810.08427", "submitter": "Filip Malmberg", "authors": "Simon Ekstr\\\"om, Filip Malmberg, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg,\n  Robin Strand", "title": "Fast Graph-Cut Based Optimization for Practical Dense Deformable\n  Registration of Volume Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Deformable image registration is a fundamental problem in medical\nimage analysis, with applications such as longitudinal studies, population\nmodeling, and atlas based image segmentation. Registration is often phrased as\nan optimization problem, i.e., finding a deformation field that is optimal\naccording to a given objective function. Discrete, combinatorial, optimization\ntechniques have successfully been employed to solve the resulting optimization\nproblem. Specifically, optimization based on $\\alpha$-expansion with minimal\ngraph cuts has been proposed as a powerful tool for image registration. The\nhigh computational cost of the graph-cut based optimization approach, however,\nlimits the utility of this approach for registration of large volume images.\nMethods: Here, we propose to accelerate graph-cut based deformable registration\nby dividing the image into overlapping sub-regions and restricting the\n$\\alpha$-expansion moves to a single sub-region at a time. Results: We\ndemonstrate empirically that this approach can achieve a large reduction in\ncomputation time -- from days to minutes -- with only a small penalty in terms\nof solution quality. Conclusion: The reduction in computation time provided by\nthe proposed method makes graph cut based deformable registration viable for\nlarge volume images. Significance: Graph cut based image registration has\npreviously been shown to produce excellent results, but the high computational\ncost has hindered the adoption of the method for registration of large medical\nvolume images. Our proposed method lifts this restriction, requiring only a\nsmall fraction of the computational cost to produce results of comparable\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 10:09:54 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Ekstr\u00f6m", "Simon", ""], ["Malmberg", "Filip", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""], ["Strand", "Robin", ""]]}, {"id": "1810.08437", "submitter": "Nuno C. Garcia", "authors": "Nuno C. Garcia, Pietro Morerio, and Vittorio Murino", "title": "Learning with privileged information via adversarial discriminative\n  modality distillation", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2929038", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous data modalities can provide complementary cues for several\ntasks, usually leading to more robust algorithms and better performance.\nHowever, while training data can be accurately collected to include a variety\nof sensory modalities, it is often the case that not all of them are available\nin real life (testing) scenarios, where a model has to be deployed. This raises\nthe challenge of how to extract information from multimodal data in the\ntraining stage, in a form that can be exploited at test time, considering\nlimitations such as noisy or missing modalities. This paper presents a new\napproach in this direction for RGB-D vision tasks, developed within the\nadversarial learning and privileged information frameworks. We consider the\npractical case of learning representations from depth and RGB videos, while\nrelying only on RGB data at test time. We propose a new approach to train a\nhallucination network that learns to distill depth information via adversarial\nlearning, resulting in a clean approach without several losses to balance or\nhyperparameters. We report state-of-the-art results on object classification on\nthe NYUD dataset and video action recognition on the largest multimodal dataset\navailable for this task, the NTU RGB+D, as well as on the Northwestern-UCLA.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 10:49:11 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 13:03:29 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Garcia", "Nuno C.", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1810.08452", "submitter": "Rodrigo Caye Daudt", "authors": "Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau", "title": "Multitask Learning for Large-scale Semantic Change Detection", "comments": "Preprint submitted to Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Change detection is one of the main problems in remote sensing, and is\nessential to the accurate processing and understanding of the large scale Earth\nobservation data available through programs such as Sentinel and Landsat. Most\nof the recently proposed change detection methods bring deep learning to this\ncontext, but openly available change detection datasets are still very scarce,\nwhich limits the methods that can be proposed and tested. In this paper we\npresent the first large scale high resolution semantic change detection (HRSCD)\ndataset, which enables the usage of deep learning methods for semantic change\ndetection. The dataset contains coregistered RGB image pairs, pixel-wise change\ninformation and land cover information. We then propose several methods using\nfully convolutional neural networks to perform semantic change detection. Most\nnotably, we present a network architecture that performs change detection and\nland cover mapping simultaneously, while using the predicted land cover\ninformation to help to predict changes. We also describe a sequential training\nscheme that allows this network to be trained without setting a hyperparameter\nthat balances different loss functions and achieves the best overall results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 12:01:51 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 15:29:38 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Daudt", "Rodrigo Caye", ""], ["Saux", "Bertrand Le", ""], ["Boulch", "Alexandre", ""], ["Gousseau", "Yann", ""]]}, {"id": "1810.08462", "submitter": "Rodrigo Caye Daudt", "authors": "Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch", "title": "Fully Convolutional Siamese Networks for Change Detection", "comments": "To appear inProc. ICIP 2018, October 07-10, 2018, Athens, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents three fully convolutional neural network architectures\nwhich perform change detection using a pair of coregistered images. Most\nnotably, we propose two Siamese extensions of fully convolutional networks\nwhich use heuristics about the current problem to achieve the best results in\nour tests on two open change detection datasets, using both RGB and\nmultispectral images. We show that our system is able to learn from scratch\nusing annotated change detection images. Our architectures achieve better\nperformance than previously proposed methods, while being at least 500 times\nfaster than related systems. This work is a step towards efficient processing\nof data from large scale Earth observation systems such as Copernicus or\nLandsat.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 12:27:49 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Daudt", "Rodrigo Caye", ""], ["Saux", "Bertrand Le", ""], ["Boulch", "Alexandre", ""]]}, {"id": "1810.08468", "submitter": "Rodrigo Caye Daudt", "authors": "Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau", "title": "Urban Change Detection for Multispectral Earth Observation Using\n  Convolutional Neural Networks", "comments": "To appear inProc. IGARSS 2018, July 22-27, 2018, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Copernicus Sentinel-2 program now provides multispectral images at a\nglobal scale with a high revisit rate. In this paper we explore the usage of\nconvolutional neural networks for urban change detection using such\nmultispectral images. We first present the new change detection dataset that\nwas used for training the proposed networks, which will be openly available to\nserve as a benchmark. The Onera Satellite Change Detection (OSCD) dataset is\ncomposed of pairs of multispectral aerial images, and the changes were manually\nannotated at pixel level. We then propose two architectures to detect changes,\nSiamese and Early Fusion, and compare the impact of using different numbers of\nspectral channels as inputs. These architectures are trained from scratch using\nthe provided dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 12:40:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Daudt", "Rodrigo Caye", ""], ["Saux", "Bertrand Le", ""], ["Boulch", "Alexandre", ""], ["Gousseau", "Yann", ""]]}, {"id": "1810.08476", "submitter": "Jiafeng Xie", "authors": "Jiafeng Xie and Bing Shuai and Jian-Fang Hu and Jingyang Lin and\n  Wei-Shi Zheng", "title": "Improving Fast Segmentation With Teacher-student Learning", "comments": "13 pages, 3 figures, conference", "journal-ref": "BMVC 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, segmentation neural networks have been significantly improved by\ndemonstrating very promising accuracies on public benchmarks. However, these\nmodels are very heavy and generally suffer from low inference speed, which\nlimits their application scenarios in practice. Meanwhile, existing fast\nsegmentation models usually fail to obtain satisfactory segmentation accuracies\non public benchmarks. In this paper, we propose a teacher-student learning\nframework that transfers the knowledge gained by a heavy and better performed\nsegmentation network (i.e. teacher) to guide the learning of fast segmentation\nnetworks (i.e. student). Specifically, both zero-order and first-order\nknowledge depicted in the fine annotated images and unlabeled auxiliary data\nare transferred to regularize our student learning. The proposed method can\nimprove existing fast segmentation models without incurring extra computational\noverhead, so it can still process images with the same fast speed. Extensive\nexperiments on the Pascal Context, Cityscape and VOC 2012 datasets demonstrate\nthat the proposed teacher-student learning framework is able to significantly\nboost the performance of student network.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 12:59:32 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Xie", "Jiafeng", ""], ["Shuai", "Bing", ""], ["Hu", "Jian-Fang", ""], ["Lin", "Jingyang", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1810.08503", "submitter": "Pingkun Yan", "authors": "Pingkun Yan, Hengtao Guo, Ge Wang, Ruben De Man, Mannudeep K. Kalra", "title": "Hybrid deep neural networks for all-cause Mortality Prediction from LDCT\n  Images", "comments": "IEEE conference format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Known for its high morbidity and mortality rates, lung cancer poses a\nsignificant threat to human health and well-being. However, the same population\nis also at high risk for other deadly diseases, such as cardiovascular disease.\nSince Low-Dose CT (LDCT) has been shown to significantly improve the lung\ncancer diagnosis accuracy, it will be very useful for clinical practice to\npredict the all-cause mortality for lung cancer patients to take corresponding\nactions. In this paper, we propose a deep learning based method, which takes\nboth chest LDCT image patches and coronary artery calcification risk scores as\ninput, for direct prediction of mortality risk of lung cancer subjects. The\nproposed method is called Hybrid Risk Network (HyRiskNet) for mortality risk\nprediction, which is an end-to-end framework utilizing hybrid imaging features,\ninstead of completely relying on automatic feature extraction. Our work\ndemonstrates the feasibility of using deep learning techniques for all-cause\nlung cancer mortality prediction from chest LDCT images. The experimental\nresults show that the proposed HyRiskNet can achieve superior performance\ncompared with the neural networks with only image input and with other\ntraditional semi-automatic scoring methods. The study also indicates that\nradiologist defined features can well complement convolutional neural networks\nfor more comprehensive feature extraction.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 13:48:58 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Yan", "Pingkun", ""], ["Guo", "Hengtao", ""], ["Wang", "Ge", ""], ["De Man", "Ruben", ""], ["Kalra", "Mannudeep K.", ""]]}, {"id": "1810.08515", "submitter": "Mark Schutera", "authors": "Mark Schutera, Niklas Goby, Dirk Neumann, Markus Reischl", "title": "Transfer Learning versus Multi-agent Learning regarding Distributed\n  Decision-Making in Highway Traffic", "comments": "Proc. of the 10th International Workshop on Agents in Traffic and\n  Transportation (ATT 2018), co-located with ECAI/IJCAI, AAMAS and ICML 2018\n  conferences (FAIM 2018)", "journal-ref": "CEUR Workshop Proceedings 2018", "doi": null, "report-no": "CEUR-WS.org/Vol-2129", "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation and traffic are currently undergoing a rapid increase in terms\nof both scale and complexity. At the same time, an increasing share of traffic\nparticipants are being transformed into agents driven or supported by\nartificial intelligence resulting in mixed-intelligence traffic. This work\nexplores the implications of distributed decision-making in mixed-intelligence\ntraffic. The investigations are carried out on the basis of an online-simulated\nhighway scenario, namely the MIT \\emph{DeepTraffic} simulation. In the first\nstep traffic agents are trained by means of a deep reinforcement learning\napproach, being deployed inside an elitist evolutionary algorithm for\nhyperparameter search. The resulting architectures and training parameters are\nthen utilized in order to either train a single autonomous traffic agent and\ntransfer the learned weights onto a multi-agent scenario or else to conduct\nmulti-agent learning directly. Both learning strategies are evaluated on\ndifferent ratios of mixed-intelligence traffic. The strategies are assessed\naccording to the average speed of all agents driven by artificial intelligence.\nTraffic patterns that provoke a reduction in traffic flow are analyzed with\nrespect to the different strategies.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 14:16:25 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Schutera", "Mark", ""], ["Goby", "Niklas", ""], ["Neumann", "Dirk", ""], ["Reischl", "Markus", ""]]}, {"id": "1810.08534", "submitter": "Wei Tang", "authors": "Wei Tang, Gui Li, Xinyuan Bao, Teng Li", "title": "MsCGAN: Multi-scale Conditional Generative Adversarial Networks for\n  Person Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To synthesize high-quality person images with arbitrary poses is challenging.\nIn this paper, we propose a novel Multi-scale Conditional Generative\nAdversarial Networks (MsCGAN), aiming to convert the input conditional person\nimage to a synthetic image of any given target pose, whose appearance and the\ntexture are consistent with the input image. MsCGAN is a multi-scale\nadversarial network consisting of two generators and two discriminators. One\ngenerator transforms the conditional person image into a coarse image of the\ntarget pose globally, and the other is to enhance the detailed quality of the\nsynthetic person image through a local reinforcement network. The outputs of\nthe two generators are then merged into a synthetic, discriminant and\nhigh-resolution image. On the other hand, the synthetic image is downsampled to\nmultiple resolutions as the input to multi-scale discriminator networks. The\nproposed multi-scale generators and discriminators handling different levels of\nvisual features can benefit to synthesizing high-resolution person images with\nrealistic appearance and texture. Experiments are conducted on the Market-1501\nand DeepFashion datasets to evaluate the proposed model, and both qualitative\nand quantitative results demonstrate the superior performance of the proposed\nMsCGAN.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 15:04:13 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 01:49:51 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 16:19:24 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Tang", "Wei", ""], ["Li", "Gui", ""], ["Bao", "Xinyuan", ""], ["Li", "Teng", ""]]}, {"id": "1810.08565", "submitter": "Brian Wang", "authors": "Brian H. Wang, Yan Wang, Kilian Q. Weinberger, and Mark Campbell", "title": "Deep Person Re-identification for Probabilistic Data Association in\n  Multiple Pedestrian Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data association method for vision-based multiple pedestrian\ntracking, using deep convolutional features to distinguish between different\npeople based on their appearances. These re-identification (re-ID) features are\nlearned such that they are invariant to transformations such as rotation,\ntranslation, and changes in the background, allowing consistent identification\nof a pedestrian moving through a scene. We incorporate re-ID features into a\ngeneral data association likelihood model for multiple person tracking,\nexperimentally validate this model by using it to perform tracking in two\nevaluation video sequences, and examine the performance improvements gained as\ncompared to several baseline approaches. Our results demonstrate that using\ndeep person re-ID for data association greatly improves tracking robustness to\nchallenges such as occlusions and path crossings.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 15:57:38 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Wang", "Brian H.", ""], ["Wang", "Yan", ""], ["Weinberger", "Kilian Q.", ""], ["Campbell", "Mark", ""]]}, {"id": "1810.08597", "submitter": "Philipp Sadler", "authors": "Philipp Sadler", "title": "Detecting cities in aerial night-time images by learning structural\n  invariants using single reference augmentation", "comments": "Project in Image Classification, Winter 2018, Prof. Dr. Tatjana\n  Scheffler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper examines, if it is possible to learn structural invariants of city\nimages by using only a single reference picture when producing transformations\nalong the variants in the dataset. Previous work explored the problem of\nlearning from only a few examples and showed that data augmentation techniques\nbenefit performance and generalization for machine learning approaches. First a\nprincipal component analysis in conjunction with a Fourier transform is trained\non a single reference augmentation training dataset using the city images.\nSecondly a convolutional neural network is trained on a similar dataset with\nmore samples. The findings are that the convolutional neural network is capable\nof finding images of the same category whereas the applied principal component\nanalysis in conjunction with a Fourier transform failed to solve this task.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 17:32:07 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Sadler", "Philipp", ""]]}, {"id": "1810.08639", "submitter": "Pedro Marrero", "authors": "Pedro D. Marrero Fernandez, Fidel A. Guerrero-Pe\\~na, Tsang Ing Ren,\n  Jorge J. G. Leandro", "title": "Fast and Robust Multiple ColorChecker Detection using Deep Convolutional\n  Neural Networks", "comments": "Submitted to Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2018.11.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ColorCheckers are reference standards that professional photographers and\nfilmmakers use to ensure predictable results under every lighting condition.\nThe objective of this work is to propose a new fast and robust method for\nautomatic ColorChecker detection. The process is divided into two steps: (1)\nColorCheckers localization and (2) ColorChecker patches recognition. For the\nColorChecker localization, we trained a detection convolutional neural network\nusing synthetic images. The synthetic images are created with the 3D models of\nthe ColorChecker and different background images. The output of the neural\nnetworks are the bounding box of each possible ColorChecker candidates in the\ninput image. Each bounding box defines a cropped image which is evaluated by a\nrecognition system, and each image is canonized with regards to color and\ndimensions. Subsequently, all possible color patches are extracted and grouped\nwith respect to the center's distance. Each group is evaluated as a candidate\nfor a ColorChecker part, and its position in the scene is estimated. Finally, a\ncost function is applied to evaluate the accuracy of the estimation. The method\nis tested using real and synthetic images. The proposed method is fast, robust\nto overlaps and invariant to affine projections. The algorithm also performs\nwell in case of multiple ColorCheckers detection.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 18:39:53 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Fernandez", "Pedro D. Marrero", ""], ["Guerrero-Pe\u00f1a", "Fidel A.", ""], ["Ren", "Tsang Ing", ""], ["Leandro", "Jorge J. G.", ""]]}, {"id": "1810.08697", "submitter": "Angelos Amanatiadis", "authors": "Angelos Amanatiadis, Vasileios Kaburlasos, Elias Kosmatopoulos", "title": "Understanding Deep Convolutional Networks through Gestalt Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superior performance of deep convolutional networks over high-dimensional\nproblems have made them very popular for several applications. Despite their\nwide adoption, their underlying mechanisms still remain unclear with their\nimprovement procedures still relying mainly on a trial and error process. We\nintroduce a novel sensitivity analysis based on the Gestalt theory for giving\ninsights into the classifier function and intermediate layers. Since Gestalt\npsychology stipulates that perception can be a product of complex interactions\namong several elements, we perform an ablation study based on this concept to\ndiscover which principles and image context significantly contribute in the\nnetwork classification. Our results reveal that ConvNets follow most of the\nvisual cortical perceptual mechanisms defined by the Gestalt principles at\nseveral levels. The proposed framework stimulates specific feature maps in\nclassification problems and reveal important network attributes that can\nproduce more explainable network models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 21:48:51 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Amanatiadis", "Angelos", ""], ["Kaburlasos", "Vasileios", ""], ["Kosmatopoulos", "Elias", ""]]}, {"id": "1810.08705", "submitter": "Jonas Unger", "authors": "Magnus Wrenninge and Jonas Unger", "title": "Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing", "comments": "For more information and download see: https://7dlabs.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Synscapes -- a synthetic dataset for street scene parsing\ncreated using photorealistic rendering techniques, and show state-of-the-art\nresults for training and validation as well as new types of analysis. We study\nthe behavior of networks trained on real data when performing inference on\nsynthetic data: a key factor in determining the equivalence of simulation\nenvironments. We also compare the behavior of networks trained on synthetic\ndata and evaluated on real-world data. Additionally, by analyzing pre-trained,\nexisting segmentation and detection models, we illustrate how uncorrelated\nimages along with a detailed set of annotations open up new avenues for\nanalysis of computer vision systems, providing fine-grain information about how\na model's performance changes according to factors such as distance, occlusion\nand relative object orientation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 22:30:44 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wrenninge", "Magnus", ""], ["Unger", "Jonas", ""]]}, {"id": "1810.08729", "submitter": "Hubert Lin", "authors": "Hubert Lin, Melinos Averkiou, Evangelos Kalogerakis, Balazs Kovacs,\n  Siddhant Ranade, Vladimir G. Kim, Siddhartha Chaudhuri, Kavita Bala", "title": "Learning Material-Aware Local Descriptors for 3D Shapes", "comments": "3DV 2018", "journal-ref": null, "doi": "10.1109/3DV.2018.00027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material understanding is critical for design, geometric modeling, and\nanalysis of functional objects. We enable material-aware 3D shape analysis by\nemploying a projective convolutional neural network architecture to learn\nmaterial- aware descriptors from view-based representations of 3D points for\npoint-wise material classification or material- aware retrieval. Unfortunately,\nonly a small fraction of shapes in 3D repositories are labeled with physical\nmate- rials, posing a challenge for learning methods. To address this\nchallenge, we crowdsource a dataset of 3080 3D shapes with part-wise material\nlabels. We focus on furniture models which exhibit interesting structure and\nmaterial variabil- ity. In addition, we also contribute a high-quality expert-\nlabeled benchmark of 115 shapes from Herman-Miller and IKEA for evaluation. We\nfurther apply a mesh-aware con- ditional random field, which incorporates\nrotational and reflective symmetries, to smooth our local material predic-\ntions across neighboring surface patches. We demonstrate the effectiveness of\nour learned descriptors for automatic texturing, material-aware retrieval, and\nphysical simulation. The dataset and code will be publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 01:55:40 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Lin", "Hubert", ""], ["Averkiou", "Melinos", ""], ["Kalogerakis", "Evangelos", ""], ["Kovacs", "Balazs", ""], ["Ranade", "Siddhant", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Bala", "Kavita", ""]]}, {"id": "1810.08753", "submitter": "Wenjun Yan", "authors": "Wenjun Yan, Yuanyuan Wang, Zeju Li, Rob J. van der Geest, Qian Tao", "title": "Left Ventricle Segmentation via Optical-Flow-Net from Short-axis Cine\n  MRI: Preserving the Temporal Coherence of Cardiac Motion", "comments": "MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative assessment of left ventricle (LV) function from cine MRI has\nsignificant diagnostic and prognostic value for cardiovascular disease\npatients. The temporal movement of LV provides essential information on the\ncontracting/relaxing pattern of heart, which is keenly evaluated by clinical\nexperts in clinical practice. Inspired by the expert way of viewing Cine MRI,\nwe propose a new CNN module that is able to incorporate the temporal\ninformation into LV segmentation from cine MRI. In the proposed CNN, the\noptical flow (OF) between neighboring frames is integrated and aggregated at\nfeature level, such that temporal coherence in cardiac motion can be taken into\naccount during segmentation. The proposed module is integrated into the U-net\narchitecture without need of additional training. Furthermore, dilated\nconvolution is introduced to improve the spatial accuracy of segmentation.\nTrained and tested on the Cardiac Atlas database, the proposed network resulted\nin a Dice index of 95% and an average perpendicular distance of 0.9 pixels for\nthe middle LV contour, significantly outperforming the original U-net that\nprocesses each frame individually. Notably, the proposed method improved the\ntemporal coherence of LV segmentation results, especially at the LV apex and\nbase where the cardiac motion is difficult to follow.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 05:41:28 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Yan", "Wenjun", ""], ["Wang", "Yuanyuan", ""], ["Li", "Zeju", ""], ["van der Geest", "Rob J.", ""], ["Tao", "Qian", ""]]}, {"id": "1810.08768", "submitter": "Wenbo Bao", "authors": "Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan\n  Yang", "title": "MEMC-Net: Motion Estimation and Motion Compensation Driven Neural\n  Network for Video Interpolation and Enhancement", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 07:47:09 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 08:35:29 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Bao", "Wenbo", ""], ["Lai", "Wei-Sheng", ""], ["Zhang", "Xiaoyun", ""], ["Gao", "Zhiyong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1810.08770", "submitter": "Lu Qi", "authors": "Lu Qi, Shu Liu, Jianping Shi, Jiaya Jia", "title": "Sequential Context Encoding for Duplicate Removal", "comments": "Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplicate removal is a critical step to accomplish a reasonable amount of\npredictions in prevalent proposal-based object detection frameworks. Albeit\nsimple and effective, most previous algorithms utilize a greedy process without\nmaking sufficient use of properties of input data. In this work, we design a\nnew two-stage framework to effectively select the appropriate proposal\ncandidate for each object. The first stage suppresses most of easy negative\nobject proposals, while the second stage selects true positives in the reduced\nproposal set. These two stages share the same network structure, \\ie, an\nencoder and a decoder formed as recurrent neural networks (RNN) with global\nattention and context gate. The encoder scans proposal candidates in a\nsequential manner to capture the global context information, which is then fed\nto the decoder to extract optimal proposals. In our extensive experiments, the\nproposed method outperforms other alternatives by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 08:26:32 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Qi", "Lu", ""], ["Liu", "Shu", ""], ["Shi", "Jianping", ""], ["Jia", "Jiaya", ""]]}, {"id": "1810.08771", "submitter": "Yi Wang", "authors": "Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia", "title": "Image Inpainting via Generative Multi-column Convolutional Neural\n  Networks", "comments": "Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generative multi-column network for image\ninpainting. This network synthesizes different image components in a parallel\nmanner within one stage. To better characterize global structures, we design a\nconfidence-driven reconstruction loss while an implicit diversified MRF\nregularization is adopted to enhance local details. The multi-column network\ncombined with the reconstruction and MRF loss propagates local and global\ninformation derived from context to the target inpainting regions. Extensive\nexperiments on challenging street view, face, natural objects and scenes\nmanifest that our method produces visual compelling results even without\npreviously common post-processing.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 08:26:32 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wang", "Yi", ""], ["Tao", "Xin", ""], ["Qi", "Xiaojuan", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1810.08774", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Arnav Jain, Divyasri Nadendla, Prabir Kumar Biswas", "title": "Improved Techniques for GAN based Facial Inpainting", "comments": "First two authors contributed equally. This work has been submitted\n  to the IEEE for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present several architectural and optimization recipes for\ngenerative adversarial network(GAN) based facial semantic inpainting. Current\nbenchmark models are susceptible to initial solutions of non-convex\noptimization criterion of GAN based inpainting. We present an end-to-end\ntrainable parametric network to deterministically start from good initial\nsolutions leading to more photo realistic reconstructions with significant\noptimization speed up. For the first time, we show how to efficiently extend\nGAN based single image inpainter models to sequences by a)learning to\ninitialize a temporal window of solutions with a recurrent neural network and\nb)imposing a temporal smoothness loss(during iterative optimization) to respect\nthe redundancy in temporal dimension of a sequence. We conduct comprehensive\nempirical evaluations on CelebA images and pseudo sequences followed by real\nlife videos of VidTIMIT dataset. The proposed method significantly outperforms\ncurrent GAN based state-of-the-art in terms of reconstruction quality with a\nsimultaneous speedup of over 15$\\times$. We also show that our proposed model\nis better in preserving facial identity in a sequence even without explicitly\nusing any face recognition module during training.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 09:19:27 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Lahiri", "Avisek", ""], ["Jain", "Arnav", ""], ["Nadendla", "Divyasri", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1810.08850", "submitter": "Saad Nadeem", "authors": "Saad Nadeem, Joseph Marino, Xianfeng Gu and Arie Kaufman", "title": "Corresponding Supine and Prone Colon Visualization Using Eigenfunction\n  Analysis and Fold Modeling", "comments": "IEEE Transactions on Visualization and Computer Graphics,\n  23(1):751-760, 2017 (11 pages, 13 figures)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics,\n  23(1):751-760, 2017", "doi": "10.1109/TVCG.2016.2598791", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for registration and visualization of corresponding\nsupine and prone virtual colonoscopy scans based on eigenfunction analysis and\nfold modeling. In virtual colonoscopy, CT scans are acquired with the patient\nin two positions, and their registration is desirable so that physicians can\ncorroborate findings between scans. Our algorithm performs this registration\nefficiently through the use of Fiedler vector representation (the second\neigenfunction of the Laplace-Beltrami operator). This representation is\nemployed to first perform global registration of the two colon positions. The\nregistration is then locally refined using the haustral folds, which are\nautomatically segmented using the 3D level sets of the Fiedler vector. The use\nof Fiedler vectors and the segmented folds presents a precise way of\nvisualizing corresponding regions across datasets and visual modalities. We\npresent multiple methods of visualizing the results, including 2D flattened\nrendering and the corresponding 3D endoluminal views. The precise fold modeling\nis used to automatically find a suitable cut for the 2D flattening, which\nprovides a less distorted visualization. Our approach is robust, and we\ndemonstrate its efficiency and efficacy by showing matched views on both the 2D\nflattened colons and in the 3D endoluminal view. We analytically evaluate the\nresults by measuring the distance between features on the registered colons,\nand we also assess our fold segmentation against 20 manually labeled datasets.\nWe have compared our results analytically to previous methods, and have found\nour method to achieve superior results. We also prove the hot spots conjecture\nfor modeling cylindrical topology using Fiedler vector representation, which\nallows our approach to be used for general cylindrical geometry modeling and\nfeature extraction.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 21:21:14 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Nadeem", "Saad", ""], ["Marino", "Joseph", ""], ["Gu", "Xianfeng", ""], ["Kaufman", "Arie", ""]]}, {"id": "1810.08878", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Qi Li", "title": "A Regressive Convolution Neural network and Support Vector Regression\n  Model for Electricity Consumption Forecasting", "comments": "Future of Information and Communications Conference (FICC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity consumption forecasting has important implications for the\nmineral companies on guiding quarterly work, normal power system operation, and\nthe management. However, electricity consumption prediction for the mineral\ncompany is different from traditional electricity load prediction since mineral\ncompany electricity consumption can be affected by various factors (e.g., ore\ngrade, processing quantity of the crude ore, ball milling fill rate). The\nproblem is non-trivial due to three major challenges for traditional methods:\ninsufficient training data, high computational cost and low prediction\naccu-racy. To tackle these challenges, we firstly propose a Regressive\nConvolution Neural Network (RCNN) to predict the electricity consumption. While\nRCNN still suffers from high computation overhead, we utilize RCNN to extract\nfeatures from the history data and Regressive Support Vector Machine (SVR)\ntrained with the features to predict the electricity consumption. The\nexperimental results show that the proposed RCNN-SVR model achieves higher\naccuracy than using the traditional RNN or SVM alone. The MSE, MAPE, and\nCV-RMSE of RCNN-SVR model are 0.8564, 1.975%, and 0.0687% respectively, which\nillustrates the low predicting error rate of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 02:12:11 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 00:31:36 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Zhang", "Youshan", ""], ["Li", "Qi", ""]]}, {"id": "1810.08881", "submitter": "Youshan Zhang", "authors": "Youshan Zhang, Jon-Patrick Allem, Jennifer B. Unger, Tess Boley Cruz", "title": "Automated identification of hookahs (waterpipes) on Instagram: an\n  application in feature extraction using Convolutional Neural Network and\n  Support Vector Machine classification", "comments": "Journal of Medical Internet Research", "journal-ref": null, "doi": "10.2196/10513", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Instagram, with millions of posts per day, can be used to inform\npublic health surveillance targets and policies. However, current research\nrelying on image-based data often relies on hand coding of images which is time\nconsuming and costly, ultimately limiting the scope of the study. Current best\npractices in automated image classification (e.g., support vector machine\n(SVM), Backpropagation (BP) neural network, and artificial neural network) are\nlimited in their capacity to accurately distinguish between objects within\nimages. Objective: This study demonstrates how convolutional neural network\n(CNN) can be used to extract unique features within an image and how SVM can\nthen be used to classify the image. Methods: Images of waterpipes or hookah (an\nemerging tobacco product possessing similar harms to that of cigarettes) were\ncollected from Instagram and used in analyses (n=840). CNN was used to extract\nunique features from images identified to contain waterpipes. A SVM classifier\nwas built to distinguish between images with and without waterpipes. Methods\nfor image classification were then compared to show how a CNN + SVM classifier\ncould improve accuracy. Results: As the number of the validated training images\nincreased, the total number of extracted features increased. Additionally, as\nthe number of features learned by the SVM classifier increased, the average\nlevel of accuracy increased. Overall, 99.5% of the 420 images classified were\ncorrectly identified as either hookah or non-hookah images. This level of\naccuracy was an improvement over earlier methods that used SVM, CNN or Bag of\nFeatures (BOF) alone. Conclusions: CNN extracts more features of the images\nallowing a SVM classifier to be better informed, resulting in higher accuracy\ncompared with methods that extract fewer features. Future research can use this\nmethod to grow the scope of image-based studies.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 02:48:36 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Youshan", ""], ["Allem", "Jon-Patrick", ""], ["Unger", "Jennifer B.", ""], ["Cruz", "Tess Boley", ""]]}, {"id": "1810.08950", "submitter": "Ruixuan Yu", "authors": "Ruixuan Yu, Jian Sun, Huibin Li", "title": "Learning Spectral Transform Network on 3D Surface for Non-rigid Shape\n  Analysis", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a network on 3D surface for non-rigid shape analysis is a\nchallenging task. In this work, we propose a novel spectral transform network\non 3D surface to learn shape descriptors. The proposed network architecture\nconsists of four stages: raw descriptor extraction, surface second-order\npooling, mixture of power function-based spectral transform, and metric\nlearning. The proposed network is simple and shallow. Quantitative experiments\non challenging benchmarks show its effectiveness for non-rigid shape retrieval\nand classification, e.g., it achieved the highest accuracies on SHREC14, 15\ndatasets as well as the Range subset of SHREC17 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 13:56:51 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Yu", "Ruixuan", ""], ["Sun", "Jian", ""], ["Li", "Huibin", ""]]}, {"id": "1810.08998", "submitter": "Saad Nadeem", "authors": "Saad Nadeem and Arie Kaufman", "title": "Visualization Framework for Colonoscopy Videos", "comments": "SPIE Medical Imaging, 2016 (7 pages, 5 figures)", "journal-ref": "SPIE Medical Imaging, Vol. 9786, p. 97861T, 2016", "doi": "10.1117/12.2216963", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visualization framework for annotating and comparing colonoscopy\nvideos, where these annotations can then be used for semi-automatic report\ngeneration at the end of the procedure. Currently, there are approximately 14\nmillion colonoscopies performed every year in the US. In this work, we create a\nvisualization tool to deal with the deluge of colonoscopy videos in a more\neffective way. We present an interactive visualization framework for the\nannotation and tagging of colonoscopy videos in an easy and intuitive way.\nThese annotations and tags can later be used for report generation for\nelectronic medical records and for comparison at an individual as well as group\nlevel. We also present important use cases and medical expert feedback for our\nvisualization framework.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:17:39 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Nadeem", "Saad", ""], ["Kaufman", "Arie", ""]]}, {"id": "1810.09008", "submitter": "El Wardani Dadi Dr.", "authors": "M. Benjelloun, E. W. Dadi, and E. M. Daoudi", "title": "3D shape retrieval basing on representatives of classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an improvement of our proposed technique for 3D\nshape retrieval in classified databases [2] which is based on representatives\nof classes. Instead of systematically matching the object-query with all 3D\nmodels of the database, our idea presented in [2] consist, for a classified\ndatabase, to represent each class by one representative that is used to orient\nthe retrieval process to the right class (the class excepted to contain 3D\nmodels similar to the query). In order to increase the chance to fall in the\nright class, our idea in this work is to represent each class by more than one\nrepresentative. In this case, instead of using only one representative to\ndecide which is the right class we use a set of representatives this will\ncontribute certainly to improving the relevance of retrieval results. The\nobtained experimental results show that the relevance is significantly\nimproved.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 19:11:12 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 21:54:02 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 21:14:15 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Benjelloun", "M.", ""], ["Dadi", "E. W.", ""], ["Daoudi", "E. M.", ""]]}, {"id": "1810.09012", "submitter": "Saad Nadeem", "authors": "Ji Hwan Park, Saad Nadeem, Seyedkoosha Mirhosseini and Arie Kaufman", "title": "C2A: Crowd Consensus Analytics for Virtual Colonoscopy", "comments": "IEEE Conference on Visual Analytics Science and Technology (VAST),\n  pp. 21-30, 2016 (10 pages, 11 figures)", "journal-ref": "IEEE Conference on Visual Analytics Science and Technology (VAST),\n  pp. 21-30, 2016", "doi": "10.1109/VAST.2016.7883508", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a medical crowdsourcing visual analytics platform called C{$^2$}A\nto visualize, classify and filter crowdsourced clinical data. More\nspecifically, C$^2$A is used to build consensus on a clinical diagnosis by\nvisualizing crowd responses and filtering out anomalous activity. Crowdsourcing\nmedical applications have recently shown promise where the non-expert users\n(the crowd) were able to achieve accuracy similar to the medical experts. This\nhas the potential to reduce interpretation/reading time and possibly improve\naccuracy by building a consensus on the findings beforehand and letting the\nmedical experts make the final diagnosis. In this paper, we focus on a virtual\ncolonoscopy (VC) application with the clinical technicians as our target users,\nand the radiologists acting as consultants and classifying segments as benign\nor malignant. In particular, C$^2$A is used to analyze and explore crowd\nresponses on video segments, created from fly-throughs in the virtual colon.\nC$^2$A provides several interactive visualization components to build crowd\nconsensus on video segments, to detect anomalies in the crowd data and in the\nVC video segments, and finally, to improve the non-expert user's work quality\nand performance by A/B testing for the optimal crowdsourcing platform and\napplication-specific parameters. Case studies and domain experts feedback\ndemonstrate the effectiveness of our framework in improving workers' output\nquality, the potential to reduce the radiologists' interpretation time, and\nhence, the potential to improve the traditional clinical workflow by marking\nthe majority of the video segments as benign based on the crowd consensus.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 19:33:33 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Park", "Ji Hwan", ""], ["Nadeem", "Saad", ""], ["Mirhosseini", "Seyedkoosha", ""], ["Kaufman", "Arie", ""]]}, {"id": "1810.09025", "submitter": "Isma\\\"el Kon\\'e", "authors": "Isma\\\"el Kon\\'e and Lahsen Boulmane", "title": "Hierarchical ResNeXt Models for Breast Cancer Histology Image\n  Classification", "comments": null, "journal-ref": "Image Analysis and Recognition. ICIAR 2018. LNCS, vol 10882.\n  Springer, Cham", "doi": "10.1007/978-3-319-93000-8_90", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopic histology image analysis is a cornerstone in early detection of\nbreast cancer. However these images are very large and manual analysis is error\nprone and very time consuming. Thus automating this process is in high demand.\nWe proposed a hierarchical system of convolutional neural networks (CNN) that\nclassifies automatically patches of these images into four pathologies: normal,\nbenign, in situ carcinoma and invasive carcinoma. We evaluated our system on\nthe BACH challenge dataset of image-wise classification and a small dataset\nthat we used to extend it. Using a train/test split of 75%/25%, we achieved an\naccuracy rate of 0.99 on the test split for the BACH dataset and 0.96 on that\nof the extension. On the test of the BACH challenge, we've reached an accuracy\nof 0.81 which rank us to the 8th out of 51 teams.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 20:55:41 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Kon\u00e9", "Isma\u00ebl", ""], ["Boulmane", "Lahsen", ""]]}, {"id": "1810.09044", "submitter": "Fatemeh Sadat Saleh", "authors": "Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann,\n  Basura Fernando, Lars Petersson, Lars Andersson", "title": "VIENA2: A Driving Anticipation Dataset", "comments": "Accepted in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action anticipation is critical in scenarios where one needs to react before\nthe action is finalized. This is, for instance, the case in automated driving,\nwhere a car needs to, e.g., avoid hitting pedestrians and respect traffic\nlights. While solutions have been proposed to tackle subsets of the driving\nanticipation tasks, by making use of diverse, task-specific sensors, there is\nno single dataset or framework that addresses them all in a consistent manner.\nIn this paper, we therefore introduce a new, large-scale dataset, called\nVIENA2, covering 5 generic driving scenarios, with a total of 25 distinct\naction classes. It contains more than 15K full HD, 5s long videos acquired in\nvarious driving conditions, weathers, daytimes and environments, complemented\nwith a common and realistic set of sensor measurements. This amounts to more\nthan 2.25M frames, each annotated with an action label, corresponding to 600\nsamples per action class. We discuss our data acquisition strategy and the\nstatistics of our dataset, and benchmark state-of-the-art action anticipation\ntechniques, including a new multi-modal LSTM architecture with an effective\nloss function for action anticipation in driving scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 00:21:28 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 22:29:20 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Aliakbarian", "Mohammad Sadegh", ""], ["Saleh", "Fatemeh Sadat", ""], ["Salzmann", "Mathieu", ""], ["Fernando", "Basura", ""], ["Petersson", "Lars", ""], ["Andersson", "Lars", ""]]}, {"id": "1810.09068", "submitter": "Salvador Medina", "authors": "Salvador Medina, Zhuyun Dai, Yingkai Gao", "title": "Where is this? Video geolocation based on neural network features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method that geolocates videos within a delimited\nwidespread area based solely on the frames visual content. Our proposed method\ntackles video-geolocation through traditional image retrieval techniques\nconsidering Google Street View as the reference point. To achieve this goal we\nuse the deep learning features obtained from NetVLAD to represent images, since\nthrough this feature vectors the similarity is their L2 norm. In this paper, we\npropose a family of voting-based methods to aggregate frame-wise geolocation\nresults which boost the video geolocation result. The best aggregation found\nthrough our experiments considers both NetVLAD and SIFT similarity, as well as\nthe geolocation density of the most similar results. To test our proposed\nmethod, we gathered a new video dataset from Pittsburgh Downtown area to\nbenefit and stimulate more work in this area. Our system achieved a precision\nof 90% while geolocating videos within a range of 150 meters or two blocks away\nfrom the original position.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 03:27:43 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 00:51:00 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Medina", "Salvador", ""], ["Dai", "Zhuyun", ""], ["Gao", "Yingkai", ""]]}, {"id": "1810.09075", "submitter": "Xiahai Zhuang", "authors": "Fuping Wu, Lei Li, Guang Yang, Tom Wong, Raad Mohiaddin, David Firmin,\n  Jennifer Keegan, Lingchao Xu, and Xiahai Zhuang", "title": "Atrial fibrosis quantification based on maximum likelihood estimator of\n  multivariate images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully-automated segmentation and quantification of the left\natrial (LA) fibrosis and scars combining two cardiac MRIs, one is the target\nlate gadolinium-enhanced (LGE) image, and the other is an anatomical MRI from\nthe same acquisition session. We formulate the joint distribution of images\nusing a multivariate mixture model (MvMM), and employ the maximum likelihood\nestimator (MLE) for texture classification of the images simultaneously. The\nMvMM can also embed transformations assigned to the images to correct the\nmisregistration. The iterated conditional mode algorithm is adopted for\noptimization. This method first extracts the anatomical shape of the LA, and\nthen estimates a prior probability map. It projects the resulting segmentation\nonto the LA surface, for quantification and analysis of scarring. We applied\nthe proposed method to 36 clinical data sets and obtained promising results\n(Accuracy: $0.809\\pm .150$, Dice: $0.556\\pm.187$). We compared the method with\nthe conventional algorithms and showed an evidently and statistically better\nperformance ($p<0.03$).\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 04:03:14 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wu", "Fuping", ""], ["Li", "Lei", ""], ["Yang", "Guang", ""], ["Wong", "Tom", ""], ["Mohiaddin", "Raad", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""], ["Xu", "Lingchao", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "1810.09091", "submitter": "Xiaolin Zhang", "authors": "Xiaolin Zhang, Yunchao Wei, Yi Yang, Thomas Huang", "title": "SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot image semantic segmentation poses a challenging task of recognizing\nthe object regions from unseen categories with only one annotated example as\nsupervision. In this paper, we propose a simple yet effective Similarity\nGuidance network to tackle the One-shot (SG-One) segmentation problem. We aim\nat predicting the segmentation mask of a query image with the reference to one\ndensely labeled support image of the same category. To obtain the robust\nrepresentative feature of the support image, we firstly adopt a masked average\npooling strategy for producing the guidance features by only taking the pixels\nbelonging to the support image into account. We then leverage the cosine\nsimilarity to build the relationship between the guidance features and features\nof pixels from the query image. In this way, the possibilities embedded in the\nproduced similarity maps can be adapted to guide the process of segmenting\nobjects. Furthermore, our SG-One is a unified framework which can efficiently\nprocess both support and query images within one network and be learned in an\nend-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In\nparticular, our SGOne achieves the mIoU score of 46.3%, surpassing the baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 05:30:04 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 03:13:22 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 00:24:22 GMT"}, {"version": "v4", "created": "Tue, 12 May 2020 11:34:33 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Xiaolin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""], ["Huang", "Thomas", ""]]}, {"id": "1810.09102", "submitter": "Xiaohan Chen", "authors": "Nitin Bansal, Xiaohan Chen, Zhangyang Wang", "title": "Can We Gain More from Orthogonality Regularizations in Training Deep\n  CNNs?", "comments": "11 pages, 1 figure, 2 tables. Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to answer the question: as the (near-) orthogonality of\nweights is found to be a favorable property for training deep convolutional\nneural networks, how can we enforce it in more effective and easy-to-use ways?\nWe develop novel orthogonality regularizations on training deep CNNs, utilizing\nvarious advanced analytical tools such as mutual coherence and restricted\nisometry property. These plug-and-play regularizations can be conveniently\nincorporated into training almost any CNN without extra hassle. We then\nbenchmark their effects on state-of-the-art models: ResNet, WideResNet, and\nResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100,\nSVHN and ImageNet. We observe consistent performance gains after applying those\nproposed regularizations, in terms of both the final accuracies achieved, and\nfaster and more stable convergences. We have made our codes and pre-trained\nmodels publicly available:\nhttps://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 06:22:54 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Bansal", "Nitin", ""], ["Chen", "Xiaohan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1810.09111", "submitter": "Haifeng Li", "authors": "Enqiang Guo, Xinsha Fu, Jiawei Zhu, Min Deng, Yu Liu, Qing Zhu,\n  Haifeng Li", "title": "Learning to Measure Change: Fully Convolutional Siamese Metric Networks\n  for Scene Change Detection", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical challenge problem of scene change detection is that noisy changes\ngenerated by varying illumination, shadows and camera viewpoint make variances\nof a scene difficult to define and measure since the noisy changes and semantic\nones are entangled. Following the intuitive idea of detecting changes by\ndirectly comparing dissimilarities between a pair of features, we propose a\nnovel fully Convolutional siamese metric Network(CosimNet) to measure changes\nby customizing implicit metrics. To learn more discriminative metrics, we\nutilize contrastive loss to reduce the distance between the unchanged feature\npairs and to enlarge the distance between the changed feature pairs.\nSpecifically, to address the issue of large viewpoint differences, we propose\nThresholded Contrastive Loss (TCL) with a more tolerant strategy to punish\nnoisy changes. We demonstrate the effectiveness of the proposed approach with\nexperiments on three challenging datasets: CDnet, PCD2015, and VL-CMU-CD. Our\napproach is robust to lots of challenging conditions, such as illumination\nchanges, large viewpoint difference caused by camera motion and zooming. In\naddition, we incorporate the distance metric into the segmentation framework\nand validate the effectiveness through visualization of change maps and feature\ndistribution. The source code is available at\nhttps://github.com/gmayday1997/ChangeDet.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:01:45 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 11:41:36 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 03:16:22 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Guo", "Enqiang", ""], ["Fu", "Xinsha", ""], ["Zhu", "Jiawei", ""], ["Deng", "Min", ""], ["Liu", "Yu", ""], ["Zhu", "Qing", ""], ["Li", "Haifeng", ""]]}, {"id": "1810.09123", "submitter": "Xiahai Zhuang", "authors": "Lei Li, Fuping Wu, Guang Yang, Tom Wong, Raad Mohiaddin, David Firmin,\n  Jenny Keegan, Lingchao Xu, Xiahai Zhuang", "title": "Atrial scars segmentation via potential learning in the graph-cuts\n  framework", "comments": "9 pages,4 figures,STACOM2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Late Gadolinium Enhancement Magnetic Resonance Imaging (LGE MRI) emerged as a\nroutine scan for patients with atrial fibrillation (AF). However, due to the\nlow image quality automating the quantification and analysis of the atrial\nscars is challenging. In this study, we pro-posed a fully automated method\nbased on the graph-cuts framework, where the potential of the graph is learned\non a surface mesh of the left atrium (LA) using an equidistant projection and a\nDeep Neural Network (DNN). For validation, we employed 100 datasets with manual\ndelineation. The results showed that the performance of the proposed method\nimproved and converged with respect to the increased size of training patches,\nwhich provide important features of the structural and texture information\nlearned by the DNN. The segmentation could be further improved when the\ncontribution from the t-link and n-link is balanced, thanks to\ninter-relationship learned by the DNN for the graph-cuts algorithm. Compared\nwith the published methods which mostly acquired manual delineation of the LA\nor LA wall, our method is fully automatic and demonstrated evidently better\nresults with statistical significance. Finally, the accuracy of quantifying the\nscars assessed by the Dice score was 0.570. The results are promising and the\nmethod can be useful in diagnosis and prognosis of AF.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:57:24 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Li", "Lei", ""], ["Wu", "Fuping", ""], ["Yang", "Guang", ""], ["Wong", "Tom", ""], ["Mohiaddin", "Raad", ""], ["Firmin", "David", ""], ["Keegan", "Jenny", ""], ["Xu", "Lingchao", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "1810.09162", "submitter": "Yan Zhang", "authors": "Yan Zhang, Li Sun", "title": "Exploring Correlations in Multiple Facial Attributes through Graph\n  Attention Network", "comments": "9 pages, 5 figures, summit to AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating multiple attributes from a single facial image gives comprehensive\ndescriptions on the high level semantics of the face. It is naturally regarded\nas a multi-task supervised learning problem with a single deep CNN, in which\nlower layers are shared, and higher ones are task-dependent with the\nmulti-branch structure. Within the traditional deep multi-task learning (DMTL)\nframework, this paper intends to fully exploit the correlations among different\nattributes by constructing a graph. The node in graph represents the feature\nvector from a particular branch for a given attribute, and the edge can be\ndefined by either the prior knowledge or the similarity between two nodes in\nthe embedding with a fully data-driven manner. We analyze that the attention\nmechanism actually takes effect in the latter case, and utilize the Graph\nAttention Layer (GAL) for exploring on the most relevant attribute feature and\nrefining the task-dependant feature by considering other attributes.\nExperiments show that by mining the correlations among attributes, our method\ncan improve the recognition accuracy on CelebA and LFWA dataset. And it also\nachieves competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 10:09:00 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Yan", ""], ["Sun", "Li", ""]]}, {"id": "1810.09168", "submitter": "Qin Zou", "authors": "Qingquan Li, Qin Zou, De Ma, Qian Wang, Song Wang", "title": "Dating Ancient Paintings of Mogao Grottoes Using Deeply Learnt Visual\n  Codes", "comments": null, "journal-ref": "Science China Information Sciences, 61(9), 092105 (2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultural heritage is the asset of all the peoples of the world. The\npreservation and inheritance of cultural heritage is conducive to the progress\nof human civilization. In northwestern China, there is a world heritage site --\nMogao Grottoes -- that has a plenty of mural paintings showing the historical\ncultures of ancient China. To study these historical cultures, one critical\nprocedure is to date the mural paintings, i.e., determining the era when they\nwere created. Until now, most mural paintings at Mogao Grottoes have been dated\nby directly referring to the mural texts or historical documents. However, some\nare still left with creation-era undetermined due to the lack of reference\nmaterials. Considering that the drawing style of mural paintings was changing\nalong the history and the drawing style can be learned and quantified through\npainting data, we formulate the problem of mural-painting dating into a problem\nof drawing-style classification. In fact, drawing styles can be expressed not\nonly in color or curvature, but also in some unknown forms -- the forms that\nhave not been observed. To this end, besides sophisticated color and shape\ndescriptors, a deep convolution neural network is designed to encode the\nimplicit drawing styles. 3860 mural paintings collected from 194 different\ngrottoes with determined creation-era labels are used to train the\nclassification model and build the dating method. In experiments, the proposed\ndating method is applied to seven mural paintings which were previously dated\nwith controversies, and the exciting new dating results are approved by the\nDunhuang expert.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 10:28:29 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Li", "Qingquan", ""], ["Zou", "Qin", ""], ["Ma", "De", ""], ["Wang", "Qian", ""], ["Wang", "Song", ""]]}, {"id": "1810.09197", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof A. Bertram, Robert Klopfleisch and Andreas\n  Maier", "title": "Field Of Interest Proposal for Augmented Mitotic Cell Count: Comparison\n  of two Convolutional Networks", "comments": "8 pages, 7 figures and one table, submitted to BIOIMAGING 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most tumor grading systems for human as for veterinary histopathology are\nbased upon the absolute count of mitotic figures in a certain reference area of\na histology slide. Since time for prognostication is limited in a diagnostic\nsetting, the pathologist will often almost arbitrarily choose a certain field\nof interest assumed to have the highest mitotic activity. However, as mitotic\nfigures are commonly very sparse on the slide and often have a patchy\ndistribution, this poses a sampling problem which is known to be able to\ninfluence the tumor prognostication. On the other hand, automatic detection of\nmitotic figures can't yet be considered reliable enough for clinical\napplication. In order to aid the work of the human expert and at the same time\nreduce variance in tumor grading, it is beneficial to assess the whole slide\nimage (WSI) for the highest mitotic activity and use this as a reference region\nfor human counting. For this task, we compare two methods for region of\ninterest proposal, both based on convolutional neural networks (CNN). For both\napproaches, the CNN performs a segmentation of the WSI to assess mitotic\nactivity. The first method performs a segmentation at the original image\nresolution, while the second approach performs a segmentation operation at a\nsignificantly reduced resolution, cutting down on processing complexity. We\nevaluate the approach using a dataset of 32 completely annotated whole slide\nimages of canine mast cell tumors, where 22 were used for training of the\nnetwork and 10 for test. Our results indicate that, while the overall\ncorrelation to the ground truth mitotic activity is considerably higher (0.94\nvs. 0.83) for the approach based upon the fine resolution network, the field of\ninterest choices are only marginally better. Both approaches propose fields of\ninterest that contain a mitotic count in the upper quartile of respective\nslides.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 12:06:29 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1810.09263", "submitter": "Yi Yang", "authors": "Yaming Wang, Xiao Tan, Yi Yang, Ziyu Li, Xiao Liu, Feng Zhou, Larry S.\n  Davis", "title": "Improving Annotation for 3D Pose Dataset of Fine-Grained Object\n  Categories", "comments": "arXiv admin note: text overlap with arXiv:1806.04314", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing 3D pose datasets of object categories are limited to generic object\ntypes and lack of fine-grained information. In this work, we introduce a new\nlarge-scale dataset that consists of 409 fine-grained categories and 31,881\nimages with accurate 3D pose annotation. Specifically, we augment three\nexisting fine-grained object recognition datasets (StanfordCars, CompCars and\nFGVC-Aircraft) by finding a specific 3D model for each sub-category from\nShapeNet and manually annotating each 2D image by adjusting a full set of 7\ncontinuous perspective parameters. Since the fine-grained shapes allow 3D\nmodels to better fit the images, we further improve the annotation quality by\ninitializing from the human annotation and conducting local search of the pose\nparameters with the objective of maximizing the IoUs between the projected mask\nand the segmentation reference estimated from state-of-the-art deep\nConvolutional Neural Networks (CNNs). We provide full statistics of the\nannotations with qualitative and quantitative comparisons suggesting that our\ndataset can be a complementary source for studying 3D pose estimation. The\ndataset can be downloaded at http://users.umiacs.umd.edu/~wym/3dpose.html.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 01:48:05 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Wang", "Yaming", ""], ["Tan", "Xiao", ""], ["Yang", "Yi", ""], ["Li", "Ziyu", ""], ["Liu", "Xiao", ""], ["Zhou", "Feng", ""], ["Davis", "Larry S.", ""]]}, {"id": "1810.09343", "submitter": "Michael Fink", "authors": "Michael Fink, Thomas Layer, Georg Mackenbrock, and Michael Sprinzl", "title": "Baseline Detection in Historical Documents using Convolutional U-Nets", "comments": "6 pages, accepted to DAS 2018", "journal-ref": "Proc. of the 13th IAPR Int. Workshop on Document Analysis Systems\n  (DAS 2018), IEEE Computer Society, pp. 37-42, 2018", "doi": "10.1109/DAS.2018.34", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Baseline detection is still a challenging task for heterogeneous collections\nof historical documents. We present a novel approach to baseline extraction in\nsuch settings, turning out the winning entry to the ICDAR 2017 Competition on\nBaseline detection (cBAD). It utilizes deep convolutional nets (CNNs) for both,\nthe actual extraction of baselines, as well as for a simple form of layout\nanalysis in a pre-processing step. To the best of our knowledge it is the first\nCNN-based system for baseline extraction applying a U-net architecture and\nsliding window detection, profiting from a high local accuracy of the candidate\nlines extracted. Final baseline post-processing complements our approach,\ncompensating for inaccuracies mainly due to missing context information during\nsliding window detection. We experimentally evaluate the components of our\nsystem individually on the cBAD dataset. Moreover, we investigate how it\ngeneralizes to different data by means of the dataset used for the baseline\nextraction task of the ICDAR 2017 Competition on Layout Analysis for\nChallenging Medieval Manuscripts (HisDoc). A comparison with the results\nreported for HisDoc shows that it also outperforms the contestants of the\nlatter.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:06:57 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Fink", "Michael", ""], ["Layer", "Thomas", ""], ["Mackenbrock", "Georg", ""], ["Sprinzl", "Michael", ""]]}, {"id": "1810.09354", "submitter": "Bo Zhou", "authors": "Bo Zhou, Xunyu Lin, Brendan Eck, Jun Hou, David L. Wilson", "title": "Generation of Virtual Dual Energy Images from Standard Single-Shot\n  Radiographs using Multi-scale and Conditional Adversarial Network", "comments": "16 pages, 7 figures, accepted by Asian Conference on Computer Vision\n  (2018 ACCV), code available at\n  https://github.com/bbbbbbzhou/Virtual-Dual-Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dual-energy (DE) chest radiographs provide greater diagnostic information\nthan standard radiographs by separating the image into bone and soft tissue,\nrevealing suspicious lesions which may otherwise be obstructed from view.\nHowever, acquisition of DE images requires two physical scans, necessitating\nspecialized hardware and processing, and images are prone to motion artifact.\nGeneration of virtual DE images from standard, single-shot chest radiographs\nwould expand the diagnostic value of standard radiographs without changing the\nacquisition procedure. We present a Multi-scale Conditional Adversarial Network\n(MCA-Net) which produces high-resolution virtual DE bone images from standard,\nsingle-shot chest radiographs. Our proposed MCA-Net is trained using the\nadversarial network so that it learns sharp details for the production of\nhigh-quality bone images. Then, the virtual DE soft tissue image is generated\nby processing the standard radiograph with the virtual bone image using a cross\nprojection transformation. Experimental results from 210 patient DE chest\nradiographs demonstrated that the algorithm can produce high-quality virtual DE\nchest radiographs. Important structures were preserved, such as coronary\ncalcium in bone images and lung lesions in soft tissue images. The average\nstructure similarity index and the peak signal to noise ratio of the produced\nbone images in testing data were 96.4 and 41.5, which are significantly better\nthan results from previous methods. Furthermore, our clinical evaluation\nresults performed on the publicly available dataset indicates the clinical\nvalues of our algorithms. Thus, our algorithm can produce high-quality DE\nimages that are potentially useful for radiologists, computer-aided\ndiagnostics, and other diagnostic tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:17:38 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 17:30:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhou", "Bo", ""], ["Lin", "Xunyu", ""], ["Eck", "Brendan", ""], ["Hou", "Jun", ""], ["Wilson", "David L.", ""]]}, {"id": "1810.09369", "submitter": "Mikhail Belyaev", "authors": "Maxim Pisov and Gleb Makarchuk and Valery Kostjuchenko and Alexandra\n  Dalechina and Andrey Golanov and Mikhail Belyaev", "title": "Brain Tumor Image Retrieval via Multitask Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification-based image retrieval systems are built by training\nconvolutional neural networks (CNNs) on a relevant classification problem and\nusing the distance in the resulting feature space as a similarity metric.\nHowever, in practical applications, it is often desirable to have\nrepresentations which take into account several aspects of the data (e.g.,\nbrain tumor type and its localization). In our work, we extend the\nclassification-based approach with multitask learning: we train a CNN on brain\nMRI scans with heterogeneous labels and implement a corresponding tumor image\nretrieval system. We validate our approach on brain tumor data which contains\ninformation about tumor types, shapes and localization. We show that our method\nallows us to build representations that contain more relevant information about\ntumors than single-task classification-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:38:25 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Pisov", "Maxim", ""], ["Makarchuk", "Gleb", ""], ["Kostjuchenko", "Valery", ""], ["Dalechina", "Alexandra", ""], ["Golanov", "Andrey", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1810.09381", "submitter": "Eldar Insafutdinov", "authors": "Eldar Insafutdinov and Alexey Dosovitskiy", "title": "Unsupervised Learning of Shape and Pose with Differentiable Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning accurate 3D shape and camera pose from a\ncollection of unlabeled category-specific images. We train a convolutional\nnetwork to predict both the shape and the pose from a single image by\nminimizing the reprojection error: given several views of an object, the\nprojections of the predicted shapes to the predicted camera poses should match\nthe provided views. To deal with pose ambiguity, we introduce an ensemble of\npose predictors which we then distill to a single \"student\" model. To allow for\nefficient learning of high-fidelity shapes, we represent the shapes by point\nclouds and devise a formulation allowing for differentiable projection of\nthese. Our experiments show that the distilled ensemble of pose predictors\nlearns to estimate the pose accurately, while the point cloud representation\nallows to predict detailed shape models. The supplementary video can be found\nat https://www.youtube.com/watch?v=LuIGovKeo60\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 16:01:20 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Insafutdinov", "Eldar", ""], ["Dosovitskiy", "Alexey", ""]]}, {"id": "1810.09444", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Takayuki Takahashi, Yota Yamamoto, Yutaka Endo,\n  Atsushi Shiraki, Takashi Nishitsuji, Naoto Hoshikawa, Takashi Kakue, Tomoyosh\n  Ito", "title": "Digital holographic particle volume reconstruction using a deep neural\n  network", "comments": null, "journal-ref": null, "doi": "10.1364/AO.58.001900", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a particle volume reconstruction directly from an in-line\nhologram using a deep neural network. Digital holographic volume reconstruction\nconventionally uses multiple diffraction calculations to obtain sectional\nreconstructed images from an in-line hologram, followed by detection of the\nlateral and axial positions, and the sizes of particles by using focus metrics.\nHowever, the axial resolution is limited by the numerical aperture of the\noptical system, and the processes are time-consuming. The method proposed here\ncan simultaneously detect the lateral and axial positions, and the particle\nsizes via a deep neural network (DNN). We numerically investigated the\nperformance of the DNN in terms of the errors in the detected positions and\nsizes. The calculation time is faster than conventional diffracted-based\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 23:25:44 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Takahashi", "Takayuki", ""], ["Yamamoto", "Yota", ""], ["Endo", "Yutaka", ""], ["Shiraki", "Atsushi", ""], ["Nishitsuji", "Takashi", ""], ["Hoshikawa", "Naoto", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyosh", ""]]}, {"id": "1810.09479", "submitter": "Bharath Raj N.", "authors": "Bharath Raj N., Venkateswaran N", "title": "Single Image Haze Removal using a Generative Adversarial Network", "comments": "Accepted for the WiSPNET 2020 conference. Please refer to the GitHub\n  repository for information on updates to the paper:\n  https://github.com/thatbrguy/Dehaze-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods to remove haze from images rely on estimating a\ntransmission map. When dealing with single images, this becomes an ill-posed\nproblem due to the lack of depth information. In this paper, we propose an\nend-to-end learning based approach which uses a modified conditional Generative\nAdversarial Network to directly remove haze from an image. We employ the usage\nof the Tiramisu model in place of the classic U-Net model as the generator\nowing to its higher parameter efficiency and performance. Moreover, a patch\nbased discriminator was used to reduce artefacts in the output. To further\nimprove the perceptual quality of the output, a hybrid weighted loss function\nwas designed and used to train the model. Experiments on synthetic and real\nworld hazy images demonstrates that our model performs competitively with the\nstate of the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:04:50 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 19:04:07 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["N.", "Bharath Raj", ""], ["N", "Venkateswaran", ""]]}, {"id": "1810.09487", "submitter": "Philipp Tschandl MD PhD", "authors": "Philipp Tschandl, Giuseppe Argenziano, Majid Razmara, Jordan Yap", "title": "Diagnostic Accuracy of Content Based Dermatoscopic Image Retrieval with\n  Deep Classification Features", "comments": null, "journal-ref": "Tschandl P, Argenziano G, Razmara M, Yap J. Diagnostic Accuracy of\n  Content Based Dermatoscopic Image Retrieval with Deep Classification\n  Features. Br J Dermatol 2018 Sep 12. doi: 10.1111/bjd.17189", "doi": "10.1111/bjd.17189", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Automated classification of medical images through neural\nnetworks can reach high accuracy rates but lack interpretability.\n  Objectives: To compare the diagnostic accuracy obtained by using content\nbased image retrieval (CBIR) to retrieve visually similar dermatoscopic images\nwith corresponding disease labels against predictions made by a neural network.\n  Methods: A neural network was trained to predict disease classes on\ndermatoscopic images from three retrospectively collected image datasets\ncontaining 888, 2750 and 16691 images respectively. Diagnosis predictions were\nmade based on the most commonly occurring diagnosis in visually similar images,\nor based on the top-1 class prediction of the softmax output from the network.\nOutcome measures were area under the ROC curve for predicting a malignant\nlesion (AUC), multiclass-accuracy and mean average precision (mAP), measured on\nunseen test images of the corresponding dataset.\n  Results: In all three datasets the skin cancer predictions from CBIR\n(evaluating the 16 most similar images) showed AUC values similar to softmax\npredictions (0.842, 0.806 and 0.852 versus 0.830, 0.810 and 0.847 respectively;\np-value>0.99 for all). Similarly, the multiclass-accuracy of CBIR was\ncomparable to softmax predictions. Networks trained for detecting only 3\nclasses performed better on a dataset with 8 classes when using CBIR as\ncompared to softmax predictions (mAP 0.184 vs. 0.368 and 0.198 vs. 0.403\nrespectively).\n  Conclusions: Presenting visually similar images based on features from a\nneural network shows comparable accuracy to the softmax probability-based\ndiagnoses of convolutional neural networks. CBIR may be more helpful than a\nsoftmax classifier in improving diagnostic accuracy of clinicians in a routine\nclinical setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:20:01 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Tschandl", "Philipp", ""], ["Argenziano", "Giuseppe", ""], ["Razmara", "Majid", ""], ["Yap", "Jordan", ""]]}, {"id": "1810.09488", "submitter": "Xiaobin Hu", "authors": "Xiaobin Hu, Hongwei Li, Yu Zhao, Chao Dong, Bjoern H. Menze and Marie\n  Piraud", "title": "Hierarchical multi-class segmentation of glioma images using networks\n  with multi-level activation function", "comments": "12pages first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many segmentation tasks, especially for the biomedical image, the\ntopological prior is vital information which is useful to exploit. The\ncontainment/nesting is a typical inter-class geometric relationship. In the\nMICCAI Brain tumor segmentation challenge, with its three hierarchically nested\nclasses 'whole tumor', 'tumor core', 'active tumor', the nested classes\nrelationship is introduced into the 3D-residual-Unet architecture. The network\ncomprises a context aggregation pathway and a localization pathway, which\nencodes increasingly abstract representation of the input as going deeper into\nthe network, and then recombines these representations with shallower features\nto precisely localize the interest domain via a localization path. The\nnested-class-prior is combined by proposing the multi-class activation function\nand its corresponding loss function. The model is trained on the training\ndataset of Brats2018, and 20% of the dataset is regarded as the validation\ndataset to determine parameters. When the parameters are fixed, we retrain the\nmodel on the whole training dataset. The performance achieved on the validation\nleaderboard is 86%, 77% and 72% Dice scores for the whole tumor, enhancing\ntumor and tumor core classes without relying on ensembles or complicated\npost-processing steps. Based on the same start-of-the-art network architecture,\nthe accuracy of nested-class (enhancing tumor) is reasonably improved from 69%\nto 72% compared with the traditional Softmax-based method which blind to\ntopological prior.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:22:34 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 09:51:19 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Hu", "Xiaobin", ""], ["Li", "Hongwei", ""], ["Zhao", "Yu", ""], ["Dong", "Chao", ""], ["Menze", "Bjoern H.", ""], ["Piraud", "Marie", ""]]}, {"id": "1810.09496", "submitter": "Yoni Kasten", "authors": "Yoni Kasten, Michael Werman", "title": "Two view constraints on the epipoles from few correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general it requires at least 7 point correspondences to compute the\nfundamental matrix between views. We use the cross ratio invariance between\ncorresponding epipolar lines, stemming from epipolar line homography, to derive\na simple formulation for the relationship between epipoles and corresponding\npoints. We show how it can be used to reduce the number of required points for\nthe epipolar geometry when some information about the epipoles is available and\ndemonstrate this with a buddy search app.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:37:36 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kasten", "Yoni", ""], ["Werman", "Michael", ""]]}, {"id": "1810.09499", "submitter": "Nicolai H\\\"ani", "authors": "Nicolai H\\\"ani, Pravakar Roy and Volkan Isler", "title": "A Comparative Study of Fruit Detection and Counting Methods for Yield\n  Mapping in Apple Orchards", "comments": "28 pages", "journal-ref": null, "doi": "10.1002/rob.21902", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for apple detection and counting based on recent deep\nlearning approaches and compare them with state-of-the-art results based on\nclassical methods. Our goal is to quantify performance improvements by neural\nnetwork-based methods compared to methods based on classical approaches.\nAdditionally, we introduce a complete system for counting apples in an entire\nrow. This task is challenging as it requires tracking fruits in images from\nboth sides of the row. We evaluate the performances of three fruit detection\nmethods and two fruit counting methods on six datasets. Results indicate that\nthe classical detection approach still outperforms the deep learning based\nmethods in the majority of the datasets. For fruit counting though, the deep\nlearning based approach performs better for all of the datasets. Combining the\nclassical detection method together with the neural network based counting\napproach, we achieve remarkable yield accuracies ranging from 95.56% to 97.83%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:45:34 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 05:21:56 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["H\u00e4ni", "Nicolai", ""], ["Roy", "Pravakar", ""], ["Isler", "Volkan", ""]]}, {"id": "1810.09528", "submitter": "Muhammad Usman Rafique", "authors": "Nathan Jacobs, Adam Kraft, Muhammad Usman Rafique, Ranti Dev Sharma", "title": "A Weakly Supervised Approach for Estimating Spatial Density Functions\n  from High-Resolution Satellite Imagery", "comments": "10 pages, 8 figures. ACM SIGSPATIAL 2018, Seattle, USA", "journal-ref": "26th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (SIGSPATIAL 18), 2018, Seattle, WA, USA", "doi": "10.1145/3274895.3274934", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network component, the regional aggregation layer, that\nmakes it possible to train a pixel-level density estimator using only\ncoarse-grained density aggregates, which reflect the number of objects in an\nimage region. Our approach is simple to use and does not require\ndomain-specific assumptions about the nature of the density function. We\nevaluate our approach on several synthetic datasets. In addition, we use this\napproach to learn to estimate high-resolution population and housing density\nfrom satellite imagery. In all cases, we find that our approach results in\nbetter density estimates than a commonly used baseline. We also show how our\nhousing density estimator can be used to classify buildings as residential or\nnon-residential.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 20:14:46 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Jacobs", "Nathan", ""], ["Kraft", "Adam", ""], ["Rafique", "Muhammad Usman", ""], ["Sharma", "Ranti Dev", ""]]}, {"id": "1810.09578", "submitter": "Nils Gessert", "authors": "Nils Gessert and Sarah Latus and Youssef S. Abdelwahed and David M.\n  Leistner and Matthias Lutz and Alexander Schlaefer", "title": "Bioresorbable Scaffold Visualization in IVOCT Images Using CNNs and\n  Weakly Supervised Localization", "comments": "Accepted at SPIE: Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioresorbable scaffolds have become a popular choice for treatment of\ncoronary heart disease, replacing traditional metal stents. Often,\nintravascular optical coherence tomography is used to assess potential\nmalapposition after implantation and for follow-up examinations later on.\nTypically, the scaffold is manually reviewed by an expert, analyzing each of\nthe hundreds of image slices. As this is time consuming, automatic stent\ndetection and visualization approaches have been proposed, mostly for metal\nstent detection based on classic image processing. As bioresorbable scaffolds\nare harder to detect, recent approaches have used feature extraction and\nmachine learning methods for automatic detection. However, these methods\nrequire detailed, pixel-level labels in each image slice and extensive feature\nengineering for the particular stent type which might limit the approaches'\ngeneralization capabilities. Therefore, we propose a deep learning-based method\nfor bioresorbable scaffold visualization using only image-level labels. A\nconvolutional neural network is trained to predict whether an image slice\ncontains a metal stent, a bioresorbable scaffold, or no device. Then, we derive\nlocal stent strut information by employing weakly supervised localization using\nsaliency maps with guided backpropagation. As saliency maps are generally\ndiffuse and noisy, we propose a novel patch-based method with image shifting\nwhich allows for high resolution stent visualization. Our convolutional neural\nnetwork model achieves a classification accuracy of 99.0 % for image-level\nstent classification which can be used for both high quality in-slice stent\nvisualization and 3D rendering of the stent structure.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 22:06:09 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Gessert", "Nils", ""], ["Latus", "Sarah", ""], ["Abdelwahed", "Youssef S.", ""], ["Leistner", "David M.", ""], ["Lutz", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1810.09582", "submitter": "Nils Gessert", "authors": "Nils Gessert and Martin Gromniak and Matthias Schl\\\"uter and Alexander\n  Schlaefer", "title": "Two-path 3D CNNs for calibration of system parameters for OCT-based\n  motion compensation", "comments": "Accepted at SPIE: Medical Imaging 2019", "journal-ref": null, "doi": "10.1117/12.2512823", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic motion compensation and adjustment of an intraoperative imaging\nmodality's field of view is a common problem during interventions. Optical\ncoherence tomography (OCT) is an imaging modality which is used in\ninterventions due to its high spatial resolution of few micrometers and its\ntemporal resolution of potentially several hundred volumes per second. However,\nperforming motion compensation with OCT is problematic due to its small field\nof view which might lead to tracked objects being lost quickly. We propose a\nnovel deep learning-based approach that directly learns input parameters of\nmotors that move the scan area for motion compensation from optical coherence\ntomography volumes. We design a two-path 3D convolutional neural network (CNN)\narchitecture that takes two volumes with an object to be tracked as its input\nand predicts the necessary motor input parameters to compensate the object's\nmovement. In this way, we learn the calibration between object movement and\nsystem parameters for motion compensation with arbitrary objects. Thus, we\navoid error-prone hand-eye calibration and handcrafted feature tracking from\nclassical approaches. We achieve an average correlation coefficient of 0.998\nbetween predicted and ground-truth motor parameters which leads to sub-voxel\naccuracy. Furthermore, we show that our deep learning model is real-time\ncapable for use with the system's high volume acquisition frequency.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 22:14:38 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Gessert", "Nils", ""], ["Gromniak", "Martin", ""], ["Schl\u00fcter", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1810.09617", "submitter": "Noa Garcia", "authors": "Noa Garcia, George Vogiatzis", "title": "How to Read Paintings: Semantic Art Understanding with Multi-Modal\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic art analysis has been mostly focused on classifying artworks into\ndifferent artistic styles. However, understanding an artistic representation\ninvolves more complex processes, such as identifying the elements in the scene\nor recognizing author influences. We present SemArt, a multi-modal dataset for\nsemantic art understanding. SemArt is a collection of fine-art painting images\nin which each image is associated to a number of attributes and a textual\nartistic comment, such as those that appear in art catalogues or museum\ncollections. To evaluate semantic art understanding, we envisage the Text2Art\nchallenge, a multi-modal retrieval task where relevant paintings are retrieved\naccording to an artistic text, and vice versa. We also propose several models\nfor encoding visual and textual artistic representations into a common semantic\nspace. Our best approach is able to find the correct image within the top 10\nranked images in the 45.5% of the test samples. Moreover, our models show\nremarkable levels of art understanding when compared against human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 00:54:42 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Garcia", "Noa", ""], ["Vogiatzis", "George", ""]]}, {"id": "1810.09619", "submitter": "Yiwen Guo", "authors": "Yiwen Guo, Chao Zhang, Changshui Zhang and Yurong Chen", "title": "Sparse DNNs with Improved Adversarial Robustness", "comments": "l1 regularization on weights --> l1 regularization on activations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are computationally/memory-intensive and\nvulnerable to adversarial attacks, making them prohibitive in some real-world\napplications. By converting dense models into sparse ones, pruning appears to\nbe a promising solution to reducing the computation/memory cost. This paper\nstudies classification models, especially DNN-based ones, to demonstrate that\nthere exists intrinsic relationships between their sparsity and adversarial\nrobustness. Our analyses reveal, both theoretically and empirically, that\nnonlinear DNN-based classifiers behave differently under $l_2$ attacks from\nsome linear ones. We further demonstrate that an appropriately higher model\nsparsity implies better robustness of nonlinear DNNs, whereas over-sparsified\nmodels can be more difficult to resist adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 01:05:41 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:32:50 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Guo", "Yiwen", ""], ["Zhang", "Chao", ""], ["Zhang", "Changshui", ""], ["Chen", "Yurong", ""]]}, {"id": "1810.09630", "submitter": "Bo Dai", "authors": "Bo Dai, Sanja Fidler, Dahua Lin", "title": "A Neural Compositional Paradigm for Image Captioning", "comments": "32nd Conference on Neural Information Processing Systems (NIPS 2018),\n  Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream captioning models often follow a sequential structure to generate\ncaptions, leading to issues such as introduction of irrelevant semantics, lack\nof diversity in the generated captions, and inadequate generalization\nperformance. In this paper, we present an alternative paradigm for image\ncaptioning, which factorizes the captioning procedure into two stages: (1)\nextracting an explicit semantic representation from the given image; and (2)\nconstructing the caption based on a recursive compositional procedure in a\nbottom-up manner. Compared to conventional ones, our paradigm better preserves\nthe semantic content through an explicit factorization of semantics and syntax.\nBy using the compositional generation procedure, caption construction follows a\nrecursive structure, which naturally fits the properties of human language.\nMoreover, the proposed compositional procedure requires less data to train,\ngeneralizes better, and yields more diverse captions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 02:16:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Dai", "Bo", ""], ["Fidler", "Sanja", ""], ["Lin", "Dahua", ""]]}, {"id": "1810.09631", "submitter": "Ting Sun", "authors": "Ting Sun, Ming Liu, Haoyang Ye, Dit-Yan Yeung", "title": "Point-cloud-based place recognition using CNN feature extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel point-cloud-based place recognition system that\nadopts a deep learning approach for feature extraction. By using a\nconvolutional neural network pre-trained on color images to extract features\nfrom a range image without fine-tuning on extra range images, significant\nimprovement has been observed when compared to using hand-crafted features. The\nresulting system is illumination invariant, rotation invariant and robust\nagainst moving objects that are unrelated to the place identity. Apart from the\nsystem itself, we also bring to the community a new place recognition dataset\ncontaining both point cloud and grayscale images covering a full $360^\\circ$\nenvironmental view. In addition, the dataset is organized in such a way that it\nfacilitates experimental validation with respect to rotation invariance or\nrobustness against unrelated moving objects separately.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 02:23:48 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Sun", "Ting", ""], ["Liu", "Ming", ""], ["Ye", "Haoyang", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1810.09650", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Ruoxi Jia, Gerald Friedland, Bo Li, Costas Spanos", "title": "One Bit Matters: Understanding Adversarial Examples as the Abuse of\n  Redundancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success achieved in machine learning (ML), adversarial\nexamples have caused concerns with regards to its trustworthiness: A small\nperturbation of an input results in an arbitrary failure of an otherwise\nseemingly well-trained ML model. While studies are being conducted to discover\nthe intrinsic properties of adversarial examples, such as their transferability\nand universality, there is insufficient theoretic analysis to help understand\nthe phenomenon in a way that can influence the design process of ML\nexperiments. In this paper, we deduce an information-theoretic model which\nexplains adversarial attacks as the abuse of feature redundancies in ML\nalgorithms. We prove that feature redundancy is a necessary condition for the\nexistence of adversarial examples. Our model helps to explain some major\nquestions raised in many anecdotal studies on adversarial examples. Our theory\nis backed up by empirical measurements of the information content of benign and\nadversarial examples on both image and text datasets. Our measurements show\nthat typical adversarial examples introduce just enough redundancy to overflow\nthe decision making of an ML model trained on corresponding benign examples. We\nconclude with actionable recommendations to improve the robustness of machine\nlearners against adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 04:23:25 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Wang", "Jingkang", ""], ["Jia", "Ruoxi", ""], ["Friedland", "Gerald", ""], ["Li", "Bo", ""], ["Spanos", "Costas", ""]]}, {"id": "1810.09658", "submitter": "Yang Tan", "authors": "Yang Tan, Hongxin Lin, Zelin Xiao, Shengyong Ding, Hongyang Chao", "title": "Face Recognition from Sequential Sparse 3D Data via Deep Registration", "comments": "To be appeared in ICB2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works have shown that face recognition with high accurate 3D data is\nmore reliable and insensitive to pose and illumination variations. Recently,\nlow-cost and portable 3D acquisition techniques like ToF(Time of Flight) and\nDoE based structured light systems enable us to access 3D data easily, e.g.,\nvia a mobile phone. However, such devices only provide sparse(limited speckles\nin structured light system) and noisy 3D data which can not support face\nrecognition directly. In this paper, we aim at achieving high-performance face\nrecognition for devices equipped with such modules which is very meaningful in\npractice as such devices will be very popular. We propose a framework to\nperform face recognition by fusing a sequence of low-quality 3D data. As 3D\ndata are sparse and noisy which can not be well handled by conventional methods\nlike the ICP algorithm, we design a PointNet-like Deep Registration\nNetwork(DRNet) which works with ordered 3D point coordinates while preserving\nthe ability of mining local structures via convolution. Meanwhile we develop a\nnovel loss function to optimize our DRNet based on the quaternion expression\nwhich obviously outperforms other widely used functions. For face recognition,\nwe design a deep convolutional network which takes the fused 3D depth-map as\ninput based on AMSoftmax model. Experiments show that our DRNet can achieve\nrotation error 0.95{\\deg} and translation error 0.28mm for registration. The\nface recognition on fused data also achieves rank-1 accuracy 99.2% , FAR-0.001\n97.5% on Bosphorus dataset which is comparable with state-of-the-art\nhigh-quality data based recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 04:58:48 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 07:47:19 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 10:56:37 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Tan", "Yang", ""], ["Lin", "Hongxin", ""], ["Xiao", "Zelin", ""], ["Ding", "Shengyong", ""], ["Chao", "Hongyang", ""]]}, {"id": "1810.09660", "submitter": "Huu Le", "authors": "Huu Le and Michael Milford", "title": "Large scale visual place recognition with sub-linear storage growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic and animal mapping systems share many of the same objectives and\nchallenges, but differ in one key aspect: where much of the research in robotic\nmapping has focused on solving the data association problem, the grid cell\nneurons underlying maps in the mammalian brain appear to intentionally break\ndata association by encoding many locations with a single grid cell neuron. One\npotential benefit of this intentional aliasing is both sub-linear map storage\nand computational requirements growth with environment size, which we\ndemonstrated in a previous proof-of-concept study that detected and encoded\nmutually complementary co-prime pattern frequencies in the visual map data. In\nthis research, we solve several of the key theoretical and practical\nlimitations of that prototype model and achieve significantly better sub-linear\nstorage growth, a factor reduction in storage requirements per map location,\nscalability to large datasets on standard compute equipment and improved\nrobustness to environments with visually challenging appearance change. These\nimprovements are achieved through several innovations including a flexible\nuser-driven choice mechanism for the periodic patterns underlying the new\nencoding method, a parallelized chunking technique that splits the map into\nsub-sections processed in parallel and a novel feature selection approach that\nselects only the image information most relevant to the encoded temporal\npatterns. We evaluate our techniques on two large benchmark datasets with the\ncomparison to the previous state-of-the-art system, as well as providing a\ndetailed analysis of system performance with respect to parameters such as\nrequired precision performance and the number of cyclic patterns encoded.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 05:04:36 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Le", "Huu", ""], ["Milford", "Michael", ""]]}, {"id": "1810.09676", "submitter": "Ehsan Adeli", "authors": "Hsu-kuang Chiu, Ehsan Adeli, Borui Wang, De-An Huang, Juan Carlos\n  Niebles", "title": "Action-Agnostic Human Pose Forecasting", "comments": "Accepted for publication in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and forecasting human dynamics is a very interesting but\nchallenging task with several prospective applications in robotics,\nhealth-care, etc. Recently, several methods have been developed for human pose\nforecasting; however, they often introduce a number of limitations in their\nsettings. For instance, previous work either focused only on short-term or\nlong-term predictions, while sacrificing one or the other. Furthermore, they\nincluded the activity labels as part of the training process, and require them\nat testing time. These limitations confine the usage of pose forecasting models\nfor real-world applications, as often there are no activity-related annotations\nfor testing scenarios. In this paper, we propose a new action-agnostic method\nfor short- and long-term human pose forecasting. To this end, we propose a new\nrecurrent neural network for modeling the hierarchical and multi-scale\ncharacteristics of the human dynamics, denoted by triangular-prism RNN\n(TP-RNN). Our model captures the latent hierarchical structure embedded in\ntemporal human pose sequences by encoding the temporal dependencies with\ndifferent time-scales. For evaluation, we run an extensive set of experiments\non Human 3.6M and Penn Action datasets and show that our method outperforms\nbaseline and state-of-the-art methods quantitatively and qualitatively. Codes\nare available at https://github.com/eddyhkchiu/pose_forecast_wacv/\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 06:17:53 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Chiu", "Hsu-kuang", ""], ["Adeli", "Ehsan", ""], ["Wang", "Borui", ""], ["Huang", "De-An", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1810.09706", "submitter": "Ang Li", "authors": "Yuanliu Liu, Ang Li, Zejian Yuan, Badong Chen, Nanning Zheng", "title": "Consistency-aware Shading Orders Selective Fusion for Intrinsic Image\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of decomposing a single image into reflectance and\nshading. The difficulty comes from the fact that the components of image---the\nsurface albedo, the direct illumination, and the ambient illumination---are\ncoupled heavily in observed image. We propose to infer the shading by ordering\npixels by their relative brightness, without knowing the absolute values of the\nimage components beforehand. The pairwise shading orders are estimated in two\nways: brightness order and low-order fittings of local shading field. The\nbrightness order is a non-local measure, which can be applied to any pair of\npixels including those whose reflectance and shading are both different. The\nlow-order fittings are used for pixel pairs within local regions of smooth\nshading. Together, they can capture both global order structure and local\nvariations of the shading. We propose a Consistency-aware Selective Fusion\n(CSF) to integrate the pairwise orders into a globally consistent order. The\niterative selection process solves the conflicts between the pairwise orders\nobtained by different estimation methods. Inconsistent or unreliable pairwise\norders will be automatically excluded from the fusion to avoid polluting the\nglobal order. Experiments on the MIT Intrinsic Image dataset show that the\nproposed model is effective at recovering the shading including deep shadows.\nOur model also works well on natural images from the IIW dataset, the UIUC\nShadow dataset and the NYU-Depth dataset, where the colors of direct lights and\nambient lights are quite different.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 07:53:30 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Liu", "Yuanliu", ""], ["Li", "Ang", ""], ["Yuan", "Zejian", ""], ["Chen", "Badong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1810.09720", "submitter": "Ang Li", "authors": "Yuanliu Liu, Zejian Yuan", "title": "Color naming guided intrinsic image decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic image decomposition is a severely under-constrained problem. User\ninteractions can help to reduce the ambiguity of the decomposition\nconsiderably. The traditional way of user interaction is to draw scribbles that\nindicate regions with constant reflectance or shading. However the effect\nscopes of the scribbles are quite limited, so dozens of scribbles are often\nneeded to rectify the whole decomposition, which is time consuming. In this\npaper we propose an efficient way of user interaction that users need only to\nannotate the color composition of the image. Color composition reveals the\nglobal distribution of reflectance, so it can help to adapt the whole\ndecomposition directly. We build a generative model of the process that the\nalbedo of the material produces both the reflectance through imaging and the\ncolor labels by color naming. Our model fuses effectively the physical\nproperties of image formation and the top-down information from human color\nperception. Experimental results show that color naming can improve the\nperformance of intrinsic image decomposition, especially in cleaning the\nshadows left in reflectance and solving the color constancy problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 08:38:39 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Liu", "Yuanliu", ""], ["Yuan", "Zejian", ""]]}, {"id": "1810.09726", "submitter": "Radek Mackowiak", "authors": "Radek Mackowiak, Philip Lenz, Omair Ghori, Ferran Diego, Oliver Lange,\n  Carsten Rother", "title": "CEREALS - Cost-Effective REgion-based Active Learning for Semantic\n  Segmentation", "comments": "Published at British Machine Vision Conference 2018 (BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art methods for semantic image segmentation are trained in a\nsupervised fashion using a large corpus of fully labeled training images.\nHowever, gathering such a corpus is expensive, due to human annotation effort,\nin contrast to gathering unlabeled data. We propose an active learning-based\nstrategy, called CEREALS, in which a human only has to hand-label a few,\nautomatically selected, regions within an unlabeled image corpus. This\nminimizes human annotation effort while maximizing the performance of a\nsemantic image segmentation method. The automatic selection procedure is\nachieved by: a) using a suitable information measure combined with an estimate\nabout human annotation effort, which is inferred from a learned cost model, and\nb) exploiting the spatial coherency of an image. The performance of CEREALS is\ndemonstrated on Cityscapes, where we are able to reduce the annotation effort\nto 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model\nthat was trained with the fully annotated training set of Cityscapes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 08:44:49 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Mackowiak", "Radek", ""], ["Lenz", "Philip", ""], ["Ghori", "Omair", ""], ["Diego", "Ferran", ""], ["Lange", "Oliver", ""], ["Rother", "Carsten", ""]]}, {"id": "1810.09734", "submitter": "Joris Roels", "authors": "Joris Roels, Julian Hennies, Yvan Saeys, Wilfried Philips, Anna\n  Kreshuk", "title": "Domain Adaptive Segmentation in Volume Electron Microscopy Imaging", "comments": "ISBI 2019 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, automated segmentation has become a necessary tool for\nvolume electron microscopy (EM) imaging. So far, the best performing techniques\nhave been largely based on fully supervised encoder-decoder CNNs, requiring a\nsubstantial amount of annotated images. Domain Adaptation (DA) aims to\nalleviate the annotation burden by 'adapting' the networks trained on existing\ngroundtruth data (source domain) to work on a different (target) domain with as\nlittle additional annotation as possible. Most DA research is focused on the\nclassification task, whereas volume EM segmentation remains rather unexplored.\nIn this work, we extend recently proposed classification DA techniques to an\nencoder-decoder layout and propose a novel method that adds a reconstruction\ndecoder to the classical encoder-decoder segmentation in order to align source\nand target encoder features. The method has been validated on the task of\nsegmenting mitochondria in EM volumes. We have performed DA from brain EM\nimages to HeLa cells and from isotropic FIB/SEM volumes to anisotropic TEM\nvolumes. In all cases, the proposed method has outperformed the extended\nclassification DA techniques and the finetuning baseline. An implementation of\nour work can be found on\nhttps://github.com/JorisRoels/domain-adaptive-segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 09:12:41 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 17:51:31 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Roels", "Joris", ""], ["Hennies", "Julian", ""], ["Saeys", "Yvan", ""], ["Philips", "Wilfried", ""], ["Kreshuk", "Anna", ""]]}, {"id": "1810.09735", "submitter": "Joris Roels", "authors": "Joris Roels, Jonas De Vylder, Jan Aelterman, Yvan Saeys, Wilfried\n  Philips", "title": "Convolutional Neural Network Pruning to Accelerate Membrane Segmentation\n  in Electron Microscopy", "comments": "5 pages, 4 figures, ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological membranes are one of the most basic structures and regions of\ninterest in cell biology. In the study of membranes, segment extraction is a\nwell-known and difficult problem because of impeding noise, directional and\nthickness variability, etc. Recent advances in electron microscopy membrane\nsegmentation are able to cope with such difficulties by training convolutional\nneural networks. However, because of the massive amount of features that have\nto be extracted while propagating forward, the practical usability diminishes,\neven with state-of-the-art GPU's. A significant part of these network features\ntypically contains redundancy through correlation and sparsity. In this work,\nwe propose a pruning method for convolutional neural networks that ensures the\ntraining loss increase is minimized. We show that the pruned networks, after\nretraining, are more efficient in terms of time and memory, without\nsignificantly affecting the network accuracy. This way, we manage to obtain\nreal-time membrane segmentation performance, for our specific electron\nmicroscopy setup.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 09:18:16 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Roels", "Joris", ""], ["De Vylder", "Jonas", ""], ["Aelterman", "Jan", ""], ["Saeys", "Yvan", ""], ["Philips", "Wilfried", ""]]}, {"id": "1810.09739", "submitter": "Joris Roels", "authors": "Joris Roels, Jan Aelterman, Jonas De Vylder, Hiep Luong, Yvan Saeys,\n  Wilfried Philips", "title": "Bayesian Deconvolution of Scanning Electron Microscopy Images Using\n  Point-spread Function Estimation and Non-local Regularization", "comments": "5 pages, 4 figures, EMBC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy is one of the most essential imaging techniques in life sciences.\nHigh-quality images are required in order to solve (potentially life-saving)\nbiomedical research problems. Many microscopy techniques do not achieve\nsufficient resolution for these purposes, being limited by physical diffraction\nand hardware deficiencies. Electron microscopy addresses optical diffraction by\nmeasuring emitted or transmitted electrons instead of photons, yielding\nnanometer resolution. Despite pushing back the diffraction limit, blur should\nstill be taken into account because of practical hardware imperfections and\nremaining electron diffraction. Deconvolution algorithms can remove some of the\nblur in post-processing but they depend on knowledge of the point-spread\nfunction (PSF) and should accurately regularize noise. Any errors in the\nestimated PSF or noise model will reduce their effectiveness. This paper\nproposes a new procedure to estimate the lateral component of the point spread\nfunction of a 3D scanning electron microscope more accurately. We also propose\na Bayesian maximum a posteriori deconvolution algorithm with a non-local image\nprior which employs this PSF estimate and previously developed noise\nstatistics. We demonstrate visual quality improvements and show that applying\nour method improves the quality of subsequent segmentation steps.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 09:25:55 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Roels", "Joris", ""], ["Aelterman", "Jan", ""], ["De Vylder", "Jonas", ""], ["Luong", "Hiep", ""], ["Saeys", "Yvan", ""], ["Philips", "Wilfried", ""]]}, {"id": "1810.09776", "submitter": "Ahmed Sabir", "authors": "Ahmed Sabir, Francesc Moreno-Noguer and Llu\\'is Padr\\'o", "title": "Visual Semantic Re-ranker for Text Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current state-of-the-art methods for text recognition are based on\npurely local information and ignore the semantic correlation between text and\nits surrounding visual context. In this paper, we propose a post-processing\napproach to improve the accuracy of text spotting by using the semantic\nrelation between the text and the scene. We initially rely on an off-the-shelf\ndeep neural network that provides a series of text hypotheses for each input\nimage. These text hypotheses are then re-ranked using the semantic relatedness\nwith the object in the image. As a result of this combination, the performance\nof the original network is boosted with a very low computational cost. The\nproposed framework can be used as a drop-in complement for any text-spotting\nalgorithm that outputs a ranking of word hypotheses. We validate our approach\non ICDAR'17 shared task dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:12:18 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 11:31:43 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sabir", "Ahmed", ""], ["Moreno-Noguer", "Francesc", ""], ["Padr\u00f3", "Llu\u00eds", ""]]}, {"id": "1810.09798", "submitter": "Fernando Alonso-Fernandez", "authors": "Fernando Alonso-Fernandez, Josef Bigun, Cristofer Englund", "title": "Expression Recognition Using the Periocular Region: A Feasibility Study", "comments": "Accepted for publication at Intl Conf on Signal Image Technology &\n  Internet Based Systems, SITIS 2018", "journal-ref": "Proc. Intl Conf on Signal Image Technology & Internet Based\n  Systems, SITIS, Gran Canaria, Spain, 26-29 Nov 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the feasibility of using the periocular region for\nexpression recognition. Most works have tried to solve this by analyzing the\nwhole face. Periocular is the facial region in the immediate vicinity of the\neye. It has the advantage of being available over a wide range of distances and\nunder partial face occlusion, thus making it suitable for unconstrained or\nuncooperative scenarios. We evaluate five different image descriptors on a\ndataset of 1,574 images from 118 subjects. The experimental results show an\naverage/overall accuracy of 67.0%/78.0% by fusion of several descriptors. While\nthis accuracy is still behind that attained with full-face methods, it is\nnoteworthy to mention that our initial approach employs only one frame to\npredict the expression, in contraposition to state of the art, exploiting\nseveral order more data comprising spatial-temporal data which is often not\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:56:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Alonso-Fernandez", "Fernando", ""], ["Bigun", "Josef", ""], ["Englund", "Cristofer", ""]]}, {"id": "1810.09801", "submitter": "Fernando Alonso-Fernandez", "authors": "Ram P. Krish, Julian Fierrez, Daniel Ramos, Fernando Alonso-Fernandez,\n  Josef Bigun", "title": "Improving Automated Latent Fingerprint Identification using Extended\n  Minutia Types", "comments": "To appear in Information Fusion journal (Elsevier)", "journal-ref": "Information Fusion, Volume 50, p. 9-19, 2019. ISSN 1566-2535", "doi": "10.1016/j.inffus.2018.10.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent fingerprints are usually processed with Automated Fingerprint\nIdentification Systems (AFIS) by law enforcement agencies to narrow down\npossible suspects from a criminal database. AFIS do not commonly use all\ndiscriminatory features available in fingerprints but typically use only some\ntypes of features automatically extracted by a feature extraction algorithm. In\nthis work, we explore ways to improve rank identification accuracies of AFIS\nwhen only a partial latent fingerprint is available. Towards solving this\nchallenge, we propose a method that exploits extended fingerprint features\n(unusual/rare minutiae) not commonly considered in AFIS. This new method can be\ncombined with any existing minutiae-based matcher. We first compute a\nsimilarity score based on least squares between latent and tenprint minutiae\npoints, with rare minutiae features as reference points. Then the similarity\nscore of the reference minutiae-based matcher at hand is modified based on a\nfitting error from the least square similarity stage. We use a realistic\nforensic fingerprint casework database in our experiments which contains rare\nminutiae features obtained from Guardia Civil, the Spanish law enforcement\nagency. Experiments are conducted using three minutiae-based matchers as a\nreference, namely: NIST-Bozorth3, VeriFinger-SDK and MCC-SDK. We report\nsignificant improvements in the rank identification accuracies when these\nminutiae matchers are augmented with our proposed algorithm based on rare\nminutiae features.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 12:02:22 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Krish", "Ram P.", ""], ["Fierrez", "Julian", ""], ["Ramos", "Daniel", ""], ["Alonso-Fernandez", "Fernando", ""], ["Bigun", "Josef", ""]]}, {"id": "1810.09805", "submitter": "Fernando Alonso-Fernandez", "authors": "Dimitrios Varytimidis, Fernando Alonso-Fernandez, Boris Duran,\n  Cristofer Englund", "title": "Action and intention recognition of pedestrians in urban traffic", "comments": "Accepted for publication at Intl Conf on Signal Image Technology &\n  Internet Based Systems, SITIS 2018", "journal-ref": "Proc. Intl Conf on Signal Image Technology & Internet Based\n  Systems, SITIS, Gran Canaria, Spain, 26-29 Nov 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action and intention recognition of pedestrians in urban settings are\nchallenging problems for Advanced Driver Assistance Systems as well as future\nautonomous vehicles to maintain smooth and safe traffic. This work investigates\na number of feature extraction methods in combination with several machine\nlearning algorithms to build knowledge on how to automatically detect the\naction and intention of pedestrians in urban traffic. We focus on the motion\nand head orientation to predict whether the pedestrian is about to cross the\nstreet or not. The work is based on the Joint Attention for Autonomous Driving\n(JAAD) dataset, which contains 346 videoclips of various traffic scenarios\ncaptured with cameras mounted in the windshield of a car. An accuracy of 72%\nfor head orientation estimation and 85% for motion detection is obtained in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 12:07:32 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Varytimidis", "Dimitrios", ""], ["Alonso-Fernandez", "Fernando", ""], ["Duran", "Boris", ""], ["Englund", "Cristofer", ""]]}, {"id": "1810.09811", "submitter": "Fernando Alonso-Fernandez", "authors": "Frida Femling, Adam Olsson, Fernando Alonso-Fernandez", "title": "Fruit and Vegetable Identification Using Machine Learning for Retail\n  Applications", "comments": "Accepted for publication at Intl Conf on Signal Image Technology &\n  Internet Based Systems, SITIS 2018", "journal-ref": "Proc. Intl Conf on Signal Image Technology & Internet Based\n  Systems, SITIS, Gran Canaria, Spain, 26-29 Nov 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach of creating a system identifying fruit and\nvegetables in the retail market using images captured with a video camera\nattached to the system. The system helps the customers to label desired fruits\nand vegetables with a price according to its weight. The purpose of the system\nis to minimize the number of human computer interactions, speed up the\nidentification process and improve the usability of the graphical user\ninterface compared to existing manual systems. The hardware of the system is\nconstituted by a Raspberry Pi, camera, display, load cell and a case. To\nclassify an object, different convolutional neural networks have been tested\nand retrained. To test the usability, a heuristic evaluation has been performed\nwith several users, concluding that the implemented system is more user\nfriendly compared to existing systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 12:24:03 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Femling", "Frida", ""], ["Olsson", "Adam", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "1810.09821", "submitter": "Qibin Hou", "authors": "Qibin Hou, Peng-Tao Jiang, Yunchao Wei, Ming-Ming Cheng", "title": "Self-Erasing Network for Integral Object Attention", "comments": "Accepted by NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, adversarial erasing for weakly-supervised object attention has been\ndeeply studied due to its capability in localizing integral object regions.\nHowever, such a strategy raises one key problem that attention regions will\ngradually expand to non-object regions as training iterations continue, which\nsignificantly decreases the quality of the produced attention maps. To tackle\nsuch an issue as well as promote the quality of object attention, we introduce\na simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions\nfrom spreading to unexpected background regions. In particular, SeeNet\nleverages two self-erasing strategies to encourage networks to use reliable\nobject and background cues for learning to attention. In this way, integral\nobject regions can be effectively highlighted without including much more\nbackground regions. To test the quality of the generated attention maps, we\nemploy the mined object regions as heuristic cues for learning semantic\nsegmentation models. Experiments on Pascal VOC well demonstrate the superiority\nof our SeeNet over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 12:53:56 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Hou", "Qibin", ""], ["Jiang", "Peng-Tao", ""], ["Wei", "Yunchao", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1810.09849", "submitter": "Zhengsu Chen", "authors": "Zhengsu Chen Jianwei Niu Qi Tian", "title": "DropFilter: Dropout for Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a large number of parameters , deep neural networks have achieved\nremarkable performance on computer vison and natural language processing tasks.\nHowever the networks usually suffer from overfitting by using too much\nparameters. Dropout is a widely use method to deal with overfitting. Although\ndropout can significantly regularize densely connected layers in neural\nnetworks, it leads to suboptimal results when using for convolutional layers.\nTo track this problem, we propose DropFilter, a new dropout method for\nconvolutional layers. DropFilter randomly suppresses the outputs of some\nfilters. Because it is observed that co-adaptions are more likely to occurs\ninter filters rather than intra filters in convolutional layers. Using\nDropFilter, we remarkably improve the performance of convolutional networks on\nCIFAR and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 13:42:25 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Tian", "Zhengsu Chen Jianwei Niu Qi", ""]]}, {"id": "1810.09941", "submitter": "Hadi Kiapour", "authors": "M. Hadi Kiapour, Robinson Piramuthu", "title": "Brand > Logo: Visual Analysis of Fashion Brands", "comments": "ECCV 2018 First Workshop on Computer Vision For Fashion, Art and\n  Design accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While lots of people may think branding begins and ends with a logo, fashion\nbrands communicate their uniqueness through a wide range of visual cues such as\ncolor, patterns and shapes. In this work, we analyze learned visual\nrepresentations by deep networks that are trained to recognize fashion brands.\nIn particular, the activation strength and extent of neurons are studied to\nprovide interesting insights about visual brand expressions. The proposed\nmethod identifies where a brand stands in the spectrum of branding strategy,\ni.e., from trademark-emblazoned goods with bold logos to implicit no logo\nmarketing. By quantifying attention maps, we are able to interpret the visual\ncharacteristics of a brand present in a single image and model the general\ndesign direction of a brand as a whole. We further investigate versatility of\nneurons and discover \"specialists\" that are highly brand-specific and\n\"generalists\" that detect diverse visual features. A human experiment based on\nthree main visual scenarios of fashion brands is conducted to verify the\nalignment of our quantitative measures with the human perception of brands.\nThis paper demonstrate how deep networks go beyond logos in order to recognize\nclothing brands in an image.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 16:06:22 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kiapour", "M. Hadi", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1810.09945", "submitter": "Wojciech Samek", "authors": "Armin W. Thomas, Hauke R. Heekeren, Klaus-Robert M\\\"uller, Wojciech\n  Samek", "title": "Analyzing Neuroimaging Data Through Recurrent Deep Learning Models", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 16:23:27 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 07:31:32 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Thomas", "Armin W.", ""], ["Heekeren", "Hauke R.", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1810.09951", "submitter": "Yujie Zhong", "authors": "Yujie Zhong, Relja Arandjelovi\\'c, Andrew Zisserman", "title": "GhostVLAD for set-based face recognition", "comments": "Accepted by ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to learn a compact representation of image\nsets for template-based face recognition. We make the following contributions:\nfirst, we propose a network architecture which aggregates and embeds the face\ndescriptors produced by deep convolutional neural networks into a compact\nfixed-length representation. This compact representation requires minimal\nmemory storage and enables efficient similarity computation. Second, we propose\na novel GhostVLAD layer that includes {\\em ghost clusters}, that do not\ncontribute to the aggregation. We show that a quality weighting on the input\nfaces emerges automatically such that informative images contribute more than\nthose with low quality, and that the ghost clusters enhance the network's\nability to deal with poor quality images. Third, we explore how input feature\ndimension, number of clusters and different training techniques affect the\nrecognition performance. Given this analysis, we train a network that far\nexceeds the state-of-the-art on the IJB-B face recognition dataset. This is\ncurrently one of the most challenging public benchmarks, and we surpass the\nstate-of-the-art on both the identification and verification protocols.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 16:31:10 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhong", "Yujie", ""], ["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1810.09988", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Xuanjing Huang", "title": "Meta-Learning Multi-task Communication", "comments": "A related blog can be found on the author's homepage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a general framework: Parameters Read-Write\nNetworks (PRaWNs) to systematically analyze current neural models for\nmulti-task learning, in which we find that existing models expect to\ndisentangle features into different spaces while features learned in practice\nare still entangled in shared space, leaving potential hazards for other\ntraining or unseen tasks.\n  We propose to alleviate this problem by incorporating an inductive bias into\nthe process of multi-task learning, that each task can keep informed of not\nonly the knowledge stored in other tasks but the way how other tasks maintain\ntheir knowledge.\n  In practice, we achieve above inductive bias by allowing different tasks to\ncommunicate by passing both hidden variables and gradients explicitly.\n  Experimentally, we evaluate proposed methods on three groups of tasks and two\ntypes of settings (\\textsc{in-task} and \\textsc{out-of-task}). Quantitative and\nqualitative results show their effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 17:42:17 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Liu", "Pengfei", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1810.10039", "submitter": "Taylor Bobrow", "authors": "Taylor L. Bobrow, Faisal Mahmood, Miguel Inserni, and Nicholas J. Durr", "title": "DeepLSR: a deep learning approach for laser speckle reduction", "comments": null, "journal-ref": null, "doi": "10.1364/BOE.10.002869", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speckle artifacts degrade image quality in virtually all modalities that\nutilize coherent energy, including optical coherence tomography, reflectance\nconfocal microscopy, ultrasound, and widefield imaging with laser illumination.\nWe present an adversarial deep learning framework for laser speckle reduction,\ncalled DeepLSR (https://durr.jhu.edu/DeepLSR), that transforms images from a\nsource domain of coherent illumination to a target domain of speckle-free,\nincoherent illumination. We apply this method to widefield images of objects\nand tissues illuminated with a multi-wavelength laser, using light emitting\ndiode-illuminated images as ground truth. In images of gastrointestinal\ntissues, DeepLSR reduces laser speckle noise by 6.4 dB, compared to a 2.9 dB\nreduction from optimized non-local means processing, a 3.0 dB reduction from\nBM3D, and a 3.7 dB reduction from an optical speckle reducer utilizing an\noscillating diffuser. Further, DeepLSR can be combined with optical speckle\nreduction to reduce speckle noise by 9.4 dB. This dramatic reduction in speckle\nnoise may enable the use of coherent light sources in applications that require\nsmall illumination sources and high-quality imaging, including medical\nendoscopy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:36:35 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 18:15:53 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 03:09:52 GMT"}, {"version": "v4", "created": "Tue, 23 Apr 2019 00:20:01 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Bobrow", "Taylor L.", ""], ["Mahmood", "Faisal", ""], ["Inserni", "Miguel", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1810.10066", "submitter": "Zhile Ren", "authors": "Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B. Sudderth\n  and Jan Kautz", "title": "A Fusion Approach for Multi-Frame Optical Flow Estimation", "comments": "Work accepted at IEEE Winter Conference on Applications of Computer\n  Vision (WACV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To date, top-performing optical flow estimation methods only take pairs of\nconsecutive frames into account. While elegant and appealing, the idea of using\nmore than two frames has not yet produced state-of-the-art results. We present\na simple, yet effective fusion approach for multi-frame optical flow that\nbenefits from longer-term temporal cues. Our method first warps the optical\nflow from previous frames to the current, thereby yielding multiple plausible\nestimates. It then fuses the complementary information carried by these\nestimates into a new optical flow field. At the time of writing, our method\nranks first among published results in the MPI Sintel and KITTI 2015\nbenchmarks. Our models will be available on https://github.com/NVlabs/PWC-Net.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 19:46:57 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 18:10:01 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ren", "Zhile", ""], ["Gallo", "Orazio", ""], ["Sun", "Deqing", ""], ["Yang", "Ming-Hsuan", ""], ["Sudderth", "Erik B.", ""], ["Kautz", "Jan", ""]]}, {"id": "1810.10090", "submitter": "Biyi Fang", "authors": "Biyi Fang, Xiao Zeng, Mi Zhang", "title": "NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for\n  Continuous Mobile Vision", "comments": "12 pages", "journal-ref": "Fang, Biyi, Xiao Zeng, and Mi Zhang. \"NestDNN: Resource-Aware\n  Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision.\"\n  Proceedings of the 24th Annual International Conference on Mobile Computing\n  and Networking. ACM, 2018", "doi": "10.1145/3241539.3241559", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile vision systems such as smartphones, drones, and augmented-reality\nheadsets are revolutionizing our lives. These systems usually run multiple\napplications concurrently and their available resources at runtime are dynamic\ndue to events such as starting new applications, closing existing applications,\nand application priority changes. In this paper, we present NestDNN, a\nframework that takes the dynamics of runtime resources into account to enable\nresource-aware multi-tenant on-device deep learning for mobile vision systems.\nNestDNN enables each deep learning model to offer flexible resource-accuracy\ntrade-offs. At runtime, it dynamically selects the optimal resource-accuracy\ntrade-off for each deep learning model to fit the model's resource demand to\nthe system's available runtime resources. In doing so, NestDNN efficiently\nutilizes the limited resources in mobile vision systems to jointly maximize the\nperformance of all the concurrently running applications. Our experiments show\nthat compared to the resource-agnostic status quo approach, NestDNN achieves as\nmuch as 4.2% increase in inference accuracy, 2.0x increase in video frame\nprocessing rate and 1.7x reduction on energy consumption.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 21:07:42 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Fang", "Biyi", ""], ["Zeng", "Xiao", ""], ["Zhang", "Mi", ""]]}, {"id": "1810.10093", "submitter": "Stan Birchfield", "authors": "Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric\n  Cameracci, Gavriel State, Omer Shapira, Stan Birchfield", "title": "Structured Domain Randomization: Bridging the Reality Gap by\n  Context-Aware Synthetic Data", "comments": "ICRA 2019; for video, see https://youtu.be/1WdjWJYx9AY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present structured domain randomization (SDR), a variant of domain\nrandomization (DR) that takes into account the structure and context of the\nscene. In contrast to DR, which places objects and distractors randomly\naccording to a uniform probability distribution, SDR places objects and\ndistractors randomly according to probability distributions that arise from the\nspecific problem at hand. In this manner, SDR-generated imagery enables the\nneural network to take the context around an object into consideration during\ndetection. We demonstrate the power of SDR for the problem of 2D bounding box\ncar detection, achieving competitive results on real data after training only\non synthetic data. On the KITTI easy, moderate, and hard tasks, we show that\nSDR outperforms other approaches to generating synthetic data (VKITTI, Sim\n200k, or DR), as well as real data collected in a different domain (BDD100K).\nMoreover, synthetic SDR data combined with real KITTI data outperforms real\nKITTI data alone.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 21:15:55 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 14:16:52 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Prakash", "Aayush", ""], ["Boochoon", "Shaad", ""], ["Brophy", "Mark", ""], ["Acuna", "David", ""], ["Cameracci", "Eric", ""], ["State", "Gavriel", ""], ["Shapira", "Omer", ""], ["Birchfield", "Stan", ""]]}, {"id": "1810.10110", "submitter": "Gilbert Rotich", "authors": "Gilbert Rotich, Rodrigo Minetto and Sudeep Sarkar", "title": "Resource-Constrained Simultaneous Detection and Labeling of Objects in\n  High-Resolution Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a strategy for detection and classification of man-made objects\nin large high-resolution satellite photos under computational resource\nconstraints. We detect and classify candidate objects by using five pipelines\nof convolutional neural network processing (CNN), run in parallel. Each\npipeline has its own unique strategy for fine tunning parameters, proposal\nregion filtering, and dealing with image scales. The conflicting region\nproposals are merged based on region confidence and not just based on overlap\nareas, which improves the quality of the final bounding-box regions selected.\nWe demonstrate this strategy using the recent xView challenge, which is a\ncomplex benchmark with more than 1,100 high-resolution images, spanning 800,000\naerial objects around the world covering a total area of 1,400 square\nkilometers at 0.3 meter ground sample distance. To tackle the\nresource-constrained problem posed by the xView challenge, where inferences are\nrestricted to be on CPU with 8GB memory limit, we used lightweight CNN's\ntrained with the single shot detector algorithm. Our approach was competitive\non sequestered sets; it was ranked third.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 22:19:09 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Rotich", "Gilbert", ""], ["Minetto", "Rodrigo", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1810.10117", "submitter": "Gerard Snaauw", "authors": "Gerard Snaauw, Dong Gong, Gabriel Maicas, Anton van den Hengel, Wiro\n  J. Niessen, Johan Verjans, Gustavo Carneiro", "title": "End-to-End Diagnosis and Segmentation Learning from Cardiac Magnetic\n  Resonance Imaging", "comments": "submitted to 2019 IEEE International Symposium on Biomedical Imaging\n  (ISBI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac magnetic resonance (CMR) is used extensively in the diagnosis and\nmanagement of cardiovascular disease. Deep learning methods have proven to\ndeliver segmentation results comparable to human experts in CMR imaging, but\nthere have been no convincing results for the problem of end-to-end\nsegmentation and diagnosis from CMR. This is in part due to a lack of\nsufficiently large datasets required to train robust diagnosis models. In this\npaper, we propose a learning method to train diagnosis models, where our\napproach is designed to work with relatively small datasets. In particular, the\noptimisation loss is based on multi-task learning that jointly trains for the\ntasks of segmentation and diagnosis classification. We hypothesize that\nsegmentation has a regularizing effect on the learning of features relevant for\ndiagnosis. Using the 100 training and 50 testing samples available from the\nAutomated Cardiac Diagnosis Challenge (ACDC) dataset, which has a balanced\ndistribution of 5 cardiac diagnoses, we observe a reduction of the\nclassification error from 32% to 22%, and a faster convergence compared to a\nbaseline without segmentation. To the best of our knowledge, this is the best\ndiagnosis results from CMR using an end-to-end diagnosis and segmentation\nlearning method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 22:40:13 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Snaauw", "Gerard", ""], ["Gong", "Dong", ""], ["Maicas", "Gabriel", ""], ["Hengel", "Anton van den", ""], ["Niessen", "Wiro J.", ""], ["Verjans", "Johan", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1810.10134", "submitter": "Huu Le", "authors": "Huu Le, Anders Eriksson, Thanh-Toan Do, Michael Milford", "title": "A Binary Optimization Approach for Constrained K-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Means clustering still plays an important role in many computer vision\nproblems. While the conventional Lloyd method, which alternates between\ncentroid update and cluster assignment, is primarily used in practice, it may\nconverge to a solution with empty clusters. Furthermore, some applications may\nrequire the clusters to satisfy a specific set of constraints, e.g., cluster\nsizes, must-link/cannot-link. Several methods have been introduced to solve\nconstrained K-Means clustering. Due to the non-convex nature of K-Means,\nhowever, existing approaches may result in sub-optimal solutions that poorly\napproximate the true clusters. In this work, we provide a new perspective to\ntackle this problem. Particularly, we reconsider constrained K-Means as a\nBinary Optimization Problem and propose a novel optimization scheme to search\nfor feasible solutions in the binary domain. This approach allows us to solve\nconstrained K-Means where multiple types of constraints can be simultaneously\nenforced. Experimental results on synthetic and real datasets show that our\nmethod provides better clustering accuracy with faster runtime compared to\nseveral commonly used techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 00:11:33 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 05:51:09 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Le", "Huu", ""], ["Eriksson", "Anders", ""], ["Do", "Thanh-Toan", ""], ["Milford", "Michael", ""]]}, {"id": "1810.10148", "submitter": "Menglin Jia", "authors": "Menglin Jia, Yichen Zhou, Mengyun Shi, Bharath Hariharan", "title": "A Deep-Learning-Based Fashion Attributes Detection Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing fashion attributes is essential in the fashion design process.\nCurrent fashion forecasting firms, such as WGSN utilizes information from all\naround the world (from fashion shows, visual merchandising, blogs, etc). They\ngather information by experience, by observation, by media scan, by interviews,\nand by exposed to new things. Such information analyzing process is called\nabstracting, which recognize similarities or differences across all the\ngarments and collections. In fact, such abstraction ability is useful in many\nfashion careers with different purposes. Fashion forecasters abstract across\ndesign collections and across time to identify fashion change and directions;\ndesigners, product developers and buyers abstract across a group of garments\nand collections to develop a cohesive and visually appeal lines; sales and\nmarketing executives abstract across product line each season to recognize\nselling points; fashion journalist and bloggers abstract across runway photos\nto recognize symbolic core concepts that can be translated into editorial\nfeatures. Fashion attributes analysis for such fashion insiders requires much\ndetailed and in-depth attributes annotation than that for consumers, and\nrequires inference on multiple domains. In this project, we propose a\ndata-driven approach for recognizing fashion attributes. Specifically, a\nmodified version of Faster R-CNN model is trained on images from a large-scale\nlocalization dataset with 594 fine-grained attributes under different\nscenarios, for example in online stores and street snapshots. This model will\nthen be used to detect garment items and classify clothing attributes for\nrunway photos and fashion illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 01:22:01 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Jia", "Menglin", ""], ["Zhou", "Yichen", ""], ["Shi", "Mengyun", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1810.10151", "submitter": "Cheng Li", "authors": "Hui Sun, Cheng Li, Boqiang Liu, Hairong Zheng, David Dagan Feng, and\n  Shanshan Wang", "title": "AUNet: Attention-guided dense-upsampling networks for breast mass\n  segmentation in whole mammograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is one of the most commonly applied tools for early breast cancer\nscreening. Automatic segmentation of breast masses in mammograms is essential\nbut challenging due to the low signal-to-noise ratio and the wide variety of\nmass shapes and sizes. Existing methods deal with these challenges mainly by\nextracting mass-centered image patches manually or automatically. However,\nmanual patch extraction is time-consuming and automatic patch extraction brings\nerrors that could not be compensated in the following segmentation step. In\nthis study, we propose a novel attention-guided dense-upsampling network\n(AUNet) for accurate breast mass segmentation in whole mammograms directly. In\nAUNet, we employ an asymmetrical encoder-decoder structure and propose an\neffective upsampling block, attention-guided dense-upsampling block (AU block).\nEspecially, the AU block is designed to have three merits. Firstly, it\ncompensates the information loss of bilinear upsampling by dense upsampling.\nSecondly, it designs a more effective method to fuse high- and low-level\nfeatures. Thirdly, it includes a channel-attention function to highlight\nrich-information channels. We evaluated the proposed method on two publicly\navailable datasets, CBIS-DDSM and INbreast. Compared to three state-of-the-art\nfully convolutional networks, AUNet achieved the best performances with an\naverage Dice similarity coefficient of 81.8% for CBIS-DDSM and 79.1% for\nINbreast.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 01:55:33 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 06:15:05 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 07:53:47 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Sun", "Hui", ""], ["Li", "Cheng", ""], ["Liu", "Boqiang", ""], ["Zheng", "Hairong", ""], ["Feng", "David Dagan", ""], ["Wang", "Shanshan", ""]]}, {"id": "1810.10155", "submitter": "Min Chen", "authors": "Min Chen, Andy Song, Shivanthan A. C. Yhanandan, and Jing Zhang", "title": "Background Subtraction using Compressed Low-resolution Images", "comments": "4 pages,36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image processing and recognition are an important part of the modern society,\nwith applications in fields such as advanced artificial intelligence, smart\nassistants, and security surveillance. The essential first step involved in\nalmost all the visual tasks is background subtraction with a static camera.\nEnsuring that this critical step is performed in the most efficient manner\nwould therefore improve all aspects related to objects recognition and\ntracking, behavior comprehension, etc.. Although background subtraction method\nhas been applied for many years, its application suffers from real-time\nrequirement. In this letter, we present a novel approach in implementing the\nbackground subtraction. The proposed method uses compressed, low-resolution\ngrayscale image for the background subtraction. These low-resolution grayscale\nimages were found to preserve the salient information very well. To verify the\nfeasibility of our methodology, two prevalent methods, ViBe and GMM, are used\nin the experiment. The results of the proposed methodology confirm the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 02:37:08 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Chen", "Min", ""], ["Song", "Andy", ""], ["Yhanandan", "Shivanthan A. C.", ""], ["Zhang", "Jing", ""]]}, {"id": "1810.10165", "submitter": "Nevan Wichers", "authors": "Nevan Wichers, Dilek Hakkani-Tur, Jindong Chen", "title": "Resolving Referring Expressions in Images With Labeled Elements", "comments": "Accepted into IEEE SLT Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images may have elements containing text and a bounding box associated with\nthem, for example, text identified via optical character recognition on a\ncomputer screen image, or a natural image with labeled objects. We present an\nend-to-end trainable architecture to incorporate the information from these\nelements and the image to segment/identify the part of the image a natural\nlanguage expression is referring to. We calculate an embedding for each element\nand then project it onto the corresponding location (i.e., the associated\nbounding box) of the image feature map. We show that this architecture gives an\nimprovement in resolving referring expressions, over only using the image, and\nother methods that incorporate the element information. We demonstrate\nexperimental results on the referring expression datasets based on COCO, and on\na webpage image referring expression dataset that we developed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 03:22:08 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 22:17:21 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Wichers", "Nevan", ""], ["Hakkani-Tur", "Dilek", ""], ["Chen", "Jindong", ""]]}, {"id": "1810.10188", "submitter": "Sanket Biswas", "authors": "Subhajit Maity, Sujan Sarkar, Avinaba Tapadar, Ayan Dutta, Sanket\n  Biswas, Sayon Nayek, Pritam Saha", "title": "Fault Area Detection in Leaf Diseases using k-means Clustering", "comments": "This article is of 5 pages in IEEE format. It has been presented as a\n  full paper in International Conference on Trends in Electronics and\n  Informatics (ICOEI 2018) and is currently under the proceedings of the\n  conference and yet to be published in IEEE Xplore", "journal-ref": null, "doi": "10.1109/ICOEI.2018.8553913", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing population the crisis of food is getting bigger day by day.In\nthis time of crisis,the leaf disease of crops is the biggest problem in the\nfood industry.In this paper, we have addressed that problem and proposed an\nefficient method to detect leaf disease.Leaf diseases can be detected from\nsample images of the leaf with the help of image processing and\nsegmentation.Using k-means clustering and Otsu's method the faulty region in a\nleaf is detected which helps to determine proper course of action to be\ntaken.Further the ratio of normal and faulty region if calculated would be able\nto predict if the leaf can be cured at all.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 05:08:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Maity", "Subhajit", ""], ["Sarkar", "Sujan", ""], ["Tapadar", "Avinaba", ""], ["Dutta", "Ayan", ""], ["Biswas", "Sanket", ""], ["Nayek", "Sayon", ""], ["Saha", "Pritam", ""]]}, {"id": "1810.10193", "submitter": "Wei Zhou", "authors": "Wei Zhou, Julie Stephany Berrio, Stewart Worrall, Eduardo Nebot", "title": "Automated Evaluation of Semantic Segmentation Robustness for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2019.2909066", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental challenges in the design of perception systems for\nautonomous vehicles is validating the performance of each algorithm under a\ncomprehensive variety of operating conditions. In the case of vision-based\nsemantic segmentation, there are known issues when encountering new scenarios\nthat are sufficiently different to the training data. In addition, even small\nvariations in environmental conditions such as illumination and precipitation\ncan affect the classification performance of the segmentation model. Given the\nreliance on visual information, these effects often translate into poor\nsemantic pixel classification which can potentially lead to catastrophic\nconsequences when driving autonomously. This paper presents a novel method for\nanalysing the robustness of semantic segmentation models and provides a number\nof metrics to evaluate the classification performance over a variety of\nenvironmental conditions. The process incorporates an additional sensor (lidar)\nto automate the process, eliminating the need for labour-intensive hand\nlabelling of validation data. The system integrity can be monitored as the\nperformance of the vision sensors are validated against a different sensor\nmodality. This is necessary for detecting failures that are inherent to vision\ntechnology. Experimental results are presented based on multiple datasets\ncollected at different times of the year with different environmental\nconditions. These results show that the semantic segmentation performance\nvaries depending on the weather, camera parameters, existence of shadows, etc..\nThe results also demonstrate how the metrics can be used to compare and\nvalidate the performance after making improvements to a model, and compare the\nperformance of different networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 05:36:53 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhou", "Wei", ""], ["Berrio", "Julie Stephany", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "1810.10220", "submitter": "Ying Tai", "authors": "Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang,\n  Chengjie Wang, Jilin Li, Feiyue Huang", "title": "DSFD: Dual Shot Face Detector", "comments": "Camera-ready version of DSFD for CVPR 2019. Code is available at:\n  https://github.com/TencentYoutuResearch/FaceDetection-DSFD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel face detection network with three novel\ncontributions that address three key aspects of face detection, including\nbetter feature learning, progressive loss design and anchor assign based data\naugmentation, respectively. First, we propose a Feature Enhance Module (FEM)\nfor enhancing the original feature maps to extend the single shot detector to\ndual shot detector. Second, we adopt Progressive Anchor Loss (PAL) computed by\ntwo different sets of anchors to effectively facilitate the features. Third, we\nuse an Improved Anchor Matching (IAM) by integrating novel anchor assign\nstrategy into data augmentation to provide better initialization for the\nregressor. Since these techniques are all related to the two-stream design, we\nname the proposed network as Dual Shot Face Detector (DSFD). Extensive\nexperiments on popular benchmarks, WIDER FACE and FDDB, demonstrate the\nsuperiority of DSFD over the state-of-the-art face detectors.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 07:26:47 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 13:13:33 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 12:09:49 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Jian", ""], ["Wang", "Yabiao", ""], ["Wang", "Changan", ""], ["Tai", "Ying", ""], ["Qian", "Jianjun", ""], ["Yang", "Jian", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "1810.10221", "submitter": "Zijie Zhuang", "authors": "Zijie Zhuang, Haizhou Ai, Long Chen, and Chong Shang", "title": "Cross-Resolution Person Re-identification with Deep Antithetical\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images with different resolutions are ubiquitous in public person\nre-identification (ReID) datasets and real-world scenes, it is thus crucial for\na person ReID model to handle the image resolution variations for improving its\ngeneralization ability. However, most existing person ReID methods pay little\nattention to this resolution discrepancy problem. One paradigm to deal with\nthis problem is to use some complicated methods for mapping all images into an\nartificial image space, which however will disrupt the natural image\ndistribution and requires heavy image preprocessing. In this paper, we analyze\nthe deficiencies of several widely-used objective functions handling image\nresolution discrepancies and propose a new framework called deep antithetical\nlearning that directly learns from the natural image space rather than creating\nan arbitrary one. We first quantify and categorize original training images\naccording to their resolutions. Then we create an antithetical training set and\nmake sure that original training images have counterparts with antithetical\nresolutions in this new set. At last, a novel Contrastive Center Loss(CCL) is\nproposed to learn from images with different resolutions without being\ninterfered by their resolution discrepancies. Extensive experimental analyses\nand evaluations indicate that the proposed framework, even using a vanilla deep\nReID network, exhibits remarkable performance improvements. Without bells and\nwhistles, our approach outperforms previous state-of-the-art methods by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 07:33:30 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhuang", "Zijie", ""], ["Ai", "Haizhou", ""], ["Chen", "Long", ""], ["Shang", "Chong", ""]]}, {"id": "1810.10286", "submitter": "Qing Lyu", "authors": "Qing Lyu, Minghao Chen, and Xiang Chen", "title": "Learning color space adaptation from synthetic to real images of cirrus\n  clouds", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud segmentation plays a crucial role in image analysis for climate\nmodeling. Manually labeling the training data for cloud segmentation is\ntime-consuming and error-prone. We explore to train segmentation networks with\nsynthetic data due to the natural acquisition of pixel-level labels.\nNevertheless, the domain gap between synthetic and real images significantly\ndegrades the performance of the trained model. We propose a color space\nadaptation method to bridge the gap, by training a color-sensitive generator\nand discriminator to adapt synthetic data to real images in color space.\nInstead of transforming images by general convolutional kernels, we adopt a set\nof closed-form operations to make color-space adjustments while preserving the\nlabels. We also construct a synthetic-to-real cirrus cloud dataset SynCloud and\ndemonstrate the adaptation efficacy on the semantic segmentation task of cirrus\nclouds. With our adapted synthetic data for training the semantic segmentation,\nwe achieve an improvement of 6:59% when applied to real images, superior to\nalternative methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 10:53:03 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 03:28:40 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lyu", "Qing", ""], ["Chen", "Minghao", ""], ["Chen", "Xiang", ""]]}, {"id": "1810.10289", "submitter": "Dongdong Yu", "authors": "Jia Sun, Dongdong Yu, Yinghong Li, Changhu Wang", "title": "Mask Propagation Network for Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a mask propagation network to treat the video\nsegmentation problem as a concept of the guided instance segmentation. Similar\nto most MaskTrack based video segmentation methods, our method takes the mask\nprobability map of previous frame and the appearance of current frame as\ninputs, and predicts the mask probability map for the current frame.\nSpecifically, we adopt the Xception backbone based DeepLab v3+ model as the\nprobability map predictor in our prediction pipeline. Besides, instead of the\nfull image and the original mask probability, our network takes the region of\ninterest of the instance, and the new mask probability which warped by the\noptical flow between the previous and current frames as the inputs. We also\nensemble the modified One-Shot Video Segmentation Network to make the final\npredictions in order to retrieve and segment the missing instance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 10:59:51 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Sun", "Jia", ""], ["Yu", "Dongdong", ""], ["Li", "Yinghong", ""], ["Wang", "Changhu", ""]]}, {"id": "1810.10293", "submitter": "Matvey Ezhov", "authors": "Matvey Ezhov, Adel Zakirov, Maxim Gusarev", "title": "Coarse-to-fine volumetric segmentation of teeth in Cone-Beam CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of localizing and segmenting individual teeth inside\n3D Cone-Beam Computed Tomography (CBCT) images. To handle large image sizes we\napproach this task with a coarse-to-fine framework, where the whole volume is\nfirst analyzed as a 33-class semantic segmentation (adults have up to 32 teeth)\nin coarse resolution, followed by binary semantic segmentation of the cropped\nregion of interest in original resolution. To improve the performance of the\nchallenging 33-class segmentation, we first train the Coarse step model on a\nlarge weakly labeled dataset, then fine-tune it on a smaller precisely labeled\ndataset. The Fine step model is trained with precise labels only. Experiments\nusing our in-house dataset show significant improvement for both\nweakly-supervised pretraining and for the addition of the Fine step.\nEmpirically, this framework yields precise teeth masks with low localization\nerrors sufficient for many real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 11:07:44 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Ezhov", "Matvey", ""], ["Zakirov", "Adel", ""], ["Gusarev", "Maxim", ""]]}, {"id": "1810.10309", "submitter": "Matvey Ezhov", "authors": "Adel Zakirov, Matvey Ezhov, Maxim Gusarev, Vladimir Alexandrovsky,\n  Evgeny Shumilov", "title": "Dental pathology detection in 3D cone-beam CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cone-beam computed tomography (CBCT) is a valuable imaging method in dental\ndiagnostics that provides information not available in traditional 2D imaging.\nHowever, interpretation of CBCT images is a time-consuming process that\nrequires a physician to work with complicated software. In this work we propose\nan automated pipeline composed of several deep convolutional neural networks\nand algorithmic heuristics. Our task is two-fold: a) find locations of each\npresent tooth inside a 3D image volume, and b) detect several common tooth\nconditions in each tooth. The proposed system achieves 96.3\\% accuracy in tooth\nlocalization and an average of 0.94 AUROC for 6 common tooth conditions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 12:04:11 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zakirov", "Adel", ""], ["Ezhov", "Matvey", ""], ["Gusarev", "Maxim", ""], ["Alexandrovsky", "Vladimir", ""], ["Shumilov", "Evgeny", ""]]}, {"id": "1810.10323", "submitter": "Dong Kyun Shin", "authors": "Dong Kyun Shin, Minhaz Uddin Ahmed and Phill Kyu Rhee", "title": "Incremental Deep Learning for Robust Object Detection in Unknown\n  Cluttered Environments", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2875720", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in streaming images is a major step in different\ndetection-based applications, such as object tracking, action recognition,\nrobot navigation, and visual surveillance applications. In mostcases, image\nquality is noisy and biased, and as a result, the data distributions are\ndisturbed and imbalanced. Most object detection approaches, such as the faster\nregion-based convolutional neural network (Faster RCNN), Single Shot Multibox\nDetector with 300x300 inputs (SSD300), and You Only Look Once version 2\n(YOLOv2), rely on simple sampling without considering distortions and noise\nunder real-world changing environments, despite poor object labeling. In this\npaper, we propose an Incremental active semi-supervised learning (IASSL)\ntechnology for unseen object detection. It combines batch-based active learning\n(AL) and bin-based semi-supervised learning (SSL) to leverage the strong points\nof AL's exploration and SSL's exploitation capabilities. A collaborative\nsampling method is also adopted to measure the uncertainty and diversity of AL\nand the confidence in SSL. Batch-based AL allows us to select more informative,\nconfident, and representative samples with low cost. Bin-based SSL divides\nstreaming image samples into several bins, and each bin repeatedly transfers\nthe discriminative knowledge of convolutional neural network (CNN) deep\nlearning to the next bin until the performance criterion is reached. IASSL can\novercome noisy and biased labels in unknown, cluttered data distributions. We\nobtain superior performance, compared to state-of-the-art technologies such as\nFaster RCNN, SSD300, and YOLOv2.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 17:10:41 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Shin", "Dong Kyun", ""], ["Ahmed", "Minhaz Uddin", ""], ["Rhee", "Phill Kyu", ""]]}, {"id": "1810.10324", "submitter": "Christopher Tralie", "authors": "Christopher J. Tralie, Paul Bendich, John Harer", "title": "Multi-scale Geometric Summaries for Similarity-based Sensor Fusion", "comments": "9 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address fusion of heterogeneous sensor data using\nwavelet-based summaries of fused self-similarity information from each sensor.\nThe technique we develop is quite general, does not require domain specific\nknowledge or physical models, and requires no training. Nonetheless, it can\nperform surprisingly well at the general task of differentiating classes of\ntime-ordered behavior sequences which are sensed by more than one modality. As\na demonstration of our capabilities in the audio to video context, we focus on\nthe differentiation of speech sequences.\n  Data from two or more modalities first are represented using self-similarity\nmatrices(SSMs) corresponding to time-ordered point clouds in feature spaces of\neach of these data sources; we note that these feature spaces can be of\nentirely different scale and dimensionality.\n  A fused similarity template is then derived from the modality-specific SSMs\nusing a technique called similarity network fusion (SNF). We investigate\npipelines using SNF as both an upstream (feature-level) and a downstream\n(ranking-level) fusion technique. Multiscale geometric features of this\ntemplate are then extracted using a recently-developed technique called the\nscattering transform, and these features are then used to differentiate speech\nsequences. This method outperforms unsupervised techniques which operate\ndirectly on the raw data, and it also outperforms stovepiped methods which\noperate on SSMs separately derived from the distinct modalities. The benefits\nof this method become even more apparent as the simulated peak signal to noise\nratio decreases.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 18:52:07 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 01:27:01 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Tralie", "Christopher J.", ""], ["Bendich", "Paul", ""], ["Harer", "John", ""]]}, {"id": "1810.10325", "submitter": "Sebastian Niehaus", "authors": "Jonas Koenig, Simon Malberg, Martin Martens, Sebastian Niehaus, Artus\n  Krohn-Grimberghe, Arunselvan Ramaswamy", "title": "Multi-Stage Reinforcement Learning For Object Detection", "comments": "Accepted for the Computer Vision Conference (CVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a reinforcement learning approach for detecting objects within an\nimage. Our approach performs a step-wise deformation of a bounding box with the\ngoal of tightly framing the object. It uses a hierarchical tree-like\nrepresentation of predefined region candidates, which the agent can zoom in on.\nThis reduces the number of region candidates that must be evaluated so that the\nagent can afford to compute new feature maps before each step to enhance\ndetection quality. We compare an approach that is based purely on zoom actions\nwith one that is extended by a second refinement stage to fine-tune the\nbounding box after each zoom step. We also improve the fitting ability by\nallowing for different aspect ratios of the bounding box. Finally, we propose\ndifferent reward functions to lead to a better guidance of the agent while\nfollowing its search trajectories. Experiments indicate that each of these\nextensions leads to more correct detections. The best performing approach\ncomprises a zoom stage and a refinement stage, uses aspect-ratio modifying\nactions and is trained using a combination of three different reward metrics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 21:41:57 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 11:11:02 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Koenig", "Jonas", ""], ["Malberg", "Simon", ""], ["Martens", "Martin", ""], ["Niehaus", "Sebastian", ""], ["Krohn-Grimberghe", "Artus", ""], ["Ramaswamy", "Arunselvan", ""]]}, {"id": "1810.10326", "submitter": "Lisa Graziani", "authors": "Lisa Graziani, Stefano Melacci, Marco Gori", "title": "Coherence Constraints in Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing facial expressions from static images or video sequences is a\nwidely studied but still challenging problem. The recent progresses obtained by\ndeep neural architectures, or by ensembles of heterogeneous models, have shown\nthat integrating multiple input representations leads to state-of-the-art\nresults. In particular, the appearance and the shape of the input face, or the\nrepresentations of some face parts, are commonly used to boost the quality of\nthe recognizer. This paper investigates the application of Convolutional Neural\nNetworks (CNNs) with the aim of building a versatile recognizer of expressions\nin static images that can be further applied to video sequences. We first study\nthe importance of different face parts in the recognition task, focussing on\nappearance and shape-related features. Then we cast the learning problem in the\nSemi-Supervised setting, exploiting video data, where only a few frames are\nsupervised. The unsupervised portion of the training data is used to enforce\nthree types of coherence, namely temporal coherence, coherence among the\npredictions on the face parts and coherence between appearance and shape-based\nrepresentation. Our experimental analysis shows that coherence constraints can\nimprove the quality of the expression recognizer, thus offering a suitable\nbasis to profitably exploit unsupervised video sequences. Finally we present\nsome examples with occlusions where the shape-based predictor performs better\nthan the appearance one.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 07:51:46 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Graziani", "Lisa", ""], ["Melacci", "Stefano", ""], ["Gori", "Marco", ""]]}, {"id": "1810.10327", "submitter": "Ha Young Kim", "authors": "Ba Rom Kang and Ha Young Kim", "title": "BshapeNet: Object Detection and Instance Segmentation with Bounding\n  Shape Masks", "comments": "10 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent object detectors use four-coordinate bounding box (bbox) regression to\npredict object locations. Providing additional information indicating the\nobject positions and coordinates will improve detection performance. Thus, we\npropose two types of masks: a bbox mask and a bounding shape (bshape) mask, to\nrepresent the object's bbox and boundary shape, respectively. For each of these\ntypes, we consider two variants: the Thick model and the Scored model, both of\nwhich have the same morphology but differ in ways to make their boundaries\nthicker. To evaluate the proposed masks, we design extended frameworks by\nadding a bshape mask (or a bbox mask) branch to a Faster R-CNN framework, and\ncall this BshapeNet (or BboxNet). Further, we propose BshapeNet+, a network\nthat combines a bshape mask branch with a Mask R-CNN to improve instance\nsegmentation as well as detection. Among our proposed models, BshapeNet+\ndemonstrates the best performance in both tasks and achieves highly competitive\nresults with state of the art (SOTA) models. Particularly, it improves the\ndetection results over Faster R-CNN+RoIAlign (37.3% and 28.9%) with a detection\nAP of 42.4% and 32.3% on MS COCO test-dev and Cityscapes val, respectively.\nFurthermore, for small objects, it achieves 24.9% AP on COCO test-dev, a\nsignificant improvement over previous SOTA models. For instance segmentation,\nit is substantially superior to Mask R-CNN on both test datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 10:12:45 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 19:04:05 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 11:19:23 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kang", "Ba Rom", ""], ["Kim", "Ha Young", ""]]}, {"id": "1810.10329", "submitter": "Nick Pears", "authors": "Rohan Watkins, Nick Pears and Suresh Manandhar", "title": "Vehicle classification using ResNets, localisation and\n  spatially-weighted pooling", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether ResNet architectures can outperform more traditional\nConvolutional Neural Networks on the task of fine-grained vehicle\nclassification. We train and test ResNet-18, ResNet-34 and ResNet-50 on the\nComprehensive Cars dataset without pre-training on other datasets. We then\nmodify the networks to use Spatially Weighted Pooling. Finally, we add a\nlocalisation step before the classification process, using a network based on\nResNet-50. We find that using Spatially Weighted Pooling and localisation both\nimprove classification accuracy of ResNet50. Spatially Weighted Pooling\nincreases accuracy by 1.5 percent points and localisation increases accuracy by\n3.4 percent points. Using both increases accuracy by 3.7 percent points giving\na top-1 accuracy of 96.351\\% on the Comprehensive Cars dataset. Our method\nachieves higher accuracy than a range of methods including those that use\ntraditional CNNs. However, our method does not perform quite as well as\npre-trained networks that use Spatially Weighted Pooling.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 13:28:19 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Watkins", "Rohan", ""], ["Pears", "Nick", ""], ["Manandhar", "Suresh", ""]]}, {"id": "1810.10330", "submitter": "Joao Reis", "authors": "Joao Reis and Gil Gon\\c{c}alves", "title": "Hyper-Process Model: A Zero-Shot Learning algorithm for Regression\n  Problems based on Shape Analysis", "comments": "36 pages, 4 figures, 2 tables, submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) can be defined by correctly solving a task where no\ntraining data is available, based on previous acquired knowledge from\ndifferent, but related tasks. So far, this area has mostly drawn the attention\nfrom computer vision community where a new unseen image needs to be correctly\nclassified, assuming the target class was not used in the training procedure.\nApart from image classification, only a couple of generic methods were proposed\nthat are applicable to both classification and regression. These learn the\nrelation among model coefficients so new ones can be predicted according to\nprovided conditions. So far, up to our knowledge, no methods exist that are\napplicable only to regression, and take advantage from such setting. Therefore,\nthe present work proposes a novel algorithm for regression problems that uses\ndata drawn from trained models, instead of model coefficients. In this case, a\nshape analyses on the data is performed to create a statistical shape model and\ngenerate new shapes to train new models. The proposed algorithm is tested in a\ntheoretical setting using the beta distribution where main problem to solve is\nto estimate a function that predicts curves, based on already learned\ndifferent, but related ones.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 11:35:16 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Reis", "Joao", ""], ["Gon\u00e7alves", "Gil", ""]]}, {"id": "1810.10331", "submitter": "Song Li", "authors": "Song Li, Geoffrey Kwok Fai Tso", "title": "Bottleneck Supervised U-Net for Pixel-wise Liver and Tumor Segmentation", "comments": "21 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a bottleneck supervised (BS) U-Net model for liver\nand tumor segmentation. Our main contributions are: first, we propose a\nvariation of the original U-Net that incorporates dense modules, inception\nmodules and dilated convolution in the encoding path; second, we propose a\nbottleneck supervised (BS) U-Net that contains an encoding U-Net and a\nsegmentation U-Net. To train the BS U-Net, the encoding U-Net is first trained\nto get encodings of the label maps that contain the anatomical information\n(shape and location). Subsequently, this information is used to guide the\ntraining of the segmentation U-Net so as to reserve the anatomical features of\nthe target objects. More specifically, the loss function for segmentation U-Net\nis set to be the weighted average of the dice loss and the MSE loss between the\nencodings and the bottleneck feature vectors. The model is applied to a public\nliver and tumor CT scan dataset. Experimental results show that besides\nachieving excellent overall segmentation performance, BS U-Net also works great\nin controlling shape distortion, reducing false positive and false negative\ncases.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:02:57 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 01:38:44 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Li", "Song", ""], ["Tso", "Geoffrey Kwok Fai", ""]]}, {"id": "1810.10333", "submitter": "Adityanarayanan Radhakrishnan", "authors": "Adityanarayanan Radhakrishnan, Karren Yang, Mikhail Belkin, Caroline\n  Uhler", "title": "Memorization in Overparameterized Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of deep neural networks to generalize well in the\noverparameterized regime has become a subject of significant research interest.\nWe show that overparameterized autoencoders exhibit memorization, a form of\ninductive bias that constrains the functions learned through the optimization\nprocess to concentrate around the training examples, although the network could\nin principle represent a much larger function class. In particular, we prove\nthat single-layer fully-connected autoencoders project data onto the\n(nonlinear) span of the training examples. In addition, we show that deep\nfully-connected autoencoders learn a map that is locally contractive at the\ntraining examples, and hence iterating the autoencoder results in convergence\nto the training examples. Finally, we prove that depth is necessary and provide\nempirical evidence that it is also sufficient for memorization in convolutional\nautoencoders. Understanding this inductive bias may shed light on the\ngeneralization properties of overparametrized deep neural networks that are\ncurrently unexplained by classical statistical theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 17:02:54 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 17:56:43 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 22:37:13 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Radhakrishnan", "Adityanarayanan", ""], ["Yang", "Karren", ""], ["Belkin", "Mikhail", ""], ["Uhler", "Caroline", ""]]}, {"id": "1810.10337", "submitter": "Robert Jasper", "authors": "Nicole Nichols and Robert Jasper", "title": "Projecting Trouble: Light Based Adversarial Attacks on Deep Learning\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work demonstrates a physical attack on a deep learning image\nclassification system using projected light onto a physical scene. Prior work\nis dominated by techniques for creating adversarial examples which directly\nmanipulate the digital input of the classifier. Such an attack is limited to\nscenarios where the adversary can directly update the inputs to the classifier.\nThis could happen by intercepting and modifying the inputs to an online API\nsuch as Clarifai or Cloud Vision. Such limitations have led to a vein of\nresearch around physical attacks where objects are constructed to be inherently\nadversarial or adversarial modifications are added to cause misclassification.\nOur work differs from other physical attacks in that we can cause\nmisclassification dynamically without altering physical objects in a permanent\nway.\n  We construct an experimental setup which includes a light projection source,\nan object for classification, and a camera to capture the scene. Experiments\nare conducted against 2D and 3D objects from CIFAR-10. Initial tests show\nprojected light patterns selected via differential evolution could degrade\nclassification from 98% to 22% and 89% to 43% probability for 2D and 3D targets\nrespectively. Subsequent experiments explore sensitivity to physical setup and\ncompare two additional baseline conditions for all 10 CIFAR classes. Some\nphysical targets are more susceptible to perturbation. Simple attacks show near\nequivalent success, and 6 of the 10 classes were disrupted by light.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 17:47:07 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Nichols", "Nicole", ""], ["Jasper", "Robert", ""]]}, {"id": "1810.10338", "submitter": "Thomas Lampert", "authors": "Thomas Lampert, Odyss\\'ee Merveille, Jessica Schmitz, Germain\n  Forestier, Friedrich Feuerhake, C\\'edric Wemmert", "title": "Strategies for Training Stain Invariant CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of Digital Pathology is the analysis of multiple digitised\nwhole slide images from differently stained tissue sections. It is common\npractice to mount consecutive sections containing corresponding microscopic\nstructures on glass slides, and to stain them differently to highlight specific\ntissue components. These multiple staining modalities result in very different\nimages but include a significant amount of consistent image information. Deep\nlearning approaches have recently been proposed to analyse these images in\norder to automatically identify objects of interest for pathologists. These\nsupervised approaches require a vast amount of annotations, which are difficult\nand expensive to acquire---a problem that is multiplied with multiple\nstainings. This article presents several training strategies that make progress\ntowards stain invariant networks. By training the network on one commonly used\nstaining modality and applying it to images that include corresponding but\ndifferently stained tissue structures, the presented unsupervised strategies\ndemonstrate significant improvements over standard training strategies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 09:41:23 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 23:13:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lampert", "Thomas", ""], ["Merveille", "Odyss\u00e9e", ""], ["Schmitz", "Jessica", ""], ["Forestier", "Germain", ""], ["Feuerhake", "Friedrich", ""], ["Wemmert", "C\u00e9dric", ""]]}, {"id": "1810.10339", "submitter": "Hamid Behjat", "authors": "Sevil Maghsadhagh, Mousa Shamsi, Anders Eklund, Hamid Behjat", "title": "Characterization of Brain Cortical Morphology Using Localized\n  Topology-Encoding Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain cortical layer has a convoluted morphology that is unique to\neach individual. Characterization of the cortical morphology is necessary in\nlongitudinal studies of structural brain change, as well as in discriminating\nindividuals in health and disease. A method for encoding the cortical\nmorphology in the form of a graph is presented. The design of graphs that\nencode the global cerebral hemisphere cortices as well as localized cortical\nregions is proposed. Spectral features of these graphs are then studied and\nproposed as descriptors of cortical morphology. As proof-of-concept of their\napplicability in characterizing cortical morphology, the descriptors are\nstudied in the context of discriminating individuals based on their sex.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 11:45:09 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Maghsadhagh", "Sevil", ""], ["Shamsi", "Mousa", ""], ["Eklund", "Anders", ""], ["Behjat", "Hamid", ""]]}, {"id": "1810.10340", "submitter": "Sjoerd van Steenkiste", "authors": "Sjoerd van Steenkiste, Karol Kurach, J\\\"urgen Schmidhuber, Sylvain\n  Gelly", "title": "Investigating Object Compositionality in Generative Adversarial Networks", "comments": "A preliminary version of this work (arXiv v1) appeared under the\n  title \"A Case for Object Compositionality in Deep Generative Models of\n  Images\" as a workshop paper at the NeurIPS2018 workshop on \"Modeling the\n  Physical World: Perception, Learning, and Control\", and at the NeurIPS2018\n  workshop on \"Relational Representation Learning\"", "journal-ref": null, "doi": "10.1016/j.neunet.2020.07.007", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models seek to recover the process with which the observed\ndata was generated. They may be used to synthesize new samples or to\nsubsequently extract representations. Successful approaches in the domain of\nimages are driven by several core inductive biases. However, a bias to account\nfor the compositional way in which humans structure a visual scene in terms of\nobjects has frequently been overlooked. In this work, we investigate object\ncompositionality as an inductive bias for Generative Adversarial Networks\n(GANs). We present a minimal modification of a standard generator to\nincorporate this inductive bias and find that it reliably learns to generate\nimages as compositions of objects. Using this general design as a backbone, we\nthen propose two useful extensions to incorporate dependencies among objects\nand background. We extensively evaluate our approach on several multi-object\nimage datasets and highlight the merits of incorporating structure for\nrepresentation learning purposes. In particular, we find that our structured\nGANs are better at generating multi-object images that are more faithful to the\nreference distribution. More so, we demonstrate how, by leveraging the\nstructure of the learned generative process, one can `invert' the learned\ngenerative model to perform unsupervised instance segmentation. On the\nchallenging CLEVR dataset, it is shown how our approach is able to improve over\nother recent purely unsupervised object-centric approaches to image generation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 20:17:11 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 10:38:57 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:10:53 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["van Steenkiste", "Sjoerd", ""], ["Kurach", "Karol", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1810.10341", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "Visions of a generalized probability theory", "comments": null, "journal-ref": "Lambert Academic Publishing, Sep 24 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Book we argue that the fruitful interaction of computer vision and\nbelief calculus is capable of stimulating significant advances in both fields.\nFrom a methodological point of view, novel theoretical results concerning the\ngeometric and algebraic properties of belief functions as mathematical objects\nare illustrated and discussed in Part II, with a focus on both a perspective\n'geometric approach' to uncertainty and an algebraic solution to the issue of\nconflicting evidence. In Part III we show how these theoretical developments\narise from important computer vision problems (such as articulated object\ntracking, data association and object pose estimation) to which, in turn, the\nevidential formalism is able to provide interesting new solutions. Finally,\nsome initial steps towards a generalization of the notion of total probability\nto belief functions are taken, in the perspective of endowing the theory of\nevidence with a complete battery of estimation and inference tools to the\nbenefit of all scientists and practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 14:00:48 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "1810.10342", "submitter": "Lily Peng", "authors": "Avinash Varadarajan, Pinal Bavishi, Paisan Raumviboonsuk, Peranut\n  Chotcomwongse, Subhashini Venugopalan, Arunachalam Narayanaswamy, Jorge\n  Cuadros, Kuniyoshi Kanai, George Bresnick, Mongkol Tadarati, Sukhum\n  Silpa-archa, Jirawut Limwattanayingyong, Variya Nganthavee, Joe Ledsam,\n  Pearse A Keane, Greg S Corrado, Lily Peng, Dale R Webster", "title": "Predicting optical coherence tomography-derived diabetic macular edema\n  grades from fundus photographs using deep learning", "comments": null, "journal-ref": "Nature Communications (2020)", "doi": "10.1038/s41467-019-13922-8", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic eye disease is one of the fastest growing causes of preventable\nblindness. With the advent of anti-VEGF (vascular endothelial growth factor)\ntherapies, it has become increasingly important to detect center-involved\ndiabetic macular edema (ci-DME). However, center-involved diabetic macular\nedema is diagnosed using optical coherence tomography (OCT), which is not\ngenerally available at screening sites because of cost and workflow\nconstraints. Instead, screening programs rely on the detection of hard exudates\nin color fundus photographs as a proxy for DME, often resulting in high false\npositive or false negative calls. To improve the accuracy of DME screening, we\ntrained a deep learning model to use color fundus photographs to predict\nci-DME. Our model had an ROC-AUC of 0.89 (95% CI: 0.87-0.91), which corresponds\nto a sensitivity of 85% at a specificity of 80%. In comparison, three retinal\nspecialists had similar sensitivities (82-85%), but only half the specificity\n(45-50%, p<0.001 for each comparison with model). The positive predictive value\n(PPV) of the model was 61% (95% CI: 56-66%), approximately double the 36-38% by\nthe retinal specialists. In addition to predicting ci-DME, our model was able\nto detect the presence of intraretinal fluid with an AUC of 0.81 (95% CI:\n0.81-0.86) and subretinal fluid with an AUC of 0.88 (95% CI: 0.85-0.91). The\nability of deep learning algorithms to make clinically relevant predictions\nthat generally require sophisticated 3D-imaging equipment from simple 2D images\nhas broad relevance to many other applications in medical imaging.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 23:22:33 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 01:46:59 GMT"}, {"version": "v3", "created": "Sat, 9 Feb 2019 17:50:50 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 23:39:59 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Varadarajan", "Avinash", ""], ["Bavishi", "Pinal", ""], ["Raumviboonsuk", "Paisan", ""], ["Chotcomwongse", "Peranut", ""], ["Venugopalan", "Subhashini", ""], ["Narayanaswamy", "Arunachalam", ""], ["Cuadros", "Jorge", ""], ["Kanai", "Kuniyoshi", ""], ["Bresnick", "George", ""], ["Tadarati", "Mongkol", ""], ["Silpa-archa", "Sukhum", ""], ["Limwattanayingyong", "Jirawut", ""], ["Nganthavee", "Variya", ""], ["Ledsam", "Joe", ""], ["Keane", "Pearse A", ""], ["Corrado", "Greg S", ""], ["Peng", "Lily", ""], ["Webster", "Dale R", ""]]}, {"id": "1810.10343", "submitter": "Felipe Medeiros", "authors": "Felipe A. Medeiros, Alessandro A. Jammal, Atalie C. Thompson", "title": "From Machine to Machine: An OCT-trained Deep Learning Algorithm for\n  Objective Quantification of Glaucomatous Damage in Fundus Photographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches using deep learning algorithms to classify glaucomatous\ndamage on fundus photographs have been limited by the requirement for human\nlabeling of a reference training set. We propose a new approach using\nspectral-domain optical coherence tomography (SDOCT) data to train a deep\nlearning algorithm to quantify glaucomatous structural damage on optic disc\nphotographs. The dataset included 32,820 pairs of optic disc photos and SDOCT\nretinal nerve fiber layer (RNFL) scans from 2,312 eyes of 1,198 subjects. A\ndeep learning convolutional neural network was trained to assess optic disc\nphotographs and predict SDOCT average RNFL thickness. The performance of the\nalgorithm was evaluated in an independent test sample. The mean prediction of\naverage RNFL thickness from all 6,292 optic disc photos in the test set was\n83.3$\\pm$14.5 $\\mu$m, whereas the mean average RNFL thickness from all\ncorresponding SDOCT scans was 82.5$\\pm$16.8 $\\mu$m (P = 0.164). There was a\nvery strong correlation between predicted and observed RNFL thickness values (r\n= 0.832; P<0.001), with mean absolute error of the predictions of 7.39 $\\mu$m.\nThe areas under the receiver operating characteristic curves for discriminating\nglaucoma from healthy eyes with the deep learning predictions and actual SDOCT\nmeasurements were 0.944 (95$\\%$ CI: 0.912- 0.966) and 0.940 (95$\\%$ CI: 0.902 -\n0.966), respectively (P = 0.724). In conclusion, we introduced a novel deep\nlearning approach to assess optic disc photographs and provide quantitative\ninformation about the amount of neural damage. This approach could potentially\nbe used to diagnose and stage glaucomatous damage from optic disc photographs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 05:03:42 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Medeiros", "Felipe A.", ""], ["Jammal", "Alessandro A.", ""], ["Thompson", "Atalie C.", ""]]}, {"id": "1810.10346", "submitter": "Weiwen Wu", "authors": "Weiwen Wu, Qian Wang, Fenglin Liu, Yining Zhu, and Hengyong Yu", "title": "Block Matching Frame based Material Reconstruction for Spectral CT", "comments": "More details can refer to\n  https://iopscience.iop.org/article/10.1088/1361-6560/ab51db/pdf", "journal-ref": "Physics in Medicine & Biology,2019", "doi": "10.1088/1361-6560/ab51db", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral computed tomography (CT) has a great potential in material\nidentification and decomposition. To achieve high-quality material composition\nimages and further suppress the x-ray beam hardening artifacts, we first\npropose a one-step material reconstruction model based on Taylor first-order\nexpansion. Then, we develop a basic material reconstruction method named\nmaterial simultaneous algebraic reconstruction technique (MSART). Considering\nthe local similarity of each material image, we incorporate a powerful block\nmatching frame (BMF) into the material reconstruction (MR) model and generate a\nBMF based MR (BMFMR) method. Because the BMFMR model contains the L0-norm\nproblem, we adopt a split-Bregman method for optimization. The numerical\nsimulation and physical phantom experiment results validate the correctness of\nthe material reconstruction algorithms and demonstrate that the BMF\nregularization outperforms the total variation and no-local mean\nregularizations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 02:05:09 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 11:01:23 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Wu", "Weiwen", ""], ["Wang", "Qian", ""], ["Liu", "Fenglin", ""], ["Zhu", "Yining", ""], ["Yu", "Hengyong", ""]]}, {"id": "1810.10348", "submitter": "Amirreza Rezvantalab", "authors": "Amirreza Rezvantalab, Habib Safigholi, Somayeh Karimijeshni", "title": "Dermatologist Level Dermoscopy Skin Cancer Classification Using\n  Different Deep Learning Convolutional Neural Networks Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the effectiveness and capability of convolutional neural\nnetworks have been studied in the classification of 8 skin diseases. Different\npre-trained state-of-the-art architectures (DenseNet 201, ResNet 152, Inception\nv3, InceptionResNet v2) were used and applied on 10135 dermoscopy skin images\nin total (HAM10000: 10015, PH2: 120). The utilized dataset includes 8\ndiagnostic categories - melanoma, melanocytic nevi, basal cell carcinoma,\nbenign keratosis, actinic keratosis and intraepithelial carcinoma,\ndermatofibroma, vascular lesions, and atypical nevi. The aim is to compare the\nability of deep learning with the performance of highly trained dermatologists.\nOverall, the mean results show that all deep learning models outperformed\ndermatologists (at least 11%). The best ROC AUC values for melanoma and basal\ncell carcinoma are 94.40% (ResNet 152) and 99.30% (DenseNet 201) versus 82.26%\nand 88.82% of dermatologists, respectively. Also, DenseNet 201 had the highest\nmacro and micro averaged AUC values for overall classification (98.16%, 98.79%,\nrespectively).\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 23:27:59 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Rezvantalab", "Amirreza", ""], ["Safigholi", "Habib", ""], ["Karimijeshni", "Somayeh", ""]]}, {"id": "1810.10350", "submitter": "Michelle Kuchera", "authors": "Michelle P. Kuchera, Raghuram Ramanujan, Jack Z. Taylor, Ryan R.\n  Strauss, Daniel Bazin, Joshua Bradt, Ruiming Chen", "title": "Machine Learning Methods for Track Classification in the AT-TPC", "comments": null, "journal-ref": "NIMA 940 (2019) 56-167, ISSN 0168-9002", "doi": "10.1016/j.nima.2019.05.097", "report-no": "NIMA-D-18-01137", "categories": "cs.CV cs.LG nucl-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate machine learning methods for event classification in the\nActive-Target Time Projection Chamber detector at the National Superconducting\nCyclotron Laboratory (NSCL) at Michigan State University. An automated method\nto single out the desired reaction product would result in more accurate\nphysics results as well as a faster analysis process. Binary and multi-class\nclassification methods were tested on data produced by the $^{46}$Ar(p,p)\nexperiment run at the NSCL in September 2015. We found a Convolutional Neural\nNetwork to be the most successful classifier of proton scattering events for\ntransfer learning. Results from this investigation and recommendations for\nevent classification in future experiments are presented.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:52:01 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 01:57:47 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 01:54:11 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Kuchera", "Michelle P.", ""], ["Ramanujan", "Raghuram", ""], ["Taylor", "Jack Z.", ""], ["Strauss", "Ryan R.", ""], ["Bazin", "Daniel", ""], ["Bradt", "Joshua", ""], ["Chen", "Ruiming", ""]]}, {"id": "1810.10351", "submitter": "Hsin-Pai Cheng", "authors": "Hsin-Pai Cheng, Yuanjun Huang, Xuyang Guo, Yifei Huang, Feng Yan, Hai\n  Li, Yiran Chen", "title": "Differentiable Fine-grained Quantization for Deep Neural Network\n  Compression", "comments": "Hsin-Pai Cheng, Yuanjun Huang and Xuyang Guo contributed equally and\n  are co-first authors for this paper. This work has been accepted by NIPS 2018\n  Workshop on Compact Deep Neural Network Representation with Industrial\n  Applications, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks have shown great performance in cognitive tasks. When\ndeploying network models on mobile devices with limited resources, weight\nquantization has been widely adopted. Binary quantization obtains the highest\ncompression but usually results in big accuracy drop. In practice, 8-bit or\n16-bit quantization is often used aiming at maintaining the same accuracy as\nthe original 32-bit precision. We observe different layers have different\naccuracy sensitivity of quantization. Thus judiciously selecting different\nprecision for different layers/structures can potentially produce more\nefficient models compared to traditional quantization methods by striking a\nbetter balance between accuracy and compression rate. In this work, we propose\na fine-grained quantization approach for deep neural network compression by\nrelaxing the search space of quantization bitwidth from discrete to a\ncontinuous domain. The proposed approach applies gradient descend based\noptimization to generate a mixed-precision quantization scheme that outperforms\nthe accuracy of traditional quantization methods under the same compression\nrate.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 21:48:03 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 08:42:06 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 04:10:24 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Cheng", "Hsin-Pai", ""], ["Huang", "Yuanjun", ""], ["Guo", "Xuyang", ""], ["Huang", "Yifei", ""], ["Yan", "Feng", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1810.10352", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Konstantinos Kamnitsas and Christian Ledig and\n  Ivana I\\v{s}gum", "title": "Generative adversarial networks and adversarial methods in biomedical\n  image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) and other adversarial methods are\nbased on a game-theoretical perspective on joint optimization of two neural\nnetworks as players in a game. Adversarial techniques have been extensively\nused to synthesize and analyze biomedical images. We provide an introduction to\nGANs and adversarial methods, with an overview of biomedical image analysis\ntasks that have benefited from such methods. We conclude with a discussion of\nstrengths and limitations of adversarial methods in biomedical image analysis,\nand propose potential future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 12:36:11 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["Kamnitsas", "Konstantinos", ""], ["Ledig", "Christian", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1810.10353", "submitter": "Mengying Lei", "authors": "Yang Li, Mengying Lei, Xianrui Zhang, Weigang Cui, Yuzhu Guo, Ting-Wen\n  Huang, and Hua-Liang Wei", "title": "Boosted Convolutional Neural Networks for Motor Imagery EEG Decoding\n  with Multiwavelet-based Time-Frequency Conditional Granger Causality Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding EEG signals of different mental states is a challenging task for\nbrain-computer interfaces (BCIs) due to nonstationarity of perceptual decision\nprocesses. This paper presents a novel boosted convolutional neural networks\n(ConvNets) decoding scheme for motor imagery (MI) EEG signals assisted by the\nmultiwavelet-based time-frequency (TF) causality analysis. Specifically,\nmultiwavelet basis functions are first combined with Geweke spectral measure to\nobtain high-resolution TF-conditional Granger causality (CGC) representations,\nwhere a regularized orthogonal forward regression (ROFR) algorithm is adopted\nto detect a parsimonious model with good generalization performance. The\ncausality images for network input preserving time, frequency and location\ninformation of connectivity are then designed based on the TF-CGC distributions\nof alpha band multichannel EEG signals. Further constructed boosted ConvNets by\nusing spatio-temporal convolutions as well as advances in deep learning\nincluding cropping and boosting methods, to extract discriminative causality\nfeatures and classify MI tasks. Our proposed approach outperforms the\ncompetition winner algorithm with 12.15% increase in average accuracy and\n74.02% decrease in associated inter subject standard deviation for the same\nbinary classification on BCI competition-IV dataset-IIa. Experiment results\nindicate that the boosted ConvNets with causality images works well in decoding\nMI-EEG signals and provides a promising framework for developing MI-BCI\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:39:12 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Li", "Yang", ""], ["Lei", "Mengying", ""], ["Zhang", "Xianrui", ""], ["Cui", "Weigang", ""], ["Guo", "Yuzhu", ""], ["Huang", "Ting-Wen", ""], ["Wei", "Hua-Liang", ""]]}, {"id": "1810.10358", "submitter": "Lin Zhang", "authors": "Lin Zhang, Valery Vishnevskiy, Andras Jakab, Orcun Goksel", "title": "Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent\n  Motion Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in\nvivo perfusion quantification with magnetic resonance imaging (MRI). However,\nits use is limited by typically low accuracy due to low signal-to-noise ratio\n(SNR) at large gradient encoding magnitudes as well as dephasing artefacts\ncaused by subject motion, which is particularly challenging in fetal MRI. To\nmitigate this problem, we propose an implicit IVIM signal acquisition model\nwith which we learn full posterior distribution of perfusion parameters using\nartificial neural networks. This posterior then encapsulates the uncertainty of\nthe inferred parameter estimates, which we validate herein via numerical\nexperiments with rejection-based Bayesian sampling. Compared to\nstate-of-the-art IVIM estimation method of segmented least-squares fitting, our\nproposed approach improves parameter estimation accuracy by 65% on synthetic\nanisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method\nincreases repeatability of parameter estimation in placenta by 46%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:22:10 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhang", "Lin", ""], ["Vishnevskiy", "Valery", ""], ["Jakab", "Andras", ""], ["Goksel", "Orcun", ""]]}, {"id": "1810.10395", "submitter": "Gerasimos Spanakis", "authors": "Ajkel Mino, Gerasimos Spanakis", "title": "LoGAN: Generating Logos with a Generative Adversarial Neural Network\n  Conditioned on color", "comments": "6 page, ICMLA18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a logo is a long, complicated, and expensive process for any\ndesigner. However, recent advancements in generative algorithms provide models\nthat could offer a possible solution. Logos are multi-modal, have very few\ncategorical properties, and do not have a continuous latent space. Yet,\nconditional generative adversarial networks can be used to generate logos that\ncould help designers in their creative process. We propose LoGAN: an improved\nauxiliary classifier Wasserstein generative adversarial neural network (with\ngradient penalty) that is able to generate logos conditioned on twelve\ndifferent colors. In 768 generated instances (12 classes and 64 logos per\nclass), when looking at the most prominent color, the conditional generation\npart of the model has an overall precision and recall of 0.8 and 0.7\nrespectively. LoGAN's results offer a first glance at how artificial\nintelligence can be used to assist designers in their creative process and open\npromising future directions, such as including more descriptive labels which\nwill provide a more exhaustive and easy-to-use system.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 10:22:17 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Mino", "Ajkel", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1810.10438", "submitter": "Michael Ying Yang", "authors": "Ye Lyu, George Vosselman, Guisong Xia, Alper Yilmaz, Michael Ying Yang", "title": "UAVid: A Semantic Segmentation Dataset for UAV Imagery", "comments": "Accepted by ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has been one of the leading research interests in\ncomputer vision recently. It serves as a perception foundation for many fields,\nsuch as robotics and autonomous driving. The fast development of semantic\nsegmentation attributes enormously to the large scale datasets, especially for\nthe deep learning related methods. There already exist several semantic\nsegmentation datasets for comparison among semantic segmentation methods in\ncomplex urban scenes, such as the Cityscapes and CamVid datasets, where the\nside views of the objects are captured with a camera mounted on the driving\ncar. There also exist semantic labeling datasets for the airborne images and\nthe satellite images, where the top views of the objects are captured. However,\nonly a few datasets capture urban scenes from an oblique Unmanned Aerial\nVehicle (UAV) perspective, where both of the top view and the side view of the\nobjects can be observed, providing more information for object recognition. In\nthis paper, we introduce our UAVid dataset, a new high-resolution UAV semantic\nsegmentation dataset as a complement, which brings new challenges, including\nlarge scale variation, moving object recognition and temporal consistency\npreservation. Our UAV dataset consists of 30 video sequences capturing 4K\nhigh-resolution images in slanted views. In total, 300 images have been densely\nlabeled with 8 classes for the semantic labeling task. We have provided several\ndeep learning baseline methods with pre-training, among which the proposed\nMulti-Scale-Dilation net performs the best via multi-scale feature extraction.\nOur UAVid website and the labeling tool have been published https://uavid.nl/.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:08:11 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 10:20:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Lyu", "Ye", ""], ["Vosselman", "George", ""], ["Xia", "Guisong", ""], ["Yilmaz", "Alper", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1810.10510", "submitter": "Ignacio Rocco", "authors": "Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi\\'c, Akihiko Torii,\n  Tomas Pajdla, Josef Sivic", "title": "Neighbourhood Consensus Networks", "comments": "In Proceedings of the 32nd Conference on Neural Information\n  Processing Systems (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding reliable dense correspondences between a\npair of images. This is a challenging task due to strong appearance differences\nbetween the corresponding scene elements and ambiguities generated by\nrepetitive patterns. The contributions of this work are threefold. First,\ninspired by the classic idea of disambiguating feature matches using semi-local\nconstraints, we develop an end-to-end trainable convolutional neural network\narchitecture that identifies sets of spatially consistent matches by analyzing\nneighbourhood consensus patterns in the 4D space of all possible\ncorrespondences between a pair of images without the need for a global\ngeometric model. Second, we demonstrate that the model can be trained\neffectively from weak supervision in the form of matching and non-matching\nimage pairs without the need for costly manual annotation of point to point\ncorrespondences. Third, we show the proposed neighbourhood consensus network\ncan be applied to a range of matching tasks including both category- and\ninstance-level matching, obtaining the state-of-the-art results on the PF\nPascal dataset and the InLoc indoor visual localization benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 17:45:17 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 07:35:45 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Rocco", "Ignacio", ""], ["Cimpoi", "Mircea", ""], ["Arandjelovi\u0107", "Relja", ""], ["Torii", "Akihiko", ""], ["Pajdla", "Tomas", ""], ["Sivic", "Josef", ""]]}, {"id": "1810.10519", "submitter": "Murilo Varges Da Silva", "authors": "Murilo Varges da Silva and Aparecido Nilceu Marana", "title": "Spatiotemporal CNNs for Pornography Detection in Videos", "comments": null, "journal-ref": "Proceedings of 23rd Iberoamerican Congress on Pattern Recognition\n  (CIARP) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing use of social networks and mobile devices, the number of\nvideos posted on the Internet is growing exponentially. Among the inappropriate\ncontents published on the Internet, pornography is one of the most worrying as\nit can be accessed by teens and children. Two spatiotemporal CNNs, VGG-C3D CNN\nand ResNet R(2+1)D CNN, were assessed for pornography detection in videos in\nthe present study. Experimental results using the Pornography-800 dataset\nshowed that these spatiotemporal CNNs performed better than some\nstate-of-the-art methods based on bag of visual words and are competitive with\nother CNN-based approaches, reaching accuracy of 95.1%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 17:55:22 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["da Silva", "Murilo Varges", ""], ["Marana", "Aparecido Nilceu", ""]]}, {"id": "1810.10529", "submitter": "Ivona Tautkute", "authors": "Ivona Tautkute and Tomasz Trzcinski", "title": "Classifying and Visualizing Emotions with Emotional DAN", "comments": "Under review at Special Issue on Deep Neural Networks for Digital\n  Media Algorithms, Fundamenta Informaticae. arXiv admin note: text overlap\n  with arXiv:1805.00326", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of human emotions remains an important and challenging task\nfor many computer vision algorithms, especially in the era of humanoid robots\nwhich coexist with humans in their everyday life. Currently proposed methods\nfor emotion recognition solve this task using multi-layered convolutional\nnetworks that do not explicitly infer any facial features in the classification\nphase. In this work, we postulate a fundamentally different approach to solve\nemotion recognition task that relies on incorporating facial landmarks as a\npart of the classification loss function. To that end, we extend a recently\nproposed Deep Alignment Network (DAN) with a term related to facial features.\nThanks to this simple modification, our model called EmotionalDAN is able to\noutperform state-of-the-art emotion classification methods on two challenging\nbenchmark dataset by up to 5%. Furthermore, we visualize image regions analyzed\nby the network when making a decision and the results indicate that our\nEmotionalDAN model is able to correctly identify facial landmarks responsible\nfor expressing the emotions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:24:46 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Tautkute", "Ivona", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1810.10551", "submitter": "V\\'it R\\r{u}\\v{z}i\\v{c}ka", "authors": "V\\'it R\\r{u}\\v{z}i\\v{c}ka and Franz Franchetti", "title": "Fast and accurate object detection in high resolution 4K and 8K video\n  using GPUs", "comments": "6 pages, 12 figures, Best Paper Finalist at IEEE High Performance\n  Extreme Computing Conference (HPEC) 2018; copyright 2018 IEEE; (DOI will be\n  filled when known)", "journal-ref": "2018 IEEE High Performance extreme Computing Conference (HPEC)", "doi": "10.1109/HPEC.2018.8547574", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has celebrated a lot of achievements on computer vision\ntasks such as object detection, but the traditionally used models work with\nrelatively low resolution images. The resolution of recording devices is\ngradually increasing and there is a rising need for new methods of processing\nhigh resolution data. We propose an attention pipeline method which uses two\nstaged evaluation of each image or video frame under rough and refined\nresolution to limit the total number of necessary evaluations. For both stages,\nwe make use of the fast object detection model YOLO v2. We have implemented our\nmodel in code, which distributes the work across GPUs. We maintain high\naccuracy while reaching the average performance of 3-6 fps on 4K video and 2\nfps on 8K video.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:00:06 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["R\u016f\u017ei\u010dka", "V\u00edt", ""], ["Franchetti", "Franz", ""]]}, {"id": "1810.10565", "submitter": "Yulun Du", "authors": "Yulun Du, Chirag Raman, Alan W Black, Louis-Philippe Morency, Maxine\n  Eskenazi", "title": "Multimodal Polynomial Fusion for Detecting Driver Distraction", "comments": "INTERSPEECH 2018", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2011", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distracted driving is deadly, claiming 3,477 lives in the U.S. in 2015 alone.\nAlthough there has been a considerable amount of research on modeling the\ndistracted behavior of drivers under various conditions, accurate automatic\ndetection using multiple modalities and especially the contribution of using\nthe speech modality to improve accuracy has received little attention. This\npaper introduces a new multimodal dataset for distracted driving behavior and\ndiscusses automatic distraction detection using features from three modalities:\nfacial expression, speech and car signals. Detailed multimodal feature analysis\nshows that adding more modalities monotonically increases the predictive\naccuracy of the model. Finally, a simple and effective multimodal fusion\ntechnique using a polynomial fusion layer shows superior distraction detection\nresults compared to the baseline SVM and neural network models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:16:42 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Du", "Yulun", ""], ["Raman", "Chirag", ""], ["Black", "Alan W", ""], ["Morency", "Louis-Philippe", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1810.10581", "submitter": "Abhik Singla", "authors": "Abhik Singla, Partha Pratim Roy and Debi Prosad Dogra", "title": "Visual Rendering of Shapes on 2D Display Devices Guided by Hand Gestures", "comments": "Submitted to Elsevier Displays Journal, 32 pages, 18 figures, 7\n  tables", "journal-ref": null, "doi": "10.1016/j.displa.2019.03.001", "report-no": null, "categories": "cs.HC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing of touchless user interface is gaining popularity in various\ncontexts. Using such interfaces, users can interact with electronic devices\neven when the hands are dirty or non-conductive. Also, user with partial\nphysical disability can interact with electronic devices using such systems.\nResearch in this direction has got major boost because of the emergence of\nlow-cost sensors such as Leap Motion, Kinect or RealSense devices. In this\npaper, we propose a Leap Motion controller-based methodology to facilitate\nrendering of 2D and 3D shapes on display devices. The proposed method tracks\nfinger movements while users perform natural gestures within the field of view\nof the sensor. In the next phase, trajectories are analyzed to extract extended\nNpen++ features in 3D. These features represent finger movements during the\ngestures and they are fed to unidirectional left-to-right Hidden Markov Model\n(HMM) for training. A one-to-one mapping between gestures and shapes is\nproposed. Finally, shapes corresponding to these gestures are rendered over the\ndisplay using MuPad interface. We have created a dataset of 5400 samples\nrecorded by 10 volunteers. Our dataset contains 18 geometric and 18\nnon-geometric shapes such as \"circle\", \"rectangle\", \"flower\", \"cone\", \"sphere\"\netc. The proposed methodology achieves an accuracy of 92.87% when evaluated\nusing 5-fold cross validation method. Our experiments revel that the extended\n3D features perform better than existing 3D features in the context of shape\nrepresentation and classification. The method can be used for developing useful\nHCI applications for smart display devices.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 16:59:52 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Singla", "Abhik", ""], ["Roy", "Partha Pratim", ""], ["Dogra", "Debi Prosad", ""]]}, {"id": "1810.10597", "submitter": "Helen L Bear", "authors": "Jake Burton, David Frank, Madhi Saleh, Nassir Navab, Helen L. Bear", "title": "The speaker-independent lipreading play-off; a survey of lipreading\n  machines", "comments": "To appear at the third IEEE International Conference on Image\n  Processing, Applications and Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipreading is a difficult gesture classification task. One problem in\ncomputer lipreading is speaker-independence. Speaker-independence means to\nachieve the same accuracy on test speakers not included in the training set as\nspeakers within the training set. Current literature is limited on\nspeaker-independent lipreading, the few independent test speaker accuracy\nscores are usually aggregated within dependent test speaker accuracies for an\naveraged performance. This leads to unclear independent results. Here we\nundertake a systematic survey of experiments with the TCD-TIMIT dataset using\nboth conventional approaches and deep learning methods to provide a series of\nwholly speaker-independent benchmarks and show that the best\nspeaker-independent machine scores 69.58% accuracy with CNN features and an SVM\nclassifier. This is less than state of the art speaker-dependent lipreading\nmachines, but greater than previously reported in independence experiments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 20:06:19 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Burton", "Jake", ""], ["Frank", "David", ""], ["Saleh", "Madhi", ""], ["Navab", "Nassir", ""], ["Bear", "Helen L.", ""]]}, {"id": "1810.10656", "submitter": "Ben Zion Vatashsky", "authors": "Ben Zion Vatashsky and Shimon Ullman", "title": "Understand, Compose and Respond - Answering Visual Questions by a\n  Composition of Abstract Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image related question defines a specific visual task that is required in\norder to produce an appropriate answer. The answer may depend on a minor detail\nin the image and require complex reasoning and use of prior knowledge. When\nhumans perform this task, they are able to do it in a flexible and robust\nmanner, integrating modularly any novel visual capability with diverse options\nfor various elaborations of the task. In contrast, current approaches to solve\nthis problem by a machine are based on casting the problem as an end-to-end\nlearning problem, which lacks such abilities.\n  We present a different approach, inspired by the aforementioned human\ncapabilities. The approach is based on the compositional structure of the\nquestion. The underlying idea is that a question has an abstract representation\nbased on its structure, which is compositional in nature. The question can\nconsequently be answered by a composition of procedures corresponding to its\nsubstructures. The basic elements of the representation are logical patterns,\nwhich are put together to represent the question. These patterns include a\nparametric representation for object classes, properties and relations. Each\nbasic pattern is mapped into a basic procedure that includes meaningful visual\ntasks, and the patterns are composed to produce the overall answering\nprocedure.\n  The UnCoRd (Understand Compose and Respond) system, based on this approach,\nintegrates existing detection and classification schemes for a set of object\nclasses, properties and relations. These schemes are incorporated in a modular\nmanner, providing elaborated answers and corrections for negative answers. In\naddition, an external knowledge base is queried for required common-knowledge.\nWe performed a qualitative analysis of the system, which demonstrates its\nrepresentation capabilities and provide suggestions for future developments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 00:03:09 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Vatashsky", "Ben Zion", ""], ["Ullman", "Shimon", ""]]}, {"id": "1810.10658", "submitter": "Jianhui Chen Mr", "authors": "Jianhui Chen and James J. Little", "title": "Sports Camera Calibration via Synthetic Data", "comments": "6 + 1 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrating sports cameras is important for autonomous broadcasting and\nsports analysis. Here we propose a highly automatic method for calibrating\nsports cameras from a single image using synthetic data. First, we develop a\nnovel camera pose engine. The camera pose engine has only three significant\nfree parameters so that it can effectively generate a lot of camera poses and\ncorresponding edge (i.e, field marking) images. Then, we learn compact deep\nfeatures via a siamese network from paired edge image and camera pose and build\na feature-pose database. After that, we use a novel two-GAN (generative\nadversarial network) model to detect field markings in real images. Finally, we\nquery an initial camera pose from the feature-pose database and refine camera\nposes using truncated distance images. We evaluate our method on both synthetic\nand real data. Our method not only demonstrates the robustness on the synthetic\ndata but also achieves the state-of-the-art accuracy on a standard soccer\ndataset and very high performance on a volleyball dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 00:10:57 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Jianhui", ""], ["Little", "James J.", ""]]}, {"id": "1810.10665", "submitter": "Jason  Weston", "authors": "Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, Jason Weston", "title": "Engaging Image Captioning Via Personality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard image captioning tasks such as COCO and Flickr30k are factual,\nneutral in tone and (to a human) state the obvious (e.g., \"a man playing a\nguitar\"). While such tasks are useful to verify that a machine understands the\ncontent of an image, they are not engaging to humans as captions. With this in\nmind we define a new task, Personality-Captions, where the goal is to be as\nengaging to humans as possible by incorporating controllable style and\npersonality traits. We collect and release a large dataset of 201,858 of such\ncaptions conditioned over 215 possible traits. We build models that combine\nexisting work from (i) sentence representations (Mazare et al., 2018) with\nTransformers trained on 1.7 billion dialogue examples; and (ii) image\nrepresentations (Mahajan et al., 2018) with ResNets trained on 3.5 billion\nsocial media images. We obtain state-of-the-art performance on Flickr30k and\nCOCO, and strong performance on our new task. Finally, online evaluations\nvalidate that our task and models are engaging to humans, with our best model\nclose to human performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 00:46:16 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 16:53:33 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Shuster", "Kurt", ""], ["Humeau", "Samuel", ""], ["Hu", "Hexiang", ""], ["Bordes", "Antoine", ""], ["Weston", "Jason", ""]]}, {"id": "1810.10687", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong and Tao Li and Wenxue Liu and Yang Chen", "title": "Structure Learning of Deep Networks via DNA Computing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) has gained state-of-the-art results in\nmany pattern recognition and computer vision tasks. However, most of the CNN\nstructures are manually designed by experienced researchers. Therefore, auto-\nmatically building high performance networks becomes an important problem. In\nthis paper, we introduce the idea of using DNA computing algorithm to\nautomatically learn high-performance architectures. In DNA computing algorithm,\nwe use short DNA strands to represent layers and long DNA strands to represent\noverall networks. We found that most of the learned models perform similarly,\nand only those performing worse during the first runs of training will perform\nworse finally than others. The indicates that: 1) Using DNA computing algorithm\nto learn deep architectures is feasible; 2) Local minima should not be a\nproblem of deep networks; 3) We can use early stop to kill the models with the\nbad performance just after several runs of training. In our experiments, an\naccuracy 99.73% was obtained on the MNIST data set and an accuracy 95.10% was\nobtained on the CIFAR-10 data set.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 02:06:24 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Li", "Tao", ""], ["Liu", "Wenxue", ""], ["Chen", "Yang", ""]]}, {"id": "1810.10703", "submitter": "Pramod Kaushik Mudrakarta", "authors": "Pramod Kaushik Mudrakarta, Mark Sandler, Andrey Zhmoginov, Andrew\n  Howard", "title": "K for the Price of 1: Parameter-efficient Multi-task and Transfer\n  Learning", "comments": "published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method that enables parameter-efficient transfer and\nmulti-task learning with deep neural networks. The basic approach is to learn a\nmodel patch - a small set of parameters - that will specialize to each task,\ninstead of fine-tuning the last layer or the entire network. For instance, we\nshow that learning a set of scales and biases is sufficient to convert a\npretrained network to perform well on qualitatively different problems (e.g.\nconverting a Single Shot MultiBox Detection (SSD) model into a 1000-class image\nclassification model while reusing 98% of parameters of the SSD feature\nextractor). Similarly, we show that re-learning existing low-parameter layers\n(such as depth-wise convolutions) while keeping the rest of the network frozen\nalso improves transfer-learning accuracy significantly. Our approach allows\nboth simultaneous (multi-task) as well as sequential transfer learning. In\nseveral multi-task learning problems, despite using much fewer parameters than\ntraditional logits-only fine-tuning, we match single-task performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 03:12:37 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 02:03:00 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Mudrakarta", "Pramod Kaushik", ""], ["Sandler", "Mark", ""], ["Zhmoginov", "Andrey", ""], ["Howard", "Andrew", ""]]}, {"id": "1810.10725", "submitter": "Mahdi S. Hosseini Dr.", "authors": "Mahdi S. Hosseini and Konstantinos N. Plataniotis", "title": "Convolutional Deblurring for Natural Imaging", "comments": "15 pages, for publication in IEEE Transaction Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel design of image deblurring in the form of\none-shot convolution filtering that can directly convolve with naturally\nblurred images for restoration. The problem of optical blurring is a common\ndisadvantage to many imaging applications that suffer from optical\nimperfections. Despite numerous deconvolution methods that blindly estimate\nblurring in either inclusive or exclusive forms, they are practically\nchallenging due to high computational cost and low image reconstruction\nquality. Both conditions of high accuracy and high speed are prerequisites for\nhigh-throughput imaging platforms in digital archiving. In such platforms,\ndeblurring is required after image acquisition before being stored, previewed,\nor processed for high-level interpretation. Therefore, on-the-fly correction of\nsuch images is important to avoid possible time delays, mitigate computational\nexpenses, and increase image perception quality. We bridge this gap by\nsynthesizing a deconvolution kernel as a linear combination of Finite Impulse\nResponse (FIR) even-derivative filters that can be directly convolved with\nblurry input images to boost the frequency fall-off of the Point Spread\nFunction (PSF) associated with the optical blur. We employ a Gaussian low-pass\nfilter to decouple the image denoising problem for image edge deblurring.\nFurthermore, we propose a blind approach to estimate the PSF statistics for two\nGaussian and Laplacian models that are common in many imaging pipelines.\nThorough experiments are designed to test and validate the efficiency of the\nproposed method using 2054 naturally blurred images across six imaging\napplications and seven state-of-the-art deconvolution methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 05:33:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 16:31:53 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Hosseini", "Mahdi S.", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "1810.10786", "submitter": "Stefan Engblom", "authors": "Jing Liu and Gijs van der Schot and Stefan Engblom", "title": "Supervised Classification Methods for Flash X-ray single particle\n  diffraction Imaging", "comments": null, "journal-ref": null, "doi": "10.1364/OE.27.003884", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Flash X-ray single-particle diffraction Imaging (FXI) experiments,\nwhich operate on modern X-ray Free Electron Lasers (XFELs), can record millions\nof interpretable diffraction patterns from individual biomolecules per day. Due\nto the stochastic nature of the XFELs, those patterns will to a varying degree\ninclude scatterings from contaminated samples. Also, the heterogeneity of the\nsample biomolecules is unavoidable and complicates data processing. Reducing\nthe data volumes and selecting high-quality single-molecule patterns are\ntherefore critical steps in the experimental set-up.\n  In this paper, we present two supervised template-based learning methods for\nclassifying FXI patterns. Our Eigen-Image and Log-Likelihood classifier can\nfind the best-matched template for a single-molecule pattern within a few\nmilliseconds. It is also straightforward to parallelize them so as to fully\nmatch the XFEL repetition rate, thereby enabling processing at site.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:02:03 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Liu", "Jing", ""], ["van der Schot", "Gijs", ""], ["Engblom", "Stefan", ""]]}, {"id": "1810.10789", "submitter": "Shenglan Liu", "authors": "Shenglan Liu, Xiang Liu, Yang Liu, Lin Feng, Hong Qiao, Jian Zhou,\n  Yang Wang", "title": "Perceptual Visual Interactive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning methods are widely used in machine learning. However, the\nlack of labels in existing data limits the application of these technologies.\nVisual interactive learning (VIL) compared with computers can avoid semantic\ngap, and solve the labeling problem of small label quantity (SLQ) samples in a\ngroundbreaking way. In order to fully understand the importance of VIL to the\ninteraction process, we re-summarize the interactive learning related\nalgorithms (e.g. clustering, classification, retrieval etc.) from the\nperspective of VIL. Note that, perception and cognition are two main visual\nprocesses of VIL. On this basis, we propose a perceptual visual interactive\nlearning (PVIL) framework, which adopts gestalt principle to design interaction\nstrategy and multi-dimensionality reduction (MDR) to optimize the process of\nvisualization. The advantage of PVIL framework is that it combines computer's\nsensitivity of detailed features and human's overall understanding of global\ntasks. Experimental results validate that the framework is superior to\ntraditional computer labeling methods (such as label propagation) in both\naccuracy and efficiency, which achieves significant classification results on\ndense distribution and sparse classes dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:06:06 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Liu", "Shenglan", ""], ["Liu", "Xiang", ""], ["Liu", "Yang", ""], ["Feng", "Lin", ""], ["Qiao", "Hong", ""], ["Zhou", "Jian", ""], ["Wang", "Yang", ""]]}, {"id": "1810.10804", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Hao Chen, Chunhua Shen, Ian Reid", "title": "Fast Neural Architecture Search of Compact Semantic Segmentation Models\n  via Auxiliary Cells", "comments": "To appear in CVPR 2019. CityScapes results with ResNet-50 are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated design of neural network architectures tailored for a specific task\nis an extremely promising, albeit inherently difficult, avenue to explore.\nWhile most results in this domain have been achieved on image classification\nand language modelling problems, here we concentrate on dense per-pixel tasks,\nin particular, semantic image segmentation using fully convolutional networks.\nIn contrast to the aforementioned areas, the design choices of a fully\nconvolutional network require several changes, ranging from the sort of\noperations that need to be used---e.g., dilated convolutions---to a solving of\na more difficult optimisation problem. In this work, we are particularly\ninterested in searching for high-performance compact segmentation\narchitectures, able to run in real-time using limited resources. To achieve\nthat, we intentionally over-parameterise the architecture during the training\ntime via a set of auxiliary cells that provide an intermediate supervisory\nsignal and can be omitted during the evaluation phase. The design of the\nauxiliary cell is emitted by a controller, a neural network with the fixed\nstructure trained using reinforcement learning. More crucially, we demonstrate\nhow to efficiently search for these architectures within limited time and\ncomputational budgets. In particular, we rely on a progressive strategy that\nterminates non-promising architectures from being further trained, and on\nPolyak averaging coupled with knowledge distillation to speed-up the\nconvergence. Quantitatively, in 8 GPU-days our approach discovers a set of\narchitectures performing on-par with state-of-the-art among compact models on\nthe semantic segmentation, pose estimation and depth prediction tasks. Code\nwill be made available here: https://github.com/drsleep/nas-segm-pytorch\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:27:23 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 01:41:47 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 01:31:24 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Chen", "Hao", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1810.10818", "submitter": "Iason Oikonomidis", "authors": "Iason Oikonomidis, Guillermo Garcia-Hernando, Angela Yao, Antonis\n  Argyros, Vincent Lepetit, Tae-Kyun Kim", "title": "HANDS18: Methods, Techniques and Applications for Hand Observation", "comments": "11 pages, 1 figure, Discussion of the HANDS 2018 workshop held in\n  conjunction with ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report outlines the proceedings of the Fourth International Workshop on\nObserving and Understanding Hands in Action (HANDS 2018). The fourth\ninstantiation of this workshop attracted significant interest from both\nacademia and the industry. The program of the workshop included regular papers\nthat are published as the workshop's proceedings, extended abstracts, invited\nposters, and invited talks. Topics of the submitted works and invited talks and\nposters included novel methods for hand pose estimation from RGB, depth, or\nskeletal data, datasets for special cases and real-world applications, and\ntechniques for hand motion re-targeting and hand gesture recognition. The\ninvited speakers are leaders in their respective areas of specialization,\ncoming from both industry and academia. The main conclusions that can be drawn\nare the turn of the community towards RGB data and the maturation of some\nmethods and techniques, which in turn has led to increasing interest for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 10:21:52 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Oikonomidis", "Iason", ""], ["Garcia-Hernando", "Guillermo", ""], ["Yao", "Angela", ""], ["Argyros", "Antonis", ""], ["Lepetit", "Vincent", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1810.10828", "submitter": "Angelica I. Aviles-Rivero", "authors": "Angelica I. Aviles-Rivero, No\\'emie Debroux, Guy Williams, Martin J.\n  Graves and Carola-Bibiane Schonlieb", "title": "Compressed Sensing Plus Motion (CS+M): A New Perspective for Improving\n  Undersampled MR Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of reconstructing high quality images from\nundersampled MRI data. This is a challenging task due to the highly ill-posed\nnature of the problem. In particular, in dynamic MRI scans, the interaction\nbetween the target structure and the physical motion affects the acquired\nmeasurements leading to blurring artefacts and loss of fine details. In this\nwork, we propose a framework for dynamic MRI reconstruction framed under a new\nmulti-task optimisation model called Compressed Sensing Plus Motion (CS+M).\nFirstly, we propose a single optimisation problem that simultaneously computes\nthe MRI reconstruction and the physical motion. Secondly, we show our model can\nbe efficiently solved by breaking it up into two more computationally tractable\nproblems. The potentials and generalisation capabilities of our approach are\ndemonstrated in different clinical applications including cardiac cine, cardiac\nperfusion and brain perfusion imaging. We show, through numerical and graphical\nexperiments, that the proposed scheme reduces blurring artefacts and preserves\nthe target shape and fine details. We also report the highest quality\nreconstruction under highly undersampling rates in comparison to several state\nof the art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 10:57:03 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:56:08 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Aviles-Rivero", "Angelica I.", ""], ["Debroux", "No\u00e9mie", ""], ["Williams", "Guy", ""], ["Graves", "Martin J.", ""], ["Schonlieb", "Carola-Bibiane", ""]]}, {"id": "1810.10850", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Jiexiang Wang, Yue Huang, Xinghao Ding, Hayit Greenspan,\n  John Paisley", "title": "An Adversarial Learning Approach to Medical Image Synthesis for Lesion\n  Detection", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of lesion within medical image data is necessary for\ndiagnosis, treatment and prognosis. Segmentation and classification approaches\nare mainly based on supervised learning with well-paired image-level or\nvoxel-level labels. However, labeling the lesion in medical images is laborious\nrequiring highly specialized knowledge. We propose a medical image synthesis\nmodel named abnormal-to-normal translation generative adversarial network\n(ANT-GAN) to generate a normal-looking medical image based on its\nabnormal-looking counterpart without the need for paired training data. Unlike\ntypical GANs, whose aim is to generate realistic samples with variations, our\nmore restrictive model aims at producing a normal-looking image corresponding\nto one containing lesions, and thus requires a special design. Being able to\nprovide a \"normal\" counterpart to a medical image can provide useful side\ninformation for medical imaging tasks like lesion segmentation or\nclassification validated by our experiments. In the other aspect, the ANT-GAN\nmodel is also capable of producing highly realistic lesion-containing image\ncorresponding to the healthy one, which shows the potential in data\naugmentation verified in our experiments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 12:57:31 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 15:44:12 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sun", "Liyan", ""], ["Wang", "Jiexiang", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Greenspan", "Hayit", ""], ["Paisley", "John", ""]]}, {"id": "1810.10853", "submitter": "Gabriele Valvano", "authors": "Gabriele Valvano, Nicola Martini, Andrea Leo, Gianmarco Santini,\n  Daniele Della Latta, Emiliano Ricciardi and Dante Chiappino", "title": "Training of a Skull-Stripping Neural Network with efficient data\n  augmentation", "comments": "April 2018. Content: 8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skull-stripping methods aim to remove the non-brain tissue from acquisition\nof brain scans in magnetic resonance (MR) imaging. Although several methods\nsharing this common purpose have been presented in literature, they all suffer\nfrom the great variability of the MR images. In this work we propose a novel\napproach based on Convolutional Neural Networks to automatically perform the\nbrain extraction obtaining cutting-edge performance in the NFBS public\ndatabase. Additionally, we focus on the efficient training of the neural\nnetwork designing an effective data augmentation pipeline. Obtained results are\nevaluated through Dice metric, obtaining a value of 96.5%, and processing time,\nwith 4.5s per volume.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:01:27 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Valvano", "Gabriele", ""], ["Martini", "Nicola", ""], ["Leo", "Andrea", ""], ["Santini", "Gianmarco", ""], ["Della Latta", "Daniele", ""], ["Ricciardi", "Emiliano", ""], ["Chiappino", "Dante", ""]]}, {"id": "1810.10863", "submitter": "Christopher Bowles", "authors": "Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger\n  Gunn, Alexander Hammers, David Alexander Dickie, Maria Vald\\'es Hern\\'andez,\n  Joanna Wardlaw and Daniel Rueckert", "title": "GAN Augmentation: Augmenting Training Data using Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest issues facing the use of machine learning in medical\nimaging is the lack of availability of large, labelled datasets. The annotation\nof medical images is not only expensive and time consuming but also highly\ndependent on the availability of expert observers. The limited amount of\ntraining data can inhibit the performance of supervised machine learning\nalgorithms which often need very large quantities of data on which to train to\navoid overfitting. So far, much effort has been directed at extracting as much\ninformation as possible from what data is available. Generative Adversarial\nNetworks (GANs) offer a novel way to unlock additional information from a\ndataset by generating synthetic samples with the appearance of real images.\nThis paper demonstrates the feasibility of introducing GAN derived synthetic\ndata to the training datasets in two brain segmentation tasks, leading to\nimprovements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage\npoints under different conditions, with the strongest effects seen fewer than\nten training image stacks are available.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:17:33 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Bowles", "Christopher", ""], ["Chen", "Liang", ""], ["Guerrero", "Ricardo", ""], ["Bentley", "Paul", ""], ["Gunn", "Roger", ""], ["Hammers", "Alexander", ""], ["Dickie", "David Alexander", ""], ["Hern\u00e1ndez", "Maria Vald\u00e9s", ""], ["Wardlaw", "Joanna", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1810.10889", "submitter": "Jason Deglint", "authors": "Jason L. Deglint, Chao Jin, Alexander Wong", "title": "Investigating the Automatic Classification of Algae Using Fusion of\n  Spectral and Morphological Characteristics of Algae via Deep Residual\n  Learning", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the impact of global climate changes and human activities, harmful\nalgae blooms in surface waters have become a growing concern due to negative\nimpacts on water related industries. Therefore, reliable and cost effective\nmethods of quantifying the type and concentration of threshold levels of algae\ncells has become critical for ensuring successful water management. In this\nwork, we present SAMSON, an innovative system to automatically classify\nmultiple types of algae from different phyla groups by combining standard\nmorphological features with their multi-wavelength signals. Two phyla with\nfocused investigation in this study are the Cyanophyta phylum (blue-green\nalgae), and the Chlorophyta phylum (green algae). We use a custom-designed\nmicroscopy imaging system which is configured to image water samples at two\nfluorescent wavelengths and seven absorption wavelengths using\ndiscrete-wavelength high-powered light emitting diodes (LEDs). Powered by\ncomputer vision and machine learning, we investigate the possibility and\neffectiveness of automatic classification using a deep residual convolutional\nneural network. More specifically, a classification accuracy of 96% was\nachieved in an experiment conducted with six different algae types. This high\nlevel of accuracy was achieved using a deep residual convolutional neural\nnetwork that learns the optimal combination of spectral and morphological\nfeatures. These findings elude to the possibility of leveraging a unique\nfingerprint of algae cell (i.e. spectral wavelengths and morphological\nfeatures) to automatically distinguish different algae types. Our work herein\ndemonstrates that, when coupled with multi-band fluorescence microscopy,\nmachine learning algorithms can potentially be used as a robust and\ncost-effective tool for identifying and enumerating algae cells.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 14:23:23 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Deglint", "Jason L.", ""], ["Jin", "Chao", ""], ["Wong", "Alexander", ""]]}, {"id": "1810.10901", "submitter": "Yida Wang", "authors": "Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari", "title": "Adversarial Semantic Scene Completion from a Single Depth Image", "comments": "2018 International Conference on 3D Vision (3DV)", "journal-ref": "Y. Wang, D. J. Tan, N. Navab and F. Tombari, 2018 International\n  Conference on 3D Vision (3DV), Verona, Italy, 2018, pp. 426-434", "doi": "10.1109/3DV.2018.00056", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to reconstruct, complete and semantically label a 3D\nscene from a single input depth image. We improve the accuracy of the regressed\nsemantic 3D maps by a novel architecture based on adversarial learning. In\nparticular, we suggest using multiple adversarial loss terms that not only\nenforce realistic outputs with respect to the ground truth, but also an\neffective embedding of the internal features. This is done by correlating the\nlatent features of the encoder working on partial 2.5D data with the latent\nfeatures extracted from a variational 3D auto-encoder trained to reconstruct\nthe complete semantic scene. In addition, differently from other approaches\nthat operate entirely through 3D convolutions, at test time we retain the\noriginal 2.5D structure of the input during downsampling to improve the\neffectiveness of the internal representation of our model. We test our approach\non the main benchmark datasets for semantic scene completion to qualitatively\nand quantitatively assess the effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 14:43:12 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Wang", "Yida", ""], ["Tan", "David Joseph", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1810.10933", "submitter": "Reed Williams", "authors": "Reed M. Williams, Horea T. Ilie\\c{s}", "title": "Practical Shape Analysis and Segmentation Methods for Point Cloud Models", "comments": "21 pages, 20 figures. To appear in The Journal of Computer Aided\n  Geometric Design's Special Issue on Heat Diffusion Equation and Optimal\n  Transport in Geometry Processing and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current point cloud processing algorithms do not have the capability to\nautomatically extract semantic information from the observed scenes, except in\nvery specialized cases. Furthermore, existing mesh analysis paradigms cannot be\ndirectly employed to automatically perform typical shape analysis tasks\ndirectly on point cloud models.\n  We present a potent framework for shape analysis, similarity, and\nsegmentation of noisy point cloud models for real objects of engineering\ninterest, models that may be incomplete. The proposed framework relies on\nspectral methods and the heat diffusion kernel to construct compact shape\nsignatures, and we show that the framework supports a variety of clustering\ntechniques that have traditionally been applied only on mesh models. We\ndeveloped and implemented one practical and convergent estimate of the\nLaplace-Beltrami operator for point clouds as well as a number of clustering\ntechniques adapted to work directly on point clouds to produce geometric\nfeatures of engineering interest. The key advantage of this framework is that\nit supports practical shape analysis capabilities that operate directly on\npoint cloud models of objects without requiring surface reconstruction or\nglobal meshing. We show that the proposed technique is robust against typical\nnoise present in possibly incomplete point clouds, and segment point clouds\nscanned by depth cameras (e.g. Kinect) into semantically-meaningful sub-shapes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:38:30 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Williams", "Reed M.", ""], ["Ilie\u015f", "Horea T.", ""]]}, {"id": "1810.10941", "submitter": "Juan Manuel Fernandez Montenegro JMFMontenegro", "authors": "Juan Manuel Fern\\'andez Montenegro", "title": "Alzheimer's Disease Diagnosis Based on Cognitive Methods in Virtual\n  Environments and Emotions Analysis", "comments": "PhD Thesis 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dementia is a syndrome characterised by the decline of different cognitive\nabilities. Alzheimer's Disease (AD) is the most common dementia affecting\ncognitive domains such as memory and learning, perceptual-motion or executive\nfunction. High rate of deaths and high cost for detection, treatments and\npatient's care count amongst its consequences. Early detection of AD is\nconsidered of high importance for improving the quality of life of patients and\ntheir families. The aim of this thesis is to introduce novel non-invasive early\ndiagnosis methods in order to speed the diagnosis, reduce the associated costs\nand make them widely accessible. Novel AD's screening tests based on virtual\nenvironments using new immersive technologies combined with advanced Human\nComputer Interaction (HCI) systems are introduced. Four tests demonstrate the\nwide range of screening mechanisms based on cognitive domain impairments that\ncan be designed using virtual environments. The use of emotion recognition to\nanalyse AD symptoms has been also proposed. A novel multimodal dataset was\nspecifically created to remark the autobiographical memory deficits of AD\npatients. Data from this dataset is used to introduce novel descriptors for\nElectroencephalogram (EEG) and facial images data. EEG features are based on\nquaternions in order to keep the correlation information between sensors,\nwhereas, for facial expression recognition, a preprocessing method for motion\nmagnification and descriptors based on origami crease pattern algorithm are\nproposed to enhance facial micro-expressions. These features have been proved\non classifiers such as SVM and Adaboost for the classification of reactions to\nautobiographical stimuli such as long and short term memories.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:56:51 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Montenegro", "Juan Manuel Fern\u00e1ndez", ""]]}, {"id": "1810.10948", "submitter": "Jianlin Su", "authors": "Jianlin Su", "title": "Training Generative Adversarial Networks Via Turing Test", "comments": "fix some clerical errors, add some experimental data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article, we introduce a new mode for training Generative Adversarial\nNetworks (GANs). Rather than minimizing the distance of evidence distribution\n$\\tilde{p}(x)$ and the generative distribution $q(x)$, we minimize the distance\nof $\\tilde{p}(x_r)q(x_f)$ and $\\tilde{p}(x_f)q(x_r)$. This adversarial pattern\ncan be interpreted as a Turing test in GANs. It allows us to use information of\nreal samples during training generator and accelerates the whole training\nprocedure. We even find that just proportionally increasing the size of\ndiscriminator and generator, it succeeds on 256x256 resolution without\nadjusting hyperparameters carefully.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:08:02 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 08:49:53 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Su", "Jianlin", ""]]}, {"id": "1810.10956", "submitter": "Tiago Carvalho", "authors": "Kemilly Dearo Garcia, Tiago Carvalho, Jo\\~ao Mendes-Moreira, Jo\\~ao\n  M.P. Cardoso and Andr\\'e C.P.L.F. de Carvalho", "title": "A Preliminary Study on Hyperparameter Configuration for Human Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition (HAR) is a classification task that aims to\nclassify human activities or predict human behavior by means of features\nextracted from sensors data. Typical HAR systems use wearable sensors and/or\nhandheld and mobile devices with built-in sensing capabilities. Due to the\nwidespread use of smartphones and to the inclusion of various sensors in all\ncontemporary smartphones (e.g., accelerometers and gyroscopes), they are\ncommonly used for extracting and collecting data from sensors and even for\nimplementing HAR systems. When using mobile devices, e.g., smartphones, HAR\nsystems need to deal with several constraints regarding battery, computation\nand memory. These constraints enforce the need of a system capable of managing\nits resources and maintain acceptable levels of classification accuracy.\nMoreover, several factors can influence activity recognition, such as\nclassification models, sensors availability and size of data window for feature\nextraction, making stable accuracy a difficult task. In this paper, we present\na semi-supervised classifier and a study regarding the influence of\nhyperparameter configuration in classification accuracy, depending on the user\nand the activities performed by each user. This study focuses on sensing data\nprovided by the PAMAP2 dataset. Experimental results show that it is possible\nto maintain classification accuracy by adjusting hyperparameters, like window\nsize and windows overlap factor, depending on user and activity performed.\nThese experiments motivate the development of a system able to automatically\nadapt hyperparameter settings for the activity performed by each user.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:26:30 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Garcia", "Kemilly Dearo", ""], ["Carvalho", "Tiago", ""], ["Mendes-Moreira", "Jo\u00e3o", ""], ["Cardoso", "Jo\u00e3o M. P.", ""], ["de Carvalho", "Andr\u00e9 C. P. L. F.", ""]]}, {"id": "1810.10974", "submitter": "Isaak Kavasidis", "authors": "Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela\n  Giordano, Joseph Schmidt, Mubarak Shah", "title": "Decoding Brain Representations by Multimodal Learning of Neural Activity\n  and Visual Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel method of exploring human brain-visual\nrepresentations, with a view towards replicating these processes in machines.\nThe core idea is to learn plausible computational and biological\nrepresentations by correlating human neural activity and natural images. Thus,\nwe first propose a model, EEG-ChannelNet, to learn a brain manifold for EEG\nclassification. After verifying that visual information can be extracted from\nEEG data, we introduce a multimodal approach that uses deep image and EEG\nencoders, trained in a siamese configuration, for learning a joint manifold\nthat maximizes a compatibility measure between visual features and brain\nrepresentations. We then carry out image classification and saliency detection\non the learned manifold. Performance analyses show that our approach\nsatisfactorily decodes visual information from neural signals. This, in turn,\ncan be used to effectively supervise the training of deep learning models, as\ndemonstrated by the high performance of image classification and saliency\ndetection on out-of-training classes. The obtained results show that the\nlearned brain-visual features lead to improved performance and simultaneously\nbring deep models more in line with cognitive neuroscience work related to\nvisual perception and attention.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:52:20 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 17:49:41 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Palazzo", "Simone", ""], ["Spampinato", "Concetto", ""], ["Kavasidis", "Isaak", ""], ["Giordano", "Daniela", ""], ["Schmidt", "Joseph", ""], ["Shah", "Mubarak", ""]]}, {"id": "1810.11027", "submitter": "Julian Merten", "authors": "Julian Merten, Carlo Giocoli, Marco Baldi, Massimo Meneghetti, Austin\n  Peel, Florian Lalande, Jean-Luc Starck and Valeria Pettorino", "title": "On the dissection of degenerate cosmologies with machine learning", "comments": "21 pages, 14 figures, 10 tables. Associated code and data respository\n  at https://www.bitbucket.org/jmerten82/mydnn . Accepted for publication by\n  the MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stz972", "report-no": null, "categories": "astro-ph.CO cs.CV gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the DUSTGRAIN-pathfinder suite of simulations, we investigate\nobservational degeneracies between nine models of modified gravity and massive\nneutrinos. Three types of machine learning techniques are tested for their\nability to discriminate lensing convergence maps by extracting dimensional\nreduced representations of the data. Classical map descriptors such as the\npower spectrum, peak counts and Minkowski functionals are combined into a joint\nfeature vector and compared to the descriptors and statistics that are common\nto the field of digital image processing. To learn new features directly from\nthe data we use a Convolutional Neural Network (CNN). For the mapping between\nfeature vectors and the predictions of their underlying model, we implement two\ndifferent classifiers; one based on a nearest-neighbour search and one that is\nbased on a fully connected neural network. We find that the neural network\nprovides a much more robust classification than the nearest-neighbour approach\nand that the CNN provides the most discriminating representation of the data.\nIt achieves the cleanest separation between the different models and the\nhighest classification success rate of 59% for a single source redshift. Once\nwe perform a tomographic CNN analysis, the total classification accuracy\nincreases significantly to 76% with no observational degeneracies remaining.\nVisualising the filter responses of the CNN at different network depths\nprovides us with the unique opportunity to learn from very complex models and\nto understand better why they perform so well.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:00:02 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 09:41:52 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Merten", "Julian", ""], ["Giocoli", "Carlo", ""], ["Baldi", "Marco", ""], ["Meneghetti", "Massimo", ""], ["Peel", "Austin", ""], ["Lalande", "Florian", ""], ["Starck", "Jean-Luc", ""], ["Pettorino", "Valeria", ""]]}, {"id": "1810.11043", "submitter": "Tianhe Yu", "authors": "Tianhe Yu, Pieter Abbeel, Sergey Levine, Chelsea Finn", "title": "One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks", "comments": "Video results available at https://sites.google.com/view/one-shot-hil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning multi-stage vision-based tasks on a real\nrobot from a single video of a human performing the task, while leveraging\ndemonstration data of subtasks with other objects. This problem presents a\nnumber of major challenges. Video demonstrations without teleoperation are easy\nfor humans to provide, but do not provide any direct supervision. Learning\npolicies from raw pixels enables full generality but calls for large function\napproximators with many parameters to be learned. Finally, compound tasks can\nrequire impractical amounts of demonstration data, when treated as a monolithic\nskill. To address these challenges, we propose a method that learns both how to\nlearn primitive behaviors from video demonstrations and how to dynamically\ncompose these behaviors to perform multi-stage tasks by \"watching\" a human\ndemonstrator. Our results on a simulated Sawyer robot and real PR2 robot\nillustrate our method for learning a variety of order fulfillment and kitchen\nserving tasks with novel objects and raw pixel inputs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:05:08 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yu", "Tianhe", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1810.11090", "submitter": "Vishwa Parekh", "authors": "Vishwa S. Parekh, Michael A. Jacobs", "title": "Radiomic Synthesis Using Deep Convolutional Neural Networks", "comments": "Submitted to ISBI 2019, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomics is a rapidly growing field that deals with modeling the textural\ninformation present in the different tissues of interest for clinical decision\nsupport. However, the process of generating radiomic images is computationally\nvery expensive and could take substantial time per radiological image for\ncertain higher order features, such as, gray-level co-occurrence matrix(GLCM),\neven with high-end GPUs. To that end, we developed RadSynth, a deep\nconvolutional neural network(CNN) model, to efficiently generate radiomic\nimages. RadSynth was tested on a breast cancer patient cohort of twenty-four\npatients(ten benign, ten malignant and four normal) for computation of GLCM\nentropy images from post-contrast DCE-MRI. RadSynth produced excellent\nsynthetic entropy images compared to traditional GLCM entropy images. The\naverage percentage difference and correlation between the two techniques were\n0.07 $\\pm$ 0.06 and 0.97, respectively. In conclusion, RadSynth presents a new\npowerful tool for fast computation and visualization of the textural\ninformation present in the radiological images.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 20:07:39 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 13:41:31 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Parekh", "Vishwa S.", ""], ["Jacobs", "Michael A.", ""]]}, {"id": "1810.11094", "submitter": "Dominique Vaufreydaz", "authors": "Thomas Guntz (PERVASIVE), James Crowley (PERVASIVE), Dominique\n  Vaufreydaz (PERVASIVE), Raffaella Balzarini (PERVASIVE), Philippe Dessus\n  (LSE, PERVASIVE)", "title": "The Role of Emotion in Problem Solving: First Results from Observing\n  Chess", "comments": null, "journal-ref": "ICMI 2018 - Workshop at 20th ACM International Conference on\n  Multimodal Interaction, Oct 2018, Boulder, Colorado, United States. pp.1-13", "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present results from recent experiments that suggest that\nchess players associate emotions to game situations and reactively use these\nassociations to guide search for planning and problem solving. We describe the\ndesign of an instrument for capturing and interpreting multimodal signals of\nhumans engaged in solving challenging problems. We review results from a pilot\nexperiment with human experts engaged in solving challenging problems in Chess\nthat revealed an unexpected observation of rapid changes in emotion as players\nattempt to solve challenging problems. We propose a cognitive model that\ndescribes the process by which subjects select chess chunks for use in\ninterpretation of the game situation and describe initial results from a second\nexperiment designed to test this model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 06:35:31 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Guntz", "Thomas", "", "PERVASIVE"], ["Crowley", "James", "", "PERVASIVE"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"], ["Balzarini", "Raffaella", "", "PERVASIVE"], ["Dessus", "Philippe", "", "LSE, PERVASIVE"]]}, {"id": "1810.11120", "submitter": "Ayan Kumar Bhunia", "authors": "Ankan Kumar Bhunia, Ayan Kumar Bhunia, Aneeshan Sain, Partha Pratim\n  Roy", "title": "Improving Document Binarization via Adversarial Noise-Texture\n  Augmentation", "comments": "IEEE International Conference on Image Processing (ICIP), 2019. The\n  full source code of the proposed system is publicly available at\n  https://github.com/ankanbhunia/AdverseBiNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization of degraded document images is an elementary step in most of the\nproblems in document image analysis domain. The paper re-visits the\nbinarization problem by introducing an adversarial learning approach. We\nconstruct a Texture Augmentation Network that transfers the texture element of\na degraded reference document image to a clean binary image. In this way, the\nnetwork creates multiple versions of the same textual content with various\nnoisy textures, thus enlarging the available document binarization datasets. At\nlast, the newly generated images are passed through a Binarization network to\nget back the clean version. By jointly training the two networks we can\nincrease the adversarial robustness of our system. Also, it is noteworthy that\nour model can learn from unpaired data. Experimental results suggest that the\nproposed method achieves superior performance over widely used DIBCO datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 22:02:54 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 10:33:02 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Bhunia", "Ankan Kumar", ""], ["Bhunia", "Ayan Kumar", ""], ["Sain", "Aneeshan", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1810.11137", "submitter": "Kedar Tatwawadi", "authors": "Ashutosh Bhown, Soham Mukherjee, Sean Yang, Shubham Chandak, Irena\n  Fischer-Hwang, Kedar Tatwawadi, Judith Fan, Tsachy Weissman", "title": "Towards improved lossy image compression: Human image reconstruction\n  with public-domain images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression has been studied extensively in the context of\ntypical loss functions such as RMSE, MS-SSIM, etc. However, compression at low\nbitrates generally produces unsatisfying results. Furthermore, the availability\nof massive public image datasets appears to have hardly been exploited in image\ncompression. Here, we present a paradigm for eliciting human image\nreconstruction in order to perform lossy image compression. In this paradigm,\none human describes images to a second human, whose task is to reconstruct the\ntarget image using publicly available images and text instructions. The\nresulting reconstructions are then evaluated by human raters on the Amazon\nMechanical Turk platform and compared to reconstructions obtained using\nstate-of-the-art compressor WebP. Our results suggest that prioritizing\nsemantic visual elements may be key to achieving significant improvements in\nimage compression, and that our paradigm can be used to develop a more\nhuman-centric loss function.\n  The images, results and additional data are available at\nhttps://compression.stanford.edu/human-compression\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 23:34:37 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 04:26:49 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 03:01:38 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Bhown", "Ashutosh", ""], ["Mukherjee", "Soham", ""], ["Yang", "Sean", ""], ["Chandak", "Shubham", ""], ["Fischer-Hwang", "Irena", ""], ["Tatwawadi", "Kedar", ""], ["Fan", "Judith", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1810.11160", "submitter": "Jia-Hong Lee", "authors": "Hsin-Rung Chou, Jia-Hong Lee, Yi-Ming Chan, and Chu-Song Chen", "title": "Data-specific Adaptive Threshold for Face Recognition and Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many face recognition systems boost the performance using deep learning\nmodels, but only a few researches go into the mechanisms for dealing with\nonline registration. Although we can obtain discriminative facial features\nthrough the state-of-the-art deep model training, how to decide the best\nthreshold for practical use remains a challenge. We develop a technique of\nadaptive threshold mechanism to improve the recognition accuracy. We also\ndesign a face recognition system along with the registering procedure to handle\nonline registration. Furthermore, we introduce a new evaluation protocol to\nbetter evaluate the performance of an algorithm for real-world scenarios. Under\nour proposed protocol, our method can achieve a 22\\% accuracy improvement on\nthe LFW dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 01:34:40 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Chou", "Hsin-Rung", ""], ["Lee", "Jia-Hong", ""], ["Chan", "Yi-Ming", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1810.11181", "submitter": "Abhishek Das", "authors": "Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra", "title": "Neural Modular Control for Embodied Question Answering", "comments": "10 pages, 3 figures, 2 tables. Published at CoRL 2018. Webpage:\n  https://embodiedqa.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modular approach for learning policies for navigation over long\nplanning horizons from language input. Our hierarchical policy operates at\nmultiple timescales, where the higher-level master policy proposes subgoals to\nbe executed by specialized sub-policies. Our choice of subgoals is\ncompositional and semantic, i.e. they can be sequentially combined in arbitrary\norderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find\nkitchen', 'find refrigerator', etc.).\n  We use imitation learning to warm-start policies at each level of the\nhierarchy, dramatically increasing sample efficiency, followed by reinforcement\nlearning. Independent reinforcement learning at each level of hierarchy enables\nsub-policies to adapt to consequences of their actions and recover from errors.\nSubsequent joint hierarchical training enables the master policy to adapt to\nthe sub-policies.\n  On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al.,\n2018), requiring navigating diverse realistic indoor environments, our approach\noutperforms prior work by a significant margin, both in terms of navigation and\nquestion answering.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 03:58:26 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 23:41:47 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Das", "Abhishek", ""], ["Gkioxari", "Georgia", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1810.11189", "submitter": "Chen Zhu", "authors": "Chen Zhu, Xiao Tan, Feng Zhou, Xiao Liu, Kaiyu Yue, Errui Ding, Yi Ma", "title": "Fine-grained Video Categorization with Redundancy Reduction Attention", "comments": "Correcting a typo in ECCV version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fine-grained categorization tasks, videos could serve as a better source\nthan static images as videos have a higher chance of containing discriminative\npatterns. Nevertheless, a video sequence could also contain a lot of redundant\nand irrelevant frames. How to locate critical information of interest is a\nchallenging task. In this paper, we propose a new network structure, known as\nRedundancy Reduction Attention (RRA), which learns to focus on multiple\ndiscriminative patterns by sup- pressing redundant feature channels.\nSpecifically, it firstly summarizes the video by weight-summing all feature\nvectors in the feature maps of selected frames with a spatio-temporal soft\nattention, and then predicts which channels to suppress or to enhance according\nto this summary with a learned non-linear transform. Suppression is achieved by\nmodulating the feature maps and threshing out weak activations. The updated\nfeature maps are then used in the next iteration. Finally, the video is\nclassified based on multiple summaries. The proposed method achieves out-\nstanding performances in multiple video classification datasets. Further- more,\nwe have collected two large-scale video datasets, YouTube-Birds and\nYouTube-Cars, for future researches on fine-grained video categorization. The\ndatasets are available at http://www.cs.umd.edu/~chenzhu/fgvc.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 05:03:34 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Zhu", "Chen", ""], ["Tan", "Xiao", ""], ["Zhou", "Feng", ""], ["Liu", "Xiao", ""], ["Yue", "Kaiyu", ""], ["Ding", "Errui", ""], ["Ma", "Yi", ""]]}, {"id": "1810.11205", "submitter": "Max-Heinrich Laves", "authors": "Max-Heinrich Laves, L\\\"uder A. Kahrs, and Tobias Ortmaier", "title": "Deep learning based 2.5D flow field estimation for maximum intensity\n  projections of 4D optical coherence tomography", "comments": "Accepted for SPIE Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In microsurgery, lasers have emerged as precise tools for bone ablation. A\nchallenge is automatic control of laser bone ablation with 4D optical coherence\ntomography (OCT). OCT as high resolution imaging modality provides volumetric\nimages of tissue and foresees information of bone position and orientation\n(pose) as well as thickness. However, existing approaches for OCT based laser\nablation control rely on external tracking systems or invasively ablated\nartificial landmarks for tracking the pose of the OCT probe relative to the\ntissue. This can be superseded by estimating the scene flow caused by relative\nmovement between OCT-based laser ablation system and patient.\n  Therefore, this paper deals with 2.5D scene flow estimation of volumetric OCT\nimages for application in laser ablation. We present a semi-supervised\nconvolutional neural network based tracking scheme for subsequent 3D OCT\nvolumes and apply it to a realistic semi-synthetic data set of ex vivo human\ntemporal bone specimen. The scene flow is estimated in a two-stage approach. In\nthe first stage, 2D lateral scene flow is computed on census-transformed\nen-face arguments-of-maximum intensity projections. Subsequent to this, the\nprojections are warped by predicted lateral flow and 1D depth flow is\nestimated. The neural network is trained semi-supervised by combining error to\nground truth and the reconstruction error of warped images with assumptions of\nspatial flow smoothness. Quantitative evaluation reveals a mean endpoint error\nof $ (4.7\\pm{}3.5) $ voxel or $ 27.5 \\pm 20.5 \\mu\\mathrm{m} $ for scene flow\nestimation caused by simulated relative movement between the OCT probe and\nbone. The scene flow estimation for 4D OCT enables its use for markerless\ntracking of mastoid bone structures for image guidance in general, and\nautomated laser ablation control.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:10:51 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 00:35:59 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Kahrs", "L\u00fcder A.", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "1810.11215", "submitter": "Hong Huy Nguyen", "authors": "Huy H. Nguyen, Junichi Yamagishi, Isao Echizen", "title": "Capsule-Forensics: Using Capsule Networks to Detect Forged Images and\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in media generation techniques have made it easier for\nattackers to create forged images and videos. State-of-the-art methods enable\nthe real-time creation of a forged version of a single video obtained from a\nsocial network. Although numerous methods have been developed for detecting\nforged images and videos, they are generally targeted at certain domains and\nquickly become obsolete as new kinds of attacks appear. The method introduced\nin this paper uses a capsule network to detect various kinds of spoofs, from\nreplay attacks using printed images or recorded videos to computer-generated\nvideos using deep convolutional neural networks. It extends the application of\ncapsule networks beyond their original intention to the solving of inverse\ngraphics problems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:50:29 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1810.11224", "submitter": "Yilei Shi", "authors": "Yilei Shi, Qingyu Li, Xiao Xiang Zhu", "title": "Building Footprint Generation Using Improved Generative Adversarial\n  Networks", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/LGRS.2018.2878486", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building footprint information is an essential ingredient for 3-D\nreconstruction of urban models. The automatic generation of building footprints\nfrom satellite images presents a considerable challenge due to the complexity\nof building shapes. In this work, we have proposed improved generative\nadversarial networks (GANs) for the automatic generation of building footprints\nfrom satellite images. We used a conditional GAN with a cost function derived\nfrom the Wasserstein distance and added a gradient penalty term. The achieved\nresults indicated that the proposed method can significantly improve the\nquality of building footprint generation compared to conditional generative\nadversarial networks, the U-Net, and other networks. In addition, our method\nnearly removes all hyperparameters tuning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 08:25:41 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Shi", "Yilei", ""], ["Li", "Qingyu", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1810.11261", "submitter": "Shivansh Rao", "authors": "Shivansh Rao, Tanzila Rahman, Mrigank Rochan, Yang Wang", "title": "Video-based Person Re-identification Using Spatial-Temporal Attention\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of video-based person re-identification. The goal is\nto identify a person from videos captured under different cameras. In this\npaper, we propose an efficient spatial-temporal attention based model for\nperson re-identification from videos. Our method generates an attention score\nfor each frame based on frame-level features. The attention scores of all\nframes in a video are used to produce a weighted feature vector for the input\nvideo. Unlike most existing deep learning methods that use global\nrepresentation, our approach focuses on attention scores. Extensive experiments\non two benchmark datasets demonstrate that our method achieves the\nstate-of-the-art performance. This is a technical report.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 10:54:39 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Rao", "Shivansh", ""], ["Rahman", "Tanzila", ""], ["Rochan", "Mrigank", ""], ["Wang", "Yang", ""]]}, {"id": "1810.11282", "submitter": "Binjie Qin", "authors": "Wenzhao Zhao, Qiegen Liu, Yisong Lv, and Binjie Qin", "title": "Texture variation adaptive image denoising with nonlocal PCA", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2916976", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image textures, as a kind of local variations, provide important information\nfor human visual system. Many image textures, especially the small-scale or\nstochastic textures are rich in high-frequency variations, and are difficult to\nbe preserved. Current state-of-the-art denoising algorithms typically adopt a\nnonlocal approach consisting of image patch grouping and group-wise denoising\nfiltering. To achieve a better image denoising while preserving the variations\nin texture, we first adaptively group high correlated image patches with the\nsame kinds of texture elements (texels) via an adaptive clustering method. This\nadaptive clustering method is applied in an\nover-clustering-and-iterative-merging approach, where its noise robustness is\nimproved with a custom merging threshold relating to the noise level and\ncluster size. For texture-preserving denoising of each cluster, considering\nthat the variations in texture are captured and wrapped in not only the\nbetween-dimension energy variations but also the within-dimension variations of\nPCA transform coefficients, we further propose a PCA-transform-domain variation\nadaptive filtering method to preserve the local variations in textures.\nExperiments on natural images show the superiority of the proposed\ntransform-domain variation adaptive filtering to traditional PCA-based hard or\nsoft threshold filtering. As a whole, the proposed denoising method achieves a\nfavorable texture preserving performance both quantitatively and visually,\nespecially for stochastic textures, which is further verified in camera raw\nimage denoising.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 11:53:39 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zhao", "Wenzhao", ""], ["Liu", "Qiegen", ""], ["Lv", "Yisong", ""], ["Qin", "Binjie", ""]]}, {"id": "1810.11335", "submitter": "Jirong Yi", "authors": "Jirong Yi, Anh Duc Le, Tianming Wang, Xiaodong Wu, Weiyu Xu", "title": "Outlier Detection using Generative Models with Theoretical Performance\n  Guarantees", "comments": "38 Pages, 15 Figures, 10 Lemmas or Theorems with Proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV eess.IV math.IT math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of recovering signals from compressed\nmeasurements contaminated with sparse outliers, which has arisen in many\napplications. In this paper, we propose a generative model neural network\napproach for reconstructing the ground truth signals under sparse outliers. We\npropose an iterative alternating direction method of multipliers (ADMM)\nalgorithm for solving the outlier detection problem via $\\ell_1$ norm\nminimization, and a gradient descent algorithm for solving the outlier\ndetection problem via squared $\\ell_1$ norm minimization. We establish the\nrecovery guarantees for reconstruction of signals using generative models in\nthe presence of outliers, and give an upper bound on the number of outliers\nallowed for recovery. Our results are applicable to both the linear generator\nneural network and the nonlinear generator neural network with an arbitrary\nnumber of layers. We conduct extensive experiments using variational\nauto-encoder and deep convolutional generative adversarial networks, and the\nexperimental results show that the signals can be successfully reconstructed\nunder outliers using our approach. Our approach outperforms the traditional\nLasso and $\\ell_2$ minimization approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 14:11:04 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yi", "Jirong", ""], ["Le", "Anh Duc", ""], ["Wang", "Tianming", ""], ["Wu", "Xiaodong", ""], ["Xu", "Weiyu", ""]]}, {"id": "1810.11348", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Wentong Liao, Chun Yang, Yanpeng Cao, Bodo\n  Rosenhahn", "title": "Security Event Recognition for Visual Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With rapidly increasing deployment of surveillance cameras, the reliable\nmethods for automatically analyzing the surveillance video and recognizing\nspecial events are demanded by different practical applications. This paper\nproposes a novel effective framework for security event analysis in\nsurveillance videos. First, convolutional neural network (CNN) framework is\nused to detect objects of interest in the given videos. Second, the owners of\nthe objects are recognized and monitored in real-time as well. If anyone moves\nany object, this person will be verified whether he/she is its owner. If not,\nthis event will be further analyzed and distinguished between two different\nscenes: moving the object away or stealing it. To validate the proposed\napproach, a new video dataset consisting of various scenarios is constructed\nfor more complex tasks. For comparison purpose, the experiments are also\ncarried out on the benchmark databases related to the task on abandoned luggage\ndetection. The experimental results show that the proposed approach outperforms\nthe state-of-the-art methods and effective in recognizing complex security\nevents.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 14:37:45 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yang", "Michael Ying", ""], ["Liao", "Wentong", ""], ["Yang", "Chun", ""], ["Cao", "Yanpeng", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1810.11392", "submitter": "Anis Kacem", "authors": "Naima Otberdout, Anis Kacem, Mohamed Daoudi, Lahoucine Ballihi, and\n  Stefano Berretti", "title": "Automatic Analysis of Facial Expressions Based on Deep Covariance\n  Trajectories", "comments": "A preliminary version of this work appeared in \"Otberdout N, Kacem A,\n  Daoudi M, Ballihi L, Berretti S. Deep Covariance Descriptors for Facial\n  Expression Recognition, in British Machine Vision Conference 2018, BMVC 2018,\n  Northumbria University, Newcastle, UK, September 3-6, 2018. ; 2018 :159.\"\n  arXiv admin note: substantial text overlap with arXiv:1805.03869", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach for facial expression recognition\nusing deep covariance descriptors. The solution is based on the idea of\nencoding local and global Deep Convolutional Neural Network (DCNN) features\nextracted from still images, in compact local and global covariance\ndescriptors. The space geometry of the covariance matrices is that of Symmetric\nPositive Definite (SPD) matrices. By conducting the classification of static\nfacial expressions using Support Vector Machine (SVM) with a valid Gaussian\nkernel on the SPD manifold, we show that deep covariance descriptors are more\neffective than the standard classification with fully connected layers and\nsoftmax. Besides, we propose a completely new and original solution to model\nthe temporal dynamic of facial expressions as deep trajectories on the SPD\nmanifold. As an extension of the classification pipeline of covariance\ndescriptors, we apply SVM with valid positive definite kernels derived from\nglobal alignment for deep covariance trajectories classification. By performing\nextensive experiments on the Oulu-CASIA, CK+, and SFEW datasets, we show that\nboth the proposed static and dynamic approaches achieve state-of-the-art\nperformance for facial expression recognition outperforming many recent\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:00:20 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 17:37:55 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 15:09:38 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Otberdout", "Naima", ""], ["Kacem", "Anis", ""], ["Daoudi", "Mohamed", ""], ["Ballihi", "Lahoucine", ""], ["Berretti", "Stefano", ""]]}, {"id": "1810.11408", "submitter": "Yan Wang", "authors": "Yan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens van der\n  Maaten, Mark Campbell, Kilian Q. Weinberger", "title": "Anytime Stereo Image Depth Estimation on Mobile Devices", "comments": "Accepted by ICRA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of stereo depth estimation in robotics require the\ngeneration of accurate disparity maps in real time under significant\ncomputational constraints. Current state-of-the-art algorithms force a choice\nbetween either generating accurate mappings at a slow pace, or quickly\ngenerating inaccurate ones, and additionally these methods typically require\nfar too many parameters to be usable on power- or memory-constrained devices.\nMotivated by these shortcomings, we propose a novel approach for disparity\nprediction in the anytime setting. In contrast to prior work, our end-to-end\nlearned approach can trade off computation and accuracy at inference time.\nDepth estimation is performed in stages, during which the model can be queried\nat any time to output its current best estimate. Our final model can process\n1242$ \\times $375 resolution images within a range of 10-35 FPS on an NVIDIA\nJetson TX2 module with only marginal increases in error -- using two orders of\nmagnitude fewer parameters than the most competitive baseline. The source code\nis available at https://github.com/mileyan/AnyNet .\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 16:22:27 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 14:53:54 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Wang", "Yan", ""], ["Lai", "Zihang", ""], ["Huang", "Gao", ""], ["Wang", "Brian H.", ""], ["van der Maaten", "Laurens", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1810.11438", "submitter": "Bowen Shi", "authors": "Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Jonathan Michaux,\n  Diane Brentari, Greg Shakhnarovich, Karen Livescu", "title": "American Sign Language fingerspelling recognition in the wild", "comments": "accepted in SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of American Sign Language fingerspelling recognition\nin the wild, using videos collected from websites. We introduce the largest\ndata set available so far for the problem of fingerspelling recognition, and\nthe first using naturally occurring video data. Using this data set, we present\nthe first attempt to recognize fingerspelling sequences in this challenging\nsetting. Unlike prior work, our video data is extremely challenging due to low\nframe rates and visual variability. To tackle the visual challenges, we train a\nspecial-purpose signing hand detector using a small subset of our data. Given\nthe hand detector output, a sequence model decodes the hypothesized\nfingerspelled letter sequence. For the sequence model, we explore\nattention-based recurrent encoder-decoders and CTC-based approaches. As the\nfirst attempt at fingerspelling recognition in the wild, this work is intended\nto serve as a baseline for future work on sign language recognition in\nrealistic conditions. We find that, as expected, letter error rates are much\nhigher than in previous work on more controlled data, and we analyze the\nsources of error and effects of model variants.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 17:43:44 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 05:14:07 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 22:45:52 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Shi", "Bowen", ""], ["Del Rio", "Aurora Martinez", ""], ["Keane", "Jonathan", ""], ["Michaux", "Jonathan", ""], ["Brentari", "Diane", ""], ["Shakhnarovich", "Greg", ""], ["Livescu", "Karen", ""]]}, {"id": "1810.11515", "submitter": "Yasin Musa Ayami", "authors": "Yasin Musa Ayami, Aboubayda Shabat, Delene Heukelman", "title": "Noise Sensitivity of Local Descriptors vs ConvNets: An application to\n  Facial Recognition", "comments": "6 pages, 5 figures, 6 Tables,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Local Binary Patterns (LBP) is a local descriptor proposed by Ojala et al\nto discriminate texture due to its discriminative power. However, the LBP is\nsensitive to noise and illumination changes. Consequently, several extensions\nto the LBP such as Median Binary Pattern (MBP) and methods such as Local\nDirectional Pattern (LDP) have been proposed to address its drawbacks. Though\nstudies by Zhou et al, suggest that the LDP exhibits poor performance in\npresence of random noise. Recently, convolution neural networks (ConvNets) were\nintroduced which are increasingly becoming popular for feature extraction due\nto their discriminative power. This study aimed at evaluating the sensitivity\nof ResNet50, a ConvNet pre-trained model and local descriptors (LBP and LDP) to\nnoise using the Extended Yale B face dataset with 5 different levels of noise\nadded to the dataset. In our findings, it was observed that despite adding\ndifferent levels of noise to the dataset, ResNet50 proved to be more robust\nthan the local descriptors (LBP and LDP).\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 20:01:22 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Ayami", "Yasin Musa", ""], ["Shabat", "Aboubayda", ""], ["Heukelman", "Delene", ""]]}, {"id": "1810.11518", "submitter": "Yasin Musa Ayami", "authors": "Yasin Musa Ayami, Aboubayda Shabat", "title": "An Acceleration Scheme to The Local Directional Pattern", "comments": "5 pages, 3 figures, 3 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study seeks to improve the running time of the Local Directional Pattern\n(LDP) during feature extraction using a newly proposed acceleration scheme to\nLDP. LDP is considered to be computationally expensive. To confirm this, the\nrunning time of the LDP to gray level co-occurrence matrix (GLCM) were it was\nestablished that the running time for LDP was two orders of magnitude higher\nthan that of the GLCM. In this study, the performance of the newly proposed\nacceleration scheme was evaluated against LDP and Local Binary patter (LBP)\nusing images from the publicly available extended Cohn-Kanade (CK+) dataset.\nBased on our findings, the proposed acceleration scheme significantly improves\nthe running time of the LDP by almost 3 times during feature extraction\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 20:12:20 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Ayami", "Yasin Musa", ""], ["Shabat", "Aboubayda", ""]]}, {"id": "1810.11547", "submitter": "Behnam Gholami", "authors": "Behnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstantinos Bousmalis,\n  Vladimir Pavlovic", "title": "Unsupervised Multi-Target Domain Adaptation: An Information Theoretic\n  Approach", "comments": "19 pages, 5 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (uDA) models focus on pairwise adaptation\nsettings where there is a single, labeled, source and a single target domain.\nHowever, in many real-world settings one seeks to adapt to multiple, but\nsomewhat similar, target domains. Applying pairwise adaptation approaches to\nthis setting may be suboptimal, as they fail to leverage shared information\namong multiple domains. In this work we propose an information theoretic\napproach for domain adaptation in the novel context of multiple target domains\nwith unlabeled instances and one source domain with labeled instances. Our\nmodel aims to find a shared latent space common to all domains, while\nsimultaneously accounting for the remaining private, domain-specific factors.\nDisentanglement of shared and private information is accomplished using a\nunified information-theoretic approach, which also serves to establish a\nstronger link between the latent representations and the observed data. The\nresulting model, accompanied by an efficient optimization algorithm, allows\nsimultaneous adaptation from a single source to multiple target domains. We\ntest our approach on three challenging publicly-available datasets, showing\nthat it outperforms several popular domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 22:27:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Gholami", "Behnam", ""], ["Sahu", "Pritish", ""], ["Rudovic", "Ognjen", ""], ["Bousmalis", "Konstantinos", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1810.11550", "submitter": "Roger Gomez Nieto", "authors": "Roger Gomez Nieto and Eugenio Tamura Morimitsu", "title": "Deep Convolutional Neural Network Applied to Quality Assessment for\n  Video Tracking", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance videos often suffer from blur and exposure distortions that\noccur during acquisition and storage, which can adversely influence following\nautomatic image analysis results on video-analytic tasks. The purpose of this\npaper is to deploy an algorithm that can automatically assess the presence of\nexposure distortion in videos. In this work we to design and build one\narchitecture for deep learning applied to recognition of distortions in a\nvideo. The goal is to know if the video present exposure distortions. Such an\nalgorithm could be used to enhance or restoration image or to create an object\ntracker distortion-aware.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 22:45:48 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nieto", "Roger Gomez", ""], ["Morimitsu", "Eugenio Tamura", ""]]}, {"id": "1810.11562", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Elin Farnell, Michael Kirby, Chris Peterson", "title": "Monitoring the shape of weather, soundscapes, and dynamical systems: a\n  new statistic for dimension-driven data analysis on large data sets", "comments": "Accepted to the 2018 IEEE International Conference on BIG DATA, 9\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality-reduction methods are a fundamental tool in the analysis of\nlarge data sets. These algorithms work on the assumption that the \"intrinsic\ndimension\" of the data is generally much smaller than the ambient dimension in\nwhich it is collected. Alongside their usual purpose of mapping data into a\nsmaller dimension with minimal information loss, dimensionality-reduction\ntechniques implicitly or explicitly provide information about the dimension of\nthe data set.\n  In this paper, we propose a new statistic that we call the $\\kappa$-profile\nfor analysis of large data sets. The $\\kappa$-profile arises from a\ndimensionality-reduction optimization problem: namely that of finding a\nprojection into $k$-dimensions that optimally preserves the secants between\npoints in the data set. From this optimal projection we extract $\\kappa,$ the\nnorm of the shortest projected secant from among the set of all normalized\nsecants. This $\\kappa$ can be computed for any $k$; thus the tuple of $\\kappa$\nvalues (indexed by dimension) becomes a $\\kappa$-profile. Algorithms such as\nthe Secant-Avoidance Projection algorithm and the Hierarchical Secant-Avoidance\nProjection algorithm, provide a computationally feasible means of estimating\nthe $\\kappa$-profile for large data sets, and thus a method of understanding\nand monitoring their behavior. As we demonstrate in this paper, the\n$\\kappa$-profile serves as a useful statistic in several representative\nsettings: weather data, soundscape data, and dynamical systems data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 00:15:34 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kvinge", "Henry", ""], ["Farnell", "Elin", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""]]}, {"id": "1810.11579", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi\n  Feng", "title": "$A^2$-Nets: Double Attention Networks", "comments": "Accepted at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to capture long-range relations is fundamental to image/video\nrecognition. Existing CNN models generally rely on increasing depth to model\nsuch relations which is highly inefficient. In this work, we propose the\n\"double attention block\", a novel component that aggregates and propagates\ninformative global features from the entire spatio-temporal space of input\nimages/videos, enabling subsequent convolution layers to access features from\nthe entire space efficiently. The component is designed with a double attention\nmechanism in two steps, where the first step gathers features from the entire\nspace into a compact set through second-order attention pooling and the second\nstep adaptively selects and distributes features to each location via another\nattention. The proposed double attention block is easy to adopt and can be\nplugged into existing deep neural networks conveniently. We conduct extensive\nablation studies and experiments on both image and video recognition tasks for\nevaluating its performance. On the image recognition task, a ResNet-50 equipped\nwith our double attention blocks outperforms a much larger ResNet-152\narchitecture on ImageNet-1k dataset with over 40% less the number of parameters\nand less FLOPs. On the action recognition task, our proposed model achieves the\nstate-of-the-art results on the Kinetics and UCF-101 datasets with\nsignificantly higher efficiency than recent works.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 02:32:22 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chen", "Yunpeng", ""], ["Kalantidis", "Yannis", ""], ["Li", "Jianshu", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1810.11594", "submitter": "Brian Hu", "authors": "Brian Hu and Stefan Mihalas", "title": "Convolutional neural networks with extra-classical receptive fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have had great success in many\nreal-world applications and have also been used to model visual processing in\nthe brain. However, these networks are quite brittle - small changes in the\ninput image can dramatically change a network's output prediction. In contrast\nto what is known from biology, these networks largely rely on feedforward\nconnections, ignoring the influence of recurrent connections. They also focus\non supervised rather than unsupervised learning. To address these issues, we\ncombine traditional supervised learning via backpropagation with a specialized\nunsupervised learning rule to learn lateral connections between neurons within\na convolutional neural network. These connections have been shown to optimally\nintegrate information from the surround, generating extra-classical receptive\nfields for the neurons in our new proposed model (CNNEx). Models with optimal\nlateral connections are more robust to noise and achieve better performance on\nnoisy versions of the MNIST and CIFAR-10 datasets. Resistance to noise can be\nfurther improved by combining our model with additional regularization\ntechniques such as dropout and weight decay. Although the image statistics of\nMNIST and CIFAR-10 differ greatly, the same unsupervised learning rule\ngeneralized to both datasets. Our results demonstrate the potential usefulness\nof combining supervised and unsupervised learning techniques and suggest that\nthe integration of lateral connections into convolutional neural networks is an\nimportant area of future research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 04:15:50 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hu", "Brian", ""], ["Mihalas", "Stefan", ""]]}, {"id": "1810.11598", "submitter": "Ting Chen", "authors": "Ting Chen and Xiaohua Zhai and Neil Houlsby", "title": "Self-Supervised GAN to Counter Forgetting", "comments": "NeurIPS'18 Continual Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs involve training two networks in an adversarial game, where each\nnetwork's task depends on its adversary. Recently, several works have framed\nGAN training as an online or continual learning problem. We focus on the\ndiscriminator, which must perform classification under an (adversarially)\nshifting data distribution. When trained on sequential tasks, neural networks\nexhibit \\emph{forgetting}. For GANs, discriminator forgetting leads to training\ninstability. To counter forgetting, we encourage the discriminator to maintain\nuseful representations by adding a self-supervision. Conditional GANs have a\nsimilar effect using labels. However, our self-supervised GAN does not require\nlabels, and closes the performance gap between conditional and unconditional\nmodels. We show that, in doing so, the self-supervised discriminator learns\nbetter representations than regular GANs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 04:49:25 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 23:00:39 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chen", "Ting", ""], ["Zhai", "Xiaohua", ""], ["Houlsby", "Neil", ""]]}, {"id": "1810.11603", "submitter": "Shou-Yu Chen", "authors": "Shou-Yu Chen, Guang-Sheng Chen and Wei-Peng Jing", "title": "A Miniaturized Semantic Segmentation Method for Remote Sensing Image", "comments": "5 pages, 3 figures, 3 tables, this paper is to be submitted to the\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to save the memory, we propose a miniaturization method for neural\nnetwork to reduce the parameter quantity existed in remote sensing (RS) image\nsemantic segmentation model. The compact convolution optimization method is\nfirst used for standard U-Net to reduce the weights quantity. With the purpose\nof decreasing model performance loss caused by miniaturization and based on the\ncharacteristics of remote sensing image, fewer down-samplings and improved\ncascade atrous convolution are then used to improve the performance of the\nminiaturized U-Net. Compared with U-Net, our proposed Micro-Net not only\nachieves 29.26 times model compression, but also basically maintains the\nperformance unchanged on the public dataset. We provide a Keras and Tensorflow\nhybrid programming implementation for our model:\nhttps://github.com/Isnot2bad/Micro-Net\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 05:58:36 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chen", "Shou-Yu", ""], ["Chen", "Guang-Sheng", ""], ["Jing", "Wei-Peng", ""]]}, {"id": "1810.11610", "submitter": "Haoye Dong", "authors": "Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu, Jian Yin", "title": "Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable advances in image synthesis research, existing works often\nfail in manipulating images under the context of large geometric\ntransformations. Synthesizing person images conditioned on arbitrary poses is\none of the most representative examples where the generation quality largely\nrelies on the capability of identifying and modeling arbitrary transformations\non different body parts. Current generative models are often built on local\nconvolutions and overlook the key challenges (e.g. heavy occlusions, different\nviews or dramatic appearance changes) when distinct geometric changes happen\nfor each part, caused by arbitrary pose manipulations. This paper aims to\nresolve these challenges induced by geometric variability and spatial\ndisplacements via a new Soft-Gated Warping Generative Adversarial Network\n(Warping-GAN), which is composed of two stages: 1) it first synthesizes a\ntarget part segmentation map given a target pose, which depicts the\nregion-level spatial layouts for guiding image synthesis with higher-level\nstructure constraints; 2) the Warping-GAN equipped with a soft-gated\nwarping-block learns feature-level mapping to render textures from the original\nimage into the generated segmentation map. Warping-GAN is capable of\ncontrolling different transformation degrees given distinct target poses.\nMoreover, the proposed warping-block is light-weight and flexible enough to be\ninjected into any networks. Human perceptual studies and quantitative\nevaluations demonstrate the superiority of our Warping-GAN that significantly\noutperforms all existing methods on two large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 07:08:18 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 18:19:04 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Dong", "Haoye", ""], ["Liang", "Xiaodan", ""], ["Gong", "Ke", ""], ["Lai", "Hanjiang", ""], ["Zhu", "Jia", ""], ["Yin", "Jian", ""]]}, {"id": "1810.11641", "submitter": "Frank Hafner", "authors": "Frank Hafner, Amran Bhuiyan, Julian F. P. Kooij, Eric Granger", "title": "A Cross-Modal Distillation Network for Person Re-identification in\n  RGB-Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification involves the recognition over time of individuals\ncaptured using multiple distributed sensors. With the advent of powerful deep\nlearning methods able to learn discriminant representations for visual\nrecognition, cross-modal person re-identification based on different sensor\nmodalities has become viable in many challenging applications in, e.g.,\nautonomous driving, robotics and video surveillance. Although some methods have\nbeen proposed for re-identification between infrared and RGB images, few\naddress depth and RGB images. In addition to the challenges for each modality\nassociated with occlusion, clutter, misalignment, and variations in pose and\nillumination, there is a considerable shift across modalities since data from\nRGB and depth images are heterogeneous. In this paper, a new cross-modal\ndistillation network is proposed for robust person re-identification between\nRGB and depth sensors. Using a two-step optimization process, the proposed\nmethod transfers supervision between modalities such that similar structural\nfeatures are extracted from both RGB and depth modalities, yielding a\ndiscriminative mapping to a common feature space. Our experiments investigate\nthe influence of the dimensionality of the embedding space, compares transfer\nlearning from depth to RGB and vice versa, and compares against other\nstate-of-the-art cross-modal re-identification methods. Results obtained with\nBIWI and RobotPKU datasets indicate that the proposed method can successfully\ntransfer descriptive structural features from the depth modality to the RGB\nmodality. It can significantly outperform state-of-the-art conventional methods\nand deep neural networks for cross-modal sensing between RGB and depth, with no\nimpact on computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 13:37:10 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 20:00:54 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Hafner", "Frank", ""], ["Bhuiyan", "Amran", ""], ["Kooij", "Julian F. P.", ""], ["Granger", "Eric", ""]]}, {"id": "1810.11649", "submitter": "Viraj Prabhu", "authors": "Utsav Garg, Viraj Prabhu, Deshraj Yadav, Ram Ramrakhya, Harsh Agrawal,\n  Dhruv Batra", "title": "Fabrik: An Online Collaborative Neural Network Editor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Fabrik, an online neural network editor that provides tools to\nvisualize, edit, and share neural networks from within a browser. Fabrik\nprovides a simple and intuitive GUI to import neural networks written in\npopular deep learning frameworks such as Caffe, Keras, and TensorFlow, and\nallows users to interact with, build, and edit models via simple drag and drop.\nFabrik is designed to be framework agnostic and support high interoperability,\nand can be used to export models back to any supported framework. Finally, it\nprovides powerful collaborative features to enable users to iterate over model\ndesign remotely and at scale.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 14:23:38 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Garg", "Utsav", ""], ["Prabhu", "Viraj", ""], ["Yadav", "Deshraj", ""], ["Ramrakhya", "Ram", ""], ["Agrawal", "Harsh", ""], ["Batra", "Dhruv", ""]]}, {"id": "1810.11654", "submitter": "Andriy Myronenko", "authors": "Andriy Myronenko", "title": "3D MRI brain tumor segmentation using autoencoder regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of brain tumors from 3D magnetic resonance images\n(MRIs) is necessary for the diagnosis, monitoring, and treatment planning of\nthe disease. Manual delineation practices require anatomical knowledge, are\nexpensive, time consuming and can be inaccurate due to human error. Here, we\ndescribe a semantic segmentation network for tumor subregion segmentation from\n3D MRIs based on encoder-decoder architecture. Due to a limited training\ndataset size, a variational auto-encoder branch is added to reconstruct the\ninput image itself in order to regularize the shared decoder and impose\nadditional constraints on its layers. The current approach won 1st place in the\nBraTS 2018 challenge.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 14:42:13 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 22:59:52 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 17:04:58 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Myronenko", "Andriy", ""]]}, {"id": "1810.11689", "submitter": "Luca Carlone", "authors": "Siyi Hu and Luca Carlone", "title": "Accelerated Inference in Markov Random Fields via Smooth Riemannian\n  Optimization", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Random Fields (MRFs) are a popular model for several pattern\nrecognition and reconstruction problems in robotics and computer vision.\nInference in MRFs is intractable in general and related work resorts to\napproximation algorithms. Among those techniques, semidefinite programming\n(SDP) relaxations have been shown to provide accurate estimates while scaling\npoorly with the problem size and being typically slow for practical\napplications. Our first contribution is to design a dual ascent method to solve\nstandard SDP relaxations that takes advantage of the geometric structure of the\nproblem to speed up computation. This technique, named Dual Ascent Riemannian\nStaircase (DARS), is able to solve large problem instances in seconds. Our\nsecond contribution is to develop a second and faster approach. The backbone of\nthis second approach is a novel SDP relaxation combined with a fast and\nscalable solver based on smooth Riemannian optimization. We show that this\napproach, named Fast Unconstrained SEmidefinite Solver (FUSES), can solve large\nproblems in milliseconds. Contrarily to local MRF solvers, e.g., loopy belief\npropagation, our approaches do not require an initial guess. Moreover, we\nleverage recent results from optimization theory to provide per-instance\nsub-optimality guarantees. We demonstrate the proposed approaches in\nmulti-class image segmentation problems. Extensive experimental evidence shows\nthat (i) FUSES and DARS produce near-optimal solutions, attaining an objective\nwithin 0.1% of the optimum, (ii) FUSES and DARS are remarkably faster than\ngeneral-purpose SDP solvers, and FUSES is more than two orders of magnitude\nfaster than DARS while attaining similar solution quality, (iii) FUSES is\nfaster than local search methods while being a global solver.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 17:53:43 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 20:28:05 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Hu", "Siyi", ""], ["Carlone", "Luca", ""]]}, {"id": "1810.11690", "submitter": "Anthony Ortiz", "authors": "Dalton Rosario, Anthony Ortiz, Olac Fuentes", "title": "3D Terrain Segmentation in the SWIR Spectrum", "comments": "Published on: IEEE Workshop on Hyperspectral Image and Signal\n  Processing Conference (WHISPERS 2018), Amsterdam, The Netherlands, September\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the automatic 3D terrain segmentation problem using hyperspectral\nshortwave IR (HS-SWIR) imagery and 3D Digital Elevation Models (DEM). The\ndatasets were independently collected, and metadata for the HS-SWIR dataset are\nunavailable. We explore an overall slope of the SWIR spectrum that correlates\nwith the presence of moisture in soil to propose a band ratio test to be used\nas a proxy for soil moisture content to distinguish two broad classes of\nobjects: live vegetation from impermeable manmade surface. We show that image\nbased localization techniques combined with the Optimal Randomized RANdom\nSample Consensus (RANSAC) algorithm achieve precise spatial matches between\nHS-SWIR data of a portion of downtown Los Angeles (LA (USA)) and the Visible\nimage of a geo-registered 3D DEM, covering a wider-area of LA. Our\nspectral-elevation rule based approach yields an overall accuracy of 97.7%,\nsegmenting the object classes into buildings, houses, trees, grass, and\nroads/parking lots.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 17:59:26 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Rosario", "Dalton", ""], ["Ortiz", "Anthony", ""], ["Fuentes", "Olac", ""]]}, {"id": "1810.11710", "submitter": "Matthew Tancik", "authors": "Matthew Tancik, Guy Satat, Ramesh Raskar", "title": "Flash Photography for Data-Driven Hidden Scene Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles, search and rescue personnel, and endoscopes use flash lights to\nlocate, identify, and view objects in their surroundings. Here we show the\nfirst steps of how all these tasks can be done around corners with consumer\ncameras. Recent techniques for NLOS imaging using consumer cameras have not\nbeen able to both localize and identify the hidden object. We introduce a\nmethod that couples traditional geometric understanding and data-driven\ntechniques. To avoid the limitation of large dataset gathering, we train the\ndata-driven models on rendered samples to computationally recover the hidden\nscene on real data. The method has three independent operating modes: 1) a\nregression output to localize a hidden object in 2D, 2) an identification\noutput to identify the object type or pose, and 3) a generative network to\nreconstruct the hidden scene from a new viewpoint. The method is able to\nlocalize 12cm wide hidden objects in 2D with 1.7cm accuracy. The method also\nidentifies the hidden object class with 87.7% accuracy (compared to 33.3%\nrandom accuracy). This paper also provides an analysis on the distribution of\ninformation that encodes the occluded object in the accessible scene. We show\nthat, unlike previously thought, the area that extends beyond the corner is\nessential for accurate object localization and identification.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 21:16:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Tancik", "Matthew", ""], ["Satat", "Guy", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1810.11714", "submitter": "Andrew Hundt", "authors": "Andrew Hundt, Varun Jain, Chia-Hung Lin, Chris Paxton, Gregory D.\n  Hager", "title": "The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints", "comments": "This is a major revision refocusing the topic towards the JHU CoSTAR\n  Block Stacking Dataset, workspace constraints, and a comparison of HyperTrees\n  with hand-designed algorithms. 12 pages, 10 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot can now grasp an object more effectively than ever before, but once\nit has the object what happens next? We show that a mild relaxation of the task\nand workspace constraints implicit in existing object grasping datasets can\ncause neural network based grasping algorithms to fail on even a simple block\nstacking task when executed under more realistic circumstances.\n  To address this, we introduce the JHU CoSTAR Block Stacking Dataset (BSD),\nwhere a robot interacts with 5.1 cm colored blocks to complete an\norder-fulfillment style block stacking task. It contains dynamic scenes and\nreal time-series data in a less constrained environment than comparable\ndatasets. There are nearly 12,000 stacking attempts and over 2 million frames\nof real data. We discuss the ways in which this dataset provides a valuable\nresource for a broad range of other topics of investigation.\n  We find that hand-designed neural networks that work on prior datasets do not\ngeneralize to this task. Thus, to establish a baseline for this dataset, we\ndemonstrate an automated search of neural network based models using a novel\nmultiple-input HyperTree MetaModel, and find a final model which makes\nreasonable 3D pose predictions for grasping and stacking on our dataset.\n  The CoSTAR BSD, code, and instructions are available at\nhttps://sites.google.com/site/costardataset.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 21:26:42 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 23:17:17 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Hundt", "Andrew", ""], ["Jain", "Varun", ""], ["Lin", "Chia-Hung", ""], ["Paxton", "Chris", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1810.11718", "submitter": "Branko Brklja\\v{c}", "authors": "Branko Brklja\\v{c} and \\v{Z}eljen Trpovski", "title": "On the role of ML estimation and Bregman divergences in sparse\n  representation of covariance and precision matrices", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation of structured signals requires modelling strategies\nthat maintain specific signal properties, in addition to preserving original\ninformation content and achieving simpler signal representation. Therefore, the\nmajor design challenge is to introduce adequate problem formulations and offer\nsolutions that will efficiently lead to desired representations. In this\ncontext, sparse representation of covariance and precision matrices, which\nappear as feature descriptors or mixture model parameters, respectively, will\nbe in the main focus of this paper.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 22:06:29 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Brklja\u010d", "Branko", ""], ["Trpovski", "\u017deljen", ""]]}, {"id": "1810.11731", "submitter": "Ahmad Al-Kabbany", "authors": "Marian K.Y. Boktor and Ahmad Al-Kabbany and Radwa Khalil and Said\n  El-Khamy", "title": "Real-time Action Recognition with Dissimilarity-based Training of\n  Specialized Module Networks", "comments": "8 pages, 5 Tables, 4 Figures, Appeared in CAINE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of real-time action recognition in trimmed\nvideos, for which deep neural networks have defined the state-of-the-art\nperformance in the recent literature. For attaining higher recognition\naccuracies with efficient computations, researchers have addressed the various\naspects of limitations in the recognition pipeline. This includes network\narchitecture, the number of input streams (where additional streams augment the\ncolor information), the cost function to be optimized, in addition to others.\nThe literature has always aimed, though, at assigning the adopted network (or\nnetworks, in case of multiple streams) the task of recognizing the whole number\nof action classes in the dataset at hand. We propose to train multiple\nspecialized module networks instead. Each module is trained to recognize a\nsubset of the action classes. Towards this goal, we present a\ndissimilarity-based optimized procedure for distributing the action classes\nover the modules, which can be trained simultaneously offline. On two standard\ndatasets--UCF-101 and HMDB-51--the proposed method demonstrates a comparable\nperformance, that is superior in some aspects, to the state-of-the-art, and\nthat satisfies the real-time constraint. We achieved 72.5\\% accuracy on the\nchallenging HMDB-51 dataset. By assigning fewer and unalike classes to each\nmodule network, this research paves the way to benefit from light-weight\narchitectures without compromising recognition accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 23:29:13 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Boktor", "Marian K. Y.", ""], ["Al-Kabbany", "Ahmad", ""], ["Khalil", "Radwa", ""], ["El-Khamy", "Said", ""]]}, {"id": "1810.11770", "submitter": "Geesara Kulathunga Kulathunga", "authors": "Vladislav Ostankovich, Geesara Prathap, Ilya Afanasyev", "title": "Towards Human Pulse Rate Estimation from Face Video: Automatic Component\n  Selection and Comparison of Blind Source Separation Methods", "comments": null, "journal-ref": null, "doi": "10.1109/IS.2018.8710532", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human heartbeat can be measured using several different ways appropriately\nbased on the patient condition which includes contact base such as measured by\nusing instruments and non-contact base such as computer vision assisted\ntechniques. Non-contact based approached are getting popular due to those\ntechniques are capable of mitigating some of the limitations of contact-based\ntechniques especially in clinical section. However, existing vision guided\napproaches are not able to prove high accurate result due to various reason\nsuch as the property of camera, illumination changes, skin tones in face image,\netc. We propose a technique that uses video as an input and returns pulse rate\nin output. Initially, key point detection is carried out on two facial\nsubregions: forehead and nose-mouth. After removing unstable features, the\ntemporal filtering is applied to isolate frequencies of interest. Then four\ncomponent analysis methods are employed in order to distinguish the\ncardiovascular pulse signal from extraneous noise caused by respiration,\nvestibular activity and other changes in facial expression. Afterwards,\nproposed peak detection technique is applied for each component which extracted\nfrom one of the four different component selection algorithms. This will enable\nto locate the positions of peaks in each component. Proposed automatic\ncomponents selection technique is employed in order to select an optimal\ncomponent which will be used to calculate the heartbeat. Finally, we conclude\nwith a comparison of four component analysis methods (PCA, FastICA, JADE,\nSHIBBS), processing face video datasets of fifteen volunteers with verification\nby an ECG/EKG Workstation as a ground truth.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 07:15:35 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ostankovich", "Vladislav", ""], ["Prathap", "Geesara", ""], ["Afanasyev", "Ilya", ""]]}, {"id": "1810.11780", "submitter": "Naveed Akhtar Dr.", "authors": "ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, Mubarak Shah", "title": "Deep Affinity Network for Multiple Object Tracking", "comments": "To appear in IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) plays an important role in solving many\nfundamental problems in video analysis in computer vision. Most MOT methods\nemploy two steps: Object Detection and Data Association. The first step detects\nobjects of interest in every frame of a video, and the second establishes\ncorrespondence between the detected objects in different frames to obtain their\ntracks. Object detection has made tremendous progress in the last few years due\nto deep learning. However, data association for tracking still relies on hand\ncrafted constraints such as appearance, motion, spatial proximity, grouping\netc. to compute affinities between the objects in different frames. In this\npaper, we harness the power of deep learning for data association in tracking\nby jointly modelling object appearances and their affinities between different\nframes in an end-to-end fashion. The proposed Deep Affinity Network (DAN)\nlearns compact; yet comprehensive features of pre-detected objects at several\nlevels of abstraction, and performs exhaustive pairing permutations of those\nfeatures in any two frames to infer object affinities. DAN also accounts for\nmultiple objects appearing and disappearing between video frames. We exploit\nthe resulting efficient affinity computations to associate objects in the\ncurrent frame deep into the previous frames for reliable on-line tracking. Our\ntechnique is evaluated on popular multiple object tracking challenges MOT15,\nMOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics\ndemonstrates that our approach is among the best performing techniques on the\nleader board for these challenges. The open source implementation of our work\nis available at https://github.com/shijieS/SST.git.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 09:07:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 01:09:38 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Sun", "ShiJie", ""], ["Akhtar", "Naveed", ""], ["Song", "HuanSheng", ""], ["Mian", "Ajmal", ""], ["Shah", "Mubarak", ""]]}, {"id": "1810.11794", "submitter": "Haisheng Su", "authors": "Haisheng Su, Xu Zhao, Tianwei Lin", "title": "Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action\n  Localization", "comments": "Accepted at ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised temporal action localization, which aims at temporally\nlocating action instances in untrimmed videos using only video-level class\nlabels during training, is an important yet challenging problem in video\nanalysis. Many current methods adopt the \"localization by classification\"\nframework: first do video classification, then locate temporal area\ncontributing to the results most. However, this framework fails to locate the\nentire action instances and gives little consideration to the local context. In\nthis paper, we present a novel architecture called Cascaded Pyramid Mining\nNetwork (CPMN) to address these issues using two effective modules. First, to\ndiscover the entire temporal interval of specific action, we design a two-stage\ncascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where\nnew and complementary regions are mined through feeding the erased feature maps\nof discovered regions back to the system. Second, to exploit hierarchical\ncontextual information in videos and reduce missing detections, we design a\npyramid module which produces a scale-invariant attention map through combining\nthe feature maps from different levels. Final, we aggregate the results of two\nmodules to perform action localization via locating high score areas in\ntemporal Class Activation Sequence (CAS). Extensive experiments conducted on\nTHUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 11:10:33 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Su", "Haisheng", ""], ["Zhao", "Xu", ""], ["Lin", "Tianwei", ""]]}, {"id": "1810.11809", "submitter": "Mingkui Tan", "authors": "Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo,\n  Qingyao Wu, Junzhou Huang, Jinhui Zhu", "title": "Discrimination-aware Channel Pruning for Deep Neural Networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning is one of the predominant approaches for deep model\ncompression. Existing pruning methods either train from scratch with sparsity\nconstraints on channels, or minimize the reconstruction error between the\npre-trained feature maps and the compressed ones. Both strategies suffer from\nsome limitations: the former kind is computationally expensive and difficult to\nconverge, whilst the latter kind optimizes the reconstruction error but ignores\nthe discriminative power of channels. To overcome these drawbacks, we\ninvestigate a simple-yet-effective method, called discrimination-aware channel\npruning, to choose those channels that really contribute to discriminative\npower. To this end, we introduce additional losses into the network to increase\nthe discriminative power of intermediate layers and then select the most\ndiscriminative channels for each layer by considering the additional loss and\nthe reconstruction error. Last, we propose a greedy algorithm to conduct\nchannel selection and parameter optimization in an iterative way. Extensive\nexperiments demonstrate the effectiveness of our method. For example, on\nILSVRC-12, our pruned ResNet-50 with 30% reduction of channels even outperforms\nthe original model by 0.39% in top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 13:18:50 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 05:26:33 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 01:44:08 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Zhuang", "Zhuangwei", ""], ["Tan", "Mingkui", ""], ["Zhuang", "Bohan", ""], ["Liu", "Jing", ""], ["Guo", "Yong", ""], ["Wu", "Qingyao", ""], ["Huang", "Junzhou", ""], ["Zhu", "Jinhui", ""]]}, {"id": "1810.11819", "submitter": "Fengchao Xiong", "authors": "Kun Qian, Jun Zhou, Fengchao Xiong, Huixin Zhou, and Juan Du", "title": "Object Tracking in Hyperspectral Videos with Convolutional Features and\n  Kernelized Correlation Filter", "comments": "Accepted by ICSM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Target tracking in hyperspectral videos is a new research topic. In this\npaper, a novel method based on convolutional network and Kernelized Correlation\nFilter (KCF) framework is presented for tracking objects of interest in\nhyperspectral videos. We extract a set of normalized three-dimensional cubes\nfrom the target region as fixed convolution filters which contain spectral\ninformation surrounding a target. The feature maps generated by convolutional\noperations are combined to form a three-dimensional representation of an\nobject, thereby providing effective encoding of local spectral-spatial\ninformation. We show that a simple two-layer convolutional networks is\nsufficient to learn robust representations without the need of offline training\nwith a large dataset. In the tracking step, KCF is adopted to distinguish\ntargets from neighboring environment. Experimental results demonstrate that the\nproposed method performs well on sample hyperspectral videos, and outperforms\nseveral state-of-the-art methods tested on grayscale and color videos in the\nsame scene.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 14:35:26 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Qian", "Kun", ""], ["Zhou", "Jun", ""], ["Xiong", "Fengchao", ""], ["Zhou", "Huixin", ""], ["Du", "Juan", ""]]}, {"id": "1810.11823", "submitter": "Christian Kehl", "authors": "Christian Kehl and Wail Mustafa and Jan Kehres and Anders Bjorholm\n  Dahl and Ulrik Lund Olsen", "title": "Multi-Spectral Imaging via Computed Tomography (MUSIC) - Comparing\n  Unsupervised Spectral Segmentations for Material Differentiation", "comments": "21 pages, 24 figures (in articles), includes 2 appendices with 8\n  additional figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-spectral computed tomography is an emerging technology for the\nnon-destructive identification of object materials and the study of their\nphysical properties. Applications of this technology can be found in various\nscientific and industrial contexts, such as luggage scanning at airports.\nMaterial distinction and its identification is challenging, even with spectral\nx-ray information, due to acquisition noise, tomographic reconstruction\nartefacts and scanning setup application constraints. We present MUSIC - and\nopen access multi-spectral CT dataset in 2D and 3D - to promote further\nresearch in the area of material identification. We demonstrate the value of\nthis dataset on the image analysis challenge of object segmentation purely\nbased on the spectral response of its composing materials. In this context, we\ncompare the segmentation accuracy of fast adaptive mean shift (FAMS) and\nunconstrained graph cuts on both datasets. We further discuss the impact of\nreconstruction artefacts and segmentation controls on the achievable results.\nDataset, related software packages and further documentation are made available\nto the imaging community in an open-access manner to promote further\ndata-driven research on the subject\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 15:28:36 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kehl", "Christian", ""], ["Mustafa", "Wail", ""], ["Kehres", "Jan", ""], ["Dahl", "Anders Bjorholm", ""], ["Olsen", "Ulrik Lund", ""]]}, {"id": "1810.11834", "submitter": "Chunwei Tian", "authors": "Chunwei Tian, Yong Xu, Lunke Fei, Junqian Wang, Jie Wen and Nan Luo", "title": "Enhanced CNN for image denoising", "comments": "CAAI Transactions on Intelligence Technology[J], 2019", "journal-ref": null, "doi": "10.1049/trit.2018.1054", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to flexible architectures of deep convolutional neural networks (CNNs),\nCNNs are successfully used for image denoising. However, they suffer from the\nfollowing drawbacks: (i) deep network architecture is very difficult to train.\n(ii) Deeper networks face the challenge of performance saturation. In this\nstudy, the authors propose a novel method called enhanced convolutional neural\ndenoising network (ECNDNet). Specifically, they use residual learning and batch\nnormalisation techniques to address the problem of training difficulties and\naccelerate the convergence of the network. In addition, dilated convolutions\nare used in the proposed network to enlarge the context information and reduce\nthe computational cost. Extensive experiments demonstrate that the ECNDNet\noutperforms the state-of-the-art methods for image denoising.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 17:00:42 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 03:06:44 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 04:22:02 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 06:29:31 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Tian", "Chunwei", ""], ["Xu", "Yong", ""], ["Fei", "Lunke", ""], ["Wang", "Junqian", ""], ["Wen", "Jie", ""], ["Luo", "Nan", ""]]}, {"id": "1810.11856", "submitter": "Shinya Sumikura", "authors": "Shinya Sumikura, Ken Sakurada, Nobuo Kawaguchi and Ryosuke Nakamura", "title": "Scale Estimation of Monocular SfM for a Multi-modal Stereo Camera", "comments": "Accepted to ACCV 2018, please see the additional results here:\n  http://youtu.be/xOLtvMZJseU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method of estimating the absolute scale of\nmonocular SfM for a multi-modal stereo camera. In the fields of computer vision\nand robotics, scale estimation for monocular SfM has been widely investigated\nin order to simplify systems. This paper addresses the scale estimation problem\nfor a stereo camera system in which two cameras capture different spectral\nimages (e.g., RGB and FIR), whose feature points are difficult to directly\nmatch using descriptors. Furthermore, the number of matching points between FIR\nimages can be comparatively small, owing to the low resolution and lack of\nthermal scene texture. To cope with these difficulties, the proposed method\nestimates the scale parameter using batch optimization, based on the epipolar\nconstraint of a small number of feature correspondences between the invisible\nlight images. The accuracy and numerical stability of the proposed method are\nverified by synthetic and real image experiments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 18:34:50 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sumikura", "Shinya", ""], ["Sakurada", "Ken", ""], ["Kawaguchi", "Nobuo", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1810.11868", "submitter": "Arijit Patra", "authors": "Arijit Patra, J. A. Noble", "title": "Sequential anatomy localization in fetal echocardiography videos", "comments": "To appear in ISBI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal heart motion is an important diagnostic indicator for structural\ndetection and functional assessment of congenital heart disease. We propose an\napproach towards integrating deep convolutional and recurrent architectures\nthat utilize localized spatial and temporal features of different anatomical\nsubstructures within a global spatiotemporal context for interpretation of\nfetal echocardiography videos. We formulate our task as a cardiac structure\nlocalization problem with convolutional architectures for aggregating global\nspatial context and detecting anatomical structures on spatial region\nproposals. This information is aggregated temporally by recurrent architectures\nto quantify the progressive motion patterns. We experimentally show that the\nresulting architecture combines anatomical landmark detection at the\nframe-level over multiple video sequences-with temporal progress of the\nassociated anatomical motions to encode local spatiotemporal fetal heart\ndynamics and is validated on a real-world clinical dataset.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 19:36:30 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 17:27:52 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Patra", "Arijit", ""], ["Noble", "J. A.", ""]]}, {"id": "1810.11919", "submitter": "Seonghyeon Nam", "authors": "Seonghyeon Nam, Yunji Kim, Seon Joo Kim", "title": "Text-Adaptive Generative Adversarial Networks: Manipulating Images with\n  Natural Language", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of manipulating images using natural\nlanguage description. Our task aims to semantically modify visual attributes of\nan object in an image according to the text describing the new visual\nappearance. Although existing methods synthesize images having new attributes,\nthey do not fully preserve text-irrelevant contents of the original image. In\nthis paper, we propose the text-adaptive generative adversarial network (TAGAN)\nto generate semantically manipulated images while preserving text-irrelevant\ncontents. The key to our method is the text-adaptive discriminator that creates\nword-level local discriminators according to input text to classify\nfine-grained attributes independently. With this discriminator, the generator\nlearns to generate images where only regions that correspond to the given text\nare modified. Experimental results show that our method outperforms existing\nmethods on CUB and Oxford-102 datasets, and our results were mostly preferred\non a user study. Extensive analysis shows that our method is able to\neffectively disentangle visual attributes and produce pleasing outputs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 01:47:09 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 02:01:11 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Nam", "Seonghyeon", ""], ["Kim", "Yunji", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1810.11957", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi, Haris Vikalo", "title": "Evolutionary Self-Expressive Models for Subspace Clustering", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, Special\n  Issue on Data Science: Robust Subspace Learning and Tracking, vol. 12, no. 6,\n  December 2018", "doi": "10.1109/JSTSP.2018.2877478", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of organizing data that evolves over time into clusters is\nencountered in a number of practical settings. We introduce evolutionary\nsubspace clustering, a method whose objective is to cluster a collection of\nevolving data points that lie on a union of low-dimensional evolving subspaces.\nTo learn the parsimonious representation of the data points at each time step,\nwe propose a non-convex optimization framework that exploits the\nself-expressiveness property of the evolving data while taking into account\nrepresentation from the preceding time step. To find an approximate solution to\nthe aforementioned non-convex optimization problem, we develop a scheme based\non alternating minimization that both learns the parsimonious representation as\nwell as adaptively tunes and infers a smoothing parameter reflective of the\nrate of data evolution. The latter addresses a fundamental challenge in\nevolutionary clustering -- determining if and to what extent one should\nconsider previous clustering solutions when analyzing an evolving data\ncollection. Our experiments on both synthetic and real-world datasets\ndemonstrate that the proposed framework outperforms state-of-the-art static\nsubspace clustering algorithms and existing evolutionary clustering schemes in\nterms of both accuracy and running time, in a range of scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 05:08:16 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1810.11981", "submitter": "Lianghua Huang Dr.", "authors": "Lianghua Huang, Xin Zhao and Kaiqi Huang", "title": "GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in\n  the Wild", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2957464", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce here a large tracking database that offers an unprecedentedly\nwide coverage of common moving objects in the wild, called GOT-10k.\nSpecifically, GOT-10k is built upon the backbone of WordNet structure and it\npopulates the majority of over 560 classes of moving objects and 87 motion\npatterns, magnitudes wider than the most recent similar-scale counterparts. The\ncontributions of this paper are summarized in the following: (1) GOT-10k offers\nover 10,000 video segments with more than 1.5 million manually labeled bounding\nboxes, enabling unified training and stable evaluation of deep trackers. (2)\nGOT-10k is by far the first video trajectory dataset that uses the semantic\nhierarchy of WordNet to guide class population. (3) For the first time, GOT-10k\nintroduces the one-shot protocol for tracker evaluation, where the training and\ntest classes are zero-overlapped. The protocol avoids biased evaluation results\ntowards familiar objects and it promotes generalization in tracker development.\n(4) We conduct extensive tracking experiments with 39 typical tracking\nalgorithms on GOT-10k and analyze their results in this paper. (5) Finally, we\ndevelop a comprehensive platform for the tracking community that offers\nfull-featured evaluation toolkits, an online evaluation server, and a\nresponsive leaderboard. The annotations of GOT-10k's test data are kept private\nto avoid tuning parameters on it. The database, toolkits, evaluation server and\nbaseline results are available at http://got-10k.aitestunion.com.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 07:22:46 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 05:57:02 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 07:49:29 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Huang", "Lianghua", ""], ["Zhao", "Xin", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1810.12000", "submitter": "Danfeng Hong", "authors": "Danfeng Hong, Naoto Yokoya, Jocelyn Chanussot, Xiao Xiang Zhu", "title": "An Augmented Linear Mixing Model to Address Spectral Variability for\n  Hyperspectral Unmixing", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2019, 28(4): 1923-1938", "doi": "10.1109/TIP.2018.2878958", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imagery collected from airborne or satellite sources inevitably\nsuffers from spectral variability, making it difficult for spectral unmixing to\naccurately estimate abundance maps. The classical unmixing model, the linear\nmixing model (LMM), generally fails to handle this sticky issue effectively. To\nthis end, we propose a novel spectral mixture model, called the augmented\nlinear mixing model (ALMM), to address spectral variability by applying a\ndata-driven learning strategy in inverse problems of hyperspectral unmixing.\nThe proposed approach models the main spectral variability (i.e., scaling\nfactors) generated by variations in illumination or typography separately by\nmeans of the endmember dictionary. It then models other spectral variabilities\ncaused by environmental conditions (e.g., local temperature and humidity,\natmospheric effects) and instrumental configurations (e.g., sensor noise), as\nwell as material nonlinear mixing effects, by introducing a spectral\nvariability dictionary. To effectively run the data-driven learning strategy,\nwe also propose a reasonable prior knowledge for the spectral variability\ndictionary, whose atoms are assumed to be low-coherent with spectral signatures\nof endmembers, which leads to a well-known low coherence dictionary learning\nproblem. Thus, a dictionary learning technique is embedded in the framework of\nspectral unmixing so that the algorithm can learn the spectral variability\ndictionary and estimate the abundance maps simultaneously. Extensive\nexperiments on synthetic and real datasets are performed to demonstrate the\nsuperiority and effectiveness of the proposed method in comparison with\nprevious state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 08:35:42 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1810.12043", "submitter": "Ahmed Karam Eldaly MSc", "authors": "Ahmed Karam Eldaly, Yoann Altmann, Ahsan Akram, Antonios Perperidis,\n  Kevin Dhaliwal, Stephen McLaughlin", "title": "Patch-Based Sparse Representation For Bacterial Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an unsupervised approach for bacterial detection in\noptical endomicroscopy images. This approach splits each image into a set of\noverlapping patches and assumes that observed intensities are linear\ncombinations of the actual intensity values associated with background image\nstructures, corrupted by additive Gaussian noise and potentially by a sparse\noutlier term modelling anomalies (which are considered to be candidate\nbacteria). The actual intensity term representing background structures is\nmodelled as a linear combination of a few atoms drawn from a dictionary which\nis learned from bacteria-free data and then fixed while analyzing new images.\nThe bacteria detection task is formulated as a minimization problem and an\nalternating direction method of multipliers (ADMM) is then used to estimate the\nunknown parameters. Simulations conducted using two ex vivo lung datasets show\ngood detection and correlation performance between bacteria counts identified\nby a trained clinician and those of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 10:31:06 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 11:00:16 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Eldaly", "Ahmed Karam", ""], ["Altmann", "Yoann", ""], ["Akram", "Ahsan", ""], ["Perperidis", "Antonios", ""], ["Dhaliwal", "Kevin", ""], ["McLaughlin", "Stephen", ""]]}, {"id": "1810.12061", "submitter": "Jianxiong Shen", "authors": "Zhenshen Qu, Jianxiong Shen, Ruikun Li, Junyu Liu, and Qiuyu Guan", "title": "PartsNet: A Unified Deep Network for Automotive Engine Precision Parts\n  Defect Detection", "comments": "2nd International Conference on Computer Science and Artificial\n  Intelligence (CSAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defect detection is a basic and essential task in automatic parts production,\nespecially for automotive engine precision parts. In this paper, we propose a\nnew idea to construct a deep convolutional network combining related knowledge\nof feature processing and the representation ability of deep learning. Our\nalgorithm consists of a pixel-wise segmentation Deep Neural Network (DNN) and a\nfeature refining network. The fully convolutional DNN is presented to learn\nbasic features of parts defects. After that, several typical traditional\nmethods which are used to refine the segmentation results are transformed into\nconvolutional manners and integrated. We assemble these methods as a shallow\nnetwork with fixed weights and empirical thresholds. These thresholds are then\nreleased to enhance its adaptation ability and realize end-to-end training.\nTesting results on different datasets show that the proposed method has good\nportability and outperforms the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 11:31:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Qu", "Zhenshen", ""], ["Shen", "Jianxiong", ""], ["Li", "Ruikun", ""], ["Liu", "Junyu", ""], ["Guan", "Qiuyu", ""]]}, {"id": "1810.12091", "submitter": "Shelan Jeawak", "authors": "Shelan S. Jeawak, Christopher B. Jones, and Steven Schockaert", "title": "Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 12:22:34 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Jeawak", "Shelan S.", ""], ["Jones", "Christopher B.", ""], ["Schockaert", "Steven", ""]]}, {"id": "1810.12121", "submitter": "Fidel Alejandro Guerrero Pe\\~na", "authors": "Fidel A. Guerrero Pe\\~na, Pedro D. Marrero Fern\\'andez, Tsang Ing Ren,\n  Jorge J. G. Leandro, Ricardo Nishihara", "title": "Burst ranking for blind multi-image deblurring", "comments": "Submitted to IEEE Transactions on Image Processing. 11 pages, 9\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new incremental aggregation algorithm for multi-image deblurring\nwith automatic image selection. The primary motivation is that current bursts\ndeblurring methods do not handle well situations in which misalignment or\nout-of-context frames are present in the burst. These real-life situations\nresult in poor reconstructions or manual selection of the images that will be\nused to deblur. Automatically selecting best frames within the burst to improve\nthe base reconstruction is challenging because the amount of possible images\nfusions is equal to the power set cardinal. Here, we approach the multi-image\ndeblurring problem as a two steps process. First, we successfully learn a\ncomparison function to rank a burst of images using a deep convolutional neural\nnetwork. Then, an incremental Fourier burst accumulation with a reconstruction\ndegradation mechanism is applied fusing only less blurred images that are\nsufficient to maximize the reconstruction quality. Experiments with the\nproposed algorithm have shown superior results when compared to other similar\napproaches, outperforming other methods described in the literature in\npreviously described situations. We validate our findings on several synthetic\nand real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:38:54 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 23:56:41 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Pe\u00f1a", "Fidel A. Guerrero", ""], ["Fern\u00e1ndez", "Pedro D. Marrero", ""], ["Ren", "Tsang Ing", ""], ["Leandro", "Jorge J. G.", ""], ["Nishihara", "Ricardo", ""]]}, {"id": "1810.12126", "submitter": "Federico Angelini", "authors": "Federico Angelini, Zeyu Fu, Yang Long, Ling Shao, Syed Mohsen Naqvi", "title": "ActionXPose: A Novel 2D Multi-view Pose-based Algorithm for Real-time\n  Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ActionXPose, a novel 2D pose-based algorithm for posture-level\nHuman Action Recognition (HAR). The proposed approach exploits 2D human poses\nprovided by OpenPose detector from RGB videos. ActionXPose aims to process\nposes data to be provided to a Long Short-Term Memory Neural Network and to a\n1D Convolutional Neural Network, which solve the classification problem.\nActionXPose is one of the first algorithms that exploits 2D human poses for\nHAR. The algorithm has real-time performance and it is robust to camera\nmovings, subject proximity changes, viewpoint changes, subject appearance\nchanges and provide high generalization degree. In fact, extensive simulations\nshow that ActionXPose can be successfully trained using different datasets at\nonce. State-of-the-art performance on popular datasets for posture-related HAR\nproblems (i3DPost, KTH) are provided and results are compared with those\nobtained by other methods, including the selected ActionXPose baseline.\nMoreover, we also proposed two novel datasets called MPOSE and ISLD recorded in\nour Intelligent Sensing Lab, to show ActionXPose generalization performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:49:07 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Angelini", "Federico", ""], ["Fu", "Zeyu", ""], ["Long", "Yang", ""], ["Shao", "Ling", ""], ["Naqvi", "Syed Mohsen", ""]]}, {"id": "1810.12142", "submitter": "Gabriele Valvano", "authors": "Gabriele Valvano, Andrea Leo, Daniele Della Latta, Nicola Martini,\n  Gianmarco Santini, Dante Chiappino and Emiliano Ricciardi", "title": "Unsupervised Data Selection for Supervised Learning", "comments": "Technical Report --- 8 pages, 3 figures New tests demonstrated that\n  the system, as is, is not able to create reproducible results. Further study\n  on the topic should be done", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research put a big effort in the development of deep learning\narchitectures and optimizers obtaining impressive results in areas ranging from\nvision to language processing. However little attention has been addressed to\nthe need of a methodological process of data collection. In this work we\nhypothesize that high quality data for supervised learning can be selected in\nan unsupervised manner and that by doing so one can obtain models capable to\ngeneralize better than in the case of random training set construction.\nHowever, preliminary results are not robust and further studies on the subject\nshould be carried out.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:24:31 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:44:25 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Valvano", "Gabriele", ""], ["Leo", "Andrea", ""], ["Della Latta", "Daniele", ""], ["Martini", "Nicola", ""], ["Santini", "Gianmarco", ""], ["Chiappino", "Dante", ""], ["Ricciardi", "Emiliano", ""]]}, {"id": "1810.12145", "submitter": "Jinlu Liu", "authors": "Gang Yang and Jinlu Liu and Xirong Li", "title": "Imagination Based Sample Construction for Zero-Shot Learning", "comments": "Accepted as a short paper in ACM SIGIR 2018", "journal-ref": null, "doi": "10.1145/3209978.3210096", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) which aims to recognize unseen classes with no\nlabeled training sample, efficiently tackles the problem of missing labeled\ndata in image retrieval. Nowadays there are mainly two types of popular methods\nfor ZSL to recognize images of unseen classes: probabilistic reasoning and\nfeature projection. Different from these existing types of methods, we propose\na new method: sample construction to deal with the problem of ZSL. Our proposed\nmethod, called Imagination Based Sample Construction (IBSC), innovatively\nconstructs image samples of target classes in feature space by mimicking human\nassociative cognition process. Based on an association between attribute and\nfeature, target samples are constructed from different parts of various\nsamples. Furthermore, dissimilarity representation is employed to select\nhigh-quality constructed samples which are used as labeled data to train a\nspecific classifier for those unseen classes. In this way, zero-shot learning\nis turned into a supervised learning problem. As far as we know, it is the\nfirst work to construct samples for ZSL thus, our work is viewed as a baseline\nfor future sample construction methods. Experiments on four benchmark datasets\nshow the superiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:26:28 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Yang", "Gang", ""], ["Liu", "Jinlu", ""], ["Li", "Xirong", ""]]}, {"id": "1810.12155", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Stephen Lin, Sangryul Jeon, Dongbo Min, and Kwanghoon\n  Sohn", "title": "Recurrent Transformer Networks for Semantic Correspondence", "comments": "Neural Information Processing Systems (NIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present recurrent transformer networks (RTNs) for obtaining dense\ncorrespondences between semantically similar images. Our networks accomplish\nthis through an iterative process of estimating spatial transformations between\nthe input images and using these transformations to generate aligned\nconvolutional activations. By directly estimating the transformations between\nan image pair, rather than employing spatial transformer networks to\nindependently normalize each individual image, we show that greater accuracy\ncan be achieved. This process is conducted in a recursive manner to refine both\nthe transformation estimates and the feature representations. In addition, a\ntechnique is presented for weakly-supervised training of RTNs that is based on\na proposed classification loss. With RTNs, state-of-the-art performance is\nattained on several benchmarks for semantic correspondence.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:37:29 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kim", "Seungryong", ""], ["Lin", "Stephen", ""], ["Jeon", "Sangryul", ""], ["Min", "Dongbo", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1810.12163", "submitter": "Stuart Golodetz", "authors": "Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin,\n  Victor A. Prisacariu, Luigi Di Stefano, Philip H. S. Torr", "title": "Real-Time RGB-D Camera Pose Estimation in Novel Scenes using a\n  Relocalisation Cascade", "comments": "Tommaso Cavallari, Stuart Golodetz, Nicholas Lord and Julien Valentin\n  assert joint first authorship", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2915068", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera pose estimation is an important problem in computer vision. Common\ntechniques either match the current image against keyframes with known poses,\ndirectly regress the pose, or establish correspondences between keypoints in\nthe image and points in the scene to estimate the pose. In recent years,\nregression forests have become a popular alternative to establish such\ncorrespondences. They achieve accurate results, but have traditionally needed\nto be trained offline on the target scene, preventing relocalisation in new\nenvironments. Recently, we showed how to circumvent this limitation by adapting\na pre-trained forest to a new scene on the fly. The adapted forests achieved\nrelocalisation performance that was on par with that of offline forests, and\nour approach was able to estimate the camera pose in close to real time. In\nthis paper, we present an extension of this work that achieves significantly\nbetter relocalisation performance whilst running fully in real time. To achieve\nthis, we make several changes to the original approach: (i) instead of\naccepting the camera pose hypothesis without question, we make it possible to\nscore the final few hypotheses using a geometric approach and select the most\npromising; (ii) we chain several instantiations of our relocaliser together in\na cascade, allowing us to try faster but less accurate relocalisation first,\nonly falling back to slower, more accurate relocalisation as necessary; and\n(iii) we tune the parameters of our cascade to achieve effective overall\nperformance. These changes allow us to significantly improve upon the\nperformance our original state-of-the-art method was able to achieve on the\nwell-known 7-Scenes and Stanford 4 Scenes benchmarks. As additional\ncontributions, we present a way of visualising the internal behaviour of our\nforests and show how to entirely circumvent the need to pre-train a forest on a\ngeneric scene.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:48:28 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 14:16:22 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Cavallari", "Tommaso", ""], ["Golodetz", "Stuart", ""], ["Lord", "Nicholas A.", ""], ["Valentin", "Julien", ""], ["Prisacariu", "Victor A.", ""], ["Di Stefano", "Luigi", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1810.12171", "submitter": "Lea M\\\"uller", "authors": "Lea M\\\"uller (1), Maha Shadaydeh (1), Martin Th\\\"ummel (1), Thomas\n  Kessler (2), Dana Schneider (2), Joachim Denzler (1 and 3) ((1) Computer\n  Vision Group, Friedrich Schiller University of Jena, (2) Department of Social\n  Psychology, Friedrich Schiller University of Jena, (3) Michael Stifel Center,\n  Jena)", "title": "Causal Inference in Nonverbal Dyadic Communication with Relevant\n  Interval Selection and Granger Causality", "comments": "Nonverbal emotional communication, Granger causality, maximally\n  coherent intervals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human nonverbal emotional communication in dyadic dialogs is a process of\nmutual influence and adaptation. Identifying the direction of influence, or\ncause-effect relation between participants is a challenging task, due to two\nmain obstacles. First, distinct emotions might not be clearly visible. Second,\nparticipants cause-effect relation is transient and variant over time. In this\npaper, we address these difficulties by using facial expressions that can be\npresent even when strong distinct facial emotions are not visible. We also\npropose to apply a relevant interval selection approach prior to causal\ninference to identify those transient intervals where adaptation process\noccurs. To identify the direction of influence, we apply the concept of Granger\ncausality to the time series of facial expressions on the set of relevant\nintervals. We tested our approach on synthetic data and then applied it to\nnewly, experimentally obtained data. Here, we were able to show that a more\nsensitive facial expression detection algorithm and a relevant interval\ndetection approach is most promising to reveal the cause-effect pattern for\ndyadic communication in various instructed interaction conditions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:58:16 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["M\u00fcller", "Lea", "", "1 and 3"], ["Shadaydeh", "Maha", "", "1 and 3"], ["Th\u00fcmmel", "Martin", "", "1 and 3"], ["Kessler", "Thomas", "", "1 and 3"], ["Schneider", "Dana", "", "1 and 3"], ["Denzler", "Joachim", "", "1 and 3"]]}, {"id": "1810.12185", "submitter": "Ilkay Oksuz", "authors": "Ilkay Oksuz, Bram Ruijsink, Esther Puyol-Anton, James Clough, Gastao\n  Cruz, Aurelien Bustin, Claudia Prieto, Rene Botnar, Daniel Rueckert, Julia A.\n  Schnabel, Andrew P. King", "title": "Automatic CNN-based detection of cardiac MR motion artefacts using\n  k-space data augmentation and curriculum learning", "comments": "Submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good quality of medical images is a prerequisite for the success of\nsubsequent image analysis pipelines. Quality assessment of medical images is\ntherefore an essential activity and for large population studies such as the UK\nBiobank (UKBB), manual identification of artefacts such as those caused by\nunanticipated motion is tedious and time-consuming. Therefore, there is an\nurgent need for automatic image quality assessment techniques. In this paper,\nwe propose a method to automatically detect the presence of motion-related\nartefacts in cardiac magnetic resonance (CMR) cine images. We compare two deep\nlearning architectures to classify poor quality CMR images: 1) 3D\nspatio-temporal Convolutional Neural Networks (3D-CNN), 2) Long-term Recurrent\nConvolutional Network (LRCN). Though in real clinical setup motion artefacts\nare common, high-quality imaging of UKBB, which comprises cross-sectional\npopulation data of volunteers who do not necessarily have health problems\ncreates a highly imbalanced classification problem. Due to the high number of\ngood quality images compared to the relatively low number of images with motion\nartefacts, we propose a novel data augmentation scheme based on synthetic\nartefact creation in k-space. We also investigate a learning approach using a\npredetermined curriculum based on synthetic artefact severity. We evaluate our\npipeline on a subset of the UK Biobank data set consisting of 3510 CMR images.\nThe LRCN architecture outperformed the 3D-CNN architecture and was able to\ndetect 2D+time short axis images with motion artefacts in less than 1ms with\nhigh recall. We compare our approach to a range of state-of-the-art quality\nassessment methods. The novel data augmentation and curriculum learning\napproaches both improved classification performance achieving overall area\nunder the ROC curve of 0.89.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:19:05 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 16:20:05 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Oksuz", "Ilkay", ""], ["Ruijsink", "Bram", ""], ["Puyol-Anton", "Esther", ""], ["Clough", "James", ""], ["Cruz", "Gastao", ""], ["Bustin", "Aurelien", ""], ["Prieto", "Claudia", ""], ["Botnar", "Rene", ""], ["Rueckert", "Daniel", ""], ["Schnabel", "Julia A.", ""], ["King", "Andrew P.", ""]]}, {"id": "1810.12186", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathana\\\"el Perraudin, Micha\\\"el Defferrard, Tomasz Kacprzak, Raphael\n  Sgier", "title": "DeepSphere: Efficient spherical Convolutional Neural Network with\n  HEALPix sampling for cosmological applications", "comments": "arXiv admin note: text overlap with arXiv:astro-ph/0409513 by other\n  authors", "journal-ref": null, "doi": "10.1016/j.ascom.2019.03.004", "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are a cornerstone of the Deep Learning\ntoolbox and have led to many breakthroughs in Artificial Intelligence. These\nnetworks have mostly been developed for regular Euclidean domains such as those\nsupporting images, audio, or video. Because of their success, CNN-based methods\nare becoming increasingly popular in Cosmology. Cosmological data often comes\nas spherical maps, which make the use of the traditional CNNs more complicated.\nThe commonly used pixelization scheme for spherical maps is the Hierarchical\nEqual Area isoLatitude Pixelisation (HEALPix). We present a spherical CNN for\nanalysis of full and partial HEALPix maps, which we call DeepSphere. The\nspherical CNN is constructed by representing the sphere as a graph. Graphs are\nversatile data structures that can act as a discrete representation of a\ncontinuous manifold. Using the graph-based representation, we define many of\nthe standard CNN operations, such as convolution and pooling. With filters\nrestricted to being radial, our convolutions are equivariant to rotation on the\nsphere, and DeepSphere can be made invariant or equivariant to rotation. This\nway, DeepSphere is a special case of a graph CNN, tailored to the HEALPix\nsampling of the sphere. This approach is computationally more efficient than\nusing spherical harmonics to perform convolutions. We demonstrate the method on\na classification problem of weak lensing mass maps from two cosmological models\nand compare the performance of the CNN with that of two baseline classifiers.\nThe results show that the performance of DeepSphere is always superior or equal\nto both of these baselines. For high noise levels and for data covering only a\nsmaller fraction of the sphere, DeepSphere achieves typically 10% better\nclassification accuracy than those baselines. Finally, we show how learned\nfilters can be visualized to introspect the neural network.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:23:18 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 17:11:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Perraudin", "Nathana\u00ebl", ""], ["Defferrard", "Micha\u00ebl", ""], ["Kacprzak", "Tomasz", ""], ["Sgier", "Raphael", ""]]}, {"id": "1810.12193", "submitter": "Feng Zheng", "authors": "Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao\n  Yu, Feiyue Huang, Rongrong Ji", "title": "Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training", "comments": "Accepted by 2019 Conference on Computer Vision and Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Re-IDentification (Re-ID) methods are highly dependent on\nprecise bounding boxes that enable images to be aligned with each other.\nHowever, due to the challenging practical scenarios, current detection models\noften produce inaccurate bounding boxes, which inevitably degenerate the\nperformance of existing Re-ID algorithms. In this paper, we propose a novel\ncoarse-to-fine pyramid model to relax the need of bounding boxes, which not\nonly incorporates local and global information, but also integrates the gradual\ncues between them. The pyramid model is able to match at different scales and\nthen search for the correct image of the same identity, even when the image\npairs are not aligned. In addition, in order to learn discriminative identity\nrepresentation, we explore a dynamic training scheme to seamlessly unify two\nlosses and extract appropriate shared information between them. Experimental\nresults clearly demonstrate that the proposed method achieves the\nstate-of-the-art results on three datasets. Especially, our approach exceeds\nthe current best method by 9.5% on the most challenging CUHK03 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:33:03 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 03:20:16 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 13:35:15 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zheng", "Feng", ""], ["Deng", "Cheng", ""], ["Sun", "Xing", ""], ["Jiang", "Xinyang", ""], ["Guo", "Xiaowei", ""], ["Yu", "Zongqiao", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "1810.12241", "submitter": "Arnab Kumar Mondal", "authors": "Arnab Kumar Mondal, Jose Dolz, Christian Desrosiers", "title": "Few-shot 3D Multi-modal Medical Image Segmentation using Generative\n  Adversarial Learning", "comments": "submitted to Medical Image Analysis for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of segmenting 3D multi-modal medical images in\nscenarios where very few labeled examples are available for training.\nLeveraging the recent success of adversarial learning for semi-supervised\nsegmentation, we propose a novel method based on Generative Adversarial\nNetworks (GANs) to train a segmentation model with both labeled and unlabeled\nimages. The proposed method prevents over-fitting by learning to discriminate\nbetween true and fake patches obtained by a generator network. Our work extends\ncurrent adversarial learning approaches, which focus on 2D single-modality\nimages, to the more challenging context of 3D volumes of multiple modalities.\nThe proposed method is evaluated on the problem of segmenting brain MRI from\nthe iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement\nis reported, compared to state-of-art segmentation networks trained in a\nfully-supervised manner. In addition, our work presents a comprehensive\nanalysis of different GAN architectures for semi-supervised segmentation,\nshowing recent techniques like feature matching to yield a higher performance\nthan conventional adversarial training approaches. Our code is publicly\navailable at https://github.com/arnab39/FewShot_GAN-Unet3D\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 16:38:57 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Mondal", "Arnab Kumar", ""], ["Dolz", "Jose", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1810.12286", "submitter": "St\\'ephanie Gu\\'erit", "authors": "St\\'ephanie Gu\\'erit and Siddharth Sivankutty and Camille Scott\\'e and\n  John Alto Lee and Herv\\'e Rigneault and Laurent Jacques", "title": "Compressive Sampling Approach for Image Acquisition with Lensless\n  Endoscope", "comments": "5 pages, 4 figures, workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lensless endoscope is a promising device designed to image tissues in\nvivo at the cellular scale. The traditional acquisition setup consists in\nraster scanning during which the focused light beam from the optical fiber\nilluminates sequentially each pixel of the field of view (FOV). The calibration\nstep to focus the beam and the sampling scheme both take time. In this\npreliminary work, we propose a scanning method based on compressive sampling\ntheory. The method does not rely on a focused beam but rather on the random\nillumination patterns generated by the single-mode fibers. Experiments are\nperformed on synthetic data for different compression rates (from 10 to 100% of\nthe FOV).\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 17:52:32 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 13:36:57 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Gu\u00e9rit", "St\u00e9phanie", ""], ["Sivankutty", "Siddharth", ""], ["Scott\u00e9", "Camille", ""], ["Lee", "John Alto", ""], ["Rigneault", "Herv\u00e9", ""], ["Jacques", "Laurent", ""]]}, {"id": "1810.12348", "submitter": "Samuel Albanie", "authors": "Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Andrea Vedaldi", "title": "Gather-Excite: Exploiting Feature Context in Convolutional Neural\n  Networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the use of bottom-up local operators in convolutional neural networks\n(CNNs) matches well some of the statistics of natural images, it may also\nprevent such models from capturing contextual long-range feature interactions.\nIn this work, we propose a simple, lightweight approach for better context\nexploitation in CNNs. We do so by introducing a pair of operators: gather,\nwhich efficiently aggregates feature responses from a large spatial extent, and\nexcite, which redistributes the pooled information to local features. The\noperators are cheap, both in terms of number of added parameters and\ncomputational complexity, and can be integrated directly in existing\narchitectures to improve their performance. Experiments on several datasets\nshow that gather-excite can bring benefits comparable to increasing the depth\nof a CNN at a fraction of the cost. For example, we find ResNet-50 with\ngather-excite operators is able to outperform its 101-layer counterpart on\nImageNet with no additional learnable parameters. We also propose a parametric\ngather-excite operator pair which yields further performance gains, relate it\nto the recently-introduced Squeeze-and-Excitation Networks, and analyse the\neffects of these changes to the CNN feature activation statistics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 18:52:37 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 06:42:57 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 10:34:06 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Hu", "Jie", ""], ["Shen", "Li", ""], ["Albanie", "Samuel", ""], ["Sun", "Gang", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1810.12366", "submitter": "Arjun Chandrasekaran", "authors": "Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit\n  Chattopadhyay, Devi Parikh", "title": "Do Explanations make VQA Models more Predictable to a Human?", "comments": "EMNLP 2018. 16 pages, 11 figures. Content overlaps with \"It Takes Two\n  to Tango: Towards Theory of AI's Mind\" (arXiv:1704.00717)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A rich line of research attempts to make deep neural networks more\ntransparent by generating human-interpretable 'explanations' of their decision\nprocess, especially for interactive tasks like Visual Question Answering (VQA).\nIn this work, we analyze if existing explanations indeed make a VQA model --\nits responses as well as failures -- more predictable to a human. Surprisingly,\nwe find that they do not. On the other hand, we find that human-in-the-loop\napproaches that treat the model as a black-box do.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 19:14:26 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Prabhu", "Viraj", ""], ["Yadav", "Deshraj", ""], ["Chattopadhyay", "Prithvijit", ""], ["Parikh", "Devi", ""]]}, {"id": "1810.12435", "submitter": "Omair Sarwar", "authors": "Omair Sarwar and Bernhard Rinner and Andrea Cavallaro", "title": "Concealing the identity of faces in oblique images with adaptive hopping\n  Gaussian mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras mounted on Micro Aerial Vehicles (MAVs) are increasingly used for\nrecreational photography. However, aerial photographs of public places often\ncontain faces of bystanders thus leading to a perceived or actual violation of\nprivacy. To address this issue, we propose to pseudo-randomly modify the\nappearance of face regions in the images using a privacy filter that prevents a\nhuman or a face recogniser from inferring the identities of people. The filter,\nwhich is applied only when the resolution is high enough for a face to be\nrecognisable, adaptively distorts the face appearance as a function of its\nresolution. Moreover, the proposed filter locally changes its parameters to\ndiscourage attacks that use parameter estimation. The filter exploits both\nglobal adaptiveness to reduce distortion and local hopping of the parameters to\nmake their estimation difficult for an attacker. In order to evaluate the\nefficiency of the proposed approach, we use a state-of-the-art face recognition\nalgorithm and synthetically generated face data with 3D geometric image\ntransformations that mimic faces captured from an MAV at different heights and\npitch angles. Experimental results show that the proposed filter protects\nprivacy while reducing distortion and exhibits resilience against attacks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:16:02 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Sarwar", "Omair", ""], ["Rinner", "Bernhard", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1810.12440", "submitter": "Manoj Acharya", "authors": "Manoj Acharya, Kushal Kafle, Christopher Kanan", "title": "TallyQA: Answering Complex Counting Questions", "comments": "To appear in AAAI 2019 ( To download the dataset please go to\n  http://www.manojacharya.com/ )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most counting questions in visual question answering (VQA) datasets are\nsimple and require no more than object detection. Here, we study algorithms for\ncomplex counting questions that involve relationships between objects,\nattribute identification, reasoning, and more. To do this, we created TallyQA,\nthe world's largest dataset for open-ended counting. We propose a new algorithm\nfor counting that uses relation networks with region proposals. Our method lets\nrelation networks be efficiently used with high-resolution imagery. It yields\nstate-of-the-art results compared to baseline and recent systems on both\nTallyQA and the HowMany-QA benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:29:45 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 18:32:07 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Acharya", "Manoj", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "1810.12445", "submitter": "Alexandre Cunha", "authors": "Alexandre Cunha", "title": "Geometric Median Shapes", "comments": "Accepted ISBI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an algorithm to compute the geometric median of shapes which is\nbased on the extension of median to high dimensions. The median finding problem\nis formulated as an optimization over distances and it is solved directly using\nthe watershed method as an optimizer. We show that computing the geometric\nmedian of shapes is robust in the presence of outliers and it is superior to\nthe mean shape which can easily be affected by the presence of outliers. The\ngeometric median shape thus faithfully represents the true central tendency of\nthe data, contaminated or not. Our approach can be applied to manifold and non\nmanifold shapes, with connected or disconnected shapes. The application of\ndistance transforms and watershed algorithm, two well established constructs of\nimage processing, lead to an algorithm that can be quickly implemented to\ngenerate fast solutions with linear storage requirements. We demonstrate our\nmethods in synthetic and natural shapes and compare median and mean results\nunder increasing contamination by strong outliers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:50:26 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 20:56:04 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 22:52:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Cunha", "Alexandre", ""]]}, {"id": "1810.12448", "submitter": "Onur Tasar", "authors": "Onur Tasar, Yuliya Tarabalka, Pierre Alliez", "title": "Incremental Learning for Semantic Segmentation of Large-Scale Remote\n  Sensing Data", "comments": null, "journal-ref": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND\n  REMOTE SENSING, 12, 2019, 3524-3537", "doi": "10.1109/JSTARS.2019.2925416", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of remarkable success of the convolutional neural networks on\nsemantic segmentation, they suffer from catastrophic forgetting: a significant\nperformance drop for the already learned classes when new classes are added on\nthe data, having no annotations for the old classes. We propose an incremental\nlearning methodology, enabling to learn segmenting new classes without\nhindering dense labeling abilities for the previous classes, although the\nentire previous data are not accessible. The key points of the proposed\napproach are adapting the network to learn new as well as old classes on the\nnew training data, and allowing it to remember the previously learned\ninformation for the old classes. For adaptation, we keep a frozen copy of the\npreviously trained network, which is used as a memory for the updated network\nin absence of annotations for the former classes. The updated network minimizes\na loss function, which balances the discrepancy between outputs for the\nprevious classes from the memory and updated networks, and the\nmis-classification rate between outputs for the new classes from the updated\nnetwork and the new ground-truth. For remembering, we either regularly feed\nsamples from the stored, little fraction of the previous data or use the memory\nnetwork, depending on whether the new data are collected from completely\ndifferent geographic areas or from the same city. Our experimental results\nprove that it is possible to add new classes to the network, while maintaining\nits performance for the previous classes, despite the whole previous training\ndata are not available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 22:51:52 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Tasar", "Onur", ""], ["Tarabalka", "Yuliya", ""], ["Alliez", "Pierre", ""]]}, {"id": "1810.12478", "submitter": "Galin Georgiev", "authors": "Galin Georgiev", "title": "Generating new pictures in complex datasets with a simple neural network", "comments": "13 pages plus some images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a version of a variational auto-encoder (VAE), which can\ngenerate good perturbations of images, when trained on a complex dataset (in\nour experiments, CIFAR-10). The net is using only two latent generative\ndimensions per class, with uni-modal probability density. The price one has to\npay for good generation is that not all training images are well reconstructed.\nAn additional classifier is required to determine which training image is well\nreconstructed and generally the weights of training images. Only training\nimages which are well reconstructed, can be perturbed. For good perturbations,\nwe use the tentative empirical drifts of well reconstructed images. The\nconstruct is not predictive in the usual statistical sense.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 01:38:21 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 22:21:43 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 23:58:48 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Georgiev", "Galin", ""]]}, {"id": "1810.12488", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira", "title": "Re-evaluating Continual Learning Scenarios: A Categorization and Case\n  for Strong Baselines", "comments": "Continual Learning Workshop, 32nd Conference on Neural Information\n  Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning has received a great deal of attention recently with\nseveral approaches being proposed. However, evaluations involve a diverse set\nof scenarios making meaningful comparison difficult. This work provides a\nsystematic categorization of the scenarios and evaluates them within a\nconsistent framework including strong baselines and state-of-the-art methods.\nThe results provide an understanding of the relative difficulty of the\nscenarios and that simple baselines (Adagrad, L2 regularization, and naive\nrehearsal strategies) can surprisingly achieve similar performance to current\nmainstream methods. We conclude with several suggestions for creating harder\nevaluation scenarios and future research directions. The code is available at\nhttps://github.com/GT-RIPL/Continual-Learning-Benchmark\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 02:08:35 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 16:50:11 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 03:51:28 GMT"}, {"version": "v4", "created": "Wed, 23 Jan 2019 16:58:13 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Liu", "Yen-Cheng", ""], ["Ramasamy", "Anita", ""], ["Kira", "Zsolt", ""]]}, {"id": "1810.12494", "submitter": "Yiming Lei", "authors": "Yiming Lei, Yukun Tian, Hongming Shan, Junping Zhang, Ge Wang,\n  Mannudeep Kalra", "title": "Shape and Margin-Aware Lung Nodule Classification in Low-dose CT Images\n  via Soft Activation Mapping", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2019.101628", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of studies on lung nodule classification lack clinical/biological\ninterpretations of the features extracted by convolutional neural network\n(CNN). The methods like class activation mapping (CAM) and gradient-based CAM\n(Grad-CAM) are tailored for interpreting localization and classification tasks\nwhile they ignored fine-grained features. Therefore, CAM and Grad-CAM cannot\nprovide optimal interpretation for lung nodule categorization task in low-dose\nCT images, in that fine-grained pathological clues like discrete and irregular\nshape and margins of nodules are capable of enhancing sensitivity and\nspecificity of nodule classification with regards to CNN. In this paper, we\nfirst develop a soft activation mapping (SAM) to enable fine-grained lung\nnodule shape \\& margin (LNSM) feature analysis with a CNN so that it can access\nrich discrete features. Secondly, by combining high-level convolutional\nfeatures with SAM, we further propose a high-level feature enhancement scheme\n(HESAM) to localize LNSM features. Experiments on the LIDC-IDRI dataset\nindicate that 1) SAM captures more fine-grained and discrete attention regions\nthan existing methods, 2) HESAM localizes more accurately on LNSM features and\nobtains the state-of-the-art predictive performance, reducing the false\npositive rate, and 3) we design and conduct a visually matching experiment\nwhich incorporates radiologists study to increase the confidence level of\napplying our method to clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 02:29:30 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 12:23:40 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Lei", "Yiming", ""], ["Tian", "Yukun", ""], ["Shan", "Hongming", ""], ["Zhang", "Junping", ""], ["Wang", "Ge", ""], ["Kalra", "Mannudeep", ""]]}, {"id": "1810.12514", "submitter": "Mehran Maghoumi", "authors": "Mehran Maghoumi and Joseph J. LaViola Jr", "title": "DeepGRU: Deep Gesture Recognition Utility", "comments": "Published in ISVC 2019. Code is available at\n  https://github.com/Maghoumi/DeepGRU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepGRU, a novel end-to-end deep network model informed by recent\ndevelopments in deep learning for gesture and action recognition, that is\nstreamlined and device-agnostic. DeepGRU, which uses only raw skeleton, pose or\nvector data is quickly understood, implemented, and trained, and yet achieves\nstate-of-the-art results on challenging datasets. At the heart of our method\nlies a set of stacked gated recurrent units (GRU), two fully-connected layers\nand a novel global attention model. We evaluate our method on seven publicly\navailable datasets, containing various number of samples and spanning over a\nbroad range of interactions (full-body, multi-actor, hand gestures, etc.). In\nall but one case we outperform the state-of-the-art pose-based methods. For\ninstance, we achieve a recognition accuracy of 84.9% and 92.3% on cross-subject\nand cross-view tests of the NTU RGB+D dataset respectively, and also 100%\nrecognition accuracy on the UT-Kinect dataset. While DeepGRU works well on\nlarge datasets with many training samples, we show that even in the absence of\na large number of training data, and with as little as four samples per class,\nDeepGRU can beat traditional methods specifically designed for small training\nsets. Lastly, we demonstrate that even without powerful hardware, and using\nonly the CPU, our method can still be trained in under 10 minutes on\nsmall-scale datasets, making it an enticing choice for rapid application\nprototyping and development.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 03:43:22 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 07:40:54 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 07:25:33 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2019 05:41:24 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Maghoumi", "Mehran", ""], ["LaViola", "Joseph J.", "Jr"]]}, {"id": "1810.12521", "submitter": "Yi Zhu", "authors": "Yi Zhu and Jia Xue and Shawn Newsam", "title": "Gated Transfer Network for Transfer Learning", "comments": "Accepted at ACCV 2018. Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have led to a series of breakthroughs in computer vision\ngiven sufficient annotated training datasets. For novel tasks with limited\nlabeled data, the prevalent approach is to transfer the knowledge learned in\nthe pre-trained models to the new tasks by fine-tuning. Classic model\nfine-tuning utilizes the fact that well trained neural networks appear to learn\ncross domain features. These features are treated equally during transfer\nlearning. In this paper, we explore the impact of feature selection in model\nfine-tuning by introducing a transfer module, which assigns weights to features\nextracted from pre-trained models. The proposed transfer module proves the\nimportance of feature selection for transferring models from source to target\ndomains. It is shown to significantly improve upon fine-tuning results with\nonly marginal extra computational cost. We also incorporate an auxiliary\nclassifier as an extra regularizer to avoid over-fitting. Finally, we build a\nGated Transfer Network (GTN) based on our transfer module and achieve\nstate-of-the-art results on six different tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:31:48 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Xue", "Jia", ""], ["Newsam", "Shawn", ""]]}, {"id": "1810.12522", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Random Temporal Skipping for Multirate Video Analysis", "comments": "Accepted at ACCV 2018. Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches to video understanding adopt temporal\njittering to simulate analyzing the video at varying frame rates. However, this\ndoes not work well for multirate videos, in which actions or subactions occur\nat different speeds. The frame sampling rate should vary in accordance with the\ndifferent motion speeds. In this work, we propose a simple yet effective\nstrategy, termed random temporal skipping, to address this situation. This\nstrategy effectively handles multirate videos by randomizing the sampling rate\nduring training. It is an exhaustive approach, which can potentially cover all\nmotion speed variations. Furthermore, due to the large temporal skipping, our\nnetwork can see video clips that originally cover over 100 frames. Such a time\nrange is enough to analyze most actions/events. We also introduce an\nocclusion-aware optical flow learning method that generates improved motion\nmaps for human action recognition. Our framework is end-to-end trainable, runs\nin real-time, and achieves state-of-the-art performance on six widely adopted\nvideo benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:35:43 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1810.12532", "submitter": "Zhirui Wang", "authors": "Zhirui Wang, Laurent Kneip", "title": "Fully automatic structure from motion with a spline-based environment\n  representation", "comments": "12 pages (10 + 2 pages references), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the common environment representation in structure from motion is given\nby a sparse point cloud, the community has also investigated the use of lines\nto better enforce the inherent regularities in man-made surroundings. Following\nthe potential of this idea, the present paper introduces a more flexible\nhigher-order extension of points that provides a general model for structural\nedges in the environment, no matter if straight or curved. Our model relies on\nlinked B\\'ezier curves, the geometric intuition of which proves great benefits\nduring parameter initialization and regularization. We present the first fully\nautomatic pipeline that is able to generate spline-based representations\nwithout any human supervision. Besides a full graphical formulation of the\nproblem, we introduce both geometric and photometric cues as well as\nhigher-level concepts such overall curve visibility and viewing angle\nrestrictions to automatically manage the correspondences in the graph. Results\nprove that curve-based structure from motion with splines is able to outperform\nstate-of-the-art sparse feature-based methods, as well as to model curved edges\nin the environment.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:59:53 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Wang", "Zhirui", ""], ["Kneip", "Laurent", ""]]}, {"id": "1810.12535", "submitter": "Qingzhong Wang", "authors": "Qingzhong Wang and Antoni B. Chan", "title": "Gated Hierarchical Attention for Image Captioning", "comments": "Accepted by ACCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention modules connecting encoder and decoders have been widely applied in\nthe field of object recognition, image captioning, visual question answering\nand neural machine translation, and significantly improves the performance. In\nthis paper, we propose a bottom-up gated hierarchical attention (GHA) mechanism\nfor image captioning. Our proposed model employs a CNN as the decoder which is\nable to learn different concepts at different layers, and apparently, different\nconcepts correspond to different areas of an image. Therefore, we develop the\nGHA in which low-level concepts are merged into high-level concepts and\nsimultaneously low-level attended features pass to the top to make predictions.\nOur GHA significantly improves the performance of the model that only applies\none level attention, for example, the CIDEr score increases from 0.923 to\n0.999, which is comparable to the state-of-the-art models that employ\nattributes boosting and reinforcement learning (RL). We also conduct extensive\nexperiments to analyze the CNN decoder and our proposed GHA, and we find that\ndeeper decoders cannot obtain better performance, and when the convolutional\ndecoder becomes deeper the model is likely to collapse during training.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 05:20:49 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 04:44:41 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Wang", "Qingzhong", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1810.12552", "submitter": "Zhijing Jin", "authors": "Zhijing Jin, Tristan Swedish, Ramesh Raskar", "title": "3D Traffic Simulation for Autonomous Vehicles in Unity and Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the recent years, there has been an explosion of studies on autonomous\nvehicles. Many collected large amount of data from human drivers. However,\ncompared to the tedious data collection approach, building a virtual simulation\nof traffic makes the autonomous vehicle research more flexible, time-saving,\nand scalable. Our work features a 3D simulation that takes in real time\nposition information parsed from street cameras. The simulation can easily\nswitch between a global bird view of the traffic and a local perspective of a\ncar. It can also filter out certain objects in its customized camera, creating\nvarious channels for objects of different categories. This provides alternative\nsupervised or unsupervised ways to train deep neural networks. Another\nadvantage of the 3D simulation is its conformation to physical laws. Its\nnaturalness to accelerate and collide prepares the system for potential deep\nreinforcement learning needs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 07:21:43 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 13:28:21 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Jin", "Zhijing", ""], ["Swedish", "Tristan", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1810.12553", "submitter": "Yuan Mao", "authors": "Yixiong Liang, Yuan Mao, Jiazhi Xia, Yao Xiang, Jianfeng Liu", "title": "Scale-Invariant Structure Saliency Selection for Fast Image Fusion", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.04.043", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a fast yet effective method for pixel-level\nscale-invariant image fusion in spatial domain based on the scale-space theory.\nSpecifically, we propose a scale-invariant structure saliency selection scheme\nbased on the difference-of-Gaussian (DoG) pyramid of images to build the\nweights or activity map. Due to the scale-invariant structure saliency\nselection, our method can keep both details of small size objects and the\nintegrity information of large size objects in images. In addition, our method\nis very efficient since there are no complex operation involved and easy to be\nimplemented and therefore can be used for fast high resolution images fusion.\nExperimental results demonstrate the proposed method yields competitive or even\nbetter results comparing to state-of-the-art image fusion methods both in terms\nof visual quality and objective evaluation metrics. Furthermore, the proposed\nmethod is very fast and can be used to fuse the high resolution images in\nreal-time. Code is available at https://github.com/yiqingmy/Fusion.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 07:22:21 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Liang", "Yixiong", ""], ["Mao", "Yuan", ""], ["Xia", "Jiazhi", ""], ["Xiang", "Yao", ""], ["Liu", "Jianfeng", ""]]}, {"id": "1810.12563", "submitter": "Haowen Luo", "authors": "Haowen Luo", "title": "Shorten Spatial-spectral RNN with Parallel-GRU for Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) attained a good performance in\nhyperspectral sensing image (HSI) classification, but CNNs consider spectra as\norderless vectors. Therefore, considering the spectra as sequences, recurrent\nneural networks (RNNs) have been applied in HSI classification, for RNNs is\nskilled at dealing with sequential data. However, for a long-sequence task,\nRNNs is difficult for training and not as effective as we expected. Besides,\nspatial contextual features are not considered in RNNs. In this study, we\npropose a Shorten Spatial-spectral RNN with Parallel-GRU (St-SS-pGRU) for HSI\nclassification. A shorten RNN is more efficient and easier for training than\nband-by-band RNN. By combining converlusion layer, the St-SSpGRU model\nconsiders not only spectral but also spatial feature, which results in a better\nperformance. An architecture named parallel-GRU is also proposed and applied in\nSt-SS-pGRU. With this architecture, the model gets a better performance and is\nmore robust.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 07:57:27 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Luo", "Haowen", ""]]}, {"id": "1810.12568", "submitter": "Xi Zhang", "authors": "Xi Zhang, Xiaolin Wu", "title": "Nonlinear Prediction of Multidimensional Signals via Deep Regression\n  with Applications to Image Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) have enjoyed great successes in\nmany signal processing applications because they can learn complex, non-linear\ncausal relationships from input to output. In this light, DCNNs are well suited\nfor the task of sequential prediction of multidimensional signals, such as\nimages, and have the potential of improving the performance of traditional\nlinear predictors. In this research we investigate how far DCNNs can push the\nenvelop in terms of prediction precision. We propose, in a case study, a\ntwo-stage deep regression DCNN framework for nonlinear prediction of\ntwo-dimensional image signals. In the first-stage regression, the proposed deep\nprediction network (PredNet) takes the causal context as input and emits a\nprediction of the present pixel. Three PredNets are trained with the regression\nobjectives of minimizing $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ norms of\nprediction residuals, respectively. The second-stage regression combines the\noutputs of the three PredNets to generate an even more precise and robust\nprediction. The proposed deep regression model is applied to lossless\npredictive image coding, and it outperforms the state-of-the-art linear\npredictors by appreciable margin.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:14:11 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1810.12574", "submitter": "Chris H. Bahnsen", "authors": "Chris H. Bahnsen and Thomas B. Moeslund", "title": "Rain Removal in Traffic Surveillance: Does it Matter?", "comments": "Published in IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2018.2872502", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying weather conditions, including rainfall and snowfall, are generally\nregarded as a challenge for computer vision algorithms. One proposed solution\nto the challenges induced by rain and snowfall is to artificially remove the\nrain from images or video using rain removal algorithms. It is the promise of\nthese algorithms that the rain-removed image frames will improve the\nperformance of subsequent segmentation and tracking algorithms. However, rain\nremoval algorithms are typically evaluated on their ability to remove synthetic\nrain on a small subset of images. Currently, their behavior is unknown on\nreal-world videos when integrated with a typical computer vision pipeline. In\nthis paper, we review the existing rain removal algorithms and propose a new\ndataset that consists of 22 traffic surveillance sequences under a broad\nvariety of weather conditions that all include either rain or snowfall. We\npropose a new evaluation protocol that evaluates the rain removal algorithms on\ntheir ability to improve the performance of subsequent segmentation, instance\nsegmentation, and feature tracking algorithms under rain and snow. If\nsuccessful, the de-rained frames of a rain removal algorithm should improve\nsegmentation performance and increase the number of accurately tracked\nfeatures. The results show that a recent single-frame-based rain removal\nalgorithm increases the segmentation performance by 19.7% on our proposed\ndataset, but it eventually decreases the feature tracking performance and\nshowed mixed results with recent instance segmentation methods. However, the\nbest video-based rain removal algorithm improves the feature tracking accuracy\nby 7.72%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:25:54 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Bahnsen", "Chris H.", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1810.12575", "submitter": "Tobias Pl\\\"otz", "authors": "Tobias Pl\\\"otz and Stefan Roth", "title": "Neural Nearest Neighbors Networks", "comments": "to appear at NIPS*2018, code available at\n  https://github.com/visinf/n3net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local methods exploiting the self-similarity of natural signals have been\nwell studied, for example in image analysis and restoration. Existing\napproaches, however, rely on k-nearest neighbors (KNN) matching in a fixed\nfeature space. The main hurdle in optimizing this feature space w.r.t.\napplication performance is the non-differentiability of the KNN selection rule.\nTo overcome this, we propose a continuous deterministic relaxation of KNN\nselection that maintains differentiability w.r.t. pairwise distances, but\nretains the original KNN as the limit of a temperature parameter approaching\nzero. To exploit our relaxation, we propose the neural nearest neighbors block\n(N3 block), a novel non-local processing layer that leverages the principle of\nself-similarity and can be used as building block in modern neural network\narchitectures. We show its effectiveness for the set reasoning task of\ncorrespondence classification as well as for image restoration, including image\ndenoising and single image super-resolution, where we outperform strong\nconvolutional neural network (CNN) baselines and recent non-local models that\nrely on KNN selection in hand-chosen features spaces.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:32:47 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Pl\u00f6tz", "Tobias", ""], ["Roth", "Stefan", ""]]}, {"id": "1810.12576", "submitter": "Alexander Matyasko", "authors": "Alexander Matyasko, Lap-Pui Chau", "title": "Improved Network Robustness with Adversary Critic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Ideally, what confuses neural network should be confusing to humans. However,\nrecent experiments have shown that small, imperceptible perturbations can\nchange the network prediction. To address this gap in perception, we propose a\nnovel approach for learning robust classifier. Our main idea is: adversarial\nexamples for the robust classifier should be indistinguishable from the regular\ndata of the adversarial target. We formulate a problem of learning robust\nclassifier in the framework of Generative Adversarial Networks (GAN), where the\nadversarial attack on classifier acts as a generator, and the critic network\nlearns to distinguish between regular and adversarial images. The classifier\ncost is augmented with the objective that its adversarial examples should\nconfuse the adversary critic. To improve the stability of the adversarial\nmapping, we introduce adversarial cycle-consistency constraint which ensures\nthat the adversarial mapping of the adversarial examples is close to the\noriginal. In the experiments, we show the effectiveness of our defense. Our\nmethod surpasses in terms of robustness networks trained with adversarial\ntraining. Additionally, we verify in the experiments with human annotators on\nMTurk that adversarial examples are indeed visually confusing. Codes for the\nproject are available at https://github.com/aam-at/adversary_critic.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:33:46 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Matyasko", "Alexander", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1810.12681", "submitter": "Chenhan Jiang", "authors": "Chenhan Jiang, Hang Xu, Xiangdan Liang, Liang Lin", "title": "Hybrid Knowledge Routed Modules for Large-scale Object Detection", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant object detection approaches treat the recognition of each region\nseparately and overlook crucial semantic correlations between objects in one\nscene. This paradigm leads to substantial performance drop when facing heavy\nlong-tail problems, where very few samples are available for rare classes and\nplenty of confusing categories exists. We exploit diverse human commonsense\nknowledge for reasoning over large-scale object categories and reaching\nsemantic coherency within one image. Particularly, we present Hybrid Knowledge\nRouted Modules (HKRM) that incorporates the reasoning routed by two kinds of\nknowledge forms: an explicit knowledge module for structured constraints that\nare summarized with linguistic knowledge (e.g. shared attributes,\nrelationships) about concepts; and an implicit knowledge module that depicts\nsome implicit constraints (e.g. common spatial layouts). By functioning over a\nregion-to-region graph, both modules can be individualized and adapted to\ncoordinate with visual patterns in each image, guided by specific knowledge\nforms. HKRM are light-weight, general-purpose and extensible by easily\nincorporating multiple knowledge to endow any detection networks the ability of\nglobal semantic reasoning. Experiments on large-scale object detection\nbenchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000\ncategories) and 30.4% on ADE in terms of mAP. Codes and trained model can be\nfound in https://github.com/chanyn/HKRM.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 11:52:10 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Jiang", "Chenhan", ""], ["Xu", "Hang", ""], ["Liang", "Xiangdan", ""], ["Lin", "Liang", ""]]}, {"id": "1810.12690", "submitter": "Arnav Bhavsar", "authors": "Vibha Gupta, Arnav Bhavsar", "title": "Role of Class-specific Features in Various Classification Frameworks for\n  Human Epithelial (HEp-2) Cell Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The antinuclear antibody detection with human epithelial cells is a popular\napproach for autoimmune diseases diagnosis. The manual evaluation demands time,\neffort and capital, and automation in screening can greatly aid the physicians\nin these respects. In this work, we employ simple, efficient and visually more\ninterpretable, class-specific features which defined based on the visual\ncharacteristics of each class. We believe that defining features with a good\nvisual interpretation, is indeed important in a scenario, where such an\napproach is used in an interactive CAD system for pathologists. Considering\nthat problem consists of few classes, and our rather simplistic feature\ndefinitions, frameworks can be structured as hierarchies of various binary\nclassifiers. These variants include frameworks which are earlier explored and\nsome which are not explored for this task. We perform various experiments which\ninclude traditional texture features and demonstrate the effectiveness of\nclass-specific features in various frameworks. We make insightful comparisons\nbetween different types of classification frameworks given their silent aspects\nand pros and cons over each other. We also demonstrate an experiment with only\nintermediates samples for testing. The proposed work yields encouraging results\nwith respect to the state-of-the-art and highlights the role of class-specific\nfeatures in different classification frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 12:06:09 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Gupta", "Vibha", ""], ["Bhavsar", "Arnav", ""]]}, {"id": "1810.12738", "submitter": "Ahmed Sabir", "authors": "Ahmed Sabir, Francesc Moreno-Noguer and Llu\\'is Padr\\'o", "title": "Visual Re-ranking with Natural Language Understanding for Text Spotting", "comments": "Accepted by ACCV 2018. arXiv admin note: substantial text overlap\n  with arXiv:1810.09776", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scene text recognition approaches are based on purely visual information\nand ignore the semantic relation between scene and text. In this paper, we\ntackle this problem from natural language processing perspective to fill the\ngap between language and vision. We propose a post-processing approach to\nimprove scene text recognition accuracy by using occurrence probabilities of\nwords (unigram language model), and the semantic correlation between scene and\ntext. For this, we initially rely on an off-the-shelf deep neural network,\nalready trained with a large amount of data, which provides a series of text\nhypotheses per input image. These hypotheses are then re-ranked using word\nfrequencies and semantic relatedness with objects or scenes in the image. As a\nresult of this combination, the performance of the original network is boosted\nwith almost no additional cost. We validate our approach on ICDAR'17 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 10:41:45 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Sabir", "Ahmed", ""], ["Moreno-Noguer", "Francesc", ""], ["Padr\u00f3", "Llu\u00eds", ""]]}, {"id": "1810.12778", "submitter": "Dong Li", "authors": "Dong Li, Dongbin Zhao, Qichao Zhang, Yaran Chen", "title": "Reinforcement Learning and Deep Learning based Lateral Control for\n  Autonomous Driving", "comments": "14 pages, 12 figures, accepted to IEEE Computational Intelligence\n  Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the vision-based autonomous driving with deep\nlearning and reinforcement learning methods. Different from the end-to-end\nlearning method, our method breaks the vision-based lateral control system down\ninto a perception module and a control module. The perception module which is\nbased on a multi-task learning neural network first takes a driver-view image\nas its input and predicts the track features. The control module which is based\non reinforcement learning then makes a control decision based on these\nfeatures. In order to improve the data efficiency, we propose visual TORCS\n(VTORCS), a deep reinforcement learning environment which is based on the open\nracing car simulator (TORCS). By means of the provided functions, one can train\nan agent with the input of an image or various physical sensor measurement, or\nevaluate the perception algorithm on this simulator. The trained reinforcement\nlearning controller outperforms the linear quadratic regulator (LQR) controller\nand model predictive control (MPC) controller on different tracks. The\nexperiments demonstrate that the perception module shows promising performance\nand the controller is capable of controlling the vehicle drive well along the\ntrack center with visual input.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 14:43:36 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Li", "Dong", ""], ["Zhao", "Dongbin", ""], ["Zhang", "Qichao", ""], ["Chen", "Yaran", ""]]}, {"id": "1810.12813", "submitter": "Panfeng Li", "authors": "Panfeng Li, Youzuo Lin, Emily Schultz-Fellenz", "title": "Contextual Hourglass Network for Semantic Segmentation of High\n  Resolution Aerial Imagery", "comments": "Submitted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation for aerial imagery is a challenging and important\nproblem in remotely sensed imagery analysis. In recent years, with the success\nof deep learning, various convolutional neural network (CNN) based models have\nbeen developed. However, due to the varying sizes of the objects and imbalanced\nclass labels, it can be challenging to obtain accurate pixel-wise semantic\nsegmentation results. To address those challenges, we develop a novel semantic\nsegmentation method and call it Contextual Hourglass Network. In our method, in\norder to improve the robustness of the prediction, we design a new contextual\nhourglass module which incorporates attention mechanism on processed\nlow-resolution featuremaps to exploit the contextual semantics. We further\nexploit the stacked encoder-decoder structure by connecting multiple contextual\nhourglass modules from end to end. This architecture can effectively extract\nrich multi-scale features and add more feedback loops for better learning\ncontextual semantics through intermediate supervision. To demonstrate the\nefficacy of our semantic segmentation method, we test it on Potsdam and\nVaihingen datasets. Through the comparisons to other baseline methods, our\nmethod yields the best results on overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:33:47 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 08:54:51 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Li", "Panfeng", ""], ["Lin", "Youzuo", ""], ["Schultz-Fellenz", "Emily", ""]]}, {"id": "1810.12819", "submitter": "Ziad Al-Halah", "authors": "Alina Roitberg, Ziad Al-Halah, Rainer Stiefelhagen", "title": "Informed Democracy: Voting-based Novelty Detection for Action\n  Recognition", "comments": "Published in BMVC 2018. First and second authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is crucial for real-life applications. While it is common\nin activity recognition to assume a closed-set setting, i.e. test samples are\nalways of training categories, this assumption is impractical in a real-world\nscenario. Test samples can be of various categories including those never seen\nbefore during training. Thus, being able to know what we know and what we do\nnot know is decisive for the model to avoid what can be catastrophic\nconsequences. We present in this work a novel approach for identifying samples\nof activity classes that are not previously seen by the classifier. Our model\nemploys a voting-based scheme that leverages the estimated uncertainty of the\nindividual classifiers in their predictions to measure the novelty of a new\ninput sample. Furthermore, the voting is privileged to a subset of informed\nclassifiers that can best estimate whether a sample is novel or not when it is\nclassified to a certain known category. In a thorough evaluation on UCF-101 and\nHMDB-51, we show that our model consistently outperforms state-of-the-art in\nnovelty detection. Additionally, by combining our model with off-the-shelf\nzero-shot learning (ZSL) approaches, our model leads to a significant\nimprovement in action classification accuracy for the generalized ZSL setting.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:42:55 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Roitberg", "Alina", ""], ["Al-Halah", "Ziad", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1810.12829", "submitter": "Guanbin Li", "authors": "Guanbin Li, Yukang Gan, Hejun Wu, Nong Xiao, Liang Lin", "title": "Cross-Modal Attentional Context Learning for RGB-D Object Detection", "comments": "Accept as a regular paper to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2878956", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing objects from simultaneously sensed photometric (RGB) and depth\nchannels is a fundamental yet practical problem in many machine vision\napplications such as robot grasping and autonomous driving. In this paper, we\naddress this problem by developing a Cross-Modal Attentional Context (CMAC)\nlearning framework, which enables the full exploitation of the context\ninformation from both RGB and depth data. Compared to existing RGB-D object\ndetection frameworks, our approach has several appealing properties. First, it\nconsists of an attention-based global context model for exploiting adaptive\ncontextual information and incorporating this information into a region-based\nCNN (e.g., Fast RCNN) framework to achieve improved object detection\nperformance. Second, our CMAC framework further contains a fine-grained object\npart attention module to harness multiple discriminative object parts inside\neach possible object region for superior local feature representation. While\ngreatly improving the accuracy of RGB-D object detection, the effective\ncross-modal information fusion as well as attentional context modeling in our\nproposed model provide an interpretable visualization scheme. Experimental\nresults demonstrate that the proposed method significantly improves upon the\nstate of the art on all public benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:57:18 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Li", "Guanbin", ""], ["Gan", "Yukang", ""], ["Wu", "Hejun", ""], ["Xiao", "Nong", ""], ["Lin", "Liang", ""]]}, {"id": "1810.12832", "submitter": "Kele Xu", "authors": "Kele Xu, Boqing Zhu, Qiuqiang Kong, Haibo Mi, Bo Ding, Dezhi Wang,\n  Huaimin Wang", "title": "General audio tagging with ensembling convolutional neural network and\n  statistical features", "comments": "Submitted to ICASSP", "journal-ref": null, "doi": "10.1121/1.5111059", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio tagging aims to infer descriptive labels from audio clips. Audio\ntagging is challenging due to the limited size of data and noisy labels. In\nthis paper, we describe our solution for the DCASE 2018 Task 2 general audio\ntagging challenge. The contributions of our solution include: We investigated a\nvariety of convolutional neural network architectures to solve the audio\ntagging task. Statistical features are applied to capture statistical patterns\nof audio features to improve the classification performance. Ensemble learning\nis applied to ensemble the outputs from the deep classifiers to utilize\ncomplementary information. a sample re-weight strategy is employed for ensemble\ntraining to address the noisy label problem. Our system achieves a mean average\nprecision (mAP@3) of 0.958, outperforming the baseline system of 0.704. Our\nsystem ranked the 1st and 4th out of 558 submissions in the public and private\nleaderboard of DCASE 2018 Task 2 challenge. Our codes are available at\nhttps://github.com/Cocoxili/DCASE2018Task2/.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:59:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xu", "Kele", ""], ["Zhu", "Boqing", ""], ["Kong", "Qiuqiang", ""], ["Mi", "Haibo", ""], ["Ding", "Bo", ""], ["Wang", "Dezhi", ""], ["Wang", "Huaimin", ""]]}, {"id": "1810.12864", "submitter": "Yu Sun", "authors": "Jiaming Liu, Yu Sun, Xiaojian Xu and Ulugbek S. Kamilov", "title": "Image Restoration using Total Variation Regularized Deep Image Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, sparsity-driven regularization has led to significant\nimprovements in image reconstruction. Traditional regularizers, such as total\nvariation (TV), rely on analytical models of sparsity. However, increasingly\nthe field is moving towards trainable models, inspired from deep learning. Deep\nimage prior (DIP) is a recent regularization framework that uses a\nconvolutional neural network (CNN) architecture without data-driven training.\nThis paper extends the DIP framework by combining it with the traditional TV\nregularization. We show that the inclusion of TV leads to considerable\nperformance gains when tested on several traditional restoration tasks such as\nimage denoising and deblurring.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:11:48 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Liu", "Jiaming", ""], ["Sun", "Yu", ""], ["Xu", "Xiaojian", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1810.12890", "submitter": "Golnaz Ghiasi", "authors": "Golnaz Ghiasi, Tsung-Yi Lin, Quoc V. Le", "title": "DropBlock: A regularization method for convolutional networks", "comments": "Accepted at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks often work well when they are over-parameterized and\ntrained with a massive amount of noise and regularization, such as weight decay\nand dropout. Although dropout is widely used as a regularization technique for\nfully connected layers, it is often less effective for convolutional layers.\nThis lack of success of dropout for convolutional layers is perhaps due to the\nfact that activation units in convolutional layers are spatially correlated so\ninformation can still flow through convolutional networks despite dropout. Thus\na structured form of dropout is needed to regularize convolutional networks. In\nthis paper, we introduce DropBlock, a form of structured dropout, where units\nin a contiguous region of a feature map are dropped together. We found that\napplying DropbBlock in skip connections in addition to the convolution layers\nincreases the accuracy. Also, gradually increasing number of dropped units\nduring training leads to better accuracy and more robust to hyperparameter\nchoices. Extensive experiments show that DropBlock works better than dropout in\nregularizing convolutional networks. On ImageNet classification, ResNet-50\narchitecture with DropBlock achieves $78.13\\%$ accuracy, which is more than\n$1.6\\%$ improvement on the baseline. On COCO detection, DropBlock improves\nAverage Precision of RetinaNet from $36.8\\%$ to $38.4\\%$.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:39:42 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Ghiasi", "Golnaz", ""], ["Lin", "Tsung-Yi", ""], ["Le", "Quoc V.", ""]]}, {"id": "1810.12941", "submitter": "Elad Ben Baruch", "authors": "Elad Ben Baruch, Yosi Keller", "title": "Joint detection and matching of feature points in multimodal images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel Convolutional Neural Network (CNN)\narchitecture for the joint detection and matching of feature points in images\nacquired by different sensors using a single forward pass. The resulting\nfeature detector is tightly coupled with the feature descriptor, in contrast to\nclassical approaches (SIFT, etc.), where the detection phase precedes and\ndiffers from computing the descriptor. Our approach utilizes two CNN\nsubnetworks, the first being a Siamese CNN and the second, consisting of dual\nnon-weight-sharing CNNs. This allows simultaneous processing and fusion of the\njoint and disjoint cues in the multimodal image patches. The proposed approach\nis experimentally shown to outperform contemporary state-of-the-art schemes\nwhen applied to multiple datasets of multimodal images. It is also shown to\nprovide repeatable feature points detections across multisensor images,\noutperforming state-of-the-art detectors. To the best of our knowledge, it is\nthe first unified approach for the detection and matching of such images.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 18:06:58 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 14:43:23 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 10:12:04 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Baruch", "Elad Ben", ""], ["Keller", "Yosi", ""]]}, {"id": "1810.12959", "submitter": "Han Liu", "authors": "Han Liu, Lei Wang, Yandong Nan, Faguang Jin, Qi Wang, Jiantao Pu", "title": "SDFN: Segmentation-based Deep Fusion Network for Thoracic Disease\n  Classification in Chest X-ray Images", "comments": "10 pages, 9 figures", "journal-ref": "Comput Med Imaging Graph, 2019", "doi": "10.1016/j.compmedimag.2019.05.005", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aims to automatically diagnose thoracic diseases depicted on the\nchest x-ray (CXR) images using deep convolutional neural networks. The existing\nmethods generally used the entire CXR images for training purposes, but this\nstrategy may suffer from two drawbacks. First, potential misalignment or the\nexistence of irrelevant objects in the entire CXR images may cause unnecessary\nnoise and thus limit the network performance. Second, the relatively low image\nresolution caused by the resizing operation, which is a common preprocessing\nprocedure for training neural networks, may lead to the loss of image details,\nmaking it difficult to detect pathologies with small lesion regions. To address\nthese issues, we present a novel method termed as segmentation-based deep\nfusion network (SDFN), which leverages the domain knowledge and the\nhigherresolution information of local lung regions. Specifically, the local\nlung regions were identified and cropped by the Lung Region Generator (LRG).\nTwo CNN-based classification models were then used as feature extractors to\nobtain the discriminative features of the entire CXR images and the cropped\nlung region images. Lastly, the obtained features were fused by the feature\nfusion module for disease classification. Evaluated by the NIH benchmark split\non the Chest X-ray 14 Dataset, our experimental result demonstrated that the\ndeveloped method achieved more accurate disease classification compared with\nthe available approaches via the receiver operating characteristic (ROC)\nanalyses. It was also found that the SDFN could localize the lesion regions\nmore precisely as compared to the traditional method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 18:37:10 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 19:15:38 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Han", ""], ["Wang", "Lei", ""], ["Nan", "Yandong", ""], ["Jin", "Faguang", ""], ["Wang", "Qi", ""], ["Pu", "Jiantao", ""]]}, {"id": "1810.12988", "submitter": "Oscar Rahnama", "authors": "Oscar Rahnama, Tommaso Cavallari, Stuart Golodetz, Simon Walker,\n  Philip H. S. Torr", "title": "R$^3$SGM: Real-time Raster-Respecting Semi-Global Matching for\n  Power-Constrained Systems", "comments": "Accepted in FPT 2018 as Oral presentation, 8 pages, 6 figures, 4\n  tables", "journal-ref": "2018 International Conference on Field-Programmable Technology\n  (FPT)", "doi": "10.1109/FPT.2018.00025", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo depth estimation is used for many computer vision applications. Though\nmany popular methods strive solely for depth quality, for real-time mobile\napplications (e.g. prosthetic glasses or micro-UAVs), speed and power\nefficiency are equally, if not more, important. Many real-world systems rely on\nSemi-Global Matching (SGM) to achieve a good accuracy vs. speed balance, but\npower efficiency is hard to achieve with conventional hardware, making the use\nof embedded devices such as FPGAs attractive for low-power applications.\nHowever, the full SGM algorithm is ill-suited to deployment on FPGAs, and so\nmost FPGA variants of it are partial, at the expense of accuracy. In a non-FPGA\ncontext, the accuracy of SGM has been improved by More Global Matching (MGM),\nwhich also helps tackle the streaking artifacts that afflict SGM. In this\npaper, we propose a novel, resource-efficient method that is inspired by MGM's\ntechniques for improving depth quality, but which can be implemented to run in\nreal time on a low-power FPGA. Through evaluation on multiple datasets (KITTI\nand Middlebury), we show that in comparison to other real-time capable stereo\napproaches, we can achieve a state-of-the-art balance between accuracy, power\nefficiency and speed, making our approach highly desirable for use in real-time\nsystems with limited power.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 20:10:02 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Rahnama", "Oscar", ""], ["Cavallari", "Tommaso", ""], ["Golodetz", "Stuart", ""], ["Walker", "Simon", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1810.13044", "submitter": "Imtiaz Ziko", "authors": "Imtiaz Masud Ziko, Eric Granger and Ismail Ben Ayed", "title": "Scalable Laplacian K-modes", "comments": "13 pages, 11 figures. Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate Laplacian K-modes for joint clustering and density mode finding,\nand propose a concave-convex relaxation of the problem, which yields a parallel\nalgorithm that scales up to large datasets and high dimensions. We optimize a\ntight bound (auxiliary function) of our relaxation, which, at each iteration,\namounts to computing an independent update for each cluster-assignment\nvariable, with guaranteed convergence. Therefore, our bound optimizer can be\ntrivially distributed for large-scale data sets. Furthermore, we show that the\ndensity modes can be obtained as byproducts of the assignment variables via\nsimple maximum-value operations whose additional computational cost is linear\nin the number of data points. Our formulation does not need storing a full\naffinity matrix and computing its eigenvalue decomposition, neither does it\nperform expensive projection steps and Lagrangian-dual inner iterates for the\nsimplex constraints of each point. Furthermore, unlike mean-shift, our\ndensity-mode estimation does not require inner-loop gradient-ascent iterates.\nIt has a complexity independent of feature-space dimension, yields modes that\nare valid data points in the input set and is applicable to discrete domains as\nwell as arbitrary kernels. We report comprehensive experiments over various\ndata sets, which show that our algorithm yields very competitive performances\nin term of optimization quality (i.e., the value of the discrete-variable\nobjective at convergence) and clustering accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 00:01:31 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 19:30:18 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Ziko", "Imtiaz Masud", ""], ["Granger", "Eric", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1810.13049", "submitter": "Siyuan Huang", "authors": "Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu,\n  Song-Chun Zhu", "title": "Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout,\n  and Camera Pose Estimation", "comments": "Accepted by NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holistic 3D indoor scene understanding refers to jointly recovering the i)\nobject bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The\nexisting methods either are ineffective or only tackle the problem partially.\nIn this paper, we propose an end-to-end model that simultaneously solves all\nthree tasks in real-time given only a single RGB image. The essence of the\nproposed method is to improve the prediction by i) parametrizing the targets\n(e.g., 3D boxes) instead of directly estimating the targets, and ii)\ncooperative training across different modules in contrast to training these\nmodules individually. Specifically, we parametrize the 3D object bounding boxes\nby the predictions from several modules, i.e., 3D camera pose and object\nattributes. The proposed method provides two major advantages: i) The\nparametrization helps maintain the consistency between the 2D image and the 3D\nworld, thus largely reducing the prediction variances in 3D coordinates. ii)\nConstraints can be imposed on the parametrization to train different modules\nsimultaneously. We call these constraints \"cooperative losses\" as they enable\nthe joint training and inference. We employ three cooperative losses for 3D\nbounding boxes, 2D projections, and physical constraints to estimate a\ngeometrically consistent and physically plausible 3D scene. Experiments on the\nSUN RGB-D dataset shows that the proposed method significantly outperforms\nprior approaches on 3D object detection, 3D layout estimation, 3D camera pose\nestimation, and holistic scene understanding.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 00:30:08 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 02:17:57 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Huang", "Siyuan", ""], ["Qi", "Siyuan", ""], ["Xiao", "Yinxue", ""], ["Zhu", "Yixin", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1810.13054", "submitter": "Ayan Kumar Bhunia", "authors": "Perla Sai Raj Kishore, Ayan Kumar Bhunia, Shuvozit Ghose, Partha\n  Pratim Roy", "title": "User Constrained Thumbnail Generation using Adaptive Convolutions", "comments": "International Conference on Acoustics, Speech, and Signal\n  Processing(ICASSP), 2019", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683761", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thumbnails are widely used all over the world as a preview for digital\nimages. In this work we propose a deep neural framework to generate thumbnails\nof any size and aspect ratio, even for unseen values during training, with high\naccuracy and precision. We use Global Context Aggregation (GCA) and a modified\nRegion Proposal Network (RPN) with adaptive convolutions to generate thumbnails\nin real time. GCA is used to selectively attend and aggregate the global\ncontext information from the entire image while the RPN is used to predict\ncandidate bounding boxes for the thumbnail image. Adaptive convolution\neliminates the problem of generating thumbnails of various aspect ratios by\nusing filter weights dynamically generated from the aspect ratio information.\nThe experimental results indicate the superior performance of the proposed\nmodel over existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 00:57:13 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 03:16:07 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 02:23:39 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Kishore", "Perla Sai Raj", ""], ["Bhunia", "Ayan Kumar", ""], ["Ghose", "Shuvozit", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1810.13059", "submitter": "Yi Zhang", "authors": "Wenchao Du, Hu Chen, Peixi Liao, Hongyu Yang, Ge Wang, Yi Zhang", "title": "Visual Attention Network for Low Dose CT", "comments": "5 pages, 6 figures. To appear on IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2019.2922851", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise and artifacts are intrinsic to low dose CT (LDCT) data acquisition, and\nwill significantly affect the imaging performance. Perfect noise removal and\nimage restoration is intractable in the context of LDCT due to the statistical\nand technical uncertainties. In this paper, we apply the generative adversarial\nnetwork (GAN) framework with a visual attention mechanism to deal with this\nproblem in a data-driven/machine learning fashion. Our main idea is to inject\nvisual attention knowledge into the learning process of GAN to provide a\npowerful prior of the noise distribution. By doing this, both the generator and\ndiscriminator networks are empowered with visual attention information so they\nwill not only pay special attention to noisy regions and surrounding structures\nbut also explicitly assess the local consistency of the recovered regions. Our\nexperiments qualitatively and quantitatively demonstrate the effectiveness of\nthe proposed method with clinic CT images.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 01:12:03 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 01:34:47 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Du", "Wenchao", ""], ["Chen", "Hu", ""], ["Liao", "Peixi", ""], ["Yang", "Hongyu", ""], ["Wang", "Ge", ""], ["Zhang", "Yi", ""]]}, {"id": "1810.13103", "submitter": "Zhongdao Wang", "authors": "Zhongdao Wang, Liang Zheng, Shengjin Wang", "title": "Query Adaptive Late Fusion for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature fusion is a commonly used strategy in image retrieval tasks, which\naggregates the matching responses of multiple visual features. Feasible sets of\nfeatures can be either descriptors (SIFT, HSV) for an entire image or the same\ndescriptor for different local parts (face, body). Ideally, the to-be-fused\nheterogeneous features are pre-assumed to be discriminative and complementary\nto each other. However, the effectiveness of different features varies\ndramatically according to different queries. That is to say, for some queries,\na feature may be neither discriminative nor complementary to existing ones,\nwhile for other queries, the feature suffices. As a result, it is important to\nestimate the effectiveness of features in a query-adaptive manner. To this end,\nthis article proposes a new late fusion scheme at the score level. We base our\nmethod on the observation that the sorted score curves contain patterns that\ndescribe their effectiveness. For example, an \"L\"-shaped curve indicates that\nthe feature is discriminative while a gradually descending curve suggests a bad\nfeature. As such, this paper introduces a query-adaptive late fusion pipeline.\nIn the hand-crafted version, it can be an unsupervised approach to tasks like\nparticular object retrieval. In the learning version, it can also be applied to\nsupervised tasks like person recognition and pedestrian retrieval, based on a\ntrainable neural module. Extensive experiments are conducted on two object\nretrieval datasets and one person recognition dataset. We show that our method\nis able to highlight the good features and suppress the bad ones, is resilient\nto distractor features, and achieves very competitive retrieval accuracy\ncompared with the state of the art. In an additional person re-identification\ndataset, the application scope and limitation of the proposed method are\nstudied.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 04:51:16 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Wang", "Zhongdao", ""], ["Zheng", "Liang", ""], ["Wang", "Shengjin", ""]]}, {"id": "1810.13125", "submitter": "Kaiyu Yue", "authors": "Kaiyu Yue, Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding, Fuxin Xu", "title": "Compact Generalized Non-local Network", "comments": "Technical report; To appear at NIPS 2018; Code is available at\n  https://github.com/KaiyuYue/cgnl-network.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-local module is designed for capturing long-range spatio-temporal\ndependencies in images and videos. Although having shown excellent performance,\nit lacks the mechanism to model the interactions between positions across\nchannels, which are of vital importance in recognizing fine-grained objects and\nactions. To address this limitation, we generalize the non-local module and\ntake the correlations between the positions of any two channels into account.\nThis extension utilizes the compact representation for multiple kernel\nfunctions with Taylor expansion that makes the generalized non-local module in\na fast and low-complexity computation flow. Moreover, we implement our\ngeneralized non-local method within channel groups to ease the optimization.\nExperimental results illustrate the clear-cut improvements and practical\napplicability of the generalized non-local module on both fine-grained object\nrecognition and video classification. Code is available at:\nhttps://github.com/KaiyuYue/cgnl-network.pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 06:43:14 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 03:24:51 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yue", "Kaiyu", ""], ["Sun", "Ming", ""], ["Yuan", "Yuchen", ""], ["Zhou", "Feng", ""], ["Ding", "Errui", ""], ["Xu", "Fuxin", ""]]}, {"id": "1810.13128", "submitter": "Megha Srivastava", "authors": "Megha Srivastava, Kalanit Grill-Spector", "title": "The Effect of Learning Strategy versus Inherent Architecture Properties\n  on the Ability of Convolutional Neural Networks to Develop Transformation\n  Invariance", "comments": "11 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As object recognition becomes an increasingly common ML task, and recent\nresearch demonstrating CNNs vulnerability to attacks and small image\nperturbations necessitate fully understanding the foundations of object\nrecognition. We focus on understanding the mechanisms behind how neural\nnetworks generalize to spatial transformations of complex objects. While humans\nexcel at discriminating between objects shown at new positions, orientations,\nand scales, past results demonstrate that this may be limited to familiar\nobjects - humans demonstrate low tolerance of spatial-variances for\npurposefully constructed novel objects. Because training artificial neural\nnetworks from scratch is similar to showing novel objects to humans, we seek to\nunderstand the factors influencing the tolerance of CNNs to spatial\ntransformations. We conduct a thorough empirical examination of seven\nConvolutional Neural Network (CNN) architectures. By training on a controlled\nface image dataset, we measure model accuracy across different degrees of 5\ntransformations: position, size, rotation, Gaussian blur, and resolution\ntransformation due to resampling. We also examine how learning strategy affects\ngeneralizability by examining how different amounts of pre-training have on\nmodel robustness. Overall, we find that the most significant contributor to\ntransformation invariance is pre-training on a large, diverse image dataset.\nMoreover, while AlexNet tends to be the least robust network, VGG and ResNet\narchitectures demonstrate higher robustness for different transformations.\nAlong with kernel visualizations and qualitative analyses, we examine\ndifferences between learning strategy and inherent architectural properties in\ncontributing to invariance of transformations, providing valuable information\ntowards understanding how to achieve greater robustness to transformations in\nCNNs.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 06:48:15 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Srivastava", "Megha", ""], ["Grill-Spector", "Kalanit", ""]]}, {"id": "1810.13166", "submitter": "Natalia D\\'iaz-Rodr\\'iguez", "authors": "Natalia D\\'iaz-Rodr\\'iguez and Vincenzo Lomonaco and David Filliat and\n  Davide Maltoni", "title": "Don't forget, there is more than forgetting: new metrics for Continual\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning consists of algorithms that learn from a stream of\ndata/tasks continuously and adaptively thought time, enabling the incremental\ndevelopment of ever more complex knowledge and skills. The lack of consensus in\nevaluating continual learning algorithms and the almost exclusive focus on\nforgetting motivate us to propose a more comprehensive set of implementation\nindependent metrics accounting for several factors we believe have practical\nimplications worth considering in the deployment of real AI systems that learn\ncontinually: accuracy or performance over time, backward and forward knowledge\ntransfer, memory overhead as well as computational efficiency. Drawing\ninspiration from the standard Multi-Attribute Value Theory (MAVT) we further\npropose to fuse these metrics into a single score for ranking purposes and we\nevaluate our proposal with five continual learning strategies on the iCIFAR-100\ncontinual learning benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:15:02 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Lomonaco", "Vincenzo", ""], ["Filliat", "David", ""], ["Maltoni", "Davide", ""]]}, {"id": "1810.13169", "submitter": "Seongmin Hwang", "authors": "Seongmin Hwang, Gwanghyun Yu, Huy Toan Nguyen, Nazeer Shahid, Doseong\n  Sin, Jinyoung Kim and Seungyou Na", "title": "Inception-Residual Block based Neural Network for Thermal Image\n  Denoising", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal cameras show noisy images due to their limited thermal resolution,\nespecially for the scenes of a low temperature difference. In order to deal\nwith a noise problem, this paper proposes a novel neural network architecture\nwith repeatable denoising inception-residual blocks(DnIRB) for noise learning.\nEach DnIRB has two sub-blocks with difference receptive fields and one shortcut\nconnection to prevent a vanishing gradient problem. The proposed approach is\ntested for thermal images. The experimental results indicate that the proposed\napproach shows the best SQNR performance and reasonable processing time\ncompared with state-of-the-art denoising methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:18:55 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 05:02:24 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Hwang", "Seongmin", ""], ["Yu", "Gwanghyun", ""], ["Nguyen", "Huy Toan", ""], ["Shahid", "Nazeer", ""], ["Sin", "Doseong", ""], ["Kim", "Jinyoung", ""], ["Na", "Seungyou", ""]]}, {"id": "1810.13170", "submitter": "Zhaoqiang Xia", "authors": "Lei Li, Zhaoqiang Xia, Xiaoyue Jiang, Fabio Roli, and Xiaoyi Feng", "title": "Face Presentation Attack Detection in Learned Color-liked Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face presentation attack detection (PAD) has become a thorny problem for\nbiometric systems and numerous countermeasures have been proposed to address\nit. However, majority of them directly extract feature descriptors and\ndistinguish fake faces from the real ones in existing color spaces (e.g. RGB,\nHSV and YCbCr). Unfortunately, it is unknown for us which color space is the\nbest or how to combine different spaces together. To make matters worse, the\nreal and fake faces are overlapped in existing color spaces. So, in this paper,\na learned distinguishable color-liked space is generated to deal with the\nproblem of face PAD. More specifically, we present an end-to-end deep learning\nnetwork that can map existing color spaces to a new learned color-liked space.\nInspired by the generator of generative adversarial network (GAN), the proposed\nnetwork consists of a space generator and a feature extractor. When training\nthe color-liked space, a new triplet combination mechanism of points-to-center\nis explored to maximize interclass distance and minimize intraclass distance,\nand also keep a safe margin between the real and presented fake faces.\nExtensive experiments on two standard face PAD databases, i.e., Relay-Attack\nand OULU-NPU, indicate that our proposed color-liked space analysis based\ncountermeasure significantly outperforms the state-of-the-art methods and show\nexcellent generalization capability.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:20:06 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 06:03:19 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Li", "Lei", ""], ["Xia", "Zhaoqiang", ""], ["Jiang", "Xiaoyue", ""], ["Roli", "Fabio", ""], ["Feng", "Xiaoyi", ""]]}, {"id": "1810.13197", "submitter": "Valentin Vielzeuf", "authors": "Valentin Vielzeuf, Corentin Kervadec, St\\'ephane Pateux, Fr\\'ed\\'eric\n  Jurie", "title": "The Many Moods of Emotion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to the facial expression generation\nproblem. Building upon the assumption of the psychological community that\nemotion is intrinsically continuous, we first design our own continuous emotion\nrepresentation with a 3-dimensional latent space issued from a neural network\ntrained on discrete emotion classification. The so-obtained representation can\nbe used to annotate large in the wild datasets and later used to trained a\nGenerative Adversarial Network. We first show that our model is able to map\nback to discrete emotion classes with a objectively and subjectively better\nquality of the images than usual discrete approaches. But also that we are able\nto pave the larger space of possible facial expressions, generating the many\nmoods of emotion. Moreover, two axis in this space may be found to generate\nsimilar expression changes as in traditional continuous representations such as\narousal-valence. Finally we show from visual interpretation, that the third\nremaining dimension is highly related to the well-known dominance dimension\nfrom psychology.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:24:08 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Kervadec", "Corentin", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1810.13200", "submitter": "Amirafshar Moshtaghpour", "authors": "Amirafshar Moshtaghpour and Jos\\'e M. Bioucas-Dias and Laurent Jacques", "title": "Compressive Single-pixel Fourier Transform Imaging using Structured\n  Illumination", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Pixel (SP) imaging is now a reality in many applications, e.g.,\nbiomedical ultrathin endoscope and fluorescent spectroscopy. In this context,\nmany schemes exist to improve the light throughput of these device, e.g., using\nstructured illumination driven by compressive sensing theory. In this work, we\nconsider the combination of SP imaging with Fourier Transform Interferometry\n(SP-FTI) to reach high-resolution HyperSpectral (HS) imaging, as desirable,\ne.g., in fluorescent spectroscopy. While this association is not new, we here\nfocus on optimizing the spatial illumination, structured as Hadamard patterns,\nduring the optical path progression. We follow a variable density sampling\nstrategy for space-time coding of the light illumination, and show\ntheoretically and numerically that this scheme allows us to reduce the number\nof measurements and light-exposure of the observed object compared to\nconventional compressive SP-FTI.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:29:07 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 11:30:23 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Moshtaghpour", "Amirafshar", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Jacques", "Laurent", ""]]}, {"id": "1810.13205", "submitter": "Chen Chen", "authors": "Chen Chen, Wenjia Bai, Daniel Rueckert", "title": "Multi-Task Learning for Left Atrial Segmentation on GE-MRI", "comments": "STACOM 2018 Workshop, MICCAI 2018", "journal-ref": "Statistical Atlases and Computational Models of the Heart. Atrial\n  Segmentation and LV Quantification Challenges, 9th International Workshop,\n  STACOM 2018, pp.292-301", "doi": "10.1007/978-3-030-12029-0_32", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the left atrium (LA) is crucial for assessing its anatomy in\nboth pre-operative atrial fibrillation (AF) ablation planning and\npost-operative follow-up studies. In this paper, we present a fully automated\nframework for left atrial segmentation in gadolinium-enhanced magnetic\nresonance images (GE-MRI) based on deep learning. We propose a fully\nconvolutional neural network and explore the benefits of multi-task learning\nfor performing both atrial segmentation and pre/post ablation classification.\nOur results show that, by sharing features between related tasks, the network\ncan gain additional anatomical information and achieve more accurate atrial\nsegmentation, leading to a mean Dice score of 0.901 on a test set of 20 3D MRI\nimages. Code of our proposed algorithm is available at\nhttps://github.com/cherise215/atria_segmentation_2018/.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:47:06 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Chen", "Chen", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1810.13230", "submitter": "Tahsin Kurc", "authors": "Quoc Dang Vu, Simon Graham, Minh Nguyen Nhat To, Muhammad Shaban,\n  Talha Qaiser, Navid Alemi Koohbanani, Syed Ali Khurram, Tahsin Kurc, Keyvan\n  Farahani, Tianhao Zhao, Rajarsi Gupta, Jin Tae Kwak, Nasir Rajpoot, Joel\n  Saltz", "title": "Methods for Segmentation and Classification of Digital Microscopy Tissue\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution microscopy images of tissue specimens provide detailed\ninformation about the morphology of normal and diseased tissue. Image analysis\nof tissue morphology can help cancer researchers develop a better understanding\nof cancer biology. Segmentation of nuclei and classification of tissue images\nare two common tasks in tissue image analysis. Development of accurate and\nefficient algorithms for these tasks is a challenging problem because of the\ncomplexity of tissue morphology and tumor heterogeneity. In this paper we\npresent two computer algorithms; one designed for segmentation of nuclei and\nthe other for classification of whole slide tissue images. The segmentation\nalgorithm implements a multiscale deep residual aggregation network to\naccurately segment nuclear material and then separate clumped nuclei into\nindividual nuclei. The classification algorithm initially carries out\npatch-level classification via a deep learning method, then patch-level\nstatistical and morphological features are used as input to a random forest\nregression model for whole slide image classification. The segmentation and\nclassification algorithms were evaluated in the MICCAI 2017 Digital Pathology\nchallenge. The segmentation algorithm achieved an accuracy score of 0.78. The\nclassification algorithm achieved an accuracy score of 0.81.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 12:03:27 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 19:08:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Vu", "Quoc Dang", ""], ["Graham", "Simon", ""], ["To", "Minh Nguyen Nhat", ""], ["Shaban", "Muhammad", ""], ["Qaiser", "Talha", ""], ["Koohbanani", "Navid Alemi", ""], ["Khurram", "Syed Ali", ""], ["Kurc", "Tahsin", ""], ["Farahani", "Keyvan", ""], ["Zhao", "Tianhao", ""], ["Gupta", "Rajarsi", ""], ["Kwak", "Jin Tae", ""], ["Rajpoot", "Nasir", ""], ["Saltz", "Joel", ""]]}, {"id": "1810.13273", "submitter": "Alexandre Boulch", "authors": "Alexandre Boulch and No\\\"elie Cherrier and Thibaut Castaings", "title": "Ionospheric activity prediction using convolutional recurrent neural\n  networks", "comments": "Under submission at IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ionosphere electromagnetic activity is a major factor of the quality of\nsatellite telecommunications, Global Navigation Satellite Systems (GNSS) and\nother vital space applications. Being able to forecast globally the Total\nElectron Content (TEC) would enable a better anticipation of potential\nperformance degradations. A few studies have proposed models able to predict\nthe TEC locally, but not worldwide for most of them. Thanks to a large record\nof past TEC maps publicly available, we propose a method based on Deep Neural\nNetworks (DNN) to forecast a sequence of global TEC maps consecutive to an\ninput sequence of TEC maps, without introducing any prior knowledge other than\nEarth rotation periodicity. By combining several state-of-the-art\narchitectures, the proposed approach is competitive with previous works on TEC\nforecasting while predicting the TEC globally.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 13:25:17 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 12:14:15 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Boulch", "Alexandre", ""], ["Cherrier", "No\u00eblie", ""], ["Castaings", "Thibaut", ""]]}, {"id": "1810.13292", "submitter": "Duc Tam Nguyen", "authors": "Duc Tam Nguyen, Zhongyu Lou, Michael Klar, Thomas Brox", "title": "Anomaly Detection With Multiple-Hypotheses Predictions", "comments": "In proceedings of the 36th International Conference on Machine\n  Learning (ICML), Long Beach, California, PMLR 97, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one-class-learning tasks, only the normal case (foreground) can be modeled\nwith data, whereas the variation of all possible anomalies is too erratic to be\ndescribed by samples. Thus, due to the lack of representative data, the\nwide-spread discriminative approaches cannot cover such learning tasks, and\nrather generative models, which attempt to learn the input density of the\nforeground, are used. However, generative models suffer from a large input\ndimensionality (as in images) and are typically inefficient learners. We\npropose to learn the data distribution of the foreground more efficiently with\na multi-hypotheses autoencoder. Moreover, the model is criticized by a\ndiscriminator, which prevents artificial data modes not supported by data, and\nenforces diversity across hypotheses. Our multiple-hypothesesbased anomaly\ndetection framework allows the reliable identification of out-of-distribution\nsamples. For anomaly detection on CIFAR-10, it yields up to 3.9% points\nimprovement over previously reported results. On a real anomaly detection task,\nthe approach reduces the error of the baseline models from 6.8% to 1.5%.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:05:44 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 12:36:22 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 06:13:45 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 15:05:09 GMT"}, {"version": "v5", "created": "Fri, 31 May 2019 12:25:42 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Nguyen", "Duc Tam", ""], ["Lou", "Zhongyu", ""], ["Klar", "Michael", ""], ["Brox", "Thomas", ""]]}, {"id": "1810.13304", "submitter": "Albert Cl\\`erigues", "authors": "Albert Cl\\`erigues, Sergi Valverde, Jose Bernal, Jordi Freixenet,\n  Arnau Oliver, Xavier Llad\\'o", "title": "Acute and sub-acute stroke lesion segmentation from multimodal MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Acute stroke lesion segmentation tasks are of great clinical interest as they\ncan help doctors make better informed treatment decisions. Magnetic resonance\nimaging (MRI) is time demanding but can provide images that are considered gold\nstandard for diagnosis. Automated stroke lesion segmentation can provide with\nan estimate of the location and volume of the lesioned tissue, which can help\nin the clinical practice to better assess and evaluate the risks of each\ntreatment. We propose a deep learning methodology for acute and sub-acute\nstroke lesion segmentation using multimodal MR imaging. The proposed method is\nevaluated using two public datasets from the 2015 Ischemic Stroke Lesion\nSegmentation challenge (ISLES 2015). These involve the tasks of sub-acute\nstroke lesion segmentation (SISS) and acute stroke penumbra estimation (SPES)\nfrom diffusion, perfusion and anatomical MRI modalities. The performance is\ncompared against state-of-the-art methods with a blind online testing set\nevaluation on each of the challenges. At the time of submitting this\nmanuscript, our approach is the first method in the online rankings for the\nSISS (DSC=0.59$\\pm$0.31) and SPES sub-tasks (DSC=0.84$\\pm$0.10). When compared\nwith the rest of submitted strategies, we achieve top rank performance with a\nlower Hausdorff distance. Better segmentation results are obtained by\nleveraging the anatomy and pathophysiology of acute stroke lesions and using a\ncombined approach to minimize the effects of class imbalance. The same training\nprocedure is used for both tasks, showing the proposed methodology can\ngeneralize well enough to deal with different unrelated tasks and imaging\nmodalities without training hyper-parameter tuning. A public version of the\nproposed method has been released to the scientific community at\nhttps://github.com/NIC-VICOROB/stroke-mri-segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:34:39 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 07:59:58 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Cl\u00e8rigues", "Albert", ""], ["Valverde", "Sergi", ""], ["Bernal", "Jose", ""], ["Freixenet", "Jordi", ""], ["Oliver", "Arnau", ""], ["Llad\u00f3", "Xavier", ""]]}, {"id": "1810.13373", "submitter": "David Barrett", "authors": "David G.T. Barrett, Ari S. Morcos and Jakob H. Macke", "title": "Analyzing biological and artificial neural networks: challenges with\n  opportunities for synergy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) transform stimuli across multiple processing\nstages to produce representations that can be used to solve complex tasks, such\nas object recognition in images. However, a full understanding of how they\nachieve this remains elusive. The complexity of biological neural networks\nsubstantially exceeds the complexity of DNNs, making it even more challenging\nto understand the representations that they learn. Thus, both machine learning\nand computational neuroscience are faced with a shared challenge: how can we\nanalyze their representations in order to understand how they solve complex\ntasks?\n  We review how data-analysis concepts and techniques developed by\ncomputational neuroscientists can be useful for analyzing representations in\nDNNs, and in turn, how recently developed techniques for analysis of DNNs can\nbe useful for understanding representations in biological neural networks. We\nexplore opportunities for synergy between the two fields, such as the use of\nDNNs as in-silico model systems for neuroscience, and how this synergy can lead\nto new hypotheses about the operating principles of biological neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 16:09:44 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Barrett", "David G. T.", ""], ["Morcos", "Ari S.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "1810.13376", "submitter": "Lei Wang", "authors": "Yi Zhen, Lei Wang, Han Liu, Jian Zhang, Jiantao Pu", "title": "Performance assessment of the deep learning technologies in grading\n  glaucoma severity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: To validate and compare the performance of eight available deep\nlearning architectures in grading the severity of glaucoma based on color\nfundus images. Materials and Methods: We retrospectively collected a dataset of\n5978 fundus images and their glaucoma severities were annotated by the\nconsensus of two experienced ophthalmologists. We preprocessed the images to\ngenerate global and local regions of interest (ROIs), namely the global\nfield-of-view images and the local disc region images. We then divided the\ngenerated images into three independent sub-groups for training, validation,\nand testing purposes. With the datasets, eight convolutional neural networks\n(CNNs) (i.e., VGG16, VGG19, ResNet, DenseNet, InceptionV3, InceptionResNet,\nXception, and NASNetMobile) were trained separately to grade glaucoma severity,\nand validated quantitatively using the area under the receiver operating\ncharacteristic (ROC) curve and the quadratic kappa score. Results: The CNNs,\nexcept VGG16 and VGG19, achieved average kappa scores of 80.36% and 78.22% when\ntrained from scratch on global and local ROIs, and 85.29% and 82.72% when\nfine-tuned using the pre-trained weights, respectively. VGG16 and VGG19\nachieved reasonable accuracy when trained from scratch, but they failed when\nusing pre-trained weights for global and local ROIs. Among these CNNs, the\nDenseNet had the highest classification accuracy (i.e., 75.50%) based on\npre-trained weights when using global ROIs, as compared to 65.50% when using\nlocal ROIs. Conclusion: The experiments demonstrated the feasibility of the\ndeep learning technology in grading glaucoma severity. In particular, global\nfield-of-view images contain relatively richer information that may be critical\nfor glaucoma assessment, suggesting that we should use the entire field-of-view\nof a fundus image for training a deep learning network.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 16:12:30 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Zhen", "Yi", ""], ["Wang", "Lei", ""], ["Liu", "Han", ""], ["Zhang", "Jian", ""], ["Pu", "Jiantao", ""]]}, {"id": "1810.13404", "submitter": "Philipp Seeb\\\"ock", "authors": "Philipp Seeb\\\"ock, Sebastian M. Waldstein, Sophie Klimscha, Hrvoje\n  Bogunovic, Thomas Schlegl, Bianca S. Gerendas, Ren\\'e Donner, Ursula\n  Schmidt-Erfurth, Georg Langs", "title": "Unsupervised Identification of Disease Marker Candidates in Retinal OCT\n  Imaging Data", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging,\n  2018", "journal-ref": null, "doi": "10.1109/TMI.2018.2877080", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and quantification of markers in medical images is\ncritical for diagnosis, prognosis, and disease management. Supervised machine\nlearning enables the detection and exploitation of findings that are known a\npriori after annotation of training examples by experts. However, supervision\ndoes not scale well, due to the amount of necessary training examples, and the\nlimitation of the marker vocabulary to known entities. In this proof-of-concept\nstudy, we propose unsupervised identification of anomalies as candidates for\nmarkers in retinal Optical Coherence Tomography (OCT) imaging data without a\nconstraint to a priori definitions. We identify and categorize marker\ncandidates occurring frequently in the data, and demonstrate that these markers\nshow predictive value in the task of detecting disease. A careful qualitative\nanalysis of the identified data driven markers reveals how their quantifiable\noccurrence aligns with our current understanding of disease course, in early-\nand late age-related macular degeneration (AMD) patients. A multi-scale deep\ndenoising autoencoder is trained on healthy images, and a one-class support\nvector machine identifies anomalies in new data. Clustering in the anomalies\nidentifies stable categories. Using these markers to classify healthy-, early\nAMD- and late AMD cases yields an accuracy of 81.40%. In a second binary\nclassification experiment on a publicly available data set (healthy vs.\nintermediate AMD) the model achieves an area under the ROC curve of 0.944.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 16:55:46 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Seeb\u00f6ck", "Philipp", ""], ["Waldstein", "Sebastian M.", ""], ["Klimscha", "Sophie", ""], ["Bogunovic", "Hrvoje", ""], ["Schlegl", "Thomas", ""], ["Gerendas", "Bianca S.", ""], ["Donner", "Ren\u00e9", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Langs", "Georg", ""]]}]