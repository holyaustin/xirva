[{"id": "1412.0003", "submitter": "Hao Su", "authors": "Hao Su, Fan Wang, Li Yi, Leonidas Guibas", "title": "3D-Assisted Image Feature Synthesis for Novel Views of an Object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing two images in a view-invariant way has been a challenging problem\nin computer vision for a long time, as visual features are not stable under\nlarge view point changes. In this paper, given a single input image of an\nobject, we synthesize new features for other views of the same object. To\naccomplish this, we introduce an aligned set of 3D models in the same class as\nthe input object image. Each 3D model is represented by a set of views, and we\nstudy the correlation of image patches between different views, seeking what we\ncall surrogates --- patches in one view whose feature content predicts well the\nfeatures of a patch in another view. In particular, for each patch in the novel\ndesired view, we seek surrogates from the observed view of the given image. For\na given surrogate, we predict that surrogate using linear combination of the\ncorresponding patches of the 3D model views, learn the coefficients, and then\ntransfer these coefficients on a per patch basis to synthesize the features of\nthe patch in the novel view. In this way we can create feature sets for all\nviews of the latent object, providing us a multi-view representation of the\nobject. View-invariant object comparisons are achieved simply by computing the\n$L^2$ distances between the features of corresponding views. We provide\ntheoretical and empirical analysis of the feature synthesis process, and\nevaluate the proposed view-agnostic distance (VAD) in fine-grained image\nretrieval (100 object classes) and classification tasks. Experimental results\nshow that our synthesized features do enable view-independent comparison\nbetween images and perform significantly better than traditional image features\nin this respect.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 22:00:30 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Su", "Hao", ""], ["Wang", "Fan", ""], ["Yi", "Li", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1412.0035", "submitter": "Aravindh Mahendran", "authors": "Aravindh Mahendran, Andrea Vedaldi", "title": "Understanding Deep Image Representations by Inverting Them", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representations, from SIFT and Bag of Visual Words to Convolutional\nNeural Networks (CNNs), are a crucial component of almost any image\nunderstanding system. Nevertheless, our understanding of them remains limited.\nIn this paper we conduct a direct analysis of the visual information contained\nin representations by asking the following question: given an encoding of an\nimage, to which extent is it possible to reconstruct the image itself? To\nanswer this question we contribute a general framework to invert\nrepresentations. We show that this method can invert representations such as\nHOG and SIFT more accurately than recent alternatives while being applicable to\nCNNs too. We then use this technique to study the inverse of recent\nstate-of-the-art CNN image representations for the first time. Among our\nfindings, we show that several layers in CNNs retain photographically accurate\ninformation about the image, with different degrees of geometric and\nphotometric invariance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 18:51:52 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Mahendran", "Aravindh", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1412.0060", "submitter": "Gr\\'egory Rogez", "authors": "Gregory Rogez, James S. Supancic III and Deva Ramanan", "title": "Egocentric Pose Recognition in Four Lines of Code", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of estimating the 3D pose of an individual's upper\nlimbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider\npose estimation during everyday interactions with objects. Past work shows that\nstrong pose+viewpoint priors and depth-based features are crucial for robust\nperformance. In egocentric views, hands and arms are observable within a well\ndefined volume in front of the camera. We call this volume an egocentric\nworkspace. A notable property is that hand appearance correlates with workspace\nlocation. To exploit this correlation, we classify arm+hand configurations in a\nglobal egocentric coordinate frame, rather than a local scanning window. This\ngreatly simplify the architecture and improves performance. We propose an\nefficient pipeline which 1) generates synthetic workspace exemplars for\ntraining using a virtual chest-mounted camera whose intrinsic parameters match\nour physical camera, 2) computes perspective-aware depth features on this\nentire volume and 3) recognizes discrete arm+hand pose classes through a sparse\nmulti-class SVM. Our method provides state-of-the-art hand pose recognition\nperformance from egocentric RGB-D images in real-time.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 02:26:33 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Rogez", "Gregory", ""], ["Supancic", "James S.", "III"], ["Ramanan", "Deva", ""]]}, {"id": "1412.0062", "submitter": "Behnam Babagholami-Mohamadabadi", "authors": "Behnam Babagholami-Mohamadabadi, Amin Jourabloo, Ali Zarghami, Shohreh\n  Kasaei", "title": "A Bayesian Framework for Sparse Representation-Based 3D Human Pose\n  Estimation", "comments": "Accepted in IEEE Signal Processing Letters (SPL), 2014", "journal-ref": null, "doi": "10.1109/LSP.2014.2301726", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian framework for 3D human pose estimation from monocular images based\non sparse representation (SR) is introduced. Our probabilistic approach aims at\nsimultaneously learning two overcomplete dictionaries (one for the visual input\nspace and the other for the pose space) with a shared sparse representation.\nExisting SR-based pose estimation approaches only offer a point estimation of\nthe dictionary and the sparse codes. Therefore, they might be unreliable when\nthe number of training examples is small. Our Bayesian framework estimates a\nposterior distribution for the sparse codes and the dictionaries from labeled\ntraining data. Hence, it is robust to overfitting on small-size training data.\nExperimental results on various human activities show that the proposed method\nis superior to the state of-the-art pose estimation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 02:53:47 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Babagholami-Mohamadabadi", "Behnam", ""], ["Jourabloo", "Amin", ""], ["Zarghami", "Ali", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "1412.0065", "submitter": "Gr\\'egory Rogez", "authors": "Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria\n  Martinez Montiel, Deva Ramanan", "title": "3D Hand Pose Detection in Egocentric RGB-D Images", "comments": "14 pages, 15 figures, extended version of the corresponding ECCV\n  workshop paper, submitted to International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the task of everyday hand pose estimation from egocentric\nviewpoints. For this task, we show that depth sensors are particularly\ninformative for extracting near-field interactions of the camera wearer with\nhis/her environment. Despite the recent advances in full-body pose estimation\nusing Kinect-like sensors, reliable monocular hand pose estimation in RGB-D\nimages is still an unsolved problem. The problem is considerably exacerbated\nwhen analyzing hands performing daily activities from a first-person viewpoint,\ndue to severe occlusions arising from object manipulations and a limited\nfield-of-view. Our system addresses these difficulties by exploiting strong\npriors over viewpoint and pose in a discriminative tracking-by-detection\nframework. Our priors are operationalized through a photorealistic synthetic\nmodel of egocentric scenes, which is used to generate training data for\nlearning depth-based pose classifiers. We evaluate our approach on an annotated\ndataset of real egocentric object manipulation scenes and compare to both\ncommercial and academic approaches. Our method provides state-of-the-art\nperformance for both hand detection and pose estimation in egocentric RGB-D\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 03:19:56 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Rogez", "Gregory", ""], ["Supancic", "James S.", "III"], ["Khademi", "Maryam", ""], ["Montiel", "Jose Maria Martinez", ""], ["Ramanan", "Deva", ""]]}, {"id": "1412.0069", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang", "title": "Pedestrian Detection aided by Deep Learning Semantic Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have achieved great success in pedestrian detection,\nowing to its ability to learn features from raw pixels. However, they mainly\ncapture middle-level representations, such as pose of pedestrian, but confuse\npositive with hard negative samples, which have large ambiguity, e.g. the shape\nand appearance of `tree trunk' or `wire pole' are similar to pedestrian in\ncertain viewpoint. This ambiguity can be distinguished by high-level\nrepresentation. To this end, this work jointly optimizes pedestrian detection\nwith semantic tasks, including pedestrian attributes (e.g. `carrying backpack')\nand scene attributes (e.g. `road', `tree', and `horizontal'). Rather than\nexpensively annotating scene attributes, we transfer attributes information\nfrom existing scene segmentation datasets to the pedestrian dataset, by\nproposing a novel deep model to learn high-level features from multiple tasks\nand multiple data sources. Since distinct tasks have distinct convergence rates\nand data from different datasets have different distributions, a multi-task\nobjective function is carefully designed to coordinate tasks and reduce\ndiscrepancies among datasets. The importance coefficients of tasks and network\nparameters in this objective function can be iteratively estimated. Extensive\nevaluations show that the proposed approach outperforms the state-of-the-art on\nthe challenging Caltech and ETH datasets, where it reduces the miss rates of\nprevious deep models by 17 and 5.5 percent, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 04:34:23 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Tian", "Yonglong", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1412.0100", "submitter": "Stefan Mathe", "authors": "Stefan Mathe, Cristian Sminchisescu", "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised\n  Detection in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art visual recognition and detection systems increasingly rely\non large amounts of training data and complex classifiers. Therefore it becomes\nincreasingly expensive both to manually annotate datasets and to keep running\ntimes at levels acceptable for practical applications. In this paper, we\npropose two solutions to address these issues. First, we introduce a weakly\nsupervised, segmentation-based approach to learn accurate detectors and image\nclassifiers from weak supervisory signals that provide only approximate\nconstraints on target localization. We illustrate our system on the problem of\naction detection in static images (Pascal VOC Actions 2012), using human visual\nsearch patterns as our training signal. Second, inspired from the\nsaccade-and-fixate operating principle of the human visual system, we use\nreinforcement learning techniques to train efficient search models for\ndetection. Our sequential method is weakly supervised and general (it does not\nrequire eye movements), finds optimal search strategies for any given detection\nconfidence function and achieves performance similar to exhaustive sliding\nwindow search at a fraction of its computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 12:18:14 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Mathe", "Stefan", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1412.0111", "submitter": "Hocine Cherifi", "authors": "Mounir Omari, Abdelkaher Ait Abdelouahad, Mohammed El Hassouni and\n  Hocine Cherifi", "title": "Color image quality assessment measure using multivariate generalized\n  Gaussian distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with color image quality assessment in the reduced-reference\nframework based on natural scenes statistics. In this context, we propose to\nmodel the statistics of the steerable pyramid coefficients by a Multivariate\nGeneralized Gaussian distribution (MGGD). This model allows taking into account\nthe high correlation between the components of the RGB color space. For each\nselected scale and orientation, we extract a parameter matrix from the three\ncolor components subbands. In order to quantify the visual degradation, we use\na closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using\n\"TID 2008\" benchmark, the proposed measure has been compared with the most\ninfluential methods according to the FRTV1 VQEG framework. Results demonstrates\nits effectiveness for a great variety of distortion type. Among other benefits\nthis measure uses only very little information about the original image.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 14:45:14 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Omari", "Mounir", ""], ["Abdelouahad", "Abdelkaher Ait", ""], ["Hassouni", "Mohammed El", ""], ["Cherifi", "Hocine", ""]]}, {"id": "1412.0165", "submitter": "Onur Ozyesil", "authors": "Onur Ozyesil and Amit Singer", "title": "Robust Camera Location Estimation by Convex Programming", "comments": "10 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $3$D structure recovery from a collection of $2$D images requires the\nestimation of the camera locations and orientations, i.e. the camera motion.\nFor large, irregular collections of images, existing methods for the location\nestimation part, which can be formulated as the inverse problem of estimating\n$n$ locations $\\mathbf{t}_1, \\mathbf{t}_2, \\ldots, \\mathbf{t}_n$ in\n$\\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions\n$\\frac{\\mathbf{t}_i - \\mathbf{t}_j}{\\|\\mathbf{t}_i - \\mathbf{t}_j\\|}$, are\nsensitive to outliers in direction measurements. In this paper, we firstly\nprovide a complete characterization of well-posed instances of the location\nestimation problem, by presenting its relation to the existing theory of\nparallel rigidity. For robust estimation of camera locations, we introduce a\ntwo-step approach, comprised of a pairwise direction estimation method robust\nto outliers in point correspondences between image pairs, and a convex program\nto maintain robustness to outlier directions. In the presence of partially\ncorrupted measurements, we empirically demonstrate that our convex formulation\ncan even recover the locations exactly. Lastly, we demonstrate the utility of\nour formulations through experiments on Internet photo collections.\n", "versions": [{"version": "v1", "created": "Sat, 29 Nov 2014 23:25:41 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 03:23:21 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Ozyesil", "Onur", ""], ["Singer", "Amit", ""]]}, {"id": "1412.0218", "submitter": "Alexander Evako V", "authors": "Alexander V. Evako", "title": "Simple pairs of points in digital spaces. Topology-preserving\n  transformations of digital spaces by contracting simple pairs of points", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformations of digital spaces preserving local and global topology play\nan important role in thinning, skeletonization and simplification of digital\nimages. In the present paper, we introduce and study contractions of simple\npair of points based on the notions of a digital contractible space and\ncontractible transformations of digital spaces. We show that the contraction of\na simple pair of points preserves local and global topology of a digital space.\nRelying on the obtained results, we study properties if digital manifolds. In\nparticular, we show that a digital n-manifold can be transformed to its\ncompressed form with the minimal number of points by sequential contractions of\nsimple pairs.\n  Key Words: Graph, digital space, contraction, splitting, simple pair,\nhomotopy, thinning\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 14:00:37 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Evako", "Alexander V.", ""]]}, {"id": "1412.0251", "submitter": "Daniele Perrone", "authors": "Daniele Perrone and Paolo Favaro", "title": "A Clearer Picture of Blind Deconvolution", "comments": "Submitted to IEEE Transaction on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution is the problem of recovering a sharp image and a blur\nkernel from a noisy blurry image. Recently, there has been a significant effort\non understanding the basic mechanisms to solve blind deconvolution. While this\neffort resulted in the deployment of effective algorithms, the theoretical\nfindings generated contrasting views on why these approaches worked. On the one\nhand, one could observe experimentally that alternating energy minimization\nalgorithms converge to the desired solution. On the other hand, it has been\nshown that such alternating minimization algorithms should fail to converge and\none should instead use a so-called Variational Bayes approach. To clarify this\nconundrum, recent work showed that a good image and blur prior is instead what\nmakes a blind deconvolution algorithm work. Unfortunately, this analysis did\nnot apply to algorithms based on total variation regularization. In this\nmanuscript, we provide both analysis and experiments to get a clearer picture\nof blind deconvolution. Our analysis reveals the very reason why an algorithm\nbased on total variation works. We also introduce an implementation of this\nalgorithm and show that, in spite of its extreme simplicity, it is very robust\nand achieves a performance comparable to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 17:47:39 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Perrone", "Daniele", ""], ["Favaro", "Paolo", ""]]}, {"id": "1412.0265", "submitter": "Sadeep Jayasumana", "authors": "Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li,\n  Mehrtash Harandi", "title": "Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2015.2414422", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an approach to exploiting kernel methods with\nmanifold-valued data. In many computer vision problems, the data can be\nnaturally represented as points on a Riemannian manifold. Due to the\nnon-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision\nand machine learning algorithms yield inferior results on such data. In this\npaper, we define Gaussian radial basis function (RBF)-based positive definite\nkernels on manifolds that permit us to embed a given manifold with a\ncorresponding metric in a high dimensional reproducing kernel Hilbert space.\nThese kernels make it possible to utilize algorithms developed for linear\nspaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with\nany given metric is not always positive definite, we present a unified\nframework for analyzing the positive definiteness of the Gaussian RBF on a\ngeneric metric space. We then use the proposed framework to identify positive\ndefinite kernels on two specific manifolds commonly encountered in computer\nvision: the Riemannian manifold of symmetric positive definite matrices and the\nGrassmann manifold, i.e., the Riemannian manifold of linear subspaces of a\nEuclidean space. We show that many popular algorithms designed for Euclidean\nspaces, such as support vector machines, discriminant analysis and principal\ncomponent analysis can be generalized to Riemannian manifolds with the help of\nsuch positive definite Gaussian kernels.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 19:25:20 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 08:52:34 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Jayasumana", "Sadeep", ""], ["Hartley", "Richard", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "1412.0296", "submitter": "George Papandreou", "authors": "George Papandreou and Iasonas Kokkinos and Pierre-Andr\\'e Savalle", "title": "Untangling Local and Global Deformations in Deep Convolutional Networks\n  for Image Classification and Sliding Window Detection", "comments": "13 pages, 7 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1406.2732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling'\n(MP) layers to extract deformation-invariant features, but we argue in favor of\na more refined treatment. First, we introduce epitomic convolution as a\nbuilding block alternative to the common convolution-MP cascade of DCNNs; while\nhaving identical complexity to MP, Epitomic Convolution allows for parameter\nsharing across different filters, resulting in faster convergence and better\ngeneralization. Second, we introduce a Multiple Instance Learning approach to\nexplicitly accommodate global translation and scaling when training a DCNN\nexclusively with class labels. For this we rely on a `patchwork' data structure\nthat efficiently lays out all image scales and positions as candidates to a\nDCNN. Factoring global and local deformations allows a DCNN to `focus its\nresources' on the treatment of non-rigid deformations and yields a substantial\nclassification accuracy improvement. Third, further pursuing this idea, we\ndevelop an efficient DCNN sliding window object detector that employs explicit\nsearch over position, scale, and aspect ratio. We provide competitive image\nclassification and localization results on the ImageNet dataset and object\ndetection results on the Pascal VOC 2007 benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 22:20:17 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Papandreou", "George", ""], ["Kokkinos", "Iasonas", ""], ["Savalle", "Pierre-Andr\u00e9", ""]]}, {"id": "1412.0439", "submitter": "Chee Seng Chan", "authors": "Chern Hong Lim, Ekta Vats and Chee Seng Chan", "title": "Fuzzy human motion analysis: A review", "comments": "Accepted in Pattern Recognition, first survey paper that discusses\n  and reviews fuzzy approaches towards HMA", "journal-ref": "Pattern Recognition 48(5) 2015 1773-1796", "doi": "10.1016/j.patcog.2014.11.016", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Motion Analysis (HMA) is currently one of the most popularly active\nresearch domains as such significant research interests are motivated by a\nnumber of real world applications such as video surveillance, sports analysis,\nhealthcare monitoring and so on. However, most of these real world applications\nface high levels of uncertainties that can affect the operations of such\napplications. Hence, the fuzzy set theory has been applied and showed great\nsuccess in the recent past. In this paper, we aim at reviewing the fuzzy set\noriented approaches for HMA, individuating how the fuzzy set may improve the\nHMA, envisaging and delineating the future perspectives. To the best of our\nknowledge, there is not found a single survey in the current literature that\nhas discussed and reviewed fuzzy approaches towards the HMA. For ease of\nunderstanding, we conceptually classify the human motion into three broad\nlevels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 11:42:51 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 18:19:13 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Lim", "Chern Hong", ""], ["Vats", "Ekta", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1412.0477", "submitter": "Luca Del Pero", "authors": "Luca Del Pero and Susanna Ricco and Rahul Sukthankar and Vittorio\n  Ferrari", "title": "Recovering Spatiotemporal Correspondence between Deformable Objects by\n  Exploiting Consistent Foreground Motion in Video", "comments": "9 pages, 14 figures. This article is obsolete. Its contents are now\n  covered in arXiv:1511.09319, where we discuss a comprehensive system for\n  behavior discovery and spatial alignment of articulated object classes from\n  unstructured video (available at https://arxiv.org/abs/1511.09319)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given unstructured videos of deformable objects, we automatically recover\nspatiotemporal correspondences to map one object to another (such as animals in\nthe wild). While traditional methods based on appearance fail in such\nchallenging conditions, we exploit consistency in object motion between\ninstances. Our approach discovers pairs of short video intervals where the\nobject moves in a consistent manner and uses these candidates as seeds for\nspatial alignment. We model the spatial correspondence between the point\ntrajectories on the object in one interval to those in the other using a\ntime-varying Thin Plate Spline deformation model. On a large dataset of tiger\nand horse videos, our method automatically aligns thousands of pairs of frames\nto a high accuracy, and outperforms the popular SIFT Flow algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 13:47:52 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 22:52:04 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 22:33:33 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Del Pero", "Luca", ""], ["Ricco", "Susanna", ""], ["Sukthankar", "Rahul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1412.0494", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Teng Zhang and Amit Singer", "title": "Orthogonal Matrix Retrieval in Cryo-Electron Microscopy", "comments": "Modified introduction and summary. Accepted to the IEEE International\n  Symposium on Biomedical Imaging", "journal-ref": null, "doi": "10.1109/ISBI.2015.7164051", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single particle reconstruction (SPR) from cryo-electron microscopy\n(cryo-EM), the 3D structure of a molecule needs to be determined from its 2D\nprojection images taken at unknown viewing directions. Zvi Kam showed already\nin 1980 that the autocorrelation function of the 3D molecule over the rotation\ngroup SO(3) can be estimated from 2D projection images whose viewing directions\nare uniformly distributed over the sphere. The autocorrelation function\ndetermines the expansion coefficients of the 3D molecule in spherical harmonics\nup to an orthogonal matrix of size $(2l+1)\\times (2l+1)$ for each\n$l=0,1,2,...$. In this paper we show how techniques for solving the phase\nretrieval problem in X-ray crystallography can be modified for the cryo-EM\nsetup for retrieving the missing orthogonal matrices. Specifically, we present\ntwo new approaches that we term Orthogonal Extension and Orthogonal\nReplacement, in which the main algorithmic components are the singular value\ndecomposition and semidefinite programming. We demonstrate the utility of these\napproaches through numerical experiments on simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 14:44:28 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 17:35:01 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhang", "Teng", ""], ["Singer", "Amit", ""]]}, {"id": "1412.0614", "submitter": "Francesco Renna", "authors": "Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves,\n  Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues", "title": "Classification and Reconstruction of High-Dimensional Signals from\n  Low-Dimensional Features in the Presence of Side Information", "comments": "62 pages, 11 figures, submitted to IEEE Transactions on Information\n  Theory. The abstract of the paper is not reported entirely in the metadata\n  due to length limitations", "journal-ref": null, "doi": "10.1109/TIT.2016.2606646", "report-no": null, "categories": "cs.IT cs.CV math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers a characterization of fundamental limits on the\nclassification and reconstruction of high-dimensional signals from\nlow-dimensional features, in the presence of side information. We consider a\nscenario where a decoder has access both to linear features of the signal of\ninterest and to linear features of the side information signal; while the side\ninformation may be in a compressed form, the objective is recovery or\nclassification of the primary signal, not the side information. The signal of\ninterest and the side information are each assumed to have (distinct) latent\ndiscrete labels; conditioned on these two labels, the signal of interest and\nside information are drawn from a multivariate Gaussian distribution. With\njoint probabilities on the latent labels, the overall signal-(side information)\nrepresentation is defined by a Gaussian mixture model. We then provide sharp\nsufficient and/or necessary conditions for these quantities to approach zero\nwhen the covariance matrices of the Gaussians are nearly low-rank. These\nconditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv\nconditions, are a function of the number of linear features extracted from the\nsignal of interest, the number of linear features extracted from the side\ninformation signal, and the geometry of these signals and their interplay.\nMoreover, on assuming that the signal of interest and the side information obey\nsuch an approximately low-rank model, we derive expansions of the\nreconstruction error as a function of the deviation from an exactly low-rank\nmodel; such expansions also allow identification of operational regimes where\nthe impact of side information on signal reconstruction is most relevant. Our\nframework, which offers a principled mechanism to integrate side information in\nhigh-dimensional data problems, is also tested in the context of imaging\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:53:25 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 13:35:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Renna", "Francesco", ""], ["Wang", "Liming", ""], ["Yuan", "Xin", ""], ["Yang", "Jianbo", ""], ["Reeves", "Galen", ""], ["Calderbank", "Robert", ""], ["Carin", "Lawrence", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1412.0623", "submitter": "Sean Bell", "authors": "Sean Bell and Paul Upchurch and Noah Snavely and Kavita Bala", "title": "Material Recognition in the Wild with the Materials in Context Database", "comments": "CVPR 2015. Sean Bell and Paul Upchurch contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing materials in real-world images is a challenging task. Real-world\nmaterials have rich surface texture, geometry, lighting conditions, and\nclutter, which combine to make the problem particularly difficult. In this\npaper, we introduce a new, large-scale, open dataset of materials in the wild,\nthe Materials in Context Database (MINC), and combine this dataset with deep\nlearning to achieve material recognition and segmentation of images in the\nwild.\n  MINC is an order of magnitude larger than previous material databases, while\nbeing more diverse and well-sampled across its 23 categories. Using MINC, we\ntrain convolutional neural networks (CNNs) for two tasks: classifying materials\nfrom patches, and simultaneous material recognition and segmentation in full\nimages. For patch-based classification on MINC we found that the best\nperforming CNN architectures can achieve 85.2% mean class accuracy. We convert\nthese trained CNN classifiers into an efficient fully convolutional framework\ncombined with a fully connected conditional random field (CRF) to predict the\nmaterial at every pixel in an image, achieving 73.1% mean class accuracy. Our\nexperiments demonstrate that having a large, well-sampled dataset such as MINC\nis crucial for real-world material recognition and segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 20:11:44 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 05:29:32 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Bell", "Sean", ""], ["Upchurch", "Paul", ""], ["Snavely", "Noah", ""], ["Bala", "Kavita", ""]]}, {"id": "1412.0680", "submitter": "Ali Ayremlou", "authors": "Ali Ayremlou, Thomas Goldstein, Ashok Veeraraghavan, Richard Baraniuk", "title": "Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse approximations using highly over-complete dictionaries is a\nstate-of-the-art tool for many imaging applications including denoising,\nsuper-resolution, compressive sensing, light-field analysis, and object\nrecognition. Unfortunately, the applicability of such methods is severely\nhampered by the computational burden of sparse approximation: these algorithms\nare linear or super-linear in both the data dimensionality and size of the\ndictionary. We propose a framework for learning the hierarchical structure of\nover-complete dictionaries that enables fast computation of sparse\nrepresentations. Our method builds on tree-based strategies for nearest\nneighbor matching, and presents domain-specific enhancements that are highly\nefficient for the analysis of image patches. Contrary to most popular methods\nfor building spatial data structures, out methods rely on shallow, balanced\ntrees with relatively few layers. We show an extensive array of experiments on\nseveral applications such as image denoising/superresolution, compressive\nvideo/light-field sensing where we practically achieve 100-1000x speedup (with\na less than 1dB loss in accuracy).\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 21:08:37 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Ayremlou", "Ali", ""], ["Goldstein", "Thomas", ""], ["Veeraraghavan", "Ashok", ""], ["Baraniuk", "Richard", ""]]}, {"id": "1412.0767", "submitter": "Du Tran", "authors": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar\n  Paluri", "title": "Learning Spatiotemporal Features with 3D Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, yet effective approach for spatiotemporal feature\nlearning using deep 3-dimensional convolutional networks (3D ConvNets) trained\non a large scale supervised video dataset. Our findings are three-fold: 1) 3D\nConvNets are more suitable for spatiotemporal feature learning compared to 2D\nConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in\nall layers is among the best performing architectures for 3D ConvNets; and 3)\nOur learned features, namely C3D (Convolutional 3D), with a simple linear\nclassifier outperform state-of-the-art methods on 4 different benchmarks and\nare comparable with current best methods on the other 2 benchmarks. In\naddition, the features are compact: achieving 52.8% accuracy on UCF101 dataset\nwith only 10 dimensions and also very efficient to compute due to the fast\ninference of ConvNets. Finally, they are conceptually very simple and easy to\ntrain and use.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 03:05:54 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 01:59:04 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 03:24:33 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2015 01:29:12 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Tran", "Du", ""], ["Bourdev", "Lubomir", ""], ["Fergus", "Rob", ""], ["Torresani", "Lorenzo", ""], ["Paluri", "Manohar", ""]]}, {"id": "1412.0774", "submitter": "Gregory Shakhnarovich", "authors": "Mohammadreza Mostajabi, Payman Yadollahpour and Gregory Shakhnarovich", "title": "Feedforward semantic segmentation with zoom-out features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a purely feed-forward architecture for semantic segmentation. We\nmap small image elements (superpixels) to rich feature representations\nextracted from a sequence of nested regions of increasing extent. These regions\nare obtained by \"zooming out\" from the superpixel all the way to scene-level\nresolution. This approach exploits statistical structure in the image and in\nthe label space without setting up explicit structured prediction mechanisms,\nand thus avoids complex and expensive inference. Instead superpixels are\nclassified by a feedforward multilayer network. Our architecture achieves new\nstate of the art performance in semantic segmentation, obtaining 64.4% average\naccuracy on the PASCAL VOC 2012 test set.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 03:31:51 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Mostajabi", "Mohammadreza", ""], ["Yadollahpour", "Payman", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1412.0781", "submitter": "Zhizhen Zhao", "authors": "Zhizhen Zhao, Yoel Shkolnisky, and Amit Singer", "title": "Fast Steerable Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy nowadays often requires the analysis of hundreds of\nthousands of 2D images as large as a few hundred pixels in each direction. Here\nwe introduce an algorithm that efficiently and accurately performs principal\ncomponent analysis (PCA) for a large set of two-dimensional images, and, for\neach image, the set of its uniform rotations in the plane and their\nreflections. For a dataset consisting of $n$ images of size $L \\times L$\npixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, while\nexisting algorithms take $O(nL^4)$. The new algorithm computes the expansion\ncoefficients of the images in a Fourier-Bessel basis efficiently using the\nnon-uniform fast Fourier transform. We compare the accuracy and efficiency of\nthe new algorithm with traditional PCA and existing algorithms for steerable\nPCA.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 04:24:03 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 18:21:40 GMT"}, {"version": "v3", "created": "Sat, 16 May 2015 02:06:04 GMT"}, {"version": "v4", "created": "Fri, 23 Oct 2015 02:14:53 GMT"}, {"version": "v5", "created": "Tue, 15 Dec 2015 19:26:37 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Zhao", "Zhizhen", ""], ["Shkolnisky", "Yoel", ""], ["Singer", "Amit", ""]]}, {"id": "1412.0801", "submitter": "Poorna Dasgupta", "authors": "Poorna Banerjee Dasgupta", "title": "Analytical Comparison of Noise Reduction Filters for Image Restoration\n  Using SNR Estimation", "comments": "4 pages,Published with International Journal of Computer Trends and\n  Technology (IJCTT), volume 17 number 3, Nov 2014", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V17(3):121-124, Nov 2014. ISSN:2231-2803. www.ijcttjournal.org", "doi": "10.14445/22312803/IJCTT-V17P123", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise removal from images is a part of image restoration in which we try to\nreconstruct or recover an image that has been degraded by using apriori\nknowledge of the degradation phenomenon. Noises present in images can be of\nvarious types with their characteristic Probability Distribution Functions\n(PDF). Noise removal techniques depend on the kind of noise present in the\nimage rather than on the image itself. This paper explores the effects of\napplying noise reduction filters having similar properties on noisy images with\nemphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing the\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 07:35:27 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Dasgupta", "Poorna Banerjee", ""]]}, {"id": "1412.0826", "submitter": "Chunhua Shen", "authors": "Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin\n  Tang, Heng Tao Shen", "title": "Hashing on Nonlinear Manifolds", "comments": "13 pages. arXiv admin note: text overlap with arXiv:1303.7043", "journal-ref": null, "doi": "10.1109/TIP.2015.2405340", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based hashing methods have attracted considerable attention due to\ntheir ability to greatly increase the scale at which existing algorithms may\noperate. Most of these methods are designed to generate binary codes preserving\nthe Euclidean similarity in the original space. Manifold learning techniques,\nin contrast, are better able to model the intrinsic structure embedded in the\noriginal high-dimensional data. The complexities of these models, and the\nproblems with out-of-sample data, have previously rendered them unsuitable for\napplication to large-scale embedding, however. In this work, how to learn\ncompact binary embeddings on their intrinsic manifolds is considered. In order\nto address the above-mentioned difficulties, an efficient, inductive solution\nto the out-of-sample data problem, and a process by which non-parametric\nmanifold learning may be used as the basis of a hashing method is proposed. The\nproposed approach thus allows the development of a range of new hashing\ntechniques exploiting the flexibility of the wide variety of manifold learning\napproaches available. It is particularly shown that hashing on the basis of\nt-SNE outperforms state-of-the-art hashing methods on large-scale benchmark\ndatasets, and is very effective for image classification with very short code\nlengths. The proposed hashing framework is shown to be easily improved, for\nexample, by minimizing the quantization error with learned orthogonal\nrotations. In addition, a supervised inductive manifold hashing framework is\ndeveloped by incorporating the label information, which is shown to greatly\nadvance the semantic retrieval performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 09:36:12 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Tang", "Zhenmin", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1412.0985", "submitter": "Joakim And\\'en", "authors": "Joakim And\\'en and Eugene Katsevich and Amit Singer", "title": "Covariance estimation using conjugate gradient for 3D classification in\n  Cryo-EM", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI.2015.7163849", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying structural variability in noisy projections of biological\nmacromolecules is a central problem in Cryo-EM. In this work, we build on a\nprevious method for estimating the covariance matrix of the three-dimensional\nstructure present in the molecules being imaged. Our proposed method allows for\nincorporation of contrast transfer function and non-uniform distribution of\nviewing angles, making it more suitable for real-world data. We evaluate its\nperformance on a synthetic dataset and an experimental dataset obtained by\nimaging a 70S ribosome complex.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 17:18:13 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 14:53:01 GMT"}, {"version": "v3", "created": "Wed, 11 Feb 2015 17:51:12 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["And\u00e9n", "Joakim", ""], ["Katsevich", "Eugene", ""], ["Singer", "Amit", ""]]}, {"id": "1412.1123", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani", "title": "DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour\n  Detection", "comments": "Accepted to CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contour detection has been a fundamental component in many image segmentation\nand object detection systems. Most previous work utilizes low-level features\nsuch as texture or saliency to detect contours and then use them as cues for a\nhigher-level task such as object detection. However, we claim that recognizing\nobjects and predicting contours are two mutually related tasks. Contrary to\ntraditional approaches, we show that we can invert the commonly established\npipeline: instead of detecting contours with low-level cues for a higher-level\nrecognition task, we exploit object-related features as high-level cues for\ncontour detection.\n  We achieve this goal by means of a multi-scale deep network that consists of\nfive convolutional layers and a bifurcated fully-connected sub-network. The\nsection from the input layer to the fifth convolutional layer is fixed and\ndirectly lifted from a pre-trained network optimized over a large-scale object\nclassification task. This section of the network is applied to four different\nscales of the image input. These four parallel and identical streams are then\nattached to a bifurcated sub-network consisting of two independently-trained\nbranches. One branch learns to predict the contour likelihood (with a\nclassification objective) whereas the other branch is trained to learn the\nfraction of human labelers agreeing about the contour presence at a given point\n(with a regression criterion).\n  We show that without any feature engineering our multi-scale deep learning\napproach achieves state-of-the-art results in contour detection.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 22:35:48 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 01:45:49 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 13:58:39 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Bertasius", "Gedas", ""], ["Shi", "Jianbo", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1412.1135", "submitter": "Judy Hoffman", "authors": "Judy Hoffman, Deepak Pathak, Trevor Darrell, Kate Saenko", "title": "Detector Discovery in the Wild: Joint Multiple Instance and\n  Representation Learning", "comments": null, "journal-ref": "Computer Vision and Pattern Recognition (CVPR) 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for detector learning which exploit joint training over\nboth weak and strong labels and which transfer learned perceptual\nrepresentations from strongly-labeled auxiliary tasks. Previous methods for\nweak-label learning often learn detector models independently using latent\nvariable optimization, but fail to share deep representation knowledge across\nclasses and usually require strong initialization. Other previous methods\ntransfer deep representations from domains with strong labels to those with\nonly weak labels, but do not optimize over individual latent boxes, and thus\nmay miss specific salient structures for a particular category. We propose a\nmodel that subsumes these previous approaches, and simultaneously trains a\nrepresentation and detectors for categories with either weak or strong labels\npresent. We provide a novel formulation of a joint multiple instance learning\nmethod that includes examples from classification-style data when available,\nand also performs domain transfer learning to improve the underlying detector\nrepresentation. Our model outperforms known methods on ImageNet-200 detection\nwith weak labels.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 23:32:34 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Hoffman", "Judy", ""], ["Pathak", "Deepak", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1412.1194", "submitter": "Feng Shi", "authors": "Feng Shi, Robert Laganiere, Emil Petriu", "title": "Gradient Boundary Histograms for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a high efficient local spatiotemporal descriptor,\ncalled gradient boundary histograms (GBH). The proposed GBH descriptor is built\non simple spatio-temporal gradients, which are fast to compute. We demonstrate\nthat it can better represent local structure and motion than other\ngradient-based descriptors, and significantly outperforms them on large\nrealistic datasets. A comprehensive evaluation shows that the recognition\naccuracy is preserved while the spatial resolution is greatly reduced, which\nyields both high efficiency and low memory usage.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 05:23:03 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Shi", "Feng", ""], ["Laganiere", "Robert", ""], ["Petriu", "Emil", ""]]}, {"id": "1412.1216", "submitter": "Alexandra Heidsieck", "authors": "Alexandra Heidsieck", "title": "Simple Two-Dimensional Object Tracking based on a Graph Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual observation and tracking of cells and other micrometer-sized\nobjects has many different biomedical applications. The automation of those\ntasks based on computer methods helps in the evaluation of such measurements.\nIn this work, we present a general purpose algorithm that excels at evaluating\ndeterministic behavior of micrometer-sized objects. Our concrete application is\nthe tracking of fast moving objects over large distances along deterministic\ntrajectories in a microscopic video. Thereby, we are able to determine\ncharacteristic properties of the objects. For this purpose, we use a set of\nbasic algorithms, including blob recognition, feature-based shape recognition\nand a graph algorithm, and combined them in a novel way. An evaluation of the\nalgorithms performance shows a high accuracy in the recognition of objects as\nwell as of complete trajectories. Moreover, a direct comparison to a similar\nalgorithm shows superior recognition rates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 07:16:13 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Heidsieck", "Alexandra", ""]]}, {"id": "1412.1219", "submitter": "Jean-Emmanuel Deschaud", "authors": "Jean-Emmanuel Deschaud (CAOR), Xavier Brun (CAOR), Fran\\c{c}ois\n  Goulette (CAOR)", "title": "Colorisation et texturation temps r\\'eel d'environnements urbains par\n  syst\\`eme mobile avec scanner laser et cam\\'era fish-eye", "comments": "in French", "journal-ref": "Revue Francaise de Photogrammetrie et de Teledetection, Revue\n  Francaise de Photogrammetrie et de Teledetection, 2010, pp.29-37", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a real time mobile mapping system mounted on a vehicle. The\nterrestrial acquisition system is based on a geolocation system and two\nsensors, namely, a laser scanner and a camera with a fish-eye lens. We produce\n3D colored points cloud and textured models of the environment. Once the system\nhas been calibrated, the data acquisition and processing are done \"on the way\".\nThis article mainly presents our methods of colorization of point cloud,\ntriangulation and texture mapping.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 07:37:04 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Deschaud", "Jean-Emmanuel", "", "CAOR"], ["Brun", "Xavier", "", "CAOR"], ["Goulette", "Fran\u00e7ois", "", "CAOR"]]}, {"id": "1412.1265", "submitter": "Yi Sun", "authors": "Yi Sun, Xiaogang Wang, Xiaoou Tang", "title": "Deeply learned face representations are sparse, selective, and robust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper designs a high-performance deep convolutional network (DeepID2+)\nfor face recognition. It is learned with the identification-verification\nsupervisory signal. By increasing the dimension of hidden representations and\nadding supervision to early convolutional layers, DeepID2+ achieves new\nstate-of-the-art on LFW and YouTube Faces benchmarks. Through empirical\nstudies, we have discovered three properties of its deep neural activations\ncritical for the high performance: sparsity, selectiveness and robustness. (1)\nIt is observed that neural activations are moderately sparse. Moderate sparsity\nmaximizes the discriminative power of the deep net as well as the distance\nbetween images. It is surprising that DeepID2+ still can achieve high\nrecognition accuracy even after the neural responses are binarized. (2) Its\nneurons in higher layers are highly selective to identities and\nidentity-related attributes. We can identify different subsets of neurons which\nare either constantly excited or inhibited when different identities or\nattributes are present. Although DeepID2+ is not taught to distinguish\nattributes during training, it has implicitly learned such high-level concepts.\n(3) It is much more robust to occlusions, although occlusion patterns are not\nincluded in the training set.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 10:37:13 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Sun", "Yi", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1412.1283", "submitter": "Kaiming He", "authors": "Jifeng Dai, Kaiming He, Jian Sun", "title": "Convolutional Feature Masking for Joint Object and Stuff Segmentation", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7299025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of semantic segmentation has witnessed considerable progress due to\nthe powerful features learned by convolutional neural networks (CNNs). The\ncurrent leading approaches for semantic segmentation exploit shape information\nby extracting CNN features from masked image regions. This strategy introduces\nartificial boundaries on the images and may impact the quality of the extracted\nfeatures. Besides, the operations on the raw image domain require to compute\nthousands of networks on a single image, which is time-consuming. In this\npaper, we propose to exploit shape information via masking convolutional\nfeatures. The proposal segments (e.g., super-pixels) are treated as masks on\nthe convolutional feature maps. The CNN features of segments are directly\nmasked out from these maps and used to train classifiers for recognition. We\nfurther propose a joint method to handle objects and \"stuff\" (e.g., grass, sky,\nwater) in the same framework. State-of-the-art results are demonstrated on\nbenchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling\ncomputational speed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 11:45:34 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 06:07:12 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2015 03:20:57 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 04:12:26 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dai", "Jifeng", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1412.1441", "submitter": "Christian Szegedy", "authors": "Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov,\n  Sergey Ioffe", "title": "Scalable, High-Quality Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current high-quality object detection approaches use the scheme of\nsalience-based object proposal methods followed by post-classification using\ndeep convolutional features. This spurred recent research in improving object\nproposal methods. However, domain agnostic proposal generation has the\nprincipal drawback that the proposals come unranked or with very weak ranking,\nmaking it hard to trade-off quality for running time. This raises the more\nfundamental question of whether high-quality proposal generation requires\ncareful engineering or can be derived just from data alone. We demonstrate that\nlearning-based proposal methods can effectively match the performance of\nhand-engineered methods while allowing for very efficient runtime-quality\ntrade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox)\napproach, we substantially advance the state-of-the-art on the ILSVRC 2014\ndetection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAP\nfor an ensemble of two models. MSC-Multibox significantly improves the proposal\nquality over its predecessor MultiBox~method: AP increases from $0.42$ to\n$0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improved\nbounding-box recall compared to Multiscale Combinatorial Grouping with less\nproposals on the Microsoft-COCO data set.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:03:55 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 19:22:26 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 03:41:42 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Szegedy", "Christian", ""], ["Reed", "Scott", ""], ["Erhan", "Dumitru", ""], ["Anguelov", "Dragomir", ""], ["Ioffe", "Sergey", ""]]}, {"id": "1412.1442", "submitter": "Maxwell Collins", "authors": "Maxwell D. Collins and Pushmeet Kohli", "title": "Memory Bounded Deep Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the use of sparsity-inducing regularizers during\ntraining of Convolution Neural Networks (CNNs). These regularizers encourage\nthat fewer connections in the convolution and fully connected layers take\nnon-zero values and in effect result in sparse connectivity between hidden\nunits in the deep network. This in turn reduces the memory and runtime cost\ninvolved in deploying the learned CNNs. We show that training with such\nregularization can still be performed using stochastic gradient descent\nimplying that it can be used easily in existing codebases. Experimental\nevaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows that\nour regularizers can result in dramatic reductions in memory requirements. For\ninstance, when applied on AlexNet, our method can reduce the memory consumption\nby a factor of four with minimal loss in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:08:38 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Collins", "Maxwell D.", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1412.1455", "submitter": "Gil Ben-Artzi", "authors": "Gil Ben-Artzi, Michael Werman, Shmuel Peleg", "title": "Event Retrieval Using Motion Barcodes", "comments": null, "journal-ref": "Proc. ICIP'15, Quebec City, Sept. 2015, pp 2621-2625", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and effective method for retrieval of videos showing a\nspecific event, even when the videos of that event were captured from\nsignificantly different viewpoints. Appearance-based methods fail in such\ncases, as appearances change with large changes of viewpoints.\n  Our method is based on a pixel-based feature, \"motion barcode\", which records\nthe existence/non-existence of motion as a function of time. While appearance,\nmotion magnitude, and motion direction can vary greatly between disparate\nviewpoints, the existence of motion is viewpoint invariant. Based on the motion\nbarcode, a similarity measure is developed for videos of the same event taken\nfrom very different viewpoints. This measure is robust to occlusions common\nunder different viewpoints, and can be computed efficiently.\n  Event retrieval is demonstrated using challenging videos from stationary and\nhand held cameras.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:53:08 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 17:29:36 GMT"}, {"version": "v3", "created": "Tue, 12 May 2015 19:10:51 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Ben-Artzi", "Gil", ""], ["Werman", "Michael", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1412.1506", "submitter": "Khamsa Djaroudib", "authors": "Khamsa Djaroudib, Abdelmalik Taleb Ahmed and Abdelmadjid Zidani", "title": "Textural Approach for Mass Abnormality Segmentation in Mammographic\n  Images", "comments": "07 pages, 11 figures, 1 tableau, 07 equations, 34 references. appears\n  in IJCSI International Journal of Computer Science Issues november 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass abnormality segmentation is a vital step for the medical diagnostic\nprocess and is attracting more and more the interest of many research groups.\nCurrently, most of the works achieved in this area have used the Gray Level\nCo-occurrence Matrix (GLCM) as texture features with a region-based approach.\nThese features come in previous phase for segmentation stage or are using as\ninputs to classification stage. The work discussed in this paper attempts to\nexperiment the GLCM method under a contour-based approach. Besides, we\nexperiment the proposed approach on various tissues densities to bring more\nsignificant results. At this end, we explored some challenging breast images\nfrom BIRADS medical Data Base. Our first experimentations showed promising\nresults with regard to the edges mass segmentation methods. This paper\ndiscusses first the main works achieved in this area. Sections 2 and 3 present\nmaterials and our methodology. The main results are showed and evaluated before\nconcluding our paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 22:08:15 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Djaroudib", "Khamsa", ""], ["Ahmed", "Abdelmalik Taleb", ""], ["Zidani", "Abdelmadjid", ""]]}, {"id": "1412.1526", "submitter": "Xianjie Chen", "authors": "Xianjie Chen, Alan Yuille", "title": "Parsing Occluded People by Flexible Compositions", "comments": "CVPR 15 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to parsing humans when there is significant\nocclusion. We model humans using a graphical model which has a tree structure\nbuilding on recent work [32, 6] and exploit the connectivity prior that, even\nin presence of occlusion, the visible nodes form a connected subtree of the\ngraphical model. We call each connected subtree a flexible composition of\nobject parts. This involves a novel method for learning occlusion cues. During\ninference we need to search over a mixture of different flexible models. By\nexploiting part sharing, we show that this inference can be done extremely\nefficiently requiring only twice as many computations as searching for the\nentire object (i.e., not modeling occlusion). We evaluate our model on the\nstandard benchmarked \"We Are Family\" Stickmen dataset and obtain significant\nperformance improvements over the best alternative algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 00:45:14 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 07:57:19 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Chen", "Xianjie", ""], ["Yuille", "Alan", ""]]}, {"id": "1412.1574", "submitter": "Liming Zhao", "authors": "Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang", "title": "Metric Learning Driven Multi-Task Structured Output Optimization for\n  Robust Keypoint Tracking", "comments": "Accepted by AAAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem in computer vision and graphics,\nkeypoint-based object tracking is typically formulated in a spatio-temporal\nstatistical learning framework. However, most existing keypoint trackers are\nincapable of effectively modeling and balancing the following three aspects in\na simultaneous manner: temporal model coherence across frames, spatial model\nconsistency within frames, and discriminative feature construction. To address\nthis issue, we propose a robust keypoint tracker based on spatio-temporal\nmulti-task structured output optimization driven by discriminative metric\nlearning. Consequently, temporal model coherence is characterized by multi-task\nstructured keypoint model learning over several adjacent frames, while spatial\nmodel consistency is modeled by solving a geometric verification based\nstructured learning problem. Discriminative feature construction is enabled by\nmetric learning to ensure the intra-class compactness and inter-class\nseparability. Finally, the above three modules are simultaneously optimized in\na joint learning scheme. Experimental results have demonstrated the\neffectiveness of our tracker.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:42:21 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Zhao", "Liming", ""], ["Li", "Xi", ""], ["Xiao", "Jun", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1412.1628", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon", "title": "Fisher Kernel for Deep Neural Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to image representation based on low-level local descriptors, deep\nneural activations of Convolutional Neural Networks (CNNs) are richer in\nmid-level representation, but poorer in geometric invariance properties. In\nthis paper, we present a straightforward framework for better image\nrepresentation by combining the two approaches. To take advantages of both\nrepresentations, we propose an efficient method to extract a fair amount of\nmulti-scale dense local activations from a pre-trained CNN. We then aggregate\nthe activations by Fisher kernel framework, which has been modified with a\nsimple scale-wise normalization essential to make it suitable for CNN\nactivations. Replacing the direct use of a single activation vector with our\nrepresentation demonstrates significant performance improvements: +17.76 (Acc.)\non MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that\nour proposal can be used as a primary image representation for better\nperformances in visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 11:30:57 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 07:16:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1412.1710", "submitter": "Kaiming He", "authors": "Kaiming He, Jian Sun", "title": "Convolutional Neural Networks at Constrained Time Cost", "comments": "8-page technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though recent advanced convolutional neural networks (CNNs) have been\nimproving the image recognition accuracy, the models are getting more complex\nand time-consuming. For real-world applications in industrial and commercial\nscenarios, engineers and developers are often faced with the requirement of\nconstrained time budget. In this paper, we investigate the accuracy of CNNs\nunder constrained time cost. Under this constraint, the designs of the network\narchitectures should exhibit as trade-offs among the factors like depth,\nnumbers of filters, filter sizes, etc. With a series of controlled comparisons,\nwe progressively modify a baseline model while preserving its time complexity.\nThis is also helpful for understanding the importance of the factors in network\ndesigns. We present an architecture that achieves very competitive accuracy in\nthe ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than\n\"AlexNet\" (16.0% top-5 error, 10-view test).\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:00:47 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1412.1732", "submitter": "Hao Zhang", "authors": "Hao Zhang, Jing Wang, Jianhua Ma, Hongbing Lu, and Zhengrong Liang", "title": "Statistical models and regularization strategies in statistical image\n  reconstruction of low-dose X-ray CT: a survey", "comments": "This paper has been withdrawn because the authors have differnt\n  opinions on some contents of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical image reconstruction (SIR) methods have shown potential to\nsubstantially improve the image quality of low-dose X-ray computed tomography\n(CT) as compared to the conventional filtered back-projection (FBP) method for\nvarious clinical tasks. According to the maximum a posterior (MAP) estimation,\nthe SIR methods can be typically formulated by an objective function consisting\nof two terms: (1) data-fidelity (or equivalently, data-fitting or\ndata-mismatch) term modeling the statistics of projection measurements, and (2)\nregularization (or equivalently, prior or penalty) term reflecting prior\nknowledge or expectation on the characteristics of the image to be\nreconstructed. Existing SIR methods for low-dose CT can be divided into two\ngroups: (1) those that use calibrated transmitted photon counts (before\nlog-transform) with penalized maximum likelihood (pML) criterion, and (2) those\nthat use calibrated line-integrals (after log-transform) with penalized\nweighted least-squares (PWLS) criterion. Accurate statistical modeling of the\nprojection measurements is a prerequisite for SIR, while the regularization\nterm in the objective function also plays a critical role for successful image\nreconstruction. This paper reviews several statistical models on CT projection\nmeasurements and various regularization strategies incorporating prior\nknowledge or expected properties of the image to be reconstructed, which\ntogether formulate the objective function of the SIR methods for low-dose X-ray\nCT.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 17:07:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 16:35:10 GMT"}, {"version": "v3", "created": "Thu, 14 May 2015 17:51:10 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Zhang", "Hao", ""], ["Wang", "Jing", ""], ["Ma", "Jianhua", ""], ["Lu", "Hongbing", ""], ["Liang", "Zhengrong", ""]]}, {"id": "1412.1740", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q.\n  Weinberger", "title": "Image Data Compression for Covariance and Histogram Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance and histogram image descriptors provide an effective way to\ncapture information about images. Both excel when used in combination with\nspecial purpose distance metrics. For covariance descriptors these metrics\nmeasure the distance along the non-Euclidean Riemannian manifold of symmetric\npositive definite matrices. For histogram descriptors the Earth Mover's\ndistance measures the optimal transport between two histograms. Although more\nprecise, these distance metrics are very expensive to compute, making them\nimpractical in many applications, even for data sets of only a few thousand\nexamples. In this paper we present two methods to compress the size of\ncovariance and histogram datasets with only marginal increases in test error\nfor k-nearest neighbor classification. Specifically, we show that we can reduce\ndata sets to 16% and in some cases as little as 2% of their original size,\nwhile approximately matching the test error of kNN classification on the full\ntraining set. In fact, because the compressed set is learned in a supervised\nfashion, it sometimes even outperforms the full data set, while requiring only\na fraction of the space and drastically reducing test-time computation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 17:22:22 GMT"}, {"version": "v2", "created": "Sat, 23 May 2015 17:07:59 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kusner", "Matt J.", ""], ["Kolkin", "Nicholas I.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1412.1842", "submitter": "Max Jaderberg", "authors": "Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman", "title": "Reading Text in the Wild with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present an end-to-end system for text spotting -- localising\nand recognising text in natural scene images -- and text based image retrieval.\nThis system is based on a region proposal mechanism for detection and deep\nconvolutional neural networks for recognition. Our pipeline uses a novel\ncombination of complementary proposal generation techniques to ensure high\nrecall, and a fast subsequent filtering stage for improving precision. For the\nrecognition and ranking of proposals, we train very large convolutional neural\nnetworks to perform word recognition on the whole proposal region at the same\ntime, departing from the character classifier based systems of the past. These\nnetworks are trained solely on data produced by a synthetic text generation\nengine, requiring no human labelled data.\n  Analysing the stages of our pipeline, we show state-of-the-art performance\nthroughout. We perform rigorous experiments across a number of standard\nend-to-end text spotting benchmarks and text-based image retrieval datasets,\nshowing a large improvement over all previous methods. Finally, we demonstrate\na real-world application of our text spotting system to allow thousands of\nhours of news footage to be instantly searchable via a text query.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 21:14:59 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Jaderberg", "Max", ""], ["Simonyan", "Karen", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1412.1871", "submitter": "Estanislao Herscovich", "authors": "Estanislao Herscovich", "title": "A higher homotopic extension of persistent (co)homology", "comments": "Any comment(s) would be highly appreciated", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV math.KT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective in this article is to show a possibly interesting structure of\nhomotopic nature appearing in persistent (co)homology. Assuming that the\nfiltration of the (say) simplicial set embedded in a finite dimensional vector\nspace induces a multiplicative filtration (which would not be a so harsh\nhypothesis in our setting) on the dg algebra given by the complex of simplicial\ncochains, we may use a result by T. Kadeishvili to get a unique (up to\nnoncanonical equivalence) A_infinity-algebra structure on the complete\npersistent cohomology of the filtered simplicial (or topological) set. We then\nprovide a construction of a (pseudo)metric on the set of all (generalized)\nbarcodes (that is, of all cohomological degrees) enriched with the\nA_infinity-algebra structure stated before, refining the usual bottleneck\nmetric, and which is also independent of the particular A_infinity-algebra\nstructure chosen (among those equivalent to each other). We think that this\ndistance might deserve some attention for topological data analysis, for it in\nparticular can recognize different linking or foldings patterns, as in the\nBorromean rings. As an aside, we give a simple proof of a result relating the\nbarcode structure between persistent homology and cohomology. This result was\nobserved in a recent article by V. de Silva, D. Morozov and M.\nVejdemo-Johansson under some restricted assumptions, which we do not suppose.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 01:09:19 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Herscovich", "Estanislao", ""]]}, {"id": "1412.1897", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Jason Yosinski, Jeff Clune", "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for\n  Unrecognizable Images", "comments": "To appear at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 05:29:43 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 22:27:00 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 16:41:04 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 23:12:56 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1412.1908", "submitter": "Rui Zhao", "authors": "Rui Zhao, Wanli Ouyang, Xiaogang Wang", "title": "Person Re-identification by Saliency Learning", "comments": "This manuscript has 14 pages with 25 figures, and a preliminary\n  version was published in ICCV 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Human eyes can recognize person identities based on small salient regions,\ni.e. human saliency is distinctive and reliable in pedestrian matching across\ndisjoint camera views. However, such valuable information is often hidden when\ncomputing similarities of pedestrian images with existing approaches. Inspired\nby our user study result of human perception on human saliency, we propose a\nnovel perspective for person re-identification based on learning human saliency\nand matching saliency distribution. The proposed saliency learning and matching\nframework consists of four steps: (1) To handle misalignment caused by drastic\nviewpoint change and pose variations, we apply adjacency constrained patch\nmatching to build dense correspondence between image pairs. (2) We propose two\nalternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a\nsaliency score for each image patch, through which distinctive features stand\nout without using identity labels in the training procedure. (3) saliency\nmatching is proposed based on patch matching. Matching patches with\ninconsistent saliency brings penalty, and images of the same identity are\nrecognized by minimizing the saliency matching cost. (4) Furthermore, saliency\nmatching is tightly integrated with patch matching in a unified structural\nRankSVM learning framework. The effectiveness of our approach is validated on\nthe VIPeR dataset and the CUHK01 dataset. Our approach outperforms the\nstate-of-the-art person re-identification methods on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 07:33:48 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Zhao", "Rui", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1412.1945", "submitter": "Aditya AV Sastry Mr.", "authors": "Aditya A.V. Sastry", "title": "Background Modelling using Octree Color Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  By assuming that the most frequently occuring color in a video or a region of\na video I propose a new algorithm for detecting foreground objects in a video.\nThe process of detecting the foreground objects is complicated because of the\nfact that there may be swaying trees, objects of the background being moved\naround or lighting changes in the video. To deal with such complexities many\nhave come up with solutions which heavily rely on expensive floating point\noperations. In this paper I used a data structure called Octree which is\nimplemented only using binary operations. Traditionally octrees were used for\ncolor quantization but here in this paper I used it as a data structure to\nstore the most frequently occuring colors in a video as well. For each of the\nstarting few video frames, I constructed a Octree using all the colors of that\nframe. Next I pruned all the trees by removing nodes below a certain height and\ngave the leaf nodes a color which is dependant on the topological path from\nthat node to its parent. Hence any two leaf nodes in two different octrees with\nthe same topological path from themselves to the root will represent the same\ncolor. Next I merged all these individual trees into a single tree retaining\nonly those nodes whose topological path to itself from the root is most common\namong all the trees. The colors represented by the leaf nodes in the resultant\ntree will be the most frequently occuring colors in the starting video frames\nof the video. Hence any color of an incomming frame that is not close to any of\nthe colors represented by the leaf node of the merged tree can be regarded as\nbelonging to a foreground object.\n  As an Octree is constructed using only binary operations, it is very fast\ncompared to other leading algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 10:44:56 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 01:50:57 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Sastry", "Aditya A. V.", ""]]}, {"id": "1412.1957", "submitter": "Swarna  Kamlam Ravindran", "authors": "Swarna Kamlam Ravindran and Anurag Mittal", "title": "CoMIC: Good features for detection and matching at object boundaries", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature or interest points typically use information aggregation in 2D\npatches which does not remain stable at object boundaries when there is object\nmotion against a significantly varying background. Level or iso-intensity\ncurves are much more stable under such conditions, especially the longer ones.\nIn this paper, we identify stable portions on long iso-curves and detect\ncorners on them. Further, the iso-curve associated with a corner is used to\ndiscard portions from the background and improve matching. Such CoMIC (Corners\non Maximally-stable Iso-intensity Curves) points yield superior results at the\nobject boundary regions compared to state-of-the-art detectors while performing\ncomparably at the interior regions as well. This is illustrated in exhaustive\nmatching experiments for both boundary and non-boundary regions in applications\nsuch as stereo and point tracking for structure from motion in video sequences.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 11:22:54 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Ravindran", "Swarna Kamlam", ""], ["Mittal", "Anurag", ""]]}, {"id": "1412.2032", "submitter": "Evangelos Matsinos", "authors": "M.J. Malinowski, E. Matsinos, S. Roth", "title": "On using the Microsoft Kinect$^{\\rm TM}$ sensors in the analysis of\n  human motion", "comments": "29 pages, 1 table, 4 figures. Versions 1 and 2 of the study have been\n  retracted due to the use of a non-representative database; save for one\n  sentence at the end of the abstract of the current version, versions 3 and 4\n  are identical", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper aims at providing the theoretical background required for\ninvestigating the use of the Microsoft Kinect$^{\\rm TM}$ (`Kinect', for short)\nsensors (original and upgraded) in the analysis of human motion. Our\nmethodology is developed in such a way that its application be easily adaptable\nto comparative studies of other systems used in capturing human-motion data.\nOur future plans include the application of this methodology to two situations:\nfirst, in a comparative study of the performance of the two Kinect sensors;\nsecond, in pursuing their validation on the basis of comparisons with a\nmarker-based system (MBS). One important feature in our approach is the\ntransformation of the MBS output into Kinect-output format, thus enabling the\nanalysis of the measurements, obtained from different systems, with the same\nsoftware application, i.e., the one we use in the analysis of Kinect-captured\ndata; one example of such a transformation, for one popular marker-placement\nscheme (`Plug-in Gait'), is detailed. We propose that the similarity of the\noutput, obtained from the different systems, be assessed on the basis of the\ncomparison of a number of waveforms, representing the variation within the gait\ncycle of quantities which are commonly used in the modelling of the human\nmotion. The data acquisition may involve commercially-available treadmills and\na number of velocity settings: for instance, walking-motion data may be\nacquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommend\nthat particular attention be called to systematic effects associated with the\nsubject's knee and lower leg, as well as to the ability of the Kinect sensors\nin reliably capturing the details in the asymmetry of the motion for the left\nand right parts of the human body. The previous versions of the study have been\nwithdrawn due to the use of a non-representative database.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 11:15:08 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 11:40:05 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2015 10:00:12 GMT"}, {"version": "v4", "created": "Wed, 27 May 2015 05:40:37 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Malinowski", "M. J.", ""], ["Matsinos", "E.", ""], ["Roth", "S.", ""]]}, {"id": "1412.2066", "submitter": "Charless Fowlkes", "authors": "Shaofei Wang and Charless C. Fowlkes", "title": "Learning Multi-target Tracking with Quadratic Object Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a model for multi-target tracking based on associating\ncollections of candidate detections across frames of a video. In order to model\npairwise interactions between different tracks, such as suppression of\noverlapping tracks and contextual cues about co-occurence of different objects,\nwe augment a standard min-cost flow objective with quadratic terms between\ndetection variables. We learn the parameters of this model using structured\nprediction and a loss function which approximates the multi-target tracking\naccuracy. We evaluate two different approaches to finding an optimal set of\ntracks under model objective based on an LP relaxation and a novel greedy\nextension to dynamic programming that handles pairwise interactions. We find\nthe greedy algorithm achieves equivalent performance to the LP relaxation while\nbeing 2-7x faster than a commercial solver. The resulting model with learned\nparameters outperforms existing methods across several categories on the KITTI\ntracking benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 17:04:35 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 05:25:44 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Wang", "Shaofei", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1412.2067", "submitter": "Nir Sharon", "authors": "Victor May, Yosi Keller, Nir Sharon, Yoel Shkolnisky", "title": "An algorithm for improving Non-Local Means operators via low-rank\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for improving a Non Local Means operator by computing its\nlow-rank approximation. The low-rank operator is constructed by applying a\nfilter to the spectrum of the original Non Local Means operator. This results\nin an operator which is less sensitive to noise while preserving important\nproperties of the original operator. The method is efficiently implemented\nbased on Chebyshev polynomials and is demonstrated on the application of\nnatural images denoising. For this application, we provide a comprehensive\ncomparison of our method with leading denoising methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 20:13:00 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["May", "Victor", ""], ["Keller", "Yosi", ""], ["Sharon", "Nir", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1412.2210", "submitter": "Sagar Gubbi", "authors": "Sagar Venkatesh Gubbi and Chandra Sekhar Seelamantula", "title": "Risk Estimation Without Using Stein's Lemma -- Application to Image\n  Denoising", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of image denoising in additive white noise without\nplacing restrictive assumptions on its statistical distribution. In the recent\nliterature, specific noise distributions have been considered and\ncorrespondingly, optimal denoising techniques have been developed. One of the\nsuccessful approaches for denoising relies on the notion of unbiased risk\nestimation, which enables one to obtain a useful substitute for the mean-square\nerror. For the case of additive white Gaussian noise contamination, the risk\nestimation procedure relies on Stein's lemma. Sophisticated wavelet-based\ndenoising techniques, which are essentially nonlinear, have been developed with\nthe help of the lemma. We show that, for linear, shift-invariant denoisers, it\nis possible to obtain unbiased risk estimates of the mean-square error without\nusing Stein's lemma. An interesting consequence of this development is that the\nunbiased risk estimator becomes agnostic to the statistical distribution of the\nnoise. As a proof of principle, we show how the new methodology can be used to\noptimize the parameters of a simple Gaussian smoother. By locally adapting the\nparameters of the Gaussian smoother, we obtain a shift-variant smoother, which\nhas a denoising performance (quantified by the improvement in peak\nsignal-to-noise ratio (PSNR)) that is competitive to far more sophisticated\nmethods reported in the literature. The proposed solution exhibits considerable\nparallelism, which we exploit in a Graphics Processing Unit (GPU)\nimplementation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 08:00:19 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 09:22:18 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Gubbi", "Sagar Venkatesh", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1412.2231", "submitter": "Canyi Lu", "authors": "Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, Zhouchen Lin", "title": "Generalized Singular Value Thresholding", "comments": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the Generalized Singular Value Thresholding (GSVT) operator\n${\\text{Prox}}_{g}^{{\\sigma}}(\\cdot)$, \\begin{equation*}\n  {\\text{Prox}}_{g}^{{\\sigma}}(B)=\\arg\\min\\limits_{X}\\sum_{i=1}^{m}g(\\sigma_{i}(X))\n+ \\frac{1}{2}||X-B||_{F}^{2}, \\end{equation*} associated with a nonconvex\nfunction $g$ defined on the singular values of $X$. We prove that GSVT can be\nobtained by performing the proximal operator of $g$ (denoted as\n$\\text{Prox}_g(\\cdot)$) on the singular values since $\\text{Prox}_g(\\cdot)$ is\nmonotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some\nconditions (many popular nonconvex surrogate functions, e.g., $\\ell_p$-norm,\n$0<p<1$, of $\\ell_0$-norm are special cases), a general solver to find\n$\\text{Prox}_g(b)$ is proposed for any $b\\geq0$. GSVT greatly generalizes the\nknown Singular Value Thresholding (SVT) which is a basic subroutine in many\nconvex low rank minimization methods. We are able to solve the nonconvex low\nrank minimization problem by using GSVT in place of SVT.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 13:08:29 GMT"}, {"version": "v2", "created": "Sun, 27 May 2018 05:56:25 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lu", "Canyi", ""], ["Zhu", "Changbo", ""], ["Xu", "Chunyan", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1412.2291", "submitter": "Konstantin Usevich", "authors": "Konstantin Usevich and Ivan Markovsky", "title": "Adjusted least squares fitting of algebraic hypersurfaces", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.laa.2015.07.023", "report-no": null, "categories": "stat.CO cs.CG cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting a set of points in Euclidean space by an\nalgebraic hypersurface. We assume that points on a true hypersurface, described\nby a polynomial equation, are corrupted by zero mean independent Gaussian\nnoise, and we estimate the coefficients of the true polynomial equation. The\nadjusted least squares estimator accounts for the bias present in the ordinary\nleast squares estimator. The adjusted least squares estimator is based on\nconstructing a quasi-Hankel matrix, which is a bias-corrected matrix of\nmoments. For the case of unknown noise variance, the estimator is defined as a\nsolution of a polynomial eigenvalue problem. In this paper, we present new\nresults on invariance properties of the adjusted least squares estimator and an\nimproved algorithm for computing the estimator for an arbitrary set of\nmonomials in the polynomial equation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 22:22:33 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 17:00:19 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Usevich", "Konstantin", ""], ["Markovsky", "Ivan", ""]]}, {"id": "1412.2306", "submitter": "Andrej Karpathy", "authors": "Andrej Karpathy, Li Fei-Fei", "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model that generates natural language descriptions of images and\ntheir regions. Our approach leverages datasets of images and their sentence\ndescriptions to learn about the inter-modal correspondences between language\nand visual data. Our alignment model is based on a novel combination of\nConvolutional Neural Networks over image regions, bidirectional Recurrent\nNeural Networks over sentences, and a structured objective that aligns the two\nmodalities through a multimodal embedding. We then describe a Multimodal\nRecurrent Neural Network architecture that uses the inferred alignments to\nlearn to generate novel descriptions of image regions. We demonstrate that our\nalignment model produces state of the art results in retrieval experiments on\nFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generated\ndescriptions significantly outperform retrieval baselines on both full images\nand on a new dataset of region-level annotations.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 02:36:07 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 05:02:53 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Karpathy", "Andrej", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1412.2309", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka and Pietro Perona and Frederick Eberhardt", "title": "Visual Causal Feature Learning", "comments": "Accepted at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a rigorous definition of the visual cause of a behavior that is\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\nrobots and other perceiving systems. Our framework generalizes standard\naccounts of causal learning to settings in which the causal variables need to\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\nwhich allows us to gain causal knowledge from observational data with minimal\nexperimental effort. The theorem provides a connection to standard inference\ntechniques in machine learning that identify features of an image that\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\nactive learning scheme to learn a manipulator function that performs optimal\nmanipulations on the image to automatically identify the visual cause of a\ntarget behavior. We illustrate our inference and learning algorithms in\nexperiments based on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:13:27 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 22:35:30 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1412.2342", "submitter": "Hayaru Shouno", "authors": "Hayaru Shouno", "title": "Bayesian Image Restoration for Poisson Corrupted Image using a Latent\n  Variational Method with Gaussian MRF", "comments": "9 pages, 6 figures, The of this manuscript is submitting to the\n  Information Processing Society of Japan(IPSJ), Transactions on Mathematical\n  Modeling and its Applications (TOM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat an image restoration problem with a Poisson noise chan- nel using a\nBayesian framework. The Poisson randomness might be appeared in observation of\nlow contrast object in the field of imaging. The noise observation is often\nhard to treat in a theo- retical analysis. In our formulation, we interpret the\nobservation through the Poisson noise channel as a likelihood, and evaluate the\nbound of it with a Gaussian function using a latent variable method. We then\nintroduce a Gaussian Markov random field (GMRF) as the prior for the Bayesian\napproach, and derive the posterior as a Gaussian distribution. The latent\nparameters in the likelihood and the hyperparameter in the GMRF prior could be\ntreated as hid- den parameters, so that, we propose an algorithm to infer them\nin the expectation maximization (EM) framework using loopy belief\npropagation(LBP). We confirm the ability of our algorithm in the computer\nsimulation, and compare it with the results of other im- age restoration\nframeworks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 10:59:55 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Shouno", "Hayaru", ""]]}, {"id": "1412.2444", "submitter": "Raka Kundu", "authors": "Raka Kundu, Amlan Chakrabarti and Prasanna Lenka", "title": "An Approach for Reducing Outliers of Non Local Means Image Denoising\n  Filter", "comments": "The paper presents an improvement in denoising algorithm for\n  ultrasound images using the filter non-local means. The paper is accepted in\n  MedImage2014 (IISc Bangalore). The research was supported by Centre of\n  Excellence in Systems Biology and Biomedical Engineering (TEQIP PHASE-II),\n  University of Calcutta and National Institute for the Orthopaedically\n  Handicapped, Kolkata, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive approach for non local means (NLM) image filtering\ntermed as non local adaptive clipped means (NLACM), which reduces the effect of\noutliers and improves the denoising quality as compared to traditional NLM.\nCommon method to neglect outliers from a data population is computation of mean\nin a range defined by mean and standard deviation. In NLACM we perform the\nmedian within the defined range based on statistical estimation of the\nneighbourhood region of a pixel to be denoised. As parameters of the range are\nindependent of any additional input and is based on local intensity values,\nhence the approach is adaptive. Experimental results for NLACM show better\nestimation of true intensity from noisy neighbourhood observation as compared\nto NLM at high noise levels. We have verified the technique for speckle noise\nreduction and we have tested it on ultrasound (US) image of lumbar spine. These\nultrasound images act as guidance for injection therapy for treatment of lumbar\nradiculopathy. We believe that the proposed approach for image denoising is\nfirst of its kind and its efficiency can be well justified as it shows better\nperformance in image restoration.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 04:43:00 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 15:23:38 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Kundu", "Raka", ""], ["Chakrabarti", "Amlan", ""], ["Lenka", "Prasanna", ""]]}, {"id": "1412.2604", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari, Ross Girshick, Jitendra Malik", "title": "Actions and Attributes from Wholes and Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the importance of parts for the tasks of action and attribute\nclassification. We develop a part-based approach by leveraging convolutional\nnetwork features inspired by recent advances in computer vision. Our part\ndetectors are a deep version of poselets and capture parts of the human body\nunder a distinct set of poses. For the tasks of action and attribute\nclassification, we train holistic convolutional neural networks and show that\nadding parts leads to top-performing results for both tasks. In addition, we\ndemonstrate the effectiveness of our approach when we replace an oracle person\ndetector, as is the default in the current evaluation protocol for both tasks,\nwith a state-of-the-art person detection system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:28:21 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 20:41:37 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1412.2672", "submitter": "Daniel Harari", "authors": "Tao Gao, Daniel Harari, Joshua Tenenbaum, Shimon Ullman", "title": "When Computer Vision Gazes at Cognition", "comments": "Tao Gao and Daniel Harari contributed equally to this work", "journal-ref": null, "doi": null, "report-no": "CBMM Memo No. 025, MIT", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint attention is a core, early-developing form of social interaction. It is\nbased on our ability to discriminate the third party objects that other people\nare looking at. While it has been shown that people can accurately determine\nwhether another person is looking directly at them versus away, little is known\nabout human ability to discriminate a third person gaze directed towards\nobjects that are further away, especially in unconstraint cases where the\nlooker can move her head and eyes freely. In this paper we address this\nquestion by jointly exploring human psychophysics and a cognitively motivated\ncomputer vision model, which can detect the 3D direction of gaze from 2D face\nimages. The synthesis of behavioral study and computer vision yields several\ninteresting discoveries. (1) Human accuracy of discriminating targets\n8{\\deg}-10{\\deg} of visual angle apart is around 40% in a free looking gaze\ntask; (2) The ability to interpret gaze of different lookers vary dramatically;\n(3) This variance can be captured by the computational model; (4) Human\noutperforms the current model significantly. These results collectively show\nthat the acuity of human joint attention is indeed highly impressive, given the\ncomputational challenge of the natural looking task. Moreover, the gap between\nhuman and model performance, as well as the variability of gaze interpretation\nacross different lookers, require further understanding of the underlying\nmechanisms utilized by humans for this challenging task.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 17:25:57 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Gao", "Tao", ""], ["Harari", "Daniel", ""], ["Tenenbaum", "Joshua", ""], ["Ullman", "Shimon", ""]]}, {"id": "1412.2684", "submitter": "Victor Stefan Aldea", "authors": "Victor Stefan Aldea", "title": "HyperSpectral classification with adaptively weighted L1-norm\n  regularization and spatial postprocessing", "comments": "v3: 11 pages, 2 Figures, 10 Tables. Updated the results for the\n  Indian Pines image; added the results for the Pavia University image", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse regression methods have been proven effective in a wide range of\nsignal processing problems such as image compression, speech coding, channel\nequalization, linear regression and classification. In this paper a new convex\nmethod of hyperspectral image classification is developed based on the sparse\nunmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization\nterm is introduced. To further enhance class separability, the algorithm is\nkernelized using an RBF kernel and the final results are improved by a\ncombination of spatial pre and post-processing operations. It is shown that the\nproposed method is competitive with state of the art algorithms such as SVM-CK,\nKSOMP-CK and KSSP-CK.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:14:04 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 21:05:52 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 03:05:43 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Aldea", "Victor Stefan", ""]]}, {"id": "1412.2697", "submitter": "Hocine Cherifi", "authors": "Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi, and\n  Driss Aboutajdine", "title": "Image quality assessment measure based on natural image statistics in\n  the Tetrolet domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a reduced reference (RR) image quality measure based on\nnatural image statistics modeling. For this purpose, Tetrolet transform is used\nsince it provides a convenient way to capture local geometric structures. This\ntransform is applied to both reference and distorted images. Then, Gaussian\nScale Mixture (GSM) is proposed to model subbands in order to take account\nstatistical dependencies between tetrolet coefficients. In order to quantify\nthe visual degradation, a measure based on Kullback Leibler Divergence (KLD) is\nprovided. The proposed measure was tested on the Cornell VCL A-57 dataset and\ncompared with other measures according to FR-TV1 VQEG framework.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:48:26 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Abdelouahad", "Abdelkaher Ait", ""], ["Hassouni", "Mohammed El", ""], ["Cherifi", "Hocine", ""], ["Aboutajdine", "Driss", ""]]}, {"id": "1412.2700", "submitter": "Sampurna Biswas", "authors": "Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai\n  and Mathews Jacob", "title": "Subspace based low rank and joint sparse matrix recovery", "comments": "5 pages, 5 figures, Asilomar 2014 conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recovery of a low rank and jointly sparse matrix from under\nsampled measurements of its columns. This problem is highly relevant in the\nrecovery of dynamic MRI data with high spatio-temporal resolution, where each\ncolumn of the matrix corresponds to a frame in the image time series; the\nmatrix is highly low-rank since the frames are highly correlated. Similarly the\nnon-zero locations of the matrix in appropriate transform/frame domains (e.g.\nwavelet, gradient) are roughly the same in different frame. The superset of the\nsupport can be safely assumed to be jointly sparse. Unlike the classical\nmultiple measurement vector (MMV) setup that measures all the snapshots using\nthe same matrix, we consider each snapshot to be measured using a different\nmeasurement matrix. We show that this approach reduces the total number of\nmeasurements, especially when the rank of the matrix is much smaller than than\nits sparsity. Our experiments in the context of dynamic imaging shows that this\napproach is very useful in realizing free breathing cardiac MRI.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 18:42:14 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 18:36:17 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Biswas", "Sampurna", ""], ["Poddar", "Sunrita", ""], ["Dasgupta", "Soura", ""], ["Mudumbai", "Raghuraman", ""], ["Jacob", "Mathews", ""]]}, {"id": "1412.2813", "submitter": "Ningning Zhao", "authors": "Ningning Zhao and Adrian Basarab and Denis Kouame and Jean-Yves\n  Tourneret", "title": "Joint Segmentation and Deconvolution of Ultrasound Images Using a\n  Hierarchical Bayesian Model based on Generalized Gaussian Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a joint segmentation and deconvolution Bayesian method\nfor medical ultrasound (US) images. Contrary to piecewise homogeneous images,\nUS images exhibit heavy characteristic speckle patterns correlated with the\ntissue structures. The generalized Gaussian distribution (GGD) has been shown\nto be one of the most relevant distributions for characterizing the speckle in\nUS images. Thus, we propose a GGD-Potts model defined by a label map coupling\nUS image segmentation and deconvolution. The Bayesian estimators of the unknown\nmodel parameters, including the US image, the label map and all the\nhyperparameters are difficult to be expressed in closed form. Thus, we\ninvestigate a Gibbs sampler to generate samples distributed according to the\nposterior of interest. These generated samples are finally used to compute the\nBayesian estimators of the unknown parameters. The performance of the proposed\nBayesian model is compared with existing approaches via several experiments\nconducted on realistic synthetic data and in vivo US images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 23:52:57 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 18:46:30 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 10:30:17 GMT"}, {"version": "v4", "created": "Mon, 2 May 2016 16:21:49 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Zhao", "Ningning", ""], ["Basarab", "Adrian", ""], ["Kouame", "Denis", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1412.2873", "submitter": "Inna Stainvas", "authors": "Inna Stainvas and Alexandra Manevitch and Isaac Leichter", "title": "Cancer Detection with Multiple Radiologists via Soft Multiple Instance\n  Logistic Regression and $L_1$ Regularization", "comments": "20 pages, report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the multiple annotation problem in medical application\nof cancer detection in digital images. The main assumption is that though\nimages are labeled by many experts, the number of images read by the same\nexpert is not large. Thus differing with the existing work on modeling each\nexpert and ground truth simultaneously, the multi annotation information is\nused in a soft manner. The multiple labels from different experts are used to\nestimate the probability of the findings to be marked as malignant. The\nlearning algorithm minimizes the Kullback Leibler (KL) divergence between the\nmodeled probabilities and desired ones constraining the model to be compact.\nThe probabilities are modeled by logit regression and multiple instance\nlearning concept is used by us.\n  Experiments on a real-life computer aided diagnosis (CAD) problem for CXR CAD\nlung cancer detection demonstrate that the proposed algorithm leads to similar\nresults as learning with a binary RVMMIL classifier or a mixture of binary\nRVMMIL models per annotator. However, this model achieves a smaller complexity\nand is more preferable in practice.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 07:12:15 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Stainvas", "Inna", ""], ["Manevitch", "Alexandra", ""], ["Leichter", "Isaac", ""]]}, {"id": "1412.3009", "submitter": "Vaishali Khairnar mrs", "authors": "Narkhede Sachin, Deven Shah, Vaishali Khairnar, Sujata Kadu", "title": "Brain Tumor Detection Based on Bilateral Symmetry Information", "comments": "06 pages,02 figures,06 graphs. arXiv admin note: text overlap with\n  arXiv:1403.6002", "journal-ref": "ISSN:2248-9622 Vol.4,Issue 6(Version 3),June 2014,pp.98-103", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in computing technology have allowed researchers across many fields\nof endeavor to collect and maintain vast amounts of observational statistical\ndata such as clinical data,biological patient data,data regarding access of web\nsites,financial data,and the like.Brain Magnetic Resonance\nImaging(MRI)segmentation is a complex problem in the field of medical imaging\ndespite various presented methods.MR image of human brain can be divided into\nseveral sub regions especially soft tissues such as gray matter,white matter\nand cerebrospinal fluid.Although edge information is the main clue in image\nsegmentation,it can not get a better result in analysis the content of images\nwithout combining other information.The segmentation of brain tissue in the\nmagnetic resonance imaging(MRI)is very important for detecting the existence\nand outlines of tumors.In this paper,an algorithm about segmentation based on\nthe symmetry character of brain MRI image is presented.Our goal is to detect\nthe position and boundary of tumors automatically.Experiments were conducted on\nreal pictures,and the results show that the algorithm is flexible and\nconvenient.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 16:16:38 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Sachin", "Narkhede", ""], ["Shah", "Deven", ""], ["Khairnar", "Vaishali", ""], ["Kadu", "Sujata", ""]]}, {"id": "1412.3128", "submitter": "Joseph Redmon", "authors": "Joseph Redmon, Anelia Angelova", "title": "Real-Time Grasp Detection Using Convolutional Neural Networks", "comments": "Accepted to ICRA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accurate, real-time approach to robotic grasp detection based\non convolutional neural networks. Our network performs single-stage regression\nto graspable bounding boxes without using standard sliding window or region\nproposal techniques. The model outperforms state-of-the-art approaches by 14\npercentage points and runs at 13 frames per second on a GPU. Our network can\nsimultaneously perform classification so that in a single step it recognizes\nthe object and finds a good grasp rectangle. A modification to this model\npredicts multiple grasps per object by using a locally constrained prediction\nmechanism. The locally constrained model performs significantly better,\nespecially on objects that can be grasped in a variety of ways.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 21:40:24 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 06:17:54 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Redmon", "Joseph", ""], ["Angelova", "Anelia", ""]]}, {"id": "1412.3159", "submitter": "Jose M. Alvarez", "authors": "Jos\\'e M. \\'Alvarez, Ferran Diego, Joan Serrat and Antonio M. L\\'opez", "title": "Road Detection via On--line Label Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based road detection is an essential functionality for supporting\nadvanced driver assistance systems (ADAS) such as road following and vehicle\nand pedestrian detection. The major challenges of road detection are dealing\nwith shadows and lighting variations and the presence of other objects in the\nscene. Current road detection algorithms characterize road areas at pixel level\nand group pixels accordingly. However, these algorithms fail in presence of\nstrong shadows and lighting variations. Therefore, we propose a road detection\nalgorithm based on video alignment. The key idea of the algorithm is to exploit\nthe similarities occurred when a vehicle follows the same trajectory more than\nonce. In this way, road areas are learned in a first ride and then, this road\nknowledge is used to infer areas depicting drivable road surfaces in subsequent\nrides. Two different experiments are conducted to validate the proposal on\ndifferent video sequences taken at different scenarios and different daytime.\nThe former aims to perform on-line road detection. The latter aims to perform\noff-line road detection and is applied to automatically generate the\nground-truth necessary to validate road detection algorithms. Qualitative and\nquantitative evaluations prove that the proposed algorithm is a valid road\ndetection approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 00:06:49 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["\u00c1lvarez", "Jos\u00e9 M.", ""], ["Diego", "Ferran", ""], ["Serrat", "Joan", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1412.3161", "submitter": "Xiaoyu Wang", "authors": "Xiaoyu Wang, Tianbao Yang, Guobin Chen, Yuanqing Lin", "title": "Object-centric Sampling for Fine-grained Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to go beyond the state-of-the-art deep convolutional\nneural network (CNN) by incorporating the information from object detection,\nfocusing on dealing with fine-grained image classification. Unfortunately, CNN\nsuffers from over-fiting when it is trained on existing fine-grained image\nclassification benchmarks, which typically only consist of less than a few tens\nof thousands training images. Therefore, we first construct a large-scale\nfine-grained car recognition dataset that consists of 333 car classes with more\nthan 150 thousand training images. With this large-scale dataset, we are able\nto build a strong baseline for CNN with top-1 classification accuracy of 81.6%.\nOne major challenge in fine-grained image classification is that many classes\nare very similar to each other while having large within-class variation. One\ncontributing factor to the within-class variation is cluttered image\nbackground. However, the existing CNN training takes uniform window sampling\nover the image, acting as blind on the location of the object of interest. In\ncontrast, this paper proposes an \\emph{object-centric sampling} (OCS) scheme\nthat samples image windows based on the object location information. The\nchallenge in using the location information lies in how to design powerful\nobject detector and how to handle the imperfectness of detection results. To\nthat end, we design a saliency-aware object detection approach specific for the\nsetting of fine-grained image classification, and the uncertainty of detection\nresults are naturally handled in our OCS scheme. Our framework is demonstrated\nto be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the\nlarge-scale fine-grained car classification dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 00:28:49 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Wang", "Xiaoyu", ""], ["Yang", "Tianbao", ""], ["Chen", "Guobin", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1412.3328", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Teddy Furon, Vincent Gripon, Michael Rabbat and Herv\\'e\n  J\\'egou", "title": "Memory vectors for similarity search in high-dimensional spaces", "comments": "Accepted to IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an indexing architecture to store and search in a database of\nhigh-dimensional vectors from the perspective of statistical signal processing\nand decision theory. This architecture is composed of several memory units,\neach of which summarizes a fraction of the database by a single representative\nvector. The potential similarity of the query to one of the vectors stored in\nthe memory unit is gauged by a simple correlation with the memory unit's\nrepresentative vector. This representative optimizes the test of the following\nhypothesis: the query is independent from any vector in the memory unit vs. the\nquery is a simple perturbation of one of the stored vectors.\n  Compared to exhaustive search, our approach finds the most similar database\nvectors significantly faster without a noticeable reduction in search quality.\nInterestingly, the reduction of complexity is provably better in\nhigh-dimensional spaces. We empirically demonstrate its practical interest in a\nlarge-scale image search scenario with off-the-shelf state-of-the-art\ndescriptors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 14:56:41 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 17:11:21 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 14:47:33 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2015 20:53:08 GMT"}, {"version": "v5", "created": "Tue, 29 Dec 2015 15:16:22 GMT"}, {"version": "v6", "created": "Tue, 3 Jan 2017 13:48:57 GMT"}, {"version": "v7", "created": "Wed, 1 Mar 2017 21:14:52 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Iscen", "Ahmet", ""], ["Furon", "Teddy", ""], ["Gripon", "Vincent", ""], ["Rabbat", "Michael", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1412.3352", "submitter": "Neda Pourali", "authors": "Neda Pourali", "title": "Web image annotation by diffusion maps manifold learning algorithm", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": "10.5121/ijfcst.2014.4606", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation is one of the most challenging problems in machine\nvision areas. The goal of this task is to predict number of keywords\nautomatically for images captured in real data. Many methods are based on\nvisual features in order to calculate similarities between image samples. But\nthe computation cost of these approaches is very high. These methods require\nmany training samples to be stored in memory. To lessen this burden, a number\nof techniques have been developed to reduce the number of features in a\ndataset. Manifold learning is a popular approach to nonlinear dimensionality\nreduction. In this paper, we investigate Diffusion maps manifold learning\nmethod for web image auto-annotation task. Diffusion maps manifold learning\nmethod is used to reduce the dimension of some visual features. Extensive\nexperiments and analysis on NUS-WIDE-LITE web image dataset with different\nvisual features show how this manifold learning dimensionality reduction method\ncan be applied effectively to image annotation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 10:38:28 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Pourali", "Neda", ""]]}, {"id": "1412.3369", "submitter": "Faruk Ahmed", "authors": "Faruk Ahmed, Daniel Tarlow and Dhruv Batra", "title": "Candidate Constrained CRFs for Loss-Aware Structured Prediction", "comments": "20 pages including Supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating computer vision systems, we are often concerned with\nperformance on a task-specific evaluation measure such as the\nIntersection-Over-Union score used in the PASCAL VOC image segmentation\nchallenge. Ideally, our systems would be tuned specifically to these evaluation\nmeasures. However, despite much work on loss-aware structured prediction, top\nperforming systems do not use these techniques. In this work, we seek to\naddress this problem, incorporating loss-aware prediction in a manner that is\namenable to the approaches taken by top performing systems. Our main idea is to\nsimultaneously leverage two systems: a highly tuned pipeline system as is found\non top of leaderboards, and a traditional CRF. We show how to combine high\nquality candidate solutions from the pipeline with the probabilistic approach\nof the CRF that is amenable to loss-aware prediction. The result is that we can\nuse loss-aware prediction methodology to improve performance of the highly\ntuned pipeline system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 16:58:44 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Ahmed", "Faruk", ""], ["Tarlow", "Daniel", ""], ["Batra", "Dhruv", ""]]}, {"id": "1412.3421", "submitter": "Juan Eugenio Iglesias", "authors": "Juan Eugenio Iglesias and Mert Rory Sabuncu", "title": "Multi-Atlas Segmentation of Biomedical Images: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-atlas segmentation (MAS), first introduced and popularized by the\npioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh,\nGhosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert\nand Hammers (2006), is becoming one of the most widely-used and successful\nimage segmentation techniques in biomedical applications. By manipulating and\nutilizing the entire dataset of \"atlases\" (training images that have been\npreviously labeled, e.g., manually by an expert), rather than some model-based\naverage representation, MAS has the flexibility to better capture anatomical\nvariation, thus offering superior segmentation accuracy. This benefit, however,\ntypically comes at a high computational cost. Recent advancements in computer\nhardware and image processing software have been instrumental in addressing\nthis challenge and facilitated the wide adoption of MAS. Today, MAS has come a\nlong way and the approach includes a wide array of sophisticated algorithms\nthat employ ideas from machine learning, probabilistic modeling, optimization,\nand computer vision, among other fields. This paper presents a survey of\npublished MAS algorithms and studies that have applied these methods to various\nbiomedical problems. In writing this survey, we have three distinct aims. Our\nprimary goal is to document how MAS was originally conceived, later evolved,\nand now relates to alternative methods. Second, this paper is intended to be a\ndetailed reference of past research activity in MAS, which now spans over a\ndecade (2003 - 2014) and entails novel methodological developments and\napplication-specific solutions. Finally, our goal is to also present a\nperspective on the future of MAS, which, we believe, will be one of the\ndominant approaches in biomedical image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 19:28:09 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 14:35:30 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Iglesias", "Juan Eugenio", ""], ["Sabuncu", "Mert Rory", ""]]}, {"id": "1412.3474", "submitter": "Eric Tzeng", "authors": "Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell", "title": "Deep Domain Confusion: Maximizing for Domain Invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reports suggest that a generic supervised deep CNN model trained on a\nlarge-scale dataset reduces, but does not remove, dataset bias on a standard\nbenchmark. Fine-tuning deep models in a new domain can require a significant\namount of data, which for many applications is simply not available. We propose\na new CNN architecture which introduces an adaptation layer and an additional\ndomain confusion loss, to learn a representation that is both semantically\nmeaningful and domain invariant. We additionally show that a domain confusion\nmetric can be used for model selection to determine the dimension of an\nadaptation layer and the best position for the layer in the CNN architecture.\nOur proposed adaptation method offers empirical performance which exceeds\npreviously published results on a standard benchmark visual domain adaptation\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 21:20:54 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Tzeng", "Eric", ""], ["Hoffman", "Judy", ""], ["Zhang", "Ning", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.3506", "submitter": "Jose M. Alvarez", "authors": "Jose M. Alvarez and Theo Gevers and Antonio M. Lopez", "title": "Road Detection by One-Class Color Classification: Dataset and\n  Experiments", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting traversable road areas ahead a moving vehicle is a key process for\nmodern autonomous driving systems. A common approach to road detection consists\nof exploiting color features to classify pixels as road or background. These\nalgorithms reduce the effect of lighting variations and weather conditions by\nexploiting the discriminant/invariant properties of different color\nrepresentations. Furthermore, the lack of labeled datasets has motivated the\ndevelopment of algorithms performing on single images based on the assumption\nthat the bottom part of the image belongs to the road surface.\n  In this paper, we first introduce a dataset of road images taken at different\ntimes and in different scenarios using an onboard camera. Then, we devise a\nsimple online algorithm and conduct an exhaustive evaluation of different\nclassifiers and the effect of using different color representation to\ncharacterize pixels.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 00:31:37 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 00:57:36 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Alvarez", "Jose M.", ""], ["Gevers", "Theo", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1412.3596", "submitter": "Shmuel Peleg", "authors": "Yair Poleg, Tavi Halperin, Chetan Arora, Shmuel Peleg", "title": "EgoSampling: Fast-Forward and Stereo for Egocentric Videos", "comments": "in IEEE CVPR 2015, Boston, MA, June 2015", "journal-ref": "CVPR'15, Boston, June 2015", "doi": "10.1109/CVPR.2015.7299109", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While egocentric cameras like GoPro are gaining popularity, the videos they\ncapture are long, boring, and difficult to watch from start to end. Fast\nforwarding (i.e. frame sampling) is a natural choice for faster video browsing.\nHowever, this accentuates the shake caused by natural head motion, making the\nfast forwarded video useless.\n  We propose EgoSampling, an adaptive frame sampling that gives more stable\nfast forwarded videos. Adaptive frame sampling is formulated as energy\nminimization, whose optimal solution can be found in polynomial time.\n  In addition, egocentric video taken while walking suffers from the left-right\nmovement of the head as the body weight shifts from one leg to another. We turn\nthis drawback into a feature: Stereo video can be created by sampling the\nframes from the left most and right most head positions of each step, forming\napproximate stereo-pairs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 10:37:55 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 21:14:09 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Poleg", "Yair", ""], ["Halperin", "Tavi", ""], ["Arora", "Chetan", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1412.3613", "submitter": "Spyridoula Xenaki", "authors": "Spyridoula D. Xenaki and Konstantinos D. Koutroumbas and Athanasios A.\n  Rontogiannis", "title": "A Novel Adaptive Possibilistic Clustering Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TFUZZ.2015.2486806", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel possibilistic c-means clustering algorithm, called\nAdaptive Possibilistic c-means, is presented. Its main feature is that {\\it\nall} its parameters, after their initialization, are properly adapted during\nits execution. Provided that the algorithm starts with a reasonable\noverestimate of the number of physical clusters formed by the data, it is\ncapable, in principle, to unravel them (a long-standing issue in the clustering\nliterature). This is due to the fully adaptive nature of the proposed algorithm\nthat enables the removal of the clusters that gradually become obsolete. In\naddition, the adaptation of all its parameters increases the flexibility of the\nalgorithm in following the variations in the formation of the clusters that\noccur from iteration to iteration. Theoretical results that are indicative of\nthe convergence behavior of the algorithm are also provided. Finally, extensive\nsimulation results on both synthetic and real data highlight the effectiveness\nof the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 11:44:43 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 00:12:50 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2015 09:58:40 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Xenaki", "Spyridoula D.", ""], ["Koutroumbas", "Konstantinos D.", ""], ["Rontogiannis", "Athanasios A.", ""]]}, {"id": "1412.3684", "submitter": "Soren Goyal", "authors": "Soren Goyal, Paul Benjamin", "title": "Object Recognition Using Deep Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of objects using Deep Neural Networks is an active area of\nresearch and many breakthroughs have been made in the last few years. The paper\nattempts to indicate how far this field has progressed. The paper briefly\ndescribes the history of research in Neural Networks and describe several of\nthe recent advances in this field. The performances of recently developed\nNeural Network Algorithm over benchmark datasets have been tabulated. Finally,\nsome the applications of this field have been provided.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:23:13 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Goyal", "Soren", ""], ["Benjamin", "Paul", ""]]}, {"id": "1412.3708", "submitter": "Marc Goessling", "authors": "Marc Goessling and Yali Amit", "title": "Compact Compositional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact and interpretable representations is a very natural task,\nwhich has not been solved satisfactorily even for simple binary datasets. In\nthis paper, we review various ways of composing experts for binary data and\nargue that competitive forms of interaction are best suited to learn\nlow-dimensional representations. We propose a new composition rule that\ndiscourages experts from focusing on similar structures and that penalizes\nopposing votes strongly so that abstaining from voting becomes more attractive.\nWe also introduce a novel sequential initialization procedure, which is based\non a process of oversimplification and correction. Experiments show that with\nour approach very intuitive models can be learned.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:19:56 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 19:23:27 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2015 22:02:42 GMT"}, {"version": "v4", "created": "Sat, 29 Oct 2016 22:49:39 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Goessling", "Marc", ""], ["Amit", "Yali", ""]]}, {"id": "1412.3709", "submitter": "Abel Gonzalez-Garcia", "authors": "Abel Gonzalez-Garcia, Alexander Vezhnevets, Vittorio Ferrari", "title": "An active search strategy for efficient object class detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object class detectors typically apply a window classifier to all the windows\nin a large set, either in a sliding window manner or using object proposals. In\nthis paper, we develop an active search strategy that sequentially chooses the\nnext window to evaluate based on all the information gathered before. This\nresults in a substantial reduction in the number of classifier evaluations and\nin a more elegant approach in general. Our search strategy is guided by two\nforces. First, we exploit context as the statistical relation between the\nappearance of a window and its location relative to the object, as observed in\nthe training set. This enables to jump across distant regions in the image\n(e.g. observing a sky region suggests that cars might be far below) and is done\nefficiently in a Random Forest framework. Second, we exploit the score of the\nclassifier to attract the search to promising areas surrounding a highly scored\nwindow, and to keep away from areas near low scored ones. Our search strategy\ncan be applied on top of any classifier as it treats it as a black-box. In\nexperiments with R-CNN on the challenging SUN2012 dataset, our method matches\nthe detection accuracy of evaluating all windows independently, while\nevaluating 9x fewer windows.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:23:38 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 11:29:51 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Gonzalez-Garcia", "Abel", ""], ["Vezhnevets", "Alexander", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1412.3717", "submitter": "Sergey Tarasenko", "authors": "Natalia Efremova and Sergey Tarasenko", "title": "Unsupervised Neural Architecture for Saliency Detection: Extended\n  Version", "comments": "10 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural network architecture for visual saliency\ndetections, which utilizes neurophysiologically plausible mechanisms for\nextraction of salient regions. The model has been significantly inspired by\nrecent findings from neurophysiology and aimed to simulate the bottom-up\nprocesses of human selective attention. Two types of features were analyzed:\ncolor and direction of maximum variance. The mechanism we employ for processing\nthose features is PCA, implemented by means of normalized Hebbian learning and\nthe waves of spikes. To evaluate performance of our model we have conducted\npsychological experiment. Comparison of simulation results with those of\nexperiment indicates good performance of our model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 11:09:01 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 12:30:14 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Efremova", "Natalia", ""], ["Tarasenko", "Sergey", ""]]}, {"id": "1412.3914", "submitter": "Tamir Bendory", "authors": "Tamar Rott and Dorin Shriki and Tamir Bendory", "title": "Edge Preserving Multi-Modal Registration Based On Gradient Intensity\n  Self-Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a challenging task in the world of medical imaging.\nParticularly, accurate edge registration plays a central role in a variety of\nclinical conditions. The Modality Independent Neighbourhood Descriptor (MIND)\ndemonstrates state of the art alignment, based on the image self-similarity.\nHowever, this method appears to be less accurate regarding edge registration.\nIn this work, we propose a new registration method, incorporating gradient\nintensity and MIND self-similarity metric. Experimental results show the\nsuperiority of this method in edge registration tasks, while preserving the\noriginal MIND performance for other image features and textures.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 08:17:28 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Rott", "Tamar", ""], ["Shriki", "Dorin", ""], ["Bendory", "Tamir", ""]]}, {"id": "1412.3919", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham (NEUROSPIN, INRIA Saclay - Ile de France), Fabian\n  Pedregosa (INRIA Saclay - Ile de France), Michael Eickenberg (LNAO, INRIA\n  Saclay - Ile de France), Philippe Gervais (NEUROSPIN, INRIA Saclay - Ile de\n  France, LNAO), Andreas Muller, Jean Kossaifi, Alexandre Gramfort (NEUROSPIN,\n  LTCI), Bertrand Thirion (NEUROSPIN, INRIA Saclay - Ile de France), G\\\"ael\n  Varoquaux (NEUROSPIN, INRIA Saclay - Ile de France, LNAO)", "title": "Machine Learning for Neuroimaging with Scikit-Learn", "comments": "Frontiers in neuroscience, Frontiers Research Foundation, 2013, pp.15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning methods are increasingly used for neuroimaging\ndata analysis. Their main virtue is their ability to model high-dimensional\ndatasets, e.g. multivariate analysis of activation images or resting-state time\nseries. Supervised learning is typically used in decoding or encoding settings\nto relate brain images to behavioral or clinical observations, while\nunsupervised learning can uncover hidden structures in sets of images (e.g.\nresting state functional MRI) or find sub-populations in large cohorts. By\nconsidering different functional neuroimaging applications, we illustrate how\nscikit-learn, a Python machine learning library, can be used to perform some\nkey analysis steps. Scikit-learn contains a very large set of statistical\nlearning algorithms, both supervised and unsupervised, and its application to\nneuroimaging data provides a versatile tool to study the brain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 08:38:35 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Abraham", "Alexandre", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Pedregosa", "Fabian", "", "INRIA Saclay - Ile de France"], ["Eickenberg", "Michael", "", "LNAO, INRIA\n  Saclay - Ile de France"], ["Gervais", "Philippe", "", "NEUROSPIN, INRIA Saclay - Ile de\n  France, LNAO"], ["Muller", "Andreas", "", "NEUROSPIN,\n  LTCI"], ["Kossaifi", "Jean", "", "NEUROSPIN,\n  LTCI"], ["Gramfort", "Alexandre", "", "NEUROSPIN,\n  LTCI"], ["Thirion", "Bertrand", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Varoquaux", "G\u00e4el", "", "NEUROSPIN, INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1412.3925", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham (NEUROSPIN, INRIA Saclay - Ile de France), Elvis\n  Dohmatob (INRIA Saclay - Ile de France), Bertrand Thirion (NEUROSPIN, INRIA\n  Saclay - Ile de France), Dimitris Samaras, Gael Varoquaux (NEUROSPIN, INRIA\n  Saclay - Ile de France)", "title": "Region segmentation for sparse decompositions: better brain\n  parcellations from rest fMRI", "comments": null, "journal-ref": "Sparsity Techniques in Medical Imaging, Sep 2014, Boston, United\n  States. pp.8", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Images acquired during resting-state provide\ninformation about the functional organization of the brain through measuring\ncorrelations between brain areas. Independent components analysis is the\nreference approach to estimate spatial components from weakly structured data\nsuch as brain signal time courses; each of these components may be referred to\nas a brain network and the whole set of components can be conceptualized as a\nbrain functional atlas. Recently, new methods using a sparsity prior have\nemerged to deal with low signal-to-noise ratio data. However, even when using\nsophisticated priors, the results may not be very sparse and most often do not\nseparate the spatial components into brain regions. This work presents\npost-processing techniques that automatically sparsify brain maps and separate\nregions properly using geometric operations, and compares these techniques\naccording to faithfulness to data and stability metrics. In particular, among\nthreshold-based approaches, hysteresis thresholding and random walker\nsegmentation, the latter improves significantly the stability of both dense and\nsparse models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 09:00:30 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Abraham", "Alexandre", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Dohmatob", "Elvis", "", "INRIA Saclay - Ile de France"], ["Thirion", "Bertrand", "", "NEUROSPIN, INRIA\n  Saclay - Ile de France"], ["Samaras", "Dimitris", "", "NEUROSPIN, INRIA\n  Saclay - Ile de France"], ["Varoquaux", "Gael", "", "NEUROSPIN, INRIA\n  Saclay - Ile de France"]]}, {"id": "1412.3949", "submitter": "Gundram Leifert", "authors": "Tobias Strau{\\ss}, Tobias Gr\\\"uning, Gundram Leifert, Roger Labahn\n  (for the University of Rostock - CITlab)", "title": "CITlab ARGUS for historical handwritten documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CITlab's recognition system for the HTRtS competition attached to\nthe 14. International Conference on Frontiers in Handwriting Recognition, ICFHR\n2014. The task comprises the recognition of historical handwritten documents.\nThe core algorithms of our system are based on multi-dimensional recurrent\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\nsoftware modules behind that as well as the basic utility technologies are\nessentially powered by PLANET's ARGUS framework for intelligent text\nrecognition and image processing.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 11:11:30 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Strau\u00df", "Tobias", "", "for the University of Rostock - CITlab"], ["Gr\u00fcning", "Tobias", "", "for the University of Rostock - CITlab"], ["Leifert", "Gundram", "", "for the University of Rostock - CITlab"], ["Labahn", "Roger", "", "for the University of Rostock - CITlab"]]}, {"id": "1412.3958", "submitter": "Mohammed Abdelsamea", "authors": "Mohammed M. Abdelsamea", "title": "An Automatic Seeded Region Growing for 2D Biomedical Image Segmentation", "comments": "appears in Proceedings of International Conference on Environment and\n  Bio-Science 2011. subset of arXiv:1407.3664", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an automatic seeded region growing algorithm is proposed for\ncellular image segmentation. First, the regions of interest (ROIs) extracted\nfrom the preprocessed image. Second, the initial seeds are automatically\nselected based on ROIs extracted from the image. Third, the most reprehensive\nseeds are selected using a machine learning algorithm. Finally, the cellular\nimage is segmented into regions where each region corresponds to a seed. The\naim of the proposed is to automatically extract the Region of Interests (ROI)\nfrom the cellular images in terms of overcoming the explosion, under\nsegmentation and over segmentation problems. Experimental results show that the\nproposed algorithm can improve the segmented image and the segmented results\nare less noisy as compared to some existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 11:47:02 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Abdelsamea", "Mohammed M.", ""]]}, {"id": "1412.4031", "submitter": "Mikhail Konnik", "authors": "Mikhail Konnik and James Welsh", "title": "High-level numerical simulations of noise in CCD and CMOS photosensors:\n  review and tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, such as development and testing of image processing\nalgorithms, it is often necessary to simulate images containing realistic noise\nfrom solid-state photosensors. A high-level model of CCD and CMOS photosensors\nbased on a literature review is formulated in this paper. The model includes\nphoto-response non-uniformity, photon shot noise, dark current Fixed Pattern\nNoise, dark current shot noise, offset Fixed Pattern Noise, source follower\nnoise, sense node reset noise, and quantisation noise. The model also includes\nvoltage-to-voltage, voltage-to-electrons, and analogue-to-digital converter\nnon-linearities. The formulated model can be used to create synthetic images\nfor testing and validation of image processing algorithms in the presence of\nrealistic images noise. An example of the simulated CMOS photosensor and a\ncomparison with a custom-made CMOS hardware sensor is presented. Procedures for\ncharacterisation from both light and dark noises are described. Experimental\nresults that confirm the validity of the numerical model are provided. The\npaper addresses the issue of the lack of comprehensive high-level photosensor\nmodels that enable engineers to simulate realistic effects of noise on the\nimages obtained from solid-state photosensors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 11:13:28 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Konnik", "Mikhail", ""], ["Welsh", "James", ""]]}, {"id": "1412.4044", "submitter": "Jun He", "authors": "Jun He, Yue Zhang", "title": "Adaptive Stochastic Gradient Descent on the Grassmannian for Robust\n  Low-Rank Subspace Recovery and Clustering", "comments": "13 pages, 12 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient\nfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to\nrobustly recover the low-rank subspace from a large matrix. In the presence of\ncolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ norm\nminimization with rank constraint problem as a stochastic optimization approach\nconstrained on Grassmann manifold. For each observed data vector, the low-rank\nsubspace $\\mathcal{S}$ is updated by taking a gradient step along the geodesic\nof Grassmannian. In order to accelerate the convergence rate of the stochastic\ngradient method, we choose to adaptively tune the constant step-size by\nleveraging the consecutive gradients. Furthermore, we demonstrate that with\nproper initialization, the K-subspaces extension, K-GASG21, can robustly\ncluster a large number of corrupted data vectors into a union of subspaces.\nNumerical experiments on synthetic and real data demonstrate the efficiency and\naccuracy of the proposed algorithms even with heavy column outliers corruption.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 16:32:48 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 08:37:45 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["He", "Jun", ""], ["Zhang", "Yue", ""]]}, {"id": "1412.4102", "submitter": "Chunyu Wang", "authors": "Chunyu Wang, John Flynn, Yizhou Wang, Alan L. Yuille", "title": "Representing Data by a Mixture of Activated Simplices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model which represents data as a mixture of simplices.\nSimplices are geometric structures that generalize triangles. We give a simple\ngeometric understanding that allows us to learn a simplicial structure\nefficiently. Our method requires that the data are unit normalized (and thus\nlie on the unit sphere). We show that under this restriction, building a model\nwith simplices amounts to constructing a convex hull inside the sphere whose\nboundary facets is close to the data. We call the boundary facets of the convex\nhull that are close to the data Activated Simplices. While the total number of\nbases used to build the simplices is a parameter of the model, the dimensions\nof the individual activated simplices are learned from the data. Simplices can\nhave different dimensions, which facilitates modeling of inhomogeneous data\nsources. The simplicial structure is bounded --- this is appropriate for\nmodeling data with constraints, such as human elbows can not bend more than 180\ndegrees. The simplices are easy to interpret and extremes within the data can\nbe discovered among the vertices. The method provides good reconstruction and\nregularization. It supports good nearest neighbor classification and it allows\nrealistic generative models to be constructed. It achieves state-of-the-art\nresults on benchmark datasets, including 3D poses and digits.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 20:12:40 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Wang", "Chunyu", ""], ["Flynn", "John", ""], ["Wang", "Yizhou", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1412.4172", "submitter": "Sadeep Jayasumana", "authors": "Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li,\n  Mehrtash Harandi", "title": "Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite\n  Matrices", "comments": "Published in CVPR 2013. arXiv admin note: substantial text overlap\n  with arXiv:1412.0265", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric Positive Definite (SPD) matrices have become popular to encode\nimage information. Accounting for the geometry of the Riemannian manifold of\nSPD matrices has proven key to the success of many algorithms. However, most\nexisting methods only approximate the true shape of the manifold locally by its\ntangent plane. In this paper, inspired by kernel methods, we propose to map SPD\nmatrices to a high dimensional Hilbert space where Euclidean geometry applies.\nTo encode the geometry of the manifold in the mapping, we introduce a family of\nprovably positive definite kernels on the Riemannian manifold of SPD matrices.\nThese kernels are derived from the Gaussian ker- nel, but exploit different\nmetrics on the manifold. This lets us extend kernel-based algorithms developed\nfor Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of\nSPD matrices. We demonstrate the benefits of our approach on the problems of\npedestrian detection, ob- ject categorization, texture analysis, 2D motion\nsegmentation and Diffusion Tensor Imaging (DTI) segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 00:48:46 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Jayasumana", "Sadeep", ""], ["Hartley", "Richard", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "1412.4174", "submitter": "Sadeep Jayasumana", "authors": "Sadeep Jayasumana, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi", "title": "A Framework for Shape Analysis via Hilbert Space Embedding", "comments": "Published in ICCV 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for 2D shape analysis using positive definite kernels\ndefined on Kendall's shape manifold. Different representations of 2D shapes are\nknown to generate different nonlinear spaces. Due to the nonlinearity of these\nspaces, most existing shape classification algorithms resort to nearest\nneighbor methods and to learning distances on shape spaces. Here, we propose to\nmap shapes on Kendall's shape manifold to a high dimensional Hilbert space\nwhere Euclidean geometry applies. To this end, we introduce a kernel on this\nmanifold that permits such a mapping, and prove its positive definiteness. This\nkernel lets us extend kernel-based algorithms developed for Euclidean spaces,\nsuch as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the\nbenefits of our approach over the state-of-the-art methods on shape\nclassification, clustering and retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 00:53:55 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Jayasumana", "Sadeep", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "1412.4175", "submitter": "Sadeep Jayasumana", "authors": "Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li,\n  Mehrtash Harandi", "title": "Optimizing Over Radial Kernels on Compact Manifolds", "comments": "Published in CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of optimizing over all possible positive definite\nradial kernels on Riemannian manifolds for classification. Kernel methods on\nRiemannian manifolds have recently become increasingly popular in computer\nvision. However, the number of known positive definite kernels on manifolds\nremain very limited. Furthermore, most kernels typically depend on at least one\nparameter that needs to be tuned for the problem at hand. A poor choice of\nkernel, or of parameter value, may yield significant performance drop-off.\nHere, we show that positive definite radial kernels on the unit n-sphere, the\nGrassmann manifold and Kendall's shape manifold can be expressed in a simple\nform whose parameters can be automatically optimized within a support vector\nmachine framework. We demonstrate the benefits of our kernel learning algorithm\non object, face, action and shape recognition.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 00:58:57 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Jayasumana", "Sadeep", ""], ["Hartley", "Richard", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "1412.4181", "submitter": "Sam Hallman", "authors": "Sam Hallman, Charless C. Fowlkes", "title": "Oriented Edge Forests for Boundary Detection", "comments": "updated to include contents of CVPR version + new figure showing\n  example segmentation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, efficient model for learning boundary detection based on\na random forest classifier. Our approach combines (1) efficient clustering of\ntraining examples based on simple partitioning of the space of local edge\norientations and (2) scale-dependent calibration of individual tree output\nprobabilities prior to multiscale combination. The resulting model outperforms\npublished results on the challenging BSDS500 boundary detection benchmark.\nFurther, on large datasets our model requires substantially less memory for\ntraining and speeds up training time by a factor of 10 over the structured\nforest model.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 02:30:59 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 19:37:56 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Hallman", "Sam", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1412.4183", "submitter": "Eugene Borovikov", "authors": "Eugene Borovikov", "title": "A survey of modern optical character recognition techniques", "comments": "Technical report surveying OCR/ICR and document understanding methods\n  as of 2004.It contains 38 pages, numerous figures, 93 references, and\n  provides a table of contents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This report explores the latest advances in the field of digital document\nrecognition. With the focus on printed document imagery, we discuss the major\ndevelopments in optical character recognition (OCR) and document image\nenhancement/restoration in application to Latin and non-Latin scripts. In\naddition, we review and discuss the available technologies for hand-written\ndocument recognition. In this report, we also provide some company-accumulated\nbenchmark results on available OCR engines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 02:39:09 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Borovikov", "Eugene", ""]]}, {"id": "1412.4196", "submitter": "Yuanting Hu", "authors": "Yuan-Ting Hu and Yen-Yu Lin and Hsin-Yi Chen and Kuang-Jui Hsu and\n  Bing-Yu Chen", "title": "Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in\n  the Homography Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim to improve the performance of feature matching, we present an\nunsupervised approach to fuse various local descriptors in the space of\nhomographies. Inspired by the observation that the homographies of correct\nfeature correspondences vary smoothly along the spatial domain, our approach\nstands on the unsupervised nature of feature matching, and can select a good\ndescriptor for matching each feature point. Specifically, the homography space\nserves as the common domain, in which a correspondence obtained by any\ndescriptor is considered as a point, for integrating various heterogeneous\ndescriptors. Both geometric coherence and spatial continuity among\ncorrespondences are considered via computing their geodesic distances in the\nspace. In this way, mutual verification across different descriptors is\nallowed, and correct correspondences will be highlighted with a high degree of\nconsistency (i.e., short geodesic distances here). It follows that one-class\nSVM can be applied to identifying these correct correspondences, and boosts the\nperformance of feature matching. The proposed approach is comprehensively\ncompared with the state-of-the-art approaches, and evaluated on four benchmarks\nof image matching. The promising results manifest its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 06:39:04 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Lin", "Yen-Yu", ""], ["Chen", "Hsin-Yi", ""], ["Hsu", "Kuang-Jui", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "1412.4205", "submitter": "Xiaosha Zhao", "authors": "Xiaosha Zhao, Mandan Liu", "title": "The application of the Bayes Ying Yang harmony based GMMs in on-line\n  signature verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, a Bayes Ying Yang(BYY) harmony based approach for\non-line signature verification is presented. In the proposed method, a simple\nbut effective Gaussian Mixture Models(GMMs) is used to represent for each\nuser's signature model based on the prior information collected. Different from\nthe early works, in this paper, we use the Bayes Ying Yang machine combined\nwith the harmony function to achieve Automatic Model Selection(AMS) during the\nparameter learning for the GMMs, so that a better approximation of the user\nmodel is assured. Experiments on a database from the First International\nSignature Verification Competition(SVC 2004) confirm that this combined\nalgorithm yields quite satisfactory results.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 08:57:49 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Zhao", "Xiaosha", ""], ["Liu", "Mandan", ""]]}, {"id": "1412.4217", "submitter": "Zeeshan Bhatti", "authors": "Dil Nawaz Hakro, A. Z. Talib, Zeeshan Bhatti, G. N. Moja", "title": "A Study of Sindhi Related and Arabic Script Adapted languages\n  Recognition", "comments": "11 pages, 8 Figures, Sindh Univ. Res. Jour. (Sci. Ser.)", "journal-ref": "Sindh University Research Journal (Science Series) Vol. 46 (3)\n  323-334 (2014)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of publications are available for the Optical Character\nRecognition (OCR). Significant researches, as well as articles are present for\nthe Latin, Chinese and Japanese scripts. Arabic script is also one of mature\nscript from OCR perspective. The adaptive languages which share Arabic script\nor its extended characters; still lacking the OCRs for their language. In this\npaper we present the efforts of researchers on Arabic and its related and\nadapted languages. This survey is organized in different sections, in which\nintroduction is followed by properties of Sindhi Language. OCR process\ntechniques and methods used by various researchers are presented. The last\nsection is dedicated for future work and conclusion is also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 10:01:51 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Hakro", "Dil Nawaz", ""], ["Talib", "A. Z.", ""], ["Bhatti", "Zeeshan", ""], ["Moja", "G. N.", ""]]}, {"id": "1412.4237", "submitter": "Alex Sawatzky", "authors": "Martin Burger, Alex Sawatzky, Gabriele Steidl", "title": "First order algorithms in variational image processing", "comments": "60 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational methods in imaging are nowadays developing towards a quite\nuniversal and flexible tool, allowing for highly successful approaches on tasks\nlike denoising, deblurring, inpainting, segmentation, super-resolution,\ndisparity, and optical flow estimation. The overall structure of such\napproaches is of the form ${\\cal D}(Ku) + \\alpha {\\cal R} (u) \\rightarrow\n\\min_u$ ; where the functional ${\\cal D}$ is a data fidelity term also\ndepending on some input data $f$ and measuring the deviation of $Ku$ from such\nand ${\\cal R}$ is a regularization functional. Moreover $K$ is a (often linear)\nforward operator modeling the dependence of data on an underlying image, and\n$\\alpha$ is a positive regularization parameter. While ${\\cal D}$ is often\nsmooth and (strictly) convex, the current practice almost exclusively uses\nnonsmooth regularization functionals. The majority of successful techniques is\nusing nonsmooth and convex functionals like the total variation and\ngeneralizations thereof or $\\ell_1$-norms of coefficients arising from scalar\nproducts with some frame system. The efficient solution of such variational\nproblems in imaging demands for appropriate algorithms. Taking into account the\nspecific structure as a sum of two very different terms to be minimized,\nsplitting algorithms are a quite canonical choice. Consequently this field has\nrevived the interest in techniques like operator splittings or augmented\nLagrangians. Here we shall provide an overview of methods currently developed\nand recent results as well as some computational studies providing a comparison\nof different methods and also illustrating their success in applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 14:06:41 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Burger", "Martin", ""], ["Sawatzky", "Alex", ""], ["Steidl", "Gabriele", ""]]}, {"id": "1412.4313", "submitter": "Michael Cogswell", "authors": "Michael Cogswell, Xiao Lin, Senthil Purushwalkam, Dhruv Batra", "title": "Combining the Best of Graphical Models and ConvNets for Semantic\n  Segmentation", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a two-module approach to semantic segmentation that incorporates\nConvolutional Networks (CNNs) and Graphical Models. Graphical models are used\nto generate a small (5-30) set of diverse segmentations proposals, such that\nthis set has high recall. Since the number of required proposals is so low, we\ncan extract fairly complex features to rank them. Our complex feature of choice\nis a novel CNN called SegNet, which directly outputs a (coarse) semantic\nsegmentation. Importantly, SegNet is specifically trained to optimize the\ncorpus-level PASCAL IOU loss function. To the best of our knowledge, this is\nthe first CNN specifically designed for semantic segmentation. This two-module\napproach achieves $52.5\\%$ on the PASCAL 2012 segmentation challenge.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 05:23:39 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 04:39:33 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Cogswell", "Michael", ""], ["Lin", "Xiao", ""], ["Purushwalkam", "Senthil", ""], ["Batra", "Dhruv", ""]]}, {"id": "1412.4433", "submitter": "Dai-Qiang Chen", "authors": "Dai-Qiang Chen", "title": "Inexact Alternating Direction Method Based on Newton descent algorithm\n  with Application to Poisson Image Deblurring", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recovery of images from the observations that are degraded by a linear\noperator and further corrupted by Poisson noise is an important task in modern\nimaging applications such as astronomical and biomedical ones. Gradient-based\nregularizers involve the popular total variation semi-norm have become standard\ntechniques for Poisson image restoration due to its edge-preserving ability.\nVarious efficient algorithms have been developed for solving the corresponding\nminimization problem with non-smooth regularization terms. In this paper,\nmotivated by the idea of the alternating direction minimization algorithm and\nthe Newton's method with upper convergent rate, we further propose inexact\nalternating direction methods utilizing the proximal Hessian matrix information\nof the objective function, in a way reminiscent of Newton descent methods.\nBesides, we also investigate the global convergence of the proposed algorithms\nunder certain conditions. Finally, we illustrate that the proposed algorithms\noutperform the current state-of-the-art algorithms through numerical\nexperiments on Poisson image deblurring.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 00:57:44 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 07:05:59 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chen", "Dai-Qiang", ""]]}, {"id": "1412.4438", "submitter": "Dai-Qiang Chen", "authors": "Dai-Qiang Chen", "title": "Fixed Point Algorithm Based on Quasi-Newton Method for Convex\n  Minimization Problem with Application to Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving an optimization problem whose objective function is the sum of two\nconvex functions has received considerable interests in the context of image\nprocessing recently. In particular, we are interested in the scenario when a\nnon-differentiable convex function such as the total variation (TV) norm is\nincluded in the objective function due to many variational models established\nin image processing have this nature. In this paper, we propose a fast fixed\npoint algorithm based on the quasi-Newton method for solving this class of\nproblem, and apply it in the field of TV-based image deblurring. The novel\nmethod is derived from the idea of the quasi-Newton method, and the fixed-point\nalgorithms based on the proximity operator, which were widely investigated very\nrecently. Utilizing the non-expansion property of the proximity operator we\nfurther investigate the global convergence of the proposed algorithm. Numerical\nexperiments on image deblurring problem with additive or multiplicative noise\nare presented to demonstrate that the proposed algorithm is superior to the\nrecently developed fixed-point algorithm in the computational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 01:12:09 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Chen", "Dai-Qiang", ""]]}, {"id": "1412.4470", "submitter": "Walid Mahdi", "authors": "Walid Mahdi, Liming Chen, Mohsen Ardebilian", "title": "Automatic video scene segmentation based on spatial-temporal clues and\n  rhythm", "comments": "25 pages, 12 figures", "journal-ref": "Video DATA Hermes Science Publishing, 2002", "doi": null, "report-no": "ISBN-10: 1903996228 ISBN-10: 1903996228 ISBN-13: 9781903996225", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever increasing computing power and data storage capacity, the potential\nfor large digital video libraries is growing rapidly.However, the massive use\nof video for the moment is limited by its opaque characteristics. Indeed, a\nuser who has to handle and retrieve sequentially needs too much time in order\nto find out segments of interest within a video. Therefore, providing an\nenvironment both convenient and efficient for video storing and retrieval,\nespecially for content-based searching as this exists in traditional textbased\ndatabase systems, has been the focus of recent and important efforts of a large\nresearch community\n  In this paper, we propose a new automatic video scene segmentation method\nthat explores two main video features; these are spatial-temporal relationship\nand rhythm of shots. The experimental evidence we obtained from a 80\nminutevideo showed that our prototype provides very high accuracy for video\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 06:12:47 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Mahdi", "Walid", ""], ["Chen", "Liming", ""], ["Ardebilian", "Mohsen", ""]]}, {"id": "1412.4526", "submitter": "Hongsheng Li", "authors": "Hongsheng Li, Rui Zhao, Xiaogang Wang", "title": "Highly Efficient Forward and Backward Propagation of Convolutional\n  Neural Networks for Pixelwise Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present highly efficient algorithms for performing forward and backward\npropagation of Convolutional Neural Network (CNN) for pixelwise classification\non images. For pixelwise classification tasks, such as image segmentation and\nobject detection, surrounding image patches are fed into CNN for predicting the\nclasses of centered pixels via forward propagation and for updating CNN\nparameters via backward propagation. However, forward and backward propagation\nwas originally designed for whole-image classification. Directly applying it to\npixelwise classification in a patch-by-patch scanning manner is extremely\ninefficient, because surrounding patches of pixels have large overlaps, which\nlead to a lot of redundant computation.\n  The proposed algorithms eliminate all the redundant computation in\nconvolution and pooling on images by introducing novel d-regularly sparse\nkernels. It generates exactly the same results as those by patch-by-patch\nscanning. Convolution and pooling operations with such kernels are able to\ncontinuously access memory and can run efficiently on GPUs. A fraction of\npatches of interest can be chosen from each training image for backward\npropagation by applying a mask to the error map at the last CNN layer. Its\ncomputation complexity is constant with respect to the number of patches\nsampled from the image. Experiments have shown that our proposed algorithms\nspeed up commonly used patch-by-patch scanning over 1500 times in both forward\nand backward propagation. The speedup increases with the sizes of images and\npatches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 10:36:49 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 02:35:23 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Li", "Hongsheng", ""], ["Zhao", "Rui", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1412.4564", "submitter": "Karel Lenc", "authors": "Andrea Vedaldi, Karel Lenc", "title": "MatConvNet - Convolutional Neural Networks for MATLAB", "comments": "Updated for release v1.0-beta20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for\nMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.\nIt exposes the building blocks of CNNs as easy-to-use MATLAB functions,\nproviding routines for computing linear convolutions with filter banks, feature\npooling, and many more. In this manner, MatConvNet allows fast prototyping of\nnew CNN architectures; at the same time, it supports efficient computation on\nCPU and GPU allowing to train complex models on large datasets such as ImageNet\nILSVRC. This document provides an overview of CNNs and how they are implemented\nin MatConvNet and gives the technical details of each computational block in\nthe toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 12:23:35 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 15:35:25 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 14:31:06 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Vedaldi", "Andrea", ""], ["Lenc", "Karel", ""]]}, {"id": "1412.4659", "submitter": "Ju Sun", "authors": "Qing Qu, Ju Sun, John Wright", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating\n  directions", "comments": "Accepted by IEEE Trans. Information Theory. The paper has been\n  revised by the reviewers' comments. The proofs have been streamlined", "journal-ref": "IEEE Transaction on Information Theory, 62(10):5855 - 5880, 2016", "doi": "10.1109/TIT.2016.2601599", "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to find the sparsest vector (direction) in a generic subspace\n$\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S})= n < p$?\nThis problem can be considered a homogeneous variant of the sparse recovery\nproblem, and finds connections to sparse dictionary learning, sparse PCA, and\nmany other problems in signal processing and machine learning. In this paper,\nwe focus on a **planted sparse model** for the subspace: the target sparse\nvector is embedded in an otherwise random subspace. Simple convex heuristics\nfor this planted recovery problem provably break down when the fraction of\nnonzero entries in the target sparse vector substantially exceeds\n$O(1/\\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach\nbased on alternating directions, which provably succeeds even when the fraction\nof nonzero entries is $\\Omega(1)$. To the best of our knowledge, this is the\nfirst practical algorithm to achieve linear scaling under the planted sparse\nmodel. Empirically, our proposed algorithm also succeeds in more challenging\ndata models, e.g., sparse dictionary learning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 16:27:29 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:23:33 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 00:54:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Qu", "Qing", ""], ["Sun", "Ju", ""], ["Wright", "John", ""]]}, {"id": "1412.4729", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach,\n  Raymond Mooney, Kate Saenko", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural\n  Networks", "comments": "NAACL-HLT 2015 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving the visual symbol grounding problem has long been a goal of\nartificial intelligence. The field appears to be advancing closer to this goal\nwith recent breakthroughs in deep learning for natural language grounding in\nstatic images. In this paper, we propose to translate videos directly to\nsentences using a unified deep neural network with both convolutional and\nrecurrent structure. Described video datasets are scarce, and most existing\nmethods have been applied to toy domains with a small vocabulary of possible\nwords. By transferring knowledge from 1.2M+ images with category labels and\n100,000+ images with captions, our method is able to create sentence\ndescriptions of open-domain videos with large vocabularies. We compare our\napproach with recent work using language generation metrics, subject, verb, and\nobject prediction accuracy, and a human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:21:50 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 00:58:38 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 04:22:06 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Xu", "Huijuan", ""], ["Donahue", "Jeff", ""], ["Rohrbach", "Marcus", ""], ["Mooney", "Raymond", ""], ["Saenko", "Kate", ""]]}, {"id": "1412.4940", "submitter": "Luca Marchesotti", "authors": "Luca Marchesotti, Naila Murray, Florent Perronnin", "title": "Discovering beautiful attributes for aesthetic image analysis", "comments": "IJCV, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic image analysis is the study and assessment of the aesthetic\nproperties of images. Current computational approaches to aesthetic image\nanalysis either provide accurate or interpretable results. To obtain both\naccuracy and interpretability by humans, we advocate the use of learned and\nnameable visual attributes as mid-level features. For this purpose, we propose\nto discover and learn the visual appearance of attributes automatically, using\na recently introduced database, called AVA, which contains more than 250,000\nimages together with their aesthetic scores and textual comments given by\nphotography enthusiasts. We provide a detailed analysis of these annotations as\nwell as the context in which they were given. We then describe how these three\nkey components of AVA - images, scores, and comments - can be effectively\nleveraged to learn visual attributes. Lastly, we show that these learned\nattributes can be successfully used in three applications: aesthetic quality\nprediction, image tagging and retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 10:23:17 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Marchesotti", "Luca", ""], ["Murray", "Naila", ""], ["Perronnin", "Florent", ""]]}, {"id": "1412.4944", "submitter": "Paul Irofti", "authors": "Paul Irofti", "title": "Efficient GPU Implementation for Single Block Orthogonal Dictionary\n  Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary training for sparse representations involves dealing with large\nchunks of data and complex algorithms that determine time consuming\nimplementations. SBO is an iterative dictionary learning algorithm based on\nconstructing unions of orthonormal bases via singular value decomposition, that\nrepresents each data item through a single best fit orthobase. In this paper we\npresent a GPGPU approach of implementing SBO in OpenCL. We provide a lock-free\nsolution that ensures full-occupancy of the GPU by following the map-reduce\nmodel for the sparse-coding stage and by making use of the Partitioned Global\nAddress Space (PGAS) model for developing parallel dictionary updates. The\nresulting implementation achieves a favourable trade-off between algorithm\ncomplexity and data representation quality compared to PAK-SVD which is the\nstandard overcomplete dictionary learning approach. We present and discuss\nnumerical results showing a significant acceleration of the execution time for\nthe dictionary learning process.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 10:39:23 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Irofti", "Paul", ""]]}, {"id": "1412.5027", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "What is a salient object? A dataset and a baseline model for salient\n  object detection", "comments": "IEEE Transactions on Image Processing, 2014", "journal-ref": null, "doi": "10.1109/TIP.2014.2383320", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection or salient region detection models, diverging from\nfixation prediction models, have traditionally been dealing with locating and\nsegmenting the most salient object or region in a scene. While the notion of\nmost salient object is sensible when multiple objects exist in a scene, current\ndatasets for evaluation of saliency detection approaches often have scenes with\nonly one single object. We introduce three main contributions in this paper:\nFirst, we take an indepth look at the problem of salient object detection by\nstudying the relationship between where people look in scenes and what they\nchoose as the most salient object when they are explicitly asked. Based on the\nagreement between fixations and saliency judgments, we then suggest that the\nmost salient object is the one that attracts the highest fraction of fixations.\nSecond, we provide two new less biased benchmark datasets containing scenes\nwith multiple objects that challenge existing saliency models. Indeed, we\nobserved a severe drop in performance of 8 state-of-the-art models on our\ndatasets (40% to 70%). Third, we propose a very simple yet powerful model based\non superpixels to be used as a baseline for model evaluation and comparison.\nWhile on par with the best models on MSRA-5K dataset, our model wins over other\nmodels on our data highlighting a serious drawback of existing models, which is\nconvoluting the processes of locating the most salient object and its\nsegmentation. We also provide a review and statistical analysis of some labeled\nscene datasets that can be used for evaluating salient object detection models.\nWe believe that our work can greatly help remedy the over-fitting of models to\nexisting biased datasets and opens new venues for future research in this\nfast-evolving field.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 23:51:50 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "1412.5068", "submitter": "Shixiang Gu", "authors": "Shixiang Gu, Luca Rigazio", "title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown deep neural networks (DNNs) to be highly susceptible to\nwell-designed, small perturbations at the input layer, or so-called adversarial\nexamples. Taking images as an example, such distortions are often\nimperceptible, but can result in 100% mis-classification for a state of the art\nDNN. We study the structure of adversarial examples and explore network\ntopology, pre-processing and training strategies to improve the robustness of\nDNNs. We perform various experiments to assess the removability of adversarial\nexamples by corrupting with additional noise and pre-processing with denoising\nautoencoders (DAEs). We find that DAEs can remove substantial amounts of the\nadversarial noise. How- ever, when stacking the DAE with the original DNN, the\nresulting network can again be attacked by new adversarial examples with even\nsmaller distortion. As a solution, we propose Deep Contractive Network, a model\nwith a new end-to-end training procedure that includes a smoothness penalty\ninspired by the contractive autoencoder (CAE). This increases the network\nrobustness to adversarial examples, without a significant performance penalty.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 23:03:49 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 16:35:05 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 14:14:24 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2015 21:43:29 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Gu", "Shixiang", ""], ["Rigazio", "Luca", ""]]}, {"id": "1412.5083", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro, Alex Bronstein", "title": "Random Forests Can Hash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash codes are a very efficient data representation needed to be able to cope\nwith the ever growing amounts of data. We introduce a random forest semantic\nhashing scheme with information-theoretic code aggregation, showing for the\nfirst time how random forest, a technique that together with deep learning have\nshown spectacular results in classification, can also be extended to\nlarge-scale retrieval. Traditional random forest fails to enforce the\nconsistency of hashes generated from each tree for the same class data, i.e.,\nto preserve the underlying similarity, and it also lacks a principled way for\ncode aggregation across trees. We start with a simple hashing scheme, where\nindependently trained random trees in a forest are acting as hashing functions.\nWe the propose a subspace model as the splitting function, and show that it\nenforces the hash consistency in a tree for data from the same class. We also\nintroduce an information-theoretic approach for aggregating codes of individual\ntrees into a single hash code, producing a near-optimal unique hash for each\nclass. Experiments on large-scale public datasets are presented, showing that\nthe proposed approach significantly outperforms state-of-the-art hashing\nmethods for retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 17:02:18 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:26:12 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:00:24 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex", ""]]}, {"id": "1412.5104", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa, Abhishek Sharma, David Jacobs", "title": "Locally Scale-Invariant Convolutional Neural Networks", "comments": "Deep Learning and Representation Learning Workshop: NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) have shown excellent results on many\nvisual classification tasks. With the exception of ImageNet, these datasets are\ncarefully crafted such that objects are well-aligned at similar scales.\nNaturally, the feature learning problem gets more challenging as the amount of\nvariation in the data increases, as the models have to learn to be invariant to\ncertain changes in appearance. Recent results on the ImageNet dataset show that\ngiven enough data, ConvNets can learn such invariances producing very\ndiscriminative features [1]. But could we do more: use less parameters, less\ndata, learn more discriminative features, if certain invariances were built\ninto the learning process? In this paper we present a simple model that allows\nConvNets to learn features in a locally scale-invariant manner without\nincreasing the number of model parameters. We show on a modified MNIST dataset\nthat when faced with scale variation, building in scale-invariance allows\nConvNets to learn more discriminative features with reduced chances of\nover-fitting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 18:09:34 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Sharma", "Abhishek", ""], ["Jacobs", "David", ""]]}, {"id": "1412.5126", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Haoping Yu, Yao Wang", "title": "A Robust Regression Approach for Background/Foreground Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background/foreground segmentation has a lot of applications in image and\nvideo processing. In this paper, a segmentation algorithm is proposed which is\nmainly designed for text and line extraction in screen content. The proposed\nmethod makes use of the fact that the background in each block is usually\nsmoothly varying and can be modeled well by a linear combination of a few\nsmoothly varying basis functions, while the foreground text and graphics create\nsharp discontinuity. The algorithm separates the background and foreground\npixels by trying to fit pixel values in the block into a smooth function using\na robust regression method. The inlier pixels that can fit well will be\nconsidered as background, while remaining outlier pixels will be considered\nforeground. This algorithm has been extensively tested on several images from\nHEVC standard test sequences for screen content coding, and is shown to have\nsuperior performance over other methods, such as the k-means clustering based\nsegmentation algorithm in DjVu. This background/foreground segmentation can be\nused in different applications such as: text extraction, separate coding of\nbackground and foreground for compression of screen content and mixed content\ndocuments, principle line extraction from palmprint and crease detection in\nfingerprint images.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 19:02:46 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 17:26:02 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Minaee", "Shervin", ""], ["Yu", "Haoping", ""], ["Wang", "Yao", ""]]}, {"id": "1412.5275", "submitter": "Azade Rezaeezade", "authors": "Ismail Nojavani, Azade Rezaeezade and Amirhassan Monadjemi", "title": "Iranian cashes recognition using mobile", "comments": "arXiv #133709", "journal-ref": "International Journal of Computer Science & Information\n  Technology, volume 6, issue 6, pp.61-71, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In economical societies of today, using cash is an inseparable aspect of\nhuman life. People use cashes for marketing, services, entertainments, bank\noperations and so on. This huge amount of contact with cash and the necessity\nof knowing the monetary value of it caused one of the most challenging problems\nfor visually impaired people. In this paper we propose a mobile phone based\napproach to identify monetary value of a picture taken from cashes using some\nimage processing and machine vision techniques. While the developed approach is\nvery fast, it can recognize the value of cash by average accuracy of about 95%\nand can overcome different challenges like rotation, scaling, collision,\nillumination changes, perspective, and some others.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 07:51:56 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Nojavani", "Ismail", ""], ["Rezaeezade", "Azade", ""], ["Monadjemi", "Amirhassan", ""]]}, {"id": "1412.5322", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "An Algebraical Model for Gray Level Images", "comments": "The 7th International Conference, Exhibition on Optimization of\n  Electrical and Electronic Equipment, OPTIM 2000, Bra\\c{s}ov, Rom\\^ania 11-12\n  May, 2000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new algebraical model for the gray level images.\nIt can be used for digital image processing. The model adresses to those images\nwhich are generated in improper light conditions (very low or high level). The\nvector space structure is able to illustrate some features into the image using\nmodified level of contrast and luminosity. Also, the defined structure could be\nused in image enhancement. The general approach is presented with experimental\nresults to demonstrate image enhancement.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 10:21:10 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.5325", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu, Vasile Buzuloiu", "title": "Color Image Enhancement In the Framework of Logarithmic Models", "comments": "The 8th IEEE International Conference on Telecommunications, Vol. 1,\n  pp. 199-204, IEEE ICT2001, June 4 - 7, 2001, Bucharest,Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mathematical model for color image processing. It\nis a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set of\nvalues for the color space. We define two operations: addition <+> and real\nscalar multiplication <x>. With these operations the space of colors becomes a\nreal vector space. Then, defining the scalar product (.|.) and the norm || .\n||, we obtain a (logarithmic) Euclidean space. We show how we can use this\nmodel for color image enhancement and we present some experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 10:32:07 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Patrascu", "Vasile", ""], ["Buzuloiu", "Vasile", ""]]}, {"id": "1412.5328", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu, Vasile Buzuloiu", "title": "A Mathematical Model for Logarithmic Image Processing", "comments": "The 5th World Multi-Conference on Systemics, Cybernetics and\n  Informatics, Vol 13, pp. 117-122, SCI2001, July 22-25, 2001, Orlando, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new mathematical model for image processing. It\nis a logarithmical one. We consider the bounded interval (-1, 1) as the set of\ngray levels. Firstly, we define two operations: addition <+> and real scalar\nmultiplication <x>. With these operations, the set of gray levels becomes a\nreal vector space. Then, defining the scalar product (.|.) and the norm || .\n||, we obtain an Euclidean space of the gray levels. Secondly, we extend these\noperations and functions for color images. We finally show the effect of\nvarious simple operations on an image.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 10:50:25 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Patrascu", "Vasile", ""], ["Buzuloiu", "Vasile", ""]]}, {"id": "1412.5334", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu, Vasile Buzuloiu", "title": "The Affine Transforms for Image Enhancement in the Context of\n  Logarithmic Models", "comments": "International Conference on Computer Vision and Graphics, ICCVG2002,\n  25-29 September, 2002, Zakopane, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logarithmic model offers new tools for image processing. An efficient\nmethod for image enhancement is to use an affine transformation with the\nlogarithmic operations: addition and scalar multiplication. We define some\ncriteria for automatically determining the parameters of the processing and\nthis is done via mean and variance computed by logarithmic operations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 10:58:46 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Patrascu", "Vasile", ""], ["Buzuloiu", "Vasile", ""]]}, {"id": "1412.5488", "submitter": "Ashirbani Saha", "authors": "Ashirbani Saha, Q. M. Jonathan Wu", "title": "Full-reference image quality assessment by combining global and local\n  distortion measures", "comments": "31 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full-reference image quality assessment (FR-IQA) techniques compare a\nreference and a distorted/test image and predict the perceptual quality of the\ntest image in terms of a scalar value representing an objective score. The\nevaluation of FR-IQA techniques is carried out by comparing the objective\nscores from the techniques with the subjective scores (obtained from human\nobservers) provided in the image databases used for the IQA. Hence, we\nreasonably assume that the goal of a human observer is to rate the distortion\npresent in the test image. The goal oriented tasks are processed by the human\nvisual system (HVS) through top-down processing which actively searches for\nlocal distortions driven by the goal. Therefore local distortion measures in an\nimage are important for the top-down processing. At the same time, bottom-up\nprocessing also takes place signifying spontaneous visual functions in the HVS.\nTo account for this, global perceptual features can be used. Therefore, we\nhypothesize that the resulting objective score for an image can be derived from\nthe combination of local and global distortion measures calculated from the\nreference and test images. We calculate the local distortion by measuring the\nlocal correlation differences from the gradient and contrast information. For\nglobal distortion, dissimilarity of the saliency maps computed from a bottom-up\nmodel of saliency is used. The motivation behind the proposed approach has been\nthoroughly discussed, accompanied by an intuitive analysis. Finally,\nexperiments are conducted in six benchmark databases suggesting the\neffectiveness of the proposed approach that achieves competitive performance\nwith the state-of-the-art methods providing an improvement in the overall\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 17:15:12 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Saha", "Ashirbani", ""], ["Wu", "Q. M. Jonathan", ""]]}, {"id": "1412.5490", "submitter": "Ashirbani Saha", "authors": "Ashirbani Saha, Q. M. Jonathan Wu", "title": "High Frequency Content based Stimulus for Perceptual Sharpness\n  Assessment in Natural Images", "comments": "13 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blind approach to evaluate the perceptual sharpness present in a natural\nimage is proposed. Though the literature demonstrates a set of variegated\nvisual cues to detect or evaluate the absence or presence of sharpness, we\nemphasize in the current work that high frequency content and local standard\ndeviation can form strong features to compute perceived sharpness in any\nnatural image, and can be considered an able alternative for the existing cues.\nUnsharp areas in a natural image happen to exhibit uniform intensity or lack of\nsharp changes between regions. Sharp region transitions in an image are caused\nby the presence of spatial high frequency content. Therefore, in the proposed\napproach, we hypothesize that using the high frequency content as the principal\nstimulus, the perceived sharpness can be quantified in an image. When an image\nis convolved with a high pass filter, higher values at any pixel location\nsignify the presence of high frequency content at those locations. Considering\nthese values as the stimulus, the exponent of the stimulus is weighted by local\nstandard deviation to impart the contribution of the local contrast within the\nformation of the sharpness map. The sharpness map highlights the relatively\nsharper regions in the image and is used to calculate the perceived sharpness\nscore of the image. The advantages of the proposed method lie in its use of\nsimple visual cues of high frequency content and local contrast to arrive at\nthe perceptual score, and requiring no training with the images. The promise of\nthe proposed method is demonstrated by its ability to compute perceived\nsharpness for within image and across image sharpness changes and for blind\nevaluation of perceptual degradation resulting due to presence of blur.\nExperiments conducted on several databases demonstrate improved performance of\nthe proposed method over that of the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 17:28:53 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 02:57:51 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Saha", "Ashirbani", ""], ["Wu", "Q. M. Jonathan", ""]]}, {"id": "1412.5661", "submitter": "Wanli Ouyang", "authors": "Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong\n  Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, Xiaoou Tang", "title": "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object\n  Detection", "comments": "CVPR15, arXiv admin note: substantial text overlap with\n  arXiv:1409.3505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose deformable deep convolutional neural networks for\ngeneric object detection. This new deep learning object detection framework has\ninnovations in multiple aspects. In the proposed new deep architecture, a new\ndeformation constrained pooling (def-pooling) layer models the deformation of\nobject parts with geometric constraint and penalty. A new pre-training strategy\nis proposed to learn feature representations more suitable for the object\ndetection task and with good generalization capability. By changing the net\nstructures, training strategies, adding and removing some key components in the\ndetection pipeline, a set of models with large diversity are obtained, which\nsignificantly improves the effectiveness of model averaging. The proposed\napproach improves the mean averaged precision obtained by RCNN\n\\cite{girshick2014rich}, which was the state-of-the-art, from 31\\% to 50.3\\% on\nthe ILSVRC2014 detection test set. It also outperforms the winner of\nILSVRC2014, GoogLeNet, by 6.1\\%. Detailed component-wise analysis is also\nprovided through extensive experimental evaluation, which provide a global view\nfor people to understand the deep learning object detection pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:41:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 03:24:08 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Zeng", "Xingyu", ""], ["Qiu", "Shi", ""], ["Luo", "Ping", ""], ["Tian", "Yonglong", ""], ["Li", "Hongsheng", ""], ["Yang", "Shuo", ""], ["Wang", "Zhe", ""], ["Loy", "Chen-Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1412.5687", "submitter": "Abhijit Bendale", "authors": "Abhijit Bendale, Terrance Boult", "title": "Towards Open World Recognition", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  (2015) 1893 - 1902", "doi": "10.1109/CVPR.2015.7298799", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the of advent rich classification models and high computational power\nvisual recognition systems have found many operational applications.\nRecognition in the real world poses multiple challenges that are not apparent\nin controlled lab environments. The datasets are dynamic and novel categories\nmust be continuously detected and then added. At prediction time, a trained\nsystem has to deal with myriad unseen categories. Operational systems require\nminimum down time, even to learn. To handle these operational issues, we\npresent the problem of Open World recognition and formally define it. We prove\nthat thresholding sums of monotonically decreasing functions of distances in\nlinearly transformed feature space can balance \"open space risk\" and empirical\nrisk. Our theory extends existing algorithms for open world recognition. We\npresent a protocol for evaluation of open world recognition systems. We present\nthe Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, adding\nobject categories incrementally while detecting outliers and managing open\nspace risk. We perform experiments on the ImageNet dataset with 1.2M+ images to\nvalidate the effectiveness of our method on large scale visual recognition\ntasks. NNO consistently yields superior results on open world recognition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 00:07:45 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Bendale", "Abhijit", ""], ["Boult", "Terrance", ""]]}, {"id": "1412.5758", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem\n  Agarwala, Jonathan Brandt, Thomas S. Huang", "title": "Decomposition-Based Domain Adaptation for Real-World Font Recognition", "comments": "This paper has been withdrawn by the author due to project concerns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a domain adaption framework to address a domain mismatch between\nsynthetic training and real-world testing data. We demonstrate our method on a\nchallenging fine-grain classification problem: recognizing a font style from an\nimage of text. In this task, it is very easy to generate lots of rendered font\nexamples but very hard to obtain real-world labeled images. This\nreal-to-synthetic domain gap caused poor generalization to new real data in\nprevious font recognition methods (Chen et al. (2014)). In this paper, we\nintroduce a Convolutional Neural Network decomposition approach, leveraging a\nlarge training corpus of synthetic data to obtain effective features for\nclassification. This is done using an adaptation technique based on a Stacked\nConvolutional Auto-Encoder that exploits a large collection of unlabeled\nreal-world text images combined with synthetic data preprocessed in a specific\nway. The proposed DeepFont method achieves an accuracy of higher than 80%\n(top-5) on a new large labeled real-world dataset we collected.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 08:51:15 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 02:45:35 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 22:41:40 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2015 22:02:59 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Jianchao", ""], ["Jin", "Hailin", ""], ["Shechtman", "Eli", ""], ["Agarwala", "Aseem", ""], ["Brandt", "Jonathan", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1412.5764", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu, Vasile Buzuloiu", "title": "Image Dynamic Range Enhancement in the Context of Logarithmic Models", "comments": "The 11th European Signal Processing Conference, EUSIPCO 2002,\n  Toulouse, France, 03-06 september 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of a scene observed under a variable illumination or with a variable\noptical aperture are not identical. Does a privileged representant exist? In\nwhich mathematical context? How to obtain it? The authors answer to such\nquestions in the context of logarithmic models for images. After a short\npresentation of the model, the paper presents two image transforms: one\nperforms an optimal enhancement of the dynamic range, and the other does the\nsame for the mean dynamic range. Experimental results are shown.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 09:09:17 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Patrascu", "Vasile", ""], ["Buzuloiu", "Vasile", ""]]}, {"id": "1412.5769", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Gray level image enhancement using the Bernstein polynomials", "comments": "Scientific Bulletin of the Politechnica, University of\n  Timisoara,Transactions on Electronics and Communications, Vol. 47 (61), No:\n  2,pp.121-126, June 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for enhancing the gray level images. This\npresented method takes part from the category of point operations and it is\nbased on piecewise linear functions. The interpolation nodes of these functions\nare calculated using the Bernstein polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 09:19:50 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.5787", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Gray Level Image Enhancement Using Polygonal Functions", "comments": "The 13th International Conference on Automation, Quality and Testing,\n  Robotics, Vol. Robotics, Image and Signal processing, pp. 129-134, May 23-25\n  2002, Cluj-Napoca, Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for enhancing the gray level images. This method\ntakes part from the category of point transforms and it is based on\ninterpolation functions. The latter have a graphic represented by polygonal\nlines. The interpolation nodes of these functions are calculated taking into\naccount the statistics of gray levels belonging to the image.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 10:12:49 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.5796", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Image Enhancement Using a Generalization of Homographic Function", "comments": "The IEEE International Conference COMMUNICATIONS 2002, pp. 429-434,\n  December 5-7, 2002, Bucharest, Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method of gray level image enhancement, based on\npoint transforms. In order to define the transform function, it was used a\ngeneralization of the homographic function.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 10:32:57 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.5802", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Contour Detection Using Contrast Formulas in the Framework of\n  Logarithmic Models", "comments": "The 8th International Conference, Exhibition on Optimization of\n  Electrical and Electronic Equipment, OPTIM 2002, Vol III, pp 751-756, 16 - 17\n  May 2002, Brasov, Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use a new logarithmic model of image representation,\ndeveloped in [1,2], for edge detection. In fact, in the framework of the new\nmodel we obtain the formulas for computing the \"contrast of a pixel\" and the\n\"contrast\" image is just the \"contour\" or edge image. In our setting the range\nof values is preserved and the quality of the contour is good for high as well\nas for low luminosity regions. We present the comparison of our results with\nthe results using classical edge detection operators.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 10:59:09 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.5808", "submitter": "Johannes Niedermayer", "authors": "Johannes Niedermayer, Peer Kr\\\"oger", "title": "Minimizing the Number of Matching Queries for Object Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To increase the computational efficiency of interest-point based object\nretrieval, researchers have put remarkable research efforts into improving the\nefficiency of kNN-based feature matching, pursuing to match thousands of\nfeatures against a database within fractions of a second. However, due to the\nhigh-dimensional nature of image features that reduces the effectivity of index\nstructures (curse of dimensionality), due to the vast amount of features stored\nin image databases (images are often represented by up to several thousand\nfeatures), this ultimate goal demanded to trade query runtimes for query\nprecision. In this paper we address an approach complementary to indexing in\norder to improve the runtimes of retrieval by querying only the most promising\nkeypoint descriptors, as this affects matching runtimes linearly and can\ntherefore lead to increased efficiency. As this reduction of kNN queries\nreduces the number of tentative correspondences, a loss of query precision is\nminimized by an additional image-level correspondence generation stage with a\ncomputational performance independent of the underlying indexing structure. We\nevaluate such an adaption of the standard recognition pipeline on a variety of\ndatasets using both SIFT and state-of-the-art binary descriptors. Our results\nsuggest that decreasing the number of queried descriptors does not necessarily\nimply a reduction in the result quality as long as alternative ways of\nincreasing query recall (by thoroughly selecting k) and MAP (using image-level\ncorrespondence generation) are considered.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 11:20:39 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 10:10:14 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2015 07:08:05 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Niedermayer", "Johannes", ""], ["Kr\u00f6ger", "Peer", ""]]}, {"id": "1412.5902", "submitter": "Teng Qiu", "authors": "Teng Qiu, Kaifu Yang, Chaoyi Li, Yongjie Li", "title": "Nearest Descent, In-Tree, and Clustering", "comments": "28 pages: text part(1-14), supplementary material(15-28)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a physically inspired graph-theoretical clustering\nmethod, which first makes the data points organized into an attractive graph,\ncalled In-Tree, via a physically inspired rule, called Nearest Descent (ND). In\nparticular, the rule of ND works to select the nearest node in the descending\ndirection of potential as the parent node of each node, which is in essence\ndifferent from the classical Gradient Descent or Steepest Descent. The\nconstructed In-Tree proves a very good candidate for clustering due to its\nparticular features and properties. In the In-Tree, the original clustering\nproblem is reduced to a problem of removing a very few of undesired edges from\nthis graph. Pleasingly, the undesired edges in In-Tree are so distinguishable\nthat they can be easily determined in either automatic or interactive way,\nwhich is in stark contrast to the cases in the widely used Minimal Spanning\nTree and k-nearest-neighbor graph. The cluster number in the proposed method\ncan be easily determined based on some intermediate plots, and the cluster\nassignment for each node is easily made by quickly searching its root node in\neach sub-graph (also an In-Tree). The proposed method is extensively evaluated\non both synthetic and real-world datasets. Overall, the proposed clustering\nmethod is a density-based one, but shows significant differences and advantages\nin comparison to the traditional ones. The proposed method is simple yet\nefficient and reliable, and is applicable to various datasets with diverse\nshapes, attributes and any high dimensionality\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 11:38:55 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 14:57:28 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Qiu", "Teng", ""], ["Yang", "Kaifu", ""], ["Li", "Chaoyi", ""], ["Li", "Yongjie", ""]]}, {"id": "1412.5903", "submitter": "Max Jaderberg", "authors": "Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman", "title": "Deep Structured Output Learning for Unconstrained Text Recognition", "comments": "arXiv admin note: text overlap with arXiv:1406.2227", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a representation suitable for the unconstrained recognition of\nwords in natural images: the general case of no fixed lexicon and unknown\nlength.\n  To this end we propose a convolutional neural network (CNN) based\narchitecture which incorporates a Conditional Random Field (CRF) graphical\nmodel, taking the whole word image as a single input. The unaries of the CRF\nare provided by a CNN that predicts characters at each position of the output,\nwhile higher order terms are provided by another CNN that detects the presence\nof N-grams. We show that this entire model (CRF, character predictor, N-gram\npredictor) can be jointly optimised by back-propagating the structured output\nloss, essentially requiring the system to perform multi-task learning, and\ntraining uses purely synthetically generated data. The resulting model is a\nmore accurate system on standard real-world text recognition benchmarks than\ncharacter prediction alone, setting a benchmark for systems that have not been\ntrained on a particular lexicon. In addition, our model achieves\nstate-of-the-art accuracy in lexicon-constrained scenarios, without being\nspecifically modelled for constrained recognition. To test the generalisation\nof our model, we also perform experiments with random alpha-numeric strings to\nevaluate the method when no visual language model is applicable.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:49:46 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 17:37:37 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 19:56:48 GMT"}, {"version": "v4", "created": "Tue, 23 Dec 2014 13:17:59 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 15:36:01 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Jaderberg", "Max", ""], ["Simonyan", "Karen", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1412.6012", "submitter": "Gundram Leifert", "authors": "Gundram Leifert, Tobias Gr\\\"uning, Tobias Strau{\\ss}, Roger Labahn\n  (for the University of Rostock - CITlab)", "title": "CITlab ARGUS for historical data tables", "comments": "arXiv admin note: text overlap with arXiv:1412.3949", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CITlab's recognition system for the ANWRESH-2014 competition\nattached to the 14. International Conference on Frontiers in Handwriting\nRecognition, ICFHR 2014. The task comprises word recognition from segmented\nhistorical documents. The core components of our system are based on\nmulti-dimensional recurrent neural networks (MDRNN) and connectionist temporal\nclassification (CTC). The software modules behind that as well as the basic\nutility technologies are essentially powered by PLANET's ARGUS framework for\nintelligent text recognition and image processing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 06:54:47 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Leifert", "Gundram", "", "for the University of Rostock - CITlab"], ["Gr\u00fcning", "Tobias", "", "for the University of Rostock - CITlab"], ["Strau\u00df", "Tobias", "", "for the University of Rostock - CITlab"], ["Labahn", "Roger", "", "for the University of Rostock - CITlab"]]}, {"id": "1412.6018", "submitter": "Rachada Kongkachandra Asst.Prof.Dr.", "authors": "Sirisak Visessenee, Sanparith Marukatat, and Rachada Kongkachandra", "title": "Automatic Training Data Synthesis for Handwriting Recognition Using the\n  Structural Crossing-Over Technique", "comments": "8 pages, 6 figures", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 5, No. 5, September 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel technique called \"Structural Crossing-Over\" to\nsynthesize qualified data for training machine learning-based handwriting\nrecognition. The proposed technique can provide a greater variety of patterns\nof training data than the existing approaches such as elastic distortion and\ntangent-based affine transformation. A couple of training characters are\nchosen, then they are analyzed by their similar and different structures, and\nfinally are crossed over to generate the new characters. The experiments are\nset to compare the performances of tangent-based affine transformation and the\nproposed approach in terms of the variety of generated characters and percent\nof recognition errors. The standard MNIST corpus including 60,000 training\ncharacters and 10,000 test characters is employed in the experiments. The\nproposed technique uses 1,000 characters to synthesize 60,000 characters, and\nthen uses these data to train and test the benchmark handwriting recognition\nsystem that exploits Histogram of Gradient (HOG) as features and Support Vector\nMachine (SVM) as recognizer. The experimental result yields 8.06% of errors. It\nsignificantly outperforms the tangent-based affine transformation and the\noriginal MNIST training data, which are 11.74% and 16.55%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 04:32:20 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Visessenee", "Sirisak", ""], ["Marukatat", "Sanparith", ""], ["Kongkachandra", "Rachada", ""]]}, {"id": "1412.6056", "submitter": "Rostislav Goroshin", "authors": "Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun", "title": "Unsupervised Learning of Spatiotemporally Coherent Metrics", "comments": "To appear at ICCV2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art classification and detection algorithms rely on\nsupervised training. In this work we study unsupervised feature learning in the\ncontext of temporally coherent video data. We focus on feature learning from\nunlabeled video data, using the assumption that adjacent video frames contain\nsemantically similar information. This assumption is exploited to train a\nconvolutional pooling auto-encoder regularized by slowness and sparsity. We\nestablish a connection between slow feature learning to metric learning and\nshow that the trained encoder can be used to define a more temporally and\nsemantically coherent metric.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 20:31:56 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 21:57:59 GMT"}, {"version": "v3", "created": "Sat, 27 Dec 2014 17:16:47 GMT"}, {"version": "v4", "created": "Thu, 26 Feb 2015 19:28:21 GMT"}, {"version": "v5", "created": "Fri, 4 Sep 2015 19:46:15 GMT"}, {"version": "v6", "created": "Tue, 8 Sep 2015 18:39:03 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Goroshin", "Ross", ""], ["Bruna", "Joan", ""], ["Tompson", "Jonathan", ""], ["Eigen", "David", ""], ["LeCun", "Yann", ""]]}, {"id": "1412.6061", "submitter": "Gundram Leifert", "authors": "Gundram Leifert, Roger Labahn, Tobias Strau{\\ss} (University of\n  Rostock - CITlab)", "title": "CITlab ARGUS for Arabic Handwriting", "comments": "http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years it turned out that multidimensional recurrent neural\nnetworks (MDRNN) perform very well for offline handwriting recognition tasks\nlike the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and\ndictionary lookup, our ARGUS software completed this task with an error rate of\n26.27% in its primary setup.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 06:55:28 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Leifert", "Gundram", "", "University of\n  Rostock - CITlab"], ["Labahn", "Roger", "", "University of\n  Rostock - CITlab"], ["Strau\u00df", "Tobias", "", "University of\n  Rostock - CITlab"]]}, {"id": "1412.6071", "submitter": "Benjamin Graham", "authors": "Benjamin Graham", "title": "Fractional Max-Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks almost always incorporate some form of spatial\npooling, and very often it is alpha times alpha max-pooling with alpha=2.\nMax-pooling act on the hidden layers of the network, reducing their size by an\ninteger multiplicative factor alpha. The amazing by-product of discarding 75%\nof your data is that you build into the network a degree of invariance with\nrespect to translations and elastic distortions. However, if you simply\nalternate convolutional layers with max-pooling layers, performance is limited\ndue to the rapid reduction in spatial size, and the disjoint nature of the\npooling regions. We have formulated a fractional version of max-pooling where\nalpha is allowed to take non-integer values. Our version of max-pooling is\nstochastic as there are lots of different ways of constructing suitable pooling\nregions. We find that our form of fractional max-pooling reduces overfitting on\na variety of datasets: for instance, we improve on the state-of-the art for\nCIFAR-100 without even using dropout.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 20:45:11 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 11:06:35 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 20:06:22 GMT"}, {"version": "v4", "created": "Tue, 12 May 2015 06:36:11 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Graham", "Benjamin", ""]]}, {"id": "1412.6092", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Image enhancement using the mean dynamic range maximization with\n  logarithmic operations", "comments": "Periodica Politechnica, Transactions on Automatic Control and\n  Computer Science, Vol.47 (61), 2002, ISSN 1224-600X, pp. 121-126, Timisoara,\n  Romania. arXiv admin note: text overlap with arXiv:1412.5764", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use a logarithmic model for gray level image enhancement. We\nbegin with a short presentation of the model and then, we propose a new formula\nfor the mean dynamic range. After that we present two image transforms: one\nperforms an optimal enhancement of the mean dynamic range using the logarithmic\naddition, and the other does the same for positive and negative values using\nthe logarithmic scalar multiplication. We present the comparison of the results\nobtained by dynamic ranges optimization with the results obtained using\nclassical image enhancement methods like gamma correction and histogram\nequalization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 10:19:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.6115", "submitter": "Yunchao Gong", "authors": "Yunchao Gong and Liu Liu and Ming Yang and Lubomir Bourdev", "title": "Compressing Deep Convolutional Networks using Vector Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) has become the most promising method\nfor object recognition, repeatedly demonstrating record breaking results for\nimage classification and object detection in recent years. However, a very deep\nCNN generally involves many layers with millions of parameters, making the\nstorage of the network model to be extremely large. This prohibits the usage of\ndeep CNNs on resource limited hardware, especially cell phones or other\nembedded devices. In this paper, we tackle this model storage issue by\ninvestigating information theoretical vector quantization methods for\ncompressing the parameters of CNNs. In particular, we have found in terms of\ncompressing the most storage demanding dense connected layers, vector\nquantization methods have a clear gain over existing matrix factorization\nmethods. Simply applying k-means clustering to the weights or conducting\nproduct quantization can lead to a very good balance between model size and\nrecognition accuracy. For the 1000-category classification task in the ImageNet\nchallenge, we are able to achieve 16-24 times compression of the network with\nonly 1% loss of classification accuracy using the state-of-the-art CNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:09:01 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Gong", "Yunchao", ""], ["Liu", "Liu", ""], ["Yang", "Ming", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1412.6124", "submitter": "Jianyu Wang", "authors": "Jianyu Wang and Alan Yuille", "title": "Semantic Part Segmentation using Compositional Model combining Shape and\n  Appearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of semantic part segmentation for\nanimals. This is more challenging than standard object detection, object\nsegmentation and pose estimation tasks because semantic parts of animals often\nhave similar appearance and highly varying shapes. To tackle these challenges,\nwe build a mixture of compositional models to represent the object boundary and\nthe boundaries of semantic parts. And we incorporate edge, appearance, and\nsemantic part cues into the compositional model. Given part-level segmentation\nannotation, we develop a novel algorithm to learn a mixture of compositional\nmodels under various poses and viewpoints for certain animal classes.\nFurthermore, a linear complexity algorithm is offered for efficient inference\nof the compositional model using dynamic programming. We evaluate our method\nfor horse and cow using a newly annotated dataset on Pascal VOC 2010 which has\npixelwise part labels. Experimental results demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:27:38 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Wang", "Jianyu", ""], ["Yuille", "Alan", ""]]}, {"id": "1412.6134", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Andrew Thompson, Robert Calderbank, Guillermo Sapiro", "title": "Data Representation using the Weyl Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weyl transform is introduced as a rich framework for data representation.\nTransform coefficients are connected to the Walsh-Hadamard transform of\nmultiscale autocorrelations, and different forms of dyadic periodicity in a\nsignal are shown to appear as different features in its Weyl coefficients. The\nWeyl transform has a high degree of symmetry with respect to a large group of\nmultiscale transformations, which allows compact yet discriminative\nrepresentations to be obtained by pooling coefficients. The effectiveness of\nthe Weyl transform is demonstrated through the example of textured image\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:34:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 18:19:32 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:03:43 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2015 14:11:54 GMT"}, {"version": "v5", "created": "Tue, 21 Jul 2015 14:12:37 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Qiu", "Qiang", ""], ["Thompson", "Andrew", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1412.6149", "submitter": "Karim Hammoudi KH", "authors": "Karim Hammoudi, Nabil Ajam, Mohamed Kasraoui, Fadi Dornaika, Karan\n  Radhakrishnan, Karthik Bandi, Qing Cai, Sai Liu", "title": "Design, Implementation and Simulation of a Cloud Computing System for\n  Enhancing Real-time Video Services by using VANET and Onboard Navigation\n  Systems", "comments": "paper accepted for publication in the proceedings of the \"17\\`eme\n  Colloque Compression et Repr\\'esentation des Signaux Audiovisuels\" (CORESA),\n  5p., Reims, France, 2014. (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a design for novel and experimental cloud computing\nsystems. The proposed system aims at enhancing computational, communicational\nand annalistic capabilities of road navigation services by merging several\nindependent technologies, namely vision-based embedded navigation systems,\nprominent Cloud Computing Systems (CCSs) and Vehicular Ad-hoc NETwork (VANET).\nThis work presents our initial investigations by describing the design of a\nglobal generic system. The designed system has been experimented with various\nscenarios of video-based road services. Moreover, the associated architecture\nhas been implemented on a small-scale simulator of an in-vehicle embedded\nsystem. The implemented architecture has been experimented in the case of a\nsimulated road service to aid the police agency. The goal of this service is to\nrecognize and track searched individuals and vehicles in a real-time monitoring\nsystem remotely connected to moving cars. The presented work demonstrates the\npotential of our system for efficiently enhancing and diversifying real-time\nvideo services in road environments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 13:52:07 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Hammoudi", "Karim", ""], ["Ajam", "Nabil", ""], ["Kasraoui", "Mohamed", ""], ["Dornaika", "Fadi", ""], ["Radhakrishnan", "Karan", ""], ["Bandi", "Karthik", ""], ["Cai", "Qing", ""], ["Liu", "Sai", ""]]}, {"id": "1412.6153", "submitter": "Arjun B Krishnan", "authors": "Arjun B. Krishnan and Jayaram Kollipara", "title": "Intelligent Indoor Mobile Robot Navigation Using Stereo Vision", "comments": "9 pages, SIPIJ August 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the existing robot navigation systems, which facilitate the use\nof laser range finders, sonar sensors or artificial landmarks, has the ability\nto locate itself in an unknown environment and then build a map of the\ncorresponding environment. Stereo vision, while still being a rapidly\ndeveloping technique in the field of autonomous mobile robots, are currently\nless preferable due to its high implementation cost. This paper aims at\ndescribing an experimental approach for the building of a stereo vision system\nthat helps the robots to avoid obstacles and navigate through indoor\nenvironments and at the same time remaining very much cost effective. This\npaper discusses the fusion techniques of stereo vision and ultrasound sensors\nwhich helps in the successful navigation through different types of complex\nenvironments. The data from the sensor enables the robot to create the two\ndimensional topological map of unknown environments and stereo vision systems\nmodels the three dimension model of the same environment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 20:01:45 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Krishnan", "Arjun B.", ""], ["Kollipara", "Jayaram", ""]]}, {"id": "1412.6154", "submitter": "Ana Romero", "authors": "Ana Romero, Julio Rubio, Francis Sergeraert", "title": "Effective persistent homology of digital images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, three Computational Topology methods (namely effective\nhomology, persistent homology and discrete vector fields) are mixed together to\nproduce algorithms for homological digital image processing. The algorithms\nhave been implemented as extensions of the Kenzo system and have shown a good\nperformance when applied on some actual images extracted from a public dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 11:45:07 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Romero", "Ana", ""], ["Rubio", "Julio", ""], ["Sergeraert", "Francis", ""]]}, {"id": "1412.6163", "submitter": "Narges Ahmidi", "authors": "Piyush Poddar, Narges Ahmidi, S. Swaroop Vedula, Lisa Ishii, Gregory\n  D. Hager, Masaru Ishii", "title": "Automated Objective Surgical Skill Assessment in the Operating Room\n  Using Unstructured Tool Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on surgical skill assessment using intraoperative tool motion\nin the operating room (OR) has focused on highly-structured surgical tasks such\nas cholecystectomy. Further, these methods only considered generic motion\nmetrics such as time and number of movements, which are of limited instructive\nvalue. In this paper, we developed and evaluated an automated approach to the\nsurgical skill assessment of nasal septoplasty in the OR. The obstructed field\nof view and highly unstructured nature of septoplasty precludes trainees from\nefficiently learning the procedure. We propose a descriptive structure of\nseptoplasty consisting of two types of activity: (1) brushing activity directed\naway from the septum plane characterizing the consistency of the surgeon's\nwrist motion and (2) activity along the septal plane characterizing the\nsurgeon's coverage pattern. We derived features related to these two activity\ntypes that classify a surgeon's level of training with an average accuracy of\nabout 72%. The features we developed provide surgeons with personalized,\nactionable feedback regarding their tool motion.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 22:09:22 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Poddar", "Piyush", ""], ["Ahmidi", "Narges", ""], ["Vedula", "S. Swaroop", ""], ["Ishii", "Lisa", ""], ["Hager", "Gregory D.", ""], ["Ishii", "Masaru", ""]]}, {"id": "1412.6279", "submitter": "Adriana Gonzalez", "authors": "Adriana Gonzalez, V\\'eronique Delouille, Laurent Jacques", "title": "Non-parametric PSF estimation from celestial transit solar images using\n  blind deconvolution", "comments": "31 pages, 47 figures", "journal-ref": "J. Space Weather Space Clim., 6, A1, 2016", "doi": "10.1051/swsc/2015040", "report-no": null, "categories": "cs.CV astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Characterization of instrumental effects in astronomical imaging is\nimportant in order to extract accurate physical information from the\nobservations. The measured image in a real optical instrument is usually\nrepresented by the convolution of an ideal image with a Point Spread Function\n(PSF). Additionally, the image acquisition process is also contaminated by\nother sources of noise (read-out, photon-counting). The problem of estimating\nboth the PSF and a denoised image is called blind deconvolution and is\nill-posed.\n  Aims: We propose a blind deconvolution scheme that relies on image\nregularization. Contrarily to most methods presented in the literature, our\nmethod does not assume a parametric model of the PSF and can thus be applied to\nany telescope.\n  Methods: Our scheme uses a wavelet analysis prior model on the image and weak\nassumptions on the PSF. We use observations from a celestial transit, where the\nocculting body can be assumed to be a black disk. These constraints allow us to\nretain meaningful solutions for the filter and the image, eliminating trivial,\ntranslated and interchanged solutions. Under an additive Gaussian noise\nassumption, they also enforce noise canceling and avoid reconstruction\nartifacts by promoting the whiteness of the residual between the blurred\nobservations and the cleaned data.\n  Results: Our method is applied to synthetic and experimental data. The PSF is\nestimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and for\nSDO/AIA using the 2012 Venus transit. Results show that the proposed\nnon-parametric blind deconvolution method is able to estimate the core of the\nPSF with a similar quality to parametric methods proposed in the literature. We\nalso show that, if these parametric estimations are incorporated in the\nacquisition model, the resulting PSF outperforms both the parametric and\nnon-parametric methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 10:38:18 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 20:03:21 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 16:34:38 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Gonzalez", "Adriana", ""], ["Delouille", "V\u00e9ronique", ""], ["Jacques", "Laurent", ""]]}, {"id": "1412.6296", "submitter": "Jifeng Dai", "authors": "Jifeng Dai, Yang Lu, Ying-Nian Wu", "title": "Generative Modeling of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural networks (CNNs) have proven to be a powerful tool\nfor discriminative learning. Recently researchers have also started to show\ninterest in the generative aspects of CNNs in order to gain a deeper\nunderstanding of what they have learned and how to further improve them. This\npaper investigates generative modeling of CNNs. The main contributions include:\n(1) We construct a generative model for the CNN in the form of exponential\ntilting of a reference distribution. (2) We propose a generative gradient for\npre-training CNNs by a non-parametric importance sampling scheme, which is\nfundamentally different from the commonly used discriminative gradient, and yet\nhas the same computational architecture and cost as the latter. (3) We propose\na generative visualization method for the CNNs by sampling from an explicit\nparametric image distribution. The proposed visualization method can directly\ndraw synthetic samples for any given node in a trained CNN by the Hamiltonian\nMonte Carlo (HMC) algorithm, without resorting to any extra hold-out images.\nExperiments on the challenging ImageNet benchmark show that the proposed\ngenerative gradient pre-training consistently helps improve the performances of\nCNNs, and the proposed generative visualization method generates meaningful and\nvaried samples of synthetic images from a large-scale deep CNN.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:34:37 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 15:07:06 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Dai", "Jifeng", ""], ["Lu", "Yang", ""], ["Wu", "Ying-Nian", ""]]}, {"id": "1412.6391", "submitter": "Pierre de Buyl", "authors": "Davide Monari, Francesco Cenni, Erwin Aertbeli\\\"en, Kaat Desloovere", "title": "Py3DFreeHandUS: a library for voxel-array reconstruction using\n  Ultrasonography and attitude sensors", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-07", "categories": "cs.CV cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In medical imaging, there is a growing interest to provide real-time images\nwith good quality for large anatomical structures. To cope with this issue, we\ndeveloped a library that allows to replace, for some specific clinical\napplications, more robust systems such as Computer Tomography (CT) and Magnetic\nResonance Imaging (MRI). Our python library Py3DFreeHandUS is a package for\nprocessing data acquired simultaneously by ultra-sonographic systems (US) and\nmarker-based optoelectronic systems. In particular, US data enables to\nvisualize subcutaneous body structures, whereas the optoelectronic system is\nable to collect the 3D position in space for reflective objects, that are\ncalled markers. By combining these two measurement devices, it is possible to\nreconstruct the real 3D morphology of body structures such as muscles, for\nrelevant clinical implications. In the present research work, the different\nsteps which allow to obtain a relevant 3D data set as well as the procedures\nfor calibrating the systems and for determining the quality of the\nreconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:47:47 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Monari", "Davide", ""], ["Cenni", "Francesco", ""], ["Aertbeli\u00ebn", "Erwin", ""], ["Desloovere", "Kaat", ""]]}, {"id": "1412.6464", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew\n  Marsza{\\l}ek, Dawid Po{\\l}ap and Marcin Wo\\'zniak", "title": "Simplified firefly algorithm for 2D image key-points search", "comments": "Published version on: 2014 IEEE Symposium on Computational\n  Intelligence for Human-like Intelligence", "journal-ref": "IEEE Symposium on Computational Intelligence for Human-like\n  Intelligence, pp. 118-125, 2014", "doi": "10.1109/CIHLI.2014.7013395", "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify an object, human eyes firstly search the field of view\nfor points or areas which have particular properties. These properties are used\nto recognise an image or an object. Then this process could be taken as a model\nto develop computer algorithms for images identification. This paper proposes\nthe idea of applying the simplified firefly algorithm to search for key-areas\nin 2D images. For a set of input test images the proposed version of firefly\nalgorithm has been examined. Research results are presented and discussed to\nshow the efficiency of this evolutionary computation method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:00:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""], ["Marsza\u0142ek", "Zbigniew", ""], ["Po\u0142ap", "Dawid", ""], ["Wo\u017aniak", "Marcin", ""]]}, {"id": "1412.6504", "submitter": "Katerina Fragkiadaki", "authors": "Katerina Fragkiadaki and Pablo Arbelaez and Panna Felsen and Jitendra\n  Malik", "title": "Learning to Segment Moving Objects in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We segment moving objects in videos by ranking spatio-temporal segment\nproposals according to \"moving objectness\": how likely they are to contain a\nmoving object. In each video frame, we compute segment proposals using multiple\nfigure-ground segmentations on per frame motion boundaries. We rank them with a\nMoving Objectness Detector trained on image and motion fields to detect moving\nobjects and discard over/under segmentations or background parts of the scene.\nWe extend the top ranked segments into spatio-temporal tubes using random\nwalkers on motion affinities of dense point trajectories. Our final tube\nranking consistently outperforms previous segmentation methods in the two\nlargest video segmentation benchmarks currently available, for any number of\nproposals. Further, our per frame moving object proposals increase the\ndetection rate up to 7\\% over previous state-of-the-art static proposal\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:01:16 GMT"}, {"version": "v2", "created": "Fri, 8 May 2015 02:05:52 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Fragkiadaki", "Katerina", ""], ["Arbelaez", "Pablo", ""], ["Felsen", "Panna", ""], ["Malik", "Jitendra", ""]]}, {"id": "1412.6505", "submitter": "Michael S. Ryoo", "authors": "M. S. Ryoo, Brandon Rothrock, Larry Matthies", "title": "Pooled Motion Features for First-Person Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new feature representation for first-person\nvideos. In first-person video understanding (e.g., activity recognition), it is\nvery important to capture both entire scene dynamics (i.e., egomotion) and\nsalient local motion observed in videos. We describe a representation framework\nbased on time series pooling, which is designed to abstract\nshort-term/long-term changes in feature descriptor elements. The idea is to\nkeep track of how descriptor values are changing over time and summarize them\nto represent motion in the activity video. The framework is general, handling\nany types of per-frame feature descriptors including conventional motion\ndescriptors like histogram of optical flows (HOF) as well as appearance\ndescriptors from more recent convolutional neural networks (CNN). We\nexperimentally confirm that our approach clearly outperforms previous feature\nrepresentations including bag-of-visual-words and improved Fisher vector (IFV)\nwhen using identical underlying feature descriptors. We also confirm that our\nfeature representation has superior performance to existing state-of-the-art\nfeatures like local spatio-temporal features and Improved Trajectory Features\n(originally developed for 3rd-person videos) when handling first-person videos.\nMultiple first-person activity datasets were tested under various settings to\nconfirm these findings.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:03:00 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 19:16:08 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Ryoo", "M. S.", ""], ["Rothrock", "Brandon", ""], ["Matthies", "Larry", ""]]}, {"id": "1412.6537", "submitter": "Eduard Trulls", "authors": "Edgar Simo-Serra and Eduard Trulls and Luis Ferraz and Iasonas\n  Kokkinos and Francesc Moreno-Noguer", "title": "Fracking Deep Convolutional Image Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel framework for learning local image\ndescriptors in a discriminative manner. For this purpose we explore a siamese\narchitecture of Deep Convolutional Neural Networks (CNN), with a Hinge\nembedding loss on the L2 distance between descriptors. Since a siamese\narchitecture uses pairs rather than single image patches to train, there exist\na large number of positive samples and an exponential number of negative\nsamples. We propose to explore this space with a stochastic sampling of the\ntraining set, in combination with an aggressive mining strategy over both the\npositive and negative samples which we denote as \"fracking\". We perform a\nthorough evaluation of the architecture hyper-parameters, and demonstrate large\nperformance gains compared to both standard CNN learning strategies,\nhand-crafted image descriptors like SIFT, and the state-of-the-art on learned\ndescriptors: up to 2.5x vs SIFT and 1.5x vs the state-of-the-art in terms of\nthe area under the curve (AUC) of the Precision-Recall curve.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:30:32 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 21:30:16 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Simo-Serra", "Edgar", ""], ["Trulls", "Eduard", ""], ["Ferraz", "Luis", ""], ["Kokkinos", "Iasonas", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1412.6553", "submitter": "Vadim Lebedev", "authors": "Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor\n  Lempitsky", "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned\n  CP-Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple two-step approach for speeding up convolution layers\nwithin large convolutional neural networks based on tensor decomposition and\ndiscriminative fine-tuning. Given a layer, we use non-linear least squares to\ncompute a low-rank CP-decomposition of the 4D convolution kernel tensor into a\nsum of a small number of rank-one tensors. At the second step, this\ndecomposition is used to replace the original convolutional layer with a\nsequence of four convolutional layers with small kernels. After such\nreplacement, the entire network is fine-tuned on the training data using\nstandard backpropagation process.\n  We evaluate this approach on two CNNs and show that it is competitive with\nprevious approaches, leading to higher obtained CPU speedups at the cost of\nlower accuracy drops for the smaller of the two networks. Thus, for the\n36-class character classification CNN, our approach obtains a 8.5x CPU speedup\nof the whole network with only minor accuracy drop (1% from 91% to 90%). For\nthe standard ImageNet architecture (AlexNet), the approach speeds up the second\nconvolution layer by a factor of 4x at the cost of $1\\%$ increase of the\noverall top-5 classification error.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 23:02:43 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 21:57:37 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 11:40:54 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Lebedev", "Vadim", ""], ["Ganin", "Yaroslav", ""], ["Rakhuba", "Maksim", ""], ["Oseledets", "Ivan", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1412.6563", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov", "title": "Self-informed neural network structure learning", "comments": "Updated with accepted workshop contribution header", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of large scale, multi-label visual recognition with a\nlarge number of possible classes. We propose a method for augmenting a trained\nneural network classifier with auxiliary capacity in a manner designed to\nsignificantly improve upon an already well-performing model, while minimally\nimpacting its computational footprint. Using the predictions of the network\nitself as a descriptor for assessing visual similarity, we define a\npartitioning of the label space into groups of visually similar entities. We\nthen augment the network with auxilliary hidden layer pathways with\nconnectivity only to these groups of label units. We report a significant\nimprovement in mean average precision on a large-scale object recognition task\nwith the augmented model, while increasing the number of multiply-adds by less\nthan 3%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 00:05:57 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 21:35:29 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Warde-Farley", "David", ""], ["Rabinovich", "Andrew", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1412.6574", "submitter": "Ali Sharif Razavian", "authors": "Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson, Atsuto Maki", "title": "Visual Instance Retrieval with Deep Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an extensive study on the availability of image\nrepresentations based on convolutional networks (ConvNets) for the task of\nvisual instance retrieval. Besides the choice of convolutional layers, we\npresent an efficient pipeline exploiting multi-scale schemes to extract local\nfeatures, in particular, by taking geometric invariance into explicit account,\ni.e. positions, scales and spatial consistency. In our experiments using five\nstandard image retrieval datasets, we demonstrate that generic ConvNet image\nrepresentations can outperform other state-of-the-art methods if they are\nextracted appropriately.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:32:43 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 19:09:15 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 18:20:51 GMT"}, {"version": "v4", "created": "Mon, 9 May 2016 08:54:31 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Razavian", "Ali Sharif", ""], ["Sullivan", "Josephine", ""], ["Carlsson", "Stefan", ""], ["Maki", "Atsuto", ""]]}, {"id": "1412.6583", "submitter": "Brian Cheung", "authors": "Brian Cheung, Jesse A. Livezey, Arjun K. Bansal, Bruno A. Olshausen", "title": "Discovering Hidden Factors of Variation in Deep Networks", "comments": "Presented at International Conference on Learning Representations\n  2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enjoyed a great deal of success because of its ability to\nlearn useful features for tasks such as classification. But there has been less\nexploration in learning the factors of variation apart from the classification\nsignal. By augmenting autoencoders with simple regularization terms during\ntraining, we demonstrate that standard deep architectures can discover and\nexplicitly represent factors of variation beyond those relevant for\ncategorization. We introduce a cross-covariance penalty (XCov) as a method to\ndisentangle factors like handwriting style for digits and subject identity in\nfaces. We demonstrate this on the MNIST handwritten digit database, the Toronto\nFaces Database (TFD) and the Multi-PIE dataset by generating manipulated\ninstances of the data. Furthermore, we demonstrate these deep networks can\nextrapolate `hidden' variation in the supervised signal.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 02:52:03 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 20:41:40 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 17:15:02 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 06:47:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Cheung", "Brian", ""], ["Livezey", "Jesse A.", ""], ["Bansal", "Arjun K.", ""], ["Olshausen", "Bruno A.", ""]]}, {"id": "1412.6596", "submitter": "Scott Reed", "authors": "Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru\n  Erhan, Andrew Rabinovich", "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art deep learning systems for visual object recognition\nand detection use purely supervised training with regularization such as\ndropout to avoid overfitting. The performance depends critically on the amount\nof labeled examples, and in current practice the labels are assumed to be\nunambiguous and accurate. However, this assumption often does not hold; e.g. in\nrecognition, class labels may be missing; in detection, objects in the image\nmay not be localized; and in general, the labeling may be subjective. In this\nwork we propose a generic way to handle noisy and incomplete labeling by\naugmenting the prediction objective with a notion of consistency. We consider a\nprediction consistent if the same prediction is made given similar percepts,\nwhere the notion of similarity is between deep network features computed from\nthe input data. In experiments we demonstrate that our approach yields\nsubstantial robustness to label noise on several datasets. On MNIST handwritten\ndigits, we show that our model is robust to label corruption. On the Toronto\nFace Database, we show that our model handles well the case of subjective\nlabels in emotion recognition, achieving state-of-the- art results, and can\nalso benefit from unlabeled face images with no modification to our method. On\nthe ILSVRC2014 detection challenge data, we show that our approach extends to\nvery deep networks, high resolution images and structured outputs, and results\nin improved scalable detection.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:11:33 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 22:30:39 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 19:48:37 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Reed", "Scott", ""], ["Lee", "Honglak", ""], ["Anguelov", "Dragomir", ""], ["Szegedy", "Christian", ""], ["Erhan", "Dumitru", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1412.6597", "submitter": "Tom Paine", "authors": "Tom Le Paine, Pooya Khorrami, Wei Han, Thomas S. Huang", "title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances", "comments": "Accepted as a workshop contribution to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks perform well on object recognition because of a\nnumber of recent advances: rectified linear units (ReLUs), data augmentation,\ndropout, and large labelled datasets. Unsupervised data has been proposed as\nanother way to improve performance. Unfortunately, unsupervised pre-training is\nnot used by state-of-the-art methods leading to the following question: Is\nunsupervised pre-training still useful given recent advances? If so, when? We\nanswer this in three parts: we 1) develop an unsupervised method that\nincorporates ReLUs and recent unsupervised regularization techniques, 2)\nanalyze the benefits of unsupervised pre-training compared to data augmentation\nand dropout on CIFAR-10 while varying the ratio of unsupervised to supervised\nsamples, 3) verify our findings on STL-10. We discover unsupervised\npre-training, as expected, helps when the ratio of unsupervised to supervised\nsamples is high, and surprisingly, hurts when the ratio is low. We also use\nunsupervised pre-training with additional color augmentation to achieve near\nstate-of-the-art performance on STL-10.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:20:55 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 22:03:40 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 21:05:34 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:26:31 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Paine", "Tom Le", ""], ["Khorrami", "Pooya", ""], ["Han", "Wei", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1412.6598", "submitter": "Sobhan Naderi Parizi", "authors": "Sobhan Naderi Parizi, Andrea Vedaldi, Andrew Zisserman and Pedro\n  Felzenszwalb", "title": "Automatic Discovery and Optimization of Parts for Image Classification", "comments": "19 pages, template changed to camera ready version, 1 reference\n  added, 1 reference fixed, Fig. 3, 4 updated (larger text)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based representations have been shown to be very useful for image\nclassification. Learning part-based models is often viewed as a two-stage\nproblem. First, a collection of informative parts is discovered, using\nheuristics that promote part distinctiveness and diversity, and then\nclassifiers are trained on the vector of part responses. In this paper we unify\nthe two stages and learn the image classifiers and a set of shared parts\njointly. We generate an initial pool of parts by randomly sampling part\ncandidates and selecting a good subset using L1/L2 regularization. All steps\nare driven \"directly\" by the same objective namely the classification loss on a\ntraining set. This lets us do away with engineered heuristics. We also\nintroduce the notion of \"negative parts\", intended as parts that are negatively\ncorrelated with one or more classes. Negative parts are complementary to the\nparts discovered by other methods, which look only for positive correlations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:25:34 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 20:13:40 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Parizi", "Sobhan Naderi", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""], ["Felzenszwalb", "Pedro", ""]]}, {"id": "1412.6604", "submitter": "Marc'Aurelio Ranzato", "authors": "MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan\n  Collobert, Sumit Chopra", "title": "Video (language) modeling: a baseline for generative models of natural\n  videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a strong baseline model for unsupervised feature learning using\nvideo data. By learning to predict missing frames or extrapolate future frames\nfrom an input video sequence, the model discovers both spatial and temporal\ncorrelations which are useful to represent complex deformations and motion\npatterns. The models we propose are largely borrowed from the language modeling\nliterature, and adapted to the vision domain by quantizing the space of image\npatches into a large dictionary. We demonstrate the approach on both a filling\nand a generation task. For the first time, we show that, after training on\nnatural videos, such a model can predict non-trivial motions over short video\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:05:51 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 01:49:29 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 15:03:04 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2015 03:39:55 GMT"}, {"version": "v5", "created": "Wed, 4 May 2016 14:01:42 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Ranzato", "MarcAurelio", ""], ["Szlam", "Arthur", ""], ["Bruna", "Joan", ""], ["Mathieu", "Michael", ""], ["Collobert", "Ronan", ""], ["Chopra", "Sumit", ""]]}, {"id": "1412.6607", "submitter": "Stefano Soatto", "authors": "Stefano Soatto, Jingming Dong, Nikolaos Karianakis", "title": "Visual Scene Representations: Contrast, Scaling and Occlusion", "comments": "UCLA Tech Report CSD140023, Nov. 12, 2014. Updated April 13, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the structure of representations, defined as approximations of\nminimal sufficient statistics that are maximal invariants to nuisance factors,\nfor visual data subject to scaling and occlusion of line-of-sight. We derive\nanalytical expressions for such representations and show that, under certain\nrestrictive assumptions, they are related to features commonly in use in the\ncomputer vision community. This link highlights the condition tacitly assumed\nby these descriptors, and also suggests ways to improve and generalize them.\nThis new interpretation draws connections to the classical theories of\nsampling, hypothesis testing and group invariance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:15:42 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 18:36:58 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 22:34:56 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 02:36:33 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2015 18:59:02 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Soatto", "Stefano", ""], ["Dong", "Jingming", ""], ["Karianakis", "Nikolaos", ""]]}, {"id": "1412.6614", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "In Search of the Real Inductive Bias: On the Role of Implicit\n  Regularization in Deep Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present experiments demonstrating that some other form of capacity\ncontrol, different from network size, plays a central role in learning\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\nfactorization, that this is an inductive bias that can help shed light on deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:52:25 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 21:00:09 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 18:51:37 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 18:48:31 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1412.6618", "submitter": "Martin Kiefel", "authors": "Martin Kiefel, Varun Jampani and Peter V. Gehler", "title": "Permutohedral Lattice CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convolutional layer that is able to process sparse\ninput features. As an example, for image recognition problems this allows an\nefficient filtering of signals that do not lie on a dense grid (like pixel\nposition), but of more general features (such as color values). The presented\nalgorithm makes use of the permutohedral lattice data structure. The\npermutohedral lattice was introduced to efficiently implement a bilateral\nfilter, a commonly used image processing operation. Its use allows for a\ngeneralization of the convolution type found in current (spatial) convolutional\nnetwork architectures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:08:54 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 14:16:58 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 11:26:34 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Kiefel", "Martin", ""], ["Jampani", "Varun", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1412.6622", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Nir Ailon", "title": "Deep metric learning using Triplet network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proven itself as a successful set of models for learning\nuseful semantic representations of data. These, however, are mostly implicitly\nlearned as part of a classification task. In this paper we propose the triplet\nnetwork model, which aims to learn useful representations by distance\ncomparisons. A similar model was defined by Wang et al. (2014), tailor made for\nlearning a ranking for image information retrieval. Here we demonstrate using\nvarious datasets that our model learns a better representation than that of its\nimmediate competitor, the Siamese network. We also discuss future possible\nusage as a framework for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:34:50 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 17:28:52 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 09:00:25 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 15:35:35 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hoffer", "Elad", ""], ["Ailon", "Nir", ""]]}, {"id": "1412.6626", "submitter": "Olivier  H\\'enaff", "authors": "Olivier J. H\\'enaff, Johannes Ball\\'e, Neil C. Rabinowitz and Eero P.\n  Simoncelli", "title": "The local low-dimensionality of natural images", "comments": "Published as conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new statistical model for photographic images, in which the\nlocal responses of a bank of linear filters are described as jointly Gaussian,\nwith zero mean and a covariance that varies slowly over spatial position. We\noptimize sets of filters so as to minimize the nuclear norms of matrices of\ntheir local activations (i.e., the sum of the singular values), thus\nencouraging a flexible form of sparsity that is not tied to any particular\ndictionary or coordinate system. Filters optimized according to this objective\nare oriented and bandpass, and their responses exhibit substantial local\ncorrelation. We show that images can be reconstructed nearly perfectly from\nestimates of the local filter response covariances alone, and with minimal\ndegradation (either visual or MSE) from low-rank approximations of these\ncovariances. As such, this representation holds much promise for use in\napplications such as denoising, compression, and texture representation, and\nmay form a useful substrate for hierarchical decompositions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:52:16 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 23:52:02 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 01:11:01 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2015 01:28:48 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["H\u00e9naff", "Olivier J.", ""], ["Ball\u00e9", "Johannes", ""], ["Rabinowitz", "Neil C.", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1412.6631", "submitter": "Wei Yu", "authors": "Wei Yu, Kuiyuan Yang, Yalong Bai, Hongxun Yao, Yong Rui", "title": "Visualizing and Comparing Convolutional Neural Networks", "comments": "9 pages and 7 figures, submit to ICLR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved comparable error rates to\nwell-trained human on ILSVRC2014 image classification task. To achieve better\nperformance, the complexity of CNNs is continually increasing with deeper and\nbigger architectures. Though CNNs achieved promising external classification\nbehavior, understanding of their internal work mechanism is still limited. In\nthis work, we attempt to understand the internal work mechanism of CNNs by\nprobing the internal representations in two comprehensive aspects, i.e.,\nvisualizing patches in the representation spaces constructed by different\nlayers, and visualizing visual information kept in each layer. We further\ncompare CNNs with different depths and show the advantages brought by deeper\narchitecture.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 08:07:32 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 10:43:23 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Yu", "Wei", ""], ["Yang", "Kuiyuan", ""], ["Bai", "Yalong", ""], ["Yao", "Hongxun", ""], ["Rui", "Yong", ""]]}, {"id": "1412.6632", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille", "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "comments": "Add a simple strategy to boost the performance of image captioning\n  task significantly. More details are shown in Section 8 of the paper. The\n  code and related data are available at https://github.com/mjhucla/mRNN-CR ;.\n  arXiv admin note: substantial text overlap with arXiv:1410.1090", "journal-ref": "ICLR 2015", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability\ndistribution of generating a word given previous words and an image. Image\ncaptions are generated by sampling from this distribution. The model consists\nof two sub-networks: a deep recurrent neural network for sentences and a deep\nconvolutional network for images. These two sub-networks interact with each\nother in a multimodal layer to form the whole m-RNN model. The effectiveness of\nour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,\nFlickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In\naddition, we apply the m-RNN model to retrieval tasks for retrieving images or\nsentences, and achieves significant performance improvement over the\nstate-of-the-art methods which directly optimize the ranking objective function\nfor retrieval. The project page of this work is:\nwww.stat.ucla.edu/~junhua.mao/m-RNN.html .\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 08:10:04 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 08:24:11 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 04:17:48 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:03:35 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2015 15:26:58 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yuille", "Alan", ""]]}, {"id": "1412.6749", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction", "comments": "Eighteen pages in all, but much of the central ideas are covered in\n  the first five and a half pages; most of the remaining pages are devoted to\n  straightforward mathematical derivations, and the presentation of three\n  algorithms. Manuscript contains no figures at this time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By drawing on ideas from optimisation theory, artificial neural networks\n(ANN), graph embeddings and sparse representations, I develop a novel\ntechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at\naddressing the feature extraction problem. The proposed method uses (preferably\ndeep) ANNs for projecting input attribute vectors to an output space wherein\npairwise distances are maximized for vectors belonging to different classes,\nbut minimized for those belonging to the same class, while simultaneously\nenforcing sparsity on the ANN outputs. The vectors that result from the\nprojection can then be used as features in any classifier of choice.\nMathematically, I formulate the proposed method as the minimisation of an\nobjective function which can be interpreted, in the ANN output space, as a\nnegative factor of the sum of the squares of the pair-wise distances between\noutput vectors belonging to different classes, added to a positive factor of\nthe sum of squares of the pair-wise distances between output vectors belonging\nto the same classes, plus sparsity and weight decay terms. To derive an\nalgorithm for minimizing the objective function via gradient descent, I use the\nmulti-variate version of the chain rule to obtain the partial derivatives of\nthe function with respect to ANN weights and biases, and find that each of the\nrequired partial derivatives can be expressed as a sum of six terms. As it\nturns out, four of those six terms can be computed using the standard back\npropagation algorithm; the fifth can be computed via a slight modification of\nthe standard backpropagation algorithm; while the sixth one can be computed via\nsimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora\nof digits and letters, the CMU PIE database of faces, the MNIST digits\ndatabase, and other standard machine learning databases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 09:28:05 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6759", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "Bi-directional Shape Correspondences (BSC): A Novel Technique for 2-d\n  Shape Warping in Quadratic Time?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Bidirectional Shape Correspondence (BSC) as a possible improvement\non the famous shape contexts (SC) framework. Our proposals derive from the\nobservation that the SC framework enforces a one-to-one correspondence between\nsample points, and that this leads to two possible drawbacks. First, this\ndenies the framework the opportunity to effect advantageous many-to-many\nmatching between points on the two shapes being compared. Second, this calls\nfor the Hungarian algorithm which unfortunately usurps cubic time. While the\ndynamic-space-warping dynamic programming algorithm has provided a standard\nsolution to the first problem above, it demands quintic time for general\nmulti-contour shapes, and w times quadratic time for the special case of\nsingle-contour shapes, even after an heuristic search window of width w has\nbeen chosen. Therefore, in this work, we propose a simple method for computing\n\"many-to-many\" correspondences for the class of all 2-d shapes in quadratic\ntime. Our approach is to explicitly let each point on the first shape choose a\nbest match on the second shape, and vice versa. Along the way, we also propose\nthe use of data-clustering techniques for dealing with the outliers problem,\nand, from another viewpoint, it turns out that this clustering can be seen as\nan autonomous, rather than pre-computed, sampling of shape boundary.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 10:28:36 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6791", "submitter": "Anoop Katti", "authors": "Anoop Katti, Anurag Mittal", "title": "Mixture of Parts Revisited: Expressive Part Interactions for Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based models with restrictive tree-structured interactions for the Human\nPose Estimation problem, leaves many part interactions unhandled. Two of the\nmost common and strong manifestations of such unhandled interactions are\nself-occlusion among the parts and the confusion in the localization of the\nnon-adjacent symmetric parts. By handling the self-occlusion in a data\nefficient manner, we improve the performance of the basic Mixture of Parts\nmodel by a large margin, especially on uncommon poses. Through addressing the\nconfusion in the symmetric limb localization using a combination of two\ncomplementing trees, we improve the performance on all the parts by atmost\ndoubling the running time. Finally, we show that the combination of the two\nsolutions improves the results. We report results that are equivalent to the\nstate-of-the-art on two standard datasets. Because of maintaining the\ntree-structured interactions and only part-level modeling of the base Mixture\nof Parts model, this is achieved in time that is much less than the best\nperforming part-based model.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 14:48:41 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Katti", "Anoop", ""], ["Mittal", "Anurag", ""]]}, {"id": "1412.6806", "submitter": "Alexey Dosovitskiy", "authors": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin\n  Riedmiller", "title": "Striving for Simplicity: The All Convolutional Net", "comments": "accepted to ICLR-2015 workshop track; no changes other than style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding -- and building on other recent work for\nfinding simple network structures -- we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n\"deconvolution approach\" for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 16:16:37 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 21:44:06 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 07:58:17 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1412.6808", "submitter": "Waheed Bajwa", "authors": "Tong Wu and Waheed U. Bajwa", "title": "Learning the nonlinear geometry of high-dimensional data: Models and\n  algorithms", "comments": "Extended version of the journal paper accepted for publication in\n  IEEE Trans. Signal Processing (20 pages, 7 figures, 4 tables)", "journal-ref": "IEEE Trans. Signal Processing, vol. 63, no. 23, pp. 6229-6244,\n  Dec. 2015", "doi": "10.1109/TSP.2015.2469637", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information processing relies on the axiom that high-dimensional data\nlie near low-dimensional geometric structures. This paper revisits the problem\nof data-driven learning of these geometric structures and puts forth two new\nnonlinear geometric models for data describing \"related\" objects/phenomena. The\nfirst one of these models straddles the two extremes of the subspace model and\nthe union-of-subspaces model, and is termed the metric-constrained\nunion-of-subspaces (MC-UoS) model. The second one of these models---suited for\ndata drawn from a mixture of nonlinear manifolds---generalizes the kernel\nsubspace model, and is termed the metric-constrained kernel union-of-subspaces\n(MC-KUoS) model. The main contributions of this paper in this regard include\nthe following. First, it motivates and formalizes the problems of MC-UoS and\nMC-KUoS learning. Second, it presents algorithms that efficiently learn an\nMC-UoS or an MC-KUoS underlying data of interest. Third, it extends these\nalgorithms to the case when parts of the data are missing. Last, but not least,\nit reports the outcomes of a series of numerical experiments involving both\nsynthetic and real data that demonstrate the superiority of the proposed\ngeometric models and learning algorithms over existing approaches in the\nliterature. These experiments also help clarify the connections between this\nwork and the literature on (subspace and kernel k-means) clustering.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 16:40:31 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 02:12:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wu", "Tong", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1412.6821", "submitter": "Roland Kwitt", "authors": "Jan Reininghaus, Stefan Huber, Ulrich Bauer, Roland Kwitt", "title": "A Stable Multi-Scale Kernel for Topological Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis offers a rich source of valuable information to\nstudy vision problems. Yet, so far we lack a theoretically sound connection to\npopular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In\nthis work, we establish such a connection by designing a multi-scale kernel for\npersistence diagrams, a stable summary representation of topological features\nin data. We show that this kernel is positive definite and prove its stability\nwith respect to the 1-Wasserstein distance. Experiments on two benchmark\ndatasets for 3D shape classification/retrieval and texture recognition show\nconsiderable performance gains of the proposed method compared to an\nalternative approach that is based on the recently introduced persistence\nlandscapes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 19:17:08 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Reininghaus", "Jan", ""], ["Huber", "Stefan", ""], ["Bauer", "Ulrich", ""], ["Kwitt", "Roland", ""]]}, {"id": "1412.6830", "submitter": "Forest Agostinelli", "authors": "Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi", "title": "Learning Activation Functions to Improve Deep Neural Networks", "comments": "Accepted as a workshop paper contribution at the International\n  Conference on Learning Representations (ICLR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 20:20:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 21:44:41 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 08:05:02 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Agostinelli", "Forest", ""], ["Hoffman", "Matthew", ""], ["Sadowski", "Peter", ""], ["Baldi", "Pierre", ""]]}, {"id": "1412.6847", "submitter": "Ziqiang Chen", "authors": "Feng Lu, Ziqiang Chen", "title": "A New Way to Factorize Linear Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation details of factorizing the 3x4 projection matrices of\nlinear cameras into their left matrix factors and the 4x4 homogeneous\ncentral(also parallel for infinite center cases) projection factors are\npresented in this work. Any full row rank 3x4 real matrix can be factorized\ninto such basic matrices which will be called LC factors.\n  A further extension to multiple view midpoint triangulation, for both pinhole\nand affine camera cases, is also presented based on such camera factorizations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 00:10:11 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Lu", "Feng", ""], ["Chen", "Ziqiang", ""]]}, {"id": "1412.6856", "submitter": "Bolei Zhou", "authors": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio\n  Torralba", "title": "Object Detectors Emerge in Deep Scene CNNs", "comments": "12 pages, ICLR 2015 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of new computational architectures for visual processing,\nsuch as convolutional neural networks (CNN) and access to image databases with\nmillions of labeled examples (e.g., ImageNet, Places), the state of the art in\ncomputer vision is advancing rapidly. One important factor for continued\nprogress is to understand the representations that are learned by the inner\nlayers of these deep architectures. Here we show that object detectors emerge\nfrom training CNNs to perform scene classification. As scenes are composed of\nobjects, the CNN for scene classification automatically discovers meaningful\nobjects detectors, representative of the learned scene categories. With object\ndetectors emerging as a result of learning to recognize scenes, our work\ndemonstrates that the same network can perform both scene recognition and\nobject localization in a single forward-pass, without ever having been\nexplicitly taught the notion of objects.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 01:14:01 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 19:06:41 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Zhou", "Bolei", ""], ["Khosla", "Aditya", ""], ["Lapedriza", "Agata", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1412.6857", "submitter": "Tyng-Luh Liu", "authors": "Jyh-Jing Hwang and Tyng-Luh Liu", "title": "Contour Detection Using Cost-Sensitive Convolutional Neural Networks", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. The main challenge lies\nin adapting a pre-trained per-image CNN model for yielding per-pixel image\nfeatures. We propose to base on the DenseNet architecture to achieve pixelwise\nfine-tuning and then consider a cost-sensitive strategy to further improve the\nlearning with a small dataset of edge and non-edge image patches. In the\nexperiment of contour detection, we look into the effectiveness of combining\nper-pixel features from different CNN layers and obtain comparable performances\nto the state-of-the-art on BSDS500.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 01:16:50 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 14:37:27 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 15:01:16 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 07:37:54 GMT"}, {"version": "v5", "created": "Tue, 12 May 2015 08:42:42 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1412.6885", "submitter": "Jun Yuan", "authors": "Jun Yuan, Bingbing Ni, Ashraf A.Kassim", "title": "Half-CNN: A General Framework for Whole-Image Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Network (CNN) has achieved great success in image\nclassification. The classification model can also be utilized at image or patch\nlevel for many other applications, such as object detection and segmentation.\nIn this paper, we propose a whole-image CNN regression model, by removing the\nfull connection layer and training the network with continuous feature maps.\nThis is a generic regression framework that fits many applications. We\ndemonstrate this method through two tasks: simultaneous face detection &\nsegmentation, and scene saliency prediction. The result is comparable with\nother models in the respective fields, using only a small scale network. Since\nthe regression model is trained on corresponding image / feature map pairs,\nthere are no requirements on uniform input size as opposed to the\nclassification model. Our framework avoids classifier design, a process that\nmay introduce too much manual intervention in model development. Yet, it is\nhighly correlated to the classification network and offers some in-deep review\nof CNN structures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 06:43:58 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Yuan", "Jun", ""], ["Ni", "Bingbing", ""], ["Kassim", "Ashraf A.", ""]]}, {"id": "1412.7006", "submitter": "Vivek Venugopalan", "authors": "Michael Giering, Vivek Venugopalan, Kishore Reddy", "title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural\n  Networks", "comments": "7 pages, double column, IEEE format, accepted at IEEE HPEC 2015", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322485", "report-no": "1214_v3", "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to simultaneously leverage multiple modes of sensor information\nis critical for perception of an automated vehicle's physical surroundings.\nSpatio-temporal alignment of registration of the incoming information is often\na prerequisite to analyzing the fused data. The persistence and reliability of\nmulti-modal registration is therefore the key to the stability of decision\nsupport systems ingesting the fused information. LiDAR-video systems like on\nthose many driverless cars are a common example of where keeping the LiDAR and\nvideo channels registered to common physical features is important. We develop\na deep learning method that takes multiple channels of heterogeneous data, to\ndetect the misalignment of the LiDAR-video inputs. A number of variations were\ntested on the Ford LiDAR-video driving test data set and will be discussed. To\nthe best of our knowledge the use of multi-modal deep convolutional neural\nnetworks for dynamic real-time LiDAR-video registration has not been presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:54:53 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 01:14:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Giering", "Michael", ""], ["Venugopalan", "Vivek", ""], ["Reddy", "Kishore", ""]]}, {"id": "1412.7007", "submitter": "Vivek Venugopalan", "authors": "Soumik Sarkar, Vivek Venugopalan, Kishore Reddy, Michael Giering,\n  Julian Ryde, Navdeep Jaitly", "title": "Occlusion Edge Detection in RGB-D Frames using Deep Convolutional\n  Networks", "comments": "7 pages, double column, IEEE HPEC 2015 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion edges in images which correspond to range discontinuity in the\nscene from the point of view of the observer are an important prerequisite for\nmany vision and mobile robot tasks. Although they can be extracted from range\ndata however extracting them from images and videos would be extremely\nbeneficial. We trained a deep convolutional neural network (CNN) to identify\nocclusion edges in images and videos with both RGB-D and RGB inputs. The use of\nCNN avoids hand-crafting of features for automatically isolating occlusion\nedges and distinguishing them from appearance edges. Other than quantitative\nocclusion edge detection results, qualitative results are provided to\ndemonstrate the trade-off between high resolution analysis and frame-level\ncomputation time which is critical for real-time robotics applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:55:17 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 12:50:48 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 01:07:23 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Sarkar", "Soumik", ""], ["Venugopalan", "Vivek", ""], ["Reddy", "Kishore", ""], ["Giering", "Michael", ""], ["Ryde", "Julian", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1412.7012", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi, Hirokazu Koma, and Muneki Yasuda", "title": "Boltzmann-Machine Learning of Prior Distributions of Binarized Natural\n  Images", "comments": "32 pages, 33 figures", "journal-ref": "J. Phys. Soc. Jpn. 85 (2016) 114803", "doi": "10.7566/JPSJ.85.114803", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior distributions of binarized natural images are learned by using a\nBoltzmann machine. According the results of this study, there emerges a\nstructure with two sublattices in the interactions, and the nearest-neighbor\nand next-nearest-neighbor interactions correspondingly take two discriminative\nvalues, which reflects the individual characteristics of the three sets of\npictures that we process. Meanwhile, in a longer spatial scale, a longer-range,\nalthough still rapidly decaying, ferromagnetic interaction commonly appears in\nall cases. The characteristic length scale of the interactions is universally\nup to approximately four lattice spacings $\\xi \\approx 4$. These results are\nderived by using the mean-field method, which effectively reduces the\ncomputational time required in a Boltzmann machine. An improved mean-field\nmethod called the Bethe approximation also gives the same results, as well as\nthe Monte Carlo method does for small size images. These reinforce the validity\nof our analysis and findings. Relations to criticality, frustration, and\nsimple-cell receptive fields are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 04:41:09 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 14:03:11 GMT"}, {"version": "v3", "created": "Fri, 7 Oct 2016 02:07:27 GMT"}, {"version": "v4", "created": "Mon, 24 Oct 2016 03:20:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Koma", "Hirokazu", ""], ["Yasuda", "Muneki", ""]]}, {"id": "1412.7024", "submitter": "Matthieu Courbariaux", "authors": "Matthieu Courbariaux, Yoshua Bengio and Jean-Pierre David", "title": "Training deep neural networks with low precision multiplications", "comments": "10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipliers are the most space and power-hungry arithmetic operators of the\ndigital implementation of deep neural networks. We train a set of\nstate-of-the-art neural networks (Maxout networks) on three benchmark datasets:\nMNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:\nfloating point, fixed point and dynamic fixed point. For each of those datasets\nand for each of those formats, we assess the impact of the precision of the\nmultiplications on the final error after training. We find that very low\nprecision is sufficient not just for running trained networks but also for\ntraining them. For example, it is possible to train Maxout networks with 10\nbits multiplications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:22:45 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 18:05:12 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 00:26:12 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 22:52:43 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 01:00:44 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Courbariaux", "Matthieu", ""], ["Bengio", "Yoshua", ""], ["David", "Jean-Pierre", ""]]}, {"id": "1412.7054", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Andrea Frome, Esteban Real", "title": "Attention for Fine-Grained Categorization", "comments": "ICLR 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents experiments extending the work of Ba et al. (2014) on\nrecurrent neural models for attention into less constrained visual\nenvironments, specifically fine-grained categorization on the Stanford Dogs\ndata set. In this work we use an RNN of the same structure but substitute a\nmore powerful visual network and perform large-scale pre-training of the visual\nnetwork outside of the attention RNN. Most work in attention models to date\nfocuses on tasks with toy or more constrained visual environments, whereas we\npresent results for fine-grained categorization better than the\nstate-of-the-art GoogLeNet classification model. We show that our model learns\nto direct high resolution attention to the most discriminative regions without\nany spatial supervision such as bounding boxes, and it is able to discriminate\nfine-grained dog breeds moderately well even when given only an initial\nlow-resolution context image and narrow, inexpensive glimpses at faces and fur\npatterns. This and similar attention models have the major advantage of being\ntrained end-to-end, as opposed to other current detection and recognition\npipelines with hand-engineered components where information is lost. While our\nmodel is state-of-the-art, further work is needed to fully leverage the\nsequential input.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:06:07 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 00:15:45 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 01:45:56 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Sermanet", "Pierre", ""], ["Frome", "Andrea", ""], ["Real", "Esteban", ""]]}, {"id": "1412.7056", "submitter": "Shuchin Aeron", "authors": "Eric Kernfeld and Shuchin Aeron and Misha Kilmer", "title": "Clustering multi-way data: a novel algebraic approach", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method for unsupervised clustering of two-way\n(matrix) data by combining two recent innovations from different fields: the\nSparse Subspace Clustering (SSC) algorithm [10], which groups points coming\nfrom a union of subspaces into their respective subspaces, and the t-product\n[18], which was introduced to provide a matrix-like multiplication for third\norder tensors. Our algorithm is analogous to SSC in that an \"affinity\" between\ndifferent data points is built using a sparse self-representation of the data.\nUnlike SSC, we employ the t-product in the self-representation. This allows us\nmore flexibility in modeling; infact, SSC is a special case of our method. When\nusing the t-product, three-way arrays are treated as matrices whose elements\n(scalars) are n-tuples or tubes. Convolutions take the place of scalar\nmultiplication. This framework allows us to embed the 2-D data into a\nvector-space-like structure called a free module over a commutative ring. These\nfree modules retain many properties of complex inner-product spaces, and we\nleverage that to provide theoretical guarantees on our algorithm. We show that\ncompared to vector-space counterparts, SSmC achieves higher accuracy and better\nable to cluster data with less preprocessing in some image clustering problems.\nIn particular we show the performance of the proposed method on Weizmann face\ndatabase, the Extended Yale B Face database and the MNIST handwritten digits\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:06:44 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 02:19:29 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Kernfeld", "Eric", ""], ["Aeron", "Shuchin", ""], ["Kilmer", "Misha", ""]]}, {"id": "1412.7062", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin\n  Murphy and Alan L. Yuille", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully\n  Connected CRFs", "comments": "14 pages. Updated related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\nart performance in high level vision tasks, such as image classification and\nobject detection. This work brings together methods from DCNNs and\nprobabilistic graphical models for addressing the task of pixel-level\nclassification (also called \"semantic image segmentation\"). We show that\nresponses at the final layer of DCNNs are not sufficiently localized for\naccurate object segmentation. This is due to the very invariance properties\nthat make DCNNs good for high level tasks. We overcome this poor localization\nproperty of deep networks by combining the responses at the final DCNN layer\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\n\"DeepLab\" system is able to localize segment boundaries at a level of accuracy\nwhich is beyond previous methods. Quantitatively, our method sets the new\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\n71.6% IOU accuracy in the test set. We show how these results can be obtained\nefficiently: Careful network re-purposing and a novel application of the 'hole'\nalgorithm from the wavelet community allow dense computation of neural net\nresponses at 8 frames per second on a modern GPU.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:18:33 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 18:21:29 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 17:14:50 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 04:00:08 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Papandreou", "George", ""], ["Kokkinos", "Iasonas", ""], ["Murphy", "Kevin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1412.7122", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko", "title": "Learning Deep Object Detectors from 3D Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced 3D CAD models are becoming easily accessible online, and can\npotentially generate an infinite number of training images for almost any\nobject category.We show that augmenting the training data of contemporary Deep\nConvolutional Neural Net (DCNN) models with such synthetic data can be\neffective, especially when real training data is limited or not well matched to\nthe target domain. Most freely available CAD models capture 3D shape but are\noften missing other low level cues, such as realistic object texture, pose, or\nbackground. In a detailed analysis, we use synthetic CAD-rendered images to\nprobe the ability of DCNN to learn without these cues, with surprising\nfindings. In particular, we show that when the DCNN is fine-tuned on the target\ndetection task, it exhibits a large degree of invariance to missing low-level\ncues, but, when pretrained on generic ImageNet classification, it learns better\nwhen the low-level cues are simulated. We show that our synthetic DCNN training\napproach significantly outperforms previous methods on the PASCAL VOC2007\ndataset when learning in the few-shot scenario and improves performance in a\ndomain shift scenario on the Office benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:10:31 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 23:44:24 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 17:56:07 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2015 01:01:39 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Peng", "Xingchao", ""], ["Sun", "Baochen", ""], ["Ali", "Karim", ""], ["Saenko", "Kate", ""]]}, {"id": "1412.7144", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Evan Shelhamer, Jonathan Long and Trevor Darrell", "title": "Fully Convolutional Multi-Class Multiple Instance Learning", "comments": "in ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) can reduce the need for costly annotation in\ntasks such as semantic segmentation by weakening the required degree of\nsupervision. We propose a novel MIL formulation of multi-class semantic\nsegmentation learning by a fully convolutional network. In this setting, we\nseek to learn a semantic segmentation model from just weak image-level labels.\nThe model is trained end-to-end to jointly optimize the representation while\ndisambiguating the pixel-image label assignment. Fully convolutional training\naccepts inputs of any size, does not need object proposal pre-processing, and\noffers a pixelwise loss map for selecting latent instances. Our multi-class MIL\nloss exploits the further supervision given by images with multiple labels. We\nevaluate this approach through preliminary experiments on the PASCAL VOC\nsegmentation challenge.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:49:54 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 01:17:59 GMT"}, {"version": "v3", "created": "Sat, 7 Feb 2015 02:12:26 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2015 05:31:10 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Shelhamer", "Evan", ""], ["Long", "Jonathan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.7155", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Lisa Anne Hendricks, Trevor Darrell", "title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "comments": "4 pages, 2 figures. Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, nested dropout was proposed as a method for ordering representation\nunits in autoencoders by their information content, without diminishing\nreconstruction cost. However, it has only been applied to training\nfully-connected autoencoders in an unsupervised setting. We explore the impact\nof nested dropout on the convolutional layers in a CNN trained by\nbackpropagation, investigating whether nested dropout can provide a simple and\nsystematic way to determine the optimal representation size with respect to the\ndesired accuracy and desired task and data complexity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:59:58 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 01:47:57 GMT"}, {"version": "v3", "created": "Sat, 28 Feb 2015 00:07:59 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 06:11:22 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Finn", "Chelsea", ""], ["Hendricks", "Lisa Anne", ""], ["Darrell", "Trevor", ""]]}, {"id": "1412.7190", "submitter": "Mathieu Aubry", "authors": "Francisco Massa, Mathieu Aubry, Renaud Marlet", "title": "Convolutional Neural Networks for joint object detection and pose\n  estimation: A comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the application of convolutional neural networks for\njointly detecting objects depicted in still images and estimating their 3D\npose. We identify different feature representations of oriented objects, and\nenergies that lead a network to learn this representations. The choice of the\nrepresentation is crucial since the pose of an object has a natural, continuous\nstructure while its category is a discrete variable. We evaluate the different\napproaches on the joint object detection and pose estimation task of the\nPascal3D+ benchmark using Average Viewpoint Precision. We show that a\nclassification approach on discretized viewpoints achieves state-of-the-art\nperformance for joint object detection and pose estimation, and significantly\noutperforms existing baselines on this benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:26:26 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 16:43:41 GMT"}, {"version": "v3", "created": "Sat, 7 Feb 2015 05:27:24 GMT"}, {"version": "v4", "created": "Sat, 28 Feb 2015 05:15:45 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Massa", "Francisco", ""], ["Aubry", "Mathieu", ""], ["Marlet", "Renaud", ""]]}, {"id": "1412.7210", "submitter": "Antti Rasmus", "authors": "Antti Rasmus, Tapani Raiko, Harri Valpola", "title": "Denoising autoencoder with modulated lateral connections learns\n  invariant representations of natural images", "comments": "Presentation at ICLR 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suitable lateral connections between encoder and decoder are shown to allow\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\nrepresentations. In regular autoencoders, detailed information needs to be\ncarried through the highest layers but lateral connections from encoder to\ndecoder relieve this pressure. It is shown that abstract invariant features can\nbe translated to detailed reconstructions when invariant features are allowed\nto modulate the strength of the lateral connection. Three dAE structures with\nmodulated and additive lateral connections, and without lateral connections\nwere compared in experiments using real-world images. The experiments verify\nthat adding modulated lateral connections to the model 1) improves the accuracy\nof the probability model for inputs, as measured by denoising performance; 2)\nresults in representations whose degree of invariance grows faster towards the\nhigher layers; and 3) supports the formation of diverse invariant poolings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:36:15 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 12:17:51 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 20:56:43 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2015 15:49:16 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Rasmus", "Antti", ""], ["Raiko", "Tapani", ""], ["Valpola", "Harri", ""]]}, {"id": "1412.7242", "submitter": "Chengyao Shen", "authors": "Chengyao Shen, Xun Huang and Qi Zhao", "title": "Learning of Proto-object Representations via Fixations on Low Resolution", "comments": "This paper has been withdrawn by the author due to incompletion of\n  the submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While previous researches in eye fixation prediction typically rely on\nintegrating low-level features (e.g. color, edge) to form a saliency map,\nrecently it has been found that the structural organization of these features\ninto a proto-object representation can play a more significant role. In this\nwork, we present a computational framework based on deep network to demonstrate\nthat proto-object representations can be learned from low-resolution image\npatches from fixation regions. We advocate the use of low-resolution inputs in\nthis work due to the following reasons: (1) Proto-objects are computed in\nparallel over an entire visual field (2) People can perceive or recognize\nobjects well even it is in low resolution. (3) Fixations from lower resolution\nimages can predict fixations on higher resolution images. In the proposed\ncomputational model, we extract multi-scale image patches on fixation regions\nfrom eye fixation datasets, resize them to low resolution and feed them into a\nhierarchical. With layer-wise unsupervised feature learning, we find that many\nproto-objects like features responsive to different shapes of object blobs are\nlearned out. Visualizations also show that these features are selective to\npotential objects in the scene and the responses of these features work well in\npredicting eye fixations on the images when combined with learned weights.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 03:14:21 GMT"}, {"version": "v2", "created": "Sat, 27 Dec 2014 08:29:00 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Shen", "Chengyao", ""], ["Huang", "Xun", ""], ["Zhao", "Qi", ""]]}, {"id": "1412.7259", "submitter": "Dong Wang", "authors": "Dong Wang and Xiaoyang Tan", "title": "Unsupervised Feature Learning with C-SVDDNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of learning feature representation\nfrom unlabeled data using a single-layer K-means network. A K-means network\nmaps the input data into a feature representation by finding the nearest\ncentroid for each input point, which has attracted researchers' great attention\nrecently due to its simplicity, effectiveness, and scalability. However, one\ndrawback of this feature mapping is that it tends to be unreliable when the\ntraining data contains noise. To address this issue, we propose a SVDD based\nfeature learning algorithm that describes the density and distribution of each\ncluster from K-means with an SVDD ball for more robust feature representation.\nFor this purpose, we present a new SVDD algorithm called C-SVDD that centers\nthe SVDD ball towards the mode of local density of each cluster, and we show\nthat the objective of C-SVDD can be solved very efficiently as a linear\nprogramming problem. Additionally, traditional unsupervised feature learning\nmethods usually take an average or sum of local representations to obtain\nglobal representation which ignore spatial relationship among them. To use\nspatial information we propose a global representation with a variant of SIFT\ndescriptor. The architecture is also extended with multiple receptive field\nscales and multiple pooling sizes. Extensive experiments on several popular\nobject recognition benchmarks, such as STL-10, MINST, Holiday and Copydays\nshows that the proposed C-SVDDNet method yields comparable or better\nperformance than that of the previous state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 05:56:50 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 05:31:03 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 09:50:54 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Wang", "Dong", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1412.7277", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey, Anand Singh Jalal", "title": "Fusing Color and Texture Cues to Categorize the Fruit Diseases from\n  Images", "comments": "arXiv admin note: substantial text overlap with arXiv:1405.4930", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The economic and production losses in agricultural industry worldwide are due\nto the presence of diseases in the several kinds of fruits. In this paper, a\nmethod for the classification of fruit diseases is proposed and experimentally\nvalidated. The image processing based proposed approach is composed of the\nfollowing main steps; in the first step K-Means clustering technique is used\nfor the defect segmentation, in the second step color and textural cues are\nextracted and fused from the segmented image, and finally images are classified\ninto one of the classes by using a Multi-class Support Vector Machine. We have\nconsidered diseases of apple as a test case and evaluated our approach for\nthree types of apple diseases namely apple scab, apple blotch and apple rot and\nnormal apples without diseases. Our experimentation points out that the\nproposed fusion scheme can significantly support accurate detection and\nautomatic classification of fruit diseases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 07:46:03 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Dubey", "Shiv Ram", ""], ["Jalal", "Anand Singh", ""]]}, {"id": "1412.7504", "submitter": "Henry O. Jacobs", "authors": "Henry O. Jacobs, Stefan Sommer", "title": "Higher-order Spatial Accuracy in Diffeomorphic Image Registration", "comments": "33 pages, pages 22-33 consist of an appendix where we list coordinate\n  formulas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discretize a cost functional for image registration problems by deriving\nTaylor expansions for the matching term. Minima of the discretized cost\nfunctionals can be computed with no spatial discretization error, and the\noptimal solutions are equivalent to minimal energy curves in the space of\n$k$-jets. We show that the solutions convergence to optimal solutions of the\noriginal cost functional as the number of particles increases with a\nconvergence rate of $O(h^{d+k})$ where $h$ is a resolution parameter. The\neffect of this approach over traditional particle methods is illustrated on\nsynthetic examples and real images.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 20:19:19 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 02:47:46 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Jacobs", "Henry O.", ""], ["Sommer", "Stefan", ""]]}, {"id": "1412.7513", "submitter": "Henry O. Jacobs", "authors": "Stefan Sommer, Henry O. Jacobs", "title": "Symmetry in Image Registration and Deformation Modeling", "comments": "23 pages, survey article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the role of symmetry in diffeomorphic registration of landmarks,\ncurves, surfaces, images and higher-order data. The infinite dimensional\nproblem of finding correspondences between objects can for a range of concrete\ndata types be reduced resulting in compact representations of shape and spatial\nstructure. This reduction is possible because the available data is incomplete\nin encoding the full deformation model. Using reduction by symmetry, we\ndescribe the reduced models in a common theoretical framework that draws on\nlinks between the registration problem and geometric mechanics. Symmetry also\narises in reduction to the Lie algebra using particle relabeling symmetry\nallowing the equations of motion to be written purely in terms of Eulerian\nvelocity field. Reduction by symmetry has recently been applied for\njet-matching and higher-order discrete approximations of the image matching\nproblem. We outline these constructions and further cases where reduction by\nsymmetry promises new approaches to registration of complex data types.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 20:44:02 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 10:25:24 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Sommer", "Stefan", ""], ["Jacobs", "Henry O.", ""]]}, {"id": "1412.7625", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "An Effective Semi-supervised Divisive Clustering Algorithm", "comments": "8 pages, 4 figures, a new (6th) member of the in-tree clustering\n  family", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Nowadays, data are generated massively and rapidly from scientific fields as\nbioinformatics, neuroscience and astronomy to business and engineering fields.\nCluster analysis, as one of the major data analysis tools, is therefore more\nsignificant than ever. We propose in this work an effective Semi-supervised\nDivisive Clustering algorithm (SDC). Data points are first organized by a\nminimal spanning tree. Next, this tree structure is transitioned to the in-tree\nstructure, and then divided into sub-trees under the supervision of the labeled\ndata, and in the end, all points in the sub-trees are directly associated with\nspecific cluster centers. SDC is fully automatic, non-iterative, involving no\nfree parameter, insensitive to noise, able to detect irregularly shaped cluster\nstructures, applicable to the data sets of high dimensionality and different\nattributes. The power of SDC is demonstrated on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 08:55:50 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 09:35:39 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1412.7626", "submitter": "Ibrahim Abdelaziz", "authors": "Ibrahim Abdelaziz, Sherif Abdou", "title": "AltecOnDB: A Large-Vocabulary Arabic Online Handwriting Recognition\n  Database", "comments": "The preprint is in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic is a semitic language characterized by a complex and rich morphology.\nThe exceptional degree of ambiguity in the writing system, the rich morphology,\nand the highly complex word formation process of roots and patterns all\ncontribute to making computational approaches to Arabic very challenging. As a\nresult, a practical handwriting recognition system should support large\nvocabulary to provide a high coverage and use the context information for\ndisambiguation. Several research efforts have been devoted for building online\nArabic handwriting recognition systems. Most of these methods are either using\ntheir small private test data sets or a standard database with limited lexicon\nand coverage. A large scale handwriting database is an essential resource that\ncan advance the research of online handwriting recognition. Currently, there is\nno online Arabic handwriting database with large lexicon, high coverage, large\nnumber of writers and training/testing data.\n  In this paper, we introduce AltecOnDB, a large scale online Arabic\nhandwriting database. AltecOnDB has 98% coverage of all the possible PAWS of\nthe Arabic language. The collected samples are complete sentences that include\ndigits and punctuation marks. The collected data is available on sentence, word\nand character levels, hence, high-level linguistic models can be used for\nperformance improvements. Data is collected from more than 1000 writers with\ndifferent backgrounds, genders and ages. Annotation and verification tools are\ndeveloped to facilitate the annotation and verification phases. We built an\nelementary recognition system to test our database and show the existing\ndifficulties when handling a large vocabulary and dealing with large amounts of\nstyles variations in the collected data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 08:58:10 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Abdelaziz", "Ibrahim", ""], ["Abdou", "Sherif", ""]]}, {"id": "1412.7659", "submitter": "Taco Cohen", "authors": "Taco S. Cohen and Max Welling", "title": "Transformation Properties of Learned Visual Representations", "comments": "T.S. Cohen & M. Welling, Transformation Properties of Learned Visual\n  Representations. In International Conference on Learning Representations\n  (ICLR), 2015", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a three-dimensional object moves relative to an observer, a change\noccurs on the observer's image plane and in the visual representation computed\nby a learned model. Starting with the idea that a good visual representation is\none that transforms linearly under scene motions, we show, using the theory of\ngroup representations, that any such representation is equivalent to a\ncombination of the elementary irreducible representations. We derive a striking\nrelationship between irreducibility and the statistical dependency structure of\nthe representation, by showing that under restricted conditions, irreducible\nrepresentations are decorrelated. Under partial observability, as induced by\nthe perspective projection of a scene onto the image plane, the motion group\ndoes not have a linear action on the space of images, so that it becomes\nnecessary to perform inference over a latent representation that does transform\nlinearly. This idea is demonstrated in a model of rotating NORB objects that\nemploys a latent representation of the non-commutative 3D rotation group SO(3).\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 13:19:20 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:46:00 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 21:20:04 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1412.7680", "submitter": "Roshan Ragel", "authors": "G. I. Gunarathna, M. A. P. Chamikara and R. G. Ragel", "title": "A Fuzzy Based Model to Identify Printed Sinhala Characters (ICIAfS14)", "comments": "The 7th International Conference on Information and Automation for\n  Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Character recognition techniques for printed documents are widely used for\nEnglish language. However, the systems that are implemented to recognize Asian\nlanguages struggle to increase the accuracy of recognition. Among other Asian\nlanguages (such as Arabic, Tamil, Chinese), Sinhala characters are unique,\nmainly because they are round in shape. This unique feature makes it a\nchallenge to extend the prevailing techniques to improve recognition of Sinhala\ncharacters. Therefore, a little attention has been given to improve the\naccuracy of Sinhala character recognition. A novel method, which makes use of\nthis unique feature, could be advantageous over other methods. This paper\ndescribes the use of a fuzzy inference system to recognize Sinhala characters.\nFeature extraction is mainly focused on distance and intersection measurements\nin different directions from the center of the letter making use of the round\nshape of characters. The results showed an overall accuracy of 90.7% for 140\ninstances of letters tested, much better than similar systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 14:56:54 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Gunarathna", "G. I.", ""], ["Chamikara", "M. A. P.", ""], ["Ragel", "R. G.", ""]]}, {"id": "1412.7689", "submitter": "Roshan Ragel", "authors": "Akmal Jahan Mac and Roshan G Ragel", "title": "Locating Tables in Scanned Documents for Reconstructing and Republishing\n  (ICIAfS14)", "comments": "The 7th International Conference on Information and Automation for\n  Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pool of knowledge available to the mankind depends on the source of learning\nresources, which can vary from ancient printed documents to present electronic\nmaterial. The rapid conversion of material available in traditional libraries\nto digital form needs a significant amount of work if we are to maintain the\nformat and the look of the electronic documents as same as their printed\ncounterparts. Most of the printed documents contain not only characters and its\nformatting but also some associated non text objects such as tables, charts and\ngraphical objects. It is challenging to detect them and to concentrate on the\nformat preservation of the contents while reproducing them. To address this\nissue, we propose an algorithm using local thresholds for word space and line\nheight to locate and extract all categories of tables from scanned document\nimages. From the experiments performed on 298 documents, we conclude that our\nalgorithm has an overall accuracy of about 75% in detecting tables from the\nscanned document images. Since the algorithm does not completely depend on rule\nlines, it can detect all categories of tables in a range of scanned documents\nwith different font types, styles and sizes to extract their formatting\nfeatures. Moreover, the algorithm can be applied to locate tables in multi\ncolumn layouts with small modification in layout analysis. Treating tables with\ntheir existing formatting features will tremendously help the reproducing of\nprinted documents for reprinting and updating purposes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 15:29:13 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Mac", "Akmal Jahan", ""], ["Ragel", "Roshan G", ""]]}, {"id": "1412.7725", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan and Hao Zhang and Baoyuan Wang and Sylvain Paris and\n  Yizhou Yu", "title": "Automatic Photo Adjustment Using Deep Neural Networks", "comments": "TOG minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo retouching enables photographers to invoke dramatic visual impressions\nby artistically enhancing their photos through stylistic color and tone\nadjustments. However, it is also a time-consuming and challenging task that\nrequires advanced skills beyond the abilities of casual photographers. Using an\nautomated algorithm is an appealing alternative to manual work but such an\nalgorithm faces many hurdles. Many photographic styles rely on subtle\nadjustments that depend on the image content and even its semantics. Further,\nthese adjustments are often spatially varying. Because of these\ncharacteristics, existing automatic algorithms are still limited and cover only\na subset of these challenges. Recently, deep machine learning has shown unique\nabilities to address hard problems that resisted machine algorithms for long.\nThis motivated us to explore the use of deep learning in the context of photo\nediting. In this paper, we explain how to formulate the automatic photo\nadjustment problem in a way suitable for this approach. We also introduce an\nimage descriptor that accounts for the local semantics of an image. Our\nexperiments demonstrate that our deep learning formulation applied using these\ndescriptors successfully capture sophisticated photographic styles. In\nparticular and unlike previous techniques, it can model local adjustments that\ndepend on the image semantics. We show on several examples that this yields\nresults that are qualitatively and quantitatively better than previous work.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 17:51:17 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 03:49:35 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Wang", "Baoyuan", ""], ["Paris", "Sylvain", ""], ["Yu", "Yizhou", ""]]}, {"id": "1412.7755", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu", "title": "Multiple Object Recognition with Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based model for recognizing multiple objects in\nimages. The proposed model is a deep recurrent neural network trained with\nreinforcement learning to attend to the most relevant regions of the input\nimage. We show that the model learns to both localize and recognize multiple\nobjects despite being given only class labels during training. We evaluate the\nmodel on the challenging task of transcribing house number sequences from\nGoogle Street View images and show that it is both more accurate than the\nstate-of-the-art convolutional networks and uses fewer parameters and less\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 20:58:23 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 16:49:23 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Ba", "Jimmy", ""], ["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1412.7844", "submitter": "Odemir Bruno PhD", "authors": "Andr\\'e R. Backes and Odemir M. Bruno", "title": "Texture analysis using volume-radius fractal dimension", "comments": "4 pages, 4 figures", "journal-ref": "Backes, A. R and Bruno, O. M. Texture analysis using volume-radius\n  fractal dimension, Applied Mathematics and Computation, Volume 219, Issue 11,\n  Pages 5870 - 5875, 2013", "doi": "10.1016/j.amc.2012.11.092", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture plays an important role in computer vision. It is one of the most\nimportant visual attributes used in image analysis, once it provides\ninformation about pixel organization at different regions of the image. This\npaper presents a novel approach for texture characterization, based on\ncomplexity analysis. The proposed approach expands the idea of the Mass-radius\nfractal dimension, a method originally developed for shape analysis, to a set\nof coordinates in 3D-space that represents the texture under analysis in a\nsignature able to characterize efficiently different texture classes in terms\nof complexity. An experiment using images from the Brodatz album illustrates\nthe method performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 17:38:33 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Backes", "Andr\u00e9 R.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1412.7849", "submitter": "Odemir Bruno PhD", "authors": "Jo\\~ao Batista Florindo, N\\'ubia Rosa da Silva, Liliane Maria\n  Romualdo, Fernanda de F\\'atima da Silva, Pedro Henrique de Cerqueira Luz,\n  Valdo Rodrigues Herling, Odemir Martinez Bruno", "title": "Brachiaria species identification using imaging techniques based on\n  fractal descriptors", "comments": "7 pages, 5 figures", "journal-ref": "Computers and Electronics in Agriculture, V 103, Pages 48-54, 2014", "doi": "10.1016/j.compag.2014.02.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of a rapid and accurate method in diagnosis and classification of\nspecies and/or cultivars of forage has practical relevance, scientific and\ntrade in various areas of study. Thus, leaf samples of fodder plant species\n\\textit{Brachiaria} were previously identified, collected and scanned to be\ntreated by means of artificial vision to make the database and be used in\nsubsequent classifications. Forage crops used were: \\textit{Brachiaria\ndecumbens} cv. IPEAN; \\textit{Brachiaria ruziziensis} Germain \\& Evrard;\n\\textit{Brachiaria Brizantha} (Hochst. ex. A. Rich.) Stapf; \\textit{Brachiaria\narrecta} (Hack.) Stent. and \\textit{Brachiaria spp}. The images were analyzed\nby the fractal descriptors method, where a set of measures are obtained from\nthe values of the fractal dimension at different scales. Therefore such values\nare used as inputs for a state-of-the-art classifier, the Support Vector\nMachine, which finally discriminates the images according to the respective\nspecies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 18:23:10 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Florindo", "Jo\u00e3o Batista", ""], ["da Silva", "N\u00fabia Rosa", ""], ["Romualdo", "Liliane Maria", ""], ["da Silva", "Fernanda de F\u00e1tima", ""], ["Luz", "Pedro Henrique de Cerqueira", ""], ["Herling", "Valdo Rodrigues", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1412.7851", "submitter": "Odemir Bruno PhD", "authors": "Jo\\~ao Batista Florindo and Odemir Martinez Bruno", "title": "Fractal descriptors based on the probability dimension: a texture\n  analysis and classification approach", "comments": "7 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1205.2821", "journal-ref": "Pattern Recognition Letters, Volume 42, Pages 107-114, 2014", "doi": "10.1016/j.patrec.2014.01.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel technique for obtaining descriptors of\ngray-level texture images. The descriptors are provided by applying a\nmultiscale transform to the fractal dimension of the image estimated through\nthe probability (Voss) method. The effectiveness of the descriptors is verified\nin a classification task using benchmark over texture datasets. The results\nobtained demonstrate the efficiency of the proposed method as a tool for the\ndescription and discrimination of texture images.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 18:50:31 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Florindo", "Jo\u00e3o Batista", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1412.7854", "submitter": "Seyedshams Feyzabadi", "authors": "Seyedshams Feyzabadi", "title": "Joint Deep Learning for Car Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional object recognition approaches apply feature extraction, part\ndeformation handling, occlusion handling and classification sequentially while\nthey are independent from each other. Ouyang and Wang proposed a model for\njointly learning of all of the mentioned processes using one deep neural\nnetwork. We utilized, and manipulated their toolbox in order to apply it in car\ndetection scenarios where it had not been tested. Creating a single deep\narchitecture from these components, improves the interaction between them and\ncan enhance the performance of the whole system. We believe that the approach\ncan be used as a general purpose object detection toolbox. We tested the\nalgorithm on UIUC car dataset, and achieved an outstanding result. The accuracy\nof our method was 97 % while the previously reported results showed an accuracy\nof up to 91 %. We strongly believe that having an experiment on a larger\ndataset can show the advantage of using deep models over shallow ones.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 18:55:49 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 17:57:31 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Feyzabadi", "Seyedshams", ""]]}, {"id": "1412.7856", "submitter": "Odemir Bruno PhD", "authors": "\\'Alvaro Gomez Z., Jo\\~ao B. Florindo, Odemir M. Bruno", "title": "Gabor wavelets combined with volumetric fractal dimension applied to\n  texture analysis", "comments": "11 pages, 2 figures", "journal-ref": "Pattern Recognition Letters, V. 36, Pages 135-143, 2014", "doi": "10.1016/j.patrec.2013.09.023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture analysis and classification remain as one of the biggest challenges\nfor the field of computer vision and pattern recognition. On this matter, Gabor\nwavelets has proven to be a useful technique to characterize distinctive\ntexture patterns. However, most of the approaches used to extract descriptors\nof the Gabor magnitude space usually fail in representing adequately the\nrichness of detail present into a unique feature vector. In this paper, we\npropose a new method to enhance the Gabor wavelets process extracting a fractal\nsignature of the magnitude spaces. Each signature is reduced using a canonical\nanalysis function and concatenated to form the final feature vector.\nExperiments were conducted on several texture image databases to prove the\npower and effectiveness of the proposed method. Results obtained shown that\nthis method outperforms other early proposed method, creating a more reliable\ntechnique for texture feature extraction.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 19:38:11 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Z.", "\u00c1lvaro Gomez", ""], ["Florindo", "Jo\u00e3o B.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1412.7880", "submitter": "Odemir Bruno PhD", "authors": "Marcos W. S. Oliveira and Dalcimar Casanova and Jo\\~ao B. Florindo and\n  Odemir Martinez Bruno", "title": "Enhancing fractal descriptors on images by combining boundary and\n  interior of Minkowski dilation", "comments": "6 pages 3 figures", "journal-ref": "Physica A, Volume 416, Pages 41-48, 2014", "doi": "10.1016/j.physa.2014.07.074", "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes to obtain novel fractal descriptors from gray-level\ntexture images by combining information from interior and boundary measures of\nthe Minkowski dilation applied to the texture surface. At first, the image is\nconverted into a surface where the height of each point is the gray intensity\nof the respective pixel in that position in the image. Thus, this surface is\nmorphologically dilated by spheres. The radius of such spheres is ranged within\nan interval and the volume and the external area of the dilated structure are\ncomputed for each radius. The final descriptors are given by such measures\nconcatenated and subject to a canonical transform to reduce the dimensionality.\nThe proposal is an enhancement to the classical Bouligand-Minkowski fractal\ndescriptors, where only the volume (interior) information is considered. As\ndifferent structures may have the same volume, but not the same area, the\nproposal yields to more rich descriptors as confirmed by results on the\nclassification of benchmark databases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 02:13:04 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Oliveira", "Marcos W. S.", ""], ["Casanova", "Dalcimar", ""], ["Florindo", "Jo\u00e3o B.", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1412.7884", "submitter": "Zhengdong Zhang", "authors": "Zhengdong Zhang, Phillip Isola, Edward H. Adelson", "title": "Sparkle Vision: Seeing the World through Random Specular Microfacets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of reproducing the world lighting from a\nsingle image of an object covered with random specular microfacets on the\nsurface. We show that such reflectors can be interpreted as a randomized\nmapping from the lighting to the image. Such specular objects have very\ndifferent optical properties from both diffuse surfaces and smooth specular\nobjects like metals, so we design special imaging system to robustly and\neffectively photograph them. We present simple yet reliable algorithms to\ncalibrate the proposed system and do the inference. We conduct experiments to\nverify the correctness of our model assumptions and prove the effectiveness of\nour pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 02:46:41 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zhang", "Zhengdong", ""], ["Isola", "Phillip", ""], ["Adelson", "Edward H.", ""]]}, {"id": "1412.7889", "submitter": "Odemir Bruno PhD", "authors": "N\\'ubia Rosa da Silva, Pieter Van der Wee\\\"en, Bernard De Baets,\n  Odemir Martinez Bruno", "title": "Improved texture image classification through the use of a\n  corrosion-inspired cellular automaton", "comments": "13 pages, 14 figures", "journal-ref": "Neurocomputing, 149, Part C, pp 1560-1572, 2015", "doi": "10.1016/j.neucom.2014.08.036", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of classifying synthetic and natural texture\nimages is addressed. To tackle this problem, an innovative method is proposed\nthat combines concepts from corrosion modeling and cellular automata to\ngenerate a texture descriptor. The core processes of metal (pitting) corrosion\nare identified and applied to texture images by incorporating the basic\nmechanisms of corrosion in the transition function of the cellular automaton.\nThe surface morphology of the image is analyzed before and during the\napplication of the transition function of the cellular automaton. In each\niteration the cumulative mass of corroded product is obtained to construct each\nof the attributes of the texture descriptor. In a final step, this texture\ndescriptor is used for image classification by applying Linear Discriminant\nAnalysis. The method was tested on the well-known Brodatz and Vistex databases.\nIn addition, in order to verify the robustness of the method, its invariance to\nnoise and rotation were tested. To that end, different variants of the original\ntwo databases were obtained through addition of noise to and rotation of the\nimages. The results showed that the method is effective for texture\nclassification according to the high success rates obtained in all cases. This\nindicates the potential of employing methods inspired on natural phenomena in\nother fields.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 03:47:06 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["da Silva", "N\u00fabia Rosa", ""], ["Van der Wee\u00ebn", "Pieter", ""], ["De Baets", "Bernard", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1412.7934", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel, Raunaq Vohra, and Ainesh Bakshi", "title": "A Novel Feature Selection and Extraction Technique for Classification", "comments": "2 pages, 2 tables, published at IEEE SMC 2014", "journal-ref": "IEEE Xplore, Proceedings of IEEE SMC 2014, pages 4033 - 4034", "doi": "10.1109/SMC.2014.6974562", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a versatile technique for the purpose of feature\nselection and extraction - Class Dependent Features (CDFs). We use CDFs to\nimprove the accuracy of classification and at the same time control\ncomputational expense by tackling the curse of dimensionality. In order to\ndemonstrate the generality of this technique, it is applied to handwritten\ndigit recognition and text categorization.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 12:57:58 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""], ["Bakshi", "Ainesh", ""]]}, {"id": "1412.7957", "submitter": "Sezer Karaoglu", "authors": "Sezer Karaoglu, Yang Liu, Theo Gevers", "title": "Detect2Rank : Combining Object Detectors Using Learning to Rank", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2499702", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important research area in the field of computer\nvision. Many detection algorithms have been proposed. However, each object\ndetector relies on specific assumptions of the object appearance and imaging\nconditions. As a consequence, no algorithm can be considered as universal. With\nthe large variety of object detectors, the subsequent question is how to select\nand combine them.\n  In this paper, we propose a framework to learn how to combine object\ndetectors. The proposed method uses (single) detectors like DPM, CN and EES,\nand exploits their correlation by high level contextual features to yield a\ncombined detection list.\n  Experiments on the PASCAL VOC07 and VOC10 datasets show that the proposed\nmethod significantly outperforms single object detectors, DPM (8.4%), CN (6.8%)\nand EES (17.0%) on VOC07 and DPM (6.5%), CN (5.5%) and EES (16.2%) on VOC10.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 16:46:52 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Karaoglu", "Sezer", ""], ["Liu", "Yang", ""], ["Gevers", "Theo", ""]]}, {"id": "1412.7963", "submitter": "Odemir Bruno PhD", "authors": "Jo\\~ao B. Florindo, Odemir M. Bruno", "title": "Texture analysis by multi-resolution fractal descriptors", "comments": "8 pages, 6 figures", "journal-ref": "Expert Systems with Applications, Volume 40, Issue 10, Pages\n  4022-4028, 2013", "doi": "10.1016/j.eswa.2013.01.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a texture descriptor based on fractal theory. The method\nis based on the Bouligand-Minkowski descriptors. We decompose the original\nimage recursively into 4 equal parts. In each recursion step, we estimate the\naverage and the deviation of the Bouligand-Minkowski descriptors computed over\neach part. Thus, we extract entropy features from both average and deviation.\nThe proposed descriptors are provided by the concatenation of such measures.\nThe method is tested in a classification experiment under well known datasets,\nthat is, Brodatz and Vistex. The results demonstrate that the proposed\ntechnique achieves better results than classical and state-of-the-art texture\ndescriptors, such as Gabor-wavelets and co-occurrence matrix.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 17:45:41 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Florindo", "Jo\u00e3o B.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1412.8070", "submitter": "Artiom Kovnatsky Artiom Kovnatsky", "authors": "Artiom Kovnatsky, Michael M. Bronstein, Xavier Bresson, Pierre\n  Vandergheynst", "title": "Functional correspondence by matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of finding dense intrinsic\ncorrespondence between manifolds using the recently introduced functional\nframework. We pose the functional correspondence problem as matrix completion\nwith manifold geometric structure and inducing functional localization with the\n$L_1$ norm. We discuss efficient numerical procedures for the solution of our\nproblem. Our method compares favorably to the accuracy of state-of-the-art\ncorrespondence algorithms on non-rigid shape matching benchmarks, and is\nespecially advantageous in settings when only scarce data is available.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 17:53:06 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Kovnatsky", "Artiom", ""], ["Bronstein", "Michael M.", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1412.8197", "submitter": "Ahmad Pahlavan Tafti", "authors": "Z. Bardosi, D. Granata, G. Lugos, A. P. Tafti, S. Saxena", "title": "Metacarpal Bones Localization in X-ray Imagery Using Particle Filter\n  Segmentation", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods such as sequential Monte Carlo Methods were proposed for\ndetection, segmentation and tracking of objects in digital images. A similar\napproach, called Shape Particle Filters was introduced for the segmentation of\nvertebra, lungs and hearts [1]. In this contribution, a global shape and a\nlocal appearance model are derived from specific object annotated X-ray images\nof the metacarpal bones. In the test data a unique labeling of the bone\nboundary and the background points and a manual annotation is given. Using a\nset of local features (Haar-like) in the neighborhood of each pixel a\nprobabilistic pixel classifier is built using the random forest algorithm. To\nfit the shape model to a new image, a label probability map is extracted and\nthen the optimal shape is obtained by maximizing the probability of each\nlandmark with the Differential Evolution algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 18:48:02 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 00:19:11 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Bardosi", "Z.", ""], ["Granata", "D.", ""], ["Lugos", "G.", ""], ["Tafti", "A. P.", ""], ["Saxena", "S.", ""]]}, {"id": "1412.8287", "submitter": "Junyan Wang", "authors": "Junyan Wang and Kap-Luk Chan", "title": "Rigid and Non-rigid Shape Evolutions for Shape Alignment and Recovery in\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The same type of objects in different images may vary in their shapes because\nof rigid and non-rigid shape deformations, occluding foreground as well as\ncluttered background. The problem concerned in this work is the shape\nextraction in such challenging situations. We approach the shape extraction\nthrough shape alignment and recovery. This paper presents a novel and general\nmethod for shape alignment and recovery by using one example shapes based on\ndeterministic energy minimization. Our idea is to use general model of shape\ndeformation in minimizing active contour energies. Given \\emph{a priori} form\nof the shape deformation, we show how the curve evolution equation\ncorresponding to the shape deformation can be derived. The curve evolution is\ncalled the prior variation shape evolution (PVSE). We also derive the\nenergy-minimizing PVSE for minimizing active contour energies. For shape\nrecovery, we propose to use the PVSE that deforms the shape while preserving\nits shape characteristics. For choosing such shape-preserving PVSE, a theory of\nshape preservability of the PVSE is established. Experimental results validate\nthe theory and the formulations, and they demonstrate the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 09:19:55 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Wang", "Junyan", ""], ["Chan", "Kap-Luk", ""]]}, {"id": "1412.8307", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, Andr\\'e van\n  Schaik, and Jonathan Tapson", "title": "Fast, simple and accurate handwritten digit classification by training\n  shallow neural network classifiers with the 'extreme learning machine'\n  algorithm", "comments": "Accepted for publication; 9 pages of text, 6 figures and 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0134254", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in training deep (multi-layer) architectures have inspired a\nrenaissance in neural network use. For example, deep convolutional networks are\nbecoming the default option for difficult tasks on large datasets, such as\nimage and speech recognition. However, here we show that error rates below 1%\non the MNIST handwritten digit benchmark can be replicated with shallow\nnon-convolutional neural networks. This is achieved by training such networks\nusing the 'Extreme Learning Machine' (ELM) approach, which also enables a very\nrapid training time (~10 minutes). Adding distortions, as is common practise\nfor MNIST, reduces error rates even further. Our methods are also shown to be\ncapable of achieving less than 5.5% error rates on the NORB image database. To\nachieve these results, we introduce several enhancements to the standard ELM\nalgorithm, which individually and in combination can significantly improve\nperformance. The main innovation is to ensure each hidden-unit operates only on\na randomly sized and positioned patch of each image. This form of random\n`receptive field' sampling of the input ensures the input weight matrix is\nsparse, with about 90% of weights equal to zero. Furthermore, combining our\nmethods with a small number of iterations of a single-batch backpropagation\nmethod can significantly reduce the number of hidden-units required to achieve\na particular performance. Our close to state-of-the-art results for MNIST and\nNORB suggest that the ease of use and accuracy of the ELM algorithm for\ndesigning a single-hidden-layer neural network classifier should cause it to be\ngiven greater consideration either as a standalone method for simpler problems,\nor as the final classification stage in deep neural networks applied to more\ndifficult problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 11:14:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 08:28:03 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Tissera", "Migel D.", ""], ["Vladusich", "Tony", ""], ["van Schaik", "Andr\u00e9", ""], ["Tapson", "Jonathan", ""]]}, {"id": "1412.8341", "submitter": "Pavel H\\'ala", "authors": "Pavel H\\'ala", "title": "Spectral classification using convolutional neural networks", "comments": "71 pages, 50 figures, Master's thesis, Masaryk University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There is a great need for accurate and autonomous spectral classification\nmethods in astrophysics. This thesis is about training a convolutional neural\nnetwork (ConvNet) to recognize an object class (quasar, star or galaxy) from\none-dimension spectra only. Author developed several scripts and C programs for\ndatasets preparation, preprocessing and postprocessing of the data. EBLearn\nlibrary (developed by Pierre Sermanet and Yann LeCun) was used to create\nConvNets. Application on dataset of more than 60000 spectra yielded success\nrate of nearly 95%. This thesis conclusively proved great potential of\nconvolutional neural networks and deep learning methods in astrophysics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 13:47:06 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["H\u00e1la", "Pavel", ""]]}, {"id": "1412.8380", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "A simple coding for cross-domain matching with dimension reduction via\n  spectral graph embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data vectors are obtained from multiple domains. They are feature vectors of\nimages or vector representations of words. Domains may have different numbers\nof data vectors with different dimensions. These data vectors from multiple\ndomains are projected to a common space by linear transformations in order to\nsearch closely related vectors across domains. We would like to find projection\nmatrices to minimize distances between closely related data vectors. This\nformulation of cross-domain matching is regarded as an extension of the\nspectral graph embedding to multi-domain setting, and it includes several\nmultivariate analysis methods of statistics such as multiset canonical\ncorrelation analysis, correspondence analysis, and principal component\nanalysis. Similar approaches are very popular recently in pattern recognition\nand vision. In this paper, instead of proposing a novel method, we will\nintroduce an embarrassingly simple idea of coding the data vectors for\nexplaining all the above mentioned approaches. A data vector is concatenated\nwith zero vectors from all other domains to make an augmented vector. The\ncross-domain matching is solved by applying the single-domain version of\nspectral graph embedding to these augmented vectors of all the domains. An\ninteresting connection to the classical associative memory model of neural\nnetworks is also discussed by noticing a coding for association. A\ncross-validation method for choosing the dimension of the common space and a\nregularization parameter will be discussed in an illustrative numerical\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 16:08:27 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 18:38:26 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "1412.8419", "submitter": "Pedro O. Pinheiro", "authors": "Remi Lebret and Pedro O. Pinheiro and Ronan Collobert", "title": "Simple Image Description Generator via a Linear Phrase-Based Approach", "comments": "Accepted as a workshop paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 18:43:10 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 05:09:13 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 03:53:26 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Lebret", "Remi", ""], ["Pinheiro", "Pedro O.", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.8496", "submitter": "Mahdi Salarian mr", "authors": "Mahdi Salarian", "title": "Accurate Localization in Dense Urban Area Using Google Street View Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate information about the location and orientation of a camera in mobile\ndevices is central to the utilization of location-based services (LBS). Most of\nsuch mobile devices rely on GPS data but this data is subject to inaccuracy due\nto imperfections in the quality of the signal provided by satellites. This\nshortcoming has spurred the research into improving the accuracy of\nlocalization. Since mobile devices have camera, a major thrust of this research\nhas been seeks to acquire the local scene and apply image retrieval techniques\nby querying a GPS-tagged image database to find the best match for the acquired\nscene.. The techniques are however computationally demanding and unsuitable for\nreal-time applications such as assistive technology for navigation by the blind\nand visually impaired which motivated out work. To overcome the high complexity\nof those techniques, we investigated the use of inertial sensors as an aid in\nimage-retrieval-based approach. Armed with information of media other than\nimages, such as data from the GPS module along with orientation sensors such as\naccelerometer and gyro, we sought to limit the size of the image set to c\nsearch for the best match. Specifically, data from the orientation sensors\nalong with Dilution of precision (DOP) from GPS are used to find the angle of\nview and estimation of position. We present analysis of the reduction in the\nimage set size for the search as well as simulations to demonstrate the\neffectiveness in a fast implementation with 98% Estimated Position Error.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 22:09:41 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Salarian", "Mahdi", ""]]}, {"id": "1412.8556", "submitter": "Jingming Dong", "authors": "Jingming Dong, Stefano Soatto", "title": "Domain-Size Pooling in Local Descriptors: DSP-SIFT", "comments": "Extended version of the CVPR 2015 paper. Technical Report UCLA CSD\n  140022", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple modification of local image descriptors, such as SIFT,\nbased on pooling gradient orientations across different domain sizes, in\naddition to spatial locations. The resulting descriptor, which we call\nDSP-SIFT, outperforms other methods in wide-baseline matching benchmarks,\nincluding those based on convolutional neural networks, despite having the same\ndimension of SIFT and requiring no training.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 03:49:47 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 20:36:01 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 05:36:05 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Dong", "Jingming", ""], ["Soatto", "Stefano", ""]]}, {"id": "1412.8656", "submitter": "Ladan Sharafyan Cigaroudy", "authors": "Nasser Aghazadeh, Ladan Sharafyan Cigaroudy", "title": "A multistep segmentation algorithm for vessel extraction in medical\n  imaging", "comments": "9 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this paper is to propose an iterative procedure for\ntubular structure segmentation of 2D images, which combines tight frame of\nCurvelet transforms with a SURE technique thresholding which is based on\nprinciple obtained by minimizing Stein Unbiased Risk Estimate for denoising.\nThis proposed algorithm is mainly based on the TFA proposal presented in [1,\n9], which we use eigenvectors of Hessian matrix of image for improving this\niterative part in segmenting unclear and narrow vessels and filling the gap\nbetween separate pieces of detected vessels. The experimental results are\npresented to demonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 15:28:26 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Aghazadeh", "Nasser", ""], ["Cigaroudy", "Ladan Sharafyan", ""]]}, {"id": "1412.8659", "submitter": "Edouard Oyallon", "authors": "Edouard Oyallon and St\\'ephane Mallat", "title": "Deep Roto-Translation Scattering for Object Classification", "comments": "9 pages, 3 figures, CVPR 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms or supervised deep convolution networks have\nconsiderably improved the efficiency of predefined feature representations such\nas SIFT. We introduce a deep scattering convolution network, with predefined\nwavelet filters over spatial and angular variables. This representation brings\nan important improvement to results previously obtained with predefined\nfeatures over object image databases such as Caltech and CIFAR. The resulting\naccuracy is comparable to results obtained with unsupervised deep learning and\ndictionary based representations. This shows that refining image\nrepresentations by using geometric priors is a promising direction to improve\nimage classification and its understanding.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 15:32:18 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 16:43:02 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Oyallon", "Edouard", ""], ["Mallat", "St\u00e9phane", ""]]}]