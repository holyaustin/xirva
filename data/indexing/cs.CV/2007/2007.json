[{"id": "2007.00024", "submitter": "You Xie", "authors": "You Xie, Nils Thuerey", "title": "Data-driven Regularization via Racecar Training for Generalizing Neural\n  Networks", "comments": "https://github.com/tum-pbs/racecar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel training approach for improving the generalization in\nneural networks. We show that in contrast to regular constraints for\northogonality, our approach represents a {\\em data-dependent} orthogonality\nconstraint, and is closely related to singular value decompositions of the\nweight matrices. We also show how our formulation is easy to realize in\npractical network architectures via a reverse pass, which aims for\nreconstructing the full sequence of internal states of the network. Despite\nbeing a surprisingly simple change, we demonstrate that this forward-backward\ntraining approach, which we refer to as {\\em racecar} training, leads to\nsignificantly more generic features being extracted from a given data set.\nNetworks trained with our approach show more balanced mutual information\nbetween input and output throughout all layers, yield improved explainability\nand, exhibit improved performance for a variety of tasks and task transfers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:00:41 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Xie", "You", ""], ["Thuerey", "Nils", ""]]}, {"id": "2007.00045", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "A. M. Hafiz", "title": "K-Nearest Neighbour and Support Vector Machine Hybrid Classification", "comments": null, "journal-ref": "International Journal of Imaging and Robotics, Vol.19, No.4,\n  pp.33-41, CESER Publications (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel K-Nearest Neighbour and Support Vector Machine hybrid\nclassification technique has been proposed that is simple and robust. It is\nbased on the concept of discriminative nearest neighbourhood classification.\nThe technique consists of using K-Nearest Neighbour Classification for test\nsamples satisfying a proximity condition. The patterns which do not pass the\nproximity condition are separated. This is followed by sifting the training set\nfor a fixed number of patterns for every class which are closest to each\nseparated test pattern respectively, based on the Euclidean distance metric.\nSubsequently, for every separated test sample, a Support Vector Machine is\ntrained on the sifted training set patterns associated with it, and\nclassification for the test sample is done. The proposed technique has been\ncompared to the state of art in this research area. Three datasets viz. the\nUnited States Postal Service (USPS) Handwritten Digit Dataset, MNIST Dataset,\nand an Arabic numeral dataset, the Modified Arabic Digits Database, MADB, have\nbeen used to evaluate the performance of the algorithm. The algorithm generally\noutperforms the other algorithms with which it has been compared.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 15:26:56 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hafiz", "A. M.", ""]]}, {"id": "2007.00046", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat", "title": "Fast Training of Deep Networks with One-Class CNNs", "comments": "Camera Ready: 2nd International Conference on Cybernetics, Cognition\n  and Machine Learning Applications(ICCCMLA), 2020, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class CNNs have shown promise in novelty detection. However, very less\nwork has been done on extending them to multiclass classification. The proposed\napproach is a viable effort in this direction. It uses one-class CNNs i.e., it\ntrains one CNN per class, for multiclass classification. An ensemble of such\none-class CNNs is used for multiclass classification. The benefits of the\napproach are generally better recognition accuracy while taking almost even\nhalf or two-thirds of the training time of a conventional multi-class deep\nnetwork. The proposed approach has been applied successfully to face\nrecognition and object recognition tasks. For face recognition, a 1000 frame\nRGB video, featuring many faces together, has been used for benchmarking of the\nproposed approach. Its database is available on request via e-mail. For object\nrecognition, the Caltech-101 Image Database and 17Flowers Dataset have also\nbeen used. The experimental results support the claims made.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:53:45 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 11:51:16 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Bhat", "Ghulam Mohiuddin", ""]]}, {"id": "2007.00047", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat", "title": "A Survey on Instance Segmentation: State of the art", "comments": "Int J Multimed Info Retr (2020)", "journal-ref": null, "doi": "10.1007/s13735-020-00195-x", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection or localization is an incremental step in progression from\ncoarse to fine digital image inference. It not only provides the classes of the\nimage objects, but also provides the location of the image objects which have\nbeen classified. The location is given in the form of bounding boxes or\ncentroids. Semantic segmentation gives fine inference by predicting labels for\nevery pixel in the input image. Each pixel is labelled according to the object\nclass within which it is enclosed. Furthering this evolution, instance\nsegmentation gives different labels for separate instances of objects belonging\nto the same class. Hence, instance segmentation may be defined as the technique\nof simultaneously solving the problem of object detection as well as that of\nsemantic segmentation. In this survey paper on instance segmentation -- its\nbackground, issues, techniques, evolution, popular datasets, related work up to\nthe state of the art and future scope have been discussed. The paper provides\nvaluable information for those who want to do research in the field of instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:39:20 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Bhat", "Ghulam Mohiuddin", ""]]}, {"id": "2007.00062", "submitter": "Ioannis Kansizoglou", "authors": "Ioannis Kansizoglou, Loukas Bampis, Antonios Gasteratos", "title": "Deep Feature Space: A Geometrical Perspective", "comments": "Accepted for publication in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3094625", "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the most prominent attributes of Neural Networks (NNs) constitutes\ntheir capability of learning to extract robust and descriptive features from\nhigh dimensional data, like images. Hence, such an ability renders their\nexploitation as feature extractors particularly frequent in an abundant of\nmodern reasoning systems. Their application scope mainly includes complex\ncascade tasks, like multi-modal recognition and deep Reinforcement Learning\n(RL). However, NNs induce implicit biases that are difficult to avoid or to\ndeal with and are not met in traditional image descriptors. Moreover, the lack\nof knowledge for describing the intra-layer properties -- and thus their\ngeneral behavior -- restricts the further applicability of the extracted\nfeatures. With the paper at hand, a novel way of visualizing and understanding\nthe vector space before the NNs' output layer is presented, aiming to enlighten\nthe deep feature vectors' properties under classification tasks. Main attention\nis paid to the nature of overfitting in the feature space and its adverse\neffect on further exploitation. We present the findings that can be derived\nfrom our model's formulation, and we evaluate them on realistic recognition\nscenarios, proving its prominence by improving the obtained results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:51:02 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 16:03:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kansizoglou", "Ioannis", ""], ["Bampis", "Loukas", ""], ["Gasteratos", "Antonios", ""]]}, {"id": "2007.00074", "submitter": "Rana Hanocka", "authors": "Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "Deep Geometric Texture Synthesis", "comments": "SIGGRAPH 2020", "journal-ref": null, "doi": "10.1145/3386569.3392471", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep generative adversarial networks for image generation have\nadvanced rapidly; yet, only a small amount of research has focused on\ngenerative models for irregular structures, particularly meshes. Nonetheless,\nmesh generation and synthesis remains a fundamental topic in computer graphics.\nIn this work, we propose a novel framework for synthesizing geometric textures.\nIt learns geometric texture statistics from local neighborhoods (i.e., local\ntriangular patches) of a single reference 3D model. It learns deep features on\nthe faces of the input triangulation, which is used to subdivide and generate\noffsets across multiple scales, without parameterization of the reference or\ntarget mesh. Our network displaces mesh vertices in any direction (i.e., in the\nnormal and tangential direction), enabling synthesis of geometric textures,\nwhich cannot be expressed by a simple 2D displacement map. Learning and\nsynthesizing on local geometric patches enables a genus-oblivious framework,\nfacilitating texture transfer between shapes of different genus.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:36:38 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hertz", "Amir", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2007.00077", "submitter": "Cody Coleman", "authors": "Cody Coleman, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter\n  Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, I.\n  Zeki Yalniz", "title": "Similarity Search for Efficient Active Learning and Search of Rare\n  Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many active learning and search approaches are intractable for large-scale\nindustrial settings with billions of unlabeled examples. Existing approaches\nsearch globally for the optimal examples to label, scaling linearly or even\nquadratically with the unlabeled data. In this paper, we improve the\ncomputational efficiency of active learning and search methods by restricting\nthe candidate pool for labeling to the nearest neighbors of the currently\nlabeled set instead of scanning over all of the unlabeled data. We evaluate\nseveral selection strategies in this setting on three large-scale computer\nvision datasets: ImageNet, OpenImages, and a de-identified and aggregated\ndataset of 10 billion images provided by a large internet company. Our approach\nachieved similar mean average precision and recall as the traditional global\napproach while reducing the computational cost of selection by up to three\norders of magnitude, thus enabling web-scale active learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:46:10 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 16:54:12 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Coleman", "Cody", ""], ["Chou", "Edward", ""], ["Katz-Samuels", "Julian", ""], ["Culatana", "Sean", ""], ["Bailis", "Peter", ""], ["Berg", "Alexander C.", ""], ["Nowak", "Robert", ""], ["Sumbaly", "Roshan", ""], ["Zaharia", "Matei", ""], ["Yalniz", "I. Zeki", ""]]}, {"id": "2007.00083", "submitter": "Sahar Siddiqui", "authors": "Sahar Siddiqui, Elena Sizikova, Gemma Roig, Najib J. Majaj, Denis G.\n  Pelli", "title": "Using Human Psychophysics to Evaluate Generalization in Scene Text\n  Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition models have advanced greatly in recent years. Inspired\nby human reading we characterize two important scene text recognition models by\nmeasuring their domains i.e. the range of stimulus images that they can read.\nThe domain specifies the ability of readers to generalize to different word\nlengths, fonts, and amounts of occlusion. These metrics identify strengths and\nweaknesses of existing models. Relative to the attention-based (Attn) model, we\ndiscover that the connectionist temporal classification (CTC) model is more\nrobust to noise and occlusion, and better at generalizing to different word\nlengths. Further, we show that in both models, adding noise to training images\nyields better generalization to occlusion. These results demonstrate the value\nof testing models till they break, complementing the traditional data science\nfocus on optimizing performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:51:26 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Siddiqui", "Sahar", ""], ["Sizikova", "Elena", ""], ["Roig", "Gemma", ""], ["Majaj", "Najib J.", ""], ["Pelli", "Denis G.", ""]]}, {"id": "2007.00095", "submitter": "Amir Rasouli", "authors": "Amir Rasouli", "title": "Deep Learning for Vision-based Prediction: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based prediction algorithms have a wide range of applications\nincluding autonomous driving, surveillance, human-robot interaction, weather\nprediction. The objective of this paper is to provide an overview of the field\nin the past five years with a particular focus on deep learning approaches. For\nthis purpose, we categorize these algorithms into video prediction, action\nprediction, trajectory prediction, body motion prediction, and other prediction\napplications. For each category, we highlight the common architectures,\ntraining methods and types of data used. In addition, we discuss the common\nevaluation metrics and datasets used for vision-based prediction tasks. A\ndatabase of all the information presented in this survey including,\ncross-referenced according to papers, datasets and metrics, can be found online\nat https://github.com/aras62/vision-based-prediction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 20:26:46 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 15:33:06 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Rasouli", "Amir", ""]]}, {"id": "2007.00112", "submitter": "Xavier Boix", "authors": "Syed Suleman Abbas Zaidi, Xavier Boix, Neeraj Prasad, Sharon\n  Gilad-Gutnick, Shlomit Ben-Ami, Pawan Sinha", "title": "Is Robustness To Transformations Driven by Invariant Neural\n  Representations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive\nrobustness to recognize objects under transformations (e.g. blur or noise) when\nthese transformations are included in the training set. A hypothesis to explain\nsuch robustness is that DCNNs develop invariant neural representations that\nremain unaltered when the image is transformed. Yet, to what extent this\nhypothesis holds true is an outstanding question, as including transformations\nin the training set could lead to properties different from invariance, e.g.\nparts of the network could be specialized to recognize either transformed or\nnon-transformed images. In this paper, we analyze the conditions under which\ninvariance emerges. To do so, we leverage that invariant representations\nfacilitate robustness to transformations for object categories that are not\nseen transformed during training. Our results with state-of-the-art DCNNs\nindicate that invariant representations strengthen as the number of transformed\ncategories in the training set is increased. This is much more prominent with\nlocal transformations such as blurring and high-pass filtering, compared to\ngeometric transformations such as rotation and thinning, that entail changes in\nthe spatial arrangement of the object. Our results contribute to a better\nunderstanding of invariant representations in deep learning, and the conditions\nunder which invariance spontaneously emerges.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:18:08 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 13:31:20 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zaidi", "Syed Suleman Abbas", ""], ["Boix", "Xavier", ""], ["Prasad", "Neeraj", ""], ["Gilad-Gutnick", "Sharon", ""], ["Ben-Ami", "Shlomit", ""], ["Sinha", "Pawan", ""]]}, {"id": "2007.00113", "submitter": "Zhe Huang", "authors": "Zhe Huang, Aamir Hasan, Kazuki Shin, Ruohua Li, and Katherine\n  Driggs-Campbell", "title": "Long-term Pedestrian Trajectory Prediction using Mutable Intention\n  Filter and Warp LSTM", "comments": "Accepted by RA-L Special Issue on Long-Term Human Motion Prediction", "journal-ref": null, "doi": "10.1109/LRA.2020.3047731", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is one of the key capabilities for robots to safely\nnavigate and interact with pedestrians. Critical insights from human intention\nand behavioral patterns need to be integrated to effectively forecast long-term\npedestrian behavior. Thus, we propose a framework incorporating a Mutable\nIntention Filter and a Warp LSTM (MIF-WLSTM) to simultaneously estimate human\nintention and perform trajectory prediction. The Mutable Intention Filter is\ninspired by particle filtering and genetic algorithms, where particles\nrepresent intention hypotheses that can be mutated throughout the pedestrian\nmotion. Instead of predicting sequential displacement over time, our Warp LSTM\nlearns to generate offsets on a full trajectory predicted by a nominal\nintention-aware linear model, which considers the intention hypotheses during\nfiltering process. Through experiments on a publicly available dataset, we show\nthat our method outperforms baseline approaches and demonstrate the robust\nperformance of our method under abnormal intention-changing scenarios. Code is\navailable at https://github.com/tedhuang96/mifwlstm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:23:00 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 22:38:22 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 01:33:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huang", "Zhe", ""], ["Hasan", "Aamir", ""], ["Shin", "Kazuki", ""], ["Li", "Ruohua", ""], ["Driggs-Campbell", "Katherine", ""]]}, {"id": "2007.00114", "submitter": "Oc\\'eane Boulais", "authors": "Oc\\'eane Boulais, Ben Woodward, Brian Schlining, Lonny Lundsten, Kevin\n  Barnard, Katy Croff Bell, and Kakani Katija", "title": "FathomNet: An underwater image training database for ocean exploration\n  and discovery", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Thousands of hours of marine video data are collected annually from remotely\noperated vehicles (ROVs) and other underwater assets. However, current manual\nmethods of analysis impede the full utilization of collected data for real time\nalgorithms for ROV and large biodiversity analyses. FathomNet is a novel\nbaseline image training set, optimized to accelerate development of modern,\nintelligent, and automated analysis of underwater imagery. Our seed data set\nconsists of an expertly annotated and continuously maintained database with\nmore than 26,000 hours of videotape, 6.8 million annotations, and 4,349 terms\nin the knowledge base. FathomNet leverages this data set by providing imagery,\nlocalizations, and class labels of underwater concepts in order to enable\nmachine learning algorithm development. To date, there are more than 80,000\nimages and 106,000 localizations for 233 different classes, including midwater\nand benthic organisms. Our experiments consisted of training various deep\nlearning algorithms with approaches to address weakly supervised localization,\nimage labeling, object detection and classification which prove to be\npromising. While we find quality results on prediction for this new dataset,\nour results indicate that we are ultimately in need of a larger data set for\nocean exploration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:23:06 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:09:24 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 04:14:21 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Boulais", "Oc\u00e9ane", ""], ["Woodward", "Ben", ""], ["Schlining", "Brian", ""], ["Lundsten", "Lonny", ""], ["Barnard", "Kevin", ""], ["Bell", "Katy Croff", ""], ["Katija", "Kakani", ""]]}, {"id": "2007.00121", "submitter": "Elena Kaye", "authors": "Elena A. Kaye, Emily A. Aherne, Cihan Duzgol, Ida H\\\"aggstr\\\"om, Erich\n  Kobler, Yousef Mazaheri, Maggie M Fung, Zhigang Zhang, Ricardo Otazo, Herbert\n  A. Vargas, Oguz Akin", "title": "Accelerating Prostate Diffusion Weighted MRI using Guided Denoising\n  Convolutional Neural Network: Retrospective Feasibility Study", "comments": "This manuscript has been accepted for publication in Radiology:\n  Artificial Intelligence (https://pubs.rsna.org/journal/ai), which is\n  published by the Radiological Society of North America (RSNA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To investigate feasibility of accelerating prostate\ndiffusion-weighted imaging (DWI) by reducing the number of acquired averages\nand denoising the resulting image using a proposed guided denoising\nconvolutional neural network (DnCNN). Materials and Methods: Raw data from the\nprostate DWI scans were retrospectively gathered (between July 2018 and July\n2019) from six single-vendor MRI scanners. 118 data sets were used for training\nand validation (age: 64.3 +- 8 years) and 37 - for testing (age: 65.1 +- 7.3\nyears). High b-value diffusion-weighted (hb-DW) data were reconstructed into\nnoisy images using two averages and reference images using all sixteen\naverages. A conventional DnCNN was modified into a guided DnCNN, which uses the\nlow b-value DWI image as a guidance input. Quantitative and qualitative reader\nevaluations were performed on the denoised hb-DW images. A cumulative link\nmixed regression model was used to compare the readers scores. The agreement\nbetween the apparent diffusion coefficient (ADC) maps (denoised vs reference)\nwas analyzed using Bland Altman analysis. Results: Compared to the DnCNN, the\nguided DnCNN produced denoised hb-DW images with higher peak signal-to-noise\nratio and structural similarity index and lower normalized mean square error (p\n< 0.001). Compared to the reference images, the denoised images received higher\nimage quality scores (p < 0.0001). The ADC values based on the denoised hb-DW\nimages were in good agreement with the reference ADC values. Conclusion:\nAccelerating prostate DWI by reducing the number of acquired averages and\ndenoising the resulting image using the proposed guided DnCNN is technically\nfeasible.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:44:42 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kaye", "Elena A.", ""], ["Aherne", "Emily A.", ""], ["Duzgol", "Cihan", ""], ["H\u00e4ggstr\u00f6m", "Ida", ""], ["Kobler", "Erich", ""], ["Mazaheri", "Yousef", ""], ["Fung", "Maggie M", ""], ["Zhang", "Zhigang", ""], ["Otazo", "Ricardo", ""], ["Vargas", "Herbert A.", ""], ["Akin", "Oguz", ""]]}, {"id": "2007.00145", "submitter": "Eric Dodds", "authors": "Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, Kofi Boakye", "title": "Modality-Agnostic Attention Fusion for visual search with text feedback", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval with natural language feedback offers the promise of catalog\nsearch based on fine-grained visual features that go beyond objects and binary\nattributes, facilitating real-world applications such as e-commerce. Our\nModality-Agnostic Attention Fusion (MAAF) model combines image and text\nfeatures and outperforms existing approaches on two visual search with\nmodifying phrase datasets, Fashion IQ and CSS, and performs competitively on a\ndataset with only single-word modifications, Fashion200k. We also introduce two\nnew challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which\nprovide new settings with rich language inputs, and we show that our approach\nwithout modification outperforms strong baselines. To better understand our\nmodel, we conduct detailed ablations on Fashion IQ and provide visualizations\nof the surprising phenomenon of words avoiding \"attending\" to the image region\nthey refer to.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 22:55:02 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Dodds", "Eric", ""], ["Culpepper", "Jack", ""], ["Herdade", "Simao", ""], ["Zhang", "Yang", ""], ["Boakye", "Kofi", ""]]}, {"id": "2007.00146", "submitter": "Aminollah Khormali", "authors": "Aminollah Khormali, DaeHun Nyang, David Mohaisen", "title": "Generating Adversarial Examples with an Optimized Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are widely used in a range of application areas, such as\ncomputer vision, computer security, etc. However, deep learning models are\nvulnerable to Adversarial Examples (AEs),carefully crafted samples to deceive\nthose models. Recent studies have introduced new adversarial attack methods,\nbut, to the best of our knowledge, none provided guaranteed quality for the\ncrafted examples as part of their creation, beyond simple quality measures such\nas Misclassification Rate (MR). In this paper, we incorporateImage Quality\nAssessment (IQA) metrics into the design and generation process of AEs. We\npropose an evolutionary-based single- and multi-objective optimization\napproaches that generate AEs with high misclassification rate and explicitly\nimprove the quality, thus indistinguishability, of the samples, while\nperturbing only a limited number of pixels. In particular, several IQA metrics,\nincluding edge analysis, Fourier analysis, and feature descriptors, are\nleveraged into the process of generating AEs. Unique characteristics of the\nevolutionary-based algorithm enable us to simultaneously optimize the\nmisclassification rate and the IQA metrics of the AEs. In order to evaluate the\nperformance of the proposed method, we conduct intensive experiments on\ndifferent well-known benchmark datasets(MNIST, CIFAR, GTSRB, and Open Image\nDataset V5), while considering various objective optimization configurations.\nThe results obtained from our experiments, when compared with the exist-ing\nattack methods, validate our initial hypothesis that the use ofIQA metrics\nwithin generation process of AEs can substantially improve their quality, while\nmaintaining high misclassification rate.Finally, transferability and human\nperception studies are provided, demonstrating acceptable performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:05:12 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Khormali", "Aminollah", ""], ["Nyang", "DaeHun", ""], ["Mohaisen", "David", ""]]}, {"id": "2007.00151", "submitter": "Sheng Liu", "authors": "Sheng Liu, Jonathan Niles-Weed, Narges Razavian, Carlos\n  Fernandez-Granda", "title": "Early-Learning Regularization Prevents Memorization of Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to perform classification via deep learning in\nthe presence of noisy annotations. When trained on noisy labels, deep neural\nnetworks have been observed to first fit the training data with clean labels\nduring an \"early learning\" phase, before eventually memorizing the examples\nwith false labels. We prove that early learning and memorization are\nfundamental phenomena in high-dimensional classification tasks, even in simple\nlinear models, and give a theoretical explanation in this setting. Motivated by\nthese findings, we develop a new technique for noisy classification tasks,\nwhich exploits the progress of the early learning phase. In contrast with\nexisting approaches, which use the model output during early learning to detect\nthe examples with clean labels, and either ignore or attempt to correct the\nfalse labels, we take a different route and instead capitalize on early\nlearning via regularization. There are two key elements to our approach. First,\nwe leverage semi-supervised learning techniques to produce target probabilities\nbased on the model outputs. Second, we design a regularization term that steers\nthe model towards these targets, implicitly preventing memorization of the\nfalse labels. The resulting framework is shown to provide robustness to noisy\nannotations on several standard benchmarks and real-world datasets, where it\nachieves results comparable to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:46:33 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:18:22 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Liu", "Sheng", ""], ["Niles-Weed", "Jonathan", ""], ["Razavian", "Narges", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "2007.00164", "submitter": "Ransalu Senanayake", "authors": "Anthony Tompkins, Ransalu Senanayake, and Fabio Ramos", "title": "Online Domain Adaptation for Occupancy Mapping", "comments": "Robotics: Science and Systems (RSS) 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating accurate spatial representations that take into account uncertainty\nis critical for autonomous robots to safely navigate in unstructured\nenvironments. Although recent LIDAR based mapping techniques can produce robust\noccupancy maps, learning the parameters of such models demand considerable\ncomputational time, discouraging them from being used in real-time and\nlarge-scale applications such as autonomous driving. Recognizing the fact that\nreal-world structures exhibit similar geometric features across a variety of\nurban environments, in this paper, we argue that it is redundant to learn all\ngeometry dependent parameters from scratch. Instead, we propose a theoretical\nframework building upon the theory of optimal transport to adapt model\nparameters to account for changes in the environment, significantly amortizing\nthe training cost. Further, with the use of high-fidelity driving simulators\nand real-world datasets, we demonstrate how parameters of 2D and 3D occupancy\nmaps can be automatically adapted to accord with local spatial changes. We\nvalidate various domain adaptation paradigms through a series of experiments,\nranging from inter-domain feature transfer to simulation-to-real-world feature\ntransfer. Experiments verified the possibility of estimating parameters with a\nnegligible computational and memory cost, enabling large-scale probabilistic\nmapping in urban environments.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 00:46:51 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Tompkins", "Anthony", ""], ["Senanayake", "Ransalu", ""], ["Ramos", "Fabio", ""]]}, {"id": "2007.00166", "submitter": "Siddhant Bansal", "authors": "Siddhant Bansal, Praveen Krishnan, C.V. Jawahar", "title": "Fused Text Recogniser and Deep Embeddings Improve Word Recognition and\n  Retrieval", "comments": "15 pages, 8 figures, Accepted in IAPR International Workshop on\n  Document Analysis Systems (DAS) 2020, \"Visit project page, at\n  http://cvit.iiit.ac.in/research/projects/cvit-projects/fused-text-recogniser-and-deep-embeddings-improve-word-recognition-and-retrieval\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition and retrieval of textual content from the large document\ncollections have been a powerful use case for the document image analysis\ncommunity. Often the word is the basic unit for recognition as well as\nretrieval. Systems that rely only on the text recogniser (OCR) output are not\nrobust enough in many situations, especially when the word recognition rates\nare poor, as in the case of historic documents or digital libraries. An\nalternative has been word spotting based methods that retrieve/match words\nbased on a holistic representation of the word. In this paper, we fuse the\nnoisy output of text recogniser with a deep embeddings representation derived\nout of the entire word. We use average and max fusion for improving the ranked\nresults in the case of retrieval. We validate our methods on a collection of\nHindi documents. We improve word recognition rate by 1.4 and retrieval by 11.13\nin the mAP.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 00:55:34 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bansal", "Siddhant", ""], ["Krishnan", "Praveen", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2007.00199", "submitter": "Meng Chang", "authors": "Meng Chang, Huajun Feng, Zhihai Xu, Qi Li", "title": "Low-light Image Restoration with Short- and Long-exposure Raw Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light imaging with handheld mobile devices is a challenging issue.\nLimited by the existing models and training data, most existing methods cannot\nbe effectively applied in real scenarios. In this paper, we propose a new\nlow-light image restoration method by using the complementary information of\nshort- and long-exposure images. We first propose a novel data generation\nmethod to synthesize realistic short- and longexposure raw images by simulating\nthe imaging pipeline in lowlight environment. Then, we design a new\nlong-short-exposure fusion network (LSFNet) to deal with the problems of\nlow-light image fusion, including high noise, motion blur, color distortion and\nmisalignment. The proposed LSFNet takes pairs of shortand long-exposure raw\nimages as input, and outputs a clear RGB image. Using our data generation\nmethod and the proposed LSFNet, we can recover the details and color of the\noriginal scene, and improve the low-light image quality effectively.\nExperiments demonstrate that our method can outperform the state-of-the art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:22:26 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 07:42:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chang", "Meng", ""], ["Feng", "Huajun", ""], ["Xu", "Zhihai", ""], ["Li", "Qi", ""]]}, {"id": "2007.00229", "submitter": "Xin Eric Wang", "authors": "Wanrong Zhu, Xin Eric Wang, Tsu-Jui Fu, An Yan, Pradyumna Narayana,\n  Kazoo Sone, Sugato Basu, William Yang Wang", "title": "Multimodal Text Style Transfer for Outdoor Vision-and-Language\n  Navigation", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging topics in Natural Language Processing (NLP) is\nvisually-grounded language understanding and reasoning. Outdoor\nvision-and-language navigation (VLN) is such a task where an agent follows\nnatural language instructions and navigates a real-life urban environment. Due\nto the lack of human-annotated instructions that illustrate intricate urban\nscenes, outdoor VLN remains a challenging task to solve. This paper introduces\na Multimodal Text Style Transfer (MTST) learning approach and leverages\nexternal multimodal resources to mitigate data scarcity in outdoor navigation\ntasks. We first enrich the navigation data by transferring the style of the\ninstructions generated by Google Maps API, then pre-train the navigator with\nthe augmented external outdoor navigation dataset. Experimental results show\nthat our MTST learning approach is model-agnostic, and our MTST approach\nsignificantly outperforms the baseline models on the outdoor VLN task,\nimproving task completion rate by 8.7% relatively on the test set.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:29:07 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:43:58 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 04:48:23 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhu", "Wanrong", ""], ["Wang", "Xin Eric", ""], ["Fu", "Tsu-Jui", ""], ["Yan", "An", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""], ["Wang", "William Yang", ""]]}, {"id": "2007.00243", "submitter": "Chaoyi Zhang", "authors": "Tiange Xiang, Chaoyi Zhang, Dongnan Liu, Yang Song, Heng Huang,\n  Weidong Cai", "title": "BiO-Net: Learning Recurrent Bi-directional Connections for\n  Encoder-Decoder Architecture", "comments": "10 pages, 4 figures, MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-Net has become one of the state-of-the-art deep learning-based approaches\nfor modern computer vision tasks such as semantic segmentation, super\nresolution, image denoising, and inpainting. Previous extensions of U-Net have\nfocused mainly on the modification of its existing building blocks or the\ndevelopment of new functional modules for performance gains. As a result, these\nvariants usually lead to an unneglectable increase in model complexity. To\ntackle this issue in such U-Net variants, in this paper, we present a novel\nBi-directional O-shape network (BiO-Net) that reuses the building blocks in a\nrecurrent manner without introducing any extra parameters. Our proposed\nbi-directional skip connections can be directly adopted into any\nencoder-decoder architecture to further enhance its capabilities in various\ntask domains. We evaluated our method on various medical image analysis tasks\nand the results show that our BiO-Net significantly outperforms the vanilla\nU-Net as well as other state-of-the-art methods. Our code is available at\nhttps://github.com/tiangexiang/BiO-Net.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 05:07:49 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 00:31:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Xiang", "Tiange", ""], ["Zhang", "Chaoyi", ""], ["Liu", "Dongnan", ""], ["Song", "Yang", ""], ["Huang", "Heng", ""], ["Cai", "Weidong", ""]]}, {"id": "2007.00265", "submitter": "Tianyi Liang", "authors": "Tianyi Liang, Long Lan, Zhigang Luo", "title": "Enhancing the Association in Multi-Object Tracking via Neighbor Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern multi-object tracking (MOT) systems follow the\ntracking-by-detection paradigm. It first localizes the objects of interest,\nthen extracting their individual appearance features to make data association.\nThe individual features, however, are susceptible to the negative effects as\nocclusions, illumination variations and inaccurate detections, thus resulting\nin the mismatch in the association inference. In this work, we propose to\nhandle this problem via making full use of the neighboring information. Our\nmotivations derive from the observations that people tend to move in a group.\nAs such, when an individual target's appearance is seriously changed, we can\nstill identify it with the help of its neighbors. To this end, we first utilize\nthe spatio-temporal relations produced by the tracking self to efficiently\nselect suitable neighbors for the targets. Subsequently, we construct neighbor\ngraph of the target and neighbors then employ the graph convolution networks\n(GCN) to learn the graph features. To the best of our knowledge, it is the\nfirst time to exploit neighbor cues via GCN in MOT. Finally, we test our\napproach on the MOT benchmarks and achieve state-of-the-art performance in\nonline tracking.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 06:21:31 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Liang", "Tianyi", ""], ["Lan", "Long", ""], ["Luo", "Zhigang", ""]]}, {"id": "2007.00290", "submitter": "Andreas Pfeuffer", "authors": "Andreas Pfeuffer and Klaus Dietmayer", "title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of\n  Fast Video-Sequence Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision tasks such as semantic segmentation perform very well in good\nweather conditions, but if the weather turns bad, they have problems to achieve\nthis performance in these conditions. One possibility to obtain more robust and\nreliable results in adverse weather conditions is to use video-segmentation\napproaches instead of commonly used single-image segmentation methods.\nVideo-segmentation approaches capture temporal information of the previous\nvideo-frames in addition to current image information, and hence, they are more\nrobust against disturbances, especially if they occur in only a few frames of\nthe video-sequence. However, video-segmentation approaches, which are often\nbased on recurrent neural networks, cannot be applied in real-time applications\nanymore, since their recurrent structures in the network are computational\nexpensive. For instance, the inference time of the LSTM-ICNet, in which\nrecurrent units are placed at proper positions in the single-segmentation\napproach ICNet, increases up to 61 percent compared to the basic ICNet. Hence,\nin this work, the LSTM-ICNet is sped up by modifying the recurrent units of the\nnetwork so that it becomes real-time capable again. Experiments on different\ndatasets and various weather conditions show that the inference time can be\ndecreased by about 23 percent by these modifications, while they achieve\nsimilar performance than the LSTM-ICNet and outperform the single-segmentation\napproach enormously in adverse weather conditions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:29:35 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Pfeuffer", "Andreas", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2007.00291", "submitter": "Max Argus", "authors": "Max Argus and Lukas Hermann and Jon Long and Thomas Brox", "title": "FlowControl: Optical Flow Based Visual Servoing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot imitation is the vision of robot programming from a single\ndemonstration, rather than by tedious construction of computer code. We present\na practical method for realizing one-shot imitation for manipulation tasks,\nexploiting modern learning-based optical flow to perform real-time visual\nservoing. Our approach, which we call FlowControl, continuously tracks a\ndemonstration video, using a specified foreground mask to attend to an object\nof interest. Using RGB-D observations, FlowControl requires no 3D object\nmodels, and is easy to set up. FlowControl inherits great robustness to visual\nappearance from decades of work in optical flow. We exhibit FlowControl on a\nrange of problems, including ones requiring very precise motions, and ones\nrequiring the ability to generalize.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:32:00 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Argus", "Max", ""], ["Hermann", "Lukas", ""], ["Long", "Jon", ""], ["Brox", "Thomas", ""]]}, {"id": "2007.00302", "submitter": "Miguel de Prado", "authors": "Miguel de Prado, Manuele Rusci, Romain Donze, Alessandro Capotondi,\n  Serge Monnerat, Luca Benini and, Nuria Pazos", "title": "Robustifying the Deployment of tinyML Models for Autonomous\n  mini-vehicles", "comments": null, "journal-ref": null, "doi": "10.1109/ISCAS51556.2021.9401154", "report-no": null, "categories": "cs.CV cs.LG cs.RO cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard-size autonomous navigation vehicles have rapidly improved thanks to\nthe breakthroughs of deep learning. However, scaling autonomous driving to\nlow-power systems deployed on dynamic environments poses several challenges\nthat prevent their adoption. To address them, we propose a closed-loop learning\nflow for autonomous driving mini-vehicles that includes the target environment\nin-the-loop. We leverage a family of compact and high-throughput tinyCNNs to\ncontrol the mini-vehicle, which learn in the target environment by imitating a\ncomputer vision algorithm, i.e., the expert. Thus, the tinyCNNs, having only\naccess to an on-board fast-rate linear camera, gain robustness to lighting\nconditions and improve over time. Further, we leverage GAP8, a parallel\nultra-low-power RISC-V SoC, to meet the inference requirements. When running\nthe family of CNNs, our GAP8's solution outperforms any other implementation on\nthe STM32L4 and NXP k64f (Cortex-M4), reducing the latency by over 13x and the\nenergy consummation by 92%.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:54:26 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 20:38:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["de Prado", "Miguel", ""], ["Rusci", "Manuele", ""], ["Donze", "Romain", ""], ["Capotondi", "Alessandro", ""], ["Monnerat", "Serge", ""], ["and", "Luca Benini", ""], ["Pazos", "Nuria", ""]]}, {"id": "2007.00311", "submitter": "Guillaume Jaume", "authors": "Guillaume Jaume, Pushpak Pati, Antonio Foncubierta-Rodriguez, Florinda\n  Feroce, Giosue Scognamiglio, Anna Maria Anniciello, Jean-Philippe Thiran,\n  Orcun Goksel, Maria Gabrani", "title": "Towards Explainable Graph Representations in Digital Pathology", "comments": "ICML'20 workshop on Computational Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability of machine learning (ML) techniques in digital pathology (DP)\nis of great significance to facilitate their wide adoption in clinics.\nRecently, graph techniques encoding relevant biological entities have been\nemployed to represent and assess DP images. Such paradigm shift from pixel-wise\nto entity-wise analysis provides more control over concept representation. In\nthis paper, we introduce a post-hoc explainer to derive compact per-instance\nexplanations emphasizing diagnostically important entities in the graph.\nAlthough we focus our analyses to cells and cellular interactions in breast\ncancer subtyping, the proposed explainer is generic enough to be extended to\nother topological representations in DP. Qualitative and quantitative analyses\ndemonstrate the efficacy of the explainer in generating comprehensive and\ncompact explanations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:05:26 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Jaume", "Guillaume", ""], ["Pati", "Pushpak", ""], ["Foncubierta-Rodriguez", "Antonio", ""], ["Feroce", "Florinda", ""], ["Scognamiglio", "Giosue", ""], ["Anniciello", "Anna Maria", ""], ["Thiran", "Jean-Philippe", ""], ["Goksel", "Orcun", ""], ["Gabrani", "Maria", ""]]}, {"id": "2007.00323", "submitter": "Alessandro Simoni", "authors": "Alessandro Simoni and Luca Bergamini and Andrea Palazzi and Simone\n  Calderara and Rita Cucchiara", "title": "Future Urban Scenes Generation Through Vehicles Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a deep learning pipeline to predict the visual future\nappearance of an urban scene. Despite recent advances, generating the entire\nscene in an end-to-end fashion is still far from being achieved. Instead, here\nwe follow a two stages approach, where interpretable information is included in\nthe loop and each actor is modelled independently. We leverage a per-object\nnovel view synthesis paradigm; i.e. generating a synthetic representation of an\nobject undergoing a geometrical roto-translation in the 3D space. Our model can\nbe easily conditioned with constraints (e.g. input trajectories) provided by\nstate-of-the-art tracking methods or by the user itself. This allows us to\ngenerate a set of diverse realistic futures starting from the same input in a\nmulti-modal fashion. We visually and quantitatively show the superiority of\nthis approach over traditional end-to-end scene-generation methods on CityFlow,\na challenging real world dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:40:16 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:08:20 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Simoni", "Alessandro", ""], ["Bergamini", "Luca", ""], ["Palazzi", "Andrea", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2007.00328", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu, Tariq Durrani", "title": "NestFuse: An Infrared and Visible Image Fusion Architecture based on\n  Nest Connection and Spatial/Channel Attention Models", "comments": "12 pages, 13 figures, 6 tables. IEEE Transactions on Instrumentation\n  and Measurement", "journal-ref": null, "doi": "10.1109/TIM.2020.3005230", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel method for infrared and visible image fusion\nwhere we develop nest connection-based network and spatial/channel attention\nmodels. The nest connection-based network can preserve significant amounts of\ninformation from input data in a multi-scale perspective. The approach\ncomprises three key elements: encoder, fusion strategy and decoder\nrespectively. In our proposed fusion strategy, spatial attention models and\nchannel attention models are developed that describe the importance of each\nspatial position and of each channel with deep features. Firstly, the source\nimages are fed into the encoder to extract multi-scale deep features. The novel\nfusion strategy is then developed to fuse these features for each scale.\nFinally, the fused image is reconstructed by the nest connection-based decoder.\nExperiments are performed on publicly available datasets. These exhibit that\nour proposed approach has better fusion performance than other state-of-the-art\nmethods. This claim is justified through both subjective and objective\nevaluation. The code of our fusion method is available at\nhttps://github.com/hli1221/imagefusion-nestfuse\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:46:23 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 06:31:34 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""], ["Durrani", "Tariq", ""]]}, {"id": "2007.00337", "submitter": "Kishor Datta Gupta", "authors": "Kishor Datta Gupta, Zahid Akhtar, Dipankar Dasgupta", "title": "Determining Sequence of Image Processing Technique (IPT) to Detect\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing secure machine learning models from adversarial examples is\nchallenging as various methods are continually being developed to generate\nadversarial attacks. In this work, we propose an evolutionary approach to\nautomatically determine Image Processing Techniques Sequence (IPTS) for\ndetecting malicious inputs. Accordingly, we first used a diverse set of attack\nmethods including adaptive attack methods (on our defense) to generate\nadversarial samples from the clean dataset. A detection framework based on a\ngenetic algorithm (GA) is developed to find the optimal IPTS, where the\noptimality is estimated by different fitness measures such as Euclidean\ndistance, entropy loss, average histogram, local binary pattern and loss\nfunctions. The \"image difference\" between the original and processed images is\nused to extract the features, which are then fed to a classification scheme in\norder to determine whether the input sample is adversarial or clean. This paper\ndescribed our methodology and performed experiments using multiple data-sets\ntested with several adversarial attacks. For each attack-type and dataset, it\ngenerates unique IPTS. A set of IPTS selected dynamically in testing time which\nworks as a filter for the adversarial attack. Our empirical experiments\nexhibited promising results indicating the approach can efficiently be used as\nprocessing for any AI model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:59:14 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:26:57 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Gupta", "Kishor Datta", ""], ["Akhtar", "Zahid", ""], ["Dasgupta", "Dipankar", ""]]}, {"id": "2007.00384", "submitter": "Tasfia Shermin", "authors": "Tasfia Shermin, Guojun Lu, Shyh Wei Teng, Manzur Murshed, Ferdous\n  Sohel", "title": "Adversarial Network with Multiple Classifiers for Open Set Domain\n  Adaptation", "comments": "Accepted in IEEE Transactions on Multimedia (in press), 2020", "journal-ref": "IEEE Transactions on Multimedia, 2020 (CODE:\n  https://github.com/tasfia/DAMC)", "doi": "10.1109/TMM.2020.3016126", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to transfer knowledge from a domain with adequate\nlabeled samples to a domain with scarce labeled samples. Prior research has\nintroduced various open set domain adaptation settings in the literature to\nextend the applications of domain adaptation methods in real-world scenarios.\nThis paper focuses on the type of open set domain adaptation setting where the\ntarget domain has both private ('unknown classes') label space and the shared\n('known classes') label space. However, the source domain only has the 'known\nclasses' label space. Prevalent distribution-matching domain adaptation methods\nare inadequate in such a setting that demands adaptation from a smaller source\ndomain to a larger and diverse target domain with more classes. For addressing\nthis specific open set domain adaptation setting, prior research introduces a\ndomain adversarial model that uses a fixed threshold for distinguishing known\nfrom unknown target samples and lacks at handling negative transfers. We extend\ntheir adversarial model and propose a novel adversarial domain adaptation model\nwith multiple auxiliary classifiers. The proposed multi-classifier structure\nintroduces a weighting module that evaluates distinctive domain characteristics\nfor assigning the target samples with weights which are more representative to\nwhether they are likely to belong to the known and unknown classes to encourage\npositive transfers during adversarial training and simultaneously reduces the\ndomain gap between the shared classes of the source and target domains. A\nthorough experimental investigation shows that our proposed method outperforms\nexisting domain adaptation methods on a number of domain adaptation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:23:07 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 04:40:09 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 10:20:22 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Shermin", "Tasfia", ""], ["Lu", "Guojun", ""], ["Teng", "Shyh Wei", ""], ["Murshed", "Manzur", ""], ["Sohel", "Ferdous", ""]]}, {"id": "2007.00394", "submitter": "Yizhak Ben-Shabat", "authors": "Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell,\n  Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould", "title": "The IKEA ASM Dataset: Understanding People Assembling Furniture through\n  Actions, Objects and Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of a large labeled dataset is a key requirement for applying\ndeep learning methods to solve various computer vision tasks. In the context of\nunderstanding human activities, existing public datasets, while large in size,\nare often limited to a single RGB camera and provide only per-frame or per-clip\naction annotations. To enable richer analysis and understanding of human\nactivities, we introduce IKEA ASM---a three million frame, multi-view,\nfurniture assembly video dataset that includes depth, atomic actions, object\nsegmentation, and human pose. Additionally, we benchmark prominent methods for\nvideo action recognition, object segmentation and human pose estimation tasks\non this challenging dataset. The dataset enables the development of holistic\nmethods, which integrate multi-modal and multi-view data to better perform on\nthese tasks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:34:46 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Ben-Shabat", "Yizhak", ""], ["Yu", "Xin", ""], ["Saleh", "Fatemeh Sadat", ""], ["Campbell", "Dylan", ""], ["Rodriguez-Opazo", "Cristian", ""], ["Li", "Hongdong", ""], ["Gould", "Stephen", ""]]}, {"id": "2007.00398", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Dimosthenis Karatzas, C.V. Jawahar", "title": "DocVQA: A Dataset for VQA on Document Images", "comments": "accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset for Visual Question Answering (VQA) on document\nimages called DocVQA. The dataset consists of 50,000 questions defined on\n12,000+ document images. Detailed analysis of the dataset in comparison with\nsimilar datasets for VQA and reading comprehension is presented. We report\nseveral baseline results by adopting existing VQA and reading comprehension\nmodels. Although the existing models perform reasonably well on certain types\nof questions, there is large performance gap compared to human performance\n(94.36% accuracy). The models need to improve specifically on questions where\nunderstanding structure of the document is crucial. The dataset, code and\nleaderboard are available at docvqa.org\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:37:40 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 04:13:51 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 05:39:39 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Mathew", "Minesh", ""], ["Karatzas", "Dimosthenis", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2007.00453", "submitter": "Camila Gonzalez", "authors": "Karol Gotkowski, Camila Gonzalez, Andreas Bucher, Anirban Mukhopadhyay", "title": "M3d-CAM: A PyTorch library to generate 3D data attention maps for\n  medical deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M3d-CAM is an easy to use library for generating attention maps of CNN-based\nPyTorch models improving the interpretability of model predictions for humans.\nThe attention maps can be generated with multiple methods like Guided\nBackpropagation, Grad-CAM, Guided Grad-CAM and Grad-CAM++. These attention maps\nvisualize the regions in the input data that influenced the model prediction\nthe most at a certain layer. Furthermore, M3d-CAM supports 2D and 3D data for\nthe task of classification as well as for segmentation. A key feature is also\nthat in most cases only a single line of code is required for generating\nattention maps for a model making M3d-CAM basically plug and play.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:55:57 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Gotkowski", "Karol", ""], ["Gonzalez", "Camila", ""], ["Bucher", "Andreas", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2007.00477", "submitter": "Chong Li", "authors": "Zhun Fan and Chong Li and Ying Chen and Jiahong Wei and Giuseppe\n  Loprencipe and Xiaopeng Chen and Paola Di Mascio", "title": "Automatic Crack Detection on Road Pavements Using Encoder Decoder\n  Architecture", "comments": null, "journal-ref": null, "doi": "10.3390/ma13132960", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the development of deep learning in computer vision and object\ndetection, the proposed algorithm considers an encoder-decoder architecture\nwith hierarchical feature learning and dilated convolution, named\nU-Hierarchical Dilated Network (U-HDN), to perform crack detection in an\nend-to-end method. Crack characteristics with multiple context information are\nautomatically able to learn and perform end-to-end crack detection. Then, a\nmulti-dilation module embedded in an encoder-decoder architecture is proposed.\nThe crack features of multiple context sizes can be integrated into the\nmulti-dilation module by dilation convolution with different dilatation rates,\nwhich can obtain much more cracks information. Finally, the hierarchical\nfeature learning module is designed to obtain a multi-scale features from the\nhigh to low-level convolutional layers, which are integrated to predict\npixel-wise crack detection. Some experiments on public crack databases using\n118 images were performed and the results were compared with those obtained\nwith other methods on the same images. The results show that the proposed U-HDN\nmethod achieves high performance because it can extract and fuse different\ncontext sizes and different levels of feature maps than other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:32:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fan", "Zhun", ""], ["Li", "Chong", ""], ["Chen", "Ying", ""], ["Wei", "Jiahong", ""], ["Loprencipe", "Giuseppe", ""], ["Chen", "Xiaopeng", ""], ["Di Mascio", "Paola", ""]]}, {"id": "2007.00491", "submitter": "Tomasz Kryjak", "authors": "Dominika Przewlocka, Mateusz Wasala, Hubert Szolc, Krzysztof Blachut,\n  Tomasz Kryjak", "title": "Optimisation of a Siamese Neural Network for Real-Time Energy Efficient\n  Object Tracking", "comments": "12 pages, accepted for ICCVG 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59006-2_14", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the research on optimisation of visual object tracking using a\nSiamese neural network for embedded vision systems is presented. It was assumed\nthat the solution shall operate in real-time, preferably for a high resolution\nvideo stream, with the lowest possible energy consumption. To meet these\nrequirements, techniques such as the reduction of computational precision and\npruning were considered. Brevitas, a tool dedicated for optimisation and\nquantisation of neural networks for FPGA implementation, was used. A number of\ntraining scenarios were tested with varying levels of optimisations - from\ninteger uniform quantisation with 16 bits to ternary and binary networks. Next,\nthe influence of these optimisations on the tracking performance was evaluated.\nIt was possible to reduce the size of the convolutional filters up to 10 times\nin relation to the original network. The obtained results indicate that using\nquantisation can significantly reduce the memory and computational complexity\nof the proposed network while still enabling precise tracking, thus allow to\nuse it in embedded vision systems. Moreover, quantisation of weights positively\naffects the network training by decreasing overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:49:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Przewlocka", "Dominika", ""], ["Wasala", "Mateusz", ""], ["Szolc", "Hubert", ""], ["Blachut", "Krzysztof", ""], ["Kryjak", "Tomasz", ""]]}, {"id": "2007.00493", "submitter": "Tomasz Kryjak", "authors": "Joanna Stanisz, Konrad Lis, Tomasz Kryjak, Marek Gorgon", "title": "Optimisation of the PointPillars network for 3D object detection in\n  point clouds", "comments": "7 pages, 2 figures, submitted to SPA 2020 conference", "journal-ref": null, "doi": "10.23919/SPA50552.2020.9241265", "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our research on the optimisation of a deep neural\nnetwork for 3D object detection in a point cloud. Techniques like quantisation\nand pruning available in the Brevitas and PyTorch tools were used. We performed\nthe experiments for the PointPillars network, which offers a reasonable\ncompromise between detection accuracy and calculation complexity. The aim of\nthis work was to propose a variant of the network which we will ultimately\nimplement in an FPGA device. This will allow for real-time LiDAR data\nprocessing with low energy consumption. The obtained results indicate that even\na significant quantisation from 32-bit floating point to 2-bit integer in the\nmain part of the algorithm, results in 5%-9% decrease of the detection\naccuracy, while allowing for almost a 16-fold reduction in size of the model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:50:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Stanisz", "Joanna", ""], ["Lis", "Konrad", ""], ["Kryjak", "Tomasz", ""], ["Gorgon", "Marek", ""]]}, {"id": "2007.00515", "submitter": "Fengmao Lv", "authors": "Haiyang Liu, Yichen Wang, Jiayi Zhao, Guowu Yang, Fengmao Lv", "title": "Learning unbiased zero-shot semantic segmentation networks via\n  transductive transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation, which aims to acquire a detailed understanding of\nimages, is an essential issue in computer vision. However, in practical\nscenarios, new categories that are different from the categories in training\nusually appear. Since it is impractical to collect labeled data for all\ncategories, how to conduct zero-shot learning in semantic segmentation\nestablishes an important problem. Although the attribute embedding of\ncategories can promote effective knowledge transfer across different\ncategories, the prediction of segmentation network reveals obvious bias to seen\ncategories. In this paper, we propose an easy-to-implement transductive\napproach to alleviate the prediction bias in zero-shot semantic segmentation.\nOur method assumes that both the source images with full pixel-level labels and\nunlabeled target images are available during training. To be specific, the\nsource images are used to learn the relationship between visual images and\nsemantic embeddings, while the target images are used to alleviate the\nprediction bias towards seen categories. We conduct comprehensive experiments\non diverse split s of the PASCAL dataset. The experimental results clearly\ndemonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:25:13 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Liu", "Haiyang", ""], ["Wang", "Yichen", ""], ["Zhao", "Jiayi", ""], ["Yang", "Guowu", ""], ["Lv", "Fengmao", ""]]}, {"id": "2007.00525", "submitter": "Jun Ma", "authors": "Jun Ma, Dong Wang, Xiao-Ping Wang, Xiaoping Yang", "title": "A Characteristic Function-based Algorithm for Geodesic Active Contours", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Active contour models have been widely used in image segmentation, and the\nlevel set method (LSM) is the most popular approach for solving the models, via\nimplicitly representing the contour by a level set function. However, the LSM\nsuffers from high computational burden and numerical instability, requiring\nadditional regularization terms or re-initialization techniques. In this paper,\nwe use characteristic functions to implicitly represent the contours, propose a\nnew representation to the geodesic active contours and derive an efficient\nalgorithm termed as the iterative convolution-thresholding method (ICTM).\nCompared to the LSM, the ICTM is simpler and much more efficient. In addition,\nthe ICTM enjoys most desired features of the level set-based methods. Extensive\nexperiments, on 2D synthetic, 2D ultrasound, 3D CT, and 3D MR images for\nnodule, organ and lesion segmentation, demonstrate that the proposed method not\nonly obtains comparable or even better segmentation results (compared to the\nLSM) but also achieves significant acceleration.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:39:14 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 14:35:09 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ma", "Jun", ""], ["Wang", "Dong", ""], ["Wang", "Xiao-Ping", ""], ["Yang", "Xiaoping", ""]]}, {"id": "2007.00548", "submitter": "Dominik Rivoir", "authors": "Dominik Rivoir and Sebastian Bodenstedt and Isabel Funke and Felix von\n  Bechtolsheim and Marius Distler and J\\\"urgen Weitz and Stefanie Speidel", "title": "Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse\n  Surgical Instrument Usage for Context-aware Assistance", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-operative anticipation of instrument usage is a necessary component for\ncontext-aware assistance in surgery, e.g. for instrument preparation or\nsemi-automation of robotic tasks. However, the sparsity of instrument\noccurrences in long videos poses a challenge. Current approaches are limited as\nthey assume knowledge on the timing of future actions or require dense temporal\nsegmentations during training and inference. We propose a novel learning task\nfor anticipation of instrument usage in laparoscopic videos that overcomes\nthese limitations. During training, only sparse instrument annotations are\nrequired and inference is done solely on image data. We train a probabilistic\nmodel to address the uncertainty associated with future events. Our approach\noutperforms several baselines and is competitive to a variant using richer\nannotations. We demonstrate the model's ability to quantify task-relevant\nuncertainties. To the best of our knowledge, we are the first to propose a\nmethod for anticipating instruments in surgery.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:17:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 10:53:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Rivoir", "Dominik", ""], ["Bodenstedt", "Sebastian", ""], ["Funke", "Isabel", ""], ["von Bechtolsheim", "Felix", ""], ["Distler", "Marius", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "2007.00558", "submitter": "Daniel Berj\\'on", "authors": "Pablo Carballeira, Carlos Carmona, C\\'esar D\\'iaz, Daniel Berj\\'on,\n  Daniel Corregidor, Juli\\'an Cabrera, Francisco Mor\\'an, Carmen Doblado,\n  Sergio Arnaldo, Mar\\'ia del Mar Mart\\'in, Narciso Garc\\'ia", "title": "FVV Live: A real-time free-viewpoint video system with consumer\n  electronics hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FVV Live is a novel end-to-end free-viewpoint video system, designed for low\ncost and real-time operation, based on off-the-shelf components. The system has\nbeen designed to yield high-quality free-viewpoint video using consumer-grade\ncameras and hardware, which enables low deployment costs and easy installation\nfor immersive event-broadcasting or videoconferencing.\n  The paper describes the architecture of the system, including acquisition and\nencoding of multiview plus depth data in several capture servers and virtual\nview synthesis on an edge server. All the blocks of the system have been\ndesigned to overcome the limitations imposed by hardware and network, which\nimpact directly on the accuracy of depth data and thus on the quality of\nvirtual view synthesis. The design of FVV Live allows for an arbitrary number\nof cameras and capture servers, and the results presented in this paper\ncorrespond to an implementation with nine stereo-based depth cameras.\n  FVV Live presents low motion-to-photon and end-to-end delays, which enables\nseamless free-viewpoint navigation and bilateral immersive communications.\nMoreover, the visual quality of FVV Live has been assessed through subjective\nassessment with satisfactory results, and additional comparative tests show\nthat it is preferred over state-of-the-art DIBR alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:40:28 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Carballeira", "Pablo", ""], ["Carmona", "Carlos", ""], ["D\u00edaz", "C\u00e9sar", ""], ["Berj\u00f3n", "Daniel", ""], ["Corregidor", "Daniel", ""], ["Cabrera", "Juli\u00e1n", ""], ["Mor\u00e1n", "Francisco", ""], ["Doblado", "Carmen", ""], ["Arnaldo", "Sergio", ""], ["Mart\u00edn", "Mar\u00eda del Mar", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "2007.00584", "submitter": "Guillaume Jaume", "authors": "Pushpak Pati, Guillaume Jaume, Lauren Alisha Fernandes, Antonio\n  Foncubierta, Florinda Feroce, Anna Maria Anniciello, Giosue Scognamiglio,\n  Nadia Brancati, Daniel Riccio, Maurizio Do Bonito, Giuseppe De Pietro,\n  Gerardo Botti, Orcun Goksel, Jean-Philippe Thiran, Maria Frucci, Maria\n  Gabrani", "title": "HACT-Net: A Hierarchical Cell-to-Tissue Graph Neural Network for\n  Histopathological Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer diagnosis, prognosis, and therapeutic response prediction are heavily\ninfluenced by the relationship between the histopathological structures and the\nfunction of the tissue. Recent approaches acknowledging the structure-function\nrelationship, have linked the structural and spatial patterns of cell\norganization in tissue via cell-graphs to tumor grades. Though cell\norganization is imperative, it is insufficient to entirely represent the\nhistopathological structure. We propose a novel hierarchical\ncell-to-tissue-graph (HACT) representation to improve the structural depiction\nof the tissue. It consists of a low-level cell-graph, capturing cell morphology\nand interactions, a high-level tissue-graph, capturing morphology and spatial\ndistribution of tissue parts, and cells-to-tissue hierarchies, encoding the\nrelative spatial distribution of the cells with respect to the tissue\ndistribution. Further, a hierarchical graph neural network (HACT-Net) is\nproposed to efficiently map the HACT representations to histopathological\nbreast cancer subtypes. We assess the methodology on a large set of annotated\ntissue regions of interest from H\\&E stained breast carcinoma whole-slides.\nUpon evaluation, the proposed method outperformed recent convolutional neural\nnetwork and graph neural network approaches for breast cancer multi-class\nsubtyping. The proposed entity-based topological analysis is more inline with\nthe pathological diagnostic procedure of the tissue. It provides more command\nover the tissue modelling, therefore encourages the further inclusion of\npathological priors into task-specific tissue representation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:22:48 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Pati", "Pushpak", ""], ["Jaume", "Guillaume", ""], ["Fernandes", "Lauren Alisha", ""], ["Foncubierta", "Antonio", ""], ["Feroce", "Florinda", ""], ["Anniciello", "Anna Maria", ""], ["Scognamiglio", "Giosue", ""], ["Brancati", "Nadia", ""], ["Riccio", "Daniel", ""], ["Bonito", "Maurizio Do", ""], ["De Pietro", "Giuseppe", ""], ["Botti", "Gerardo", ""], ["Goksel", "Orcun", ""], ["Thiran", "Jean-Philippe", ""], ["Frucci", "Maria", ""], ["Gabrani", "Maria", ""]]}, {"id": "2007.00586", "submitter": "Vivien Sainte Fare Garnot", "authors": "Vivien Sainte Fare Garnot and Loic Landrieu", "title": "Lightweight Temporal Self-Attention for Classifying Satellite Image Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing accessibility and precision of Earth observation satellite\ndata offers considerable opportunities for industrial and state actors alike.\nThis calls however for efficient methods able to process time-series on a\nglobal scale. Building on recent work employing multi-headed self-attention\nmechanisms to classify remote sensing time sequences, we propose a modification\nof the Temporal Attention Encoder. In our network, the channels of the temporal\ninputs are distributed among several compact attention heads operating in\nparallel. Each head extracts highly-specialized temporal features which are in\nturn concatenated into a single representation. Our approach outperforms other\nstate-of-the-art time series classification algorithms on an open-access\nsatellite image dataset, while using significantly fewer parameters and with a\nreduced computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:23:28 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 13:14:23 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 13:36:34 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Garnot", "Vivien Sainte Fare", ""], ["Landrieu", "Loic", ""]]}, {"id": "2007.00596", "submitter": "Fan Chen", "authors": "Fan Chen and Karl Rohe", "title": "A New Basis for Sparse PCA", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The statistical and computational performance of sparse principal component\nanalysis (PCA) can be dramatically improved when the principal components are\nallowed to be sparse in a rotated eigenbasis. For this, we propose a new method\nfor sparse PCA. In the simplest version of the algorithm, the component scores\nand loadings are initialized with a low-rank singular value decomposition.\nThen, the singular vectors are rotated with orthogonal rotations to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nsingular vectors. This approach differs from prior approaches because it uses\nan orthogonal rotation to approximate a sparse basis. Our sparse PCA framework\nis versatile; for example, it extends naturally to the two-way analysis of a\ndata matrix for simultaneous dimensionality reduction of rows and columns. We\nidentify the close relationship between sparse PCA and independent component\nanalysis for separating sparse signals. We provide empirical evidence showing\nthat for the same level of sparsity, the proposed sparse PCA method is more\nstable and can explain more variance compared to alternative methods. Through\nthree applications---sparse coding of images, analysis of transcriptome\nsequencing data, and large-scale clustering of Twitter accounts, we demonstrate\nthe usefulness of sparse PCA in exploring modern multivariate data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:32:22 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Rohe", "Karl", ""]]}, {"id": "2007.00603", "submitter": "Christina Runkel", "authors": "Christina Runkel, Stefan Dorenkamp, Hartmut Bauermeister, Michael\n  Moeller", "title": "Exploiting the Logits: Joint Sign Language Recognition and\n  Spell-Correction", "comments": "First two authors contributed equally. Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have excelled in the automatic semantic analysis\nof images, reaching human-level performances on challenging benchmarks. Yet,\nthe semantic analysis of videos remains challenging due to the significantly\nhigher dimensionality of the input data, respectively, the significantly higher\nneed for annotated training examples. By studying the automatic recognition of\nGerman sign language videos, we demonstrate that on the relatively scarce\ntraining data of 2.800 videos, modern deep learning architectures for video\nanalysis (such as ResNeXt) along with transfer learning on large gesture\nrecognition tasks, can achieve about 75% character accuracy. Considering that\nthis leaves us with a probability of under 25% that a 5 letter word is spelled\ncorrectly, spell-correction systems are crucial for producing readable outputs.\nThe contribution of this paper is to propose a convolutional neural network for\nspell-correction that expects the softmax outputs of the character recognition\nnetwork (instead of a misspelled word) as an input. We demonstrate that purely\nlearning on softmax inputs in combination with scarce training data yields\noverfitting as the network learns the inputs by heart. In contrast, training\nthe network on several variants of the logits of the classification output i.e.\nscaling by a constant factor, adding of random noise, mixing of softmax and\nhardmax inputs or purely training on hardmax inputs, leads to better\ngeneralization while benefitting from the significant information hidden in\nthese outputs (that have 98% top-5 accuracy), yielding a readable text despite\nthe comparably low character accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:40:00 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Runkel", "Christina", ""], ["Dorenkamp", "Stefan", ""], ["Bauermeister", "Hartmut", ""], ["Moeller", "Michael", ""]]}, {"id": "2007.00622", "submitter": "Weichen Dai", "authors": "Weichen Dai, Yu Zhang, Shenzhou Chen, Donglei Sun, Da Kong", "title": "A Multi-spectral Dataset for Evaluating Motion Estimation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible images have been widely used for motion estimation. Thermal images,\nin contrast, are more challenging to be used in motion estimation since they\ntypically have lower resolution, less texture, and more noise. In this paper, a\nnovel dataset for evaluating the performance of multi-spectral motion\nestimation systems is presented. All the sequences are recorded from a handheld\nmulti-spectral device. It consists of a standard visible-light camera, a\nlong-wave infrared camera, an RGB-D camera, and an inertial measurement unit\n(IMU). The multi-spectral images, including both color and thermal images in\nfull sensor resolution (640 x 480), are obtained from a standard and a\nlong-wave infrared camera at 32Hz with hardware-synchronization. The depth\nimages are captured by a Microsoft Kinect2 and can have benefits for learning\ncross-modalities stereo matching. For trajectory evaluation, accurate\nground-truth camera poses obtained from a motion capture system are provided.\nIn addition to the sequences with bright illumination, the dataset also\ncontains dim, varying, and complex illumination scenes. The full dataset,\nincluding raw data and calibration data with detailed data format\nspecifications, is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:11:02 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 08:46:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dai", "Weichen", ""], ["Zhang", "Yu", ""], ["Chen", "Shenzhou", ""], ["Sun", "Donglei", ""], ["Kong", "Da", ""]]}, {"id": "2007.00631", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox,\n  Animesh Garg", "title": "Causal Discovery in Physical Systems from Videos", "comments": "NeurIPS 2020. Project page: https://yunzhuli.github.io/V-CDN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery is at the core of human cognition. It enables us to reason\nabout the environment and make counterfactual predictions about unseen\nscenarios that can vastly differ from our previous experiences. We consider the\ntask of causal discovery from videos in an end-to-end fashion without\nsupervision on the ground-truth graph structure. In particular, our goal is to\ndiscover the structural dependencies among environmental and object variables:\ninferring the type and strength of interactions that have a causal effect on\nthe behavior of the dynamical system. Our model consists of (a) a perception\nmodule that extracts a semantically meaningful and temporally consistent\nkeypoint representation from images, (b) an inference module for determining\nthe graph distribution induced by the detected keypoints, and (c) a dynamics\nmodule that can predict the future by conditioning on the inferred graph. We\nassume access to different configurations and environmental conditions, i.e.,\ndata from unknown interventions on the underlying system; thus, we can hope to\ndiscover the correct underlying causal graph without explicit interventions. We\nevaluate our method in a planar multi-body interaction environment and\nscenarios involving fabrics of different shapes like shirts and pants.\nExperiments demonstrate that our model can correctly identify the interactions\nfrom a short sequence of images and make long-term future predictions. The\ncausal structure assumed by the model also allows it to make counterfactual\npredictions and extrapolate to systems of unseen interaction graphs or graphs\nof various sizes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:29:57 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 19:11:32 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 20:47:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Yunzhu", ""], ["Torralba", "Antonio", ""], ["Anandkumar", "Animashree", ""], ["Fox", "Dieter", ""], ["Garg", "Animesh", ""]]}, {"id": "2007.00639", "submitter": "Jun Niu", "authors": "Jun Niu", "title": "End-to-End JPEG Decoding and Artifacts Suppression Using Heterogeneous\n  Residual Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning models separate JPEG artifacts suppression from the\ndecoding protocol as independent task. In this work, we take one step forward\nto design a true end-to-end heterogeneous residual convolutional neural network\n(HR-CNN) with spectrum decomposition and heterogeneous reconstruction\nmechanism. Benefitting from the full CNN architecture and GPU acceleration, the\nproposed model considerably improves the reconstruction efficiency. Numerical\nexperiments show that the overall reconstruction speed reaches to the same\nmagnitude of the standard CPU JPEG decoding protocol, while both decoding and\nartifacts suppression are completed together. We formulate the JPEG artifacts\nsuppression task as an interactive process of decoding and image detail\nreconstructions. A heterogeneous, fully convolutional, mechanism is proposed to\nparticularly address the uncorrelated nature of different spectral channels.\nDirectly starting from the JPEG code in k-space, the network first extracts the\nspectral samples channel by channel, and restores the spectral snapshots with\nexpanded throughput. These intermediate snapshots are then heterogeneously\ndecoded and merged into the pixel space image. A cascaded residual learning\nsegment is designed to further enhance the image details. Experiments verify\nthat the model achieves outstanding performance in JPEG artifacts suppression,\nwhile its full convolutional operations and elegant network structure offers\nhigher computational efficiency for practical online usage compared with other\ndeep learning models on this topic.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:44:00 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Niu", "Jun", ""]]}, {"id": "2007.00643", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan\n  Salakhutdinov", "title": "Object Goal Navigation using Goal-Oriented Semantic Exploration", "comments": "Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge.\n  See the project webpage at\n  https://devendrachaplot.github.io/projects/semantic-exploration.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of object goal navigation which involves\nnavigating to an instance of the given object category in unseen environments.\nEnd-to-end learning-based navigation methods struggle at this task as they are\nineffective at exploration and long-term planning. We propose a modular system\ncalled, `Goal-Oriented Semantic Exploration' which builds an episodic semantic\nmap and uses it to explore the environment efficiently based on the goal object\ncategory. Empirical results in visually realistic simulation environments show\nthat the proposed model outperforms a wide range of baselines including\nend-to-end learning-based methods as well as modular map-based methods and led\nto the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation\nanalysis indicates that the proposed model learns semantic priors of the\nrelative arrangement of objects in a scene, and uses them to explore\nefficiently. Domain-agnostic module design allow us to transfer our model to a\nmobile robot platform and achieve similar performance for object goal\nnavigation in the real-world.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:52:32 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 01:38:41 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Gandhi", "Dhiraj", ""], ["Gupta", "Abhinav", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2007.00644", "submitter": "Rohan Taori", "authors": "Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin\n  Recht, Ludwig Schmidt", "title": "Measuring Robustness to Natural Distribution Shifts in Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how robust current ImageNet models are to distribution shifts\narising from natural variations in datasets. Most research on robustness\nfocuses on synthetic image perturbations (noise, simulated weather artifacts,\nadversarial examples, etc.), which leaves open how robustness on synthetic\ndistribution shift relates to distribution shift arising in real data. Informed\nby an evaluation of 204 ImageNet models in 213 different test conditions, we\nfind that there is often little to no transfer of robustness from current\nsynthetic to natural distribution shift. Moreover, most current techniques\nprovide no robustness to the natural distribution shifts in our testbed. The\nmain exception is training on larger and more diverse datasets, which in\nmultiple cases increases robustness, but is still far from closing the\nperformance gaps. Our results indicate that distribution shifts arising in real\ndata are currently an open research problem. We provide our testbed and data as\na resource for future work at https://modestyachts.github.io/imagenet-testbed/ .\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:53:26 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 09:55:13 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Taori", "Rohan", ""], ["Dave", "Achal", ""], ["Shankar", "Vaishaal", ""], ["Carlini", "Nicholas", ""], ["Recht", "Benjamin", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "2007.00649", "submitter": "Hao  Chen", "authors": "Hao Chen, Abhinav Shrivastava", "title": "Group Ensemble: Learning an Ensemble of ConvNets in a single ConvNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning is a general technique to improve accuracy in machine\nlearning. However, the heavy computation of a ConvNets ensemble limits its\nusage in deep learning. In this paper, we present Group Ensemble Network\n(GENet), an architecture incorporating an ensemble of ConvNets in a single\nConvNet. Through a shared-base and multi-head structure, GENet is divided into\nseveral groups to make explicit ensemble learning possible in a single ConvNet.\nOwing to group convolution and the shared-base, GENet can fully leverage the\nadvantage of explicit ensemble learning while retaining the same computation as\na single ConvNet. Additionally, we present Group Averaging, Group Wagging and\nGroup Boosting as three different strategies to aggregate these ensemble\nmembers. Finally, GENet outperforms larger single networks, standard ensembles\nof smaller networks, and other recent state-of-the-art methods on CIFAR and\nImageNet. Specifically, group ensemble reduces the top-1 error by 1.83% for\nResNeXt-50 on ImageNet. We also demonstrate its effectiveness on action\nrecognition and object detection tasks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:56:06 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Hao", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2007.00653", "submitter": "Taesung Park", "authors": "Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman,\n  Alexei A. Efros, Richard Zhang", "title": "Swapping Autoencoder for Deep Image Manipulation", "comments": "NeurIPS 2020. Please visit https://taesung.me/SwappingAutoencoder/\n  for an introductory video. v2 mainly contains reorganization of the\n  Introduction and Broader Impact section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have become increasingly effective at producing\nrealistic images from randomly sampled seeds, but using such models for\ncontrollable manipulation of existing images remains challenging. We propose\nthe Swapping Autoencoder, a deep model designed specifically for image\nmanipulation, rather than random sampling. The key idea is to encode an image\nwith two independent components and enforce that any swapped combination maps\nto a realistic image. In particular, we encourage the components to represent\nstructure and texture, by enforcing one component to encode co-occurrent patch\nstatistics across different parts of an image. As our method is trained with an\nencoder, finding the latent codes for a new input image becomes trivial, rather\nthan cumbersome. As a result, it can be used to manipulate real input images in\nvarious ways, including texture swapping, local and global editing, and latent\ncode vector arithmetic. Experiments on multiple datasets show that our model\nproduces better results and is substantially more efficient compared to recent\ngenerative models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:59:57 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:41:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Park", "Taesung", ""], ["Zhu", "Jun-Yan", ""], ["Wang", "Oliver", ""], ["Lu", "Jingwan", ""], ["Shechtman", "Eli", ""], ["Efros", "Alexei A.", ""], ["Zhang", "Richard", ""]]}, {"id": "2007.00711", "submitter": "Miguel Villarreal-Vasquez", "authors": "Miguel Villarreal-Vasquez and Bharat Bhargava", "title": "ConFoc: Content-Focus Protection Against Trojan Attacks on Neural\n  Networks", "comments": "13 pages (excluding references), 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been applied successfully in computer\nvision. However, their wide adoption in image-related applications is\nthreatened by their vulnerability to trojan attacks. These attacks insert some\nmisbehavior at training using samples with a mark or trigger, which is\nexploited at inference or testing time. In this work, we analyze the\ncomposition of the features learned by DNNs at training. We identify that they,\nincluding those related to the inserted triggers, contain both content\n(semantic information) and style (texture information), which are recognized as\na whole by DNNs at testing time. We then propose a novel defensive technique\nagainst trojan attacks, in which DNNs are taught to disregard the styles of\ninputs and focus on their content only to mitigate the effect of triggers\nduring the classification. The generic applicability of the approach is\ndemonstrated in the context of a traffic sign and a face recognition\napplication. Each of them is exposed to a different attack with a variety of\ntriggers. Results show that the method reduces the attack success rate\nsignificantly to values < 1% in all the tested attacks while keeping as well as\nimproving the initial accuracy of the models when processing both benign and\nadversarial data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:25:34 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Villarreal-Vasquez", "Miguel", ""], ["Bhargava", "Bharat", ""]]}, {"id": "2007.00720", "submitter": "Avishek Bose", "authors": "Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone,\n  Pascal Vincent, Simon Lacoste-Julien and William L. Hamilton", "title": "Adversarial Example Games", "comments": "Appears in: Advances in Neural Information Processing Systems 33\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of adversarial examples capable of fooling trained neural\nnetwork classifiers calls for a much better understanding of possible attacks\nto guide the development of safeguards against them. This includes attack\nmethods in the challenging non-interactive blackbox setting, where adversarial\nattacks are generated without any access, including queries, to the target\nmodel. Prior attacks in this setting have relied mainly on algorithmic\ninnovations derived from empirical observations (e.g., that momentum helps),\nlacking principled transferability guarantees. In this work, we provide a\ntheoretical foundation for crafting transferable adversarial examples to entire\nhypothesis classes. We introduce Adversarial Example Games (AEG), a framework\nthat models the crafting of adversarial examples as a min-max game between a\ngenerator of attacks and a classifier. AEG provides a new way to design\nadversarial examples by adversarially training a generator and a classifier\nfrom a given hypothesis class (e.g., architecture). We prove that this game has\nan equilibrium, and that the optimal generator is able to craft adversarial\nexamples that can attack any classifier from the corresponding hypothesis\nclass. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets,\noutperforming prior state-of-the-art approaches with an average relative\nimprovement of $29.9\\%$ and $47.2\\%$ against undefended and robust models\n(Table 2 & 3) respectively.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:47:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 05:56:03 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 02:47:01 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2020 02:15:06 GMT"}, {"version": "v5", "created": "Fri, 20 Nov 2020 05:07:25 GMT"}, {"version": "v6", "created": "Sat, 9 Jan 2021 01:44:02 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bose", "Avishek Joey", ""], ["Gidel", "Gauthier", ""], ["Berard", "Hugo", ""], ["Cianflone", "Andre", ""], ["Vincent", "Pascal", ""], ["Lacoste-Julien", "Simon", ""], ["Hamilton", "William L.", ""]]}, {"id": "2007.00729", "submitter": "Gordon Christie", "authors": "Gordon Christie, Rodrigo Rene Rai Munoz Abujder, Kevin Foster, Shea\n  Hagstrom, Gregory D. Hager, Myron Z. Brown", "title": "Learning Geocentric Object Pose in Oblique Monocular Images", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An object's geocentric pose, defined as the height above ground and\norientation with respect to gravity, is a powerful representation of real-world\nstructure for object detection, segmentation, and localization tasks using RGBD\nimages. For close-range vision tasks, height and orientation have been derived\ndirectly from stereo-computed depth and more recently from monocular depth\npredicted by deep networks. For long-range vision tasks such as Earth\nobservation, depth cannot be reliably estimated with monocular images. Inspired\nby recent work in monocular height above ground prediction and optical flow\nprediction from static images, we develop an encoding of geocentric pose to\naddress this challenge and train a deep network to compute the representation\ndensely, supervised by publicly available airborne lidar. We exploit these\nattributes to rectify oblique images and remove observed object parallax to\ndramatically improve the accuracy of localization and to enable accurate\nalignment of multiple images taken from very different oblique viewpoints. We\ndemonstrate the value of our approach by extending two large-scale public\ndatasets for semantic segmentation in oblique satellite images. All of our data\nand code are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:06:19 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Christie", "Gordon", ""], ["Abujder", "Rodrigo Rene Rai Munoz", ""], ["Foster", "Kevin", ""], ["Hagstrom", "Shea", ""], ["Hager", "Gregory D.", ""], ["Brown", "Myron Z.", ""]]}, {"id": "2007.00737", "submitter": "Robert Leishman", "authors": "Kyung Kim, Robert C. Leishman, and Scott L. Nykl", "title": "Virtual Testbed for Monocular Visual Navigation of Small Unmanned\n  Aircraft Systems", "comments": null, "journal-ref": "The Journal of Defense Modeling and Simulation: Applications,\n  Methodology, Technology 2020", "doi": "10.1177/1548512920954545", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular visual navigation methods have seen significant advances in the\nlast decade, recently producing several real-time solutions for autonomously\nnavigating small unmanned aircraft systems without relying on GPS. This is\ncritical for military operations which may involve environments where GPS\nsignals are degraded or denied. However, testing and comparing visual\nnavigation algorithms remains a challenge since visual data is expensive to\ngather. Conducting flight tests in a virtual environment is an attractive\nsolution prior to committing to outdoor testing.\n  This work presents a virtual testbed for conducting simulated flight tests\nover real-world terrain and analyzing the real-time performance of visual\nnavigation algorithms at 31 Hz. This tool was created to ultimately find a\nvisual odometry algorithm appropriate for further GPS-denied navigation\nresearch on fixed-wing aircraft, even though all of the algorithms were\ndesigned for other modalities. This testbed was used to evaluate three current\nstate-of-the-art, open-source monocular visual odometry algorithms on a\nfixed-wing platform: Direct Sparse Odometry, Semi-Direct Visual Odometry, and\nORB-SLAM2 (with loop closures disabled).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:35:26 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kim", "Kyung", ""], ["Leishman", "Robert C.", ""], ["Nykl", "Scott L.", ""]]}, {"id": "2007.00741", "submitter": "Aydogan Ozcan", "authors": "Tairan Liu, Kevin de Haan, Bijie Bai, Yair Rivenson, Yi Luo, Hongda\n  Wang, David Karalli, Hongxiang Fu, Yibo Zhang, John FitzGerald, and Aydogan\n  Ozcan", "title": "Deep learning-based holographic polarization microscopy", "comments": "20 pages, 8 figures", "journal-ref": "ACS Photonics (2020)", "doi": "10.1021/acsphotonics.0c01051", "report-no": null, "categories": "physics.optics cs.CV eess.IV physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarized light microscopy provides high contrast to birefringent specimen\nand is widely used as a diagnostic tool in pathology. However, polarization\nmicroscopy systems typically operate by analyzing images collected from two or\nmore light paths in different states of polarization, which lead to relatively\ncomplex optical designs, high system costs or experienced technicians being\nrequired. Here, we present a deep learning-based holographic polarization\nmicroscope that is capable of obtaining quantitative birefringence retardance\nand orientation information of specimen from a phase recovered hologram, while\nonly requiring the addition of one polarizer/analyzer pair to an existing\nholographic imaging system. Using a deep neural network, the reconstructed\nholographic images from a single state of polarization can be transformed into\nimages equivalent to those captured using a single-shot computational polarized\nlight microscope (SCPLM). Our analysis shows that a trained deep neural network\ncan extract the birefringence information using both the sample specific\nmorphological features as well as the holographic amplitude and phase\ndistribution. To demonstrate the efficacy of this method, we tested it by\nimaging various birefringent samples including e.g., monosodium urate (MSU) and\ntriamcinolone acetonide (TCA) crystals. Our method achieves similar results to\nSCPLM both qualitatively and quantitatively, and due to its simpler optical\ndesign and significantly larger field-of-view, this method has the potential to\nexpand the access to polarization microscopy and its use for medical diagnosis\nin resource limited settings.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:39:50 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Liu", "Tairan", ""], ["de Haan", "Kevin", ""], ["Bai", "Bijie", ""], ["Rivenson", "Yair", ""], ["Luo", "Yi", ""], ["Wang", "Hongda", ""], ["Karalli", "David", ""], ["Fu", "Hongxiang", ""], ["Zhang", "Yibo", ""], ["FitzGerald", "John", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2007.00748", "submitter": "Mariia Dobko", "authors": "Ostap Viniavskyi, Mariia Dobko, Oles Dobosevych", "title": "Weakly-Supervised Segmentation for Disease Localization in Chest X-Ray\n  Images", "comments": "Accepted to AIME 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks have proven effective in solving the task\nof semantic segmentation. However, their efficiency heavily relies on the\npixel-level annotations that are expensive to get and often require domain\nexpertise, especially in medical imaging. Weakly supervised semantic\nsegmentation helps to overcome these issues and also provides explainable deep\nlearning models. In this paper, we propose a novel approach to the semantic\nsegmentation of medical chest X-ray images with only image-level class labels\nas supervision. We improve the disease localization accuracy by combining three\napproaches as consecutive steps. First, we generate pseudo segmentation labels\nof abnormal regions in the training images through a supervised classification\nmodel enhanced with a regularization procedure. The obtained activation maps\nare then post-processed and propagated into a second classification\nmodel-Inter-pixel Relation Network, which improves the boundaries between\ndifferent object classes. Finally, the resulting pseudo-labels are used to\ntrain a proposed fully supervised segmentation model. We analyze the robustness\nof the presented method and test its performance on two distinct datasets:\nPASCAL VOC 2012 and SIIM-ACR Pneumothorax. We achieve significant results in\nthe segmentation on both datasets using only image-level annotations. We show\nthat this approach is applicable to chest X-rays for detecting an anomalous\nvolume of air in the pleural space between the lung and the chest wall. Our\ncode has been made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:48:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Viniavskyi", "Ostap", ""], ["Dobko", "Mariia", ""], ["Dobosevych", "Oles", ""]]}, {"id": "2007.00760", "submitter": "Mason Chen", "authors": "Mason T. Chen and Nicholas J. Durr", "title": "Rapid tissue oxygenation mapping from snapshot structured-light images\n  with adversarial deep learning", "comments": null, "journal-ref": null, "doi": "10.1117/1.JBO.25.11.112907", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial frequency domain imaging (SFDI) is a powerful technique for mapping\ntissue oxygen saturation over a wide field of view. However, current SFDI\nmethods either require a sequence of several images with different illumination\npatterns or, in the case of single snapshot optical properties (SSOP),\nintroduce artifacts and sacrifice accuracy. To avoid this tradeoff, we\nintroduce OxyGAN: a data-driven, content-aware method to estimate tissue\noxygenation directly from single structured light images using end-to-end\ngenerative adversarial networks. Conventional SFDI is used to obtain ground\ntruth tissue oxygenation maps for ex vivo human esophagi, in vivo hands and\nfeet, and an in vivo pig colon sample under 659 nm and 851 nm sinusoidal\nillumination. We benchmark OxyGAN by comparing to SSOP and to a two-step hybrid\ntechnique that uses a previously-developed deep learning model to predict\noptical properties followed by a physical model to calculate tissue\noxygenation. When tested on human feet, a cross-validated OxyGAN maps tissue\noxygenation with an accuracy of 96.5%. When applied to sample types not\nincluded in the training set, such as human hands and pig colon, OxyGAN\nachieves a 93.0% accuracy, demonstrating robustness to various tissue types. On\naverage, OxyGAN outperforms SSOP and a hybrid model in estimating tissue\noxygenation by 24.9% and 24.7%, respectively. Lastly, we optimize OxyGAN\ninference so that oxygenation maps are computed ~10 times faster than previous\nwork, enabling video-rate, 25Hz imaging. Due to its rapid acquisition and\nprocessing speed, OxyGAN has the potential to enable real-time, high-fidelity\ntissue oxygenation mapping that may be useful for many clinical applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:14:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Mason T.", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "2007.00767", "submitter": "Xuesong Wang", "authors": "Xuesong Wang, Lina Yao, Xianzhi Wang, Feiping Nie", "title": "NP-PROV: Neural Processes with Position-Relevant-Only Variances", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Processes (NPs) families encode distributions over functions to a\nlatent representation, given context data, and decode posterior mean and\nvariance at unknown locations. Since mean and variance are derived from the\nsame latent space, they may fail on out-of-domain tasks where fluctuations in\nfunction values amplify the model uncertainty. We present a new member named\nNeural Processes with Position-Relevant-Only Variances (NP-PROV). NP-PROV\nhypothesizes that a target point close to a context point has small\nuncertainty, regardless of the function value at that position. The resulting\napproach derives mean and variance from a function-value-related space and a\nposition-related-only latent space separately. Our evaluation on synthetic and\nreal-world datasets reveals that NP-PROV can achieve state-of-the-art\nlikelihood while retaining a bounded variance when drifts exist in the function\nvalue.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 06:11:21 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Xuesong", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Nie", "Feiping", ""]]}, {"id": "2007.00779", "submitter": "Alessandro Lameiras Koerich", "authors": "Thiago M. Paix\\~ao, Rodrigo F. Berriel, Maria C. S. Boeres, Alessandro\n  L. Koerich, Claudine Badue, Alberto F. de Souza, Thiago Oliveira-Santos", "title": "Self-supervised Deep Reconstruction of Mixed Strip-shredded Text\n  Documents", "comments": "Accepted for publication in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of shredded documents consists of coherently arranging\nfragments of paper (shreds) to recover the original document(s). A great\nchallenge in computational reconstruction is to properly evaluate the\ncompatibility between the shreds. While traditional pixel-based approaches are\nnot robust to real shredding, more sophisticated solutions compromise\nsignificantly time performance. The solution presented in this work extends our\nprevious deep learning method for single-page reconstruction to a more\nrealistic/complex scenario: the reconstruction of several mixed shredded\ndocuments at once. In our approach, the compatibility evaluation is modeled as\na two-class (valid or invalid) pattern recognition problem. The model is\ntrained in a self-supervised manner on samples extracted from\nsimulated-shredded documents, which obviates manual annotation. Experimental\nresults on three datasets -- including a new collection of 100 strip-shredded\ndocuments produced for this work -- have shown that the proposed method\noutperforms the competing ones on complex scenarios, achieving accuracy\nsuperior to 90%.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:48:05 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Paix\u00e3o", "Thiago M.", ""], ["Berriel", "Rodrigo F.", ""], ["Boeres", "Maria C. S.", ""], ["Koerich", "Alessandro L.", ""], ["Badue", "Claudine", ""], ["de Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2007.00792", "submitter": "Haoyi Wang", "authors": "Haoyi Wang, Victor Sanchez, Chang-Tsun Li", "title": "Age-Oriented Face Synthesis with Conditional Discriminator Pool and\n  Adversarial Triplet Loss", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3084106", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The vanilla Generative Adversarial Networks (GAN) are commonly used to\ngenerate realistic images depicting aged and rejuvenated faces. However, the\nperformance of such vanilla GANs in the age-oriented face synthesis task is\noften compromised by the mode collapse issue, which may result in the\ngeneration of faces with minimal variations and a poor synthesis accuracy. In\naddition, recent age-oriented face synthesis methods use the L1 or L2\nconstraint to preserve the identity information on synthesized faces, which\nimplicitly limits the identity permanence capabilities when these constraints\nare associated with a trivial weighting factor. In this paper, we propose a\nmethod for the age-oriented face synthesis task that achieves a high synthesis\naccuracy with strong identity permanence capabilities. Specifically, to achieve\na high synthesis accuracy, our method tackles the mode collapse issue with a\nnovel Conditional Discriminator Pool (CDP), which consists of multiple\ndiscriminators, each targeting one particular age category. To achieve strong\nidentity permanence capabilities, our method uses a novel Adversarial Triplet\nloss. This loss, which is based on the Triplet loss, adds a ranking operation\nto further pull the positive embedding towards the anchor embedding resulting\nin significantly reduced intra-class variances in the feature space. Through\nextensive experiments, we show that our proposed method outperforms\nstate-of-the-art methods in terms of synthesis accuracy and identity permanence\ncapabilities, qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:18:21 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 23:31:58 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Haoyi", ""], ["Sanchez", "Victor", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "2007.00799", "submitter": "Yash Patel", "authors": "Yash Patel, Tomas Hodan, Jiri Matas", "title": "Learning Surrogates via Deep Embedding", "comments": "ECCV 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a technique for training a neural network by minimizing a\nsurrogate loss that approximates the target evaluation metric, which may be\nnon-differentiable. The surrogate is learned via a deep embedding where the\nEuclidean distance between the prediction and the ground truth corresponds to\nthe value of the evaluation metric. The effectiveness of the proposed technique\nis demonstrated in a post-tuning setup, where a trained model is tuned using\nthe learned surrogate. Without a significant computational overhead and any\nbells and whistles, improvements are demonstrated on challenging and practical\ntasks of scene-text recognition and detection. In the recognition task, the\nmodel is tuned using a surrogate approximating the edit distance metric and\nachieves up to $39\\%$ relative improvement in the total edit distance. In the\ndetection task, the surrogate approximates the intersection over union metric\nfor rotated bounding boxes and yields up to $4.25\\%$ relative improvement in\nthe $F_{1}$ score.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:55:41 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:39:16 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Patel", "Yash", ""], ["Hodan", "Tomas", ""], ["Matas", "Jiri", ""]]}, {"id": "2007.00801", "submitter": "Senthil Yogamani", "authors": "Arindam Das, Pavel Krizek, Ganesh Sistu, Fabian Burger, Sankaralingam\n  Madasamy, Michal Uricar, Varun Ravi Kumar, Senthil Yogamani", "title": "TiledSoilingNet: Tile-level Soiling Detection on Automotive\n  Surround-view Cameras Using Coverage Metric", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automotive cameras, particularly surround-view cameras, tend to get soiled by\nmud, water, snow, etc. For higher levels of autonomous driving, it is necessary\nto have a soiling detection algorithm which will trigger an automatic cleaning\nsystem. Localized detection of soiling in an image is necessary to control the\ncleaning system. It is also necessary to enable partial functionality in\nunsoiled areas while reducing confidence in soiled areas. Although this can be\nsolved using a semantic segmentation task, we explore a more efficient solution\ntargeting deployment in low power embedded system. We propose a novel method to\nregress the area of each soiling type within a tile directly. We refer to this\nas coverage. The proposed approach is better than learning the dominant class\nin a tile as multiple soiling types occur within a tile commonly. It also has\nthe advantage of dealing with coarse polygon annotation, which will cause the\nsegmentation task. The proposed soiling coverage decoder is an order of\nmagnitude faster than an equivalent segmentation decoder. We also integrated it\ninto an object detection and semantic segmentation multi-task model using an\nasynchronous back-propagation algorithm. A portion of the dataset used will be\nreleased publicly as part of our WoodScape dataset to encourage further\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:00:47 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Das", "Arindam", ""], ["Krizek", "Pavel", ""], ["Sistu", "Ganesh", ""], ["Burger", "Fabian", ""], ["Madasamy", "Sankaralingam", ""], ["Uricar", "Michal", ""], ["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2007.00806", "submitter": "Soroush Vosoughi Dr", "authors": "Chris Miller and Soroush Vosoughi", "title": "Query-Free Adversarial Transfer via Undertrained Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples -- minor\nperturbations added to a model's input which cause the model to output an\nincorrect prediction. We introduce a new method for improving the efficacy of\nadversarial attacks in a black-box setting by undertraining the surrogate model\nwhich the attacks are generated on. Using two datasets and five model\narchitectures, we show that this method transfers well across architectures and\noutperforms state-of-the-art methods by a wide margin. We interpret the\neffectiveness of our approach as a function of reduced surrogate model loss\nfunction curvature and increased universal gradient characteristics, and show\nthat our approach reduces the presence of local loss maxima which hinder\ntransferability. Our results suggest that finding strong single surrogate\nmodels is a highly effective and simple method for generating transferable\nadversarial attacks, and that this method represents a valuable route for\nfuture study in this field.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:12:22 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 06:05:53 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Miller", "Chris", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2007.00822", "submitter": "Bingbing Zhuang", "authors": "Buyu Liu, Bingbing Zhuang, Samuel Schulter, Pan Ji, Manmohan\n  Chandraker", "title": "Understanding Road Layout from Videos as a Whole", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of inferring the layout of complex road\nscenes from video sequences. To this end, we formulate it as a top-view road\nattributes prediction problem and our goal is to predict these attributes for\neach frame both accurately and consistently. In contrast to prior work, we\nexploit the following three novel aspects: leveraging camera motions in videos,\nincluding context cuesand incorporating long-term video information.\nSpecifically, we introduce a model that aims to enforce prediction consistency\nin videos. Our model consists of one LSTM and one Feature Transform Module\n(FTM). The former implicitly incorporates the consistency constraint with its\nhidden states, and the latter explicitly takes the camera motion into\nconsideration when aggregating information along videos. Moreover, we propose\nto incorporate context information by introducing road participants, e.g.\nobjects, into our model. When the entire video sequence is available, our model\nis also able to encode both local and global cues, e.g. information from both\npast and future frames. Experiments on two data sets show that: (1)\nIncorporating either globalor contextual cues improves the prediction accuracy\nand leveraging both gives the best performance. (2) Introducing the LSTM and\nFTM modules improves the prediction consistency in videos. (3) The proposed\nmethod outperforms the SOTA by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 00:59:15 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Buyu", ""], ["Zhuang", "Bingbing", ""], ["Schulter", "Samuel", ""], ["Ji", "Pan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.00833", "submitter": "Guotai Wang", "authors": "Guotai Wang, Michael Aertsen, Jan Deprest, Sebastien Ourselin, Tom\n  Vercauteren, Shaoting Zhang", "title": "Uncertainty-Guided Efficient Interactive Refinement of Fetal Brain\n  Segmentation from Stacks of MRI Slices", "comments": "10 pages, 4 figures, Accepted by MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59719-1_28", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the fetal brain from stacks of motion-corrupted fetal MRI\nslices is important for motion correction and high-resolution volume\nreconstruction. Although Convolutional Neural Networks (CNNs) have been widely\nused for automatic segmentation of the fetal brain, their results may still\nbenefit from interactive refinement for challenging slices. To improve the\nefficiency of interactive refinement process, we propose an Uncertainty-Guided\nInteractive Refinement (UGIR) framework. We first propose a grouped\nconvolution-based CNN to obtain multiple automatic segmentation predictions\nwith uncertainty estimation in a single forward pass, then guide the user to\nprovide interactions only in a subset of slices with the highest uncertainty. A\nnovel interactive level set method is also proposed to obtain a refined result\ngiven the initial segmentation and user interactions. Experimental results show\nthat: (1) our proposed CNN obtains uncertainty estimation in real time which\ncorrelates well with mis-segmentations, (2) the proposed interactive level set\nis effective and efficient for refinement, (3) UGIR obtains accurate refinement\nresults with around 30% improvement of efficiency by using uncertainty to guide\nuser interactions. Our code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 01:50:42 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Wang", "Guotai", ""], ["Aertsen", "Michael", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2007.00842", "submitter": "Martin Skrodzki", "authors": "Sunil Kumar Yadav and Martin Skrodzki and Eric Zimmermann and Konrad\n  Polthier", "title": "Surface Denoising based on Normal Filtering in a Robust Statistics\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a surface acquisition process using 3D scanners, noise is inevitable\nand an important step in geometry processing is to remove these noise\ncomponents from these surfaces (given as points-set or triangulated mesh). The\nnoise-removal process (denoising) can be performed by filtering the surface\nnormals first and by adjusting the vertex positions according to filtered\nnormals afterwards. Therefore, in many available denoising algorithms, the\ncomputation of noise-free normals is a key factor. A variety of filters have\nbeen introduced for noise-removal from normals, with different focus points\nlike robustness against outliers or large amplitude of noise. Although these\nfilters are performing well in different aspects, a unified framework is\nmissing to establish the relation between them and to provide a theoretical\nanalysis beyond the performance of each method.\n  In this paper, we introduce such a framework to establish relations between a\nnumber of widely-used nonlinear filters for face normals in mesh denoising and\nvertex normals in point set denoising. We cover robust statistical estimation\nwith M-smoothers and their application to linear and non-linear normal\nfiltering. Although these methods originate in different mathematical theories\n- which include diffusion-, bilateral-, and directional curvature-based\nalgorithms - we demonstrate that all of them can be cast into a unified\nframework of robust statistics using robust error norms and their corresponding\ninfluence functions. This unification contributes to a better understanding of\nthe individual methods and their relations with each other. Furthermore, the\npresented framework provides a platform for new techniques to combine the\nadvantages of known filters and to compare them with available methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 02:31:24 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Yadav", "Sunil Kumar", ""], ["Skrodzki", "Martin", ""], ["Zimmermann", "Eric", ""], ["Polthier", "Konrad", ""]]}, {"id": "2007.00843", "submitter": "Michael Potter", "authors": "Michael Potter (1), Henry Gridley (1), Noah Lichtenstein (1), Kevin\n  Hines (1), John Nguyen (1), Jacob Walsh (1) ((1) Northeastern University)", "title": "Low-light Environment Neural Surveillance", "comments": "Pre-print, accepted to IEEE International Workshop on Machine\n  Learning for Signal Processing 2020 Conference Proceedings. Code and dataset\n  are available at https://github.com/mcgridles/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement an end-to-end system for real-time crime detection in\nlow-light environments. Unlike Closed-Circuit Television, which performs\nreactively, the Low-Light Environment Neural Surveillance provides real time\ncrime alerts. The system uses a low-light video feed processed in real-time by\nan optical-flow network, spatial and temporal networks, and a Support Vector\nMachine to identify shootings, assaults, and thefts. We create a low-light\naction-recognition dataset, LENS-4, which will be publicly available. An IoT\ninfrastructure set up via Amazon Web Services interprets messages from the\nlocal board hosting the camera for action recognition and parses the results in\nthe cloud to relay messages. The system achieves 71.5% accuracy at 20 FPS. The\nuser interface is a mobile app which allows local authorities to receive\nnotifications and to view a video of the crime scene. Citizens have a public\napp which enables law enforcement to push crime alerts based on user proximity.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 02:45:41 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Potter", "Michael", "", "Northeastern University"], ["Gridley", "Henry", "", "Northeastern University"], ["Lichtenstein", "Noah", "", "Northeastern University"], ["Hines", "Kevin", "", "Northeastern University"], ["Nguyen", "John", "", "Northeastern University"], ["Walsh", "Jacob", "", "Northeastern University"]]}, {"id": "2007.00858", "submitter": "Yilin Chen", "authors": "Yilin Chen, Ming Li, Yongfei Wu, Xueyu Liu, Fang Hao, Daoxiang Zhou,\n  Xiaoshuang Zhou and Chen Wang", "title": "MSA-MIL: A deep residual multiple instance learning model based on\n  multi-scale annotation for classification and visualization of glomerular\n  spikes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Membranous nephropathy (MN) is a frequent type of adult nephrotic syndrome,\nwhich has a high clinical incidence and can cause various complications. In the\nbiopsy microscope slide of membranous nephropathy, spikelike projections on the\nglomerular basement membrane is a prominent feature of the MN. However, due to\nthe whole biopsy slide contains large number of glomeruli, and each glomerulus\nincludes many spike lesions, the pathological feature of the spikes is not\nobvious. It thus is time-consuming for doctors to diagnose glomerulus one by\none and is difficult for pathologists with less experience to diagnose. In this\npaper, we establish a visualized classification model based on the multi-scale\nannotation multi-instance learning (MSA-MIL) to achieve glomerular\nclassification and spikes visualization. The MSA-MIL model mainly involves\nthree parts. Firstly, U-Net is used to extract the region of the glomeruli to\nensure that the features learned by the succeeding algorithm are focused inside\nthe glomeruli itself. Secondly, we use MIL to train an instance-level\nclassifier combined with MSA method to enhance the learning ability of the\nnetwork by adding a location-level labeled reinforced dataset, thereby\nobtaining an example-level feature representation with rich semantics. Lastly,\nthe predicted scores of each tile in the image are summarized to obtain\nglomerular classification and visualization of the classification results of\nthe spikes via the usage of sliding window method. The experimental results\nconfirm that the proposed MSA-MIL model can effectively and accurately classify\nnormal glomeruli and spiked glomerulus and visualize the position of spikes in\nthe glomerulus. Therefore, the proposed model can provide a good foundation for\nassisting the clinical doctors to diagnose the glomerular membranous\nnephropathy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 03:49:44 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 17:04:28 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Yilin", ""], ["Li", "Ming", ""], ["Wu", "Yongfei", ""], ["Liu", "Xueyu", ""], ["Hao", "Fang", ""], ["Zhou", "Daoxiang", ""], ["Zhou", "Xiaoshuang", ""], ["Wang", "Chen", ""]]}, {"id": "2007.00861", "submitter": "Omar Elharrouss", "authors": "Omar Elharrouss, Nandhini Subramanian, Somaya Al-Maadeed", "title": "An encoder-decoder-based method for COVID-19 lung infection segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novelty of the COVID-19 disease and the speed of spread has created a\ncolossal chaos, impulse among researchers worldwide to exploit all the\nresources and capabilities to understand and analyze characteristics of the\ncoronavirus in term of the ways it spreads and virus incubation time. For that,\nthe existing medical features like CT and X-ray images are used. For example,\nCT-scan images can be used for the detection of lung infection. But the\nchallenges of these features such as the quality of the image and infection\ncharacteristics limitate the effectiveness of these features. Using artificial\nintelligence (AI) tools and computer vision algorithms, the accuracy of\ndetection can be more accurate and can help to overcome these issues. This\npaper proposes a multi-task deep-learning-based method for lung infection\nsegmentation using CT-scan images. Our proposed method starts by segmenting the\nlung regions that can be infected. Then, segmenting the infections in these\nregions. Also, to perform a multi-class segmentation the proposed model is\ntrained using the two-stream inputs. The multi-task learning used in this paper\nallows us to overcome shortage of labeled data. Also, the multi-input stream\nallows the model to do the learning on many features that can improve the\nresults. To evaluate the proposed method, many features have been used. Also,\nfrom the experiments, the proposed method can segment lung infections with a\nhigh degree performance even with shortage of data and labeled images. In\naddition, comparing with the state-of-the-art method our method achieves good\nperformance results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:02:03 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 20:00:32 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Elharrouss", "Omar", ""], ["Subramanian", "Nandhini", ""], ["Al-Maadeed", "Somaya", ""]]}, {"id": "2007.00862", "submitter": "Dapeng Zhao", "authors": "Dapeng Zhao, Jean Oh", "title": "Noticing Motion Patterns: Temporal CNN with a Novel Convolution Operator\n  for Human Trajectory Prediction", "comments": "Accepted by IEEE Robotics and Automation Letters (RA-L)\n  (https://www.ieee-ras.org/publications/ra-l/special-issues/cfp-special-long-term-human-motion-prediction)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Convolutional Neural Network-based approach to learn, detect,and\nextract patterns in sequential trajectory data, known here as Social Pattern\nExtraction Convolution (Social-PEC). A set of experiments carried out on the\nhuman trajectory prediction problem shows that our model performs comparably to\nthe state of the art and outperforms in some cases. More importantly,the\nproposed approach unveils the obscurity in the previous use of pooling layer,\npresenting a way to intuitively explain the decision-making process.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:07:01 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 05:01:16 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 04:06:12 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhao", "Dapeng", ""], ["Oh", "Jean", ""]]}, {"id": "2007.00864", "submitter": "Shail Dave", "authors": "Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral\n  Shrivastava, Baoxin Li", "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML\n  Models: A Survey and Insights", "comments": "Accepted for publication in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:08:40 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 17:41:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dave", "Shail", ""], ["Baghdadi", "Riyadh", ""], ["Nowatzki", "Tony", ""], ["Avancha", "Sasikanth", ""], ["Shrivastava", "Aviral", ""], ["Li", "Baoxin", ""]]}, {"id": "2007.00889", "submitter": "Kazue Kudo", "authors": "Hinako Asaoka and Kazue Kudo", "title": "Image Analysis Based on Nonnegative/Binary Matrix Factorization", "comments": "3 pages, 1 figure", "journal-ref": "J. Phys. Soc. Jpn. 89, 085001 (2020)", "doi": "10.7566/JPSJ.89.085001", "report-no": null, "categories": "cs.CV cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using nonnegative/binary matrix factorization (NBMF), a matrix can be\ndecomposed into a nonnegative matrix and a binary matrix. Our analysis of\nfacial images, based on NBMF and using the Fujitsu Digital Annealer, leads to\nsuccessful image reconstruction and image classification. The NBMF algorithm\nconverges in fewer iterations than those required for the convergence of\nnonnegative matrix factorization (NMF), although both techniques perform\ncomparably in image classification.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 05:22:36 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Asaoka", "Hinako", ""], ["Kudo", "Kazue", ""]]}, {"id": "2007.00899", "submitter": "Bin Zhang", "authors": "Bin Zhang, Jian Li, Yabiao Wang, Zhipeng Cui, Yili Xia, Chengjie Wang,\n  Jilin Li, Feiyue Huang", "title": "ACFD: Asymmetric Cartoon Face Detector", "comments": "1st place of IJCAI 2020 iCartoon Face Challenge (Detection Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartoon face detection is a more challenging task than human face detection\ndue to many difficult scenarios is involved. Aiming at the characteristics of\ncartoon faces, such as huge differences within the intra-faces, in this paper,\nwe propose an asymmetric cartoon face detector, named ACFD. Specifically, it\nconsists of the following modules: a novel backbone VoVNetV3 comprised of\nseveral asymmetric one-shot aggregation modules (AOSA), asymmetric\nbi-directional feature pyramid network (ABi-FPN), dynamic anchor match strategy\n(DAM) and the corresponding margin binary classification loss (MBC). In\nparticular, to generate features with diverse receptive fields, multi-scale\npyramid features are extracted by VoVNetV3, and then fused and enhanced\nsimultaneously by ABi-FPN for handling the faces in some extreme poses and have\ndisparate aspect ratios. Besides, DAM is used to match enough high-quality\nanchors for each face, and MBC is for the strong power of discrimination. With\nthe effectiveness of these modules, our ACFD achieves the 1st place on the\ndetection track of 2020 iCartoon Face Challenge under the constraints of model\nsize 200MB, inference time 50ms per image, and without any pretrained models.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 05:57:34 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhang", "Bin", ""], ["Li", "Jian", ""], ["Wang", "Yabiao", ""], ["Cui", "Zhipeng", ""], ["Xia", "Yili", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2007.00900", "submitter": "Kamran Alipour", "authors": "Kamran Alipour, Arijit Ray, Xiao Lin, Jurgen P. Schulze, Yi Yao,\n  Giedrius T. Burachas", "title": "The Impact of Explanations on AI Competency Prediction in VQA", "comments": "Submitted to HCCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability is one of the key elements for building trust in AI systems.\nAmong numerous attempts to make AI explainable, quantifying the effect of\nexplanations remains a challenge in conducting human-AI collaborative tasks.\nAside from the ability to predict the overall behavior of AI, in many\napplications, users need to understand an AI agent's competency in different\naspects of the task domain. In this paper, we evaluate the impact of\nexplanations on the user's mental model of AI agent competency within the task\nof visual question answering (VQA). We quantify users' understanding of\ncompetency, based on the correlation between the actual system performance and\nuser rankings. We introduce an explainable VQA system that uses spatial and\nobject features and is powered by the BERT language model. Each group of users\nsees only one kind of explanation to rank the competencies of the VQA model.\nThe proposed model is evaluated through between-subject experiments to probe\nexplanations' impact on the user's perception of competency. The comparison\nbetween two VQA models shows BERT based explanations and the use of object\nfeatures improve the user's prediction of the model's competencies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 06:11:28 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Alipour", "Kamran", ""], ["Ray", "Arijit", ""], ["Lin", "Xiao", ""], ["Schulze", "Jurgen P.", ""], ["Yao", "Yi", ""], ["Burachas", "Giedrius T.", ""]]}, {"id": "2007.00959", "submitter": "Mingyuan Jiu", "authors": "Mingyuan Jiu, Nelly Pustelnik", "title": "A deep primal-dual proximal network for image restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image restoration remains a challenging task in image processing. Numerous\nmethods tackle this problem, often solved by minimizing a non-smooth penalized\nco-log-likelihood function. Although the solution is easily interpretable with\ntheoretic guarantees, its estimation relies on an optimization process that can\ntake time. Considering the research effort in deep learning for image\nclassification and segmentation, this class of methods offers a serious\nalternative to perform image restoration but stays challenging to solve inverse\nproblems. In this work, we design a deep network, named DeepPDNet, built from\nprimal-dual proximal iterations associated with the minimization of a standard\npenalized likelihood with an analysis prior, allowing us to take advantage of\nboth worlds.\n  We reformulate a specific instance of the Condat-Vu primal-dual hybrid\ngradient (PDHG) algorithm as a deep network with fixed layers. The learned\nparameters are both the PDHG algorithm step-sizes and the analysis linear\noperator involved in the penalization (including the regularization parameter).\nThese parameters are allowed to vary from a layer to another one. Two different\nlearning strategies: \"Full learning\" and \"Partial learning\" are proposed, the\nfirst one is the most efficient numerically while the second one relies on\nstandard constraints ensuring convergence in the standard PDHG iterations.\nMoreover, global and local sparse analysis prior are studied to seek a better\nfeature representation. We apply the proposed methods to image restoration on\nthe MNIST and BSD68 datasets and to single image super-resolution on the BSD100\nand SET14 datasets. Extensive results show that the proposed DeepPDNet\ndemonstrates excellent performance on the MNIST and the more complex BSD68,\nBSD100, and SET14 datasets for image restoration and single image\nsuper-resolution task.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 08:29:52 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 06:10:23 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Pustelnik", "Nelly", ""]]}, {"id": "2007.00961", "submitter": "Bishwo Adhikari Mr.", "authors": "Bishwo Adhikari and Heikki Huttunen", "title": "Iterative Bounding Box Annotation for Object Detection", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual annotation of bounding boxes for object detection in digital images is\ntedious, and time and resource consuming. In this paper, we propose a\nsemi-automatic method for efficient bounding box annotation. The method trains\nthe object detector iteratively on small batches of labeled images and learns\nto propose bounding boxes for the next batch, after which the human annotator\nonly needs to correct possible errors. We propose an experimental setup for\nsimulating the human actions and use it for comparing different iteration\nstrategies, such as the order in which the data is presented to the annotator.\nWe experiment on our method with three datasets and show that it can reduce the\nhuman annotation effort significantly, saving up to 75% of total manual\nannotation work.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 08:40:12 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Adhikari", "Bishwo", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2007.00970", "submitter": "Ettore Randazzo", "authors": "Ettore Randazzo, Eyvind Niklasson, Alexander Mordvintsev", "title": "MPLP: Learning a Message Passing Learning Protocol", "comments": "Code at\n  https://github.com/google-research/self-organising-systems/tree/master/mplp;\n  code base link fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel method for learning the weights of an artificial neural\nnetwork - a Message Passing Learning Protocol (MPLP). In MPLP, we abstract\nevery operations occurring in ANNs as independent agents. Each agent is\nresponsible for ingesting incoming multidimensional messages from other agents,\nupdating its internal state, and generating multidimensional messages to be\npassed on to neighbouring agents. We demonstrate the viability of MPLP as\nopposed to traditional gradient-based approaches on simple feed-forward neural\nnetworks, and present a framework capable of generalizing to non-traditional\nneural network architectures. MPLP is meta learned using end-to-end\ngradient-based meta-optimisation. We further discuss the observed properties of\nMPLP and hypothesize its applicability on various fields of deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:03:14 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 10:28:35 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Randazzo", "Ettore", ""], ["Niklasson", "Eyvind", ""], ["Mordvintsev", "Alexander", ""]]}, {"id": "2007.00977", "submitter": "Dorien Herremans", "authors": "Kanish Garg, Ajeet kumar Singh, Dorien Herremans, Brejesh Lall", "title": "PerceptionGAN: Real-world Image Construction from Provided Text through\n  Perceptual Understanding", "comments": "Proceedings of IEEE International Conference on Imaging, Vision &\n  Pattern Recognition, (IVPR 2020, Japan)", "journal-ref": "Proceedings of IEEE International Conference on Imaging, Vision &\n  Pattern Recognition, (IVPR 2020, Japan)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating an image from a provided descriptive text is quite a challenging\ntask because of the difficulty in incorporating perceptual information (object\nshapes, colors, and their interactions) along with providing high relevancy\nrelated to the provided text. Current methods first generate an initial\nlow-resolution image, which typically has irregular object shapes, colors, and\ninteraction between objects. This initial image is then improved by\nconditioning on the text. However, these methods mainly address the problem of\nusing text representation efficiently in the refinement of the initially\ngenerated image, while the success of this refinement process depends heavily\non the quality of the initially generated image, as pointed out in the DM-GAN\npaper. Hence, we propose a method to provide good initialized images by\nincorporating perceptual understanding in the discriminator module. We improve\nthe perceptual information at the first stage itself, which results in\nsignificant improvement in the final generated image. In this paper, we have\napplied our approach to the novel StackGAN architecture. We then show that the\nperceptual information included in the initial image is improved while modeling\nimage distribution at multiple stages. Finally, we generated realistic\nmulti-colored images conditioned by text. These images have good quality along\nwith containing improved basic perceptual information. More importantly, the\nproposed method can be integrated into the pipeline of other state-of-the-art\ntext-based-image-generation models to generate initial low-resolution images.\nWe also worked on improving the refinement process in StackGAN by augmenting\nthe third stage of the generator-discriminator pair in the StackGAN\narchitecture. Our experimental analysis and comparison with the\nstate-of-the-art on a large but sparse dataset MS COCO further validate the\nusefulness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:23:08 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Garg", "Kanish", ""], ["Singh", "Ajeet kumar", ""], ["Herremans", "Dorien", ""], ["Lall", "Brejesh", ""]]}, {"id": "2007.00981", "submitter": "Marcelo Saval Calvo", "authors": "Andr\\'es Fuster-Guill\\'o, Jorge Azor\\'in-L\\'opez, Marcelo Saval-Calvo,\n  Juan Miguel Castillo-Zaragoza, Nahuel Garcia-DUrso, Robert B Fisher", "title": "RGB-D-based Framework to Acquire, Visualize and Measure the Human Body\n  for Dietetic Treatments", "comments": null, "journal-ref": "Sensors, 20(13), 3690 (2020)", "doi": "10.3390/s20133690", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research aims to improve dietetic-nutritional treatment using\nstate-of-the-art RGB-D sensors and virtual reality (VR) technology. Recent\nstudies show that adherence to treatment can be improved using multimedia\ntechnologies. However, there are few studies using 3D data and VR technologies\nfor this purpose. On the other hand, obtaining 3D measurements of the human\nbody and analyzing them over time (4D) in patients undergoing dietary treatment\nis a challenging field. The main contribution of the work is to provide a\nframework to study the effect of 4D body model visualization on adherence to\nobesity treatment. The system can obtain a complete 3D model of a body using\nlow-cost technology, allowing future straightforward transference with\nsufficient accuracy and realistic visualization, enabling the analysis of the\nevolution (4D) of the shape during the treatment of obesity. The 3D body models\nwill be used for studying the effect of visualization on adherence to obesity\ntreatment using 2D and VR devices. Moreover, we will use the acquired 3D models\nto obtain measurements of the body. An analysis of the accuracy of the proposed\nmethods for obtaining measurements with both synthetic and real objects has\nbeen carried out.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:30:47 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Fuster-Guill\u00f3", "Andr\u00e9s", ""], ["Azor\u00edn-L\u00f3pez", "Jorge", ""], ["Saval-Calvo", "Marcelo", ""], ["Castillo-Zaragoza", "Juan Miguel", ""], ["Garcia-DUrso", "Nahuel", ""], ["Fisher", "Robert B", ""]]}, {"id": "2007.00992", "submitter": "Dongyoon Han", "authors": "Dongyoon Han, Sangdoo Yun, Byeongho Heo, and YoungJoon Yoo", "title": "Rethinking Channel Dimensions for Efficient Model Design", "comments": "13 pages, 8 figures, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an efficient model within the limited computational cost is\nchallenging. We argue the accuracy of a lightweight model has been further\nlimited by the design convention: a stage-wise configuration of the channel\ndimensions, which looks like a piecewise linear function of the network stage.\nIn this paper, we study an effective channel dimension configuration towards\nbetter performance than the convention. To this end, we empirically study how\nto design a single layer properly by analyzing the rank of the output feature.\nWe then investigate the channel configuration of a model by searching network\narchitectures concerning the channel configuration under the computational cost\nrestriction. Based on the investigation, we propose a simple yet effective\nchannel configuration that can be parameterized by the layer index. As a\nresult, our proposed model following the channel parameterization achieves\nremarkable performance on ImageNet classification and transfer learning tasks\nincluding COCO object detection, COCO instance segmentation, and fine-grained\nclassifications. Code and ImageNet pretrained models are available at\nhttps://github.com/clovaai/rexnet.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:01:12 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 06:11:17 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 06:14:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Han", "Dongyoon", ""], ["Yun", "Sangdoo", ""], ["Heo", "Byeongho", ""], ["Yoo", "YoungJoon", ""]]}, {"id": "2007.01001", "submitter": "A. K. Qin", "authors": "Ziqiang Li, Hong Pan, Yaping Zhu, A. K. Qin", "title": "PGD-UNet: A Position-Guided Deformable Network for Simultaneous\n  Segmentation of Organs and Tumors", "comments": "Accepted by the 2020 International Joint Conference on Neural\n  Networks (IJCNN 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise segmentation of organs and tumors plays a crucial role in clinical\napplications. It is a challenging task due to the irregular shapes and various\nsizes of organs and tumors as well as the significant class imbalance between\nthe anatomy of interest (AOI) and the background region. In addition, in most\nsituation tumors and normal organs often overlap in medical images, but current\napproaches fail to delineate both tumors and organs accurately. To tackle such\nchallenges, we propose a position-guided deformable UNet, namely PGD-UNet,\nwhich exploits the spatial deformation capabilities of deformable convolution\nto deal with the geometric transformation of both organs and tumors. Position\ninformation is explicitly encoded into the network to enhance the capabilities\nof deformation. Meanwhile, we introduce a new pooling module to preserve\nposition information lost in conventional max-pooling operation. Besides, due\nto unclear boundaries between different structures as well as the subjectivity\nof annotations, labels are not necessarily accurate for medical image\nsegmentation tasks. It may cause the overfitting of the trained network due to\nlabel noise. To address this issue, we formulate a novel loss function to\nsuppress the influence of potential label noise on the training process. Our\nmethod was evaluated on two challenging segmentation tasks and achieved very\npromising segmentation accuracy in both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:23:57 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Li", "Ziqiang", ""], ["Pan", "Hong", ""], ["Zhu", "Yaping", ""], ["Qin", "A. K.", ""]]}, {"id": "2007.01016", "submitter": "A. K. Qin", "authors": "Boyu Zhang, A. K. Qin, Hong Pan, Timos Sellis", "title": "A Novel DNN Training Framework via Data Sampling and Multi-Task\n  Optimization", "comments": "Accepted by the 2020 International Joint Conference on Neural\n  Networks (IJCNN 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional DNN training paradigms typically rely on one training set and\none validation set, obtained by partitioning an annotated dataset used for\ntraining, namely gross training set, in a certain way. The training set is used\nfor training the model while the validation set is used to estimate the\ngeneralization performance of the trained model as the training proceeds to\navoid over-fitting. There exist two major issues in this paradigm. Firstly, the\nvalidation set may hardly guarantee an unbiased estimate of generalization\nperformance due to potential mismatching with test data. Secondly, training a\nDNN corresponds to solve a complex optimization problem, which is prone to\ngetting trapped into inferior local optima and thus leads to undesired training\nresults. To address these issues, we propose a novel DNN training framework. It\ngenerates multiple pairs of training and validation sets from the gross\ntraining set via random splitting, trains a DNN model of a pre-specified\nstructure on each pair while making the useful knowledge (e.g., promising\nnetwork parameters) obtained from one model training process to be transferred\nto other model training processes via multi-task optimization, and outputs the\nbest, among all trained models, which has the overall best performance across\nthe validation sets from all pairs. The knowledge transfer mechanism featured\nin this new framework can not only enhance training effectiveness by helping\nthe model training process to escape from local optima but also improve on\ngeneralization performance via implicit regularization imposed on one model\ntraining process from other model training processes. We implement the proposed\nframework, parallelize the implementation on a GPU cluster, and apply it to\ntrain several widely used DNN models. Experimental results demonstrate the\nsuperiority of the proposed framework over the conventional training paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:58:57 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhang", "Boyu", ""], ["Qin", "A. K.", ""], ["Pan", "Hong", ""], ["Sellis", "Timos", ""]]}, {"id": "2007.01042", "submitter": "Marcel Bengs", "authors": "Marcel Bengs, Nils Gessert, Wiebke Laffers, Dennis Eggert, Stephan\n  Westermann, Nina A. Mueller, Andreas O. H. Gerstner, Christian Betz,\n  Alexander Schlaefer", "title": "Spectral-Spatial Recurrent-Convolutional Networks for In-Vivo\n  Hyperspectral Tumor Type Classification", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of cancerous tissue is crucial for long-term patient\nsurvival. In the head and neck region, a typical diagnostic procedure is an\nendoscopic intervention where a medical expert manually assesses tissue using\nRGB camera images. While healthy and tumor regions are generally easier to\ndistinguish, differentiating benign and malignant tumors is very challenging.\nThis requires an invasive biopsy, followed by histological evaluation for\ndiagnosis. Also, during tumor resection, tumor margins need to be verified by\nhistological analysis. To avoid unnecessary tissue resection, a non-invasive,\nimage-based diagnostic tool would be very valuable. Recently, hyperspectral\nimaging paired with deep learning has been proposed for this task,\ndemonstrating promising results on ex-vivo specimens. In this work, we\ndemonstrate the feasibility of in-vivo tumor type classification using\nhyperspectral imaging and deep learning. We analyze the value of using multiple\nhyperspectral bands compared to conventional RGB images and we study several\nmachine learning models' ability to make use of the additional spectral\ninformation. Based on our insights, we address spectral and spatial processing\nusing recurrent-convolutional models for effective spectral aggregating and\nspatial feature learning. Our best model achieves an AUC of 76.3%,\nsignificantly outperforming previous conventional and deep learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:00:53 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Bengs", "Marcel", ""], ["Gessert", "Nils", ""], ["Laffers", "Wiebke", ""], ["Eggert", "Dennis", ""], ["Westermann", "Stephan", ""], ["Mueller", "Nina A.", ""], ["Gerstner", "Andreas O. H.", ""], ["Betz", "Christian", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2007.01044", "submitter": "Marcel Bengs", "authors": "Marcel Bengs, Nils Gessert, Alexander Schlaefer", "title": "4D Spatio-Temporal Convolutional Networks for Object Position Estimation\n  in OCT Volumes", "comments": "Accepted at CURAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking and localizing objects is a central problem in computer-assisted\nsurgery. Optical coherence tomography (OCT) can be employed as an optical\ntracking system, due to its high spatial and temporal resolution. Recently, 3D\nconvolutional neural networks (CNNs) have shown promising performance for pose\nestimation of a marker object using single volumetric OCT images. While this\napproach relied on spatial information only, OCT allows for a temporal stream\nof OCT image volumes capturing the motion of an object at high volumes rates.\nIn this work, we systematically extend 3D CNNs to 4D spatio-temporal CNNs to\nevaluate the impact of additional temporal information for marker object\ntracking. Across various architectures, our results demonstrate that using a\nstream of OCT volumes and employing 4D spatio-temporal convolutions leads to a\n30% lower mean absolute error compared to single volume processing with 3D\nCNNs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:02:20 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Bengs", "Marcel", ""], ["Gessert", "Nils", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2007.01053", "submitter": "Yinghao Xu", "authors": "Yinghao Xu, Ceyuan Yang, Ziwei Liu, Bo Dai, Bolei Zhou", "title": "Unsupervised Landmark Learning from Unpaired Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent attempts for unsupervised landmark learning leverage synthesized image\npairs that are similar in appearance but different in poses. These methods\nlearn landmarks by encouraging the consistency between the original images and\nthe images reconstructed from swapped appearances and poses. While synthesized\nimage pairs are created by applying pre-defined transformations, they can not\nfully reflect the real variances in both appearances and poses. In this paper,\nwe aim to open the possibility of learning landmarks on unpaired data (i.e.\nunaligned image pairs) sampled from a natural image collection, so that they\ncan be different in both appearances and poses. To this end, we propose a\ncross-image cycle consistency framework ($C^3$) which applies the\nswapping-reconstruction strategy twice to obtain the final supervision.\nMoreover, a cross-image flow module is further introduced to impose the\nequivariance between estimated landmarks across images. Through comprehensive\nexperiments, our proposed framework is shown to outperform strong baselines by\na large margin. Besides quantitative results, we also provide visualization and\ninterpretation on our learned models, which not only verifies the effectiveness\nof the learned landmarks, but also leads to important insights that are\nbeneficial for future research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:57:20 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Xu", "Yinghao", ""], ["Yang", "Ceyuan", ""], ["Liu", "Ziwei", ""], ["Dai", "Bo", ""], ["Zhou", "Bolei", ""]]}, {"id": "2007.01062", "submitter": "Ella Gale", "authors": "Ella M. Gale and Nicholas Martin and Ryan Blything and Anh Nguyen and\n  Jeffrey S. Bowers", "title": "Are there any 'object detectors' in the hidden layers of CNNs trained to\n  identify objects or scenes?", "comments": "Published in Vision Research 2020, 19 pages, 8 figures", "journal-ref": "Vision Research, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods of measuring unit selectivity have been developed with the\naim of better understanding how neural networks work. But the different\nmeasures provide divergent estimates of selectivity, and this has led to\ndifferent conclusions regarding the conditions in which selective object\nrepresentations are learned and the functional relevance of these\nrepresentations. In an attempt to better characterize object selectivity, we\nundertake a comparison of various selectivity measures on a large set of units\nin AlexNet, including localist selectivity, precision, class-conditional mean\nactivity selectivity (CCMAS), network dissection,the human interpretation of\nactivation maximization (AM) images, and standard signal-detection measures. We\nfind that the different measures provide different estimates of object\nselectivity, with precision and CCMAS measures providing misleadingly high\nestimates. Indeed, the most selective units had a poor hit-rate or a high\nfalse-alarm rate (or both) in object classification, making them poor object\ndetectors. We fail to find any units that are even remotely as selective as the\n'grandmother cell' units reported in recurrent neural networks. In order to\ngeneralize these results, we compared selectivity measures on units in VGG-16\nand GoogLeNet trained on the ImageNet or Places-365 datasets that have been\ndescribed as 'object detectors'. Again, we find poor hit-rates and high\nfalse-alarm rates for object classification. We conclude that signal-detection\nmeasures provide a better assessment of single-unit selectivity compared to\ncommon alternative approaches, and that deep convolutional networks of image\nclassification do not learn object detectors in their hidden layers.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:33:37 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Gale", "Ella M.", ""], ["Martin", "Nicholas", ""], ["Blything", "Ryan", ""], ["Nguyen", "Anh", ""], ["Bowers", "Jeffrey S.", ""]]}, {"id": "2007.01065", "submitter": "Ziyang Song", "authors": "Ziyang Song, Ziyi Yin, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen\n  Ling, Shenghao Zhang", "title": "Attention-Oriented Action Recognition for Real-Time Human-Robot\n  Interaction", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the notable progress made in action recognition tasks, not much work\nhas been done in action recognition specifically for human-robot interaction.\nIn this paper, we deeply explore the characteristics of the action recognition\ntask in interaction scenarios and propose an attention-oriented multi-level\nnetwork framework to meet the need for real-time interaction. Specifically, a\nPre-Attention network is employed to roughly focus on the interactor in the\nscene at low resolution firstly and then perform fine-grained pose estimation\nat high resolution. The other compact CNN receives the extracted skeleton\nsequence as input for action recognition, utilizing attention-like mechanisms\nto capture local spatial-temporal patterns and global semantic information\neffectively. To evaluate our approach, we construct a new action dataset\nspecially for the recognition task in interaction scenarios. Experimental\nresults on our dataset and high efficiency (112 fps at 640 x 480 RGBD) on the\nmobile computing platform (Nvidia Jetson AGX Xavier) demonstrate excellent\napplicability of our method on action recognition in real-time human-robot\ninteraction.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:41:28 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Song", "Ziyang", ""], ["Yin", "Ziyi", ""], ["Yuan", "Zejian", ""], ["Zhang", "Chong", ""], ["Chi", "Wanchao", ""], ["Ling", "Yonggen", ""], ["Zhang", "Shenghao", ""]]}, {"id": "2007.01072", "submitter": "Marcel Hildebrandt", "authors": "Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, Stephan\n  G\\\"unnemann", "title": "Scene Graph Reasoning for Visual Question Answering", "comments": "ICML Workshop Graph Representation Learning and Beyond (GRL+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is concerned with answering free-form questions\nabout an image. Since it requires a deep linguistic understanding of the\nquestion and the ability to associate it with various objects that are present\nin the image, it is an ambitious task and requires techniques from both\ncomputer vision and natural language processing. We propose a novel method that\napproaches the task by performing context-driven, sequential reasoning based on\nthe objects and their semantic and spatial relationships present in the scene.\nAs a first step, we derive a scene graph which describes the objects in the\nimage, as well as their attributes and their mutual relationships. A\nreinforcement agent then learns to autonomously navigate over the extracted\nscene graph to generate paths, which are then the basis for deriving answers.\nWe conduct a first experimental study on the challenging GQA dataset with\nmanually curated scene graphs, where our method almost reaches the level of\nhuman performance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:02:54 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Hildebrandt", "Marcel", ""], ["Li", "Hang", ""], ["Koner", "Rajat", ""], ["Tresp", "Volker", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2007.01076", "submitter": "Remis Balaniuk", "authors": "Remis Balaniuk and Olga Isupova and Steven Reece", "title": "Mining and Tailings Dam Detection In Satellite Imagery Using Deep\n  Learning", "comments": "Preprint submitted to Remote Sensing of Environment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the combination of free cloud computing, free open-source\nsoftware, and deep learning methods to analyse a real, large-scale problem: the\nautomatic country-wide identification and classification of surface mines and\nmining tailings dams in Brazil. Locations of officially registered mines and\ndams were obtained from the Brazilian government open data resource.\nMultispectral Sentinel-2 satellite imagery, obtained and processed at the\nGoogle Earth Engine platform, was used to train and test deep neural networks\nusing the TensorFlow 2 API and Google Colab platform. Fully Convolutional\nNeural Networks were used in an innovative way, to search for unregistered ore\nmines and tailing dams in large areas of the Brazilian territory. The efficacy\nof the approach is demonstrated by the discovery of 263 mines that do not have\nan official mining concession. This exploratory work highlights the potential\nof a set of new technologies, freely available, for the construction of low\ncost data science tools that have high social impact. At the same time, it\ndiscusses and seeks to suggest practical solutions for the complex and serious\nproblem of illegal mining and the proliferation of tailings dams, which pose\nhigh risks to the population and the environment, especially in developing\ncountries. Code is made publicly available at:\nhttps://github.com/remis/mining-discovery-with-deep-learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:08:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Balaniuk", "Remis", ""], ["Isupova", "Olga", ""], ["Reece", "Steven", ""]]}, {"id": "2007.01089", "submitter": "Tamami Nakano", "authors": "Tamami Nakano, Atsuya Sakata, Akihiro Kishimoto", "title": "Estimating Blink Probability for Highlight Detection in Figure Skating\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlight detection in sports videos has a broad viewership and huge\ncommercial potential. It is thus imperative to detect highlight scenes more\nsuitably for human interest with high temporal accuracy. Since people\ninstinctively suppress blinks during attention-grabbing events and\nsynchronously generate blinks at attention break points in videos, the\ninstantaneous blink rate can be utilized as a highly accurate temporal\nindicator of human interest. Therefore, in this study, we propose a novel,\nautomatic highlight detection method based on the blink rate. The method trains\na one-dimensional convolution network (1D-CNN) to assess blink rates at each\nvideo frame from the spatio-temporal pose features of figure skating videos.\nExperiments show that the method successfully estimates the blink rate in 94%\nof the video clips and predicts the temporal change in the blink rate around a\njump event with high accuracy. Moreover, the method detects not only the\nrepresentative athletic action, but also the distinctive artistic expression of\nfigure skating performance as key frames. This suggests that the\nblink-rate-based supervised learning approach enables high-accuracy highlight\ndetection that more closely matches human sensibility.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:23:03 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Nakano", "Tamami", ""], ["Sakata", "Atsuya", ""], ["Kishimoto", "Akihiro", ""]]}, {"id": "2007.01108", "submitter": "Nikita Albert", "authors": "Nikita Albert", "title": "Evaluation of Contemporary Convolutional Neural Network Architectures\n  for Detecting COVID-19 from Chest Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interpreting chest radiograph, a.ka. chest x-ray, images is a necessary and\ncrucial diagnostic tool used by medical professionals to detect and identify\nmany diseases that may plague a patient. Although the images themselves contain\na wealth of valuable information, their usefulness may be limited by how well\nthey are interpreted, especially when the reviewing radiologist may be fatigued\nor when or an experienced radiologist is unavailable. Research in the use of\ndeep learning models to analyze chest radiographs yielded impressive results\nwhere, in some instances, the models outperformed practicing radiologists.\nAmidst the COVID-19 pandemic, researchers have explored and proposed the use of\nsaid deep models to detect COVID-19 infections from radiographs as a possible\nway to help ease the strain on medical resources. In this study, we train and\nevaluate three model architectures, proposed for chest radiograph analysis,\nunder varying conditions, find issues that discount the impressive model\nperformances proposed by contemporary studies on this subject, and propose\nmethodologies to train models that yield more reliable results.. Code, scripts,\npre-trained models, and visualizations are available at\nhttps://github.com/nalbert/COVID-detection-from-radiographs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:22:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Albert", "Nikita", ""]]}, {"id": "2007.01120", "submitter": "Jianren Wang", "authors": "Jianren Wang, Yihui He", "title": "Motion Prediction in Visual Object Tracking", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.03280", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking (VOT) is an essential component for many applications,\nsuch as autonomous driving or assistive robotics. However, recent works tend to\ndevelop accurate systems based on more computationally expensive feature\nextractors for better instance matching. In contrast, this work addresses the\nimportance of motion prediction in VOT. We use an off-the-shelf object detector\nto obtain instance bounding boxes. Then, a combination of camera motion\ndecouple and Kalman filter is used for state estimation. Although our baseline\nsystem is a straightforward combination of standard methods, we obtain\nstate-of-the-art results. Our method establishes new state-of-the-art\nperformance on VOT (VOT-2016 and VOT-2018). Our proposed method improves the\nEAO on VOT-2016 from 0.472 of prior art to 0.505, from 0.410 to 0.431 on\nVOT-2018. To show the generalizability, we also test our method on video object\nsegmentation (VOS: DAVIS-2016 and DAVIS-2017) and observe consistent\nimprovement.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:29:41 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Jianren", ""], ["He", "Yihui", ""]]}, {"id": "2007.01126", "submitter": "Farzad Khalvati", "authors": "Partoo Vafaeikia, Khashayar Namdar, Farzad Khalvati", "title": "A Brief Review of Deep Multi-task Learning and Auxiliary Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) optimizes several learning tasks simultaneously and\nleverages their shared information to improve generalization and the prediction\nof the model for each task. Auxiliary tasks can be added to the main task to\nultimately boost the performance. In this paper, we provide a brief review on\nthe recent deep multi-task learning (dMTL) approaches followed by methods on\nselecting useful auxiliary tasks that can be used in dMTL to improve the\nperformance of the model for the main task.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:23:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Vafaeikia", "Partoo", ""], ["Namdar", "Khashayar", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2007.01135", "submitter": "Rasheed El-Bouri", "authors": "Rasheed el-Bouri, David Eyre, Peter Watkinson, Tingting Zhu, David\n  Clifton", "title": "Student-Teacher Curriculum Learning via Reinforcement Learning:\n  Predicting Hospital Inpatient Admission Location", "comments": "16 pages, 31 figures, In Proceedings of the 37th International\n  Conference on Machine Learning", "journal-ref": "In Proceedings of the 37th International Conference on Machine\n  Learning, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable prediction of hospital admission location is important\ndue to resource-constraints and space availability in a clinical setting,\nparticularly when dealing with patients who come from the emergency department.\nIn this work we propose a student-teacher network via reinforcement learning to\ndeal with this specific problem. A representation of the weights of the student\nnetwork is treated as the state and is fed as an input to the teacher network.\nThe teacher network's action is to select the most appropriate batch of data to\ntrain the student network on from a training set sorted according to entropy.\nBy validating on three datasets, not only do we show that our approach\noutperforms state-of-the-art methods on tabular data and performs competitively\non image recognition, but also that novel curricula are learned by the teacher\nnetwork. We demonstrate experimentally that the teacher network can actively\nlearn about the student network and guide it to achieve better performance than\nif trained alone.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:00:43 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["el-Bouri", "Rasheed", ""], ["Eyre", "David", ""], ["Watkinson", "Peter", ""], ["Zhu", "Tingting", ""], ["Clifton", "David", ""]]}, {"id": "2007.01142", "submitter": "Dr. Mohammed Javed", "authors": "Mohammed Javed and P. Nagabhushan", "title": "Automatic Page Segmentation Without Decompressing the Run-Length\n  Compressed Text Documents", "comments": "Appeared in the Ph.D. Thesis (2016) of Dr. Mohammed Javed, entitled\n  \"On the Possibility of Processing Document Images in Compressed Domain\" from\n  Department of Studies in Computer Science, University of Mysore, Karnataka,\n  India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Page segmentation is considered to be the crucial stage for the automatic\nanalysis of documents with complex layouts. This has traditionally been carried\nout in uncompressed documents, although most of the documents in real life\nexist in a compressed form warranted by the requirement to make storage and\ntransfer efficient. However, carrying out page segmentation directly in\ncompressed documents without going through the stage of decompression is a\nchallenging goal. This research paper proposes demonstrating the possibility of\ncarrying out a page segmentation operation directly in the run-length data of\nthe CCITT Group-3 compressed text document, which could be single- or\nmulti-columned and might even have some text regions in the inverted text color\nmode. Therefore, before carrying out the segmentation of the text document into\ncolumns, each column into paragraphs, each paragraph into text lines, each line\ninto words, and, finally, each word into characters, a pre-processing of the\ntext document needs to be carried out. The pre-processing stage identifies the\nnormal text regions and inverted text regions, and the inverted text regions\nare toggled to the normal mode. In the sequel to initiate column separation, a\nnew strategy of incremental assimilation of white space runs in the vertical\ndirection and the auto-estimation of certain related parameters is proposed. A\nprocedure to realize column-segmentation employing these extracted parameters\nhas been devised. Subsequently, what follows first is a two-level horizontal\nrow separation process, which segments every column into paragraphs, and in\nturn, into text-lines. Then, there is a two-level vertical column separation\nprocess, which completes the separation into words and characters.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:29:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Javed", "Mohammed", ""], ["Nagabhushan", "P.", ""]]}, {"id": "2007.01151", "submitter": "Lucas Mourot", "authors": "Lucas Mourot, Fran\\c{c}ois Le Clerc, C\\'edric Th\\'ebault and Pierre\n  Hellier", "title": "JUMPS: Joints Upsampling Method for Pose Sequences", "comments": "8 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Pose Estimation is a low-level task useful forsurveillance, human\naction recognition, and scene understandingat large. It also offers promising\nperspectives for the animationof synthetic characters. For all these\napplications, and especiallythe latter, estimating the positions of many joints\nis desirablefor improved performance and realism. To this purpose, wepropose a\nnovel method called JUMPS for increasing the numberof joints in 2D pose\nestimates and recovering occluded ormissing joints. We believe this is the\nfirst attempt to addressthe issue. We build on a deep generative model that\ncombines aGenerative Adversarial Network (GAN) and an encoder. TheGAN learns\nthe distribution of high-resolution human posesequences, the encoder maps the\ninput low-resolution sequencesto its latent space. Inpainting is obtained by\ncomputing the latentrepresentation whose decoding by the GAN generator\noptimallymatches the joints locations at the input. Post-processing a 2Dpose\nsequence using our method provides a richer representationof the character\nmotion. We show experimentally that thelocalization accuracy of the additional\njoints is on average onpar with the original pose estimates.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:36:40 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 08:10:42 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 14:11:15 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2020 15:00:49 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Mourot", "Lucas", ""], ["Clerc", "Fran\u00e7ois Le", ""], ["Th\u00e9bault", "C\u00e9dric", ""], ["Hellier", "Pierre", ""]]}, {"id": "2007.01152", "submitter": "Gabriele Valvano", "authors": "Gabriele Valvano, Andrea Leo, Sotirios A. Tsaftaris", "title": "Learning to Segment from Scribbles using Multi-scale Adversarial\n  Attention Gates", "comments": "Paper accepted for publication at: IEEE Transaction on Medical\n  Imaging - Project page:\n  https://vios-s.github.io/multiscale-adversarial-attention-gates", "journal-ref": "IEEE Transactions on Medical Imaging, 2021", "doi": "10.1109/TMI.2021.3069634", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, fine-grained image segmentation datasets, annotated at pixel-level,\nare difficult to obtain, particularly in medical imaging, where annotations\nalso require expert knowledge. Weakly-supervised learning can train models by\nrelying on weaker forms of annotation, such as scribbles. Here, we learn to\nsegment using scribble annotations in an adversarial game. With unpaired\nsegmentation masks, we train a multi-scale GAN to generate realistic\nsegmentation masks at multiple resolutions, while we use scribbles to learn\ntheir correct position in the image. Central to the model's success is a novel\nattention gating mechanism, which we condition with adversarial signals to act\nas a shape prior, resulting in better object localization at multiple scales.\nSubject to adversarial conditioning, the segmentor learns attention maps that\nare semantic, suppress the noisy activations outside the objects, and reduce\nthe vanishing gradient problem in the deeper layers of the segmentor. We\nevaluated our model on several medical (ACDC, LVSC, CHAOS) and non-medical\n(PPSS) datasets, and we report performance levels matching those achieved by\nmodels trained with fully annotated segmentation masks. We also demonstrate\nextensions in a variety of settings: semi-supervised learning; combining\nmultiple scribble sources (a crowdsourcing scenario) and multi-task learning\n(combining scribble and mask supervision). We release expert-made scribble\nannotations for the ACDC dataset, and the code used for the experiments, at\nhttps://vios-s.github.io/multiscale-adversarial-attention-gates\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:39:08 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:45:16 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 15:54:11 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Valvano", "Gabriele", ""], ["Leo", "Andrea", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2007.01192", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz and Mahmoud Hassaballah", "title": "Digit Image Recognition Using an Ensemble of One-Versus-All Deep Network\n  Classifiers", "comments": "ICTCS 2020 Camera Ready Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiclass deep network classifiers, the burden of classifying samples of\ndifferent classes is put on a single classifier. As the result the optimum\nclassification accuracy is not obtained. Also training times are large due to\nrunning the CNN training on single CPU/GPU. However it is known that using\nensembles of classifiers increases the performance. Also, the training times\ncan be reduced by running each member of the ensemble on a separate processor.\nEnsemble learning has been used in the past for traditional methods to a\nvarying extent and is a hot topic. With the advent of deep learning, ensemble\nlearning has been applied to the former as well. However, an area which is\nunexplored and has potential is One-Versus-All (OVA) deep ensemble learning. In\nthis paper we explore it and show that by using OVA ensembles of deep networks,\nimprovements in performance of deep networks can be obtained. As shown in this\npaper, the classification capability of deep networks can be further increased\nby using an ensemble of binary classification (OVA) deep networks. We implement\na novel technique for the case of digit image recognition and test and evaluate\nit on the same. In the proposed approach, a single OVA deep network classifier\nis dedicated to each category. Subsequently, OVA deep network ensembles have\nbeen investigated. Every network in an ensemble has been trained by an OVA\ntraining technique using the Stochastic Gradient Descent with Momentum\nAlgorithm (SGDMA). For classification of a test sample, the sample is presented\nto each network in the ensemble. After prediction score voting, the network\nwith the largest score is assumed to have classified the sample. The\nexperimentation has been done on the MNIST digit dataset, the USPS+ digit\ndataset, and MATLAB digit image dataset. Our proposed technique outperforms the\nbaseline on digit image recognition for all datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 15:37:39 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 13:04:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Hassaballah", "Mahmoud", ""]]}, {"id": "2007.01193", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat", "title": "Reinforcement Learning Based Handwritten Digit Recognition with\n  Two-State Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet efficient Hybrid Classifier based on Deep Learning\nand Reinforcement Learning. Q-Learning is used with two Q-states and four\nactions. Conventional techniques use feature maps extracted from Convolutional\nNeural Networks (CNNs) and include them in the Qstates along with past history.\nThis leads to difficulties with these approaches as the number of states is\nvery large number due to high dimensions of the feature maps. Since our method\nuses only two Q-states it is simple and has much lesser number of parameters to\noptimize and also thus has a straightforward reward function. Also, the\napproach uses unexplored actions for image processing vis-a-vis other\ncontemporary techniques. Three datasets have been used for benchmarking of the\napproach. These are the MNIST Digit Image Dataset, the USPS Digit Image Dataset\nand the MATLAB Digit Image Dataset. The performance of the proposed hybrid\nclassifier has been compared with other contemporary techniques like a\nwell-established Reinforcement Learning Technique, AlexNet, CNN-Nearest\nNeighbor Classifier and CNNSupport Vector Machine Classifier. Our approach\noutperforms these contemporary hybrid classifiers on all the three datasets\nused.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:23:36 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 10:17:30 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Bhat", "Ghulam Mohiuddin", ""]]}, {"id": "2007.01216", "submitter": "Joon Son Chung", "authors": "Joon Son Chung, Jaesung Huh, Arsha Nagrani, Triantafyllos Afouras,\n  Andrew Zisserman", "title": "Spot the conversation: speaker diarisation in the wild", "comments": "The dataset will be available for download from\n  http://www.robots.ox.ac.uk/~vgg/data/voxceleb/voxconverse.html . The\n  development set will be released in July 2020, and the test set will be\n  released in October 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-2337", "report-no": null, "categories": "cs.SD cs.CV eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is speaker diarisation of videos collected 'in the\nwild'. We make three key contributions. First, we propose an automatic\naudio-visual diarisation method for YouTube videos. Our method consists of\nactive speaker detection using audio-visual methods and speaker verification\nusing self-enrolled speaker models. Second, we integrate our method into a\nsemi-automatic dataset creation pipeline which significantly reduces the number\nof hours required to annotate videos with diarisation labels. Finally, we use\nthis pipeline to create a large-scale diarisation dataset called VoxConverse,\ncollected from 'in the wild' videos, which we will release publicly to the\nresearch community. Our dataset consists of overlapping speech, a large and\ndiverse speaker pool, and challenging background conditions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:55:54 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 12:07:11 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Joon Son", ""], ["Huh", "Jaesung", ""], ["Nagrani", "Arsha", ""], ["Afouras", "Triantafyllos", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2007.01217", "submitter": "Leixin Zhou", "authors": "Leixin Zhou, Xiaodong Wu", "title": "Globally Optimal Surface Segmentation using Deep Learning with Learnable\n  Smoothness Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated surface segmentation is important and challenging in many medical\nimage analysis applications. Recent deep learning based methods have been\ndeveloped for various object segmentation tasks. Most of them are a\nclassification based approach, e.g. U-net, which predicts the probability of\nbeing target object or background for each voxel. One problem of those methods\nis lacking of topology guarantee for segmented objects, and usually post\nprocessing is needed to infer the boundary surface of the object. In this\npaper, a novel model based on convolutional neural network (CNN) followed by a\nlearnable surface smoothing block is proposed to tackle the surface\nsegmentation problem with end-to-end training. To the best of our knowledge,\nthis is the first study to learn smoothness priors end-to-end with CNN for\ndirect surface segmentation with global optimality. Experiments carried out on\nSpectral Domain Optical Coherence Tomography (SD-OCT) retinal layer\nsegmentation and Intravascular Ultrasound (IVUS) vessel wall segmentation\ndemonstrated very promising results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:56:46 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhou", "Leixin", ""], ["Wu", "Xiaodong", ""]]}, {"id": "2007.01243", "submitter": "Juan I. Forcen", "authors": "J.I.Forcen, Miguel Pagola, Edurne Barrenechea and Humberto Bustince", "title": "Learning ordered pooling weights in image classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.06.028", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial pooling is an important step in computer vision systems like\nConvolutional Neural Networks or the Bag-of-Words method. The spatial pooling\npurpose is to combine neighbouring descriptors to obtain a single descriptor\nfor a given region (local or global). The resultant combined vector must be as\ndiscriminant as possible, in other words, must contain relevant information,\nwhile removing irrelevant and confusing details. Maximum and average are the\nmost common aggregation functions used in the pooling step. To improve the\naggregation of relevant information without degrading their discriminative\npower for image classification, we introduce a simple but effective scheme\nbased on Ordered Weighted Average (OWA) aggregation operators. We present a\nmethod to learn the weights of the OWA aggregation operator in a Bag-of-Words\nframework and in Convolutional Neural Networks, and provide an extensive\nevaluation showing that OWA based pooling outperforms classical aggregation\noperators.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:51:05 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 16:38:56 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Forcen", "J. I.", ""], ["Pagola", "Miguel", ""], ["Barrenechea", "Edurne", ""], ["Bustince", "Humberto", ""]]}, {"id": "2007.01247", "submitter": "Athanasios Kapoutsis Ch.", "authors": "Dimitrios I. Koutras, Athanasios Ch. Kapoutsis and Elias B.\n  Kosmatopoulos", "title": "Autonomous and cooperative design of the monitor positions for a team of\n  UAVs to maximize the quantity and quality of detected objects", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/LRA.2020.3004780", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of positioning a swarm of UAVs inside a\ncompletely unknown terrain, having as objective to maximize the overall\nsituational awareness. The situational awareness is expressed by the number and\nquality of unique objects of interest, inside the UAVs' fields of view. YOLOv3\nand a system to identify duplicate objects of interest were employed to assign\na single score to each UAVs' configuration. Then, a novel navigation algorithm,\ncapable of optimizing the previously defined score, without taking into\nconsideration the dynamics of either UAVs or environment, is proposed. A\ncornerstone of the proposed approach is that it shares the same convergence\ncharacteristics as the block coordinate descent (BCD) family of approaches. The\neffectiveness and performance of the proposed navigation scheme were evaluated\nutilizing a series of experiments inside the AirSim simulator. The experimental\nevaluation indicates that the proposed navigation algorithm was able to\nconsistently navigate the swarm of UAVs to \"strategic\" monitoring positions and\nalso adapt to the different number of swarm sizes. Source code is available at\nhttps://github.com/dimikout3/ConvCAOAirSim.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:52:57 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Koutras", "Dimitrios I.", ""], ["Kapoutsis", "Athanasios Ch.", ""], ["Kosmatopoulos", "Elias B.", ""]]}, {"id": "2007.01251", "submitter": "Nicolas Basty", "authors": "Nicolas Basty, Yi Liu, Madeleine Cule, E. Louise Thomas, Jimmy D. Bell\n  and Brandon Whitcher", "title": "Image Processing and Quality Control for Abdominal Magnetic Resonance\n  Imaging in the UK Biobank", "comments": "Fixed 2 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An end-to-end image analysis pipeline is presented for the abdominal MRI\nprotocol used in the UK Biobank on the first 38,971 participants. Emphasis is\non the processing steps necessary to ensure a high-level of data quality and\nconsistency is produced in order to prepare the datasets for downstream\nquantitative analysis, such as segmentation and parameter estimation. Quality\ncontrol procedures have been incorporated to detect and, where possible,\ncorrect issues in the raw data. Detection of fat-water swaps in the Dixon\nseries is performed by a deep learning model and corrected automatically. Bone\njoints are predicted using a hybrid atlas-based registration and deep learning\nmodel for the shoulders, hips and knees. Simultaneous estimation of proton\ndensity fat fraction and transverse relaxivity (R2*) is performed using both\nthe magnitude and phase information for the single-slice multiecho series.\nApproximately 98.1% of the two-point Dixon acquisitions were successfully\nprocessed and passed quality control, with 99.98% of the high-resolution\nT1-weighted 3D volumes succeeding. Approximately 99.98% of the single-slice\nmultiecho acquisitions covering the liver were successfully processed and\npassed quality control, with 97.6% of the single-slice multiecho acquisitions\ncovering the pancreas succeeding. At least one fat-water swap was detected in\n1.8% of participants. With respect to the bone joints, approximately 3.3% of\nparticipants were missing at least one knee joint and 0.8% were missing at\nleast one shoulder joint. For the participants who received both single-slice\nmultiecho acquisition protocols for the liver a systematic difference between\nthe two protocols was identified and modeled using multiple linear regression.\nThe findings presented here will be invaluable for scientists who seek to use\nimage-derived phenotypes from the abdominal MRI protocol.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:01:25 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 07:29:04 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Basty", "Nicolas", ""], ["Liu", "Yi", ""], ["Cule", "Madeleine", ""], ["Thomas", "E. Louise", ""], ["Bell", "Jimmy D.", ""], ["Whitcher", "Brandon", ""]]}, {"id": "2007.01259", "submitter": "Hui Xie", "authors": "Hui Xie, Zhe Pan, Leixin Zhou, Fahim A Zaman, Danny Chen, Jost B\n  Jonas, Yaxing Wang, and Xiaodong Wu", "title": "Globally Optimal Segmentation of Mutually Interacting Surfaces using\n  Deep Learning", "comments": "11 pages main content and reference, plus 10 pages appendix, total 21\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmentation of multiple surfaces in medical images is a challenging problem,\nfurther complicated by the frequent presence of weak boundary and mutual\ninfluence between adjacent objects. The traditional graph-based optimal surface\nsegmentation method has proven its effectiveness with its ability of capturing\nvarious surface priors in a uniform graph model. However, its efficacy heavily\nrelies on handcrafted features that are used to define the surface cost for the\n\"goodness\" of a surface. Recently, deep learning (DL) is emerging as powerful\ntools for medical image segmentation thanks to its superior feature learning\ncapability. Unfortunately, due to the scarcity of training data in medical\nimaging, it is nontrivial for DL networks to implicitly learn the global\nstructure of the target surfaces, including surface interactions. In this work,\nwe propose to parameterize the surface cost functions in the graph model and\nleverage DL to learn those parameters. The multiple optimal surfaces are then\nsimultaneously detected by minimizing the total surface cost while explicitly\nenforcing the mutual surface interaction constraints. The optimization problem\nis solved by the primal-dual Internal Point Method, which can be implemented by\na layer of neural networks, enabling efficient end-to-end training of the whole\nnetwork. Experiments on Spectral Domain Optical Coherence Tomography (SD-OCT)\nretinal layer segmentation and Intravascular Ultrasound (IVUS) vessel wall\nsegmentation demonstrated very promising results. All source code is public to\nfacilitate further research at this direction.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:13:35 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 19:54:06 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 16:08:35 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Xie", "Hui", ""], ["Pan", "Zhe", ""], ["Zhou", "Leixin", ""], ["Zaman", "Fahim A", ""], ["Chen", "Danny", ""], ["Jonas", "Jost B", ""], ["Wang", "Yaxing", ""], ["Wu", "Xiaodong", ""]]}, {"id": "2007.01261", "submitter": "Luyu Yang", "authors": "Luyu Yang, Yogesh Balaji, Ser-Nam Lim, Abhinav Shrivastava", "title": "Curriculum Manager for Source Selection in Multi-Source Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Multi-Source Unsupervised Domain Adaptation depends\nsignificantly on the effectiveness of transfer from labeled source domain\nsamples. In this paper, we proposed an adversarial agent that learns a dynamic\ncurriculum for source samples, called Curriculum Manager for Source Selection\n(CMSS). The Curriculum Manager, an independent network module, constantly\nupdates the curriculum during training, and iteratively learns which domains or\nsamples are best suited for aligning to the target. The intuition behind this\nis to force the Curriculum Manager to constantly re-measure the transferability\nof latent domains over time to adversarially raise the error rate of the domain\ndiscriminator. CMSS does not require any knowledge of the domain labels, yet it\noutperforms other methods on four well-known benchmarks by significant margins.\nWe also provide interpretable results that shed light on the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:15:01 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Yang", "Luyu", ""], ["Balaji", "Yogesh", ""], ["Lim", "Ser-Nam", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2007.01272", "submitter": "S\\'ebastien Ehrhardt", "authors": "Sebastien Ehrhardt and Oliver Groth and Aron Monszpart and Martin\n  Engelcke and Ingmar Posner and Niloy Mitra and Andrea Vedaldi", "title": "RELATE: Physically Plausible Multi-Object Scene Synthesis Using\n  Structured Latent Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RELATE, a model that learns to generate physically plausible\nscenes and videos of multiple interacting objects. Similar to other generative\napproaches, RELATE is trained end-to-end on raw, unlabeled data. RELATE\ncombines an object-centric GAN formulation with a model that explicitly\naccounts for correlations between individual objects. This allows the model to\ngenerate realistic scenes and videos from a physically-interpretable\nparameterization. Furthermore, we show that modeling the object correlation is\nnecessary to learn to disentangle object positions and identity. We find that\nRELATE is also amenable to physically realistic scene editing and that it\nsignificantly outperforms prior art in object-centric scene generation in both\nsynthetic (CLEVR, ShapeStacks) and real-world data (cars). In addition, in\ncontrast to state-of-the-art methods in object-centric generative modeling,\nRELATE also extends naturally to dynamic scenes and generates videos of high\nvisual fidelity. Source code, datasets and more results are available at\nhttp://geometry.cs.ucl.ac.uk/projects/2020/relate/.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:27:27 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 18:03:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ehrhardt", "Sebastien", ""], ["Groth", "Oliver", ""], ["Monszpart", "Aron", ""], ["Engelcke", "Martin", ""], ["Posner", "Ingmar", ""], ["Mitra", "Niloy", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2007.01289", "submitter": "Yedid Hoshen", "authors": "Yael Vinker and Eliahu Horwitz and Nir Zabari and Yedid Hoshen", "title": "Deep Single Image Manipulation", "comments": "Project page: http://www.vision.huji.ac.il/deepsim/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image manipulation has attracted much research over the years due to the\npopularity and commercial importance of the task. In recent years, deep neural\nnetwork methods have been proposed for many image manipulation tasks. A major\nissue with deep methods is the need to train on large amounts of data from the\nsame distribution as the target image, whereas collecting datasets encompassing\nthe entire long-tail of images is impossible. In this paper, we demonstrate\nthat simply training a conditional adversarial generator on the single target\nimage is sufficient for performing complex image manipulations. We find that\nthe key for enabling single image training is extensive augmentation of the\ninput image and provide a novel augmentation method. Our network learns to map\nbetween a primitive representation of the image (e.g. edges) to the image\nitself. At manipulation time, our generator allows for making general image\nchanges by modifying the primitive input representation and mapping it through\nthe network. We extensively evaluate our method and find that it provides\nremarkable performance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:55:27 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Vinker", "Yael", ""], ["Horwitz", "Eliahu", ""], ["Zabari", "Nir", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2007.01293", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Raymond A. Yeh, Alexander G. Schwing", "title": "Not All Unlabeled Data are Equal: Learning to Weight Data in\n  Semi-supervised Learning", "comments": "NeurIPS camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing semi-supervised learning (SSL) algorithms use a single weight to\nbalance the loss of labeled and unlabeled examples, i.e., all unlabeled\nexamples are equally weighted. But not all unlabeled data are equal. In this\npaper we study how to use a different weight for every unlabeled example.\nManual tuning of all those weights -- as done in prior work -- is no longer\npossible. Instead, we adjust those weights via an algorithm based on the\ninfluence function, a measure of a model's dependency on one training example.\nTo make the approach efficient, we propose a fast and effective approximation\nof the influence function. We demonstrate that this technique outperforms\nstate-of-the-art methods on semi-supervised image and language classification\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:59:05 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 04:29:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Yeh", "Raymond A.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2007.01294", "submitter": "Han Hu", "authors": "Ze Liu and Han Hu and Yue Cao and Zheng Zhang and Xin Tong", "title": "A Closer Look at Local Aggregation Operators in Point Cloud Analysis", "comments": "Code available at https://github.com/zeliu98/CloserLook3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of network architecture for point cloud processing are mainly\ndriven by new designs of local aggregation operators. However, the impact of\nthese operators to network performance is not carefully investigated due to\ndifferent overall network architecture and implementation details in each\nsolution. Meanwhile, most of operators are only applied in shallow\narchitectures. In this paper, we revisit the representative local aggregation\noperators and study their performance using the same deep residual\narchitecture. Our investigation reveals that despite the different designs of\nthese operators, all of these operators make surprisingly similar contributions\nto the network performance under the same network input and feature numbers and\nresult in the state-of-the-art accuracy on standard benchmarks. This finding\nstimulate us to rethink the necessity of sophisticated design of local\naggregation operator for point cloud processing. To this end, we propose a\nsimple local aggregation operator without learnable weights, named Position\nPooling (PosPool), which performs similarly or slightly better than existing\nsophisticated operators. In particular, a simple deep residual network with\nPosPool layers achieves outstanding performance on all benchmarks, which\noutperforms the previous state-of-the methods on the challenging PartNet\ndatasets by a large margin (7.4 mIoU). The code is publicly available at\nhttps://github.com/zeliu98/CloserLook3D\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:59:12 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Ze", ""], ["Hu", "Han", ""], ["Cao", "Yue", ""], ["Zhang", "Zheng", ""], ["Tong", "Xin", ""]]}, {"id": "2007.01298", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz", "title": "Image Classification by Reinforcement Learning with Two-State Q-Learning", "comments": "HICO-2021 Camera Ready Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple and efficient Hybrid Classifier is presented which is\nbased on deep learning and reinforcement learning. Here, Q-Learning has been\nused with two states and 'two or three' actions. Other techniques found in the\nliterature use feature map extracted from Convolutional Neural Networks and use\nthese in the Q-states along with past history. This leads to technical\ndifficulties in these approaches because the number of states is high due to\nlarge dimensions of the feature map. Because the proposed technique uses only\ntwo Q-states it is straightforward and consequently has much lesser number of\noptimization parameters, and thus also has a simple reward function. Also, the\nproposed technique uses novel actions for processing images as compared to\nother techniques found in literature. The performance of the proposed technique\nis compared with other recent algorithms like ResNet50, InceptionV3, etc. on\npopular databases including ImageNet, Cats and Dogs Dataset, and Caltech-101\nDataset. The proposed approach outperforms others techniques on all the\ndatasets used.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:54:48 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:45:58 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 13:23:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hafiz", "Abdul Mueed", ""]]}, {"id": "2007.01309", "submitter": "Nik Dennler", "authors": "Nik Dennler, Antonio Foncubierta-Rodriguez, Titus Neupert, Marilyne\n  Sousa", "title": "Learning-based Defect Recognition for Quasi-Periodic Microscope Images", "comments": "11 pages + references and appendix, 5 figures. V2: Added references.\n  Corrected typos. Elaborated methodology. In sample figure, replaced grain\n  boundary image with more representative image. Results are unchanged", "journal-ref": null, "doi": "10.1016/j.micron.2021.103069", "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling crystalline material defects is crucial, as they affect\nproperties of the material that may be detrimental or beneficial for the final\nperformance of a device. Defect analysis on the sub-nanometer scale is enabled\nby high-resolution (scanning) transmission electron microscopy [HR(S)TEM],\nwhere the identification of defects is currently carried out based on human\nexpertise. However, the process is tedious, highly time consuming and, in some\ncases, yields ambiguous results. Here we propose a semi-supervised machine\nlearning method that assists in the detection of lattice defects from atomic\nresolution microscope images. It involves a convolutional neural network that\nclassifies image patches as defective or non-defective, a graph-based heuristic\nthat chooses one non-defective patch as a model, and finally an automatically\ngenerated convolutional filter bank, which highlights symmetry breaking such as\nstacking faults, twin defects and grain boundaries. Additionally, we suggest a\nvariance filter to segment amorphous regions and beam defects. The algorithm is\ntested on III-V/Si crystalline materials and successfully evaluated against\ndifferent metrics, showing promising results even for extremely small training\ndata sets. By combining the data-driven classification generality, robustness\nand speed of deep learning with the effectiveness of image filters in\nsegmenting faulty symmetry arrangements, we provide a valuable open-source tool\nto the microscopist community that can streamline future HR(S)TEM analyses of\ncrystalline materials.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:00:02 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 11:14:57 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Dennler", "Nik", ""], ["Foncubierta-Rodriguez", "Antonio", ""], ["Neupert", "Titus", ""], ["Sousa", "Marilyne", ""]]}, {"id": "2007.01335", "submitter": "Francois Drielsma", "authors": "Francois Drielsma, Qing Lin, Pierre C\\^ote de Soux, Laura Domin\\'e,\n  Ran Itay, Dae Heun Koh, Bradley J. Nelson, Kazuhiro Terao, Ka Vang Tsang,\n  Tracy L. Usher", "title": "Clustering of Electromagnetic Showers and Particle Interactions with\n  Graph Neural Networks in Liquid Argon Time Projection Chambers Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid Argon Time Projection Chambers (LArTPCs) are a class of detectors that\nproduce high resolution images of charged particles within their sensitive\nvolume. In these images, the clustering of distinct particles into\nsuperstructures is of central importance to the current and future neutrino\nphysics program. Electromagnetic (EM) activity typically exhibits spatially\ndetached fragments of varying morphology and orientation that are challenging\nto efficiently assemble using traditional algorithms. Similarly, particles that\nare spatially removed from each other in the detector may originate from a\ncommon interaction. Graph Neural Networks (GNNs) were developed in recent years\nto find correlations between objects embedded in an arbitrary space. The Graph\nParticle Aggregator (GrapPA) first leverages GNNs to predict the adjacency\nmatrix of EM shower fragments and to identify the origin of showers, i.e.\nprimary fragments. On the PILArNet public LArTPC simulation dataset, the\nalgorithm achieves achieves a shower clustering accuracy characterized by a\nmean adjusted Rand index (ARI) of 97.8 % and a primary identification accuracy\nof 99.8 %. It yields a relative shower energy resolution of $(4.1+1.4/\\sqrt{E\n(\\text{GeV})})\\,\\%$ and a shower direction resolution of\n$(2.1/\\sqrt{E(\\text{GeV})})^{\\circ}$. The optimized algorithm is then applied\nto the related task of clustering particle instances into interactions and\nyields a mean ARI of 99.2 % for an interaction density of\n$\\sim\\mathcal{O}(1)\\,m^{-3}$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:32:25 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 16:34:10 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 04:42:41 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Drielsma", "Francois", ""], ["Lin", "Qing", ""], ["de Soux", "Pierre C\u00f4te", ""], ["Domin\u00e9", "Laura", ""], ["Itay", "Ran", ""], ["Koh", "Dae Heun", ""], ["Nelson", "Bradley J.", ""], ["Terao", "Kazuhiro", ""], ["Tsang", "Ka Vang", ""], ["Usher", "Tracy L.", ""]]}, {"id": "2007.01356", "submitter": "Yifei Wang", "authors": "Yifei Wang, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng\n  Yang", "title": "Decoder-free Robustness Disentanglement without (Additional) Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Training (AT) is proposed to alleviate the adversarial\nvulnerability of machine learning models by extracting only robust features\nfrom the input, which, however, inevitably leads to severe accuracy reduction\nas it discards the non-robust yet useful features. This motivates us to\npreserve both robust and non-robust features and separate them with\ndisentangled representation learning. Our proposed Adversarial Asymmetric\nTraining (AAT) algorithm can reliably disentangle robust and non-robust\nrepresentations without additional supervision on robustness. Empirical results\nshow our method does not only successfully preserve accuracy by combining two\nrepresentations, but also achieve much better disentanglement than previous\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:51:40 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wang", "Yifei", ""], ["Peng", "Dan", ""], ["Liu", "Furui", ""], ["Li", "Zhenguo", ""], ["Chen", "Zhitang", ""], ["Yang", "Jiansheng", ""]]}, {"id": "2007.01369", "submitter": "Abhinav Goel", "authors": "Abhinav Goel, Caleb Tung, Sara Aghajanzadeh, Isha Ghodgaonkar, Shreya\n  Ghosh, George K. Thiruvathukal, Yung-Hsiang Lu", "title": "Low-Power Object Counting with Hierarchical Neural Networks", "comments": "Paper accepted to ISLPED 2020: ACM/IEEE International Symposium on\n  Low Power Electronics and Design", "journal-ref": null, "doi": "10.1145/3370748.3406569", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) can achieve state-of-the-art accuracy in many\ncomputer vision tasks, such as object counting. Object counting takes two\ninputs: an image and an object query and reports the number of occurrences of\nthe queried object. To achieve high accuracy on such tasks, DNNs require\nbillions of operations, making them difficult to deploy on\nresource-constrained, low-power devices. Prior work shows that a significant\nnumber of DNN operations are redundant and can be eliminated without affecting\nthe accuracy. To reduce these redundancies, we propose a hierarchical DNN\narchitecture for object counting. This architecture uses a Region Proposal\nNetwork (RPN) to propose regions-of-interest (RoIs) that may contain the\nqueried objects. A hierarchical classifier then efficiently finds the RoIs that\nactually contain the queried objects. The hierarchy contains groups of visually\nsimilar object categories. Small DNNs are used at each node of the hierarchy to\nclassify between these groups. The RoIs are incrementally processed by the\nhierarchical classifier. If the object in an RoI is in the same group as the\nqueried object, then the next DNN in the hierarchy processes the RoI further;\notherwise, the RoI is discarded. By using a few small DNNs to process each\nimage, this method reduces the memory requirement, inference time, energy\nconsumption, and number of operations with negligible accuracy loss when\ncompared with the existing object counters.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:13:01 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Goel", "Abhinav", ""], ["Tung", "Caleb", ""], ["Aghajanzadeh", "Sara", ""], ["Ghodgaonkar", "Isha", ""], ["Ghosh", "Shreya", ""], ["Thiruvathukal", "George K.", ""], ["Lu", "Yung-Hsiang", ""]]}, {"id": "2007.01377", "submitter": "Somdip Dey Mr.", "authors": "Somdip Dey, Amit Kumar Singh, Xiaohang Wang, and Klaus Dieter\n  McDonald-Maier", "title": "DATE: Defense Against TEmperature Side-Channel Attacks in DVFS Enabled\n  MPSoCs", "comments": "13 pages, 18 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the constant rise in utilizing embedded devices in daily life, side\nchannels remain a challenge to information flow control and security in such\nsystems. One such important security flaw could be exploited through\ntemperature side-channel attacks, where heat dissipation and propagation from\nthe processing elements are observed over time in order to deduce security\nflaws. In our proposed methodology, DATE: Defense Against TEmperature\nside-channel attacks, we propose a novel approach of reducing spatial and\ntemporal thermal gradient, which makes the system more secure against\ntemperature side-channel attacks, and at the same time increases the\nreliability of the device in terms of lifespan. In this paper, we have also\nintroduced a new metric, Thermal-Security-in-Multi-Processors (TSMP), which is\ncapable of quantifying the security against temperature side-channel attacks on\ncomputing systems, and DATE is evaluated to be 139.24% more secure at the most\nfor certain applications than the state-of-the-art, while reducing thermal\ncycle by 67.42% at the most.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:41:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dey", "Somdip", ""], ["Singh", "Amit Kumar", ""], ["Wang", "Xiaohang", ""], ["McDonald-Maier", "Klaus Dieter", ""]]}, {"id": "2007.01381", "submitter": "Renu Sharma", "authors": "Renu Sharma and Arun Ross", "title": "D-NetPAD: An Explainable and Interpretable Iris Presentation Attack\n  Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An iris recognition system is vulnerable to presentation attacks, or PAs,\nwhere an adversary presents artifacts such as printed eyes, plastic eyes, or\ncosmetic contact lenses to circumvent the system. In this work, we propose an\neffective and robust iris PA detector called D-NetPAD based on the DenseNet\nconvolutional neural network architecture. It demonstrates generalizability\nacross PA artifacts, sensors and datasets. Experiments conducted on a\nproprietary dataset and a publicly available dataset (LivDet-2017) substantiate\nthe effectiveness of the proposed method for iris PA detection. The proposed\nmethod results in a true detection rate of 98.58\\% at a false detection rate of\n0.2\\% on the proprietary dataset and outperfoms state-of-the-art methods on the\nLivDet-2017 dataset. We visualize intermediate feature distributions and\nfixation heatmaps using t-SNE plots and Grad-CAM, respectively, in order to\nexplain the performance of D-NetPAD. Further, we conduct a frequency analysis\nto explain the nature of features being extracted by the network. The source\ncode and trained model are available at https://github.com/iPRoBe-lab/D-NetPAD.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:44:36 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Sharma", "Renu", ""], ["Ross", "Arun", ""]]}, {"id": "2007.01383", "submitter": "David Ho", "authors": "David Joon Ho, Narasimhan P. Agaram, Peter J. Schueffler, Chad M.\n  Vanderbilt, Marc-Henri Jean, Meera R. Hameed, Thomas J. Fuchs", "title": "Deep Interactive Learning: An Efficient Labeling Approach for Deep\n  Learning-Based Osteosarcoma Treatment Response Assessment", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59722-1_52", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteosarcoma is the most common malignant primary bone tumor. Standard\ntreatment includes pre-operative chemotherapy followed by surgical resection.\nThe response to treatment as measured by ratio of necrotic tumor area to\noverall tumor area is a known prognostic factor for overall survival. This\nassessment is currently done manually by pathologists by looking at glass\nslides under the microscope which may not be reproducible due to its subjective\nnature. Convolutional neural networks (CNNs) can be used for automated\nsegmentation of viable and necrotic tumor on osteosarcoma whole slide images.\nOne bottleneck for supervised learning is that large amounts of accurate\nannotations are required for training which is a time-consuming and expensive\nprocess. In this paper, we describe Deep Interactive Learning (DIaL) as an\nefficient labeling approach for training CNNs. After an initial labeling step\nis done, annotators only need to correct mislabeled regions from previous\nsegmentation predictions to improve the CNN model until the satisfactory\npredictions are achieved. Our experiments show that our CNN model trained by\nonly 7 hours of annotation using DIaL can successfully estimate ratios of\nnecrosis within expected inter-observer variation rate for non-standardized\nmanual surgical pathology task.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:46:53 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ho", "David Joon", ""], ["Agaram", "Narasimhan P.", ""], ["Schueffler", "Peter J.", ""], ["Vanderbilt", "Chad M.", ""], ["Jean", "Marc-Henri", ""], ["Hameed", "Meera R.", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "2007.01386", "submitter": "James Davis", "authors": "Jim Davis", "title": "Posterior Model Adaptation With Updated Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification approaches based on the direct estimation and analysis of\nposterior probabilities will degrade if the original class priors begin to\nchange. We prove that a unique (up to scale) solution is possible to recover\nthe data likelihoods for a test example from its original class posteriors and\ndataset priors. Given the recovered likelihoods and a set of new priors, the\nposteriors can be re-computed using Bayes' Rule to reflect the influence of the\nnew priors. The method is simple to compute and allows a dynamic update of the\noriginal posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:07:05 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Davis", "Jim", ""]]}, {"id": "2007.01388", "submitter": "Farshid Varno", "authors": "Farshid Varno and Lucas May Petry and Lisa Di Jorio and Stan Matwin", "title": "Learn Faster and Forget Slower via Fast and Stable Task Adaptation", "comments": "52 pages, 15 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Training Deep Neural Networks (DNNs) is still highly time-consuming and\ncompute-intensive. It has been shown that adapting a pretrained model may\nsignificantly accelerate this process. With a focus on classification, we show\nthat current fine-tuning techniques make the pretrained models catastrophically\nforget the transferred knowledge even before anything about the new task is\nlearned. Such rapid knowledge loss undermines the merits of transfer learning\nand may result in a much slower convergence rate compared to when the maximum\namount of knowledge is exploited. We investigate the source of this problem\nfrom different perspectives and to alleviate it, introduce Fast And Stable\nTask-adaptation (FAST), an easy to apply fine-tuning algorithm. The paper\nprovides a novel geometric perspective on how the loss landscape of source and\ntarget tasks are linked in different transfer learning strategies. We\nempirically show that compared to prevailing fine-tuning practices, FAST learns\nthe target task faster and forgets the source task slower.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:13:55 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 16:01:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Varno", "Farshid", ""], ["Petry", "Lucas May", ""], ["Di Jorio", "Lisa", ""], ["Matwin", "Stan", ""]]}, {"id": "2007.01418", "submitter": "Brian Okorn", "authors": "Brian Okorn, Mengyun Xu, Martial Hebert, David Held", "title": "Learning Orientation Distributions for Object Pose Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/IROS45743.2020.9340860", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to operate robustly in the real world, they should be aware of\ntheir uncertainty. However, most methods for object pose estimation return a\nsingle point estimate of the object's pose. In this work, we propose two\nlearned methods for estimating a distribution over an object's orientation. Our\nmethods take into account both the inaccuracies in the pose estimation as well\nas the object symmetries. Our first method, which regresses from deep learned\nfeatures to an isotropic Bingham distribution, gives the best performance for\norientation distribution estimation for non-symmetric objects. Our second\nmethod learns to compare deep features and generates a non-parameteric\nhistogram distribution. This method gives the best performance on objects with\nunknown symmetries, accurately modeling both symmetric and non-symmetric\nobjects, without any requirement of symmetry annotation. We show that both of\nthese methods can be used to augment an existing pose estimator. Our evaluation\ncompares our methods to a large number of baseline approaches for uncertainty\nestimation across a variety of different types of objects.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:30:56 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 01:12:11 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Okorn", "Brian", ""], ["Xu", "Mengyun", ""], ["Hebert", "Martial", ""], ["Held", "David", ""]]}, {"id": "2007.01419", "submitter": "Yimeng Min", "authors": "Yimeng Min", "title": "Persistent Neurons", "comments": "add some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NN)-based learning algorithms are strongly affected by the\nchoices of initialization and data distribution. Different optimization\nstrategies have been proposed for improving the learning trajectory and finding\na better optima. However, designing improved optimization strategies is a\ndifficult task under the conventional landscape view. Here, we propose\npersistent neurons, a trajectory-based strategy that optimizes the learning\ntask using information from previous converged solutions. More precisely, we\nutilize the end of trajectories and let the parameters explore new landscapes\nby penalizing the model from converging to the previous solutions under the\nsame initialization. Persistent neurons can be regarded as a stochastic\ngradient method with informed bias where individual updates are corrupted by\ndeterministic error terms. Specifically, we show that persistent neurons, under\ncertain data distribution, is able to converge to more optimal solutions while\ninitializations under popular framework find bad local minima. We further\ndemonstrate that persistent neurons helps improve the model's performance under\nboth good and poor initializations. We evaluate the full and partial persistent\nmodel and show it can be used to boost the performance on a range of NN\nstructures, such as AlexNet and residual neural network (ResNet).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:36:49 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 09:16:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Min", "Yimeng", ""]]}, {"id": "2007.01441", "submitter": "Nalini Singh", "authors": "Nalini M. Singh, Juan Eugenio Iglesias, Elfar Adalsteinsson, Adrian V.\n  Dalca, Polina Golland", "title": "Joint Frequency and Image Space Learning for Fourier Imaging", "comments": "16 pages, 13 figures, image reconstruction, motion correction,\n  denoising, magnetic resonance imaging, deep learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that neural network layers that explicitly combine frequency\nand image feature representations are a versatile building block for analysis\nof imaging data acquired in the frequency space. Our work is motivated by the\nchallenges arising in MRI acquisition where the signal is a corrupted Fourier\ntransform of the desired image. The joint learning schemes proposed and\nanalyzed in this paper enable both correction of artifacts native to the\nfrequency space and manipulation of image space representations to reconstruct\ncoherent image structures. This is in contrast to most current deep learning\napproaches for image reconstruction that apply learned data manipulations\nsolely in the frequency space or solely in the image space. We demonstrate the\nadvantages of joint convolutional learning on three diverse tasks: image\nreconstruction from undersampled acquisitions, motion correction, and image\ndenoising in brain and knee MRI. We further demonstrate advantages of the joint\nlearning approaches across training schemes using a wide variety of loss\nfunctions. Unlike purely image based and purely frequency based architectures,\nthe joint models produce consistently high quality output images across all\ntasks and datasets. Joint image and frequency space feature representations\npromise to significantly improve modeling and reconstruction of images acquired\nin the frequency space. Our code is available at\nhttps://github.com/nalinimsingh/interlacer.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 23:54:46 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 06:22:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Singh", "Nalini M.", ""], ["Iglesias", "Juan Eugenio", ""], ["Adalsteinsson", "Elfar", ""], ["Dalca", "Adrian V.", ""], ["Golland", "Polina", ""]]}, {"id": "2007.01464", "submitter": "Yirui Wang", "authors": "Haomin Chen, Yirui Wang, Kang Zheng, Weijian Li, Chi-Tung Cheng, Adam\n  P. Harrison, Jing Xiao, Gregory D. Hager, Le Lu, Chien-Hung Liao, Shun Miao", "title": "Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for\n  Accurate Pelvic Fracture Detection in X-ray Images", "comments": "ECCV 2020 (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual cues of enforcing bilaterally symmetric anatomies as normal findings\nare widely used in clinical practice to disambiguate subtle abnormalities from\nmedical images. So far, inadequate research attention has been received on\neffectively emulating this practice in CAD methods. In this work, we exploit\nsemantic anatomical symmetry or asymmetry analysis in a complex CAD scenario,\ni.e., anterior pelvic fracture detection in trauma PXRs, where semantically\npathological (refer to as fracture) and non-pathological (e.g., pose)\nasymmetries both occur. Visually subtle yet pathologically critical fracture\nsites can be missed even by experienced clinicians, when limited diagnosis time\nis permitted in emergency care. We propose a novel fracture detection framework\nthat builds upon a Siamese network enhanced with a spatial transformer layer to\nholistically analyze symmetric image features. Image features are spatially\nformatted to encode bilaterally symmetric anatomies. A new contrastive feature\nlearning component in our Siamese network is designed to optimize the deep\nimage features being more salient corresponding to the underlying semantic\nasymmetries (caused by pelvic fracture occurrences). Our proposed method have\nbeen extensively evaluated on 2,359 PXRs from unique patients (the largest\nstudy to-date), and report an area under ROC curve score of 0.9771. This is the\nhighest among state-of-the-art fracture detection methods, with improved\nclinical indications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 02:33:24 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 04:04:28 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 14:30:38 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Chen", "Haomin", ""], ["Wang", "Yirui", ""], ["Zheng", "Kang", ""], ["Li", "Weijian", ""], ["Cheng", "Chi-Tung", ""], ["Harrison", "Adam P.", ""], ["Xiao", "Jing", ""], ["Hager", "Gregory D.", ""], ["Lu", "Le", ""], ["Liao", "Chien-Hung", ""], ["Miao", "Shun", ""]]}, {"id": "2007.01466", "submitter": "Meng Cao", "authors": "Meng Cao, Haozhi Huang, Hao Wang, Xuan Wang, Li Shen, Sheng Wang,\n  Linchao Bao, Zhifeng Li, Jiebo Luo", "title": "Task-agnostic Temporally Consistent Facial Video Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has witnessed the advances in facial image editing tasks. For\nvideo editing, however, previous methods either simply apply transformations\nframe by frame or utilize multiple frames in a concatenated or iterative\nfashion, which leads to noticeable visual flickers. In addition, these methods\nare confined to dealing with one specific task at a time without any\nextensibility. In this paper, we propose a task-agnostic temporally consistent\nfacial video editing framework. Based on a 3D reconstruction model, our\nframework is designed to handle several editing tasks in a more unified and\ndisentangled manner. The core design includes a dynamic training sample\nselection mechanism and a novel 3D temporal loss constraint that fully exploits\nboth image and video datasets and enforces temporal consistency. Compared with\nthe state-of-the-art facial image editing methods, our framework generates\nvideo portraits that are more photo-realistic and temporally smooth.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 02:49:20 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cao", "Meng", ""], ["Huang", "Haozhi", ""], ["Wang", "Hao", ""], ["Wang", "Xuan", ""], ["Shen", "Li", ""], ["Wang", "Sheng", ""], ["Bao", "Linchao", ""], ["Li", "Zhifeng", ""], ["Luo", "Jiebo", ""]]}, {"id": "2007.01475", "submitter": "Xinjing Cheng", "authors": "Xinjing Cheng, Peng Wang, Yanqi Zhou, Chenye Guan and Ruigang Yang", "title": "ODE-CNN: Omnidirectional Depth Extension Networks", "comments": "Accepted by ICRA 2020, 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional 360{\\deg} camera proliferates rapidly for autonomous robots\nsince it significantly enhances the perception ability by widening the field of\nview(FoV). However, corresponding 360{\\deg} depth sensors, which are also\ncritical for the perception system, are still difficult or expensive to have.\nIn this paper, we propose a low-cost 3D sensing system that combines an\nomnidirectional camera with a calibrated projective depth camera, where the\ndepth from the limited FoV can be automatically extended to the rest of the\nrecorded omnidirectional image. To accurately recover the missing depths, we\ndesign an omnidirectional depth extension convolutional neural\nnetwork(ODE-CNN), in which a spherical feature transform layer(SFTL) is\nembedded at the end of feature encoding layers, and a deformable convolutional\nspatial propagation network(D-CSPN) is appended at the end of feature decoding\nlayers. The former resamples the neighborhood of each pixel in the\nomnidirectional coordination to the projective coordination, which reduces the\ndifficulty of feature learning, and the later automatically finds a proper\ncontext to well align the structures in the estimated depths via CNN w.r.t. the\nreference image, which significantly improves the visual quality. Finally, we\ndemonstrate the effectiveness of proposed ODE-CNN over the popular 360D dataset\nand show that ODE-CNN significantly outperforms (relatively 33% reduction\nin-depth error) other state-of-the-art (SoTA) methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:14:09 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cheng", "Xinjing", ""], ["Wang", "Peng", ""], ["Zhou", "Yanqi", ""], ["Guan", "Chenye", ""], ["Yang", "Ruigang", ""]]}, {"id": "2007.01476", "submitter": "Shipeng Fu", "authors": "Shipeng Fu, Zhen Li, Jun Xu, Ming-Ming Cheng, Zitao Liu, Xiaomin Yang", "title": "Interactive Knowledge Distillation", "comments": "This work (IAKD) and BERT-of-Theseus (see arXiv:2002.02925) are\n  concurrent works. IAKD was first submitted to CVPR2020 and now is accepted in\n  Neurocomputing. Thank the authors in BERT-of-Theseus for pointing out this\n  issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is a standard teacher-student learning framework to\ntrain a light-weight student network under the guidance of a well-trained large\nteacher network. As an effective teaching strategy, interactive teaching has\nbeen widely employed at school to motivate students, in which teachers not only\nprovide knowledge but also give constructive feedback to students upon their\nresponses, to improve their learning performance. In this work, we propose an\nInterActive Knowledge Distillation (IAKD) scheme to leverage the interactive\nteaching strategy for efficient knowledge distillation. In the distillation\nprocess, the interaction between teacher and student networks is implemented by\na swapping-in operation: randomly replacing the blocks in the student network\nwith the corresponding blocks in the teacher network. In the way, we directly\ninvolve the teacher's powerful feature transformation ability to largely boost\nthe student's performance. Experiments with typical settings of teacher-student\nnetworks demonstrate that the student networks trained by our IAKD achieve\nbetter performance than those trained by conventional knowledge distillation\nmethods on diverse image classification datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:22:04 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 07:30:36 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 07:21:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Fu", "Shipeng", ""], ["Li", "Zhen", ""], ["Xu", "Jun", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Zitao", ""], ["Yang", "Xiaomin", ""]]}, {"id": "2007.01480", "submitter": "Chih-Hsing Ho", "authors": "Chih-Hsing Ho, Shang-Ho (Lawrence) Tsai", "title": "RSAC: Regularized Subspace Approximation Classifier for Lightweight\n  Continuous Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous learning seeks to perform the learning on the data that arrives\nfrom time to time. While prior works have demonstrated several possible\nsolutions, these approaches require excessive training time as well as memory\nusage. This is impractical for applications where time and storage are\nconstrained, such as edge computing. In this work, a novel training algorithm,\nregularized subspace approximation classifier (RSAC), is proposed to achieve\nlightweight continuous learning. RSAC contains a feature reduction module and\nclassifier module with regularization. Extensive experiments show that RSAC is\nmore efficient than prior continuous learning works and outperforms these works\non various experimental settings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:38:06 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Ho", "Chih-Hsing", "", "Lawrence"], ["Shang-Ho", "", "", "Lawrence"], ["Tsai", "", ""]]}, {"id": "2007.01486", "submitter": "Shibo Shen", "authors": "Shibo Shen, Rongpeng Li, Zhifeng Zhao, Honggang Zhang, Yugeng Zhou", "title": "Learning to Prune in Training via Dynamic Channel Propagation", "comments": "accepted by ICPR-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel network training mechanism called \"dynamic\nchannel propagation\" to prune the neural networks during the training period.\nIn particular, we pick up a specific group of channels in each convolutional\nlayer to participate in the forward propagation in training time according to\nthe significance level of channel, which is defined as channel utility. The\nutility values with respect to all selected channels are updated simultaneously\nwith the error back-propagation process and will adaptively change.\nFurthermore, when the training ends, channels with high utility values are\nretained whereas those with low utility values are discarded. Hence, our\nproposed scheme trains and prunes neural networks simultaneously. We\nempirically evaluate our novel training scheme on various representative\nbenchmark datasets and advanced convolutional neural network (CNN)\narchitectures, including VGGNet and ResNet. The experiment results verify the\nsuperior performance and robust effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:02:41 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Shen", "Shibo", ""], ["Li", "Rongpeng", ""], ["Zhao", "Zhifeng", ""], ["Zhang", "Honggang", ""], ["Zhou", "Yugeng", ""]]}, {"id": "2007.01491", "submitter": "Chong Yu", "authors": "Chong Yu, Jeff Pool", "title": "Self-Supervised GAN Compression", "comments": "The appendix for this paper is in the following repository\n  https://gitlab.com/dxxz/Self-Supervised-GAN-Compression-Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning's success has led to larger and larger models to handle more\nand more complex tasks; trained models can contain millions of parameters.\nThese large models are compute- and memory-intensive, which makes it a\nchallenge to deploy them with minimized latency, throughput, and storage\nrequirements. Some model compression methods have been successfully applied to\nimage classification and detection or language models, but there has been very\nlittle work compressing generative adversarial networks (GANs) performing\ncomplex tasks. In this paper, we show that a standard model compression\ntechnique, weight pruning, cannot be applied to GANs using existing methods. We\nthen develop a self-supervised compression technique which uses the trained\ndiscriminator to supervise the training of a compressed generator. We show that\nthis framework has a compelling performance to high degrees of sparsity, can be\neasily applied to new tasks and models, and enables meaningful comparisons\nbetween different pruning granularities.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:18:54 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 16:43:57 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yu", "Chong", ""], ["Pool", "Jeff", ""]]}, {"id": "2007.01496", "submitter": "Shuo Lei", "authors": "Shuo Lei, Xuchao Zhang, Jianfeng He, Fanglan Chen, Chang-Tien Lu", "title": "Few-Shot Semantic Segmentation Augmented with Image-Level Weak\n  Annotations", "comments": "Accpeted to ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great progress made by deep neural networks in the semantic\nsegmentation task, traditional neural-networkbased methods typically suffer\nfrom a shortage of large amounts of pixel-level annotations. Recent progress in\nfewshot semantic segmentation tackles the issue by only a few pixel-level\nannotated examples. However, these few-shot approaches cannot easily be applied\nto multi-way or weak annotation settings. In this paper, we advance the\nfew-shot segmentation paradigm towards a scenario where image-level annotations\nare available to help the training process of a few pixel-level annotations.\nOur key idea is to learn a better prototype representation of the class by\nfusing the knowledge from the image-level labeled data. Specifically, we\npropose a new framework, called PAIA, to learn the class prototype\nrepresentation in a metric space by integrating image-level annotations.\nFurthermore, by considering the uncertainty of pseudo-masks, a distilled soft\nmasked average pooling strategy is designed to handle distractions in\nimage-level annotations. Extensive empirical results on two datasets show\nsuperior performance of PAIA.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:58:20 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 17:55:54 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lei", "Shuo", ""], ["Zhang", "Xuchao", ""], ["He", "Jianfeng", ""], ["Chen", "Fanglan", ""], ["Lu", "Chang-Tien", ""]]}, {"id": "2007.01499", "submitter": "Qing Li", "authors": "Qing Li, Siyuan Huang, Yining Hong, Song-Chun Zhu", "title": "A Competence-aware Curriculum for Visual Concepts Learning via Question\n  Answering", "comments": "ECCV 2020 (Oral) Camera Ready. Project page:\n  https://liqing-ustc.github.io/CL-mIRT/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can progressively learn visual concepts from easy to hard questions.\nTo mimic this efficient learning ability, we propose a competence-aware\ncurriculum for visual concept learning in a question-answering manner.\nSpecifically, we design a neural-symbolic concept learner for learning the\nvisual concepts and a multi-dimensional Item Response Theory (mIRT) model for\nguiding the learning process with an adaptive curriculum. The mIRT effectively\nestimates the concept difficulty and the model competence at each learning step\nfrom accumulated model responses. The estimated concept difficulty and model\ncompetence are further utilized to select the most profitable training samples.\nExperimental results on CLEVR show that with a competence-aware curriculum, the\nproposed method achieves state-of-the-art performances with superior data\nefficiency and convergence speed. Specifically, the proposed model only uses\n40% of training data and converges three times faster compared with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 05:08:09 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 21:57:39 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Qing", ""], ["Huang", "Siyuan", ""], ["Hong", "Yining", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2007.01500", "submitter": "Sapir Kaplan", "authors": "Sapir Kaplan and Raja Giryes", "title": "Self-supervised Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has been used recently to achieve improved\nperformance in various tasks and most prominently in image classification. Yet,\ncurrent search strategies rely on large labeled datasets, which limit their\nusage in the case where only a smaller fraction of the data is annotated.\nSelf-supervised learning has shown great promise in training neural networks\nusing unlabeled data. In this work, we propose a self-supervised neural\narchitecture search (SSNAS) that allows finding novel network models without\nthe need for labeled data. We show that such a search leads to comparable\nresults to supervised training with a \"fully labeled\" NAS and that it can\nimprove the performance of self-supervised learning. Moreover, we demonstrate\nthe advantage of the proposed approach when the number of labels in the search\nis relatively small.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 05:09:30 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kaplan", "Sapir", ""], ["Giryes", "Raja", ""]]}, {"id": "2007.01504", "submitter": "Mengxi Jia", "authors": "Mengxi Jia, Yunpeng Zhai, Shijian Lu, Siwei Ma, Jian Zhang", "title": "A Similarity Inference Metric for RGB-Infrared Cross-Modality Person\n  Re-identification", "comments": "Accepted by IJCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared (IR) cross-modality person re-identification (re-ID), which aims\nto search an IR image in RGB gallery or vice versa, is a challenging task due\nto the large discrepancy between IR and RGB modalities. Existing methods\naddress this challenge typically by aligning feature distributions or image\nstyles across modalities, whereas the very useful similarities among gallery\nsamples of the same modality (i.e. intra-modality sample similarities) is\nlargely neglected. This paper presents a novel similarity inference metric\n(SIM) that exploits the intra-modality sample similarities to circumvent the\ncross-modality discrepancy targeting optimal cross-modality image matching. SIM\nworks by successive similarity graph reasoning and mutual nearest-neighbor\nreasoning that mine cross-modality sample similarities by leveraging\nintra-modality sample similarities from two different perspectives. Extensive\nexperiments over two cross-modality re-ID datasets (SYSU-MM01 and RegDB) show\nthat SIM achieves significant accuracy improvement but with little extra\ntraining as compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 05:28:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Jia", "Mengxi", ""], ["Zhai", "Yunpeng", ""], ["Lu", "Shijian", ""], ["Ma", "Siwei", ""], ["Zhang", "Jian", ""]]}, {"id": "2007.01514", "submitter": "Shinya Matsubara", "authors": "Shinya Matsubara, Akihiko Honda, Yonghoon Ji, Kazunori Umeda", "title": "Three-dimensional Human Tracking of a Mobile Robot by Fusion of Tracking\n  Results of Two Cameras", "comments": "4 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a process that uses two cameras to obtain\nthree-dimensional (3D) information of a target object for human tracking.\nResults of human detection and tracking from two cameras are integrated to\nobtain the 3D information. OpenPose is used for human detection. In the case of\na general processing a stereo camera, a range image of the entire scene is\nacquired as precisely as possible, and then the range image is processed.\nHowever, there are problems such as incorrect matching and computational cost\nfor the calibration process. A new stereo vision framework is proposed to cope\nwith the problems. The effectiveness of the proposed framework and the method\nis verified through target-tracking experiments.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 06:46:49 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Matsubara", "Shinya", ""], ["Honda", "Akihiko", ""], ["Ji", "Yonghoon", ""], ["Umeda", "Kazunori", ""]]}, {"id": "2007.01524", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Donghyeon Cho, Priyadarshini Panda, Sungeun Hong", "title": "Progressive Domain Adaptation from a Source Pre-trained Model", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation assumes that samples from source and target domains are\nfreely accessible during a training phase. However, such an assumption is\nrarely plausible in the real-world and possibly causes data-privacy issues,\nespecially when the label of the source domain can be a sensitive attribute as\nan identifier. To avoid accessing source data that may contain sensitive\ninformation, we introduce progressive domain adaptation (PrDA). Our key idea is\nto leverage a pre-trained model from the source domain and progressively update\nthe target model in a self-learning manner. We observe that target samples with\nlower self-entropy measured by the pre-trained source model are more likely to\nbe classified correctly. From this, we select the reliable samples with the\nself-entropy criterion and define these as class prototypes. We then assign\npseudo labels for every target sample based on the similarity score with class\nprototypes. Furthermore, to reduce the uncertainty from the pseudo labeling\nprocess, we propose set-to-set distance-based filtering which does not require\nany tunable hyperparameters. Finally, we train the target model with the\nfiltered pseudo labels with regularization from the pre-trained source model.\nSurprisingly, without direct usage of labeled source samples, our PrDA\noutperforms conventional domain adaptation methods on benchmark datasets. Our\ncode is publicly available at\nhttps://github.com/youngryan1993/PrDA-Progressive-Domain-Adaptation-from-a-Source-Pre-trained-Model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 07:21:30 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 02:00:51 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 15:57:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kim", "Youngeun", ""], ["Cho", "Donghyeon", ""], ["Panda", "Priyadarshini", ""], ["Hong", "Sungeun", ""]]}, {"id": "2007.01546", "submitter": "Yunpeng Zhai", "authors": "Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji and\n  Yonghong Tian", "title": "Multiple Expert Brainstorming for Domain Adaptive Person\n  Re-identification", "comments": "Accepted by ECCV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the best performing deep neural models are ensembles of multiple\nbase-level networks, nevertheless, ensemble learning with respect to domain\nadaptive person re-ID remains unexplored. In this paper, we propose a multiple\nexpert brainstorming network (MEB-Net) for domain adaptive person re-ID,\nopening up a promising direction about model ensemble problem under\nunsupervised conditions. MEB-Net adopts a mutual learning strategy, where\nmultiple networks with different architectures are pre-trained within a source\ndomain as expert models equipped with specific features and knowledge, while\nthe adaptation is then accomplished through brainstorming (mutual learning)\namong expert models. MEB-Net accommodates the heterogeneity of experts learned\nwith different architectures and enhances discrimination capability of the\nadapted re-ID model, by introducing a regularization scheme about authority of\nexperts. Extensive experiments on large-scale datasets (Market-1501 and\nDukeMTMC-reID) demonstrate the superior performance of MEB-Net over the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:16:19 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 13:42:38 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 13:11:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhai", "Yunpeng", ""], ["Ye", "Qixiang", ""], ["Lu", "Shijian", ""], ["Jia", "Mengxi", ""], ["Ji", "Rongrong", ""], ["Tian", "Yonghong", ""]]}, {"id": "2007.01548", "submitter": "Ammar Kamoona", "authors": "Ammar Mansoor Kamoona, Amirali Khodadadian Gosta, Alireza\n  Bab-Hadiashar, Reza Hoseinnezhad", "title": "Multiple Instance-Based Video Anomaly Detection using Deep Temporal\n  Encoding-Decoding", "comments": "The paper is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a weakly supervised deep temporal encoding-decoding\nsolution for anomaly detection in surveillance videos using multiple instance\nlearning. The proposed approach uses both abnormal and normal video clips\nduring the training phase which is developed in the multiple instance framework\nwhere we treat video as a bag and video clips as instances in the bag. Our main\ncontribution lies in the proposed novel approach to consider temporal relations\nbetween video instances. We deal with video instances (clips) as a sequential\nvisual data rather than independent instances. We employ a deep temporal and\nencoder network that is designed to capture spatial-temporal evolution of video\ninstances over time. We also propose a new loss function that is smoother than\nsimilar loss functions recently presented in the computer vision literature,\nand therefore; enjoys faster convergence and improved tolerance to local minima\nduring the training phase. The proposed temporal encoding-decoding approach\nwith modified loss is benchmarked against the state-of-the-art in simulation\nstudies. The results show that the proposed method performs similar to or\nbetter than the state-of-the-art solutions for anomaly detection in video\nsurveillance applications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:22:42 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 05:53:21 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Kamoona", "Ammar Mansoor", ""], ["Gosta", "Amirali Khodadadian", ""], ["Bab-Hadiashar", "Alireza", ""], ["Hoseinnezhad", "Reza", ""]]}, {"id": "2007.01549", "submitter": "Zhenbo Xu", "authors": "Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Xiangbo Su, Yuchen Yuan,\n  Hongwu Zhang, Shilei Wen, Errui Ding, Liusheng Huang", "title": "PointTrack++ for Effective Online Multi-Object Tracking and Segmentation", "comments": "CVPR2020 MOTS Challenge Winner. PointTrack++ ranks first on KITTI\n  MOTS (http://www.cvlibs.net/datasets/kitti/eval_mots.php)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-object tracking and segmentation (MOTS) is a novel computer vision\ntask that aims to jointly perform multiple object tracking (MOT) and instance\nsegmentation. In this work, we present PointTrack++, an effective on-line\nframework for MOTS, which remarkably extends our recently proposed PointTrack\nframework. To begin with, PointTrack adopts an efficient one-stage framework\nfor instance segmentation, and learns instance embeddings by converting compact\nimage representations to un-ordered 2D point cloud. Compared with PointTrack,\nour proposed PointTrack++ offers three major improvements. Firstly, in the\ninstance segmentation stage, we adopt a semantic segmentation decoder trained\nwith focal loss to improve the instance selection quality. Secondly, to further\nboost the segmentation performance, we propose a data augmentation strategy by\ncopy-and-paste instances into training images. Finally, we introduce a better\ntraining strategy in the instance association stage to improve the\ndistinguishability of learned instance embeddings. The resulting framework\nachieves the state-of-the-art performance on the 5th BMTT MOTChallenge.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:28:37 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Xu", "Zhenbo", ""], ["Zhang", "Wei", ""], ["Tan", "Xiao", ""], ["Yang", "Wei", ""], ["Su", "Xiangbo", ""], ["Yuan", "Yuchen", ""], ["Zhang", "Hongwu", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""], ["Huang", "Liusheng", ""]]}, {"id": "2007.01550", "submitter": "Zhenbo Xu", "authors": "Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Huan Huang, Shilei Wen,\n  Errui Ding, Liusheng Huang", "title": "Segment as Points for Efficient Online Multi-Object Tracking and\n  Segmentation", "comments": "ECCV2020 ORAL (top 2%). Code already available at\n  https://github.com/detectRecog/PointTrack. A highly effective method for\n  learning features based on instance segments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multi-object tracking and segmentation (MOTS) methods follow the\ntracking-by-detection paradigm and adopt convolutions for feature extraction.\nHowever, as affected by the inherent receptive field, convolution based feature\nextraction inevitably mixes up the foreground features and the background\nfeatures, resulting in ambiguities in the subsequent instance association. In\nthis paper, we propose a highly effective method for learning instance\nembeddings based on segments by converting the compact image representation to\nun-ordered 2D point cloud representation. Our method generates a new\ntracking-by-points paradigm where discriminative instance embeddings are\nlearned from randomly selected points rather than images. Furthermore, multiple\ninformative data modalities are converted into point-wise representations to\nenrich point-wise features. The resulting online MOTS framework, named\nPointTrack, surpasses all the state-of-the-art methods including 3D tracking\nmethods by large margins (5.4% higher MOTSA and 18 times faster over\nMOTSFusion) with the near real-time speed (22 FPS). Evaluations across three\ndatasets demonstrate both the effectiveness and efficiency of our method.\nMoreover, based on the observation that current MOTS datasets lack crowded\nscenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher\ninstance density. Both APOLLO MOTS and our codes are publicly available at\nhttps://github.com/detectRecog/PointTrack.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:29:35 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Xu", "Zhenbo", ""], ["Zhang", "Wei", ""], ["Tan", "Xiao", ""], ["Yang", "Wei", ""], ["Huang", "Huan", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""], ["Huang", "Liusheng", ""]]}, {"id": "2007.01556", "submitter": "Bin Wang", "authors": "Bin Wang, Bing Xue, Mengjie Zhang", "title": "Surrogate-assisted Particle Swarm Optimisation for Evolving\n  Variable-length Transferable Blocks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have demonstrated promising performance on\nimage classification tasks, but the manual design process becomes more and more\ncomplex due to the fast depth growth and the increasingly complex topologies of\nconvolutional neural networks. As a result, neural architecture search has\nemerged to automatically design convolutional neural networks that outperform\nhandcrafted counterparts. However, the computational cost is immense, e.g.\n22,400 GPU-days and 2,000 GPU-days for two outstanding neural architecture\nsearch works named NAS and NASNet, respectively, which motivates this work. A\nnew effective and efficient surrogate-assisted particle swarm optimisation\nalgorithm is proposed to automatically evolve convolutional neural networks.\nThis is achieved by proposing a novel surrogate model, a new method of creating\na surrogate dataset and a new encoding strategy to encode variable-length\nblocks of convolutional neural networks, all of which are integrated into a\nparticle swarm optimisation algorithm to form the proposed method. The proposed\nmethod shows its effectiveness by achieving competitive error rates of 3.49% on\nthe CIFAR-10 dataset, 18.49% on the CIFAR-100 dataset, and 1.82% on the SVHN\ndataset. The convolutional neural network blocks are efficiently learned by the\nproposed method from CIFAR-10 within 3 GPU-days due to the acceleration\nachieved by the surrogate model and the surrogate dataset to avoid the training\nof 80.1% of convolutional neural network blocks represented by the particles.\nWithout any further search, the evolved blocks from CIFAR-10 can be\nsuccessfully transferred to CIFAR-100 and SVHN, which exhibits the\ntransferability of the block learned by the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:48:21 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wang", "Bin", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "2007.01571", "submitter": "Zhenwei He", "authors": "Zhenwei He and Lei Zhang", "title": "Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN", "comments": "The paper is accepted in ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional object detection models inevitably encounter a performance drop\nas the domain disparity exists. Unsupervised domain adaptive object detection\nis proposed recently to reduce the disparity between domains, where the source\ndomain is label-rich while the target domain is label-agnostic. The existing\nmodels follow a parameter shared siamese structure for adversarial domain\nalignment, which, however, easily leads to the collapse and out-of-control risk\nof the source domain and brings negative impact to feature adaption. The main\nreason is that the labeling unfairness (asymmetry) between source and target\nmakes the parameter sharing mechanism unable to adapt. Therefore, in order to\navoid the source domain collapse risk caused by parameter sharing, we propose\nan asymmetric tri-way Faster-RCNN (ATF) for domain adaptive object detection.\nOur ATF model has two distinct merits: 1) A ancillary net supervised by source\nlabel is deployed to learn ancillary target features and simultaneously\npreserve the discrimination of source domain, which enhances the structural\ndiscrimination (object classification vs. bounding box regression) of domain\nalignment. 2) The asymmetric structure consisting of a chief net and an\nindependent ancillary net essentially overcomes the parameter sharing aroused\nsource risk collapse. The adaption safety of the proposed ATF detector is\nguaranteed. Extensive experiments on a number of datasets, including\nCityscapes, Foggy-cityscapes, KITTI, Sim10k, Pascal VOC, Clipart and\nWatercolor, demonstrate the SOTA performance of our method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:30:18 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["He", "Zhenwei", ""], ["Zhang", "Lei", ""]]}, {"id": "2007.01575", "submitter": "S\\\"oren Dittmer", "authors": "S\\\"oren Dittmer, Carola-Bibiane Sch\\\"onlieb, Peter Maass", "title": "Ground Truth Free Denoising by Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learned unsupervised denoising method for arbitrary types of\ndata, which we explore on images and one-dimensional signals. The training is\nsolely based on samples of noisy data and examples of noise, which --\ncritically -- do not need to come in pairs. We only need the assumption that\nthe noise is independent and additive (although we describe how this can be\nextended). The method rests on a Wasserstein Generative Adversarial Network\nsetting, which utilizes two critics and one generator.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:39:25 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dittmer", "S\u00f6ren", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Maass", "Peter", ""]]}, {"id": "2007.01593", "submitter": "S\\\"oren Dittmer", "authors": "S\\\"oren Dittmer, Tobias Kluth, Mads Thorstein Roar Henriksen and Peter\n  Maass", "title": "Deep image prior for 3D magnetic particle imaging: A quantitative\n  comparison of regularization techniques on Open MPI dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic particle imaging (MPI) is an imaging modality exploiting the\nnonlinear magnetization behavior of (super-)paramagnetic nanoparticles to\nobtain a space- and often also time-dependent concentration of a tracer\nconsisting of these nanoparticles. MPI has a continuously increasing number of\npotential medical applications. One prerequisite for successful performance in\nthese applications is a proper solution to the image reconstruction problem.\nMore classical methods from inverse problems theory, as well as novel\napproaches from the field of machine learning, have the potential to deliver\nhigh-quality reconstructions in MPI. We investigate a novel reconstruction\napproach based on a deep image prior, which builds on representing the solution\nby a deep neural network. Novel approaches, as well as variational and\niterative regularization techniques, are compared quantitatively in terms of\npeak signal-to-noise ratios and structural similarity indices on the publicly\navailable Open MPI dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 10:13:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dittmer", "S\u00f6ren", ""], ["Kluth", "Tobias", ""], ["Henriksen", "Mads Thorstein Roar", ""], ["Maass", "Peter", ""]]}, {"id": "2007.01595", "submitter": "David Rozenberszki", "authors": "David Rozenberszki, Andras Majdik", "title": "LOL: Lidar-Only Odometry and Localization in 3D Point Cloud Maps", "comments": "Accepted paper for ICRA 2020, Github repository for implementation\n  at: https://github.com/RozDavid/LOL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the problem of odometry and localization for\nLidar-equipped vehicles driving in urban environments, where a premade target\nmap exists to localize against. In our problem formulation, to correct the\naccumulated drift of the Lidar-only odometry we apply a place recognition\nmethod to detect geometrically similar locations between the online 3D point\ncloud and the a priori offline map. In the proposed system, we integrate a\nstate-of-the-art Lidar-only odometry algorithm with a recently proposed 3D\npoint segment matching method by complementing their advantages. Also, we\npropose additional enhancements in order to reduce the number of false matches\nbetween the online point cloud and the target map, and to refine the position\nestimation error whenever a good match is detected. We demonstrate the utility\nof the proposed LOL system on several Kitti datasets of different lengths and\nenvironments, where the relocalization accuracy and the precision of the\nvehicle's trajectory were significantly improved in every case, while still\nbeing able to maintain real-time performance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 10:20:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Rozenberszki", "David", ""], ["Majdik", "Andras", ""]]}, {"id": "2007.01598", "submitter": "Xinpeng Ding", "authors": "Xinpeng Ding, Nannan Wang, Xinbo Gao, Jie Li, Xiaoyu Wang and\n  Tongliang Liu", "title": "Weakly Supervised Temporal Action Localization with Segment-Level Labels", "comments": "18 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization presents a trade-off between test performance\nand annotation-time cost. Fully supervised methods achieve good performance\nwith time-consuming boundary annotations. Weakly supervised methods with\ncheaper video-level category label annotations result in worse performance. In\nthis paper, we introduce a new segment-level supervision setting: segments are\nlabeled when annotators observe actions happening here. We incorporate this\nsegment-level supervision along with a novel localization module in the\ntraining. Specifically, we devise a partial segment loss regarded as a loss\nsampling to learn integral action parts from labeled segments. Since the\nlabeled segments are only parts of actions, the model tends to overfit along\nwith the training process. To tackle this problem, we first obtain a similarity\nmatrix from discriminative features guided by a sphere loss. Then, a\npropagation loss is devised based on the matrix to act as a regularization\nterm, allowing implicit unlabeled segments propagation during training.\nExperiments validate that our method can outperform the video-level supervision\nmethods with almost same the annotation time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 10:32:19 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Ding", "Xinpeng", ""], ["Wang", "Nannan", ""], ["Gao", "Xinbo", ""], ["Li", "Jie", ""], ["Wang", "Xiaoyu", ""], ["Liu", "Tongliang", ""]]}, {"id": "2007.01618", "submitter": "Fei-Fei Huang", "authors": "Feifei Huang, Jie Li and Xuelin Zhu", "title": "Balanced Symmetric Cross Entropy for Large Scale Imbalanced and Noisy\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolution neural network has attracted many attentions in large-scale\nvisual classification task, and achieves significant performance improvement\ncompared to traditional visual analysis methods. In this paper, we explore many\nkinds of deep convolution neural network architectures for large-scale product\nrecognition task, which is heavily class-imbalanced and noisy labeled data,\nmaking it more challenged. Extensive experiments show that PNASNet achieves\nbest performance among a variety of convolutional architectures. Together with\nensemble technology and negative learning loss for noisy labeled data, we\nfurther improve the model performance on online test data. Finally, our\nproposed method achieves 0.1515 mean top-1 error on online test data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:24:43 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Huang", "Feifei", ""], ["Li", "Jie", ""], ["Zhu", "Xuelin", ""]]}, {"id": "2007.01625", "submitter": "Fabricio Breve", "authors": "Jefferson Antonio Ribeiro Passerini and Fabricio Aparecido Breve", "title": "Complex Network Construction for Interactive Image Segmentation using\n  Particle Competition and Cooperation: A New Approach", "comments": "The 20th International Conference on Computational Science and its\n  Applications (ICCSA2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the interactive image segmentation task, the Particle Competition and\nCooperation (PCC) model is fed with a complex network, which is built from the\ninput image. In the network construction phase, a weight vector is needed to\ndefine the importance of each element in the feature set, which consists of\ncolor and location information of the corresponding pixels, thus demanding a\nspecialist's intervention. The present paper proposes the elimination of the\nweight vector through modifications in the network construction phase. The\nproposed model and the reference model, without the use of a weight vector,\nwere compared using 151 images extracted from the Grabcut dataset, the PASCAL\nVOC dataset and the Alpha matting dataset. Each model was applied 30 times to\neach image to obtain an error average. These simulations resulted in an error\nrate of only 0.49\\% when classifying pixels with the proposed model while the\nreference model had an error rate of 3.14\\%. The proposed method also presented\nless error variation in the diversity of the evaluated images, when compared to\nthe reference model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:42:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Passerini", "Jefferson Antonio Ribeiro", ""], ["Breve", "Fabricio Aparecido", ""]]}, {"id": "2007.01628", "submitter": "Wenxi Liu", "authors": "Yuzhen Niu, Jianbin Wu, Wenxi Liu, Wenzhong Guo, Rynson W.H. Lau", "title": "HDR-GAN: HDR Image Reconstruction from Multi-Exposed LDR Images with\n  Large Motions", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3064433", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high dynamic range (HDR) images from multiple low-dynamic range\n(LDR) exposures in dynamic scenes is challenging. There are two major problems\ncaused by the large motions of foreground objects. One is the severe\nmisalignment among the LDR images. The other is the missing content due to the\nover-/under-saturated regions caused by the moving objects, which may not be\neasily compensated for by the multiple LDR exposures. Thus, it requires the HDR\ngeneration model to be able to properly fuse the LDR images and restore the\nmissing details without introducing artifacts. To address these two problems,\nwe propose in this paper a novel GAN-based model, HDR-GAN, for synthesizing HDR\nimages from multi-exposed LDR images. To our best knowledge, this work is the\nfirst GAN-based approach for fusing multi-exposed LDR images for HDR\nreconstruction. By incorporating adversarial learning, our method is able to\nproduce faithful information in the regions with missing content. In addition,\nwe also propose a novel generator network, with a reference-based residual\nmerging block for aligning large object motions in the feature domain, and a\ndeep HDR supervision scheme for eliminating artifacts of the reconstructed HDR\nimages. Experimental results demonstrate that our model achieves\nstate-of-the-art reconstruction performance over the prior HDR methods on\ndiverse scenes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:42:35 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Niu", "Yuzhen", ""], ["Wu", "Jianbin", ""], ["Liu", "Wenxi", ""], ["Guo", "Wenzhong", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2007.01671", "submitter": "Youssef Dawoud", "authors": "Youssef Dawoud, Julia Hornauer, Gustavo Carneiro, and Vasileios\n  Belagiannis", "title": "Few-Shot Microscopy Image Cell Segmentation", "comments": "16 pages, 4 figures, Accepted by ECML-PKDD 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic cell segmentation in microscopy images works well with the support\nof deep neural networks trained with full supervision. Collecting and\nannotating images, though, is not a sustainable solution for every new\nmicroscopy database and cell type. Instead, we assume that we can access a\nplethora of annotated image data sets from different domains (sources) and a\nlimited number of annotated image data sets from the domain of interest\n(target), where each domain denotes not only different image appearance but\nalso a different type of cell segmentation problem. We pose this problem as\nmeta-learning where the goal is to learn a generic and adaptable few-shot\nlearning model from the available source domain data sets and cell segmentation\ntasks. The model can be afterwards fine-tuned on the few annotated images of\nthe target domain that contains different image appearance and different cell\ntype. In our meta-learning training, we propose the combination of three\nobjective functions to segment the cells, move the segmentation results away\nfrom the classification boundary using cross-domain tasks, and learn an\ninvariant representation between tasks of the source domains. Our experiments\non five public databases show promising results from 1- to 10-shot\nmeta-learning using standard segmentation neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:12:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dawoud", "Youssef", ""], ["Hornauer", "Julia", ""], ["Carneiro", "Gustavo", ""], ["Belagiannis", "Vasileios", ""]]}, {"id": "2007.01682", "submitter": "Miao Tian", "authors": "Miao Tian, Dongyan Guo, Ying Cui, Xiang Pan, Shengyong Chen", "title": "Improving auto-encoder novelty detection using channel attention and\n  entropy minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is a important research area which mainly solves the\nclassification problem of inliers which usually consists of normal samples and\noutliers composed of abnormal samples. Auto-encoder is often used for novelty\ndetection. However, the generalization ability of the auto-encoder may cause\nthe undesirable reconstruction of abnormal elements and reduce the\nidentification ability of the model. To solve the problem, we focus on the\nperspective of better reconstructing the normal samples as well as retaining\nthe unique information of normal samples to improve the performance of\nauto-encoder for novelty detection. Firstly, we introduce attention mechanism\ninto the task. Under the action of attention mechanism, auto-encoder can pay\nmore attention to the representation of inlier samples through adversarial\ntraining. Secondly, we apply the information entropy into the latent layer to\nmake it sparse and constrain the expression of diversity. Experimental results\non three public datasets show that the proposed method achieves comparable\nperformance compared with previous popular approaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:41:34 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 05:36:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tian", "Miao", ""], ["Guo", "Dongyan", ""], ["Cui", "Ying", ""], ["Pan", "Xiang", ""], ["Chen", "Shengyong", ""]]}, {"id": "2007.01711", "submitter": "Yue Wang", "authors": "Yue Wang, Yuke Li, James H. Elder, Huchuan Lu, Runmin Wu, Lu Zhang", "title": "Synergistic saliency and depth prediction for RGB-D saliency detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information available from an RGB-D camera can be useful in segmenting\nsalient objects when figure/ground cues from RGB channels are weak. This has\nmotivated the development of several RGB-D saliency datasets and algorithms\nthat use all four channels of the RGB-D data for both training and inference.\nUnfortunately, existing RGB-D saliency datasets are small, which may lead to\noverfitting and limited generalization for diverse scenarios. Here we propose a\nsemi-supervised system for RGB-D saliency detection that can be trained on\nsmaller RGB-D saliency datasets without saliency ground truth, while also make\neffective joint use of a large RGB saliency dataset with saliency ground truth\ntogether. To generalize our method on RGB-D saliency datasets, a novel\nprediction-guided cross-refinement module which jointly estimates both saliency\nand depth by mutual refinement between two respective tasks, and an adversarial\nlearning approach are employed. Critically, our system does not require\nsaliency ground-truth for the RGB-D datasets, which saves the massive human\nlabor for hand labeling, and does not require the depth data for inference,\nallowing the method to be used for the much broader range of applications where\nonly RGB data are available. Evaluation on seven RGB-D datasets demonstrates\nthat even without saliency ground truth for RGB-D datasets and using only the\nRGB data of RGB-D datasets at inference, our semi-supervised system performs\nfavorable against state-of-the-art fully-supervised RGB-D saliency detection\nmethods that use saliency ground truth for RGB-D datasets at training and depth\ndata at inference on two largest testing datasets. Our approach also achieves\ncomparable results on other popular RGB-D saliency benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:24:41 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 06:23:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Yue", ""], ["Li", "Yuke", ""], ["Elder", "James H.", ""], ["Lu", "Huchuan", ""], ["Wu", "Runmin", ""], ["Zhang", "Lu", ""]]}, {"id": "2007.01724", "submitter": "Paritosh Mittal", "authors": "Paritosh Mittal, Shankar M Venkatesan, Viswanath Veera, Aloknath De", "title": "Deep Fence Estimation using Stereo Guidance and Adversarial Learning", "comments": "It was previously submitted to IEEE ICIP 2020. A previous version was\n  also submitted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People capture memorable images of events and exhibits that are often\noccluded by a wire mesh loosely termed as fence. Recent works in removing fence\nhave limited performance due to the difficulty in initial fence segmentation.\nThis work aims to accurately segment fence using a novel fence guidance mask\n(FM) generated from stereo image pair. This binary guidance mask contains\ndeterministic cues about the structure of fence and is given as additional\ninput to the deep fence estimation model. We also introduce a directional\nconnectivity loss (DCL), which is used alongside adversarial loss to precisely\ndetect thin wires. Experimental results obtained on real world scenarios\ndemonstrate the superiority of proposed method over state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:46:30 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Mittal", "Paritosh", ""], ["Venkatesan", "Shankar M", ""], ["Veera", "Viswanath", ""], ["De", "Aloknath", ""]]}, {"id": "2007.01738", "submitter": "Jingwei Xu", "authors": "Jingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, Trevor Darrell", "title": "Video Prediction via Example Guidance", "comments": "Project Page: https://sites.google.com/view/vpeg-supp/home", "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video prediction tasks, one major challenge is to capture the multi-modal\nnature of future contents and dynamics. In this work, we propose a simple yet\neffective framework that can efficiently predict plausible future states. The\nkey insight is that the potential distribution of a sequence could be\napproximated with analogous ones in a repertoire of training pool, namely,\nexpert examples. By further incorporating a novel optimization scheme into the\ntraining procedure, plausible predictions can be sampled efficiently from\ndistribution constructed from the retrieved examples. Meanwhile, our method\ncould be seamlessly integrated with existing stochastic predictive models;\nsignificant enhancement is observed with comprehensive experiments in both\nquantitative and qualitative aspects. We also demonstrate the generalization\nability to predict the motion of unseen class, i.e., without access to\ncorresponding data during training phase.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:57:24 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Xu", "Jingwei", ""], ["Xu", "Huazhe", ""], ["Ni", "Bingbing", ""], ["Yang", "Xiaokang", ""], ["Darrell", "Trevor", ""]]}, {"id": "2007.01755", "submitter": "Bin-Bin Gao", "authors": "Bin-Bin Gao, Hong-Yu Zhou", "title": "Learning to Discover Multi-Class Attentional Regions for Multi-Label\n  Image Recognition", "comments": "13 pages, Accepted by IEEE TIP (5-Jun-2021)", "journal-ref": null, "doi": "10.1109/TIP.2021.3088605", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image recognition is a practical and challenging task compared to\nsingle-label image classification. However, previous works may be suboptimal\nbecause of a great number of object proposals or complex attentional region\ngeneration modules. In this paper, we propose a simple but efficient two-stream\nframework to recognize multi-category objects from global image to local\nregions, similar to how human beings perceive objects. To bridge the gap\nbetween global and local streams, we propose a multi-class attentional region\nmodule which aims to make the number of attentional regions as small as\npossible and keep the diversity of these regions as high as possible. Our\nmethod can efficiently and effectively recognize multi-class objects with an\naffordable computation cost and a parameter-free region localization module.\nOver three benchmarks on multi-label image classification, we create new\nstate-of-the-art results with a single model only using image semantics without\nlabel dependency. In addition, the effectiveness of the proposed method is\nextensively demonstrated under different factors such as global pooling\nstrategy, input size and network architecture. Code has been made available\nat~\\url{https://github.com/gaobb/MCAR}.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:22:46 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 16:43:01 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 08:27:59 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Gao", "Bin-Bin", ""], ["Zhou", "Hong-Yu", ""]]}, {"id": "2007.01758", "submitter": "Guan Shanyan", "authors": "Shanyan Guan, Ying Tai, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang\n  Yang", "title": "Collaborative Learning for Faster StyleGAN Embedding", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent code of the recent popular model StyleGAN has learned disentangled\nrepresentations thanks to the multi-layer style-based generator. Embedding a\ngiven image back to the latent space of StyleGAN enables wide interesting\nsemantic image editing applications. Although previous works are able to yield\nimpressive inversion results based on an optimization framework, which however\nsuffers from the efficiency issue. In this work, we propose a novel\ncollaborative learning framework that consists of an efficient embedding\nnetwork and an optimization-based iterator. On one hand, with the progress of\ntraining, the embedding network gives a reasonable latent code initialization\nfor the iterator. On the other hand, the updated latent code from the iterator\nin turn supervises the embedding network. In the end, high-quality latent code\ncan be obtained efficiently with a single forward pass through our embedding\nnetwork. Extensive experiments demonstrate the effectiveness and efficiency of\nour work.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:27:37 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Guan", "Shanyan", ""], ["Tai", "Ying", ""], ["Ni", "Bingbing", ""], ["Zhu", "Feida", ""], ["Huang", "Feiyue", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2007.01760", "submitter": "Philipp Liznerski", "authors": "Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe\n  Franks, Marius Kloft, and Klaus-Robert M\\\"uller", "title": "Explainable Deep One-Class Classification", "comments": "25 pages, published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep one-class classification variants for anomaly detection learn a mapping\nthat concentrates nominal samples in feature space causing anomalies to be\nmapped away. Because this transformation is highly non-linear, finding\ninterpretations poses a significant challenge. In this paper we present an\nexplainable deep one-class classification method, Fully Convolutional Data\nDescription (FCDD), where the mapped samples are themselves also an explanation\nheatmap. FCDD yields competitive detection performance and provides reasonable\nexplanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet.\nOn MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps,\nFCDD sets a new state of the art in the unsupervised setting. Our method can\nincorporate ground-truth anomaly maps during training and using even a few of\nthese (~5) improves performance significantly. Finally, using FCDD's\nexplanations we demonstrate the vulnerability of deep one-class classification\nmodels to spurious image features such as image watermarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:29:06 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 16:11:43 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 10:35:33 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liznerski", "Philipp", ""], ["Ruff", "Lukas", ""], ["Vandermeulen", "Robert A.", ""], ["Franks", "Billy Joe", ""], ["Kloft", "Marius", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "2007.01769", "submitter": "Thomas Eboli", "authors": "Thomas Eboli, Jian Sun, Jean Ponce", "title": "End-to-end Interpretable Learning of Non-blind Image Deblurring", "comments": "Accepted at ECCV2020 (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-blind image deblurring is typically formulated as a linear least-squares\nproblem regularized by natural priors on the corresponding sharp picture's\ngradients, which can be solved, for example, using a half-quadratic splitting\nmethod with Richardson fixed-point iterations for its least-squares updates and\na proximal operator for the auxiliary variable updates. We propose to\nprecondition the Richardson solver using approximate inverse filters of the\n(known) blur and natural image prior kernels. Using convolutions instead of a\ngeneric linear preconditioner allows extremely efficient parameter sharing\nacross the image, and leads to significant gains in accuracy and/or speed\ncompared to classical FFT and conjugate-gradient methods. More importantly, the\nproposed architecture is easily adapted to learning both the preconditioner and\nthe proximal operator using CNN embeddings. This yields a simple and efficient\nalgorithm for non-blind image deblurring which is fully interpretable, can be\nlearned end to end, and whose accuracy matches or exceeds the state of the art,\nquite significantly, in the non-uniform case.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:45:01 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 14:44:59 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Eboli", "Thomas", ""], ["Sun", "Jian", ""], ["Ponce", "Jean", ""]]}, {"id": "2007.01771", "submitter": "Bin-Bin Gao", "authors": "Bin-Bin Gao, Xin-Xin Liu, Hong-Yu Zhou, Jianxin Wu, Xin Geng", "title": "Learning Expectation of Label Distribution for Facial Age and\n  Attractiveness Estimation", "comments": "submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes (e.g., age and attractiveness) estimation performance has\nbeen greatly improved by using convolutional neural networks. However, existing\nmethods have an inconsistency between the training objectives and the\nevaluation metric, so they may be suboptimal. In addition, these methods always\nadopt image classification or face recognition models with a large amount of\nparameters, which carry expensive computation cost and storage overhead. In\nthis paper, we firstly analyze the essential relationship between two\nstate-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking\nmethod is in fact learning label distribution implicitly. This result thus\nfirstly unifies two existing popular state-of-the-art methods into the DLDL\nframework. Second, in order to alleviate the inconsistency and reduce resource\nconsumption, we design a lightweight network architecture and propose a unified\nframework which can jointly learn facial attribute distribution and regress\nattribute value. The effectiveness of our approach has been demonstrated on\nboth facial age and attractiveness estimation tasks. Our method achieves new\nstate-of-the-art results using the single model with 36$\\times$(6$\\times$)\nfewer parameters and 2.6$\\times$(2.1$\\times$) faster inference speed on facial\nage (attractiveness) estimation. Moreover, our method can achieve comparable\nresults as the state-of-the-art even though the number of parameters is further\nreduced to 0.9M (3.8MB disk storage).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:46:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Gao", "Bin-Bin", ""], ["Liu", "Xin-Xin", ""], ["Zhou", "Hong-Yu", ""], ["Wu", "Jianxin", ""], ["Geng", "Xin", ""]]}, {"id": "2007.01780", "submitter": "Amelia Pollard", "authors": "Amelia Elizabeth Pollard and Jonathan L. Shapiro", "title": "Visual Question Answering as a Multi-Task Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering(VQA) is a highly complex problem set, relying on\nmany sub-problems to produce reasonable answers. In this paper, we present the\nhypothesis that Visual Question Answering should be viewed as a multi-task\nproblem, and provide evidence to support this hypothesis. We demonstrate this\nby reformatting two commonly used Visual Question Answering datasets, COCO-QA\nand DAQUAR, into a multi-task format and train these reformatted datasets on\ntwo baseline networks, with one designed specifically to eliminate other\npossible causes for performance changes as a result of the reformatting. Though\nthe networks demonstrated in this paper do not achieve strongly competitive\nresults, we find that the multi-task approach to Visual Question Answering\nresults in increases in performance of 5-9% against the single-task formatting,\nand that the networks reach convergence much faster than in the single-task\ncase. Finally we discuss possible reasons for the observed difference in\nperformance, and perform additional experiments which rule out causes not\nassociated with the learning of the dataset as a multi-task problem.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:07:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Pollard", "Amelia Elizabeth", ""], ["Shapiro", "Jonathan L.", ""]]}, {"id": "2007.01787", "submitter": "Matias Valdenegro-Toro", "authors": "Swaroop Bhandary K and Nico Hochgeschwender and Paul Pl\\\"oger and\n  Frank Kirchner and Matias Valdenegro-Toro", "title": "Evaluating Uncertainty Estimation Methods on 3D Semantic Segmentation of\n  Point Clouds", "comments": "12 pages, 19 figures, ICML 2020 Workshop on Uncertainty and\n  Robustness in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are extensively used in various safety critical\napplications. Hence these models along with being accurate need to be highly\nreliable. One way of achieving this is by quantifying uncertainty. Bayesian\nmethods for UQ have been extensively studied for Deep Learning models applied\non images but have been less explored for 3D modalities such as point clouds\noften used for Robots and Autonomous Systems. In this work, we evaluate three\nuncertainty quantification methods namely Deep Ensembles, MC-Dropout and\nMC-DropConnect on the DarkNet21Seg 3D semantic segmentation model and\ncomprehensively analyze the impact of various parameters such as number of\nmodels in ensembles or forward passes, and drop probability values, on task\nperformance and uncertainty estimate quality. We find that Deep Ensembles\noutperforms other methods in both performance and uncertainty metrics. Deep\nensembles outperform other methods by a margin of 2.4% in terms of mIOU, 1.3%\nin terms of accuracy, while providing reliable uncertainty for decision making.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:22:34 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["K", "Swaroop Bhandary", ""], ["Hochgeschwender", "Nico", ""], ["Pl\u00f6ger", "Paul", ""], ["Kirchner", "Frank", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "2007.01806", "submitter": "Estefania Talavera", "authors": "Alina Matei, Andreea Glavan, and Estefania Talavera", "title": "Deep learning for scene recognition from visual data: a survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning techniques has exploded during the last few years,\nresulting in a direct contribution to the field of artificial intelligence.\nThis work aims to be a review of the state-of-the-art in scene recognition with\ndeep learning models from visual data. Scene recognition is still an emerging\nfield in computer vision, which has been addressed from a single image and\ndynamic image perspective. We first give an overview of available datasets for\nimage and video scene recognition. Later, we describe ensemble techniques\nintroduced by research papers in the field. Finally, we give some remarks on\nour findings and discuss what we consider challenges in the field and future\nlines of research. This paper aims to be a future guide for model selection for\nthe task of scene recognition.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:53:18 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Matei", "Alina", ""], ["Glavan", "Andreea", ""], ["Talavera", "Estefania", ""]]}, {"id": "2007.01807", "submitter": "Hao Wang", "authors": "Hao Wang and Hao He and Dina Katabi", "title": "Continuously Indexed Domain Adaptation", "comments": "Accepted at ICML 2020. Talk:\n  https://www.youtube.com/watch?v=KtZPSCD-WhQ Code and Project Page:\n  https://github.com/hehaodele/CIDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain adaptation focuses on transferring knowledge between domains\nwith categorical indices (e.g., between datasets A and B). However, many tasks\ninvolve continuously indexed domains. For example, in medical applications, one\noften needs to transfer disease analysis and prediction across patients of\ndifferent ages, where age acts as a continuous domain index. Such tasks are\nchallenging for prior domain adaptation methods since they ignore the\nunderlying relation among domains. In this paper, we propose the first method\nfor continuously indexed domain adaptation. Our approach combines traditional\nadversarial adaptation with a novel discriminator that models the\nencoding-conditioned domain index distribution. Our theoretical analysis\ndemonstrates the value of leveraging the domain index to generate invariant\nfeatures across a continuous range of domains. Our empirical results show that\nour approach outperforms the state-of-the-art domain adaption methods on both\nsynthetic and real-world medical datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:53:50 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 02:31:43 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Hao", ""], ["He", "Hao", ""], ["Katabi", "Dina", ""]]}, {"id": "2007.01813", "submitter": "Tong Qin", "authors": "Tong Qin, Tongqing Chen, Yilun Chen, and Qing Su", "title": "AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous\n  Vehicles in the Parking Lot", "comments": "The IEEE/RSJ International Conference on Intelligent Robots and\n  Systems, IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous valet parking is a specific application for autonomous vehicles.\nIn this task, vehicles need to navigate in narrow, crowded and GPS-denied\nparking lots. Accurate localization ability is of great importance. Traditional\nvisual-based methods suffer from tracking lost due to texture-less regions,\nrepeated structures, and appearance changes. In this paper, we exploit robust\nsemantic features to build the map and localize vehicles in parking lots.\nSemantic features contain guide signs, parking lines, speed bumps, etc, which\ntypically appear in parking lots. Compared with traditional features, these\nsemantic features are long-term stable and robust to the perspective and\nillumination change. We adopt four surround-view cameras to increase the\nperception range. Assisting by an IMU (Inertial Measurement Unit) and wheel\nencoders, the proposed system generates a global visual semantic map. This map\nis further used to localize vehicles at the centimeter level. We analyze the\naccuracy and recall of our system and compare it against other methods in real\nexperiments. Furthermore, we demonstrate the practicability of the proposed\nsystem by the autonomous parking application.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:02:46 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 14:13:25 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Qin", "Tong", ""], ["Chen", "Tongqing", ""], ["Chen", "Yilun", ""], ["Su", "Qing", ""]]}, {"id": "2007.01818", "submitter": "Quang Truong", "authors": "Quang Truong, Hy Dang, Zhankai Ye, Minh Nguyen, Bo Mei", "title": "Image-based Vehicle Re-identification Model with Adaptive Attention\n  Modules and Metadata Re-ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification is a challenging task due to intra-class\nvariability and inter-class similarity across non-overlapping cameras. To\ntackle these problems, recently proposed methods require additional annotation\nto extract more features for false positive image exclusion. In this paper, we\npropose a model powered by adaptive attention modules that requires fewer label\nannotations but still out-performs the previous models. We also include a\nre-ranking method that takes account of the importance of metadata feature\nembeddings in our paper. The proposed method is evaluated on CVPR AI City\nChallenge 2020 dataset and achieves mAP of 37.25% in Track 2.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:14:18 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Truong", "Quang", ""], ["Dang", "Hy", ""], ["Ye", "Zhankai", ""], ["Nguyen", "Minh", ""], ["Mei", "Bo", ""]]}, {"id": "2007.01837", "submitter": "Issam Hadj Laradji", "authors": "Issam H. Laradji, Rafael Pardinas, Pau Rodriguez, David Vazquez", "title": "LOOC: Localize Overlapping Objects with Count Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring count annotations generally requires less human effort than\npoint-level and bounding box annotations. Thus, we propose the novel problem\nsetup of localizing objects in dense scenes under this weaker supervision. We\npropose LOOC, a method to Localize Overlapping Objects with Count supervision.\nWe train LOOC by alternating between two stages. In the first stage, LOOC\nlearns to generate pseudo point-level annotations in a semi-supervised manner.\nIn the second stage, LOOC uses a fully-supervised localization method that\ntrains on these pseudo labels. The localization method is used to progressively\nimprove the quality of the pseudo labels. We conducted experiments on popular\ncounting datasets. For localization, LOOC achieves a strong new baseline in the\nnovel problem setup where only count supervision is available. For counting,\nLOOC outperforms current state-of-the-art methods that only use count as their\nsupervision. Code is available at: https://github.com/ElementAI/looc.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:44:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Laradji", "Issam H.", ""], ["Pardinas", "Rafael", ""], ["Rodriguez", "Pau", ""], ["Vazquez", "David", ""]]}, {"id": "2007.01851", "submitter": "Lerrel Pinto", "authors": "Dhiraj Gandhi, Abhinav Gupta, Lerrel Pinto", "title": "Swoosh! Rattle! Thump! -- Actions that Sound", "comments": "To be presented at Robotics: Science and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truly intelligent agents need to capture the interplay of all their senses to\nbuild a rich physical understanding of their world. In robotics, we have seen\ntremendous progress in using visual and tactile perception; however, we have\noften ignored a key sense: sound. This is primarily due to the lack of data\nthat captures the interplay of action and sound. In this work, we perform the\nfirst large-scale study of the interactions between sound and robotic action.\nTo do this, we create the largest available sound-action-vision dataset with\n15,000 interactions on 60 objects using our robotic platform Tilt-Bot. By\ntilting objects and allowing them to crash into the walls of a robotic tray, we\ncollect rich four-channel audio information. Using this data, we explore the\nsynergies between sound and action and present three key insights. First, sound\nis indicative of fine-grained object class information, e.g., sound can\ndifferentiate a metal screwdriver from a metal wrench. Second, sound also\ncontains information about the causal effects of an action, i.e. given the\nsound produced, we can predict what action was applied to the object. Finally,\nobject representations derived from audio embeddings are indicative of implicit\nphysical properties. We demonstrate that on previously unseen objects, audio\nembeddings generated through interactions can predict forward models 24% better\nthan passive visual embeddings. Project videos and data are at\nhttps://dhiraj100892.github.io/swoosh/\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 17:57:54 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Gandhi", "Dhiraj", ""], ["Gupta", "Abhinav", ""], ["Pinto", "Lerrel", ""]]}, {"id": "2007.01855", "submitter": "Ehsan Kazemi Dr", "authors": "Ehsan Kazemi, Thomas Kerdreux and Liqiang Wang", "title": "Trace-Norm Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White box adversarial perturbations are sought via iterative optimization\nalgorithms most often minimizing an adversarial loss on a $l_p$ neighborhood of\nthe original image, the so-called distortion set. Constraining the adversarial\nsearch with different norms results in disparately structured adversarial\nexamples. Here we explore several distortion sets with structure-enhancing\nalgorithms. These new structures for adversarial examples, yet pervasive in\noptimization, are for instance a challenge for adversarial theoretical\ncertification which again provides only $l_p$ certificates. Because adversarial\nrobustness is still an empirical field, defense mechanisms should also\nreasonably be evaluated against differently structured attacks. Besides, these\nstructured adversarial perturbations may allow for larger distortions size than\ntheir $l_p$ counter-part while remaining imperceptible or perceptible as\nnatural slight distortions of the image. Finally, they allow some control on\nthe generation of the adversarial perturbation, like (localized) bluriness.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 13:37:19 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Kerdreux", "Thomas", ""], ["Wang", "Liqiang", ""]]}, {"id": "2007.01857", "submitter": "Dalcimar Casanova", "authors": "Muriel Mazzetto and Marcelo Teixeira and \\'Erick Oliveira Rodrigues\n  and Dalcimar Casanova", "title": "Deep Learning Models for Visual Inspection on Automotive Assembling Line", "comments": "arXiv admin note: text overlap with arXiv:1802.08717,\n  arXiv:1703.05921 by other authors", "journal-ref": "International Journal of Advanced Engineering Research and Science\n  7(4) (2020) 473-494", "doi": "10.22161/ijaers.74.56", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automotive manufacturing assembly tasks are built upon visual inspections\nsuch as scratch identification on machined surfaces, part identification and\nselection, etc, which guarantee product and process quality. These tasks can be\nrelated to more than one type of vehicle that is produced within the same\nmanufacturing line. Visual inspection was essentially human-led but has\nrecently been supplemented by the artificial perception provided by computer\nvision systems (CVSs). Despite their relevance, the accuracy of CVSs varies\naccordingly to environmental settings such as lighting, enclosure and quality\nof image acquisition. These issues entail costly solutions and override part of\nthe benefits introduced by computer vision systems, mainly when it interferes\nwith the operating cycle time of the factory. In this sense, this paper\nproposes the use of deep learning-based methodologies to assist in visual\ninspection tasks while leaving very little footprints in the manufacturing\nenvironment and exploring it as an end-to-end tool to ease CVSs setup. The\nproposed approach is illustrated by four proofs of concept in a real automotive\nassembly line based on models for object detection, semantic segmentation, and\nanomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:00:45 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Mazzetto", "Muriel", ""], ["Teixeira", "Marcelo", ""], ["Rodrigues", "\u00c9rick Oliveira", ""], ["Casanova", "Dalcimar", ""]]}, {"id": "2007.01864", "submitter": "Di Yuan", "authors": "Di Yuan, Nana Fan, Xiaojun Chang, Qiao Liu and Zhenyu He", "title": "Accurate Bounding-box Regression with Distance-IoU Loss for Visual\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing trackers are based on using a classifier and multi-scale\nestimation to estimate the target state. Consequently, and as expected,\ntrackers have become more stable while tracking accuracy has stagnated. While\ntrackers adopt a maximum overlap method based on an intersection-over-union\n(IoU) loss to mitigate this problem, there are defects in the IoU loss itself,\nthat make it impossible to continue to optimize the objective function when a\ngiven bounding box is completely contained within/without another bounding box;\nthis makes it very challenging to accurately estimate the target state.\nAccordingly, in this paper, we address the above-mentioned problem by proposing\na novel tracking method based on a distance-IoU (DIoU) loss, such that the\nproposed tracker consists of target estimation and target classification. The\ntarget estimation part is trained to predict the DIoU score between the target\nground-truth bounding-box and the estimated bounding-box. The DIoU loss can\nmaintain the advantage provided by the IoU loss while minimizing the distance\nbetween the center points of two bounding boxes, thereby making the target\nestimation more accurate. Moreover, we introduce a classification part that is\ntrained online and optimized with a Conjugate-Gradient-based strategy to\nguarantee real-time tracking speed. Comprehensive experimental results\ndemonstrate that the proposed method achieves competitive tracking accuracy\nwhen compared to state-of-the-art trackers while with a real-time tracking\nspeed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:57:54 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 07:51:48 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 04:04:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yuan", "Di", ""], ["Fan", "Nana", ""], ["Chang", "Xiaojun", ""], ["Liu", "Qiao", ""], ["He", "Zhenyu", ""]]}, {"id": "2007.01866", "submitter": "Rui Aguiar", "authors": "Rui Aguiar, Jon Braatz", "title": "Selecting Regions of Interest in Large Multi-Scale Images for Cancer\n  Pathology", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in object detection and image classification using\nConvolutional Neural Networks (CNNs) are revolutionizing the state of the art\nin medical imaging, and microscopy in particular presents abundant\nopportunities for computer vision algorithms to assist medical professionals in\ndiagnosis of diseases ranging from malaria to cancer. High resolution scans of\nmicroscopy slides called Whole Slide Images (WSIs) offer enough information for\na cancer pathologist to come to a conclusion regarding cancer presence,\nsubtype, and severity based on measurements of features within the slide image\nat multiple scales and resolutions. WSIs' extremely high resolutions and\nfeature scales ranging from gross anatomical structures down to cell nuclei\npreclude the use of standard CNN models for object detection and\nclassification, which have typically been designed for images with dimensions\nin the hundreds of pixels and with objects on the order of the size of the\nimage itself. We explore parallel approaches based on Reinforcement Learning\nand Beam Search to learn to progressively zoom into the WSI to detect Regions\nof Interest (ROIs) in liver pathology slides containing one of two types of\nliver cancer, namely Hepatocellular Carcinoma (HCC) and Cholangiocarcinoma\n(CC). These ROIs can then be presented directly to the pathologist to aid in\nmeasurement and diagnosis or be used for automated classification of tumor\nsubtype.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:27:41 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Aguiar", "Rui", ""], ["Braatz", "Jon", ""]]}, {"id": "2007.01867", "submitter": "Wenxin Liu", "authors": "Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I. Mourikis,\n  Kostas Daniilidis, Vijay Kumar, Jakob Engel", "title": "TLIO: Tight Learned Inertial Odometry", "comments": "Correcting graph and bibliography. Adding journal reference\n  information and DOI, in IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2020.3007421", "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a tightly-coupled Extended Kalman Filter framework\nfor IMU-only state estimation. Strap-down IMU measurements provide relative\nstate estimates based on IMU kinematic motion model. However the integration of\nmeasurements is sensitive to sensor bias and noise, causing significant drift\nwithin seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet)\nshowed the capability of using trained neural networks to obtain accurate 2D\ndisplacement estimates from segments of IMU data and obtained good position\nestimates from concatenating them. This paper demonstrates a network that\nregresses 3D displacement estimates and its uncertainty, giving us the ability\nto tightly fuse the relative state measurement into a stochastic cloning EKF to\nsolve for pose, velocity and sensor biases. We show that our network, trained\nwith pedestrian data from a headset, can produce statistically consistent\nmeasurement and uncertainty to be used as the update step in the filter, and\nthe tightly-coupled system outperforms velocity integration approaches in\nposition estimates, and AHRS attitude filter in orientation estimates.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:13:34 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:46:41 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 23:15:52 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Wenxin", ""], ["Caruso", "David", ""], ["Ilg", "Eddy", ""], ["Dong", "Jing", ""], ["Mourikis", "Anastasios I.", ""], ["Daniilidis", "Kostas", ""], ["Kumar", "Vijay", ""], ["Engel", "Jakob", ""]]}, {"id": "2007.01883", "submitter": "Juan-Manuel Perez-Rua", "authors": "Juan-Manuel Perez-Rua, Antoine Toisoul, Brais Martinez, Victor\n  Escorcia, Li Zhang, Xiatian Zhu, Tao Xiang", "title": "Egocentric Action Recognition by Video Attention and Temporal Context", "comments": "EPIC-Kitchens challenges@CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present the submission of Samsung AI Centre Cambridge to the CVPR2020\nEPIC-Kitchens Action Recognition Challenge. In this challenge, action\nrecognition is posed as the problem of simultaneously predicting a single\n`verb' and `noun' class label given an input trimmed video clip. That is, a\n`verb' and a `noun' together define a compositional `action' class. The\nchallenging aspects of this real-life action recognition task include small\nfast moving objects, complex hand-object interactions, and occlusions. At the\ncore of our submission is a recently-proposed spatial-temporal video attention\nmodel, called `W3' (`What-Where-When') attention~\\cite{perez2020knowing}. We\nfurther introduce a simple yet effective contextual learning mechanism to model\n`action' class scores directly from long-term temporal behaviour based on the\n`verb' and `noun' prediction scores. Our solution achieves strong performance\non the challenge metrics without using object-specific reasoning nor extra\ntraining data. In particular, our best solution with multimodal ensemble\nachieves the 2$^{nd}$ best position for `verb', and 3$^{rd}$ best for `noun'\nand `action' on the Seen Kitchens test set.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:00:32 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Perez-Rua", "Juan-Manuel", ""], ["Toisoul", "Antoine", ""], ["Martinez", "Brais", ""], ["Escorcia", "Victor", ""], ["Zhang", "Li", ""], ["Zhu", "Xiatian", ""], ["Xiang", "Tao", ""]]}, {"id": "2007.01899", "submitter": "Alejandro Posada", "authors": "Negin Sokhandan, Pegah Kamousi, Alejandro Posada, Eniola Alese, Negar\n  Rostamzadeh", "title": "A Few-Shot Sequential Approach for Object Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of few-shot multi-class object counting\nwith point-level annotations. The proposed technique leverages a class agnostic\nattention mechanism that sequentially attends to objects in the image and\nextracts their relevant features. This process is employed on an adapted\nprototypical-based few-shot approach that uses the extracted features to\nclassify each one either as one of the classes present in the support set\nimages or as background. The proposed technique is trained on point-level\nannotations and uses a novel loss function that disentangles class-dependent\nand class-agnostic aspects of the model to help with the task of few-shot\nobject counting. We present our results on a variety of\nobject-counting/detection datasets, including FSOD and MS COCO. In addition, we\nintroduce a new dataset that is specifically designed for weakly supervised\nmulti-class object counting/detection and contains considerably different\nclasses and distribution of number of classes/instances per image compared to\nthe existing datasets. We demonstrate the robustness of our approach by testing\nour system on a totally different distribution of classes from what it has been\ntrained on.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:23:39 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 20:11:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sokhandan", "Negin", ""], ["Kamousi", "Pegah", ""], ["Posada", "Alejandro", ""], ["Alese", "Eniola", ""], ["Rostamzadeh", "Negar", ""]]}, {"id": "2007.01915", "submitter": "Sirin Haddad", "authors": "Sirin Haddad and Siew Kei Lam", "title": "Graph2Kernel Grid-LSTM: A Multi-Cued Model for Pedestrian Trajectory\n  Prediction by Learning Adaptive Neighborhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian trajectory prediction is a prominent research track that has\nadvanced towards modelling of crowd social and contextual interactions, with\nextensive usage of Long Short-Term Memory (LSTM) for temporal representation of\nwalking trajectories.\n  Existing approaches use virtual neighborhoods as a fixed grid for pooling\nsocial states of pedestrians with tuning process that controls how social\ninteractions are being captured. This entails performance customization to\nspecific scenes but lowers the generalization capability of the approaches. In\nour work, we deploy \\textit{Grid-LSTM}, a recent extension of LSTM, which\noperates over multidimensional feature inputs. We present a new perspective to\ninteraction modeling by proposing that pedestrian neighborhoods can become\nadaptive in design. We use \\textit{Grid-LSTM} as an encoder to learn about\npotential future neighborhoods and their influence on pedestrian motion given\nthe visual and the spatial boundaries. Our model outperforms state-of-the-art\napproaches that collate resembling features over several publicly-tested\nsurveillance videos. The experiment results clearly illustrate the\ngeneralization of our approach across datasets that varies in scene features\nand crowd dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 19:05:48 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 08:48:41 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Haddad", "Sirin", ""], ["Lam", "Siew Kei", ""]]}, {"id": "2007.01922", "submitter": "Bahram Zonooz", "authors": "Fahad Sarfraz, Elahe Arani and Bahram Zonooz", "title": "Knowledge Distillation Beyond Model Compression", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is commonly deemed as an effective model\ncompression technique in which a compact model (student) is trained under the\nsupervision of a larger pretrained model or an ensemble of models (teacher).\nVarious techniques have been proposed since the original formulation, which\nmimic different aspects of the teacher such as the representation space,\ndecision boundary, or intra-data relationship. Some methods replace the one-way\nknowledge distillation from a static teacher with collaborative learning\nbetween a cohort of students. Despite the recent advances, a clear\nunderstanding of where knowledge resides in a deep neural network and an\noptimal method for capturing knowledge from teacher and transferring it to\nstudent remains an open question. In this study, we provide an extensive study\non nine different KD methods which covers a broad spectrum of approaches to\ncapture and transfer knowledge. We demonstrate the versatility of the KD\nframework on different datasets and network architectures under varying\ncapacity gaps between the teacher and student. The study provides intuition for\nthe effects of mimicking different aspects of the teacher and derives insights\nfrom the performance of the different distillation approaches to guide the\ndesign of more effective KD methods. Furthermore, our study shows the\neffectiveness of the KD framework in learning efficiently under varying\nseverity levels of label noise and class imbalance, consistently providing\ngeneralization gains over standard training. We emphasize that the efficacy of\nKD goes much beyond a model compression technique and it should be considered\nas a general-purpose training paradigm which offers more robustness to common\nchallenges in the real-world datasets compared to the standard training\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 19:54:04 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sarfraz", "Fahad", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2007.01940", "submitter": "Ashish Kubade", "authors": "Ashish Kubade, Avinash Sharma, K S Rajan", "title": "Feedback Neural Network based Super-resolution of DEM for generating\n  high fidelity features", "comments": "Accepted for publication in IEEE IGARSS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution Digital Elevation Models(DEMs) are an important requirement\nfor many applications like modelling water flow, landslides, avalanches etc.\nYet publicly available DEMs have low resolution for most parts of the world.\nDespite tremendous success in image super resolution task using deep learning\nsolutions, there are very few works that have used these powerful systems on\nDEMs to generate HRDEMs. Motivated from feedback neural networks, we propose a\nnovel neural network architecture that learns to add high frequency details\niteratively to low resolution DEM, turning it into a high resolution DEM\nwithout compromising its fidelity. Our experiments confirm that without any\nadditional modality such as aerial images(RGB), our network DSRFB achieves\nRMSEs of 0.59 to 1.27 across 4 different datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 21:10:19 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kubade", "Ashish", ""], ["Sharma", "Avinash", ""], ["Rajan", "K S", ""]]}, {"id": "2007.01941", "submitter": "Tristan Konolige", "authors": "Tristan Konolige, Jed Brown", "title": "Multigrid for Bundle Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundle adjustment is an important global optimization step in many structure\nfrom motion pipelines. Performance is dependent on the speed of the linear\nsolver used to compute steps towards the optimum. For large problems, the\ncurrent state of the art scales superlinearly with the number of cameras in the\nproblem. We investigate the conditioning of global bundle adjustment problems\nas the number of images increases in different regimes and fundamental\nconsequences in terms of superlinear scaling of the current state of the art\nmethods. We present an unsmoothed aggregation multigrid preconditioner that\naccurately represents the global modes that underlie poor scaling of existing\nmethods and demonstrate solves of up to 13 times faster than the state of the\nart on large, challenging problem sets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 21:14:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Konolige", "Tristan", ""], ["Brown", "Jed", ""]]}, {"id": "2007.01947", "submitter": "Wenguan Wang", "authors": "Guolei Sun and Wenguan Wang and Jifeng Dai and Luc Van Gool", "title": "Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation", "comments": "Full version of ECCV2020 Oral, CVPR2020 LID workshop Best Paper and\n  LID challenge Track1 winner; website: https://github.com/GuoleiSun/MCIS_wsss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning semantic segmentation from\nimage-level supervision only. Current popular solutions leverage object\nlocalization maps from classifiers as supervision signals, and struggle to make\nthe localization maps capture more complete object content. Rather than\nprevious efforts that primarily focus on intra-image information, we address\nthe value of cross-image semantic relations for comprehensive object pattern\nmining. To achieve this, two neural co-attentions are incorporated into the\nclassifier to complimentarily capture cross-image semantic similarities and\ndifferences. In particular, given a pair of training images, one co-attention\nenforces the classifier to recognize the common semantics from co-attentive\nobjects, while the other one, called contrastive co-attention, drives the\nclassifier to identify the unshared semantics from the rest, uncommon objects.\nThis helps the classifier discover more object patterns and better ground\nsemantics in image regions. In addition to boosting object pattern learning,\nthe co-attention can leverage context from other related images to improve\nlocalization map inference, hence eventually benefiting semantic segmentation\nlearning. More essentially, our algorithm provides a unified framework that\nhandles well different WSSS settings, i.e., learning WSSS with (1) precise\nimage-level supervision only, (2) extra simple single-label data, and (3) extra\nnoisy web data. It sets new state-of-the-arts on all these settings,\ndemonstrating well its efficacy and generalizability. Moreover, our approach\nranked 1st place in the Weakly-Supervised Semantic Segmentation Track of\nCVPR2020 Learning from Imperfect Data Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 21:53:46 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 11:51:59 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sun", "Guolei", ""], ["Wang", "Wenguan", ""], ["Dai", "Jifeng", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.01951", "submitter": "Liwei Wang", "authors": "Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, Dong Yu", "title": "Improving Weakly Supervised Visual Grounding by Contrastive Knowledge\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised phrase grounding aims at learning region-phrase\ncorrespondences using only image-sentence pairs. A major challenge thus lies in\nthe missing links between image regions and sentence phrases during training.\nTo address this challenge, we leverage a generic object detector at training\ntime, and propose a contrastive learning framework that accounts for both\nregion-phrase and image-sentence matching. Our core innovation is the learning\nof a region-phrase score function, based on which an image-sentence score\nfunction is further constructed. Importantly, our region-phrase score function\nis learned by distilling from soft matching scores between the detected object\nnames and candidate phrases within an image-sentence pair, while the\nimage-sentence score function is supervised by ground-truth image-sentence\npairs. The design of such score functions removes the need of object detection\nat test time, thereby significantly reducing the inference cost. Without bells\nand whistles, our approach achieves state-of-the-art results on visual phrase\ngrounding, surpassing previous methods that require expensive object detectors\nat test time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 22:02:00 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 05:11:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Liwei", ""], ["Huang", "Jing", ""], ["Li", "Yin", ""], ["Xu", "Kun", ""], ["Yang", "Zhengyuan", ""], ["Yu", "Dong", ""]]}, {"id": "2007.01971", "submitter": "Ping Yu", "authors": "Ping Yu, Yang Zhao, Chunyuan Li, Junsong Yuan, Changyou Chen", "title": "Structure-Aware Human-Action Generation", "comments": "accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generating long-range skeleton-based human actions has been a challenging\nproblem since small deviations of one frame can cause a malformed action\nsequence. Most existing methods borrow ideas from video generation, which\nnaively treat skeleton nodes/joints as pixels of images without considering the\nrich inter-frame and intra-frame structure information, leading to potential\ndistorted actions. Graph convolutional networks (GCNs) is a promising way to\nleverage structure information to learn structure representations. However,\ndirectly adopting GCNs to tackle such continuous action sequences both in\nspatial and temporal spaces is challenging as the action graph could be huge.\nTo overcome this issue, we propose a variant of GCNs to leverage the powerful\nself-attention mechanism to adaptively sparsify a complete action graph in the\ntemporal space. Our method could dynamically attend to important past frames\nand construct a sparse graph to apply in the GCN framework, well-capturing the\nstructure information in action sequences. Extensive experimental results\ndemonstrate the superiority of our method on two standard human action datasets\ncompared with existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:18:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 21:31:44 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 20:05:43 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yu", "Ping", ""], ["Zhao", "Yang", ""], ["Li", "Chunyuan", ""], ["Yuan", "Junsong", ""], ["Chen", "Changyou", ""]]}, {"id": "2007.01975", "submitter": "Ricardo Bigolin Lanfredi", "authors": "Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Clement Vachet, Tolga\n  Tasdizen", "title": "Interpretation of Disease Evidence for Medical Images Using Adversarial\n  Deformation Fields", "comments": "Accepted for MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high complexity of deep learning models is associated with the difficulty\nof explaining what evidence they recognize as correlating with specific disease\nlabels. This information is critical for building trust in models and finding\ntheir biases. Until now, automated deep learning visualization solutions have\nidentified regions of images used by classifiers, but these solutions are too\ncoarse, too noisy, or have a limited representation of the way images can\nchange. We propose a novel method for formulating and presenting spatial\nexplanations of disease evidence, called deformation field interpretation with\ngenerative adversarial networks (DeFI-GAN). An adversarially trained generator\nproduces deformation fields that modify images of diseased patients to resemble\nimages of healthy patients. We validate the method studying chronic obstructive\npulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer's\ndisease (AD) evidence in brain MRIs. When extracting disease evidence in\nlongitudinal data, we show compelling results against a baseline producing\ndifference maps. DeFI-GAN also highlights disease biomarkers not found by\nprevious methods and potential biases that may help in investigations of the\ndataset and of the adopted learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:51:54 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lanfredi", "Ricardo Bigolin", ""], ["Schroeder", "Joyce D.", ""], ["Vachet", "Clement", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "2007.01992", "submitter": "Qi Liu", "authors": "Qi Liu, Shihua Yuan, Zirui Li", "title": "A Survey on Sensor Technologies for Unmanned Ground Vehicles", "comments": "9 pages,6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned ground vehicles have a huge development potential in both civilian\nand military fields, and have become the focus of research in various\ncountries. In addition, high-precision, high-reliability sensors are\nsignificant for UGVs' efficient operation. This paper proposes a brief review\non sensor technologies for UGVs. Firstly, characteristics of various sensors\nare introduced. Then the strengths and weaknesses of different sensors as well\nas their application scenarios are compared. Furthermore, sensor applications\nin some existing UGVs are summarized. Finally, the hotspots of sensor\ntechnologies are forecasted to point the development direction.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 03:15:13 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 04:51:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Qi", ""], ["Yuan", "Shihua", ""], ["Li", "Zirui", ""]]}, {"id": "2007.02010", "submitter": "Yanwei Fu", "authors": "Yanwei Fu, Chen Liu, Donghao Li, Xinwei Sun, Jinshan Zeng, Yuan Yao", "title": "DessiLBI: Exploring Structural Sparsity of Deep Networks via\n  Differential Inclusion Paths", "comments": "conference , 23 pages https://github.com/corwinliu9669/dS2LBI", "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterization is ubiquitous nowadays in training neural networks to\nbenefit both optimization in seeking global optima and generalization in\nreducing prediction error. However, compressive networks are desired in many\nreal world applications and direct training of small networks may be trapped in\nlocal optima. In this paper, instead of pruning or distilling\nover-parameterized models to compressive ones, we propose a new approach based\non differential inclusions of inverse scale spaces. Specifically, it generates\na family of models from simple to complex ones that couples a pair of\nparameters to simultaneously train over-parameterized deep models and\nstructural sparsity on weights of fully connected and convolutional layers.\nSuch a differential inclusion scheme has a simple discretization, proposed as\nDeep structurally splitting Linearized Bregman Iteration (DessiLBI), whose\nglobal convergence analysis in deep learning is established that from any\ninitializations, algorithmic iterations converge to a critical point of\nempirical risks. Experimental evidence shows that DessiLBI achieve comparable\nand even better performance than the competitive optimizers in exploring the\nstructural sparsity of several widely used backbones on the benchmark datasets.\nRemarkably, with early stopping, DessiLBI unveils \"winning tickets\" in early\nepochs: the effective sparse structure with comparable test accuracy to fully\ntrained over-parameterized models.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 04:40:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fu", "Yanwei", ""], ["Liu", "Chen", ""], ["Li", "Donghao", ""], ["Sun", "Xinwei", ""], ["Zeng", "Jinshan", ""], ["Yao", "Yuan", ""]]}, {"id": "2007.02017", "submitter": "Qing Jin", "authors": "Linjie Yang, Qing Jin", "title": "FracBits: Mixed Precision Quantization via Fractional Bit-Widths", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization helps to reduce model size and latency of deep neural\nnetworks. Mixed precision quantization is favorable with customized hardwares\nsupporting arithmetic operations at multiple bit-widths to achieve maximum\nefficiency. We propose a novel learning-based algorithm to derive mixed\nprecision models end-to-end under target computation constraints and model\nsizes. During the optimization, the bit-width of each layer / kernel in the\nmodel is at a fractional status of two consecutive bit-widths which can be\nadjusted gradually. With a differentiable regularization term, the resource\nconstraints can be met during the quantization-aware training which results in\nan optimized mixed precision model. Further, our method can be naturally\ncombined with channel pruning for better computation cost allocation. Our final\nmodels achieve comparable or better performance than previous quantization\nmethods with mixed precision on MobilenetV1/V2, ResNet18 under different\nresource constraints on ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 06:09:09 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 03:22:55 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Yang", "Linjie", ""], ["Jin", "Qing", ""]]}, {"id": "2007.02018", "submitter": "Jinxiu Liang", "authors": "Jinxiu Liang, Yong Xu, Yuhui Quan, Jingwen Wang, Haibin Ling and Hui\n  Ji", "title": "Deep Bilateral Retinex for Low-Light Image Enhancement", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light images, i.e. the images captured in low-light conditions, suffer\nfrom very poor visibility caused by low contrast, color distortion and\nsignificant measurement noise. Low-light image enhancement is about improving\nthe visibility of low-light images. As the measurement noise in low-light\nimages is usually significant yet complex with spatially-varying\ncharacteristic, how to handle the noise effectively is an important yet\nchallenging problem in low-light image enhancement. Based on the Retinex\ndecomposition of natural images, this paper proposes a deep learning method for\nlow-light image enhancement with a particular focus on handling the measurement\nnoise. The basic idea is to train a neural network to generate a set of\npixel-wise operators for simultaneously predicting the noise and the\nillumination layer, where the operators are defined in the bilateral space.\nSuch an integrated approach allows us to have an accurate prediction of the\nreflectance layer in the presence of significant spatially-varying measurement\nnoise. Extensive experiments on several benchmark datasets have shown that the\nproposed method is very competitive to the state-of-the-art methods, and has\nsignificant advantage over others when processing images captured in extremely\nlow lighting conditions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 06:26:44 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Liang", "Jinxiu", ""], ["Xu", "Yong", ""], ["Quan", "Yuhui", ""], ["Wang", "Jingwen", ""], ["Ling", "Haibin", ""], ["Ji", "Hui", ""]]}, {"id": "2007.02024", "submitter": "Bin Yan", "authors": "Bin Yan, Dong Wang, Huchuan Lu, Xiaoyun Yang", "title": "Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box\n  Estimation", "comments": "This version is out-of-update. Please refer to the latest one\n  arXiv:2012.06815", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the multiple-stage strategy has become a popular trend for\nvisual tracking. This strategy first utilizes a base tracker to coarsely locate\nthe target and then exploits a refinement module to obtain more accurate\nresults. However, existing refinement modules suffer from the limited\ntransferability and precision. In this work, we propose a novel, flexible and\naccurate refinement module called Alpha-Refine, which exploits a precise\npixel-wise correlation layer together with a spatial-aware non-local layer to\nfuse features and can predict three complementary outputs: bounding box,\ncorners and mask. To wisely choose the most adequate output, we also design a\nlight-weight branch selector module. We apply the proposed Alpha-Refine module\nto five famous and state-of-the-art base trackers: DiMP, ATOM, SiamRPN++,\nRTMDNet and ECO. The comprehensive experiments on TrackingNet, LaSOT and\nVOT2018 benchmarks demonstrate that our approach significantly improves the\ntracking performance in comparison with other existing refinement methods. The\nsource codes will be available at\nhttps://github.com/MasterBin-IIAU/AlphaRefine.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 07:02:25 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 10:50:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yan", "Bin", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2007.02026", "submitter": "Farzan Shenavarmasouleh", "authors": "Farzan Shenavarmasouleh and Hamid R. Arabnia", "title": "DRDr: Automatic Masking of Exudates and Microaneurysms Caused By\n  Diabetic Retinopathy Using Mask R-CNN and Transfer Learning", "comments": "The 6th International Conference on Health Informatics & Medical\n  Systems (HIMS'20: July 27-30, 2020, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of identifying two main types of lesions -\nExudates and Microaneurysms - caused by Diabetic Retinopathy (DR) in the eyes\nof diabetic patients. We make use of Convolutional Neural Networks (CNNs) and\nTransfer Learning to locate and generate high-quality segmentation mask for\neach instance of the lesion that can be found in the patients' fundus images.\nWe create our normalized database out of e-ophtha EX and e-ophtha MA and tweak\nMask R-CNN to detect small lesions. Moreover, we employ data augmentation and\nthe pre-trained weights of ResNet101 to compensate for our small dataset. Our\nmodel achieves promising test mAP of 0.45, altogether showing that it can aid\nclinicians and ophthalmologist in the process of detecting and treating the\ninfamous DR.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 07:20:03 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Shenavarmasouleh", "Farzan", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2007.02035", "submitter": "Quande Liu", "authors": "Quande Liu, Qi Dou, Pheng-Ann Heng", "title": "Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to\n  Unseen Domains", "comments": "Early accepted by MICCAI 2020; code and dataset are available at\n  https://github.com/liuquande/SAML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model generalization capacity at domain shift (e.g., various imaging\nprotocols and scanners) is crucial for deep learning methods in real-world\nclinical deployment. This paper tackles the challenging problem of domain\ngeneralization, i.e., learning a model from multi-domain source data such that\nit can directly generalize to an unseen target domain. We present a novel\nshape-aware meta-learning scheme to improve the model generalization in\nprostate MRI segmentation. Our learning scheme roots in the gradient-based\nmeta-learning, by explicitly simulating domain shift with virtual meta-train\nand meta-test during training. Importantly, considering the deficiencies\nencountered when applying a segmentation model to unseen domains (i.e.,\nincomplete shape and ambiguous boundary of the prediction masks), we further\nintroduce two complementary loss objectives to enhance the meta-optimization,\nby particularly encouraging the shape compactness and shape smoothness of the\nsegmentations under simulated domain shift. We evaluate our method on prostate\nMRI data from six different institutions with distribution shifts acquired from\npublic datasets. Experimental results show that our approach outperforms many\nstate-of-the-art generalization methods consistently across all six settings of\nunseen domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 07:56:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Liu", "Quande", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2007.02036", "submitter": "Junyeong Kim", "authors": "Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, Chang D. Yoo", "title": "Modality Shifting Attention Network for Multi-modal Video Question\n  Answering", "comments": "CVPR2020 accepted; poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a network referred to as Modality Shifting Attention\nNetwork (MSAN) for Multimodal Video Question Answering (MVQA) task. MSAN\ndecomposes the task into two sub-tasks: (1) localization of temporal moment\nrelevant to the question, and (2) accurate prediction of the answer based on\nthe localized moment. The modality required for temporal localization may be\ndifferent from that for answer prediction, and this ability to shift modality\nis essential for performing the task. To this end, MSAN is based on (1) the\nmoment proposal network (MPN) that attempts to locate the most appropriate\ntemporal moment from each of the modalities, and also on (2) the heterogeneous\nreasoning network (HRN) that predicts the answer using an attention mechanism\non both modalities. MSAN is able to place importance weight on the two\nmodalities for each sub-task using a component referred to as Modality\nImportance Modulation (MIM). Experimental results show that MSAN outperforms\nprevious state-of-the-art by achieving 71.13\\% test accuracy on TVQA benchmark\ndataset. Extensive ablation studies and qualitative analysis are conducted to\nvalidate various components of the network.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:01:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kim", "Junyeong", ""], ["Ma", "Minuk", ""], ["Pham", "Trung", ""], ["Kim", "Kyungsu", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2007.02041", "submitter": "Pengyu Zhang", "authors": "Pengyu Zhang and Jie Zhao and Dong Wang and Huchuan Lu and Xiaoyun\n  Yang", "title": "Jointly Modeling Motion and Appearance Cues for Robust RGB-T Tracking", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we propose a novel RGB-T tracking framework by jointly\nmodeling both appearance and motion cues. First, to obtain a robust appearance\nmodel, we develop a novel late fusion method to infer the fusion weight maps of\nboth RGB and thermal (T) modalities. The fusion weights are determined by using\noffline-trained global and local multimodal fusion networks, and then adopted\nto linearly combine the response maps of RGB and T modalities. Second, when the\nappearance cue is unreliable, we comprehensively take motion cues, i.e., target\nand camera motions, into account to make the tracker robust. We further propose\na tracker switcher to switch the appearance and motion trackers flexibly.\nNumerous results on three recent RGB-T tracking datasets show that the proposed\ntracker performs significantly better than other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:11:33 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Pengyu", ""], ["Zhao", "Jie", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2007.02042", "submitter": "Chaobing Zheng", "authors": "Chaobing Zheng, Zhengguo Li, Yi Yang and Shiqian Wu", "title": "Single Image Brightening via Multi-Scale Exposure Fusion with Hybrid\n  Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A small ISO and a small exposure time are usually used to capture an image in\nthe back or low light conditions which results in an image with negligible\nmotion blur and small noise but look dark. In this paper, a single image\nbrightening algorithm is introduced to brighten such an image. The proposed\nalgorithm includes a unique hybrid learning framework to generate two virtual\nimages with large exposure times. The virtual images are first generated via\nintensity mapping functions (IMFs) which are computed using camera response\nfunctions (CRFs) and this is a model-driven approach. Both the virtual images\nare then enhanced by using a data-driven approach, i.e. a residual\nconvolutional neural network to approach the ground truth images. The\nmodel-driven approach and the data-driven one compensate each other in the\nproposed hybrid learning framework. The final brightened image is obtained by\nfusing the original image and two virtual images via a multi-scale exposure\nfusion algorithm with properly defined weights. Experimental results show that\nthe proposed brightening algorithm outperforms existing algorithms in terms of\nthe MEF-SSIM metric.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:23:07 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 09:36:36 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zheng", "Chaobing", ""], ["Li", "Zhengguo", ""], ["Yang", "Yi", ""], ["Wu", "Shiqian", ""]]}, {"id": "2007.02045", "submitter": "Rui Gong", "authors": "Rui Gong, Danda Pani Paudel, Ajad Chhatkuli, and Luc Van Gool", "title": "Self-Calibration Supported Robust Projective Structure-from-Motion", "comments": "21 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical Structure-from-Motion (SfM) pipelines rely on finding correspondences\nacross images, recovering the projective structure of the observed scene and\nupgrading it to a metric frame using camera self-calibration constraints.\nSolving each problem is mainly carried out independently from the others. For\ninstance, camera self-calibration generally assumes correct matches and a good\nprojective reconstruction have been obtained. In this paper, we propose a\nunified SfM method, in which the matching process is supported by\nself-calibration constraints. We use the idea that good matches should yield a\nvalid calibration. In this process, we make use of the Dual Image of Absolute\nQuadric projection equations within a multiview correspondence framework, in\norder to obtain robust matching from a set of putative correspondences. The\nmatching process classifies points as inliers or outliers, which is learned in\nan unsupervised manner using a deep neural network. Together with theoretical\nreasoning why the self-calibration constraints are necessary, we show\nexperimental results demonstrating robust multiview matching and accurate\ncamera calibration by exploiting these constraints.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:47:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gong", "Rui", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.02054", "submitter": "Jianfeng Zhang", "authors": "Jianfeng Zhang, Xuecheng Nie, Jiashi Feng", "title": "Inference Stage Optimization for Cross-scenario 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing 3D human pose estimation models suffer performance drop when\napplying to new scenarios with unseen poses due to their limited\ngeneralizability. In this work, we propose a novel framework, Inference Stage\nOptimization (ISO), for improving the generalizability of 3D pose models when\nsource and target data come from different pose distributions. Our main insight\nis that the target data, even though not labeled, carry valuable priors about\ntheir underlying distribution. To exploit such information, the proposed ISO\nperforms geometry-aware self-supervised learning (SSL) on each single target\ninstance and updates the 3D pose model before making prediction. In this way,\nthe model can mine distributional knowledge about the target scenario and\nquickly adapt to it with enhanced generalization performance. In addition, to\nhandle sequential target data, we propose an online mode for implementing our\nISO framework via streaming the SSL, which substantially enhances its\neffectiveness. We systematically analyze why and how our ISO framework works on\ndiverse benchmarks under cross-scenario setup. Remarkably, it yields new\nstate-of-the-art of 83.6% 3D PCK on MPI-INF-3DHP, improving upon the previous\nbest result by 9.7%. Code will be released.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 09:45:18 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Jianfeng", ""], ["Nie", "Xuecheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2007.02065", "submitter": "Xuesong Li", "authors": "Xuesong Li, Jose Guivant", "title": "Efficient and accurate object detection with simultaneous classification\n  and tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interacting with the environment, such as object detection and tracking, is a\ncrucial ability of mobile robots. Besides high accuracy, efficiency in terms of\nprocessing effort and energy consumption are also desirable. To satisfy both\nrequirements, we propose a detection framework based on simultaneous\nclassification and tracking in the point stream. In this framework, a tracker\nperforms data association in sequences of the point cloud, guiding the detector\nto avoid redundant processing (i.e. classifying already-known objects). For\nobjects whose classification is not sufficiently certain, a fusion model is\ndesigned to fuse selected key observations that provide different perspectives\nacross the tracking span. Therefore, performance (accuracy and efficiency of\ndetection) can be enhanced. This method is particularly suitable for detecting\nand tracking moving objects, a process that would require expensive\ncomputations if solved using conventional procedures. Experiments were\nconducted on the benchmark dataset, and the results showed that the proposed\nmethod outperforms original tracking-by-detection approaches in both efficiency\nand accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 10:22:33 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Xuesong", ""], ["Guivant", "Jose", ""]]}, {"id": "2007.02066", "submitter": "Yun Li", "authors": "Yun Li, Weiqun Wu, Zechun Liu, Chi Zhang, Xiangyu Zhang, Haotian Yao,\n  Baoqun Yin", "title": "Weight-dependent Gates for Differentiable Neural Network Pruning", "comments": "ECCV workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple and effective network pruning framework,\nwhich introduces novel weight-dependent gates to prune filter adaptively. We\nargue that the pruning decision should depend on the convolutional weights, in\nother words, it should be a learnable function of filter weights. We thus\nconstruct the weight-dependent gates (W-Gates) to learn the information from\nfilter weights and obtain binary filter gates to prune or keep the filters\nautomatically. To prune the network under hardware constraint, we train a\nLatency Predict Net (LPNet) to estimate the hardware latency of candidate\npruned networks. Based on the proposed LPNet, we can optimize W-Gates and the\npruning ratio of each layer under latency constraint. The whole framework is\ndifferentiable and can be optimized by gradient-based method to achieve a\ncompact network with better trade-off between accuracy and efficiency. We have\ndemonstrated the effectiveness of our method on Resnet34, Resnet50 and\nMobileNet V2, achieving up to 1.33/1.28/1.1 higher Top-1 accuracy with lower\nhardware latency on ImageNet. Compared with state-of-the-art pruning methods,\nour method achieves superior performance.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 10:29:07 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 03:44:08 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Li", "Yun", ""], ["Wu", "Weiqun", ""], ["Liu", "Zechun", ""], ["Zhang", "Chi", ""], ["Zhang", "Xiangyu", ""], ["Yao", "Haotian", ""], ["Yin", "Baoqun", ""]]}, {"id": "2007.02072", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Pranay Gupta, Anirudh Thatipelli, Aditya Aggarwal, Shubh Maheshwari,\n  Neel Trivedi, Sourav Das, Ravi Kiran Sarvadevabhatla", "title": "Quo Vadis, Skeleton Action Recognition ?", "comments": "To appear in International Journal of Computer Vision (IJCV). Project\n  page: https://skeleton.iiit.ac.in/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study current and upcoming frontiers across the landscape\nof skeleton-based human action recognition. To study skeleton-action\nrecognition in the wild, we introduce Skeletics-152, a curated and 3-D\npose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale\naction dataset. We extend our study to include out-of-context actions by\nintroducing Skeleton-Mimetics, a dataset derived from the recently introduced\nMimetics dataset. We also introduce Metaphorics, a dataset with caption-style\nannotated YouTube videos of the popular social game Dumb Charades and\ninterpretative dance performances. We benchmark state-of-the-art models on the\nNTU-120 dataset and provide multi-layered assessment of the results. The\nresults from benchmarking the top performers of NTU-120 on the newly introduced\ndatasets reveal the challenges and domain gap induced by actions in the wild.\nOverall, our work characterizes the strengths and limitations of existing\napproaches and datasets. Via the introduced datasets, our work enables new\nfrontiers for human action recognition.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 11:02:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 16:30:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gupta", "Pranay", ""], ["Thatipelli", "Anirudh", ""], ["Aggarwal", "Aditya", ""], ["Maheshwari", "Shubh", ""], ["Trivedi", "Neel", ""], ["Das", "Sourav", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2007.02075", "submitter": "Diego Valsesia", "authors": "Andrea Bordone Molini, Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "Speckle2Void: Deep Self-Supervised SAR Despeckling with Blind-Spot\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction from synthetic aperture radar (SAR) images is heavily\nimpaired by speckle noise, hence despeckling is a crucial preliminary step in\nscene analysis algorithms. The recent success of deep learning envisions a new\ngeneration of despeckling techniques that could outperform classical\nmodel-based methods. However, current deep learning approaches to despeckling\nrequire supervision for training, whereas clean SAR images are impossible to\nobtain. In the literature, this issue is tackled by resorting to either\nsynthetically speckled optical images, which exhibit different properties with\nrespect to true SAR images, or multi-temporal SAR images, which are difficult\nto acquire or fuse accurately. In this paper, inspired by recent works on\nblind-spot denoising networks, we propose a self-supervised Bayesian\ndespeckling method. The proposed method is trained employing only noisy SAR\nimages and can therefore learn features of real SAR images rather than\nsynthetic data. Experiments show that the performance of the proposed approach\nis very close to the supervised training approach on synthetic data and\nsuperior on real data in both quantitative and visual assessments.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 11:38:48 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Molini", "Andrea Bordone", ""], ["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "2007.02078", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra", "title": "Registration of Histopathogy Images Using Structural Information From\n  Fine Grained Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration is an important part of many clinical workflows and factually,\nincluding information of structures of interest improves registration\nperformance. We propose a novel approach of combining segmentation information\nin a registration framework using self supervised segmentation feature maps\nextracted using a pre-trained segmentation network followed by clustering.\nUsing self supervised feature maps enables us to use segmentation information\ndespite the unavailability of manual segmentations. Experimental results show\nour approach effectively replaces manual segmentation maps and demonstrate the\npossibility of obtaining state of the art registration performance in real\nworld cases where manual segmentation maps are unavailable.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 12:05:03 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Mahapatra", "Dwarikanath", ""]]}, {"id": "2007.02080", "submitter": "Dimitri Korsch", "authors": "Dimitri Korsch, Paul Bodesheim, Joachim Denzler", "title": "End-to-end Learning of a Fisher Vector Encoding for Part Features in\n  Fine-grained Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based approaches for fine-grained recognition do not show the expected\nperformance gain over global methods, although being able to explicitly focus\non small details that are relevant for distinguishing highly similar classes.\nWe assume that part-based methods suffer from a missing representation of local\nfeatures, which is invariant to the order of parts and can handle a varying\nnumber of visible parts appropriately. The order of parts is artificial and\noften only given by ground-truth annotations, whereas viewpoint variations and\nocclusions result in parts that are not observable. Therefore, we propose\nintegrating a Fisher vector encoding of part features into convolutional neural\nnetworks. The parameters for this encoding are estimated jointly with those of\nthe neural network in an end-to-end manner. Our approach improves\nstate-of-the-art accuracies for bird species classification on CUB-200-2011\nfrom 90.40\\% to 90.95\\%, on NA-Birds from 89.20\\% to 90.30\\%, and on Birdsnap\nfrom 84.30\\% to 86.97\\%.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 12:17:25 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Korsch", "Dimitri", ""], ["Bodesheim", "Paul", ""], ["Denzler", "Joachim", ""]]}, {"id": "2007.02084", "submitter": "Mikko Lauri", "authors": "Mikko Lauri, Joni Pajarinen, Jan Peters, Simone Frintrop", "title": "Multi-Sensor Next-Best-View Planning as Matroid-Constrained Submodular\n  Maximization", "comments": "8 pages, 7 figures. Accepted for publication in IEEE Robotics and\n  Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2020.3007445", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scene models are useful in robotics for tasks such as path planning,\nobject manipulation, and structural inspection. We consider the problem of\ncreating a 3D model using depth images captured by a team of multiple robots.\nEach robot selects a viewpoint and captures a depth image from it, and the\nimages are fused to update the scene model. The process is repeated until a\nscene model of desired quality is obtained. Next-best-view planning uses the\ncurrent scene model to select the next viewpoints. The objective is to select\nviewpoints so that the images captured using them improve the quality of the\nscene model the most. In this paper, we address next-best-view planning for\nmultiple depth cameras. We propose a utility function that scores sets of\nviewpoints and avoids overlap between multiple sensors. We show that\nmulti-sensor next-best-view planning with this utility function is an instance\nof submodular maximization under a matroid constraint. This allows the planning\nproblem to be solved by a polynomial-time greedy algorithm that yields a\nsolution within a constant factor from the optimal. We evaluate the performance\nof our planning algorithm in simulated experiments with up to 8 sensors, and in\nreal-world experiments using two robot arms equipped with depth cameras.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 12:28:18 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lauri", "Mikko", ""], ["Pajarinen", "Joni", ""], ["Peters", "Jan", ""], ["Frintrop", "Simone", ""]]}, {"id": "2007.02096", "submitter": "Li Wang", "authors": "Yue Sun, Kun Gao, Zhengwang Wu, Zhihao Lei, Ying Wei, Jun Ma, Xiaoping\n  Yang, Xue Feng, Li Zhao, Trung Le Phan, Jitae Shin, Tao Zhong, Yu Zhang,\n  Lequan Yu, Caizi Li, Ramesh Basnet, M. Omair Ahmad, M.N.S. Swamy, Wenao Ma,\n  Qi Dou, Toan Duc Bui, Camilo Bermudez Noguera, Bennett Landman (Senior\n  Member, IEEE), Ian H. Gotlib, Kathryn L. Humphreys, Sarah Shultz, Longchuan\n  Li, Sijie Niu, Weili Lin, Valerie Jewells, Gang Li (Senior Member, IEEE),\n  Dinggang Shen (Fellow, IEEE), Li Wang (Senior Member, IEEE)", "title": "Multi-Site Infant Brain Segmentation Algorithms: The iSeg-2019 Challenge", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 40(5), 1363-1376, 2021", "doi": "10.1109/TMI.2021.3055428", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better understand early brain growth patterns in health and disorder, it\nis critical to accurately segment infant brain magnetic resonance (MR) images\ninto white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Deep\nlearning-based methods have achieved state-of-the-art performance; however, one\nof major limitations is that the learning-based methods may suffer from the\nmulti-site issue, that is, the models trained on a dataset from one site may\nnot be applicable to the datasets acquired from other sites with different\nimaging protocols/scanners. To promote methodological development in the\ncommunity, iSeg-2019 challenge (http://iseg2019.web.unc.edu) provides a set of\n6-month infant subjects from multiple sites with different protocols/scanners\nfor the participating methods. Training/validation subjects are from UNC (MAP)\nand testing subjects are from UNC/UMN (BCP), Stanford University, and Emory\nUniversity. By the time of writing, there are 30 automatic segmentation methods\nparticipating in iSeg-2019. We review the 8 top-ranked teams by detailing their\npipelines/implementations, presenting experimental results and evaluating\nperformance in terms of the whole brain, regions of interest, and gyral\nlandmark curves. We also discuss their limitations and possible future\ndirections for the multi-site issue. We hope that the multi-site dataset in\niSeg-2019 and this review article will attract more researchers on the\nmulti-site issue.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 13:39:48 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 13:24:15 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sun", "Yue", "", "Senior\n  Member, IEEE"], ["Gao", "Kun", "", "Senior\n  Member, IEEE"], ["Wu", "Zhengwang", "", "Senior\n  Member, IEEE"], ["Lei", "Zhihao", "", "Senior\n  Member, IEEE"], ["Wei", "Ying", "", "Senior\n  Member, IEEE"], ["Ma", "Jun", "", "Senior\n  Member, IEEE"], ["Yang", "Xiaoping", "", "Senior\n  Member, IEEE"], ["Feng", "Xue", "", "Senior\n  Member, IEEE"], ["Zhao", "Li", "", "Senior\n  Member, IEEE"], ["Phan", "Trung Le", "", "Senior\n  Member, IEEE"], ["Shin", "Jitae", "", "Senior\n  Member, IEEE"], ["Zhong", "Tao", "", "Senior\n  Member, IEEE"], ["Zhang", "Yu", "", "Senior\n  Member, IEEE"], ["Yu", "Lequan", "", "Senior\n  Member, IEEE"], ["Li", "Caizi", "", "Senior\n  Member, IEEE"], ["Basnet", "Ramesh", "", "Senior\n  Member, IEEE"], ["Ahmad", "M. Omair", "", "Senior\n  Member, IEEE"], ["Swamy", "M. N. S.", "", "Senior\n  Member, IEEE"], ["Ma", "Wenao", "", "Senior\n  Member, IEEE"], ["Dou", "Qi", "", "Senior\n  Member, IEEE"], ["Bui", "Toan Duc", "", "Senior\n  Member, IEEE"], ["Noguera", "Camilo Bermudez", "", "Senior\n  Member, IEEE"], ["Landman", "Bennett", "", "Senior\n  Member, IEEE"], ["Gotlib", "Ian H.", "", "Senior Member, IEEE"], ["Humphreys", "Kathryn L.", "", "Senior Member, IEEE"], ["Shultz", "Sarah", "", "Senior Member, IEEE"], ["Li", "Longchuan", "", "Senior Member, IEEE"], ["Niu", "Sijie", "", "Senior Member, IEEE"], ["Lin", "Weili", "", "Senior Member, IEEE"], ["Jewells", "Valerie", "", "Senior Member, IEEE"], ["Li", "Gang", "", "Senior Member, IEEE"], ["Shen", "Dinggang", "", "Fellow, IEEE"], ["Wang", "Li", "", "Senior Member, IEEE"]]}, {"id": "2007.02099", "submitter": "Jianan Li", "authors": "Jianan Li, Jiashi Feng", "title": "Local Grid Rendering Networks for 3D Object Detection in Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of 3D object detection models over point clouds highly\ndepends on their capability of modeling local geometric patterns. Conventional\npoint-based models exploit local patterns through a symmetric function (e.g.\nmax pooling) or based on graphs, which easily leads to loss of fine-grained\ngeometric structures. Regarding capturing spatial patterns, CNNs are powerful\nbut it would be computationally costly to directly apply convolutions on point\ndata after voxelizing the entire point clouds to a dense regular 3D grid. In\nthis work, we aim to improve performance of point-based models by enhancing\ntheir pattern learning ability through leveraging CNNs while preserving\ncomputational efficiency. We propose a novel and principled Local Grid\nRendering (LGR) operation to render the small neighborhood of a subset of input\npoints into a low-resolution 3D grid independently, which allows small-size\nCNNs to accurately model local patterns and avoids convolutions over a dense\ngrid to save computation cost. With the LGR operation, we introduce a new\ngeneric backbone called LGR-Net for point cloud feature extraction with simple\ndesign and high efficiency. We validate LGR-Net for 3D object detection on the\nchallenging ScanNet and SUN RGB-D datasets. It advances state-of-the-art\nresults significantly by 5.5 and 4.5 mAP, respectively, with only slight\nincreased computation overhead.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 13:57:43 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Jianan", ""], ["Feng", "Jiashi", ""]]}, {"id": "2007.02106", "submitter": "Odysseas Kechagias-Stamatis", "authors": "O. Kechagias-Stamatis and N. Aouf", "title": "Automatic Target Recognition on Synthetic Aperture Radar Imagery: A\n  Survey", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Target Recognition (ATR) for military applications is one of the\ncore processes towards enhancing intelligencer and autonomously operating\nmilitary platforms. Spurred by this and given that Synthetic Aperture Radar\n(SAR) presents several advantages over its counterpart data domains, this paper\nsurveys and assesses current SAR ATR architectures that employ the most popular\ndataset for the SAR domain, namely the Moving and Stationary Target Acquisition\nand Recognition (MSTAR) dataset. Based on the current methodology trends, we\npropose a taxonomy for the SAR ATR architectures, along with a direct\ncomparison of the strengths and weaknesses of each method under both standard\nand extended operational conditions. Additionally, despite MSTAR being the\nstandard SAR ATR benchmarking dataset we also highlight its weaknesses and\nsuggest future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:22:30 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 21:11:18 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kechagias-Stamatis", "O.", ""], ["Aouf", "N.", ""]]}, {"id": "2007.02108", "submitter": "Yang Li", "authors": "Yang Li, Tianwei Zhang, Yoshihiko Nakamura and Tatsuya Harada", "title": "SplitFusion: Simultaneous Tracking and Mapping for Non-Rigid Scenes", "comments": "Accepted to IROS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SplitFusion, a novel dense RGB-D SLAM framework that\nsimultaneously performs tracking and dense reconstruction for both rigid and\nnon-rigid components of the scene. SplitFusion first adopts deep learning based\nsemantic instant segmentation technique to split the scene into rigid or\nnon-rigid surfaces. The split surfaces are independently tracked via rigid or\nnon-rigid ICP and reconstructed through incremental depth map fusion.\nExperimental results show that the proposed approach can provide not only\naccurate environment maps but also well-reconstructed non-rigid targets, e.g.\nthe moving humans.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:27:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Yang", ""], ["Zhang", "Tianwei", ""], ["Nakamura", "Yoshihiko", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2007.02145", "submitter": "Marc Masana Castrillo", "authors": "Marc Masana, Bart{\\l}omiej Twardowski, Joost van de Weijer", "title": "On Class Orderings for Incremental Learning", "comments": "Accepted at CL-ICML 2020. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The influence of class orderings in the evaluation of incremental learning\nhas received very little attention. In this paper, we investigate the impact of\nclass orderings for incrementally learned classifiers. We propose a method to\ncompute various orderings for a dataset. The orderings are derived by simulated\nannealing optimization from the confusion matrix and reflect different\nincremental learning scenarios, including maximally and minimally confusing\ntasks. We evaluate a wide range of state-of-the-art incremental learning\nmethods on the proposed orderings. Results show that orderings can have a\nsignificant impact on performance and the ranking of the methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 17:07:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 06:46:16 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Masana", "Marc", ""], ["Twardowski", "Bart\u0142omiej", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2007.02149", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dipto Sarkar, Shailesh Deshpande, Edward Curry", "title": "Human Assisted Artificial Intelligence Based Technique to Create Natural\n  Features for OpenStreetMap", "comments": "3 pages, 2 Figures, Submitted to FOSS4G Europe 2020 Academic Track\n  (Postponed to 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an AI-based technique using freely available\nsatellite images like Landsat and Sentinel to create natural features over OSM\nin congruence with human editors acting as initiators and validators. The\nmethod is based on Interactive Machine Learning technique where human inputs\nare coupled with the machine to solve complex problems efficiently as compare\nto pure autonomous process. We use a bottom-up approach where a machine\nlearning (ML) pipeline in loop with editors is used to extract classes using\nspectral signatures of images and later convert them to editable features to\ncreate natural features.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 17:26:46 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 18:33:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yadav", "Piyush", ""], ["Sarkar", "Dipto", ""], ["Deshpande", "Shailesh", ""], ["Curry", "Edward", ""]]}, {"id": "2007.02157", "submitter": "Zitong Yu", "authors": "Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, Guoying Zhao", "title": "Face Anti-Spoofing with Human Material Perception", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) plays a vital role in securing the face recognition\nsystems from presentation attacks. Most existing FAS methods capture various\ncues (e.g., texture, depth and reflection) to distinguish the live faces from\nthe spoofing faces. All these cues are based on the discrepancy among physical\nmaterials (e.g., skin, glass, paper and silicone). In this paper we rephrase\nface anti-spoofing as a material recognition problem and combine it with\nclassical human material perception [1], intending to extract discriminative\nand robust features for FAS. To this end, we propose the Bilateral\nConvolutional Networks (BCN), which is able to capture intrinsic material-based\npatterns via aggregating multi-level bilateral macro- and micro- information.\nFurthermore, Multi-level Feature Refinement Module (MFRM) and multi-head\nsupervision are utilized to learn more robust features. Comprehensive\nexperiments are performed on six benchmark datasets, and the proposed method\nachieves superior performance on both intra- and cross-dataset testings. One\nhighlight is that we achieve overall 11.3$\\pm$9.5\\% EER for cross-type testing\nin SiW-M dataset, which significantly outperforms previous results. We hope\nthis work will facilitate future cooperation between FAS and material\ncommunities.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:25:53 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yu", "Zitong", ""], ["Li", "Xiaobai", ""], ["Niu", "Xuesong", ""], ["Shi", "Jingang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2007.02171", "submitter": "Gunjan Aggarwal", "authors": "Gunjan Aggarwal, Devi Parikh", "title": "Neuro-Symbolic Generative Art: A Preliminary Study", "comments": "Accepted as a short paper at ICCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two classes of generative art approaches: neural, where a deep\nmodel is trained to generate samples from a data distribution, and symbolic or\nalgorithmic, where an artist designs the primary parameters and an autonomous\nsystem generates samples within these constraints. In this work, we propose a\nnew hybrid genre: neuro-symbolic generative art. As a preliminary study, we\ntrain a generative deep neural network on samples from the symbolic approach.\nWe demonstrate through human studies that subjects find the final artifacts and\nthe creation process using our neuro-symbolic approach to be more creative than\nthe symbolic approach 61% and 82% of the time respectively.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 19:40:00 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Aggarwal", "Gunjan", ""], ["Parikh", "Devi", ""]]}, {"id": "2007.02180", "submitter": "Issam Hadj Laradji", "authors": "Issam Laradji, Pau Rodriguez, Oscar Ma\\~nas, Keegan Lensink, Marco\n  Law, Lironne Kurzman, William Parker, David Vazquez, and Derek Nowrouzezahrai", "title": "A Weakly Supervised Consistency-based Learning Method for COVID-19\n  Segmentation in CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) has spread aggressively across the world\ncausing an existential health crisis. Thus, having a system that automatically\ndetects COVID-19 in tomography (CT) images can assist in quantifying the\nseverity of the illness. Unfortunately, labelling chest CT scans requires\nsignificant domain expertise, time, and effort. We address these labelling\nchallenges by only requiring point annotations, a single pixel for each\ninfected region on a CT image. This labeling scheme allows annotators to label\na pixel in a likely infected region, only taking 1-3 seconds, as opposed to\n10-15 seconds to segment a region. Conventionally, segmentation models train on\npoint-level annotations using the cross-entropy loss function on these labels.\nHowever, these models often suffer from low precision. Thus, we propose a\nconsistency-based (CB) loss function that encourages the output predictions to\nbe consistent with spatial transformations of the input images. The experiments\non 3 open-source COVID-19 datasets show that this loss function yields\nsignificant improvement over conventional point-level loss functions and almost\nmatches the performance of models trained with full supervision with much less\nhuman effort. Code is available at:\n\\url{https://github.com/IssamLaradji/covid19_weak_supervision}.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 20:41:17 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 11:56:15 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Laradji", "Issam", ""], ["Rodriguez", "Pau", ""], ["Ma\u00f1as", "Oscar", ""], ["Lensink", "Keegan", ""], ["Law", "Marco", ""], ["Kurzman", "Lironne", ""], ["Parker", "William", ""], ["Vazquez", "David", ""], ["Nowrouzezahrai", "Derek", ""]]}, {"id": "2007.02190", "submitter": "Ayan Das", "authors": "Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang and Yi-Zhe Song", "title": "B\\'ezierSketch: A generative model for scalable vector sketches", "comments": "Accepted as poster at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of neural generative models of human sketches is a fascinating\ncontemporary modeling problem due to the links between sketch image generation\nand the human drawing process. The landmark SketchRNN provided breakthrough by\nsequentially generating sketches as a sequence of waypoints. However this leads\nto low-resolution image generation, and failure to model long sketches. In this\npaper we present B\\'ezierSketch, a novel generative model for fully vector\nsketches that are automatically scalable and high-resolution. To this end, we\nfirst introduce a novel inverse graphics approach to stroke embedding that\ntrains an encoder to embed each stroke to its best fit B\\'ezier curve. This\nenables us to treat sketches as short sequences of paramaterized strokes and\nthus train a recurrent sketch generator with greater capacity for longer\nsketches, while producing scalable high-resolution results. We report\nqualitative and quantitative results on the Quick, Draw! benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:30:52 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 15:13:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Das", "Ayan", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2007.02196", "submitter": "Jaya Krishna Mandivarapu Mr", "authors": "Jaya Krishna Mandivarapu, Blake Camp, Rolando Estrada", "title": "Deep Active Learning via Open Set Recognition", "comments": "Withdrawn to address fundamental concerns with the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data is easy to acquire but expensive and\ntime-consuming to label prominent examples include medical imaging and NLP.\nThis disparity has only grown in recent years as our ability to collect data\nimproves. Under these constraints, it makes sense to select only the most\ninformative instances from the unlabeled pool and request an oracle (e.g., a\nhuman expert) to provide labels for those samples. The goal of active learning\nis to infer the informativeness of unlabeled samples so as to minimize the\nnumber of requests to the oracle. Here, we formulate active learning as an\nopen-set recognition problem. In this paradigm, only some of the inputs belong\nto known classes; the classifier must identify the rest as unknown. More\nspecifically, we leverage variational neural networks (VNNs), which produce\nhigh-confidence (i.e., low-entropy) predictions only for inputs that closely\nresemble the training data. We use the inverse of this confidence measure to\nselect the samples that the oracle should label. Intuitively, unlabeled samples\nthat the VNN is uncertain about are more informative for future training. We\ncarried out an extensive evaluation of our novel, probabilistic formulation of\nactive learning, achieving state-of-the-art results on MNIST, CIFAR-10, and\nCIFAR-100. Additionally, unlike current active learning methods, our algorithm\ncan learn tasks without the need for task labels. As our experiments show, when\nthe unlabeled pool consists of a mixture of samples from multiple datasets, our\napproach can automatically distinguish between samples from seen vs. unseen\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 22:09:17 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:56:01 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 14:27:20 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 18:47:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mandivarapu", "Jaya Krishna", ""], ["Camp", "Blake", ""], ["Estrada", "Rolando", ""]]}, {"id": "2007.02200", "submitter": "Benyamin Ghojogh", "authors": "Milad Sikaroudi, Benyamin Ghojogh, Amir Safarpoor, Fakhri Karray, Mark\n  Crowley, H.R. Tizhoosh", "title": "Offline versus Online Triplet Mining based on Extreme Distances of\n  Histopathology Patches", "comments": "Accepted for presentation at the 15th International Symposium on\n  Visual Computing (ISVC) 2020, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the effect of offline and online triplet mining for colorectal\ncancer (CRC) histopathology dataset containing 100,000 patches. We consider the\nextreme, i.e., farthest and nearest patches to a given anchor, both in online\nand offline mining. While many works focus solely on selecting the triplets\nonline (batch-wise), we also study the effect of extreme distances and neighbor\npatches before training in an offline fashion. We analyze extreme cases'\nimpacts in terms of embedding distance for offline versus online mining,\nincluding easy positive, batch semi-hard, batch hard triplet mining,\nneighborhood component analysis loss, its proxy version, and distance weighted\nsampling. We also investigate online approaches based on extreme distance and\ncomprehensively compare offline, and online mining performance based on the\ndata patterns and explain offline mining as a tractable generalization of the\nonline mining with large mini-batch size. As well, we discuss the relations of\ndifferent colorectal tissue types in terms of extreme distances. We found that\noffline and online mining approaches have comparable performances for a\nspecific architecture, such as ResNet-18 in this study. Moreover, we found the\nassorted case, including different extreme distances, is promising, especially\nin the online approach.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 22:33:08 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 15:17:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sikaroudi", "Milad", ""], ["Ghojogh", "Benyamin", ""], ["Safarpoor", "Amir", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2007.02209", "submitter": "Yiwen Guo", "authors": "Yiwen Guo and Long Chen and Yurong Chen and Changshui Zhang", "title": "On Connections between Regularizations for Improving DNN Robustness", "comments": "Accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes regularization terms proposed recently for improving the\nadversarial robustness of deep neural networks (DNNs), from a theoretical point\nof view. Specifically, we study possible connections between several effective\nmethods, including input-gradient regularization, Jacobian regularization,\ncurvature regularization, and a cross-Lipschitz functional. We investigate them\non DNNs with general rectified linear activations, which constitute one of the\nmost prevalent families of models for image classification and a host of other\nmachine learning applications. We shed light on essential ingredients of these\nregularizations and re-interpret their functionality. Through the lens of our\nstudy, more principled and efficient regularizations can possibly be invented\nin the near future.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 23:43:32 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Guo", "Yiwen", ""], ["Chen", "Long", ""], ["Chen", "Yurong", ""], ["Zhang", "Changshui", ""]]}, {"id": "2007.02232", "submitter": "Renato J Cintra", "authors": "R. J. Cintra", "title": "An Integer Approximation Method for Discrete Sinusoidal Transforms", "comments": "13 pages, 5 figures, 8 tables", "journal-ref": "Circuits, Systems, and Signal Processing, vol. 30, n. 6, 2011", "doi": "10.1007/s00034-011-9318-5", "report-no": null, "categories": "eess.SP cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate methods have been considered as a means to the evaluation of\ndiscrete transforms. In this work, we propose and analyze a class of integer\ntransforms for the discrete Fourier, Hartley, and cosine transforms (DFT, DHT,\nand DCT), based on simple dyadic rational approximation methods. The introduced\nmethod is general, applicable to several block-lengths, whereas existing\napproaches are usually dedicated to specific transform sizes. The suggested\napproximate transforms enjoy low multiplicative complexity and the\northogonality property is achievable via matrix polar decomposition. We show\nthat the obtained transforms are competitive with archived methods in\nliterature. New 8-point square wave approximate transforms for the DFT, DHT,\nand DCT are also introduced as particular cases of the introduced methodology.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 03:37:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cintra", "R. J.", ""]]}, {"id": "2007.02240", "submitter": "Pritha Ganguly", "authors": "Pritha Ganguly, Nitesh Methani, Mitesh M. Khapra and Pratyush Kumar", "title": "A Systematic Evaluation of Object Detection Networks for Scientific\n  Plots", "comments": "This work has been accepted and will be presented at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are existing object detection methods adequate for detecting text and visual\nelements in scientific plots which are arguably different than the objects\nfound in natural images? To answer this question, we train and compare the\naccuracy of various SOTA object detection networks on the PlotQA dataset. At\nthe standard IOU setting of 0.5, most networks perform well with mAP scores\ngreater than 80% in detecting the relatively simple objects in plots. However,\nthe performance drops drastically when evaluated at a stricter IOU of 0.9 with\nthe best model giving a mAP of 35.70%. Note that such a stricter evaluation is\nessential when dealing with scientific plots where even minor localisation\nerrors can lead to large errors in downstream numerical inferences. Given this\npoor performance, we propose minor modifications to existing models by\ncombining ideas from different object detection networks. While this\nsignificantly improves the performance, there are still 2 main issues: (i)\nperformance on text objects which are essential for reasoning is very poor, and\n(ii) inference time is unacceptably large considering the simplicity of plots.\nTo solve this open problem, we make a series of contributions: (a) an efficient\nregion proposal method based on Laplacian edge detectors, (b) a feature\nrepresentation of region proposals that includes neighbouring information, (c)\na linking component to join multiple region proposals for detecting longer\ntextual objects, and (d) a custom loss function that combines a smooth L1-loss\nwith an IOU-based loss. Combining these ideas, our final model is very accurate\nat extreme IOU values achieving a mAP of 93.44%@0.9 IOU. Simultaneously, our\nmodel is very efficient with an inference time 16x lesser than the current\nmodels, including one-stage detectors. With these contributions, we enable\nfurther exploration on the automated reasoning of plots.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 05:30:53 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 07:37:10 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ganguly", "Pritha", ""], ["Methani", "Nitesh", ""], ["Khapra", "Mitesh M.", ""], ["Kumar", "Pratyush", ""]]}, {"id": "2007.02246", "submitter": "Yong Lee", "authors": "Yong Lee, Shaohua Zhang, Miao Li, Xiaoyu He", "title": "Blind Inverse Gamma Correction with Maximized Differential Entropy", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unwanted nonlinear gamma distortion frequently occurs in a great diversity of\nimages during the procedures of image acquisition, processing, and/or display.\nAnd the gamma distortion often varies with capture setup change and luminance\nvariation. Blind inverse gamma correction, which automatically determines a\nproper restoration gamma value from a given image, is of paramount importance\nto attenuate the distortion. For blind inverse gamma correction, an adaptive\ngamma transformation method (AGT-ME) is proposed directly from a maximized\ndifferential entropy model. And the corresponding optimization has a\nmathematical concise closed-form solution, resulting in efficient\nimplementation and accurate gamma restoration of AGT-ME. Considering the human\neye has a non-linear perception sensitivity, a modified version AGT-ME-VISUAL\nis also proposed to achieve better visual performance. Tested on variable\ndatasets, AGT-ME could obtain an accurate estimation of a large range of gamma\ndistortion (0.1 to 3.0), outperforming the state-of-the-art methods. Besides,\nthe proposed AGT-ME and AGT-ME-VISUAL were applied to three typical\napplications, including automatic gamma adjustment, natural/medical image\ncontrast enhancement, and fringe projection profilometry image restoration.\nFurthermore, the AGT-ME/ AGT-ME-VISUAL is general and can be seamlessly\nextended to the masked image, multi-channel (color or spectrum) image, or\nmulti-frame video, and free of the arbitrary tuning parameter. Besides, the\ncorresponding Python code (https://github.com/yongleex/AGT-ME) is also provided\nfor interested users.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 06:15:01 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lee", "Yong", ""], ["Zhang", "Shaohua", ""], ["Li", "Miao", ""], ["He", "Xiaoyu", ""]]}, {"id": "2007.02248", "submitter": "Kishor Datta Gupta", "authors": "Anik Khan, Kishor Datta Gupta, Deepak Venugopal, Nirman Kumar", "title": "CIDMP: Completely Interpretable Detection of Malaria Parasite in Red\n  Blood Cells using Lower-dimensional Feature Space", "comments": "Accepted in The 2020 International Joint Conference on Neural\n  Networks (IJCNN 2020) At Glasgow (UK)", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206885", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting if red blood cells (RBC) are infected with the malaria parasite is\nan important problem in Pathology. Recently, supervised machine learning\napproaches have been used for this problem, and they have had reasonable\nsuccess. In particular, state-of-the-art methods such as Convolutional Neural\nNetworks automatically extract increasingly complex feature hierarchies from\nthe image pixels. While such generalized automatic feature extraction methods\nhave significantly reduced the burden of feature engineering in many domains,\nfor niche tasks such as the one we consider in this paper, they result in two\nmajor problems. First, they use a very large number of features (that may or\nmay not be relevant) and therefore training such models is computationally\nexpensive. Further, more importantly, the large feature-space makes it very\nhard to interpret which features are truly important for predictions. Thus, a\ncriticism of such methods is that learning algorithms pose opaque black boxes\nto its users, in this case, medical experts. The recommendation of such\nalgorithms can be understood easily, but the reason for their recommendation is\nnot clear. This is the problem of non-interpretability of the model, and the\nbest-performing algorithms are usually the least interpretable. To address\nthese issues, in this paper, we propose an approach to extract a very small\nnumber of aggregated features that are easy to interpret and compute, and\nempirically show that we obtain high prediction accuracy even with a\nsignificantly reduced feature-space.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 06:28:09 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Khan", "Anik", ""], ["Gupta", "Kishor Datta", ""], ["Venugopal", "Deepak", ""], ["Kumar", "Nirman", ""]]}, {"id": "2007.02250", "submitter": "Shengyang Chen", "authors": "Shengyang Chen, Chih-Yung Wen, Yajing Zou and Wu Chen", "title": "Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback\n  Loops", "comments": "14 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel stereo visual inertial pose estimation\nmethod. Compared to the widely used filter-based or optimization-based\napproaches, the pose estimation process is modeled as a control system.\nDesigned feedback or feedforward loops are introduced to achieve the stable\ncontrol of the system, which include a gradient decreased feedback loop, a\nroll-pitch feed forward loop and a bias estimation feedback loop. This system,\nnamed FLVIS (Feedforward-feedback Loop-based Visual Inertial System), is\nevaluated on the popular EuRoc MAV dataset. FLVIS achieves high accuracy and\nrobustness with respect to other state-of-the-art visual SLAM approaches. The\nsystem has also been implemented and tested on a UAV platform. The source code\nof this research is public to the research community.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 06:40:11 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 09:07:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chen", "Shengyang", ""], ["Wen", "Chih-Yung", ""], ["Zou", "Yajing", ""], ["Chen", "Wu", ""]]}, {"id": "2007.02252", "submitter": "Gaochang Wu", "authors": "Gaochang Wu, Yebin Liu, Lu Fang, Tianyou Chai", "title": "Spatial-Angular Attention Network for Light Field Reconstruction", "comments": "12 pages, 10 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based light field reconstruction methods demand in constructing a\nlarge receptive field by deepening the network to capture correspondences\nbetween input views. In this paper, we propose a spatial-angular attention\nnetwork to perceive correspondences in the light field non-locally, and\nreconstruction high angular resolution light field in an end-to-end manner.\nMotivated by the non-local attention mechanism, a spatial-angular attention\nmodule specifically for the high-dimensional light field data is introduced to\ncompute the responses from all the positions in the epipolar plane for each\npixel in the light field, and generate an attention map that captures\ncorrespondences along the angular dimension. We then propose a multi-scale\nreconstruction structure to efficiently implement the non-local attention in\nthe low spatial scale, while also preserving the high frequency components in\nthe high spatial scales. Extensive experiments demonstrate the superior\nperformance of the proposed spatial-angular attention network for\nreconstructing sparsely-sampled light fields with non-Lambertian effects.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 06:55:29 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wu", "Gaochang", ""], ["Liu", "Yebin", ""], ["Fang", "Lu", ""], ["Chai", "Tianyou", ""]]}, {"id": "2007.02267", "submitter": "Ashish Sinha", "authors": "Ashish Sinha, K S Suresh", "title": "Deep Learning based Dimple Segmentation for Quantitative Fractography", "comments": "Accepted as a poster only at IC-MSE 2021. In review for publication\n  in a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we try to address the challenging problem of dimple detection\nand segmentation in Titanium alloys using machine learning methods, especially\nneural networks. The images i.e. fractographs are obtained using a Scanning\nElection Microscope (SEM). To determine the cause of fracture in metals we\naddress the problem of segmentation of dimples in fractographs i.e. the\nfracture surface of metals using supervised machine learning methods.\nDetermining the cause of fracture would help us in material property,\nmechanical property prediction and development of new fracture-resistant\nmaterials. This method would also help in correlating the topography of the\nfracture surface with the mechanical properties of the material. Our proposed\nnovel model achieves the best performance as compared to other previous\napproaches. To the best of our knowledge, this is one the first work in\nfractography using fully convolutional neural networks with self-attention for\nsupervised learning of dimple fractography, though it can be easily extended to\naccount for brittle characteristics as well.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 08:43:58 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 16:10:15 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 10:33:09 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sinha", "Ashish", ""], ["Suresh", "K S", ""]]}, {"id": "2007.02268", "submitter": "Lijie Wang", "authors": "Lijie Wang, Xueting Wang and Toshihiko Yamasaki", "title": "Image Aesthetics Prediction Using Multiple Patches Preserving the\n  Original Aspect Ratio of Contents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of social networking services has created an increasing demand for\nselecting, editing, and generating impressive images. This trend increases the\nimportance of evaluating image aesthetics as a complementary function of\nautomatic image processing. We propose a multi-patch method, named MPA-Net\n(Multi-Patch Aggregation Network), to predict image aesthetics scores by\nmaintaining the original aspect ratios of contents in the images. Through an\nexperiment involving the large-scale AVA dataset, which contains 250,000\nimages, we show that the effectiveness of the equal-interval multi-patch\nselection approach for aesthetics score prediction is significant compared to\nthe single-patch prediction and random patch selection approaches. For this\ndataset, MPA-Net outperforms the neural image assessment algorithm, which was\nregarded as a baseline method. In particular, MPA-Net yields a 0.073 (11.5%)\nhigher linear correlation coefficient (LCC) of aesthetics scores and a 0.088\n(14.4%) higher Spearman's rank correlation coefficient (SRCC). MPA-Net also\nreduces the mean square error (MSE) by 0.0115 (4.18%) and achieves results for\nthe LCC and SRCC that are comparable to those of the state-of-the-art\ncontinuous aesthetics score prediction methods. Most notably, MPA-Net yields a\nsignificant lower MSE especially for images with aspect ratios far from 1.0,\nindicating that MPA-Net is useful for a wide range of image aspect ratios.\nMPA-Net uses only images and does not require external information during the\ntraining nor prediction stages. Therefore, MPA-Net has great potential for\napplications aside from aesthetics score prediction such as other human\nsubjectivity prediction.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 08:49:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wang", "Lijie", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2007.02269", "submitter": "Zhou Daquan", "authors": "Zhou Daquan, Qibin Hou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan", "title": "Rethinking Bottleneck Structure for Efficient Mobile Network Design", "comments": "A journal version under review of the previous paper published as a\n  ECCV20 conference paper, improved segmentation and detection results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverted residual block is dominating architecture design for mobile\nnetworks recently. It changes the classic residual bottleneck by introducing\ntwo design rules: learning inverted residuals and using linear bottlenecks. In\nthis paper, we rethink the necessity of such design changes and find it may\nbring risks of information loss and gradient confusion. We thus propose to flip\nthe structure and present a novel bottleneck design, called the sandglass\nblock, that performs identity mapping and spatial transformation at higher\ndimensions and thus alleviates information loss and gradient confusion\neffectively. Extensive experiments demonstrate that, different from the common\nbelief, such bottleneck structure is more beneficial than the inverted ones for\nmobile networks. In ImageNet classification, by simply replacing the inverted\nresidual block with our sandglass block without increasing parameters and\ncomputation, the classification accuracy can be improved by more than 1.7% over\nMobileNetV2. On Pascal VOC 2007 test set, we observe that there is also 0.9%\nmAP improvement in object detection. We further verify the effectiveness of the\nsandglass block by adding it into the search space of neural architecture\nsearch method DARTS. With 25% parameter reduction, the classification accuracy\nis improved by 0.13% over previous DARTS models. Code can be found at:\nhttps://github.com/zhoudaquan/rethinking_bottleneck_design.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 08:55:26 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 14:37:51 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 13:58:02 GMT"}, {"version": "v4", "created": "Fri, 27 Nov 2020 16:02:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Daquan", "Zhou", ""], ["Hou", "Qibin", ""], ["Chen", "Yunpeng", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2007.02272", "submitter": "Feng Liao", "authors": "Xiaoling Huang and Feng Liao", "title": "Automatically Generating Codes from Graphical Screenshots Based on Deep\n  Autocoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During software front-end development, the work to convert Graphical User\nInterface(GUI) image to the corresponding front-end code is an inevitable\ntedious work. There have been some attempts to make this work to be automatic.\nHowever, the GUI code generated by these models is not accurate due to the lack\nof attention mechanism guidance. To solve this problem, we propose PixCoder\nbased on an artificially supervised attention mechanism. The approach is to\ntrain a neural network to predict the style sheets in the input GUI image and\nthen output a vector. PixCoder generate the GUI code targeting specific\nplatform according to the output vector. The experimental results have shown\nthe accuracy of the GUI code generated by PixCoder is over 95%.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 09:40:48 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Huang", "Xiaoling", ""], ["Liao", "Feng", ""]]}, {"id": "2007.02277", "submitter": "Javed Iqbal", "authors": "Javed Iqbal and Mohsen Ali", "title": "Weakly Supervised Domain Adaptation for Built-up Region Segmentation in\n  Aerial and Satellite Imagery", "comments": "Accepted at ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel domain adaptation algorithm to handle the\nchallenges posed by the satellite and aerial imagery, and demonstrates its\neffectiveness on the built-up region segmentation problem. Built-up area\nestimation is an important component in understanding the human impact on the\nenvironment, the effect of public policy, and general urban population\nanalysis. The diverse nature of aerial and satellite imagery and lack of\nlabeled data covering this diversity makes machine learning algorithms\ndifficult to generalize for such tasks, especially across multiple domains. On\nthe other hand, due to the lack of strong spatial context and structure, in\ncomparison to the ground imagery, the application of existing unsupervised\ndomain adaptation methods results in the sub-optimal adaptation. We thoroughly\nstudy the limitations of existing domain adaptation methods and propose a\nweakly-supervised adaptation strategy where we assume image-level labels are\navailable for the target domain. More specifically, we design a built-up area\nsegmentation network (as encoder-decoder), with an image classification head\nadded to guide the adaptation. The devised system is able to address the\nproblem of visual differences in multiple satellite and aerial imagery\ndatasets, ranging from high resolution (HR) to very high resolution (VHR). A\nrealistic and challenging HR dataset is created by hand-tagging the 73.4 sq-km\nof Rwanda, capturing a variety of build-up structures over different terrain.\nThe developed dataset is spatially rich compared to existing datasets and\ncovers diverse built-up scenarios including built-up areas in forests and\ndeserts, mud houses, tin, and colored rooftops. Extensive experiments are\nperformed by adapting from the single-source domain, to segment out the target\ndomain. We achieve high gains ranging 11.6%-52% in IoU over the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 10:05:01 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Iqbal", "Javed", ""], ["Ali", "Mohsen", ""]]}, {"id": "2007.02278", "submitter": "Hao Xu", "authors": "Hao Xu and Ka Hei Hui and Chi-Wing Fu and Hao Zhang", "title": "TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network", "comments": "SIGGRAPH 2020, Technical paper. ACM Trans. Graph., Vol. 39, No. 4,\n  Article 129. Homapage:\n  https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/index.html", "journal-ref": null, "doi": "10.1145/3386569.3392380", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first neural optimization framework to solve a classical\ninstance of the tiling problem. Namely, we seek a non-periodic tiling of an\narbitrary 2D shape using one or more types of tiles: the tiles maximally fill\nthe shape's interior without overlaps or holes. To start, we reformulate tiling\nas a graph problem by modeling candidate tile locations in the target shape as\ngraph nodes and connectivity between tile locations as edges. Further, we build\na graph convolutional neural network, coined TilinGNN, to progressively\npropagate and aggregate features over graph edges and predict tile placements.\nTilinGNN is trained by maximizing the tiling coverage on target shapes, while\navoiding overlaps and holes between the tiles. Importantly, our network is\nself-supervised, as we articulate these criteria as loss terms defined on the\nnetwork outputs, without the need of ground-truth tiling solutions. After\ntraining, the runtime of TilinGNN is roughly linear to the number of candidate\ntile locations, significantly outperforming traditional combinatorial search.\nWe conducted various experiments on a variety of shapes to showcase the speed\nand versatility of TilinGNN. We also present comparisons to alternative methods\nand manual solutions, robustness analysis, and ablation studies to demonstrate\nthe quality of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 10:06:06 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Xu", "Hao", ""], ["Hui", "Ka Hei", ""], ["Fu", "Chi-Wing", ""], ["Zhang", "Hao", ""]]}, {"id": "2007.02295", "submitter": "Ellie-K. Stathopoulou", "authors": "Elisavet Konstantina Stathopoulou, Fabio Remondino", "title": "Multi view stereo with semantic priors", "comments": null, "journal-ref": null, "doi": "10.5194/isprs-archives-XLII-2-W15-1135-2019", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Patch-based stereo is nowadays a commonly used image-based technique for\ndense 3D reconstruction in large scale multi-view applications. The typical\nsteps of such a pipeline can be summarized in stereo pair selection, depth map\ncomputation, depth map refinement and, finally, fusion in order to generate a\ncomplete and accurate representation of the scene in 3D. In this study, we aim\nto support the standard dense 3D reconstruction of scenes as implemented in the\nopen source library OpenMVS by using semantic priors. To this end, during the\ndepth map fusion step, along with the depth consistency check between depth\nmaps of neighbouring views referring to the same part of the 3D scene, we\nimpose extra semantic constraints in order to remove possible errors and\nselectively obtain segmented point clouds per label, boosting automation\ntowards this direction. I n order to reassure semantic coherence between\nneighbouring views, additional semantic criterions can be considered, aiming to\nelim inate mismatches of pixels belonging in different classes.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 11:30:29 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Stathopoulou", "Elisavet Konstantina", ""], ["Remondino", "Fabio", ""]]}, {"id": "2007.02306", "submitter": "Bart Iver Van Blokland", "authors": "Bart Iver van Blokland and Theoharis Theoharis", "title": "Radial Intersection Count Image: a Clutter Resistant 3D Shape Descriptor", "comments": "18 pages, 16 figures, to be published in Computers & Graphics", "journal-ref": "Computers & Graphics, Volume 91, 2020, Pages 118-128", "doi": "10.1016/j.cag.2020.07.007", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel shape descriptor for cluttered scenes is presented, the Radial\nIntersection Count Image (RICI), and is shown to significantly outperform the\nclassic Spin Image (SI) and 3D Shape Context (3DSC) in both uncluttered and,\nmore significantly, cluttered scenes. It is also faster to compute and compare.\nThe clutter resistance of the RICI is mainly due to the design of a novel\ndistance function, capable of disregarding clutter to a great extent. As\nopposed to the SI and 3DSC, which both count point samples, the RICI uses\nintersection counts with the mesh surface, and is therefore noise-free. For\nefficient RICI construction, novel algorithms of general interest were\ndeveloped. These include an efficient circle-triangle intersection algorithm\nand an algorithm for projecting a point into SI-like ($\\alpha$, $\\beta$)\ncoordinates. The 'clutterbox experiment' is also introduced as a better way of\nevaluating descriptors' response to clutter. The SI, 3DSC, and RICI are\nevaluated in this framework and the advantage of the RICI is clearly\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 12:11:38 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["van Blokland", "Bart Iver", ""], ["Theoharis", "Theoharis", ""]]}, {"id": "2007.02319", "submitter": "Herve Lombaert", "authors": "Tal Arbel, Ismail Ben Ayed, Marleen de Bruijne, Maxime Descoteaux,\n  Herve Lombaert, Chris Pal", "title": "Medical Imaging with Deep Learning: MIDL 2020 -- Short Paper Track", "comments": "Accepted extended abstracts can also be found at\n  https://openreview.net/group?id=MIDL.io/2020/Conference#abstract-accept-papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This compendium gathers all the accepted extended abstracts from the Third\nInternational Conference on Medical Imaging with Deep Learning (MIDL 2020),\nheld in Montreal, Canada, 6-9 July 2020. Note that only accepted extended\nabstracts are listed here, the Proceedings of the MIDL 2020 Full Paper Track\nare published in the Proceedings of Machine Learning Research (PMLR).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 20:01:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Arbel", "Tal", ""], ["Ayed", "Ismail Ben", ""], ["de Bruijne", "Marleen", ""], ["Descoteaux", "Maxime", ""], ["Lombaert", "Herve", ""], ["Pal", "Chris", ""]]}, {"id": "2007.02343", "submitter": "Yunfei Liu", "authors": "Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu", "title": "Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks", "comments": "Accepted by ECCV-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that DNNs can be compromised by backdoor attacks\ncrafted at training time. A backdoor attack installs a backdoor into the victim\nmodel by injecting a backdoor pattern into a small proportion of the training\ndata. At test time, the victim model behaves normally on clean test data, yet\nconsistently predicts a specific (likely incorrect) target class whenever the\nbackdoor pattern is present in a test example. While existing backdoor attacks\nare effective, they are not stealthy. The modifications made on training data\nor labels are often suspicious and can be easily detected by simple data\nfiltering or human inspection. In this paper, we present a new type of backdoor\nattack inspired by an important natural phenomenon: reflection. Using\nmathematical modeling of physical reflection models, we propose reflection\nbackdoor (Refool) to plant reflections as backdoor into a victim model. We\ndemonstrate on 3 computer vision tasks and 5 datasets that, Refool can attack\nstate-of-the-art DNNs with high success rate, and is resistant to\nstate-of-the-art backdoor defenses.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 13:56:48 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 13:46:10 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Yunfei", ""], ["Ma", "Xingjun", ""], ["Bailey", "James", ""], ["Lu", "Feng", ""]]}, {"id": "2007.02355", "submitter": "Nermin Samet", "authors": "Nermin Samet, Samet Hicsonmez, Emre Akbas", "title": "HoughNet: Integrating near and long-range evidence for bottom-up object\n  detection", "comments": "ECCV 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents HoughNet, a one-stage, anchor-free, voting-based,\nbottom-up object detection method. Inspired by the Generalized Hough Transform,\nHoughNet determines the presence of an object at a certain location by the sum\nof the votes cast on that location. Votes are collected from both near and\nlong-distance locations based on a log-polar vote field. Thanks to this voting\nmechanism, HoughNet is able to integrate both near and long-range,\nclass-conditional evidence for visual recognition, thereby generalizing and\nenhancing current object detection methodology, which typically relies on only\nlocal evidence. On the COCO dataset, HoughNet's best model achieves 46.4 $AP$\n(and 65.1 $AP_{50}$), performing on par with the state-of-the-art in bottom-up\nobject detection and outperforming most major one-stage and two-stage methods.\nWe further validate the effectiveness of our proposal in another task, namely,\n\"labels to photo\" image generation by integrating the voting module of HoughNet\nto two different GAN models and showing that the accuracy is significantly\nimproved in both cases. Code is available at\nhttps://github.com/nerminsamet/houghnet.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 14:45:01 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 10:37:21 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 07:12:13 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Samet", "Nermin", ""], ["Hicsonmez", "Samet", ""], ["Akbas", "Emre", ""]]}, {"id": "2007.02361", "submitter": "Fengbei Liu", "authors": "Fengbei Liu, Yaqub Jonmohamadi, Gabriel Maicas, Ajay K. Pandey,\n  Gustavo Carneiro", "title": "Self-supervised Depth Estimation to Regularise Semantic Segmentation in\n  Knee Arthroscopy", "comments": "10 pages, 6 figures", "journal-ref": "MICAAI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-operative automatic semantic segmentation of knee joint structures can\nassist surgeons during knee arthroscopy in terms of situational awareness.\nHowever, due to poor imaging conditions (e.g., low texture, overexposure,\netc.), automatic semantic segmentation is a challenging scenario, which\njustifies the scarce literature on this topic. In this paper, we propose a\nnovel self-supervised monocular depth estimation to regularise the training of\nthe semantic segmentation in knee arthroscopy. To further regularise the depth\nestimation, we propose the use of clean training images captured by the stereo\narthroscope of routine objects (presenting none of the poor imaging conditions\nand with rich texture information) to pre-train the model. We fine-tune such\nmodel to produce both the semantic segmentation and self-supervised monocular\ndepth using stereo arthroscopic images taken from inside the knee. Using a data\nset containing 3868 arthroscopic images captured during cadaveric knee\narthroscopy with semantic segmentation annotations, 2000 stereo image pairs of\ncadaveric knee arthroscopy, and 2150 stereo image pairs of routine objects, we\nshow that our semantic segmentation regularised by self-supervised depth\nestimation produces a more accurate segmentation than a state-of-the-art\nsemantic segmentation approach modeled exclusively with semantic segmentation\nannotation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 15:13:44 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Liu", "Fengbei", ""], ["Jonmohamadi", "Yaqub", ""], ["Maicas", "Gabriel", ""], ["Pandey", "Ajay K.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2007.02363", "submitter": "Wei Lian", "authors": "Wei Lian, WangMeng Zuo, Lei Zhang", "title": "Aligning Partially Overlapping Point Sets: an Inner Approximation\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning partially overlapping point sets where there is no prior information\nabout the value of the transformation is a challenging problem in computer\nvision. To achieve this goal, we first reduce the objective of the robust point\nmatching algorithm to a function of a low dimensional variable. The resulting\nfunction, however, is only concave over a finite region including the feasible\nregion. To cope with this issue, we employ the inner approximation optimization\nalgorithm which only operates within the region where the objective function is\nconcave. Our algorithm does not need regularization on transformation, and thus\ncan handle the situation where there is no prior information about the values\nof the transformations. Our method is also $\\epsilon-$globally optimal and thus\nis guaranteed to be robust. Moreover, its most computationally expensive\nsubroutine is a linear assignment problem which can be efficiently solved.\nExperimental results demonstrate the better robustness of the proposed method\nover state-of-the-art algorithms. Our method is also efficient when the number\nof transformation parameters is small.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 15:23:33 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lian", "Wei", ""], ["Zuo", "WangMeng", ""], ["Zhang", "Lei", ""]]}, {"id": "2007.02367", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom (Member, IEEE), Raj P. Kapur, TJ Browen, and Vijayan\n  K. Asari (Senior Member, IEEE)", "title": "GanglionNet: Objectively Assess the Density and Distribution of Ganglion\n  Cells With NABLA-N Network", "comments": "8 pages, 8 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hirschsprungs disease (HD) is a birth defect which is diagnosed and managed\nby multiple medical specialties such as pediatric gastroenterology, surgery,\nradiology, and pathology. HD is characterized by absence of ganglion cells in\nthe distal intestinal tract with a gradual normalization of ganglion cell\nnumbers in adjacent upstream bowel, termed as the transition zone (TZ).\nDefinitive surgical management to remove the abnormal bowel requires accurate\nassessment of ganglion cell density in histological sections from the TZ, which\nis difficult, time-consuming and prone to operator error. We present an\nautomated method to detect and count immunostained ganglion cells using a new\nNABLA_N network based deep learning (DL) approach, called GanglionNet. The\nmorphological image analysis methods are applied for refinement of the regions\nfor counting of the cells and define ganglia regions (a set of ganglion cells)\nfrom the predicted masks. The proposed model is trained with single point\nannotated samples by the expert pathologist. The GanglionNet is tested on ten\ncompletely new High Power Field (HPF) images with dimension of 2560x1920 pixels\nand the outputs are compared against the manual counting results by the expert\npathologist. The proposed method shows a robust 97.49% detection accuracy for\nganglion cells, when compared to counts by the expert pathologist, which\ndemonstrates the robustness of GanglionNet. The proposed DL based ganglion cell\ndetection and counting method will simplify and standardize TZ diagnosis for HD\npatients.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 15:46:13 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Alom", "Md Zahangir", "", "Member, IEEE"], ["Kapur", "Raj P.", "", "Senior Member, IEEE"], ["Browen", "TJ", "", "Senior Member, IEEE"], ["Asari", "Vijayan K.", "", "Senior Member, IEEE"]]}, {"id": "2007.02374", "submitter": "Wenxiao Zhang", "authors": "Wenxiao Zhang, Qingan Yan and Chunxia Xiao", "title": "Detail Preserved Point Cloud Completion via Separated Feature\n  Aggregation", "comments": "To be appeared in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud shape completion is a challenging problem in 3D vision and\nrobotics. Existing learning-based frameworks leverage encoder-decoder\narchitectures to recover the complete shape from a highly encoded global\nfeature vector. Though the global feature can approximately represent the\noverall shape of 3D objects, it would lead to the loss of shape details during\nthe completion process. In this work, instead of using a global feature to\nrecover the whole complete surface, we explore the functionality of multi-level\nfeatures and aggregate different features to represent the known part and the\nmissing part separately. We propose two different feature aggregation\nstrategies, named global \\& local feature aggregation(GLFA) and residual\nfeature aggregation(RFA), to express the two kinds of features and reconstruct\ncoordinates from their combination. In addition, we also design a refinement\ncomponent to prevent the generated point cloud from non-uniform distribution\nand outliers. Extensive experiments have been conducted on the ShapeNet\ndataset. Qualitative and quantitative evaluations demonstrate that our proposed\nnetwork outperforms current state-of-the art methods especially on detail\npreservation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:11:55 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Wenxiao", ""], ["Yan", "Qingan", ""], ["Xiao", "Chunxia", ""]]}, {"id": "2007.02375", "submitter": "Ting Yao", "authors": "Yingwei Pan and Yehao Li and Jianjie Luo and Jun Xu and Ting Yao and\n  Tao Mei", "title": "Auto-captions on GIF: A Large-scale Video-sentence Dataset for\n  Vision-language Pre-training", "comments": "The Auto-captions on GIF dataset is available at\n  \\url{http://www.auto-video-captions.top/2020/dataset}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present Auto-captions on GIF, which is a new large-scale\npre-training dataset for generic video understanding. All video-sentence pairs\nare created by automatically extracting and filtering video caption annotations\nfrom billions of web pages. Auto-captions on GIF dataset can be utilized to\npre-train the generic feature representation or encoder-decoder structure for\nvideo captioning, and other downstream tasks (e.g., sentence localization in\nvideos, video question answering, etc.) as well. We present a detailed analysis\nof Auto-captions on GIF dataset in comparison to existing video-sentence\ndatasets. We also provide an evaluation of a Transformer-based encoder-decoder\nstructure for vision-language pre-training, which is further adapted to video\ncaptioning downstream task and yields the compelling generalizability on\nMSR-VTT. The dataset is available at\n\\url{http://www.auto-video-captions.top/2020/dataset}.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:11:57 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Luo", "Jianjie", ""], ["Xu", "Jun", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "2007.02379", "submitter": "Baoquan Zhang", "authors": "Baoquan Zhang, Ka-Cheong Leung, Yunming Ye, Xutao Li", "title": "MetaConcept: Learn to Abstract via Concept Graph for Weakly-Supervised\n  Few-Shot Learning", "comments": "Accepted by Pattern Recognition 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning has been proved to be an effective framework to address\nfew-shot learning problems. The key challenge is how to minimize the\ngeneralization error of base learner across tasks. In this paper, we explore\nthe concept hierarchy knowledge by leveraging concept graph, and take the\nconcept graph as explicit meta-knowledge for the base learner, instead of\nlearning implicit meta-knowledge, so as to boost the classification performance\nof meta-learning on weakly-supervised few-shot learning problems. To this end,\nwe propose a novel meta-learning framework, called MetaConcept, which learns to\nabstract concepts via the concept graph. Specifically, we firstly propose a\nnovel regularization with multi-level conceptual abstraction to constrain a\nmeta-learner to learn to abstract concepts via the concept graph (i.e.\nidentifying the concepts from low to high levels). Then, we propose a meta\nconcept inference network as the meta-learner for the base learner, aiming to\nquickly adapt to a novel task by the joint inference of the abstract concepts\nand a few annotated samples. We have conducted extensive experiments on two\nweakly-supervised few-shot learning benchmarks, namely, WS-ImageNet-Pure and\nWS-ImageNet-Mix. Our experimental results show that 1) the proposed MetaConcept\noutperforms state-of-the-art methods with an improvement of 2% to 6% in\nclassification accuracy; 2) the proposed MetaConcept can be able to yield a\ngood performance though merely training with weakly-labeled data sets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:29:09 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 04:54:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhang", "Baoquan", ""], ["Leung", "Ka-Cheong", ""], ["Ye", "Yunming", ""], ["Li", "Xutao", ""]]}, {"id": "2007.02381", "submitter": "Charu Sharma", "authors": "Charu Sharma and Manohar Kaul", "title": "Simplicial Complex based Point Correspondence between Images warped onto\n  Manifolds", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent increase in the availability of warped images projected onto a\nmanifold (e.g., omnidirectional spherical images), coupled with the success of\nhigher-order assignment methods, has sparked an interest in the search for\nimproved higher-order matching algorithms on warped images due to projection.\nAlthough currently, several existing methods \"flatten\" such 3D images to use\nplanar graph / hypergraph matching methods, they still suffer from severe\ndistortions and other undesired artifacts, which result in inaccurate matching.\nAlternatively, current planar methods cannot be trivially extended to\neffectively match points on images warped onto manifolds. Hence, matching on\nthese warped images persists as a formidable challenge. In this paper, we pose\nthe assignment problem as finding a bijective map between two graph induced\nsimplicial complexes, which are higher-order analogues of graphs. We propose a\nconstrained quadratic assignment problem (QAP) that matches each p-skeleton of\nthe simplicial complexes, iterating from the highest to the lowest dimension.\nThe accuracy and robustness of our approach are illustrated on both synthetic\nand real-world spherical / warped (projected) images with known ground-truth\ncorrespondences. We significantly outperform existing state-of-the-art\nspherical matching methods on a diverse set of datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:41:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 21:06:40 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 15:12:41 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Sharma", "Charu", ""], ["Kaul", "Manohar", ""]]}, {"id": "2007.02388", "submitter": "Heming Zhang", "authors": "Heming Zhang, Xuewen Yang, Jianchao Tan, Chi-Hao Wu, Jue Wang, C.-C.\n  Jay Kuo", "title": "Learning Color Compatibility in Fashion Outfits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color compatibility is important for evaluating the compatibility of a\nfashion outfit, yet it was neglected in previous studies. We bring this\nimportant problem to researchers' attention and present a compatibility\nlearning framework as solution to various fashion tasks. The framework consists\nof a novel way to model outfit compatibility and an innovative learning scheme.\nSpecifically, we model the outfits as graphs and propose a novel graph\nconstruction to better utilize the power of graph neural networks. Then we\nutilize both ground-truth labels and pseudo labels to train the compatibility\nmodel in a weakly-supervised manner.Extensive experimental results verify the\nimportance of color compatibility alone with the effectiveness of our\nframework. With color information alone, our model's performance is already\ncomparable to previous methods that use deep image features. Our full model\ncombining the aforementioned contributions set the new state-of-the-art in\nfashion compatibility prediction.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:09:31 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Heming", ""], ["Yang", "Xuewen", ""], ["Tan", "Jianchao", ""], ["Wu", "Chi-Hao", ""], ["Wang", "Jue", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2007.02393", "submitter": "Seung-Hun Nam", "authors": "Seung-Hun Nam, Wonhyuk Ahn, In-Jae Yu, Myung-Joon Kwon, Minseok Son,\n  Heung-Kyu Lee", "title": "Deep Convolutional Neural Network for Identifying Seam-Carving Forgery", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2020.3037662", "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seam carving is a representative content-aware image retargeting approach to\nadjust the size of an image while preserving its visually prominent content. To\nmaintain visually important content, seam-carving algorithms first calculate\nthe connected path of pixels, referred to as the seam, according to a defined\ncost function and then adjust the size of an image by removing and duplicating\nrepeatedly calculated seams. Seam carving is actively exploited to overcome\ndiversity in the resolution of images between applications and devices; hence,\ndetecting the distortion caused by seam carving has become important in image\nforensics. In this paper, we propose a convolutional neural network (CNN)-based\napproach to classifying seam-carving-based image retargeting for reduction and\nexpansion. To attain the ability to learn low-level features, we designed a CNN\narchitecture comprising five types of network blocks specialized for capturing\nsubtle signals. An ensemble module is further adopted to both enhance\nperformance and comprehensively analyze the features in the local areas of the\ngiven image. To validate the effectiveness of our work, extensive experiments\nbased on various CNN-based baselines were conducted. Compared to the baselines,\nour work exhibits state-of-the-art performance in terms of three-class\nclassification (original, seam inserted, and seam removed). In addition, our\nmodel with the ensemble module is robust for various unseen cases. The\nexperimental results also demonstrate that our method can be applied to\nlocalize both seam-removed and seam-inserted areas.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:20:51 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:33:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nam", "Seung-Hun", ""], ["Ahn", "Wonhyuk", ""], ["Yu", "In-Jae", ""], ["Kwon", "Myung-Joon", ""], ["Son", "Minseok", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2007.02394", "submitter": "Yulin Wang", "authors": "Yulin Wang, Jiayi Guo, Shiji Song, Gao Huang", "title": "Meta-Semi: A Meta-learning Approach for Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based semi-supervised learning (SSL) algorithms have led to\npromising results in recent years. However, they tend to introduce multiple\ntunable hyper-parameters, making them less practical in real SSL scenarios\nwhere the labeled data is scarce for extensive hyper-parameter search. In this\npaper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that\nrequires tuning only one additional hyper-parameter, compared with a standard\nsupervised deep learning algorithm, to achieve competitive performance under\nvarious conditions of SSL. We start by defining a meta optimization problem\nthat minimizes the loss on labeled data through dynamically reweighting the\nloss on unlabeled samples, which are associated with soft pseudo labels during\ntraining. As the meta problem is computationally intensive to solve directly,\nwe propose an efficient algorithm to dynamically obtain the approximate\nsolutions. We show theoretically that Meta-Semi converges to the stationary\npoint of the loss function on labeled data under mild conditions. Empirically,\nMeta-Semi outperforms state-of-the-art SSL algorithms significantly on the\nchallenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves\ncompetitive performance on CIFAR-10 and SVHN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:31:14 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wang", "Yulin", ""], ["Guo", "Jiayi", ""], ["Song", "Shiji", ""], ["Huang", "Gao", ""]]}, {"id": "2007.02419", "submitter": "Keval Morabia", "authors": "Keval Morabia, Jatin Arora, Tara Vijaykumar", "title": "Attention-based Joint Detection of Object and Semantic Part", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of joint detection of objects like dog\nand its semantic parts like face, leg, etc. Our model is created on top of two\nFaster-RCNN models that share their features to perform a novel Attention-based\nfeature fusion of related Object and Part features to get enhanced\nrepresentations of both. These representations are used for final\nclassification and bounding box regression separately for both models. Our\nexperiments on the PASCAL-Part 2010 dataset show that joint detection can\nsimultaneously improve both object detection and part detection in terms of\nmean Average Precision (mAP) at IoU=0.5.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 18:54:10 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Morabia", "Keval", ""], ["Arora", "Jatin", ""], ["Vijaykumar", "Tara", ""]]}, {"id": "2007.02424", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Shijian Lu, Dayan Guan, and Xiaobing Zhang", "title": "Contextual-Relation Consistent Domain Adaptation for Semantic\n  Segmentation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in unsupervised domain adaptation for semantic segmentation\nhave shown great potentials to relieve the demand of expensive per-pixel\nannotations. However, most existing works address the domain discrepancy by\naligning the data distributions of two domains at a global image level whereas\nthe local consistencies are largely neglected. This paper presents an\ninnovative local contextual-relation consistent domain adaptation (CrCDA)\ntechnique that aims to achieve local-level consistencies during the\nglobal-level alignment. The idea is to take a closer look at region-wise\nfeature representations and align them for local-level consistencies.\nSpecifically, CrCDA learns and enforces the prototypical local\ncontextual-relations explicitly in the feature space of a labelled source\ndomain while transferring them to an unlabelled target domain via\nbackpropagation-based adversarial learning. An adaptive entropy max-min\nadversarial learning scheme is designed to optimally align these hundreds of\nlocal contextual-relations across domain without requiring discriminator or\nextra computation overhead. The proposed CrCDA has been evaluated extensively\nover two challenging domain adaptive segmentation tasks (e.g., GTA5 to\nCityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior\nsegmentation performance as compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 19:00:46 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 12:22:36 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Huang", "Jiaxing", ""], ["Lu", "Shijian", ""], ["Guan", "Dayan", ""], ["Zhang", "Xiaobing", ""]]}, {"id": "2007.02433", "submitter": "Muhammad Junaid Umer", "authors": "Muhammad Junaid Umer and Qaiser Riaz", "title": "Estimation of Ground Contacts from Human Gait by a Wearable Inertial\n  Measurement Unit using machine learning", "comments": "Not completely discussed with supervisor need some improvements in\n  article to prepare a final draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotics system for rehabilitation of movement disorders and motion\nassistance are gaining increased intention. In this scenario estimation of\nground contact is an active area of research in robotics and healthcare. This\narticle addresses the estimation and classification of right and left foot\nduring the healthy human gait based on the IMU sensor data of chest and lower\nback. For this purpose we have collected an IMU data of 48 subjects by using\ntwo smartphones at chest and lower back of the human body and one smart watch\nat right ankle of the body. To show the robustness of our approach data was\ncollected at six different surfaces (road tiles carpet grass concrete and\nsoil). The recorded data of lower back and chest sensor was segmented into\nsingle steps on the basis of right ankle sensor data, then we computed a total\nof 408 features from time frequency and wavelet domain of each segmented step.\nFor classification task we have trained two machine learning classifiers SVM\nand RF with 10 fold cross validation method. We performed classification\nexperiments at individual surfaces, hard surfaces, soft surfaces and all\nsurfaces, highest accuracy was achieved at individual surfaces with accuracy\nindex of 98.88%. Furthermore, classification rate at hard soft and all surface\nare 95.60%, 94.38% and 95.05% respectively. The results shows that estimation\nof ground contact form normal human walk at different surfaces can be performed\nwith high accuracy using 6D data of angular velocities and accelerations from\nchest and lower back location of the body.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 19:47:06 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 18:05:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Umer", "Muhammad Junaid", ""], ["Riaz", "Qaiser", ""]]}, {"id": "2007.02442", "submitter": "Katja Schwarz", "authors": "Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger", "title": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While 2D generative adversarial networks have enabled high-resolution image\nsynthesis, they largely lack an understanding of the 3D world and the image\nformation process. Thus, they do not provide precise control over camera\nviewpoint or object pose. To address this problem, several recent approaches\nleverage intermediate voxel-based representations in combination with\ndifferentiable rendering. However, existing methods either produce low image\nresolution or fall short in disentangling camera and scene properties, e.g.,\nthe object identity may vary with the viewpoint. In this paper, we propose a\ngenerative model for radiance fields which have recently proven successful for\nnovel view synthesis of a single scene. In contrast to voxel-based\nrepresentations, radiance fields are not confined to a coarse discretization of\nthe 3D space, yet allow for disentangling camera and scene properties while\ndegrading gracefully in the presence of reconstruction ambiguity. By\nintroducing a multi-scale patch-based discriminator, we demonstrate synthesis\nof high-resolution images while training our model from unposed 2D images\nalone. We systematically analyze our approach on several challenging synthetic\nand real-world datasets. Our experiments reveal that radiance fields are a\npowerful representation for generative image synthesis, leading to 3D\nconsistent models that render with high fidelity.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 20:37:39 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 18:04:29 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 21:22:49 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 11:33:35 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Schwarz", "Katja", ""], ["Liao", "Yiyi", ""], ["Niemeyer", "Michael", ""], ["Geiger", "Andreas", ""]]}, {"id": "2007.02443", "submitter": "Jary Pomponi", "authors": "Jary Pomponi, Simone Scardapane, Aurelio Uncini", "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems. A preliminary unpublished version of this work was presented in the\n  LifelongML workshop, at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 20:43:52 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 17:33:26 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 09:53:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pomponi", "Jary", ""], ["Scardapane", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "2007.02447", "submitter": "Zhengyang Shen", "authors": "Zhengyang Shen, Zhenlin Xu, Sahin Olut, Marc Niethammer", "title": "Anatomical Data Augmentation via Fluid-based Image Registration", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fluid-based image augmentation method for medical image\nanalysis. In contrast to existing methods, our framework generates anatomically\nmeaningful images via interpolation from the geodesic subspace underlying given\nsamples. Our approach consists of three steps: 1) given a source image and a\nset of target images, we construct a geodesic subspace using the Large\nDeformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample\ntransformations from the resulting geodesic subspace; 3) we obtain deformed\nimages and segmentations via interpolation. Experiments on brain (LPBA) and\nknee (OAI) data illustrate the performance of our approach on two tasks: 1)\ndata augmentation during training and testing for image segmentation; 2)\none-shot learning for single atlas image segmentation. We demonstrate that our\napproach generates anatomically meaningful data and improves performance on\nthese tasks over competing approaches. Code is available at\nhttps://github.com/uncbiag/easyreg.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 21:06:00 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Shen", "Zhengyang", ""], ["Xu", "Zhenlin", ""], ["Olut", "Sahin", ""], ["Niethammer", "Marc", ""]]}, {"id": "2007.02454", "submitter": "Zeyi Huang Mr", "authors": "Zeyi Huang and Haohan Wang and Eric P. Xing and Dong Huang", "title": "Self-Challenging Improves Cross-Domain Generalization", "comments": "to appear at ECCV2020 as an oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) conduct image classification by\nactivating dominant features that correlated with labels. When the training and\ntesting data are under similar distributions, their dominant features are\nsimilar, which usually facilitates decent performance on the testing data. The\nperformance is nonetheless unmet when tested on samples from different\ndistributions, leading to the challenges in cross-domain image classification.\nWe introduce a simple training heuristic, Representation Self-Challenging\n(RSC), that significantly improves the generalization of CNN to the\nout-of-domain data. RSC iteratively challenges (discards) the dominant features\nactivated on the training data, and forces the network to activate remaining\nfeatures that correlates with labels. This process appears to activate feature\nrepresentations applicable to out-of-domain data without prior knowledge of new\ndomain and without learning extra network parameters. We present theoretical\nproperties and conditions of RSC for improving cross-domain generalization. The\nexperiments endorse the simple, effective and architecture-agnostic nature of\nour RSC method.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 21:42:26 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Huang", "Zeyi", ""], ["Wang", "Haohan", ""], ["Xing", "Eric P.", ""], ["Huang", "Dong", ""]]}, {"id": "2007.02457", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez-Fern\\'andez, Lamberto Ballan, Gabriel\n  Jim\\'enez-Avalos, Jorge Coronel, Mirko Zimic", "title": "Using Capsule Neural Network to predict Tuberculosis in lens-free\n  microscopic images", "comments": "HSYS Workshop at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis, caused by a bacteria called Mycobacterium tuberculosis, is one\nof the most serious public health problems worldwide. This work seeks to\nfacilitate and automate the prediction of tuberculosis by the MODS method and\nusing lens-free microscopy, which is easy to use by untrained personnel. We\nemploy the CapsNet architecture in our collected dataset and show that it has a\nbetter accuracy than traditional CNN architectures.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 22:18:13 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""], ["Ballan", "Lamberto", ""], ["Jim\u00e9nez-Avalos", "Gabriel", ""], ["Coronel", "Jorge", ""], ["Zimic", "Mirko", ""]]}, {"id": "2007.02471", "submitter": "Mohammad Zalbagi Darestani", "authors": "Mohammad Zalbagi Darestani and Reinhard Heckel", "title": "Accelerated MRI with Un-trained Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are highly effective for image\nreconstruction problems. Typically, CNNs are trained on large amounts of\ntraining images. Recently, however, un-trained CNNs such as the Deep Image\nPrior and Deep Decoder have achieved excellent performance for image\nreconstruction problems such as denoising and inpainting, \\emph{without using\nany training data}. Motivated by this development, we address the\nreconstruction problem arising in accelerated MRI with un-trained neural\nnetworks. We propose a highly optimized un-trained recovery approach based on a\nvariation of the Deep Decoder and show that it significantly outperforms other\nun-trained methods, in particular sparsity-based classical compressed sensing\nmethods and naive applications of un-trained neural networks. We also compare\nperformance (both in terms of reconstruction accuracy and computational cost)\nin an ideal setup for trained methods, specifically on the fastMRI dataset,\nwhere the training and test data come from the same distribution. We find that\nour un-trained algorithm achieves similar performance to a baseline trained\nneural network, but a state-of-the-art trained network outperforms the\nun-trained one. Finally, we perform a comparison on a non-ideal setup where the\ntrain and test distributions are slightly different, and find that our\nun-trained method achieves similar performance to a state-of-the-art\naccelerated MRI reconstruction method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:01:25 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 23:06:26 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 18:23:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Darestani", "Mohammad Zalbagi", ""], ["Heckel", "Reinhard", ""]]}, {"id": "2007.02482", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez-Fern\\'andez, Lamberto Ballan, Gabriel\n  Jim\\'enez-Avalos, Jorge Coronel, Mirko Zimic", "title": "Automatic semantic segmentation for prediction of tuberculosis using\n  lens-free microscopy images", "comments": "ML for Global Health Workshop at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis (TB), caused by a germ called Mycobacterium tuberculosis, is one\nof the most serious public health problems in Peru and the world. The\ndevelopment of this project seeks to facilitate and automate the diagnosis of\ntuberculosis by the MODS method and using lens-free microscopy, due they are\neasier to calibrate and easier to use (by untrained personnel) in comparison\nwith lens microscopy. Thus, we employ a U-Net network in our collected dataset\nto perform the automatic segmentation of the TB cords in order to predict\ntuberculosis. Our initial results show promising evidence for automatic\nsegmentation of TB cords.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:36:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""], ["Ballan", "Lamberto", ""], ["Jim\u00e9nez-Avalos", "Gabriel", ""], ["Coronel", "Jorge", ""], ["Zimic", "Mirko", ""]]}, {"id": "2007.02491", "submitter": "Bailin Li", "authors": "Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang and Liang Lin", "title": "EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning", "comments": "Accepted in ECCV 2020(Oral). Codes are available on\n  https://github.com/anonymous47823493/EagleEye", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding out the computational redundant part of a trained Deep Neural Network\n(DNN) is the key question that pruning algorithms target on. Many algorithms\ntry to predict model performance of the pruned sub-nets by introducing various\nevaluation methods. But they are either inaccurate or very complicated for\ngeneral application. In this work, we present a pruning method called EagleEye,\nin which a simple yet efficient evaluation component based on adaptive batch\nnormalization is applied to unveil a strong correlation between different\npruned DNN structures and their final settled accuracy. This strong correlation\nallows us to fast spot the pruned candidates with highest potential accuracy\nwithout actually fine-tuning them. This module is also general to plug-in and\nimprove some existing pruning algorithms. EagleEye achieves better pruning\nperformance than all of the studied pruning algorithms in our experiments.\nConcretely, to prune MobileNet V1 and ResNet-50, EagleEye outperforms all\ncompared methods by up to 3.8%. Even in the more challenging experiments of\npruning the compact model of MobileNet V1, EagleEye achieves the highest\naccuracy of 70.9% with an overall 50% operations (FLOPs) pruned. All accuracy\nresults are Top-1 ImageNet classification accuracy. Source code and models are\naccessible to open-source community\nhttps://github.com/anonymous47823493/EagleEye .\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 01:32:31 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 09:32:58 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Li", "Bailin", ""], ["Wu", "Bowen", ""], ["Su", "Jiang", ""], ["Wang", "Guangrun", ""], ["Lin", "Liang", ""]]}, {"id": "2007.02500", "submitter": "Guansong Pang", "authors": "Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel", "title": "Deep Learning for Anomaly Detection: A Review", "comments": "Survey paper, 36 pages, 180 references, 2 figures, 4 tables", "journal-ref": "ACM Computing Surveys, 2020", "doi": "10.1145/3439950", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection, a.k.a. outlier detection or novelty detection, has been a\nlasting yet active research area in various research communities for several\ndecades. There are still some unique problem complexities and challenges that\nrequire advanced approaches. In recent years, deep learning enabled anomaly\ndetection, i.e., deep anomaly detection, has emerged as a critical direction.\nThis paper surveys the research of deep anomaly detection with a comprehensive\ntaxonomy, covering advancements in three high-level categories and 11\nfine-grained categories of the methods. We review their key intuitions,\nobjective functions, underlying assumptions, advantages and disadvantages, and\ndiscuss how they address the aforementioned challenges. We further discuss a\nset of possible future opportunities and new perspectives on addressing the\nchallenges.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 02:21:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 01:51:13 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 04:53:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pang", "Guansong", ""], ["Shen", "Chunhua", ""], ["Cao", "Longbing", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2007.02501", "submitter": "Zixu Zhao", "authors": "Zixu Zhao, Yueming Jin, Xiaojie Gao, Qi Dou, Pheng-Ann Heng", "title": "Learning Motion Flows for Semi-supervised Instrument Segmentation from\n  Robotic Surgical Video", "comments": "Accepted for MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing low hertz labeling for surgical videos at intervals can greatly\nreleases the burden of surgeons. In this paper, we study the semi-supervised\ninstrument segmentation from robotic surgical videos with sparse annotations.\nUnlike most previous methods using unlabeled frames individually, we propose a\ndual motion based method to wisely learn motion flows for segmentation\nenhancement by leveraging temporal dynamics. We firstly design a flow predictor\nto derive the motion for jointly propagating the frame-label pairs given the\ncurrent labeled frame. Considering the fast instrument motion, we further\nintroduce a flow compensator to estimate intermediate motion within continuous\nframes, with a novel cycle learning strategy. By exploiting generated data\npairs, our framework can recover and even enhance temporal consistency of\ntraining sequences to benefit segmentation. We validate our framework with\nbinary, part, and type tasks on 2017 MICCAI EndoVis Robotic Instrument\nSegmentation Challenge dataset. Results show that our method outperforms the\nstate-of-the-art semi-supervised methods by a large margin, and even exceeds\nfully supervised training on two tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 02:39:32 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhao", "Zixu", ""], ["Jin", "Yueming", ""], ["Gao", "Xiaojie", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2007.02503", "submitter": "Xun Yang", "authors": "Xun Yang, Jianfeng Dong, Yixin Cao, Xun Wang, Meng Wang, Tat-Seng Chua", "title": "Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval", "comments": "Accepted For 43rd International ACM SIGIR Conference on Research and\n  Development in Information Retrieval (SIGIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of user-generated videos on the Internet has intensified the\nneed for text-based video retrieval systems. Traditional methods mainly favor\nthe concept-based paradigm on retrieval with simple queries, which are usually\nineffective for complex queries that carry far more complex semantics.\nRecently, embedding-based paradigm has emerged as a popular approach. It aims\nto map the queries and videos into a shared embedding space where\nsemantically-similar texts and videos are much closer to each other. Despite\nits simplicity, it forgoes the exploitation of the syntactic structure of text\nqueries, making it suboptimal to model the complex queries.\n  To facilitate video retrieval with complex queries, we propose a\nTree-augmented Cross-modal Encoding method by jointly learning the linguistic\nstructure of queries and the temporal representation of videos. Specifically,\ngiven a complex user query, we first recursively compose a latent semantic tree\nto structurally describe the text query. We then design a tree-augmented query\nencoder to derive structure-aware query representation and a temporal attentive\nvideo encoder to model the temporal characteristics of videos. Finally, both\nthe query and videos are mapped into a joint embedding space for matching and\nranking. In this approach, we have a better understanding and modeling of the\ncomplex queries, thereby achieving a better video retrieval performance.\nExtensive experiments on large scale video retrieval benchmark datasets\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 02:50:27 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yang", "Xun", ""], ["Dong", "Jianfeng", ""], ["Cao", "Yixin", ""], ["Wang", "Xun", ""], ["Wang", "Meng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2007.02515", "submitter": "Tao Yang", "authors": "Tao Yang, Zhixiong Nan, He Zhang, Shitao Chen and Nanning Zheng", "title": "Traffic Agent Trajectory Prediction Using Social Convolution and\n  Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trajectory prediction is significant for the decision-making of\nautonomous driving vehicles. In this paper, we propose a model to predict the\ntrajectories of target agents around an autonomous vehicle. The main idea of\nour method is considering the history trajectories of the target agent and the\ninfluence of surrounding agents on the target agent. To this end, we encode the\ntarget agent history trajectories as an attention mask and construct a social\nmap to encode the interactive relationship between the target agent and its\nsurrounding agents. Given a trajectory sequence, the LSTM networks are firstly\nutilized to extract the features for all agents, based on which the attention\nmask and social map are formed. Then, the attention mask and social map are\nfused to get the fusion feature map, which is processed by the social\nconvolution to obtain a fusion feature representation. Finally, this fusion\nfeature is taken as the input of a variable-length LSTM to predict the\ntrajectory of the target agent. We note that the variable-length LSTM enables\nour model to handle the case that the number of agents in the sensing scope is\nhighly dynamic in traffic scenes. To verify the effectiveness of our method, we\nwidely compare with several methods on a public dataset, achieving a 20% error\ndecrease. In addition, the model satisfies the real-time requirement with the\n32 fps.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:48:08 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Yang", "Tao", ""], ["Nan", "Zhixiong", ""], ["Zhang", "He", ""], ["Chen", "Shitao", ""], ["Zheng", "Nanning", ""]]}, {"id": "2007.02517", "submitter": "Yingnan Fu", "authors": "Yingnan Fu, Tingting Liu, Ming Gao, Aoying Zhou", "title": "EDSL: An Encoder-Decoder Architecture with Symbol-Level Features for\n  Printed Mathematical Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Printed Mathematical expression recognition (PMER) aims to transcribe a\nprinted mathematical expression image into a structural expression, such as\nLaTeX expression. It is a crucial task for many applications, including\nautomatic question recommendation, automatic problem solving and analysis of\nthe students, etc. Currently, the mainstream solutions rely on solving image\ncaptioning tasks, all addressing image summarization. As such, these methods\ncan be suboptimal for solving MER problem.\n  In this paper, we propose a new method named EDSL, shorted for\nencoder-decoder with symbol-level features, to identify the printed\nmathematical expressions from images. The symbol-level image encoder of EDSL\nconsists of segmentation module and reconstruction module. By performing\nsegmentation module, we identify all the symbols and their spatial information\nfrom images in an unsupervised manner. We then design a novel reconstruction\nmodule to recover the symbol dependencies after symbol segmentation.\nEspecially, we employ a position correction attention mechanism to capture the\nspatial relationships between symbols. To alleviate the negative impact from\nlong output, we apply the transformer model for transcribing the encoded image\ninto the sequential and structural output. We conduct extensive experiments on\ntwo real datasets to verify the effectiveness and rationality of our proposed\nEDSL method. The experimental results have illustrated that EDSL has achieved\n92.7\\% and 89.0\\% in evaluation metric Match, which are 3.47\\% and 4.04\\%\nhigher than the state-of-the-art method. Our code and datasets are available at\nhttps://github.com/abcAnonymous/EDSL .\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:53:52 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fu", "Yingnan", ""], ["Liu", "Tingting", ""], ["Gao", "Ming", ""], ["Zhou", "Aoying", ""]]}, {"id": "2007.02519", "submitter": "Aditya Kusupati", "authors": "Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron\n  Walsman, Aniruddha Kembhavi, Ali Farhadi", "title": "Are We Overfitting to Experimental Setups in Recognition?", "comments": "17 pages, 6 figures. Project page:\n  https://raivn.cs.washington.edu/projects/FLUID/ More baselines and insights", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling robust intelligence in the real-world entails systems that offer\ncontinuous inference while learning from varying amounts of data and\nsupervision. The machine learning community has organically broken down this\nchallenging goal into manageable sub-tasks such as supervised, few-shot, and\ncontinual learning. In light of substantial progress on each sub-task, we pose\nthe question, \"How well does this progress translate to more practical\nscenarios?\" To investigate this question, we construct a new framework, FLUID,\nwhich removes certain assumptions made by current experimental setups while\nintegrating these sub-tasks via the following design choices -- consuming\nsequential data, allowing for flexible training phases, being compute aware,\nand working in an open-world setting. Evaluating a broad set of methods on\nFLUID leads to new insights including strong evidence that methods are\noverfitting to their experimental setup. For example, we find that\nrepresentative few-shot methods are substantially worse than simple baselines,\nself-supervised representations from MoCo fail to learn new classes when the\ndownstream task contains a mix of new and old classes, and pretraining largely\nmitigates the problem of catastrophic forgetting. Finally, we propose two new\nsimple methods which outperform all other evaluated methods which further\nquestions our progress towards robust, real-world systems. Project page:\nhttps://raivn.cs.washington.edu/projects/FLUID/.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 04:31:54 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 04:17:35 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 22:41:28 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 05:34:25 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wallingford", "Matthew", ""], ["Kusupati", "Aditya", ""], ["Alizadeh-Vahid", "Keivan", ""], ["Walsman", "Aaron", ""], ["Kembhavi", "Aniruddha", ""], ["Farhadi", "Ali", ""]]}, {"id": "2007.02571", "submitter": "Albert Matveev", "authors": "Albert Matveev, Alexey Artemov, Denis Zorin and Evgeny Burnaev", "title": "Geometric Attention for Prediction of Differential Properties in 3D\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of differential geometric quantities in discrete 3D data\nrepresentations is one of the crucial steps in the geometry processing\npipeline. Specifically, estimating normals and sharp feature lines from raw\npoint cloud helps improve meshing quality and allows us to use more precise\nsurface reconstruction techniques. When designing a learnable approach to such\nproblems, the main difficulty is selecting neighborhoods in a point cloud and\nincorporating geometric relations between the points. In this study, we present\na geometric attention mechanism that can provide such properties in a learnable\nfashion. We establish the usefulness of the proposed technique with several\nexperiments on the prediction of normal vectors and the extraction of feature\nlines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 07:40:26 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 16:36:49 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 09:10:21 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Matveev", "Albert", ""], ["Artemov", "Alexey", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2007.02574", "submitter": "Chenxu Luo", "authors": "Chenxu Luo, Lin Sun, Dariush Dabiri, Alan Yuille", "title": "Probabilistic Multi-modal Trajectory Prediction with Lane Attention for\n  Autonomous Vehicles", "comments": "IROS 2020. 3rd place solution for the Argoverse motion forecasting\n  challenge at NeurIPS 2019. A variation of the method also won the 1st place\n  in the Nuscenes prediction challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is crucial for autonomous vehicles. The planning system\nnot only needs to know the current state of the surrounding objects but also\ntheir possible states in the future. As for vehicles, their trajectories are\nsignificantly influenced by the lane geometry and how to effectively use the\nlane information is of active interest. Most of the existing works use\nrasterized maps to explore road information, which does not distinguish\ndifferent lanes. In this paper, we propose a novel instance-aware\nrepresentation for lane representation. By integrating the lane features and\ntrajectory features, a goal-oriented lane attention module is proposed to\npredict the future locations of the vehicle. We show that the proposed lane\nrepresentation together with the lane attention module can be integrated into\nthe widely used encoder-decoder framework to generate diverse predictions. Most\nimportantly, each generated trajectory is associated with a probability to\nhandle the uncertainty. Our method does not suffer from collapsing to one\nbehavior modal and can cover diverse possibilities. Extensive experiments and\nablation studies on the benchmark datasets corroborate the effectiveness of our\nproposed method. Notably, our proposed method ranks third place in the\nArgoverse motion forecasting competition at NeurIPS 2019.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 07:57:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Luo", "Chenxu", ""], ["Sun", "Lin", ""], ["Dabiri", "Dariush", ""], ["Yuille", "Alan", ""]]}, {"id": "2007.02577", "submitter": "Yifei Zhang", "authors": "Yifei Zhang, Chang Liu, Yu Zhou, Wei Wang, Weiping Wang and Qixiang Ye", "title": "Progressive Cluster Purification for Unsupervised Feature Learning", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised feature learning, sample specificity based methods ignore the\ninter-class information, which deteriorates the discriminative capability of\nrepresentation models. Clustering based methods are error-prone to explore the\ncomplete class boundary information due to the inevitable class inconsistent\nsamples in each cluster. In this work, we propose a novel clustering based\nmethod, which, by iteratively excluding class inconsistent samples during\nprogressive cluster formation, alleviates the impact of noise samples in a\nsimple-yet-effective manner. Our approach, referred to as Progressive Cluster\nPurification (PCP), implements progressive clustering by gradually reducing the\nnumber of clusters during training, while the sizes of clusters continuously\nexpand consistently with the growth of model representation capability. With a\nwell-designed cluster purification mechanism, it further purifies clusters by\nfiltering noise samples which facilitate the subsequent feature learning by\nutilizing the refined clusters as pseudo-labels. Experiments on commonly used\nbenchmarks demonstrate that the proposed PCP improves baseline method with\nsignificant margins. Our code will be available at\nhttps://github.com/zhangyifei0115/PCP.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 08:11:03 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 17:11:45 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Yifei", ""], ["Liu", "Chang", ""], ["Zhou", "Yu", ""], ["Wang", "Wei", ""], ["Wang", "Weiping", ""], ["Ye", "Qixiang", ""]]}, {"id": "2007.02578", "submitter": "Diego Valsesia", "authors": "Francesca Pistilli, Giulia Fracastoro, Diego Valsesia, Enrico Magli", "title": "Learning Graph-Convolutional Representations for Point Cloud Denoising", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are an increasingly relevant data type but they are often\ncorrupted by noise. We propose a deep neural network based on\ngraph-convolutional layers that can elegantly deal with the\npermutation-invariance problem encountered by learning-based point cloud\nprocessing methods. The network is fully-convolutional and can build complex\nhierarchies of features by dynamically constructing neighborhood graphs from\nsimilarity among the high-dimensional feature representations of the points.\nWhen coupled with a loss promoting proximity to the ideal surface, the proposed\napproach significantly outperforms state-of-the-art methods on a variety of\nmetrics. In particular, it is able to improve in terms of Chamfer measure and\nof quality of the surface normals that can be estimated from the denoised data.\nWe also show that it is especially robust both at high noise levels and in\npresence of structured noise such as the one encountered in real LiDAR scans.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 08:11:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pistilli", "Francesca", ""], ["Fracastoro", "Giulia", ""], ["Valsesia", "Diego", ""], ["Magli", "Enrico", ""]]}, {"id": "2007.02587", "submitter": "Roland Haase", "authors": "Thomas Vogt, Roland Haase, Danielle Bednarski, Jan Lellmann", "title": "On the Connection between Dynamical Optimal Transport and Functional\n  Lifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional lifting methods provide a tool for approximating solutions of\ndifficult non-convex problems by embedding them into a larger space. In this\nwork, we investigate a mathematically rigorous formulation based on embedding\ninto the space of pointwise probability measures over a fixed range $\\Gamma$.\nInterestingly, this approach can be derived as a generalization of the theory\nof dynamical optimal transport. Imposing the established continuity equation as\na constraint corresponds to variational models with first-order regularization.\nBy modifying the continuity equation, the approach can also be extended to\nmodels with higher-order regularization.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 08:53:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Vogt", "Thomas", ""], ["Haase", "Roland", ""], ["Bednarski", "Danielle", ""], ["Lellmann", "Jan", ""]]}, {"id": "2007.02595", "submitter": "Zhanzhan Cheng", "authors": "Sanli Tang, Zhanzhan Cheng, Shiliang Pu, Dashan Guo, Yi Niu and Fei Wu", "title": "Learning a Domain Classifier Bank for Unsupervised Adaptive Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In real applications, object detectors based on deep networks still face\nchallenges of the large domain gap between the labeled training data and\nunlabeled testing data. To reduce the gap, recent techniques are proposed by\naligning the image/instance-level features between source and unlabeled target\ndomains. However, these methods suffer from the suboptimal problem mainly\nbecause of ignoring the category information of object instances. To tackle\nthis issue, we develop a fine-grained domain alignment approach with a\nwell-designed domain classifier bank that achieves the instance-level alignment\nrespecting to their categories. Specifically, we first employ the mean teacher\nparadigm to generate pseudo labels for unlabeled samples. Then we implement the\nclass-level domain classifiers and group them together, called domain\nclassifier bank, in which each domain classifier is responsible for aligning\nfeatures of a specific class. We assemble the bare object detector with the\nproposed fine-grained domain alignment mechanism as the adaptive detector, and\noptimize it with a developed crossed adaptive weighting mechanism. Extensive\nexperiments on three popular transferring benchmarks demonstrate the\neffectiveness of our method and achieve the new remarkable state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:12:46 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Tang", "Sanli", ""], ["Cheng", "Zhanzhan", ""], ["Pu", "Shiliang", ""], ["Guo", "Dashan", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2007.02606", "submitter": "Rhydian Windsor", "authors": "Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman", "title": "A Convolutional Approach to Vertebrae Detection and Labelling in Whole\n  Spine MRI", "comments": "Accepted full paper to Medical Image Computing and Computer Assisted\n  Intervention 2020. 11 pages plus appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a novel convolutional method for the detection and identification\nof vertebrae in whole spine MRIs. This involves using a learnt vector field to\ngroup detected vertebrae corners together into individual vertebral bodies and\nconvolutional image-to-image translation followed by beam search to label\nvertebral levels in a self-consistent manner. The method can be applied without\nmodification to lumbar, cervical and thoracic-only scans across a range of\ndifferent MR sequences. The resulting system achieves 98.1% detection rate and\n96.5% identification rate on a challenging clinical dataset of whole spine\nscans and matches or exceeds the performance of previous systems on lumbar-only\nscans. Finally, we demonstrate the clinical applicability of this method, using\nit for automated scoliosis detection in both lumbar and whole spine MR scans.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:37:12 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 13:46:56 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 13:10:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Windsor", "Rhydian", ""], ["Jamaludin", "Amir", ""], ["Kadir", "Timor", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2007.02617", "submitter": "Maksym Andriushchenko", "authors": "Maksym Andriushchenko, Nicolas Flammarion", "title": "Understanding and Improving Fast Adversarial Training", "comments": "The camera-ready version (accepted at NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work focused on making adversarial training computationally\nefficient for deep learning models. In particular, Wong et al. (2020) showed\nthat $\\ell_\\infty$-adversarial training with fast gradient sign method (FGSM)\ncan fail due to a phenomenon called \"catastrophic overfitting\", when the model\nquickly loses its robustness over a single epoch of training. We show that\nadding a random step to FGSM, as proposed in Wong et al. (2020), does not\nprevent catastrophic overfitting, and that randomness is not important per se\n-- its main role being simply to reduce the magnitude of the perturbation.\nMoreover, we show that catastrophic overfitting is not inherent to deep and\noverparametrized networks, but can occur in a single-layer convolutional\nnetwork with a few filters. In an extreme case, even a single filter can make\nthe network highly non-linear locally, which is the main reason why FGSM\ntraining fails. Based on this observation, we propose a new regularization\nmethod, GradAlign, that prevents catastrophic overfitting by explicitly\nmaximizing the gradient alignment inside the perturbation set and improves the\nquality of the FGSM solution. As a result, GradAlign allows to successfully\napply FGSM training also for larger $\\ell_\\infty$-perturbations and reduce the\ngap to multi-step adversarial training. The code of our experiments is\navailable at https://github.com/tml-epfl/understanding-fast-adv-training.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 10:16:43 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:20:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Andriushchenko", "Maksym", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2007.02622", "submitter": "Albert Bou", "authors": "Albert Bou and Gianni De Fabritiis", "title": "PyTorchRL: Modular and Distributed Reinforcement Learning in PyTorch", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has proved successful at solving challenging\nenvironments but often requires scaling to large sampling and computing\nresources. Furthermore, advancing RL requires tools that are flexible enough to\neasily prototype new methods, yet avoiding impractically slow experimental\nturnaround times. To this end, we present PyTorchRL, a PyTorch-based library\nfor RL with a modular design that allows composing agents from a set of\nreusable and easily extendable modules. Additionally, PyTorchRL permits the\ndefinition of distributed training architectures with flexibility and\nindependence of the Agent components. In combination, these two features can\naccelerate the pace at which ideas are implemented and tested, simplifying\nresearch and enabling to tackle more challenging RL problems. We present\nseveral interesting use-cases of PyTorchRL and showcase the library by\nobtaining the highest to-date test performance on the Obstacle Tower Unity3D\nchallenge environment.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 10:22:07 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 16:39:37 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bou", "Albert", ""], ["De Fabritiis", "Gianni", ""]]}, {"id": "2007.02632", "submitter": "Mahsa Ehsanpour", "authors": "Mahsa Ehsanpour, Alireza Abedin, Fatemeh Saleh, Javen Shi, Ian Reid,\n  Hamid Rezatofighi", "title": "Joint Learning of Social Groups, Individuals Action and Sub-group\n  Activities in Videos", "comments": "Accepted in the European Conference On Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the art solutions for human activity understanding from a video\nstream formulate the task as a spatio-temporal problem which requires joint\nlocalization of all individuals in the scene and classification of their\nactions or group activity over time. Who is interacting with whom, e.g. not\neveryone in a queue is interacting with each other, is often not predicted.\nThere are scenarios where people are best to be split into sub-groups, which we\ncall social groups, and each social group may be engaged in a different social\nactivity. In this paper, we solve the problem of simultaneously grouping people\nby their social interactions, predicting their individual actions and the\nsocial activity of each social group, which we call the social task. Our main\ncontributions are: i) we propose an end-to-end trainable framework for the\nsocial task; ii) our proposed method also sets the state-of-the-art results on\ntwo widely adopted benchmarks for the traditional group activity recognition\ntask (assuming individuals of the scene form a single group and predicting a\nsingle group activity label for the scene); iii) we introduce new annotations\non an existing group activity dataset, re-purposing it for the social task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 10:42:11 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 00:57:21 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Ehsanpour", "Mahsa", ""], ["Abedin", "Alireza", ""], ["Saleh", "Fatemeh", ""], ["Shi", "Javen", ""], ["Reid", "Ian", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "2007.02639", "submitter": "Matthias Perkonigg", "authors": "Johannes Hofmanninger, Matthias Perkonigg, James A. Brink, Oleg\n  Pianykh, Christian Herold, Georg Langs", "title": "Dynamic memory to alleviate catastrophic forgetting in continuous\n  learning settings", "comments": "The first two authors contributed equally. Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, technical progress or changes in diagnostic procedures\nlead to a continuous change in image appearance. Scanner manufacturer,\nreconstruction kernel, dose, other protocol specific settings or administering\nof contrast agents are examples that influence image content independent of the\nscanned biology. Such domain and task shifts limit the applicability of machine\nlearning algorithms in the clinical routine by rendering models obsolete over\ntime. Here, we address the problem of data shifts in a continuous learning\nscenario by adapting a model to unseen variations in the source domain while\ncounteracting catastrophic forgetting effects. Our method uses a dynamic memory\nto facilitate rehearsal of a diverse training data subset to mitigate\nforgetting. We evaluated our approach on routine clinical CT data obtained with\ntwo different scanner protocols and synthetic classification tasks. Experiments\nshow that dynamic memory counters catastrophic forgetting in a setting with\nmultiple data shifts without the necessity for explicit knowledge about when\nthese shifts occur.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:02:38 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:05:02 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Hofmanninger", "Johannes", ""], ["Perkonigg", "Matthias", ""], ["Brink", "James A.", ""], ["Pianykh", "Oleg", ""], ["Herold", "Christian", ""], ["Langs", "Georg", ""]]}, {"id": "2007.02662", "submitter": "Van Huy Vo", "authors": "Huy V. Vo, Patrick P\\'erez and Jean Ponce", "title": "Toward unsupervised, multi-object discovery in large-scale image\n  collections", "comments": "Accepted for publication in European Conference on Computer Vision\n  (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of discovering the objects present in a\ncollection of images without any supervision. We build on the optimization\napproach of Vo et al. (CVPR'19) with several key novelties: (1) We propose a\nnovel saliency-based region proposal algorithm that achieves significantly\nhigher overlap with ground-truth objects than other competitive methods. This\nprocedure leverages off-the-shelf CNN features trained on classification tasks\nwithout any bounding box information, but is otherwise unsupervised. (2) We\nexploit the inherent hierarchical structure of proposals as an effective\nregularizer for the approach to object discovery of Vo et al., boosting its\nperformance to significantly improve over the state of the art on several\nstandard benchmarks. (3) We adopt a two-stage strategy to select promising\nproposals using small random sets of images before using the whole image\ncollection to discover the objects it depicts, allowing us to tackle, for the\nfirst time (to the best of our knowledge), the discovery of multiple objects in\neach one of the pictures making up datasets with up to 20,000 images, an over\nfive-fold increase compared to existing methods, and a first step toward true\nlarge-scale unsupervised image interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:43:47 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 11:11:31 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Vo", "Huy V.", ""], ["P\u00e9rez", "Patrick", ""], ["Ponce", "Jean", ""]]}, {"id": "2007.02663", "submitter": "Yuan Lan", "authors": "Yuan Lan, Yang Xiang, Luchan Zhang", "title": "An Elastic Interaction-Based Loss Function for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have shown their success in medical image\nsegmentation since they are easy to manipulate and robust to various types of\ndatasets. The commonly used loss functions in the deep segmentation task are\npixel-wise loss functions. This results in a bottleneck for these models to\nachieve high precision for complicated structures in biomedical images. For\nexample, the predicted small blood vessels in retinal images are often\ndisconnected or even missed under the supervision of the pixel-wise losses.\nThis paper addresses this problem by introducing a long-range elastic\ninteraction-based training strategy. In this strategy, convolutional neural\nnetwork (CNN) learns the target region under the guidance of the elastic\ninteraction energy between the boundary of the predicted region and that of the\nactual object. Under the supervision of the proposed loss, the boundary of the\npredicted region is attracted strongly by the object boundary and tends to stay\nconnected. Experimental results show that our method is able to achieve\nconsiderable improvements compared to commonly used pixel-wise loss functions\n(cross entropy and dice Loss) and other recent loss functions on three retinal\nvessel segmentation datasets, DRIVE, STARE and CHASEDB1.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:49:14 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 13:09:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lan", "Yuan", ""], ["Xiang", "Yang", ""], ["Zhang", "Luchan", ""]]}, {"id": "2007.02684", "submitter": "Ramachandra Raghavendra Prof.", "authors": "Sushma Venkatesh, Kiran Raja, Raghavendra Ramachandra, Christoph Busch", "title": "On the Influence of Ageing on Face Morph Attacks: Vulnerability and\n  Detection", "comments": "Accepted in IJCB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face morphing attacks have raised critical concerns as they demonstrate a new\nvulnerability of Face Recognition Systems (FRS), which are widely deployed in\nborder control applications. The face morphing process uses the images from\nmultiple data subjects and performs an image blending operation to generate a\nmorphed image of high quality. The generated morphed image exhibits similar\nvisual characteristics corresponding to the biometric characteristics of the\ndata subjects that contributed to the composite image and thus making it\ndifficult for both humans and FRS, to detect such attacks. In this paper, we\nreport a systematic investigation on the vulnerability of the\nCommercial-Off-The-Shelf (COTS) FRS when morphed images under the influence of\nageing are presented. To this extent, we have introduced a new morphed face\ndataset with ageing derived from the publicly available MORPH II face dataset,\nwhich we refer to as MorphAge dataset. The dataset has two bins based on age\nintervals, the first bin - MorphAge-I dataset has 1002 unique data subjects\nwith the age variation of 1 year to 2 years while the MorphAge-II dataset\nconsists of 516 data subjects whose age intervals are from 2 years to 5 years.\nTo effectively evaluate the vulnerability for morphing attacks, we also\nintroduce a new evaluation metric, namely the Fully Mated Morphed Presentation\nMatch Rate (FMMPMR), to quantify the vulnerability effectively in a realistic\nscenario. Extensive experiments are carried out by using two different COTS FRS\n(COTS I - Cognitec and COTS II - Neurotechnology) to quantify the vulnerability\nwith ageing. Further, we also evaluate five different Morph Attack Detection\n(MAD) techniques to benchmark their detection performance with ageing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:32:41 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 16:56:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Venkatesh", "Sushma", ""], ["Raja", "Kiran", ""], ["Ramachandra", "Raghavendra", ""], ["Busch", "Christoph", ""]]}, {"id": "2007.02693", "submitter": "Aviv Navon", "authors": "Aviv Navon and Idan Achituve and Haggai Maron and Gal Chechik and\n  Ethan Fetaya", "title": "Auxiliary Learning by Implicit Differentiation", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks with auxiliary tasks is a common practice for\nimproving the performance on a main task of interest. Two main challenges arise\nin this multi-task learning setting: (i) designing useful auxiliary tasks; and\n(ii) combining auxiliary tasks into a single coherent loss. Here, we propose a\nnovel framework, AuxiLearn, that targets both challenges based on implicit\ndifferentiation. First, when useful auxiliaries are known, we propose learning\na network that combines all losses into a single coherent objective function.\nThis network can learn non-linear interactions between tasks. Second, when no\nuseful auxiliary task is known, we describe how to learn a network that\ngenerates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series\nof tasks and domains, including image segmentation and learning with attributes\nin the low data regime, and find that it consistently outperforms competing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:35:07 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 07:40:34 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 06:52:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Navon", "Aviv", ""], ["Achituve", "Idan", ""], ["Maron", "Haggai", ""], ["Chechik", "Gal", ""], ["Fetaya", "Ethan", ""]]}, {"id": "2007.02711", "submitter": "Li-Heng Chen", "authors": "Li-Heng Chen and Christos G. Bampis and Zhi Li and Andrey Norkin and\n  Alan C. Bovik", "title": "Perceptually Optimizing Deep Image Compression", "comments": "7 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1910.08845", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean squared error (MSE) and $\\ell_p$ norms have largely dominated the\nmeasurement of loss in neural networks due to their simplicity and analytical\nproperties. However, when used to assess visual information loss, these simple\nnorms are not highly consistent with human perception. Here, we propose a\ndifferent proxy approach to optimize image analysis networks against\nquantitative perceptual models. Specifically, we construct a proxy network,\nwhich mimics the perceptual model while serving as a loss layer of the\nnetwork.We experimentally demonstrate how this optimization framework can be\napplied to train an end-to-end optimized image compression network. By building\non top of a modern deep image compression models, we are able to demonstrate an\naveraged bitrate reduction of $28.7\\%$ over MSE optimization, given a specified\nperceptual quality (VMAF) level.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:33:28 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 15:04:06 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Chen", "Li-Heng", ""], ["Bampis", "Christos G.", ""], ["Li", "Zhi", ""], ["Norkin", "Andrey", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2007.02713", "submitter": "Yingjie Zhai", "authors": "Yingjie Zhai, Deng-Ping Fan, Jufeng Yang, Ali Borji, Ling Shao, Junwei\n  Han, Liang Wang", "title": "Bifurcated backbone strategy for RGB-D salient object detection", "comments": "A preliminary version of this work has been accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-level feature fusion is a fundamental topic in computer vision. It has\nbeen exploited to detect, segment and classify objects at various scales. When\nmulti-level features meet multi-modal cues, the optimal feature aggregation and\nmulti-modal learning strategy become a hot potato. In this paper, we leverage\nthe inherent multi-modal and multi-level nature of RGB-D salient object\ndetection to devise a novel cascaded refinement network. In particular, first,\nwe propose to regroup the multi-level features into teacher and student\nfeatures using a bifurcated backbone strategy (BBS). Second, we introduce a\ndepth-enhanced module (DEM) to excavate informative depth cues from the channel\nand spatial views. Then, RGB and depth modalities are fused in a complementary\nway. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is\nsimple, efficient, and backbone-independent. Extensive experiments show that\nBBS-Net significantly outperforms eighteen SOTA models on eight challenging\ndatasets under five evaluation measures, demonstrating the superiority of our\napproach ($\\sim 4 \\%$ improvement in S-measure $vs.$ the top-ranked model:\nDMRA-iccv2019). In addition, we provide a comprehensive analysis on the\ngeneralization ability of different RGB-D datasets and provide a powerful\ntraining set for future research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:01:30 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 07:30:28 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Zhai", "Yingjie", ""], ["Fan", "Deng-Ping", ""], ["Yang", "Jufeng", ""], ["Borji", "Ali", ""], ["Shao", "Ling", ""], ["Han", "Junwei", ""], ["Wang", "Liang", ""]]}, {"id": "2007.02721", "submitter": "Thibaut Issenhuth", "authors": "Thibaut Issenhuth and J\\'er\\'emie Mary and Cl\\'ement Calauz\\`enes", "title": "Do Not Mask What You Do Not Need to Mask: a Parser-Free Virtual Try-On", "comments": "Accepted at ECCV 2020. arXiv admin note: text overlap with\n  arXiv:1906.01347", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2D virtual try-on task has recently attracted a great interest from the\nresearch community, for its direct potential applications in online shopping as\nwell as for its inherent and non-addressed scientific challenges. This task\nrequires fitting an in-shop cloth image on the image of a person, which is\nhighly challenging because it involves cloth warping, image compositing, and\nsynthesizing. Casting virtual try-on into a supervised task faces a difficulty:\navailable datasets are composed of pairs of pictures (cloth, person wearing the\ncloth). Thus, we have no access to ground-truth when the cloth on the person\nchanges. State-of-the-art models solve this by masking the cloth information on\nthe person with both a human parser and a pose estimator. Then, image synthesis\nmodules are trained to reconstruct the person image from the masked person\nimage and the cloth image. This procedure has several caveats: firstly, human\nparsers are prone to errors; secondly, it is a costly pre-processing step,\nwhich also has to be applied at inference time; finally, it makes the task\nharder than it is since the mask covers information that should be kept such as\nhands or accessories. In this paper, we propose a novel student-teacher\nparadigm where the teacher is trained in the standard way (reconstruction)\nbefore guiding the student to focus on the initial task (changing the cloth).\nThe student additionally learns from an adversarial loss, which pushes it to\nfollow the distribution of the real images. Consequently, the student exploits\ninformation that is masked to the teacher. A student trained without the\nadversarial loss would not use this information. Also, getting rid of both\nhuman parser and pose estimator at inference time allows obtaining a real-time\nvirtual try-on.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:39:39 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 16:04:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Issenhuth", "Thibaut", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Calauz\u00e8nes", "Cl\u00e9ment", ""]]}, {"id": "2007.02722", "submitter": "Aocheng Li", "authors": "Aocheng Li, Jie Guo, Yanwen Guo", "title": "Image Stitching Based on Planar Region Consensus", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3086079", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image stitching for two images without a global transformation between them\nis notoriously difficult. In this paper, noticing the importance of planar\nstructure under perspective geometry, we propose a new image stitching method\nwhich stitches images by allowing for the alignment of a set of matched\ndominant planar regions. Clearly different from previous methods resorting to\nplane segmentation, the key to our approach is to utilize rich semantic\ninformation directly from RGB images to extract planar image regions with a\ndeep Convolutional Neural Network (CNN). We specifically design a new module to\nmake fully use of existing semantic segmentation networks to accommodate planar\nsegmentation. To train the network, a dataset for planar region segmentation is\ncontributed. With the planar region knowledge, a set of local transformations\ncan be obtained by constraining matched regions, enabling more precise\nalignment in the overlapping area. We also use planar knowledge to estimate a\ntransformation field over the whole image. The final mosaic is obtained by a\nmesh-based optimization framework which maintains high alignment accuracy and\nrelaxes similarity transformation at the same time. Extensive experiments with\nquantitative comparisons show that our method can deal with different\nsituations and outperforms the state-of-the-arts on challenging scenes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:07:20 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Li", "Aocheng", ""], ["Guo", "Jie", ""], ["Guo", "Yanwen", ""]]}, {"id": "2007.02734", "submitter": "Hadi Mohaghegh Dolatabadi", "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie", "title": "Black-box Adversarial Example Generation with Normalizing Flows", "comments": "Accepted to the 2nd workshop on Invertible Neural Networks,\n  Normalizing Flows, and Explicit Likelihood Models (ICML 2020), Virtual\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network classifiers suffer from adversarial vulnerability:\nwell-crafted, unnoticeable changes to the input data can affect the classifier\ndecision. In this regard, the study of powerful adversarial attacks can help\nshed light on sources of this malicious behavior. In this paper, we propose a\nnovel black-box adversarial attack using normalizing flows. We show how an\nadversary can be found by searching over a pre-trained flow-based model base\ndistribution. This way, we can generate adversaries that resemble the original\ndata closely as the perturbations are in the shape of the data. We then\ndemonstrate the competitive performance of the proposed approach against\nwell-known black-box adversarial attack methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:14:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Dolatabadi", "Hadi M.", ""], ["Erfani", "Sarah", ""], ["Leckie", "Christopher", ""]]}, {"id": "2007.02749", "submitter": "Chunnnan Wang", "authors": "Chunnan Wang, Hongzhi Wang, Guosheng Feng, Fei Geng", "title": "Multi-Objective Neural Architecture Search Based on Diverse Structures\n  and Adaptive Recommendation", "comments": "11pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search space of neural architecture search (NAS) for convolutional neural\nnetwork (CNN) is huge. To reduce searching cost, most NAS algorithms use fixed\nouter network level structure, and search the repeatable cell structure only.\nSuch kind of fixed architecture performs well when enough cells and channels\nare used. However, when the architecture becomes more lightweight, the\nperformance decreases significantly. To obtain better lightweight\narchitectures, more flexible and diversified neural architectures are in\ndemand, and more efficient methods should be designed for larger search space.\nMotivated by this, we propose MoARR algorithm, which utilizes the existing\nresearch results and historical information to quickly find architectures that\nare both lightweight and accurate. We use the discovered high-performance cells\nto construct network architectures. This method increases the network\narchitecture diversity while also reduces the search space of cell structure\ndesign. In addition, we designs a novel multi-objective method to effectively\nanalyze the historical evaluation information, so as to efficiently search for\nthe Pareto optimal architectures with high accuracy and small parameter number.\nExperimental results show that our MoARR can achieve a powerful and lightweight\nmodel (with 1.9% error rate and 2.3M parameters) on CIFAR-10 in 6 GPU hours,\nwhich is better than the state-of-the-arts. The explored architecture is\ntransferable to ImageNet and achieves 76.0% top-1 accuracy with 4.9M\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:42:33 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 15:14:07 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wang", "Chunnan", ""], ["Wang", "Hongzhi", ""], ["Feng", "Guosheng", ""], ["Geng", "Fei", ""]]}, {"id": "2007.02790", "submitter": "Zhe Xu", "authors": "Zhe Xu, Jie Luo, Jiangpeng Yan, Ritvik Pulya, Xiu Li, William Wells\n  III, Jayender Jagadeesan", "title": "Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image\n  Registration", "comments": "accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration between Computed Tomography (CT) images and\nMagnetic Resonance (MR) imaging is essential for many image-guided therapies.\nIn this paper, we propose a novel translation-based unsupervised deformable\nimage registration method. Distinct from other translation-based methods that\nattempt to convert the multimodal problem (e.g., CT-to-MR) into a unimodal\nproblem (e.g., MR-to-MR) via image-to-image translation, our method leverages\nthe deformation fields estimated from both: (i) the translated MR image and\n(ii) the original CT image in a dual-stream fashion, and automatically learns\nhow to fuse them to achieve better registration performance. The multimodal\nregistration network can be effectively trained by computationally efficient\nsimilarity metrics without any ground-truth deformation. Our method has been\nevaluated on two clinical datasets and demonstrates promising results compared\nto state-of-the-art traditional and learning-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:44:06 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 06:12:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Xu", "Zhe", ""], ["Luo", "Jie", ""], ["Yan", "Jiangpeng", ""], ["Pulya", "Ritvik", ""], ["Li", "Xiu", ""], ["Wells", "William", "III"], ["Jagadeesan", "Jayender", ""]]}, {"id": "2007.02798", "submitter": "Sam Bond-Taylor", "authors": "Sam Bond-Taylor and Chris G. Willcocks", "title": "Gradient Origin Networks", "comments": "16 pages, 17 figures, accepted at ICLR 2021, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new type of generative model that is able to quickly\nlearn a latent representation without an encoder. This is achieved using\nempirical Bayes to calculate the expectation of the posterior, which is\nimplemented by initialising a latent vector with zeros, then using the gradient\nof the log-likelihood of the data with respect to this zero vector as new\nlatent points. The approach has similar characteristics to autoencoders, but\nwith a simpler architecture, and is demonstrated in a variational autoencoder\nequivalent that permits sampling. This also allows implicit representation\nnetworks to learn a space of implicit functions without requiring a\nhypernetwork, retaining their representation advantages across datasets. The\nexperiments show that the proposed method converges faster, with significantly\nlower reconstruction error than autoencoders, while requiring half the\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:00:11 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 08:44:05 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 17:18:20 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 15:38:55 GMT"}, {"version": "v5", "created": "Wed, 24 Mar 2021 14:15:29 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bond-Taylor", "Sam", ""], ["Willcocks", "Chris G.", ""]]}, {"id": "2007.02808", "submitter": "Mohamed Ilyes Lakhal", "authors": "Mohamed Ilyes Lakhal, Davide Boscaini, Fabio Poiesi, Oswald Lanz,\n  Andrea Cavallaro", "title": "Novel-View Human Action Synthesis", "comments": "Asian Conference on Computer Vision (ACCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel-View Human Action Synthesis aims to synthesize the movement of a body\nfrom a virtual viewpoint, given a video from a real viewpoint. We present a\nnovel 3D reasoning to synthesize the target viewpoint. We first estimate the 3D\nmesh of the target body and transfer the rough textures from the 2D images to\nthe mesh. As this transfer may generate sparse textures on the mesh due to\nframe resolution or occlusions. We produce a semi-dense textured mesh by\npropagating the transferred textures both locally, within local geodesic\nneighborhoods, and globally, across symmetric semantic parts. Next, we\nintroduce a context-based generator to learn how to correct and complete the\nresidual appearance information. This allows the network to independently focus\non learning the foreground and background synthesis tasks. We validate the\nproposed solution on the public NTU RGB+D dataset. The code and resources are\navailable at https://bit.ly/36u3h4K.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:11:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 01:11:53 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 10:02:36 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lakhal", "Mohamed Ilyes", ""], ["Boscaini", "Davide", ""], ["Poiesi", "Fabio", ""], ["Lanz", "Oswald", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2007.02811", "submitter": "Mahdi Rezaei", "authors": "Fatemeh Serpush, Mahdi Rezaei", "title": "Complex Human Action Recognition in Live Videos Using Hybrid FR-DL\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated human action recognition is one of the most attractive and\npractical research fields in computer vision, in spite of its high\ncomputational costs. In such systems, the human action labelling is based on\nthe appearance and patterns of the motions in the video sequences; however, the\nconventional methodologies and classic neural networks cannot use temporal\ninformation for action recognition prediction in the upcoming frames in a video\nsequence. On the other hand, the computational cost of the preprocessing stage\nis high. In this paper, we address challenges of the preprocessing phase, by an\nautomated selection of representative frames among the input sequences.\nFurthermore, we extract the key features of the representative frame rather\nthan the entire features. We propose a hybrid technique using background\nsubtraction and HOG, followed by application of a deep neural network and\nskeletal modelling method. The combination of a CNN and the LSTM recursive\nnetwork is considered for feature selection and maintaining the previous\ninformation, and finally, a Softmax-KNN classifier is used for labelling human\nactivities. We name our model as Feature Reduction & Deep Learning based action\nrecognition method, or FR-DL in short. To evaluate the proposed method, we use\nthe UCF dataset for the benchmarking which is widely-used among researchers in\naction recognition research. The dataset includes 101 complicated activities in\nthe wild. Experimental results show a significant improvement in terms of\naccuracy and speed in comparison with six state-of-the-art articles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:12:50 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Serpush", "Fatemeh", ""], ["Rezaei", "Mahdi", ""]]}, {"id": "2007.02833", "submitter": "Amelia Pollard", "authors": "Amelia Elizabeth Pollard and Jonathan L. Shapiro", "title": "Eliminating Catastrophic Interference with Biased Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a model to take advantage of the multi-task nature of complex\ndatasets by learning to separate tasks and subtasks in and end to end manner by\nbiasing competitive interactions in the network. This method does not require\nadditional labelling or reformatting of data in a dataset. We propose an\nalternate view to the monolithic one-task-fits-all learning of multi-task\nproblems, and describe a model based on a theory of neuronal attention from\nneuroscience, proposed by Desimone. We create and exhibit a new toy dataset,\nbased on the MNIST dataset, which we call MNIST-QA, for testing Visual Question\nAnswering architectures in a low-dimensional environment while preserving the\nmore difficult components of the Visual Question Answering task, and\ndemonstrate the proposed network architecture on this new dataset, as well as\non COCO-QA and DAQUAR-FULL. We then demonstrate that this model eliminates\ncatastrophic interference between tasks on a newly created toy dataset and\nprovides competitive results in the Visual Question Answering space. We provide\nfurther evidence that Visual Question Answering can be approached as a\nmulti-task problem, and demonstrate that this new architecture based on the\nBiased Competition model is capable of learning to separate and learn the tasks\nin an end-to-end fashion without the need for task labels.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:15:15 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pollard", "Amelia Elizabeth", ""], ["Shapiro", "Jonathan L.", ""]]}, {"id": "2007.02839", "submitter": "Vivek Parmar", "authors": "Vivek Parmar, Narayani Bhatia, Shubham Negi and Manan Suri", "title": "Exploration of Optimized Semantic Segmentation Architectures for\n  edge-Deployment on Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an analysis on the impact of network parameters for\nsemantic segmentation architectures in context of UAV data processing. We\npresent the analysis on the DroneDeploy Segmentation benchmark. Based on the\ncomparative analysis we identify the optimal network architecture to be\nFPN-EfficientNetB3 with pretrained encoder backbones based on Imagenet Dataset.\nThe network achieves IoU score of 0.65 and F1-score of 0.71 over the validation\ndataset. We also compare the various architectures in terms of their memory\nfootprint and inference latency with further exploration of the impact of\nTensorRT based optimizations. We achieve memory savings of ~4.1x and latency\nimprovement of 10% compared to Model: FPN and Backbone: InceptionResnetV2.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:49:18 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Parmar", "Vivek", ""], ["Bhatia", "Narayani", ""], ["Negi", "Shubham", ""], ["Suri", "Manan", ""]]}, {"id": "2007.02846", "submitter": "Fangyun Wei", "authors": "Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, Stephen Lin", "title": "Point-Set Anchors for Object Detection, Instance Segmentation and Pose\n  Estimation", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent approach for object detection and human pose estimation is to\nregress bounding boxes or human keypoints from a central point on the object or\nperson. While this center-point regression is simple and efficient, we argue\nthat the image features extracted at a central point contain limited\ninformation for predicting distant keypoints or bounding box boundaries, due to\nobject deformation and scale/orientation variation. To facilitate inference, we\npropose to instead perform regression from a set of points placed at more\nadvantageous positions. This point set is arranged to reflect a good\ninitialization for the given task, such as modes in the training data for pose\nestimation, which lie closer to the ground truth than the central point and\nprovide more informative features for regression. As the utility of a point set\ndepends on how well its scale, aspect ratio and rotation matches the target, we\nadopt the anchor box technique of sampling these transformations to generate\nadditional point-set candidates. We apply this proposed framework, called\nPoint-Set Anchors, to object detection, instance segmentation, and human pose\nestimation. Our results show that this general-purpose approach can achieve\nperformance competitive with state-of-the-art methods for each of these tasks.\nCode is available at \\url{https://github.com/FangyunWei/PointSetAnchor}\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:59:56 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 14:11:49 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 07:15:08 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 06:14:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wei", "Fangyun", ""], ["Sun", "Xiao", ""], ["Li", "Hongyang", ""], ["Wang", "Jingdong", ""], ["Lin", "Stephen", ""]]}, {"id": "2007.02884", "submitter": "Mathias Unberath", "authors": "Mathias Unberath, Kevin Yu, Roghayeh Barmaki, Alex Johnson, Nassir\n  Navab", "title": "Augment Yourself: Mixed Reality Self-Augmentation Using Optical\n  See-through Head-mounted Displays and Physical Mirrors", "comments": "This manuscript was initially submitted to IEEE VR TVCG 2018 on\n  November 22, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical see-though head-mounted displays (OST HMDs) are one of the key\ntechnologies for merging virtual objects and physical scenes to provide an\nimmersive mixed reality (MR) environment to its user. A fundamental limitation\nof HMDs is, that the user itself cannot be augmented conveniently as, in casual\nposture, only the distal upper extremities are within the field of view of the\nHMD. Consequently, most MR applications that are centered around the user, such\nas virtual dressing rooms or learning of body movements, cannot be realized\nwith HMDs. In this paper, we propose a novel concept and prototype system that\ncombines OST HMDs and physical mirrors to enable self-augmentation and provide\nan immersive MR environment centered around the user. Our system, to the best\nof our knowledge the first of its kind, estimates the user's pose in the\nvirtual image generated by the mirror using an RGBD camera attached to the HMD\nand anchors virtual objects to the reflection rather than the user directly. We\nevaluate our system quantitatively with respect to calibration accuracy and\ninfrared signal degradation effects due to the mirror, and show its potential\nin applications where large mirrors are already an integral part of the\nfacility. Particularly, we demonstrate its use for virtual fitting rooms,\ngaming applications, anatomy learning, and personal fitness. In contrast to\ncompeting devices such as LCD-equipped smart mirrors, the proposed system\nconsists of only an HMD with RGBD camera and, thus, does not require a prepared\nenvironment making it very flexible and generic. In future work, we will aim to\ninvestigate how the system can be optimally used for physical rehabilitation\nand personal training as a promising application.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:53:47 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Unberath", "Mathias", ""], ["Yu", "Kevin", ""], ["Barmaki", "Roghayeh", ""], ["Johnson", "Alex", ""], ["Navab", "Nassir", ""]]}, {"id": "2007.02895", "submitter": "Noor Akhmad Setiawan PhD", "authors": "Kuntoro Adi Nugroho, Noor Akhmad Setiawan, Teguh Bharata Adji", "title": "Coronary Heart Disease Diagnosis Based on Improved Ensemble Learning", "comments": null, "journal-ref": "J. Converg. Inf. Technol, 8(13), p.13. 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis is required before performing proper treatments for\ncoronary heart disease. Machine learning based approaches have been proposed by\nmany researchers to improve the accuracy of coronary heart disease diagnosis.\nEnsemble learning and cascade generalization are among the methods which can be\nused to improve the generalization ability of learning algorithm. The objective\nof this study is to develop heart disease diagnosis method based on ensemble\nlearning and cascade generalization. Cascade generalization method with loose\ncoupling strategy is proposed in this study. C4. 5 and RIPPER algorithm were\nused as meta-level algorithm and Naive Bayes was used as baselevel algorithm.\nBagging and Random Subspace were evaluated for constructing the ensemble. The\nhybrid cascade ensemble methods are compared with the learning algorithms in\nnon-ensemble mode and non-cascade mode. The methods are also compared with\nRotation Forest. Based on the evaluation result, the hybrid cascade ensemble\nmethod demonstrated the best result for the given heart disease diagnosis case.\nAccuracy and diversity evaluation was performed to analyze the impact of the\ncascade strategy. Based on the result, the accuracy of the classifiers in the\nensemble is increased but the diversity is decreased.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:14:30 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Nugroho", "Kuntoro Adi", ""], ["Setiawan", "Noor Akhmad", ""], ["Adji", "Teguh Bharata", ""]]}, {"id": "2007.02915", "submitter": "Deng Weijian", "authors": "Weijian Deng and Liang Zheng", "title": "Are Labels Always Necessary for Classifier Accuracy Evaluation?", "comments": "CVPR 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To calculate the model accuracy on a computer vision task, e.g., object\nrecognition, we usually require a test set composing of test samples and their\nground truth labels. Whilst standard usage cases satisfy this requirement, many\nreal-world scenarios involve unlabeled test data, rendering common model\nevaluation methods infeasible. We investigate this important and under-explored\nproblem, Automatic model Evaluation (AutoEval). Specifically, given a labeled\ntraining set and a classifier, we aim to estimate the classification accuracy\non unlabeled test datasets. We construct a meta-dataset: a dataset comprised of\ndatasets generated from the original images via various transformations such as\nrotation, background substitution, foreground scaling, etc. As the\nclassification accuracy of the model on each sample (dataset) is known from the\noriginal dataset labels, our task can be solved via regression. Using the\nfeature statistics to represent the distribution of a sample dataset, we can\ntrain regression models (e.g., a regression neural network) to predict model\nperformance. Using synthetic meta-dataset and real-world datasets in training\nand testing, respectively, we report a reasonable and promising prediction of\nthe model accuracy. We also provide insights into the application scope,\nlimitation, and potential future direction of AutoEval.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:45:39 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 13:45:23 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 06:32:00 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Deng", "Weijian", ""], ["Zheng", "Liang", ""]]}, {"id": "2007.02919", "submitter": "Xiang Xu", "authors": "Xiang Xu, Megha Nawhal, Greg Mori, Manolis Savva", "title": "MCMI: Multi-Cycle Image Translation with Mutual Information Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mutual information-based framework for unsupervised\nimage-to-image translation. Our MCMI approach treats single-cycle image\ntranslation models as modules that can be used recurrently in a multi-cycle\ntranslation setting where the translation process is bounded by mutual\ninformation constraints between the input and output images. The proposed\nmutual information constraints can improve cross-domain mappings by optimizing\nout translation functions that fail to satisfy the Markov property during image\ntranslations. We show that models trained with MCMI produce higher quality\nimages and learn more semantically-relevant mappings compared to\nstate-of-the-art image translation methods. The MCMI framework can be applied\nto existing unpaired image-to-image translation models with minimum\nmodifications. Qualitative experiments and a perceptual study demonstrate the\nimage quality improvements and generality of our approach using several\nbackbone models and a variety of image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:50:43 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Xu", "Xiang", ""], ["Nawhal", "Megha", ""], ["Mori", "Greg", ""], ["Savva", "Manolis", ""]]}, {"id": "2007.02980", "submitter": "Asif Iqbal Khan", "authors": "Asif Iqbal Khan, SMK Quadri and Saba Banday", "title": "Deep Learning for Apple Diseases: Classification and Identification", "comments": null, "journal-ref": null, "doi": "10.1504/IJCISTUDIES.2021.10033513", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diseases and pests cause huge economic loss to the apple industry every year.\nThe identification of various apple diseases is challenging for the farmers as\nthe symptoms produced by different diseases may be very similar, and may be\npresent simultaneously. This paper is an attempt to provide the timely and\naccurate detection and identification of apple diseases. In this study, we\npropose a deep learning based approach for identification and classification of\napple diseases. The first part of the study is dataset creation which includes\ndata collection and data labelling. Next, we train a Convolutional Neural\nNetwork (CNN) model on the prepared dataset for automatic classification of\napple diseases. CNNs are end-to-end learning algorithms which perform automatic\nfeature extraction and learn complex features directly from raw images, making\nthem suitable for wide variety of tasks like image classification, object\ndetection, segmentation etc. We applied transfer learning to initialize the\nparameters of the proposed deep model. Data augmentation techniques like\nrotation, translation, reflection and scaling were also applied to prevent\noverfitting. The proposed CNN model obtained encouraging results, reaching\naround 97.18% of accuracy on our prepared dataset. The results validate that\nthe proposed method is effective in classifying various types of apple diseases\nand can be used as a practical tool by farmers.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 18:08:58 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Khan", "Asif Iqbal", ""], ["Quadri", "SMK", ""], ["Banday", "Saba", ""]]}, {"id": "2007.03021", "submitter": "Dmitry Pozdnyakov", "authors": "Dmitry Pozdnyakov", "title": "Determination of the most representative descriptor among a set of\n  feature vectors for the same object", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On an example of solution of the face recognition problem the approach for\nestimation of the most representative descriptor among a set of feature vectors\nfor the same face is considered in present study. The estimation is based on\nrobust calculation of the mode-median mixture vector for the set as the\ndescriptor by means of Welsch/Leclerc loss function application in case of very\nsparse filling of the feature space with feature vectors\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:09:06 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Pozdnyakov", "Dmitry", ""]]}, {"id": "2007.03028", "submitter": "Chen-Han Tsai", "authors": "Chen-Han Tsai, Nahum Kiryati, Eli Konen, Miri Sklair-Levy, Arnaldo\n  Mayer", "title": "Labeling of Multilingual Breast MRI Reports", "comments": "10 pages, 5 figures, MICCAI LABELS Workshop 2020", "journal-ref": "Lecture Notes in Computer Science 12446 (2020) 233-241", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical reports are an essential medium in recording a patient's condition\nthroughout a clinical trial. They contain valuable information that can be\nextracted to generate a large labeled dataset needed for the development of\nclinical tools. However, the majority of medical reports are stored in an\nunregularized format, and a trained human annotator (typically a doctor) must\nmanually assess and label each case, resulting in an expensive and time\nconsuming procedure. In this work, we present a framework for developing a\nmultilingual breast MRI report classifier using a custom-built language\nrepresentation called LAMBR. Our proposed method overcomes practical challenges\nfaced in clinical settings, and we demonstrate improved performance in\nextracting labels from medical reports when compared with conventional\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:22:44 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:06:47 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 13:07:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Tsai", "Chen-Han", ""], ["Kiryati", "Nahum", ""], ["Konen", "Eli", ""], ["Sklair-Levy", "Miri", ""], ["Mayer", "Arnaldo", ""]]}, {"id": "2007.03032", "submitter": "Saurav Jha", "authors": "Saurav Jha, Martin Schiemer, Juan Ye", "title": "Continual Learning in Human Activity Recognition: an Empirical Analysis\n  of Regularization", "comments": "7 pages, 5 figures, 3 tables (Appendix included)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the growing trend of continual learning techniques for deep neural\nnetworks focusing on the domain of computer vision, there is a need to identify\nwhich of these generalizes well to other tasks such as human activity\nrecognition (HAR). As recent methods have mostly been composed of loss\nregularization terms and memory replay, we provide a constituent-wise analysis\nof some prominent task-incremental learning techniques employing these on HAR\ndatasets. We find that most regularization approaches lack substantial effect\nand provide an intuition of when they fail. Thus, we make the case that the\ndevelopment of continual learning algorithms should be motivated by rather\ndiverse task domains.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:30:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jha", "Saurav", ""], ["Schiemer", "Martin", ""], ["Ye", "Juan", ""]]}, {"id": "2007.03035", "submitter": "Anvar Khafizov Mr.", "authors": "Maxim Grigoriev, Anvar Khafizov, Vladislav Kokhan, Viktor Asadchikov", "title": "Robust Technique for Representative Volume Element Identification in\n  Noisy Microtomography Images of Porous Materials Based on Pores Morphology\n  and Their Spatial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microtomography is a powerful method of materials investigation. It enables\nto obtain physical properties of porous media non-destructively that is useful\nin studies. One of the application ways is a calculation of porosity, pore\nsizes, surface area, and other parameters of metal-ceramic (cermet) membranes\nwhich are widely spread in the filtration industry. The microtomography\napproach is efficient because all of those parameters are calculated\nsimultaneously in contrast to the conventional techniques. Nevertheless, the\ncalculations on Micro-CT reconstructed images appear to be time-consuming,\nconsequently representative volume element should be chosen to speed them up.\nThis research sheds light on representative elementary volume identification\nwithout consideration of any physical parameters such as porosity, etc. Thus,\nthe volume element could be found even in noised and grayscale images. The\nproposed method is flexible and does not overestimate the volume size in the\ncase of anisotropic samples. The obtained volume element could be used for\ncomputations of the domain's physical characteristics if the image is filtered\nand binarized, or for selections of optimal filtering parameters for denoising\nprocedure.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:34:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Grigoriev", "Maxim", ""], ["Khafizov", "Anvar", ""], ["Kokhan", "Vladislav", ""], ["Asadchikov", "Viktor", ""]]}, {"id": "2007.03047", "submitter": "Vivien Sainte Fare Garnot", "authors": "Vivien Sainte Fare Garnot and Loic Landrieu", "title": "Leveraging Class Hierarchies with Metric-Guided Prototype Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all errors are created equal. This is especially true for many key\nmachine learning applications. In the case of classification tasks, the\nseverity of errors can be summarized under the form of a cost matrix, which\nassesses the gravity of confusing each pair of classes. When the target classes\nare organized into a hierarchical structure, this matrix defines a metric. We\npropose to integrate this metric in a new and versatile classification layer in\norder to model the disparity of errors. Our method relies on jointly learning a\nfeature-extracting network and a set of class representations, or prototypes,\nwhich incorporate the error metric into their relative arrangement in the\nembedding space. Our approach allows for consistent improvement of the severity\nof the network's errors with regard to the cost matrix. Furthermore, when the\ninduced metric contains insight on the data structure, our approach improves\nthe overall precision as well. Experiments on four different public datasets --\nfrom agricultural time series classification to depth image semantic\nsegmentation -- validate our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:22:08 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 15:08:46 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Garnot", "Vivien Sainte Fare", ""], ["Landrieu", "Loic", ""]]}, {"id": "2007.03052", "submitter": "Yuhang Lu", "authors": "Yuhang Lu, Weijian Li, Kang Zheng, Yirui Wang, Adam P. Harrison,\n  Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao", "title": "Learning to Segment Anatomical Structures Accurately from One Exemplar", "comments": "MICCAI2020 (Early accept)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of critical anatomical structures is at the core of\nmedical image analysis. The main bottleneck lies in gathering the requisite\nexpert-labeled image annotations in a scalable manner. Methods that permit to\nproduce accurate anatomical structure segmentation without using a large amount\nof fully annotated training images are highly desirable. In this work, we\npropose a novel contribution of Contour Transformer Network (CTN), a one-shot\nanatomy segmentor including a naturally built-in human-in-the-loop mechanism.\nSegmentation is formulated by learning a contour evolution behavior process\nbased on graph convolutional networks (GCNs). Training of our CTN model\nrequires only one labeled image exemplar and leverages additional unlabeled\ndata through newly introduced loss functions that measure the global shape and\nappearance consistency of contours. We demonstrate that our one-shot learning\nmethod significantly outperforms non-learning-based methods and performs\ncompetitively to the state-of-the-art fully supervised deep learning\napproaches. With minimal human-in-the-loop editing feedback, the segmentation\nperformance can be further improved and tailored towards the observer desired\noutcomes. This can facilitate the clinician designed imaging-based biomarker\nassessments (to support personalized quantitative clinical diagnosis) and\noutperforms fully supervised baselines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:27:38 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 01:00:27 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lu", "Yuhang", ""], ["Li", "Weijian", ""], ["Zheng", "Kang", ""], ["Wang", "Yirui", ""], ["Harrison", "Adam P.", ""], ["Lin", "Chihung", ""], ["Wang", "Song", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Kuo", "Chang-Fu", ""], ["Miao", "Shun", ""]]}, {"id": "2007.03053", "submitter": "Mohammad Saeed Rad", "authors": "Mohammad Saeed Rad, Thomas Yu, Claudiu Musat, Hazim Kemal Ekenel,\n  Behzad Bozorgtabar, Jean-Philippe Thiran", "title": "Benefiting from Bicubically Down-Sampled Images for Learning Real-World\n  Image Super-Resolution", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) has traditionally been based on pairs of\nhigh-resolution images (HR) and their low-resolution (LR) counterparts obtained\nartificially with bicubic downsampling. However, in real-world SR, there is a\nlarge variety of realistic image degradations and analytically modeling these\nrealistic degradations can prove quite difficult. In this work, we propose to\nhandle real-world SR by splitting this ill-posed problem into two comparatively\nmore well-posed steps. First, we train a network to transform real LR images to\nthe space of bicubically downsampled images in a supervised manner, by using\nboth real LR/HR pairs and synthetic pairs. Second, we take a generic SR network\ntrained on bicubically downsampled images to super-resolve the transformed LR\nimage. The first step of the pipeline addresses the problem by registering the\nlarge variety of degraded images to a common, well understood space of images.\nThe second step then leverages the already impressive performance of SR on\nbicubically downsampled images, sidestepping the issues of end-to-end training\non datasets with many different image degradations. We demonstrate the\neffectiveness of our proposed method by comparing it to recent methods in\nreal-world SR and show that our proposed approach outperforms the\nstate-of-the-art works in terms of both qualitative and quantitative results,\nas well as results of an extensive user study conducted on several real image\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:27:58 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 18:25:16 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Rad", "Mohammad Saeed", ""], ["Yu", "Thomas", ""], ["Musat", "Claudiu", ""], ["Ekenel", "Hazim Kemal", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2007.03056", "submitter": "Srijan Das", "authors": "Srijan Das, Saurav Sharma, Rui Dai, Francois Bremond, Monique Thonnat", "title": "VPN: Learning Video-Pose Embedding for Activities of Daily Living", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the spatio-temporal aspect of recognizing\nActivities of Daily Living (ADL). ADL have two specific properties (i) subtle\nspatio-temporal patterns and (ii) similar visual patterns varying with time.\nTherefore, ADL may look very similar and often necessitate to look at their\nfine-grained details to distinguish them. Because the recent spatio-temporal 3D\nConvNets are too rigid to capture the subtle visual patterns across an action,\nwe propose a novel Video-Pose Network: VPN. The 2 key components of this VPN\nare a spatial embedding and an attention network. The spatial embedding\nprojects the 3D poses and RGB cues in a common semantic space. This enables the\naction recognition framework to learn better spatio-temporal features\nexploiting both modalities. In order to discriminate similar actions, the\nattention network provides two functionalities - (i) an end-to-end learnable\npose backbone exploiting the topology of human body, and (ii) a coupler to\nprovide joint spatio-temporal attention weights across a video. Experiments\nshow that VPN outperforms the state-of-the-art results for action\nclassification on a large scale human activity dataset: NTU-RGB+D 120, its\nsubset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota\nSmarthome and a small scale human-object interaction dataset Northwestern UCLA.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:39:08 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Das", "Srijan", ""], ["Sharma", "Saurav", ""], ["Dai", "Rui", ""], ["Bremond", "Francois", ""], ["Thonnat", "Monique", ""]]}, {"id": "2007.03059", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, George Drettakis and Adrien Bousseau", "title": "Guided Fine-Tuning for Large-Scale Material Transfer", "comments": "Published in Computer Graphics Forum, 39(4); Proceedings of the\n  Eurographics Symposium on Rendering 2020", "journal-ref": null, "doi": "10.1111/cgf.14056", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to transfer the appearance of one or a few exemplar\nSVBRDFs to a target image representing similar materials. Our solution is\nextremely simple: we fine-tune a deep appearance-capture network on the\nprovided exemplars, such that it learns to extract similar SVBRDF values from\nthe target image. We introduce two novel material capture and design workflows\nthat demonstrate the strength of this simple approach. Our first workflow\nallows to produce plausible SVBRDFs of large-scale objects from only a few\npictures. Specifically, users only need take a single picture of a large\nsurface and a few close-up flash pictures of some of its details. We use\nexisting methods to extract SVBRDF parameters from the close-ups, and our\nmethod to transfer these parameters to the entire surface, enabling the\nlightweight capture of surfaces several meters wide such as murals, floors and\nfurniture. In our second workflow, we provide a powerful way for users to\ncreate large SVBRDFs from internet pictures by transferring the appearance of\nexisting, pre-designed SVBRDFs. By selecting different exemplars, users can\ncontrol the materials assigned to the target image, greatly enhancing the\ncreative possibilities offered by deep appearance capture.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:55:37 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:12:21 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Drettakis", "George", ""], ["Bousseau", "Adrien", ""]]}, {"id": "2007.03073", "submitter": "Jiayi Wang", "authors": "Jiayi Wang, Franziska Mueller, Florian Bernard, Christian Theobalt", "title": "Generative Model-Based Loss to the Rescue: A Method to Overcome\n  Annotation Errors for Depth-Based Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use a model-based generative loss for training hand pose\nestimators on depth images based on a volumetric hand model. This additional\nloss allows training of a hand pose estimator that accurately infers the entire\nset of 21 hand keypoints while only using supervision for 6 easy-to-annotate\nkeypoints (fingertips and wrist). We show that our partially-supervised method\nachieves results that are comparable to those of fully-supervised methods which\nenforce articulation consistency. Moreover, for the first time we demonstrate\nthat such an approach can be used to train on datasets that have erroneous\nannotations, i.e. \"ground truth\" with notable measurement errors, while\nobtaining predictions that explain the depth images better than the given\n\"ground truth\".\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:24:25 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 11:36:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Jiayi", ""], ["Mueller", "Franziska", ""], ["Bernard", "Florian", ""], ["Theobalt", "Christian", ""]]}, {"id": "2007.03074", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Chun-Liang Li, Youssef Mroueh, Yiming Yang", "title": "Kernel Stein Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in gradient-based Explicit Generative Modeling where\nsamples can be derived from iterative gradient updates based on an estimate of\nthe score function of the data distribution. Recent advances in Stochastic\nGradient Langevin Dynamics (SGLD) demonstrates impressive results with\nenergy-based models on high-dimensional and complex data distributions. Stein\nVariational Gradient Descent (SVGD) is a deterministic sampling algorithm that\niteratively transports a set of particles to approximate a given distribution,\nbased on functional gradient descent that decreases the KL divergence. SVGD has\npromising results on several Bayesian inference applications. However, applying\nSVGD on high dimensional problems is still under-explored. The goal of this\nwork is to study high dimensional inference with SVGD. We first identify key\nchallenges in practical kernel SVGD inference in high-dimension. We propose\nnoise conditional kernel SVGD (NCK-SVGD), that works in tandem with the\nrecently introduced Noise Conditional Score Network estimator. NCK is crucial\nfor successful inference with SVGD in high dimension, as it adapts the kernel\nto the noise level of the score estimate. As we anneal the noise, NCK-SVGD\ntargets the real data distribution. We then extend the annealed SVGD with an\nentropic regularization. We show that this offers a flexible control between\nsample quality and diversity, and verify it empirically by precision and recall\nevaluations. The NCK-SVGD produces samples comparable to GANs and annealed SGLD\non computer vision benchmarks, including MNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:26:04 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Li", "Chun-Liang", ""], ["Mroueh", "Youssef", ""], ["Yang", "Yiming", ""]]}, {"id": "2007.03083", "submitter": "Dae Heun Koh", "authors": "Dae Heun Koh, Pierre C\\^ote de Soux, Laura Domin\\'e, Fran\\c{c}ois\n  Drielsma, Ran Itay, Qing Lin, Kazuhiro Terao, Ka Vang Tsang, Tracy Usher (for\n  the DeepLearnPhysics Collaboration)", "title": "Scalable, Proposal-free Instance Segmentation Network for 3D Pixel\n  Clustering and Particle Trajectory Reconstruction in Liquid Argon Time\n  Projection Chambers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid Argon Time Projection Chambers (LArTPCs) are high resolution particle\nimaging detectors, employed by accelerator-based neutrino oscillation\nexperiments for high precision physics measurements. While images of particle\ntrajectories are intuitive to analyze for physicists, the development of a high\nquality, automated data reconstruction chain remains challenging. One of the\nmost critical reconstruction steps is particle clustering: the task of grouping\n3D image pixels into different particle instances that share the same particle\ntype. In this paper, we propose the first scalable deep learning algorithm for\nparticle clustering in LArTPC data using sparse convolutional neural networks\n(SCNN). Building on previous works on SCNNs and proposal free instance\nsegmentation, we build an end-to-end trainable instance segmentation network\nthat learns an embedding of the image pixels to perform point cloud clustering\nin a transformed space. We benchmark the performance of our algorithm on\nPILArNet, a public 3D particle imaging dataset, with respect to common\nclustering evaluation metrics. 3D pixels were successfully clustered into\nindividual particle trajectories with 90% of them having an adjusted Rand index\nscore greater than 92% with a mean pixel clustering efficiency and purity above\n96%. This work contributes to the development of an end-to-end optimizable full\ndata reconstruction chain for LArTPCs, in particular pixel-based 3D imaging\ndetectors including the near detector of the Deep Underground Neutrino\nExperiment. Our algorithm is made available in the open access repository, and\nwe share our Singularity software container, which can be used to reproduce our\nwork on the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:37:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Koh", "Dae Heun", "", "for\n  the DeepLearnPhysics Collaboration"], ["de Soux", "Pierre C\u00f4te", "", "for\n  the DeepLearnPhysics Collaboration"], ["Domin\u00e9", "Laura", "", "for\n  the DeepLearnPhysics Collaboration"], ["Drielsma", "Fran\u00e7ois", "", "for\n  the DeepLearnPhysics Collaboration"], ["Itay", "Ran", "", "for\n  the DeepLearnPhysics Collaboration"], ["Lin", "Qing", "", "for\n  the DeepLearnPhysics Collaboration"], ["Terao", "Kazuhiro", "", "for\n  the DeepLearnPhysics Collaboration"], ["Tsang", "Ka Vang", "", "for\n  the DeepLearnPhysics Collaboration"], ["Usher", "Tracy", "", "for\n  the DeepLearnPhysics Collaboration"]]}, {"id": "2007.03085", "submitter": "Divyansh Garg", "authors": "Divyansh Garg, Yan Wang, Bharath Hariharan, Mark Campbell, Kilian Q.\n  Weinberger and Wei-Lun Chao", "title": "Wasserstein Distances for Stereo Disparity Estimation", "comments": "Accepted to NeurIPS 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to depth or disparity estimation output a distribution\nover a set of pre-defined discrete values. This leads to inaccurate results\nwhen the true depth or disparity does not match any of these values. The fact\nthat this distribution is usually learned indirectly through a regression loss\ncauses further problems in ambiguous regions around object boundaries. We\naddress these issues using a new neural network architecture that is capable of\noutputting arbitrary depth values, and a new loss function that is derived from\nthe Wasserstein distance between the true and the predicted distributions. We\nvalidate our approach on a variety of tasks, including stereo disparity and\ndepth estimation, and the downstream 3D object detection. Our approach\ndrastically reduces the error in ambiguous regions, especially around object\nboundaries that greatly affect the localization of objects in 3D, achieving the\nstate-of-the-art in 3D object detection for autonomous driving. Our code will\nbe available at https://github.com/Div99/W-Stereo-Disp.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:37:50 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 09:42:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Garg", "Divyansh", ""], ["Wang", "Yan", ""], ["Hariharan", "Bharath", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2007.03098", "submitter": "Jiri Matas", "authors": "Kl\\'ara Janou\\v{s}kov\\'a, Jiri Matas, Lluis Gomez, Dimosthenis\n  Karatzas", "title": "Text Recognition -- Real World Data and Where to Find Them", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for exploiting weakly annotated images to improve text\nextraction pipelines. The approach uses an arbitrary end-to-end text\nrecognition system to obtain text region proposals and their, possibly\nerroneous, transcriptions. The proposed method includes matching of imprecise\ntranscription to weak annotations and edit distance guided neighbourhood\nsearch. It produces nearly error-free, localised instances of scene text, which\nwe treat as \"pseudo ground truth\" (PGT).\n  We apply the method to two weakly-annotated datasets. Training with the\nextracted PGT consistently improves the accuracy of a state of the art\nrecognition model, by 3.7~\\% on average, across different benchmark datasets\n(image domains) and 24.5~\\% on one of the weakly annotated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:23:27 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 15:07:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Janou\u0161kov\u00e1", "Kl\u00e1ra", ""], ["Matas", "Jiri", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "2007.03107", "submitter": "Vittorio Mazzia", "authors": "Francesco Salvetti, Vittorio Mazzia, Aleem Khaliq, Marcello Chiaberge", "title": "Multi-image Super Resolution of Remotely Sensed Images using Residual\n  Feature Attention Deep Neural Networks", "comments": null, "journal-ref": "Remote Sens. 2020, 12(14), 2207", "doi": "10.3390/rs12142207", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been consistently proved\nstate-of-the-art results in image Super-Resolution (SR), representing an\nexceptional opportunity for the remote sensing field to extract further\ninformation and knowledge from captured data. However, most of the works\npublished in the literature have been focusing on the Single-Image\nSuper-Resolution problem so far. At present, satellite based remote sensing\nplatforms offer huge data availability with high temporal resolution and low\nspatial resolution. In this context, the presented research proposes a novel\nresidual attention model (RAMS) that efficiently tackles the multi-image\nsuper-resolution task, simultaneously exploiting spatial and temporal\ncorrelations to combine multiple images. We introduce the mechanism of visual\nfeature attention with 3D convolutions in order to obtain an aware data fusion\nand information extraction of the multiple low-resolution images, transcending\nlimitations of the local region of convolutional operations. Moreover, having\nmultiple inputs with the same scene, our representation learning network makes\nextensive use of nestled residual connections to let flow redundant\nlow-frequency signals and focus the computation on more important\nhigh-frequency components. Extensive experimentation and evaluations against\nother available solutions, either for single or multi-image super-resolution,\nhave demonstrated that the proposed deep learning-based solution can be\nconsidered state-of-the-art for Multi-Image Super-Resolution for remote sensing\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:54:02 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 10:02:13 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Salvetti", "Francesco", ""], ["Mazzia", "Vittorio", ""], ["Khaliq", "Aleem", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2007.03123", "submitter": "Kalun Ho", "authors": "Kalun Ho, Janis Keuper, Franz-Josef Pfreundt and Margret Keuper", "title": "Learning Embeddings for Image Clustering: An Empirical Study of Triplet\n  Loss Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we evaluate two different image clustering objectives, k-means\nclustering and correlation clustering, in the context of Triplet Loss induced\nfeature space embeddings. Specifically, we train a convolutional neural network\nto learn discriminative features by optimizing two popular versions of the\nTriplet Loss in order to study their clustering properties under the assumption\nof noisy labels. Additionally, we propose a new, simple Triplet Loss\nformulation, which shows desirable properties with respect to formal clustering\nobjectives and outperforms the existing methods. We evaluate all three Triplet\nloss formulations for K-means and correlation clustering on the CIFAR-10 image\nclassification dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:38:14 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ho", "Kalun", ""], ["Keuper", "Janis", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Margret", ""]]}, {"id": "2007.03141", "submitter": "Yuan Wu", "authors": "Yuan Wu, Diana Inkpen and Ahmed El-Roby", "title": "Dual Mixup Regularized Learning for Adversarial Domain Adaptation", "comments": "This paper has been accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on unsupervised domain adaptation (UDA) rely on adversarial\nlearning to disentangle the explanatory and transferable features for domain\nadaptation. However, there are two issues with the existing methods. First, the\ndiscriminability of the latent space cannot be fully guaranteed without\nconsidering the class-aware information in the target domain. Second, samples\nfrom the source and target domains alone are not sufficient for\ndomain-invariant feature extracting in the latent space. In order to alleviate\nthe above issues, we propose a dual mixup regularized learning (DMRL) method\nfor UDA, which not only guides the classifier in enhancing consistent\npredictions in-between samples, but also enriches the intrinsic structures of\nthe latent space. The DMRL jointly conducts category and domain mixup\nregularizations on pixel level to improve the effectiveness of models. A series\nof empirical studies on four domain adaptation benchmarks demonstrate that our\napproach can achieve the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 00:24:14 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 22:01:18 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wu", "Yuan", ""], ["Inkpen", "Diana", ""], ["El-Roby", "Ahmed", ""]]}, {"id": "2007.03154", "submitter": "Yunjie Tian", "authors": "Yunjie Tian, Chang Liu, Lingxi Xie, Jianbin Jiao, Qixiang Ye", "title": "Discretization-Aware Architecture Search", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search cost of neural architecture search (NAS) has been largely reduced\nby weight-sharing methods. These methods optimize a super-network with all\npossible edges and operations, and determine the optimal sub-network by\ndiscretization, \\textit{i.e.}, pruning off weak candidates. The discretization\nprocess, performed on either operations or edges, incurs significant inaccuracy\nand thus the quality of the final architecture is not guaranteed. This paper\npresents discretization-aware architecture search (DA\\textsuperscript{2}S),\nwith the core idea being adding a loss term to push the super-network towards\nthe configuration of desired topology, so that the accuracy loss brought by\ndiscretization is largely alleviated. Experiments on standard image\nclassification benchmarks demonstrate the superiority of our approach, in\nparticular, under imbalanced target network configurations that were not\nstudied before.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:18:58 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Tian", "Yunjie", ""], ["Liu", "Chang", ""], ["Xie", "Lingxi", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "2007.03162", "submitter": "Yufan He", "authors": "Yufan He, Aaron Carass, Lianrui Zuo, Blake E. Dewey and Jerry L.\n  Prince", "title": "Self domain adapted network", "comments": "early accept in miccai2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain shift is a major problem for deploying deep networks in clinical\npractice. Network performance drops significantly with (target) images obtained\ndifferently than its (source) training data. Due to a lack of target label\ndata, most work has focused on unsupervised domain adaptation (UDA). Current\nUDA methods need both source and target data to train models which perform\nimage translation (harmonization) or learn domain-invariant features. However,\ntraining a model for each target domain is time consuming and computationally\nexpensive, even infeasible when target domain data are scarce or source data\nare unavailable due to data privacy. In this paper, we propose a novel self\ndomain adapted network (SDA-Net) that can rapidly adapt itself to a single test\nsubject at the testing stage, without using extra data or training a UDA model.\nThe SDA-Net consists of three parts: adaptors, task model, and auto-encoders.\nThe latter two are pre-trained offline on labeled source images. The task model\nperforms tasks like synthesis, segmentation, or classification, which may\nsuffer from the domain shift problem. At the testing stage, the adaptors are\ntrained to transform the input test image and features to reduce the domain\nshift as measured by the auto-encoders, and thus perform domain adaptation. We\nvalidated our method on retinal layer segmentation from different OCT scanners\nand T1 to T2 synthesis with T1 from different MRI scanners and with different\nimaging parameters. Results show that our SDA-Net, with a single test subject\nand a short amount of time for self adaptation at the testing stage, can\nachieve significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:41:34 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["He", "Yufan", ""], ["Carass", "Aaron", ""], ["Zuo", "Lianrui", ""], ["Dewey", "Blake E.", ""], ["Prince", "Jerry L.", ""]]}, {"id": "2007.03169", "submitter": "Dongsu Zhang", "authors": "Dongsu Zhang, Junha Chun, Sang Kyun Cha, Young Min Kim", "title": "Spatial Semantic Embedding Network: Fast 3D Instance Segmentation with\n  Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose spatial semantic embedding network (SSEN), a simple, yet efficient\nalgorithm for 3D instance segmentation using deep metric learning. The raw 3D\nreconstruction of an indoor environment suffers from occlusions, noise, and is\nproduced without any meaningful distinction between individual entities. For\nhigh-level intelligent tasks from a large scale scene, 3D instance segmentation\nrecognizes individual instances of objects. We approach the instance\nsegmentation by simply learning the correct embedding space that maps\nindividual instances of objects into distinct clusters that reflect both\nspatial and semantic information. Unlike previous approaches that require\ncomplex pre-processing or post-processing, our implementation is compact and\nfast with competitive performance, maintaining scalability on large scenes with\nhigh resolution voxels. We demonstrate the state-of-the-art performance of our\nalgorithm in the ScanNet 3D instance segmentation benchmark on AP score.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 02:17:44 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zhang", "Dongsu", ""], ["Chun", "Junha", ""], ["Cha", "Sang Kyun", ""], ["Kim", "Young Min", ""]]}, {"id": "2007.03195", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi, Rajeev Yasarla, Deepak Sam Babu, R. Venkatesh\n  Babu, Vishal M. Patel", "title": "Learning to Count in the Crowd from Limited Labeled Data", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent crowd counting approaches have achieved excellent performance.\nHowever, they are essentially based on fully supervised paradigm and require\nlarge number of annotated samples. Obtaining annotations is an expensive and\nlabour-intensive process. In this work, we focus on reducing the annotation\nefforts by learning to count in the crowd from limited number of labeled\nsamples while leveraging a large pool of unlabeled data. Specifically, we\npropose a Gaussian Process-based iterative learning mechanism that involves\nestimation of pseudo-ground truth for the unlabeled data, which is then used as\nsupervision for training the network. The proposed method is shown to be\neffective under the reduced data (semi-supervised) settings for several\ndatasets like ShanghaiTech, UCF-QNRF, WorldExpo, UCSD, etc. Furthermore, we\ndemonstrate that the proposed method can be leveraged to enable the network in\nlearning to count from synthetic dataset while being able to generalize better\nto real-world datasets (synthetic-to-real transfer).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 04:17:01 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:01:17 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Yasarla", "Rajeev", ""], ["Babu", "Deepak Sam", ""], ["Babu", "R. Venkatesh", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2007.03198", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Jonathan Peck, Wesley De Neve, Bart Goossens, Yvan Saeys\n  and Arnout Van Messem", "title": "Regional Image Perturbation Reduces $L_p$ Norms of Adversarial Examples\n  While Maintaining Model-to-model Transferability", "comments": "Accepted for the ICML 2020, Workshop on Uncertainty and Robustness in\n  Deep Learning (UDL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional adversarial attacks often rely on complicated methods for generating\nadversarial perturbations, making it hard to compare their efficacy against\nwell-known attacks. In this study, we show that effective regional\nperturbations can be generated without resorting to complex methods. We develop\na very simple regional adversarial perturbation attack method using\ncross-entropy sign, one of the most commonly used losses in adversarial machine\nlearning. Our experiments on ImageNet with multiple models reveal that, on\naverage, $76\\%$ of the generated adversarial examples maintain model-to-model\ntransferability when the perturbation is applied to local image regions.\nDepending on the selected region, these localized adversarial examples require\nsignificantly less $L_p$ norm distortion (for $p \\in \\{0, 2, \\infty\\}$)\ncompared to their non-local counterparts. These localized attacks therefore\nhave the potential to undermine defenses that claim robustness under the\naforementioned norms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 04:33:16 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 08:23:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ozbulak", "Utku", ""], ["Peck", "Jonathan", ""], ["De Neve", "Wesley", ""], ["Goossens", "Bart", ""], ["Saeys", "Yvan", ""], ["Van Messem", "Arnout", ""]]}, {"id": "2007.03199", "submitter": "Hang Min", "authors": "Hang Min, Darryl McClymont, Shekhar S. Chandra, Stuart Crozier and\n  Andrew P. Bradley", "title": "Automatic lesion detection, segmentation and characterization via 3D\n  multiscale morphological sifting in breast MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on computer aided detection/diagnosis (CAD) in 4D breast\nmagnetic resonance imaging (MRI) regard lesion detection, segmentation and\ncharacterization as separate tasks, and typically require users to manually\nselect 2D MRI slices or regions of interest as the input. In this work, we\npresent a breast MRI CAD system that can handle 4D multimodal breast MRI data,\nand integrate lesion detection, segmentation and characterization with no user\nintervention. The proposed CAD system consists of three major stages: region\ncandidate generation, feature extraction and region candidate classification.\nBreast lesions are firstly extracted as region candidates using the novel 3D\nmultiscale morphological sifting (MMS). The 3D MMS, which uses linear\nstructuring elements to extract lesion-like patterns, can segment lesions from\nbreast images accurately and efficiently. Analytical features are then\nextracted from all available 4D multimodal breast MRI sequences, including T1-,\nT2-weighted and DCE sequences, to represent the signal intensity, texture,\nmorphological and enhancement kinetic characteristics of the region candidates.\nThe region candidates are lastly classified as lesion or normal tissue by the\nrandom under-sampling boost (RUSboost), and as malignant or benign lesion by\nthe random forest. Evaluated on a breast MRI dataset which contains a total of\n117 cases with 95 malignant and 46 benign lesions, the proposed system achieves\na true positive rate (TPR) of 0.90 at 3.19 false positives per patient (FPP)\nfor lesion detection and a TPR of 0.91 at a FPP of 2.95 for identifying\nmalignant lesions without any user intervention. The average dice similarity\nindex (DSI) is 0.72 for lesion segmentation. Compared with previously proposed\nsystems evaluated on the same breast MRI dataset, the proposed CAD system\nachieves a favourable performance in breast lesion detection and\ncharacterization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 04:39:13 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Min", "Hang", ""], ["McClymont", "Darryl", ""], ["Chandra", "Shekhar S.", ""], ["Crozier", "Stuart", ""], ["Bradley", "Andrew P.", ""]]}, {"id": "2007.03200", "submitter": "Fan Yang", "authors": "Fan Yang, Xin Chang, Chenyu Dang, Ziqiang Zheng, Sakriani Sakti,\n  Satoshi Nakamura, Yang Wu", "title": "ReMOTS: Self-Supervised Refining Multi-Object Tracking and Segmentation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to improve the performance of Multiple Object Tracking and\nSegmentation (MOTS) by refinement. However, it remains challenging for refining\nMOTS results, which could be attributed to that appearance features are not\nadapted to target videos and it is also difficult to find proper thresholds to\ndiscriminate them. To tackle this issue, we propose a self-supervised refining\nMOTS (i.e., ReMOTS) framework. ReMOTS mainly takes four steps to refine MOTS\nresults from the data association perspective. (1) Training the appearance\nencoder using predicted masks. (2) Associating observations across adjacent\nframes to form short-term tracklets. (3) Training the appearance encoder using\nshort-term tracklets as reliable pseudo labels. (4) Merging short-term\ntracklets to long-term tracklets utilizing adopted appearance features and\nthresholds that are automatically obtained from statistical information. Using\nReMOTS, we reached the $1^{st}$ place on CVPR 2020 MOTS Challenge 1, with an\nsMOTSA score of $69.9$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 04:49:10 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 03:48:50 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 12:38:59 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yang", "Fan", ""], ["Chang", "Xin", ""], ["Dang", "Chenyu", ""], ["Zheng", "Ziqiang", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""], ["Wu", "Yang", ""]]}, {"id": "2007.03207", "submitter": "Yinjie Lei", "authors": "Yan Liu, Lingqiao Liu, Peng Wang, Pingping Zhang, and Yinjie Lei", "title": "Semi-Supervised Crowd Counting via Self-Training on Surrogate Tasks", "comments": "To be appeared in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing crowd counting systems rely on the availability of the object\nlocation annotation which can be expensive to obtain. To reduce the annotation\ncost, one attractive solution is to leverage a large number of unlabeled images\nto build a crowd counting model in semi-supervised fashion. This paper tackles\nthe semi-supervised crowd counting problem from the perspective of feature\nlearning. Our key idea is to leverage the unlabeled images to train a generic\nfeature extractor rather than the entire network of a crowd counter. The\nrationale of this design is that learning the feature extractor can be more\nreliable and robust towards the inevitable noisy supervision generated from the\nunlabeled data. Also, on top of a good feature extractor, it is possible to\nbuild a density map regressor with much fewer density map annotations.\nSpecifically, we proposed a novel semi-supervised crowd counting method which\nis built upon two innovative components: (1) a set of inter-related binary\nsegmentation tasks are derived from the original density map regression task as\nthe surrogate prediction target; (2) the surrogate target predictors are\nlearned from both labeled and unlabeled data by utilizing a proposed\nself-training scheme which fully exploits the underlying constraints of these\nbinary segmentation tasks. Through experiments, we show that the proposed\nmethod is superior over the existing semisupervised crowd counting method and\nother representative baselines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:30:53 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 06:40:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Yan", ""], ["Liu", "Lingqiao", ""], ["Wang", "Peng", ""], ["Zhang", "Pingping", ""], ["Lei", "Yinjie", ""]]}, {"id": "2007.03218", "submitter": "Anuraganand Sharma Dr", "authors": "Anuraganand Sharma, Dinesh Kumar", "title": "Classification with 2-D Convolutional Neural Networks for breast cancer\n  diagnosis", "comments": "26 pages - preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Breast cancer is the most common cancer in women. Classification of\ncancer/non-cancer patients with clinical records requires high sensitivity and\nspecificity for an acceptable diagnosis test. The state-of-the-art\nclassification model - Convolutional Neural Network (CNN), however, cannot be\nused with clinical data that are represented in 1-D format. CNN has been\ndesigned to work on a set of 2-D matrices whose elements show some correlation\nwith neighboring elements such as in image data. Conversely, the data examples\nrepresented as a set of 1-D vectors -- apart from the time series data --\ncannot be used with CNN, but with other classification models such as\nArtificial Neural Networks or RandomForest. We have proposed some novel\npreprocessing methods of data wrangling that transform a 1-D data vector, to a\n2-D graphical image with appropriate correlations among the fields to be\nprocessed on CNN. We tested our methods on Wisconsin Original Breast Cancer\n(WBC) and Wisconsin Diagnostic Breast Cancer (WDBC) datasets. To our knowledge,\nthis work is novel on non-image to image data transformation for the non-time\nseries data. The transformed data processed with CNN using VGGnet-16 shows\ncompetitive results for the WBC dataset and outperforms other known methods for\nthe WDBC dataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:08:06 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 00:24:24 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Sharma", "Anuraganand", ""], ["Kumar", "Dinesh", ""]]}, {"id": "2007.03221", "submitter": "Banghuai Li", "authors": "Zixuan Xu, Banghuai Li, Miao Geng, Ye Yuan", "title": "AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses", "comments": "To appear in AAAI 2021", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark localization aims to detect the predefined points of human\nfaces, and the topic has been rapidly improved with the recent development of\nneural network based methods. However, it remains a challenging task when\ndealing with faces in unconstrained scenarios, especially with large pose\nvariations. In this paper, we target the problem of facial landmark\nlocalization across large poses and address this task based on a\nsplit-and-aggregate strategy. To split the search space, we propose a set of\nanchor templates as references for regression, which well addresses the large\nvariations of face poses. Based on the prediction of each anchor template, we\npropose to aggregate the results, which can reduce the landmark uncertainty due\nto the large poses. Overall, our proposed approach, named AnchorFace, obtains\nstate-of-the-art results with extremely efficient inference speed on four\nchallenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be\navailable at https://github.com/nothingelse92/AnchorFace.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:17:08 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 03:15:56 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 07:11:57 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Xu", "Zixuan", ""], ["Li", "Banghuai", ""], ["Geng", "Miao", ""], ["Yuan", "Ye", ""]]}, {"id": "2007.03227", "submitter": "Rafael Makrigiorgis", "authors": "Rafael Makrigiorgis, Panayiotis Kolios, Stelios Timotheou, Theocharis\n  Theocharides, Christos G. Panayiotou", "title": "Extracting the fundamental diagram from aerial footage", "comments": "5 pages, 7 figures, 2020 IEEE 91st Vehicular Technology Conference\n  (VTC2020-Spring)", "journal-ref": null, "doi": "10.1109/VTC2020-Spring48590.2020.9128534", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient traffic monitoring is playing a fundamental role in successfully\ntackling congestion in transportation networks. Congestion is strongly\ncorrelated with two measurable characteristics, the demand and the network\ndensity that impact the overall system behavior. At large, this system behavior\nis characterized through the fundamental diagram of a road segment, a region or\nthe network. In this paper we devise an innovative way to obtain the\nfundamental diagram through aerial footage obtained from drone platforms. The\nderived methodology consists of 3 phases: vehicle detection, vehicle tracking\nand traffic state estimation. We elaborate on the algorithms developed for each\nof the 3 phases and demonstrate the applicability of the results in a\nreal-world setting.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:34:50 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 13:40:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Makrigiorgis", "Rafael", ""], ["Kolios", "Panayiotis", ""], ["Timotheou", "Stelios", ""], ["Theocharides", "Theocharis", ""], ["Panayiotou", "Christos G.", ""]]}, {"id": "2007.03230", "submitter": "Li-Huang Tsai", "authors": "Li-Huang Tsai, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei and\n  Da-Cheng Juan", "title": "Robust Processing-In-Memory Neural Networks via Noise-Aware\n  Normalization", "comments": "7 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analog computing hardwares, such as Processing-in-memory (PIM) accelerators,\nhave gradually received more attention for accelerating the neural network\ncomputations. However, PIM accelerators often suffer from intrinsic noise in\nthe physical components, making it challenging for neural network models to\nachieve the same performance as on the digital hardware. Previous works in\nmitigating intrinsic noise assumed the knowledge of the noise model, and\nretraining the neural networks accordingly was required. In this paper, we\npropose a noise-agnostic method to achieve robust neural network performance\nagainst any noise setting. Our key observation is that the degradation of\nperformance is due to the distribution shifts in network activations, which are\ncaused by the noise. To properly track the shifts and calibrate the biased\ndistributions, we propose a \"noise-aware\" batch normalization layer, which is\nable to align the distributions of the activations under variational noise\ninherent in the analog environments. Our method is simple, easy to implement,\ngeneral to various noise settings, and does not need to retrain the models. We\nconduct experiments on several tasks in computer vision, including\nclassification, object detection and semantic segmentation. The results\ndemonstrate the effectiveness of our method, achieving robust performance under\na wide range of noise settings, more reliable than existing methods. We believe\nthat our simple yet general method can facilitate the adoption of analog\ncomputing devices for neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:51:28 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 05:35:46 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Tsai", "Li-Huang", ""], ["Chang", "Shih-Chieh", ""], ["Chen", "Yu-Ting", ""], ["Pan", "Jia-Yu", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "2007.03241", "submitter": "Bichuan Guo", "authors": "Yanghao Li, Bichuan Guo, Jiangtao Wen, Zhen Xia, Shan Liu, Yuxing Han", "title": "Learning Model-Blind Temporal Denoisers without Ground Truths", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoisers trained with synthetic data often fail to cope with the diversity\nof unknown noises, giving way to methods that can adapt to existing noise\nwithout knowing its ground truth. Previous image-based method leads to noise\noverfitting if directly applied to video denoisers, and has inadequate temporal\ninformation management especially in terms of occlusion and lighting variation,\nwhich considerably hinders its denoising performance. In this paper, we propose\na general framework for video denoising networks that successfully addresses\nthese challenges. A novel twin sampler assembles training data by decoupling\ninputs from targets without altering semantics, which not only effectively\nsolves the noise overfitting problem, but also generates better occlusion masks\nefficiently by checking optical flow consistency. An online denoising scheme\nand a warping loss regularizer are employed for better temporal alignment.\nLighting variation is quantified based on the local similarity of aligned\nframes. Our method consistently outperforms the prior art by 0.6-3.2dB PSNR on\nmultiple noises, datasets and network architectures. State-of-the-art results\non reducing model-blind video noises are achieved. Extensive ablation studies\nare conducted to demonstrate the significance of each technical components.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:19:48 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 13:51:22 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Yanghao", ""], ["Guo", "Bichuan", ""], ["Wen", "Jiangtao", ""], ["Xia", "Zhen", ""], ["Liu", "Shan", ""], ["Han", "Yuxing", ""]]}, {"id": "2007.03260", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen\n  Guo, Guiguang Ding", "title": "Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter\npruning), which aims to slim down a convolutional neural network (CNN) by\nreducing the width (number of output channels) of convolutional layers.\nInspired by the neurobiology research about the independence of remembering and\nforgetting, we propose to re-parameterize a CNN into the remembering parts and\nforgetting parts, where the former learn to maintain the performance and the\nlatter learn for efficiency. By training the re-parameterized model using\nregular SGD on the former but a novel update rule with penalty gradients on the\nlatter, we realize structured sparsity, enabling us to equivalently convert the\nre-parameterized model into the original architecture with narrower layers.\nSuch a methodology distinguishes ResRep from the traditional learning-based\npruning paradigm that applies a penalty on parameters to produce structured\nsparsity, which may suppress the parameters essential for the remembering. Our\nmethod slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a\nnarrower one with only 45% FLOPs and no accuracy drop, which is the first to\nachieve lossless pruning with such a high compression ratio, to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:56:45 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:37:57 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 14:37:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ding", "Xiaohan", ""], ["Hao", "Tianxiang", ""], ["Tan", "Jianchao", ""], ["Liu", "Ji", ""], ["Han", "Jungong", ""], ["Guo", "Yuchen", ""], ["Ding", "Guiguang", ""]]}, {"id": "2007.03262", "submitter": "Chenglong Li", "authors": "Zhengzheng Tu, Yan Ma, Zhun Li, Chenglong Li, Jieming Xu, Yongtao Liu", "title": "RGBT Salient Object Detection: A Large-scale Dataset and Benchmark", "comments": "12 pages, 10 figures\n  https://github.com/lz118/RGBT-Salient-Object-Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Salient object detection in complex scenes and environments is a challenging\nresearch topic. Most works focus on RGB-based salient object detection, which\nlimits its performance of real-life applications when confronted with adverse\nconditions such as dark environments and complex backgrounds. Taking advantage\nof RGB and thermal infrared images becomes a new research direction for\ndetecting salient object in complex scenes recently, as thermal infrared\nspectrum imaging provides the complementary information and has been applied to\nmany computer vision tasks. However, current research for RGBT salient object\ndetection is limited by the lack of a large-scale dataset and comprehensive\nbenchmark. This work contributes such a RGBT image dataset named VT5000,\nincluding 5000 spatially aligned RGBT image pairs with ground truth\nannotations. VT5000 has 11 challenges collected in different scenes and\nenvironments for exploring the robustness of algorithms. With this dataset, we\npropose a powerful baseline approach, which extracts multi-level features\nwithin each modality and aggregates these features of all modalities with the\nattention mechanism, for accurate RGBT salient object detection. Extensive\nexperiments show that the proposed baseline approach outperforms the\nstate-of-the-art methods on VT5000 dataset and other two public datasets. In\naddition, we carry out a comprehensive analysis of different algorithms of RGBT\nsalient object detection on VT5000 dataset, and then make several valuable\nconclusions and provide some potential research directions for RGBT salient\nobject detection.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:58:14 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 02:17:41 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 07:18:44 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2020 02:07:42 GMT"}, {"version": "v5", "created": "Wed, 18 Nov 2020 12:27:14 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tu", "Zhengzheng", ""], ["Ma", "Yan", ""], ["Li", "Zhun", ""], ["Li", "Chenglong", ""], ["Xu", "Jieming", ""], ["Liu", "Yongtao", ""]]}, {"id": "2007.03263", "submitter": "Lei Shi", "authors": "Lei Shi, Yifan Zhang, Jian Cheng and Hanqing Lu", "title": "Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dynamic skeletal data, represented as the 2D/3D coordinates of human joints,\nhas been widely studied for human action recognition due to its high-level\nsemantic information and environmental robustness. However, previous methods\nheavily rely on designing hand-crafted traversal rules or graph topologies to\ndraw dependencies between the joints, which are limited in performance and\ngeneralizability. In this work, we present a novel decoupled spatial-temporal\nattention network(DSTA-Net) for skeleton-based action recognition. It involves\nsolely the attention blocks, allowing for modeling spatial-temporal\ndependencies between joints without the requirement of knowing their positions\nor mutual connections. Specifically, to meet the specific requirements of the\nskeletal data, three techniques are proposed for building attention blocks,\nnamely, spatial-temporal attention decoupling, decoupled position encoding and\nspatial global regularization. Besides, from the data aspect, we introduce a\nskeletal data decoupling technique to emphasize the specific characteristics of\nspace/time and different motion scales, resulting in a more comprehensive\nunderstanding of the human actions.To test the effectiveness of the proposed\nmethod, extensive experiments are conducted on four challenging datasets for\nskeleton-based gesture and action recognition, namely, SHREC, DHG, NTU-60 and\nNTU-120, where DSTA-Net achieves state-of-the-art performance on all of them.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:58:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Shi", "Lei", ""], ["Zhang", "Yifan", ""], ["Cheng", "Jian", ""], ["Lu", "Hanqing", ""]]}, {"id": "2007.03269", "submitter": "Yashwant Temburu Kumar", "authors": "Prathmesh Sawant, Yashwant Temburu, Mandar Datar, Imran Ahmed, Vinayak\n  Shriniwas and Sachin Patkar", "title": "Single Storage Semi-Global Matching for Real Time Depth Processing", "comments": "10 pages, Published in National Conference on Computer Vision,\n  Pattern Recognition, Image Processing and Graphics(NCVPRIPG) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth-map is the key computation in computer vision and robotics. One of the\nmost popular approach is via computation of disparity-map of images obtained\nfrom Stereo Camera. Semi Global Matching (SGM) method is a popular choice for\ngood accuracy with reasonable computation time. To use such compute-intensive\nalgorithms for real-time applications such as for autonomous aerial vehicles,\nblind Aid, etc. acceleration using GPU, FPGA is necessary. In this paper, we\nshow the design and implementation of a stereo-vision system, which is based on\nFPGA-implementation of More Global Matching(MGM). MGM is a variant of SGM. We\nuse 4 paths but store a single cumulative cost value for a corresponding pixel.\nOur stereo-vision prototype uses Zedboard containing an ARM-based Zynq-SoC,\nZED-stereo-camera / ELP stereo-camera / Intel RealSense D435i, and VGA for\nvisualization. The power consumption attributed to the custom FPGA-based\nacceleration of disparity map computation required for depth-map is just 0.72\nwatt. The update rate of the disparity map is realistic 10.5 fps.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:12:25 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sawant", "Prathmesh", ""], ["Temburu", "Yashwant", ""], ["Datar", "Mandar", ""], ["Ahmed", "Imran", ""], ["Shriniwas", "Vinayak", ""], ["Patkar", "Sachin", ""]]}, {"id": "2007.03281", "submitter": "Mohammad Idrees Bhat", "authors": "Mohammad Idrees Bhat and B. Sharada", "title": "Spectral Graph-based Features for Recognition of Handwritten Characters:\n  A Case Study on Handwritten Devanagari Numerals", "comments": "16 pages, 8 figures", "journal-ref": "Journal of Intelligent Systems,29,2018,799-813", "doi": "10.1515/jisys-2017-0448", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of different writing styles, unconstrained cursiveness and\nrelationship between different primitive parts is an essential and challenging\ntask for recognition of handwritten characters. As feature representation is\ninadequate, appropriate interpretation/description of handwritten characters\nseems to be a challenging task. Although existing research in handwritten\ncharacters is extensive, it still remains a challenge to get the effective\nrepresentation of characters in feature space. In this paper, we make an\nattempt to circumvent these problems by proposing an approach that exploits the\nrobust graph representation and spectral graph embedding concept to\ncharacterise and effectively represent handwritten characters, taking into\naccount writing styles, cursiveness and relationships. For corroboration of the\nefficacy of the proposed method, extensive experiments were carried out on the\nstandard handwritten numeral Computer Vision Pattern Recognition, Unit of\nIndian Statistical Institute Kolkata dataset. The experimental results\ndemonstrate promising findings, which can be used in future studies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:40:08 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Bhat", "Mohammad Idrees", ""], ["Sharada", "B.", ""]]}, {"id": "2007.03282", "submitter": "Miao Hao", "authors": "Miao Hao, Yitao Liu, Xiangyu Zhang, Jian Sun", "title": "LabelEnc: A New Intermediate Supervision Method for Object Detection", "comments": "To appear in ECCV 2020. Code is available at\n  https://github.com/megvii-model/LabelEnc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new intermediate supervision method, named\nLabelEnc, to boost the training of object detection systems. The key idea is to\nintroduce a novel label encoding function, mapping the ground-truth labels into\nlatent embedding, acting as an auxiliary intermediate supervision to the\ndetection backbone during training. Our approach mainly involves a two-step\ntraining procedure. First, we optimize the label encoding function via an\nAutoEncoder defined in the label space, approximating the \"desired\"\nintermediate representations for the target object detector. Second, taking\nadvantage of the learned label encoding function, we introduce a new auxiliary\nloss attached to the detection backbones, thus benefiting the performance of\nthe derived detector. Experiments show our method improves a variety of\ndetection systems by around 2% on COCO dataset, no matter one-stage or\ntwo-stage frameworks. Moreover, the auxiliary structures only exist during\ntraining, i.e. it is completely cost-free in inference time. Code is available\nat: https://github.com/megvii-model/LabelEnc\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:55:05 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 03:16:25 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 02:37:24 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Hao", "Miao", ""], ["Liu", "Yitao", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2007.03292", "submitter": "Christian Abbet", "authors": "Christian Abbet, and Inti Zlobec, and Behzad Bozorgtabar, and\n  Jean-Philippe Thiran", "title": "Divide-and-Rule: Self-Supervised Learning for Survival Analysis in\n  Colorectal Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the long-term rapid increase in incidences of colorectal cancer (CRC),\nthere is an urgent clinical need to improve risk stratification. The\nconventional pathology report is usually limited to only a few\nhistopathological features. However, most of the tumor microenvironments used\nto describe patterns of aggressive tumor behavior are ignored. In this work, we\naim to learn histopathological patterns within cancerous tissue regions that\ncan be used to improve prognostic stratification for colorectal cancer. To do\nso, we propose a self-supervised learning method that jointly learns a\nrepresentation of tissue regions as well as a metric of the clustering to\nobtain their underlying patterns. These histopathological patterns are then\nused to represent the interaction between complex tissues and predict clinical\noutcomes directly. We furthermore show that the proposed approach can benefit\nfrom linear predictors to avoid overfitting in patient outcomes predictions. To\nthis end, we introduce a new well-characterized clinicopathological dataset,\nincluding a retrospective collective of 374 patients, with their survival time\nand treatment information. Histomorphological clusters obtained by our method\nare evaluated by training survival models. The experimental results demonstrate\nstatistically significant patient stratification, and our approach outperformed\nthe state-of-the-art deep clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:15:36 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Abbet", "Christian", ""], ["Zlobec", "Inti", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2007.03294", "submitter": "Guotai Wang", "authors": "Guotai Wang, Tao Song, Qiang Dong, Mei Cui, Ning Huang, Shaoting Zhang", "title": "Automatic Ischemic Stroke Lesion Segmentation from Computed Tomography\n  Perfusion Images by Image Synthesis and Attention-Based Deep Neural Networks", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.media.2020.101787", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ischemic stroke lesion segmentation from Computed Tomography Perfusion (CTP)\nimages is important for accurate diagnosis of stroke in acute care units.\nHowever, it is challenged by low image contrast and resolution of the perfusion\nparameter maps, in addition to the complex appearance of the lesion. To deal\nwith this problem, we propose a novel framework based on synthesized pseudo\nDiffusion-Weighted Imaging (DWI) from perfusion parameter maps to obtain better\nimage quality for more accurate segmentation. Our framework consists of three\ncomponents based on Convolutional Neural Networks (CNNs) and is trained\nend-to-end. First, a feature extractor is used to obtain both a low-level and\nhigh-level compact representation of the raw spatiotemporal Computed Tomography\nAngiography (CTA) images. Second, a pseudo DWI generator takes as input the\nconcatenation of CTP perfusion parameter maps and our extracted features to\nobtain the synthesized pseudo DWI. To achieve better synthesis quality, we\npropose a hybrid loss function that pays more attention to lesion regions and\nencourages high-level contextual consistency. Finally, we segment the lesion\nregion from the synthesized pseudo DWI, where the segmentation network is based\non switchable normalization and channel calibration for better performance.\nExperimental results showed that our framework achieved the top performance on\nISLES 2018 challenge and: 1) our method using synthesized pseudo DWI\noutperformed methods segmenting the lesion from perfusion parameter maps\ndirectly; 2) the feature extractor exploiting additional spatiotemporal CTA\nimages led to better synthesized pseudo DWI quality and higher segmentation\naccuracy; and 3) the proposed loss functions and network structure improved the\npseudo DWI synthesis and lesion segmentation performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:19:23 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Guotai", ""], ["Song", "Tao", ""], ["Dong", "Qiang", ""], ["Cui", "Mei", ""], ["Huang", "Ning", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2007.03304", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang", "title": "Learning to Generate Novel Domains for Domain Generalization", "comments": "ECCV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on domain generalization (DG), the task of learning from\nmultiple source domains a model that generalizes well to unseen domains. A main\nchallenge for DG is that the available source domains often exhibit limited\ndiversity, hampering the model's ability to learn to generalize. We therefore\nemploy a data generator to synthesize data from pseudo-novel domains to augment\nthe source domains. This explicitly increases the diversity of available\ntraining domains and leads to a more generalizable model. To train the\ngenerator, we model the distribution divergence between source and synthesized\npseudo-novel domains using optimal transport, and maximize the divergence. To\nensure that semantics are preserved in the synthesized data, we further impose\ncycle-consistency and classification losses on the generator. Our method,\nL2A-OT (Learning to Augment by Optimal Transport) outperforms current\nstate-of-the-art DG methods on four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:34:17 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 23:06:41 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 11:50:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""], ["Xiang", "Tao", ""]]}, {"id": "2007.03310", "submitter": "Xiaoze Jiang", "authors": "Xiaoze Jiang, Jing Yu, Yajing Sun, Zengchang Qin, Zihao Zhu, Yue Hu,\n  Qi Wu", "title": "DAM: Deliberation, Abandon and Memory Networks for Generating Detailed\n  and Non-repetitive Responses in Visual Dialogue", "comments": "Accepted by IJCAI 2020. SOLE copyright holder is IJCAI (International\n  Joint Conferences on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Dialogue task requires an agent to be engaged in a conversation with\nhuman about an image. The ability of generating detailed and non-repetitive\nresponses is crucial for the agent to achieve human-like conversation. In this\npaper, we propose a novel generative decoding architecture to generate\nhigh-quality responses, which moves away from decoding the whole encoded\nsemantics towards the design that advocates both transparency and flexibility.\nIn this architecture, word generation is decomposed into a series of\nattention-based information selection steps, performed by the novel recurrent\nDeliberation, Abandon and Memory (DAM) module. Each DAM module performs an\nadaptive combination of the response-level semantics captured from the encoder\nand the word-level semantics specifically selected for generating each word.\nTherefore, the responses contain more detailed and non-repetitive descriptions\nwhile maintaining the semantic accuracy. Furthermore, DAM is flexible to\ncooperate with existing visual dialogue encoders and adaptive to the encoder\nstructures by constraining the information selection mode in DAM. We apply DAM\nto three typical encoders and verify the performance on the VisDial v1.0\ndataset. Experimental results show that the proposed models achieve new\nstate-of-the-art performance with high-quality responses. The code is available\nat https://github.com/JXZe/DAM.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:49:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Jiang", "Xiaoze", ""], ["Yu", "Jing", ""], ["Sun", "Yajing", ""], ["Qin", "Zengchang", ""], ["Zhu", "Zihao", ""], ["Hu", "Yue", ""], ["Wu", "Qi", ""]]}, {"id": "2007.03325", "submitter": "Graham Spinks", "authors": "Graham Spinks, Marie-Francine Moens", "title": "Structured (De)composable Representations Trained with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel technique for representing templates and instances\nof concept classes. A template representation refers to the generic\nrepresentation that captures the characteristics of an entire class. The\nproposed technique uses end-to-end deep learning to learn structured and\ncomposable representations from input images and discrete labels. The obtained\nrepresentations are based on distance estimates between the distributions given\nby the class label and those given by contextual information, which are modeled\nas environments. We prove that the representations have a clear structure\nallowing to decompose the representation into factors that represent classes\nand environments. We evaluate our novel technique on classification and\nretrieval tasks involving different modalities (visual and language data).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:20:31 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Spinks", "Graham", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2007.03331", "submitter": "Lingxi Xie", "authors": "Kaifeng Bi, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian", "title": "GOLD-NAS: Gradual, One-Level, Differentiable", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a large literature of neural architecture search, but most\nexisting work made use of heuristic rules that largely constrained the search\nflexibility. In this paper, we first relax these manually designed constraints\nand enlarge the search space to contain more than $10^{160}$ candidates. In the\nnew space, most existing differentiable search methods can fail dramatically.\nWe then propose a novel algorithm named Gradual One-Level Differentiable Neural\nArchitecture Search (GOLD-NAS) which introduces a variable resource constraint\nto one-level optimization so that the weak operators are gradually pruned out\nfrom the super-network. In standard image classification benchmarks, GOLD-NAS\ncan find a series of Pareto-optimal architectures within a single search\nprocedure. Most of the discovered architectures were never studied before, yet\nthey achieve a nice tradeoff between recognition accuracy and model complexity.\nWe believe the new space and search algorithm can advance the search of\ndifferentiable NAS.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:37:49 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Bi", "Kaifeng", ""], ["Xie", "Lingxi", ""], ["Chen", "Xin", ""], ["Wei", "Longhui", ""], ["Tian", "Qi", ""]]}, {"id": "2007.03338", "submitter": "Mehdi Ghatee Dr.", "authors": "Marzieh Heidari, Mehdi Ghatee, Ahmad Nickabadi, Arash Pourhasan Nezhad", "title": "Diverse and Styled Image Captioning Using SVD-Based Mixture of Recurrent\n  Experts", "comments": "13 pages, 4 figures and 5 tables, extracted from an MSc thesis in the\n  Amirkabir University of Technology, Tehran, Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With great advances in vision and natural language processing, the generation\nof image captions becomes a need. In a recent paper, Mathews, Xie and He [1],\nextended a new model to generate styled captions by separating semantics and\nstyle. In continuation of this work, here a new captioning model is developed\nincluding an image encoder to extract the features, a mixture of recurrent\nnetworks to embed the set of extracted features to a set of words, and a\nsentence generator that combines the obtained words as a stylized sentence. The\nresulted system that entitled as Mixture of Recurrent Experts (MoRE), uses a\nnew training algorithm that derives singular value decomposition (SVD) from\nweighting matrices of Recurrent Neural Networks (RNNs) to increase the\ndiversity of captions. Each decomposition step depends on a distinctive factor\nbased on the number of RNNs in MoRE. Since the used sentence generator gives a\nstylized language corpus without paired images, our captioning model can do the\nsame. Besides, the styled and diverse captions are extracted without training\non a densely labeled or styled dataset. To validate this captioning model, we\nuse Microsoft COCO which is a standard factual image caption dataset. We show\nthat the proposed captioning model can generate a diverse and stylized image\ncaptions without the necessity of extra-labeling. The results also show better\ndescriptions in terms of content accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 11:00:27 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Heidari", "Marzieh", ""], ["Ghatee", "Mehdi", ""], ["Nickabadi", "Ahmad", ""], ["Nezhad", "Arash Pourhasan", ""]]}, {"id": "2007.03347", "submitter": "Hussain Mohammed Kabir Dr", "authors": "H M Dipu Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas\n  Khosravi, Amir F Atiya, Saeid Nahavandi, Dipti Srinivasan", "title": "SpinalNet: Deep Neural Network with Gradual Input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the past few years, deep neural networks (DNNs) have garnered remarkable\nsuccess in a diverse range of real-world applications. However, DNNs consider a\nlarge number of inputs and consist of a large number of parameters, resulting\nin high computational demand. We study the human somatosensory system and\npropose the SpinalNet to achieve higher accuracy with less computational\nresources. In a typical neural network (NN) architecture, the hidden layers\nreceive inputs in the first layer and then transfer the intermediate outcomes\nto the next layer. In the proposed SpinalNet, the structure of hidden layers\nallocates to three sectors: 1) Input row, 2) Intermediate row, and 3) output\nrow. The intermediate row of the SpinalNet contains a few neurons. The role of\ninput segmentation is in enabling each hidden layer to receive a part of the\ninputs and outputs of the previous layer. Therefore, the number of incoming\nweights in a hidden layer is significantly lower than traditional DNNs. As all\nlayers of the SpinalNet directly contributes to the output row, the vanishing\ngradient problem does not exist. We also investigate the SpinalNet\nfully-connected layer to several well-known DNN models and perform traditional\nlearning and transfer learning. We observe significant error reductions with\nlower computational costs in most of the DNNs. We have also obtained the\nstate-of-the-art (SOTA) performance for QMNIST, Kuzushiji-MNIST, EMNIST\n(Letters, Digits, and Balanced), STL-10, Bird225, Fruits 360, and Caltech-101\ndatasets. The scripts of the proposed SpinalNet are available with the\nfollowing link: https://github.com/dipuk0506/SpinalNet\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 11:27:00 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 08:22:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kabir", "H M Dipu", ""], ["Abdar", "Moloud", ""], ["Jalali", "Seyed Mohammad Jafar", ""], ["Khosravi", "Abbas", ""], ["Atiya", "Amir F", ""], ["Nahavandi", "Saeid", ""], ["Srinivasan", "Dipti", ""]]}, {"id": "2007.03349", "submitter": "Xingjian Li", "authors": "Xingjian Li, Haoyi Xiong, Haozhe An, Chengzhong Xu, Dejing Dou", "title": "RIFLE: Backpropagation in Depth for Deep Transfer Learning through\n  Re-Initializing the Fully-connected LayEr", "comments": "Accepted by ICML'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning the deep convolution neural network(CNN) using a pre-trained\nmodel helps transfer knowledge learned from larger datasets to the target task.\nWhile the accuracy could be largely improved even when the training dataset is\nsmall, the transfer learning outcome is usually constrained by the pre-trained\nmodel with close CNN weights (Liu et al., 2019), as the backpropagation here\nbrings smaller updates to deeper CNN layers. In this work, we propose RIFLE - a\nsimple yet effective strategy that deepens backpropagation in transfer learning\nsettings, through periodically Re-Initializing the Fully-connected LayEr with\nrandom scratch during the fine-tuning procedure. RIFLE brings meaningful\nupdates to the weights of deep CNN layers and improves low-level feature\nlearning, while the effects of randomization can be easily converged throughout\nthe overall learning procedure. The experiments show that the use of RIFLE\nsignificantly improves deep transfer learning accuracy on a wide range of\ndatasets, out-performing known tricks for the similar purpose, such as Dropout,\nDropConnect, StochasticDepth, Disturb Label and Cyclic Learning Rate, under the\nsame settings with 0.5% -2% higher testing accuracy. Empirical cases and\nablation studies further indicate RIFLE brings meaningful updates to deep CNN\nlayers with accuracy improved.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 11:27:43 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Li", "Xingjian", ""], ["Xiong", "Haoyi", ""], ["An", "Haozhe", ""], ["Xu", "Chengzhong", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.03357", "submitter": "Mobarakol Islam", "authors": "Mobarakol Islam, Lalithkumar Seenivasan, Lim Chwee Ming, Hongliang Ren", "title": "Learning and Reasoning with the Graph Structure Representation in\n  Robotic Surgery", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to infer graph representations and performing spatial reasoning in a\ncomplex surgical environment can play a vital role in surgical scene\nunderstanding in robotic surgery. For this purpose, we develop an approach to\ngenerate the scene graph and predict surgical interactions between instruments\nand surgical region of interest (ROI) during robot-assisted surgery. We design\nan attention link function and integrate with a graph parsing network to\nrecognize the surgical interactions. To embed each node with corresponding\nneighbouring node features, we further incorporate SageConv into the network.\nThe scene graph generation and active edge classification mostly depend on the\nembedding or feature extraction of node and edge features from complex image\nrepresentation. Here, we empirically demonstrate the feature extraction methods\nby employing label smoothing weighted loss. Smoothing the hard label can avoid\nthe over-confident prediction of the model and enhances the feature\nrepresentation learned by the penultimate layer. To obtain the graph scene\nlabel, we annotate the bounding box and the instrument-ROI interactions on the\nrobotic scene segmentation challenge 2018 dataset with an experienced clinical\nexpert in robotic surgery and employ it to evaluate our propositions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 11:49:34 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 23:04:11 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 21:50:45 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Islam", "Mobarakol", ""], ["Seenivasan", "Lalithkumar", ""], ["Ming", "Lim Chwee", ""], ["Ren", "Hongliang", ""]]}, {"id": "2007.03373", "submitter": "Louis B\\'ethune", "authors": "Louis B\\'ethune, Yacouba Kaloga, Pierre Borgnat, Aur\\'elien Garivier,\n  Amaury Habrard", "title": "Hierarchical and Unsupervised Graph Representation Learning with\n  Loukas's Coarsening", "comments": "19 pages, 15 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel algorithm for unsupervised graph representation learning\nwith attributed graphs. It combines three advantages addressing some current\nlimitations of the literature: i) The model is inductive: it can embed new\ngraphs without re-training in the presence of new data; ii) The method takes\ninto account both micro-structures and macro-structures by looking at the\nattributed graphs at different scales; iii) The model is end-to-end\ndifferentiable: it is a building block that can be plugged into deep learning\npipelines and allows for back-propagation. We show that combining a coarsening\nmethod having strong theoretical guarantees with mutual information\nmaximization suffices to produce high quality embeddings. We evaluate them on\nclassification tasks with common benchmarks of the literature. We show that our\nalgorithm is competitive with state of the art among unsupervised graph\nrepresentation learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:04:38 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 15:15:18 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["B\u00e9thune", "Louis", ""], ["Kaloga", "Yacouba", ""], ["Borgnat", "Pierre", ""], ["Garivier", "Aur\u00e9lien", ""], ["Habrard", "Amaury", ""]]}, {"id": "2007.03375", "submitter": "Raul Gomez", "authors": "Raul Gomez, Jaume Gibert, Lluis Gomez, Dimosthenis Karatzas", "title": "Location Sensitive Image Retrieval and Tagging", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People from different parts of the globe describe objects and concepts in\ndistinct manners. Visual appearance can thus vary across different geographic\nlocations, which makes location a relevant contextual information when\nanalysing visual data. In this work, we address the task of image retrieval\nrelated to a given tag conditioned on a certain location on Earth. We present\nLocSens, a model that learns to rank triplets of images, tags and coordinates\nby plausibility, and two training strategies to balance the location influence\nin the final ranking. LocSens learns to fuse textual and location information\nof multimodal queries to retrieve related images at different levels of\nlocation granularity, and successfully utilizes location information to improve\nimage tagging.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:09:01 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Gomez", "Raul", ""], ["Gibert", "Jaume", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "2007.03378", "submitter": "Pablo L\\'opez-Garc\\'ia", "authors": "Laurin Herbsthofer, Barbara Prietl, Martina Tomberger, Thomas Pieber,\n  Pablo L\\'opez-Garc\\'ia", "title": "C2G-Net: Exploiting Morphological Properties for Image Classification", "comments": "10 pages, 5 figures (Figure 3 with 4 sub-figures), Appendix A and\n  Appendix B after the references. Originally submitted to ICML2020 but\n  rejected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose C2G-Net, a pipeline for image classification that\nexploits the morphological properties of images containing a large number of\nsimilar objects like biological cells. C2G-Net consists of two components: (1)\nCell2Grid, an image compression algorithm that identifies objects using\nsegmentation and arranges them on a grid, and (2) DeepLNiNo, a CNN architecture\nwith less than 10,000 trainable parameters aimed at facilitating model\ninterpretability. To test the performance of C2G-Net we used multiplex\nimmunohistochemistry images for predicting relapse risk in colon cancer.\nCompared to conventional CNN architectures trained on raw images, C2G-Net\nachieved similar prediction accuracy while training time was reduced by 85% and\nits model was is easier to interpret.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:16:17 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Herbsthofer", "Laurin", ""], ["Prietl", "Barbara", ""], ["Tomberger", "Martina", ""], ["Pieber", "Thomas", ""], ["L\u00f3pez-Garc\u00eda", "Pablo", ""]]}, {"id": "2007.03380", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang,\n  Ming-Ming Cheng, Huazhu Fu, Jianbing Shen", "title": "Re-thinking Co-Salient Object Detection", "comments": "22pages, 18 figures. CVPR2020-CoSOD3K extension. Code:\n  https://github.com/DengPingFan/CoEGNet", "journal-ref": "TPAMI 2021", "doi": "10.1109/TPAMI.2021.3060412", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we conduct a comprehensive study on the co-salient object\ndetection (CoSOD) problem for images. CoSOD is an emerging and rapidly growing\nextension of salient object detection (SOD), which aims to detect the\nco-occurring salient objects in a group of images. However, existing CoSOD\ndatasets often have a serious data bias, assuming that each group of images\ncontains salient objects of similar visual appearances. This bias can lead to\nthe ideal settings and effectiveness of models trained on existing datasets,\nbeing impaired in real-life situations, where similarities are usually semantic\nor conceptual. To tackle this issue, we first introduce a new benchmark, called\nCoSOD3k in the wild, which requires a large amount of semantic context, making\nit more challenging than existing CoSOD datasets. Our CoSOD3k consists of 3,316\nhigh-quality, elaborately selected images divided into 160 groups with\nhierarchical annotations. The images span a wide range of categories, shapes,\nobject sizes, and backgrounds. Second, we integrate the existing SOD techniques\nto build a unified, trainable CoSOD framework, which is long overdue in this\nfield. Specifically, we propose a novel CoEG-Net that augments our prior model\nEGNet with a co-attention projection strategy to enable fast common information\nlearning. CoEG-Net fully leverages previous large-scale SOD datasets and\nsignificantly improves the model scalability and stability. Third, we\ncomprehensively summarize 40 cutting-edge algorithms, benchmarking 18 of them\nover three challenging CoSOD datasets (iCoSeg, CoSal2015, and our CoSOD3k), and\nreporting more detailed (i.e., group-level) performance analysis. Finally, we\ndiscuss the challenges and future works of CoSOD. We hope that our study will\ngive a strong boost to growth in the CoSOD community. The benchmark toolbox and\nresults are available on our project page at http://dpfan.net/CoSOD3K/.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:20:51 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 02:08:22 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 07:13:01 GMT"}, {"version": "v4", "created": "Sun, 2 May 2021 01:47:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Li", "Tengpeng", ""], ["Lin", "Zheng", ""], ["Ji", "Ge-Peng", ""], ["Zhang", "Dingwen", ""], ["Cheng", "Ming-Ming", ""], ["Fu", "Huazhu", ""], ["Shen", "Jianbing", ""]]}, {"id": "2007.03397", "submitter": "Giulia Orr\\`u", "authors": "Roberto Casula, Giulia Orr\\`u, Daniele Angioni, Xiaoyi Feng, Gian Luca\n  Marcialis, Fabio Roli", "title": "Are spoofs from latent fingerprints a real threat for the best\n  state-of-art liveness detectors?", "comments": "Accepted for the 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the threat level of realistic attacks using latent\nfingerprints against sensors equipped with state-of-art liveness detectors and\nfingerprint verification systems which integrate such liveness algorithms. To\nthe best of our knowledge, only a previous investigation was done with spoofs\nfrom latent prints. In this paper, we focus on using snapshot pictures of\nlatent fingerprints. These pictures provide molds, that allows, after some\ndigital processing, to fabricate high-quality spoofs. Taking a snapshot picture\nis much simpler than developing fingerprints left on a surface by magnetic\npowders and lifting the trace by a tape. What we are interested here is to\nevaluate preliminary at which extent attacks of the kind can be considered a\nreal threat for state-of-art fingerprint liveness detectors and verification\nsystems. To this aim, we collected a novel data set of live and spoof images\nfabricated with snapshot pictures of latent fingerprints. This data set provide\na set of attacks at the most favourable conditions. We refer to this method and\nthe related data set as \"ScreenSpoof\". Then, we tested with it the performances\nof the best liveness detection algorithms, namely, the three winners of the\nLivDet competition. Reported results point out that the ScreenSpoof method is a\nthreat of the same level, in terms of detection and verification errors, than\nthat of attacks using spoofs fabricated with the full consensus of the victim.\nWe think that this is a notable result, never reported in previous work.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 13:04:40 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 15:03:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Casula", "Roberto", ""], ["Orr\u00f9", "Giulia", ""], ["Angioni", "Daniele", ""], ["Feng", "Xiaoyi", ""], ["Marcialis", "Gian Luca", ""], ["Roli", "Fabio", ""]]}, {"id": "2007.03408", "submitter": "Nicolas Papadakis", "authors": "Antoine Houdard and Arthur Leclaire and Nicolas Papadakis and Julien\n  Rabin", "title": "Wasserstein Generative Models for Patch-based Texture Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework to train a generative model for texture\nimage synthesis from a single example. To do so, we exploit the local\nrepresentation of images via the space of patches, that is, square sub-images\nof fixed size (e.g. $4\\times 4$). Our main contribution is to consider optimal\ntransport to enforce the multiscale patch distribution of generated images,\nwhich leads to two different formulations. First, a pixel-based optimization\nmethod is proposed, relying on discrete optimal transport. We show that it is\nrelated to a well-known texture optimization framework based on iterated patch\nnearest-neighbor projections, while avoiding some of its shortcomings. Second,\nin a semi-discrete setting, we exploit the differential properties of\nWasserstein distances to learn a fully convolutional network for texture\ngeneration. Once estimated, this network produces realistic and arbitrarily\nlarge texture samples in real time. The two formulations result in non-convex\nconcave problems that can be optimized efficiently with convergence properties\nand improved stability compared to adversarial approaches, without relying on\nany regularization. By directly dealing with the patch distribution of\nsynthesized images, we also overcome limitations of state-of-the art\ntechniques, such as patch aggregation issues that usually lead to low frequency\nartifacts (e.g. blurring) in traditional patch-based approaches, or statistical\ninconsistencies (e.g. color or patterns) in learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:32:55 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Houdard", "Antoine", ""], ["Leclaire", "Arthur", ""], ["Papadakis", "Nicolas", ""], ["Rabin", "Julien", ""]]}, {"id": "2007.03409", "submitter": "Darius Burschka", "authors": "Darius Burschka and Christian Robl and Sebastian Ohrendorf-Weiss", "title": "Optical Navigation in Unstructured Dynamic Railroad Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for optical navigation in unstructured, dynamic\nrailroad environments. We propose a way how to cope with the estimation of the\ntrain motion from sole observations of the planar track bed. The occasional\nsignificant occlusions during the operation of the train limit the available\nobservation to this difficult to track, repetitive area. This approach is a\nstep towards replacement of the expensive train management infrastructure with\nlocal intelligence on the train for SmartRail 4.0.\n  We derive our approach for robust estimation of translation and rotation in\nthis difficult environments and provide experimental validation of the approach\non real rail scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 13:21:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Burschka", "Darius", ""], ["Robl", "Christian", ""], ["Ohrendorf-Weiss", "Sebastian", ""]]}, {"id": "2007.03480", "submitter": "Jong Chul Ye", "authors": "Junghyun Lee, Jawook Gu, and Jong Chul Ye", "title": "Unsupervised CT Metal Artifact Learning using Attention-guided\n  beta-CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal artifact reduction (MAR) is one of the most important research topics\nin computed tomography (CT). With the advance of deep learning technology for\nimage reconstruction,various deep learning methods have been also suggested for\nmetal artifact removal, among which supervised learning methods are most\npopular. However, matched non-metal and metal image pairs are difficult to\nobtain in real CT acquisition. Recently, a promising unsupervised learning for\nMAR was proposed using feature disentanglement, but the resulting network\narchitecture is complication and difficult to handle large size clinical\nimages. To address this, here we propose a much simpler and much effective\nunsupervised MAR method for CT. The proposed method is based on a novel\nbeta-cycleGAN architecture derived from the optimal transport theory for\nappropriate feature space disentanglement. Another important contribution is to\nshow that attention mechanism is the key element to effectively remove the\nmetal artifacts. Specifically, by adding the convolutional block attention\nmodule (CBAM) layers with a proper disentanglement parameter, experimental\nresults confirm that we can get more improved MAR that preserves the detailed\ntexture of the original image.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:11:47 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lee", "Junghyun", ""], ["Gu", "Jawook", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2007.03496", "submitter": "Benjin Zhu", "authors": "Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu,\n  Zeming Li, Jian Sun", "title": "AutoAssign: Differentiable Label Assignment for Dense Object Detection", "comments": "Revised for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining positive/negative samples for object detection is known as label\nassignment. Here we present an anchor-free detector named AutoAssign. It\nrequires little human knowledge and achieves appearance-aware through a fully\ndifferentiable weighting mechanism. During training, to both satisfy the prior\ndistribution of data and adapt to category characteristics, we present Center\nWeighting to adjust the category-specific prior distributions. To adapt to\nobject appearances, Confidence Weighting is proposed to adjust the specific\nassign strategy of each instance. The two weighting modules are then combined\nto generate positive and negative weights to adjust each location's confidence.\nExtensive experiments on the MS COCO show that our method steadily surpasses\nother best sampling strategies by large margins with various backbones.\nMoreover, our best model achieves 52.1% AP, outperforming all existing\none-stage detectors. Besides, experiments on other datasets, e.g., PASCAL VOC,\nObjects365, and WiderFace, demonstrate the broad applicability of AutoAssign.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:32:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:18:33 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 15:57:47 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhu", "Benjin", ""], ["Wang", "Jianfeng", ""], ["Jiang", "Zhengkai", ""], ["Zong", "Fuhang", ""], ["Liu", "Songtao", ""], ["Li", "Zeming", ""], ["Sun", "Jian", ""]]}, {"id": "2007.03506", "submitter": "Diego Doimo", "authors": "Diego Doimo, Aldo Glielmo, Alessio Ansuini, Alessandro Laio", "title": "Hierarchical nucleation in deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks (DCNs) learn meaningful representations where\ndata that share the same abstract characteristics are positioned closer and\ncloser. Understanding these representations and how they are generated is of\nunquestioned practical and theoretical interest. In this work we study the\nevolution of the probability density of the ImageNet dataset across the hidden\nlayers in some state-of-the-art DCNs. We find that the initial layers generate\na unimodal probability density getting rid of any structure irrelevant for\nclassification. In subsequent layers density peaks arise in a hierarchical\nfashion that mirrors the semantic hierarchy of the concepts. Density peaks\ncorresponding to single categories appear only close to the output and via a\nvery sharp transition which resembles the nucleation process of a heterogeneous\nliquid. This process leaves a footprint in the probability density of the\noutput layer where the topography of the peaks allows reconstructing the\nsemantic relationships of the categories.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:42:18 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 15:14:19 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Doimo", "Diego", ""], ["Glielmo", "Aldo", ""], ["Ansuini", "Alessio", ""], ["Laio", "Alessandro", ""]]}, {"id": "2007.03513", "submitter": "Daniel T Chang", "authors": "Daniel T. Chang", "title": "Distance-Geometric Graph Convolutional Network (DG-GCN) for\n  Three-Dimensional (3D) Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.01785", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distance-geometric graph representation adopts a unified scheme\n(distance) for representing the geometry of three-dimensional(3D) graphs. It is\ninvariant to rotation and translation of the graph and it reflects pair-wise\nnode interactions and their generally local nature. To facilitate the\nincorporation of geometry in deep learning on 3D graphs, we propose a\nmessage-passing graph convolutional network based on the distance-geometric\ngraph representation: DG-GCN (distance-geometric graph convolution network). It\nutilizes continuous-filter convolutional layers, with filter-generating\nnetworks, that enable learning of filter weights from distances, thereby\nincorporating the geometry of 3D graphs in graph convolutions. Our results for\nthe ESOL and FreeSolv datasets show major improvement over those of standard\ngraph convolutions. They also show significant improvement over those of\ngeometric graph convolutions employing edge weight / edge distance power laws.\nOur work demonstrates the utility and value of DG-GCN for end-to-end deep\nlearning on 3D graphs, particularly molecular graphs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:20:52 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 00:50:32 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 17:42:26 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 17:53:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chang", "Daniel T.", ""]]}, {"id": "2007.03514", "submitter": "Aleksei Shpilman", "authors": "Mikita Sazanovich, Konstantin Chaika, Kirill Krinkin, Aleksei Shpilman", "title": "Imitation Learning Approach for AI Driving Olympics Trained on\n  Real-world and Simulation Data Simultaneously", "comments": "Accepted to the Workshop on AI for Autonomous Driving (AIAD), the\n  37th International Conference on Machine Learning (ICML2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our winning approach to solving the Lane Following\nChallenge at the AI Driving Olympics Competition through imitation learning on\na mixed set of simulation and real-world data. AI Driving Olympics is a\ntwo-stage competition: at stage one, algorithms compete in a simulated\nenvironment with the best ones advancing to a real-world final. One of the main\nproblems that participants encounter during the competition is that algorithms\ntrained for the best performance in simulated environments do not hold up in a\nreal-world environment and vice versa. Classic control algorithms also do not\ntranslate well between tasks since most of them have to be tuned to specific\ndriving conditions such as lighting, road type, camera position, etc. To\novercome this problem, we employed the imitation learning algorithm and trained\nit on a dataset collected from sources both from simulation and real-world,\nforcing our model to perform equally well in all environments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:48:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sazanovich", "Mikita", ""], ["Chaika", "Konstantin", ""], ["Krinkin", "Kirill", ""], ["Shpilman", "Aleksei", ""]]}, {"id": "2007.03532", "submitter": "Luca Stornaiuolo", "authors": "Luca Stornaiuolo, Nima Dehmamy, Albert-L\\'aszl\\'o Barab\\'asi, Mauro\n  Martino", "title": "3D Topology Transformation with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generation and transformation of images and videos using artificial\nintelligence have flourished over the past few years. Yet, there are only a few\nworks aiming to produce creative 3D shapes, such as sculptures. Here we show a\nnovel 3D-to-3D topology transformation method using Generative Adversarial\nNetworks (GAN). We use a modified pix2pix GAN, which we call Vox2Vox, to\ntransform the volumetric style of a 3D object while retaining the original\nobject shape. In particular, we show how to transform 3D models into two new\nvolumetric topologies - the 3D Network and the Ghirigoro. We describe how to\nuse our approach to construct customized 3D representations. We believe that\nthe generated 3D shapes are novel and inspirational. Finally, we compare the\nresults between our approach and a baseline algorithm that directly convert the\n3D shapes, without using our GAN.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:03:16 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Stornaiuolo", "Luca", ""], ["Dehmamy", "Nima", ""], ["Barab\u00e1si", "Albert-L\u00e1szl\u00f3", ""], ["Martino", "Mauro", ""]]}, {"id": "2007.03535", "submitter": "Yingqian Wang", "authors": "Yingqian Wang, Jungang Yang, Longguang Wang, Xinyi Ying, Tianhao Wu,\n  Wei An, Yulan Guo", "title": "Light Field Image Super-Resolution Using Deformable Convolution", "comments": "Accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3042059", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Light field (LF) cameras can record scenes from multiple perspectives, and\nthus introduce beneficial angular information for image super-resolution (SR).\nHowever, it is challenging to incorporate angular information due to\ndisparities among LF images. In this paper, we propose a deformable convolution\nnetwork (i.e., LF-DFnet) to handle the disparity problem for LF image SR.\nSpecifically, we design an angular deformable alignment module (ADAM) for\nfeature-level alignment. Based on ADAM, we further propose a\ncollect-and-distribute approach to perform bidirectional alignment between the\ncenter-view feature and each side-view feature. Using our approach, angular\ninformation can be well incorporated and encoded into features of each view,\nwhich benefits the SR reconstruction of all LF images. Moreover, we develop a\nbaseline-adjustable LF dataset to evaluate SR performance under different\ndisparity variations. Experiments on both public and our self-developed\ndatasets have demonstrated the superiority of our method. Our LF-DFnet can\ngenerate high-resolution images with more faithful details and achieve\nstate-of-the-art reconstruction accuracy. Besides, our LF-DFnet is more robust\nto disparity variations, which has not been well addressed in literature.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:07:33 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 12:31:33 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 04:05:25 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2020 12:01:05 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Wang", "Yingqian", ""], ["Yang", "Jungang", ""], ["Wang", "Longguang", ""], ["Ying", "Xinyi", ""], ["Wu", "Tianhao", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2007.03538", "submitter": "Jixin Wang", "authors": "Jixin Wang, Sanping Zhou, Chaowei Fang, Le Wang, Jinjun Wang", "title": "Meta Corrupted Pixels Mining for Medical Image Segmentation", "comments": "Accepted By MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved satisfactory performance in piles of\nmedical image analysis tasks. However the training of deep neural network\nrequires a large amount of samples with high-quality annotations. In medical\nimage segmentation, it is very laborious and expensive to acquire precise\npixel-level annotations. Aiming at training deep segmentation models on\ndatasets with probably corrupted annotations, we propose a novel Meta Corrupted\nPixels Mining (MCPM) method based on a simple meta mask network. Our method is\ntargeted at automatically estimate a weighting map to evaluate the importance\nof every pixel in the learning of segmentation network. The meta mask network\nwhich regards the loss value map of the predicted segmentation results as\ninput, is capable of identifying out corrupted layers and allocating small\nweights to them. An alternative algorithm is adopted to train the segmentation\nnetwork and the meta mask network, simultaneously. Extensive experimental\nresults on LIDC-IDRI and LiTS datasets show that our method outperforms\nstate-of-the-art approaches which are devised for coping with corrupted\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:12:20 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wang", "Jixin", ""], ["Zhou", "Sanping", ""], ["Fang", "Chaowei", ""], ["Wang", "Le", ""], ["Wang", "Jinjun", ""]]}, {"id": "2007.03560", "submitter": "Ting Yao", "authors": "Jiajun Deng and Yingwei Pan and Ting Yao and Wengang Zhou and Houqiang\n  Li and Tao Mei", "title": "Single Shot Video Object Detector", "comments": "Accepted by IEEE Transactions on Multimedia; The code is available at\n  \\url{https://github.com/ddjiajun/SSVD}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single shot detectors that are potentially faster and simpler than two-stage\ndetectors tend to be more applicable to object detection in videos.\nNevertheless, the extension of such object detectors from image to video is not\ntrivial especially when appearance deterioration exists in videos, \\emph{e.g.},\nmotion blur or occlusion. A valid question is how to explore temporal coherence\nacross frames for boosting detection. In this paper, we propose to address the\nproblem by enhancing per-frame features through aggregation of neighboring\nframes. Specifically, we present Single Shot Video Object Detector (SSVD) -- a\nnew architecture that novelly integrates feature aggregation into a one-stage\ndetector for object detection in videos. Technically, SSVD takes Feature\nPyramid Network (FPN) as backbone network to produce multi-scale features.\nUnlike the existing feature aggregation methods, SSVD, on one hand, estimates\nthe motion and aggregates the nearby features along the motion path, and on the\nother, hallucinates features by directly sampling features from the adjacent\nframes in a two-stream structure. Extensive experiments are conducted on\nImageNet VID dataset, and competitive results are reported when comparing to\nstate-of-the-art approaches. More remarkably, for $448 \\times 448$ input, SSVD\nachieves 79.2% mAP on ImageNet VID, by processing one frame in 85 ms on an\nNvidia Titan X Pascal GPU. The code is available at\n\\url{https://github.com/ddjiajun/SSVD}.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:36:26 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Deng", "Jiajun", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Mei", "Tao", ""]]}, {"id": "2007.03578", "submitter": "Dongfang Yang", "authors": "Dongfang Yang, Ekim Yurtsever, Vishnu Renganathan, Keith A. Redmill,\n  \\\"Umit \\\"Ozg\\\"uner", "title": "A Vision-based Social Distancing and Critical Density Detection System\n  for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social distancing has been proven as an effective measure against the spread\nof the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are\nnot used to tracking the required 6-feet (2-meters) distance between themselves\nand their surroundings. An active surveillance system capable of detecting\ndistances between individuals and warning them can slow down the spread of the\ndeadly disease. Furthermore, measuring social density in a region of interest\n(ROI) and modulating inflow can decrease social distancing violation occurrence\nchance.\n  On the other hand, recording data and labeling individuals who do not follow\nthe measures will breach individuals' rights in free-societies. Here we propose\nan Artificial Intelligence (AI) based real-time social distancing detection and\nwarning system considering four important ethical factors: (1) the system\nshould never record/cache data, (2) the warnings should not target the\nindividuals, (3) no human supervisor should be in the detection/warning loop,\nand (4) the code should be open-source and accessible to the public. Against\nthis backdrop, we propose using a monocular camera and deep learning-based\nreal-time object detectors to measure social distancing. If a violation is\ndetected, a non-intrusive audio-visual warning signal is emitted without\ntargeting the individual who breached the social distancing measure. Also, if\nthe social density is over a critical value, the system sends a control signal\nto modulate inflow into the ROI. We tested the proposed method across\nreal-world datasets to measure its generality and performance. The proposed\nmethod is ready for deployment, and our code is open-sourced.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:55:50 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 22:53:16 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yang", "Dongfang", ""], ["Yurtsever", "Ekim", ""], ["Renganathan", "Vishnu", ""], ["Redmill", "Keith A.", ""], ["\u00d6zg\u00fcner", "\u00dcmit", ""]]}, {"id": "2007.03579", "submitter": "Abdelrahman Abdallah", "authors": "Daniyar Nurseitov, Kairat Bostanbekov, Daniyar Kurmankhojayev, Anel\n  Alimova, Abdelrahman Abdallah", "title": "HKR For Handwritten Kazakh & Russian Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new Russian and Kazakh database (with about 95%\nof Russian and 5% of Kazakh words/sentences respectively) for offline\nhandwriting recognition. A few pre-processing and segmentation procedures have\nbeen developed together with the database. The database is written in Cyrillic\nand shares the same 33 characters. Besides these characters, the Kazakh\nalphabet also contains 9 additional specific characters. This dataset is a\ncollection of forms. The sources of all the forms in the datasets were\ngenerated by \\LaTeX which subsequently was filled out by persons with their\nhandwriting. The database consists of more than 1400 filled forms. There are\napproximately 63000 sentences, more than 715699 symbols produced by\napproximately 200 different writers. It can serve researchers in the field of\nhandwriting recognition tasks by using deep and machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:57:41 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 16:53:54 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Nurseitov", "Daniyar", ""], ["Bostanbekov", "Kairat", ""], ["Kurmankhojayev", "Daniyar", ""], ["Alimova", "Anel", ""], ["Abdallah", "Abdelrahman", ""]]}, {"id": "2007.03584", "submitter": "Xiao Wang", "authors": "Bo Jiang, Sheng Wang, Xiao Wang, Aihua Zheng", "title": "SaADB: A Self-attention Guided ADB Network for Person Re-identification", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on\nperson image representation and re-ID task via feature erasing. However, BDB\ndrops the features randomly which may lead to sub-optimal results. In this\npaper, we propose a novel Self-attention guided Adaptive DropBlock network\n(SaADB) for person re-ID which can adaptively erase the most discriminative\nregions. Specifically, SaADB first obtains a self-attention map by channel-wise\npooling and returns a drop mask by thresholding the self-attention map. Then,\nthe input features and self-attention guided drop mask are multiplied to\ngenerate the dropped feature maps. Meanwhile, we utilize the spatial and\nchannel attention to learn a better feature map and iteratively train with the\nfeature dropping module for person re-ID. Experiments on several benchmark\ndatasets demonstrate that the proposed SaADB significantly beats the prevalent\ncompetitors in person re-ID.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:06:22 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 00:42:38 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Sheng", ""], ["Wang", "Xiao", ""], ["Zheng", "Aihua", ""]]}, {"id": "2007.03593", "submitter": "Yuankai Huo", "authors": "Aadarsh Jha, Haichun Yang, Ruining Deng, Meghan E. Kapp, Agnes B.\n  Fogo, Yuankai Huo", "title": "Instance Segmentation for Whole Slide Imaging: End-to-End or\n  Detect-Then-Segment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic instance segmentation of glomeruli within kidney Whole Slide\nImaging (WSI) is essential for clinical research in renal pathology. In\ncomputer vision, the end-to-end instance segmentation methods (e.g., Mask-RCNN)\nhave shown their advantages relative to detect-then-segment approaches by\nperforming complementary detection and segmentation tasks simultaneously. As a\nresult, the end-to-end Mask-RCNN approach has been the de facto standard method\nin recent glomerular segmentation studies, where downsampling and patch-based\ntechniques are used to properly evaluate the high resolution images from WSI\n(e.g., >10,000x10,000 pixels on 40x). However, in high resolution WSI, a single\nglomerulus itself can be more than 1,000x1,000 pixels in original resolution\nwhich yields significant information loss when the corresponding features maps\nare downsampled via the Mask-RCNN pipeline. In this paper, we assess if the\nend-to-end instance segmentation framework is optimal for high-resolution WSI\nobjects by comparing Mask-RCNN with our proposed detect-then-segment framework.\nBeyond such a comparison, we also comprehensively evaluate the performance of\nour detect-then-segment pipeline through: 1) two of the most prevalent\nsegmentation backbones (U-Net and DeepLab_v3); 2) six different image\nresolutions (from 512x512 to 28x28); and 3) two different color spaces (RGB and\nLAB). Our detect-then-segment pipeline, with the DeepLab_v3 segmentation\nframework operating on previously detected glomeruli of 512x512 resolution,\nachieved a 0.953 dice similarity coefficient (DSC), compared with a 0.902 DSC\nfrom the end-to-end Mask-RCNN pipeline. Further, we found that neither RGB nor\nLAB color spaces yield better performance when compared against each other in\nthe context of a detect-then-segment framework. Detect-then-segment pipeline\nachieved better segmentation performance compared with End-to-end method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:23:10 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Jha", "Aadarsh", ""], ["Yang", "Haichun", ""], ["Deng", "Ruining", ""], ["Kapp", "Meghan E.", ""], ["Fogo", "Agnes B.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2007.03600", "submitter": "Kamran Ali", "authors": "Kamran Ali, Alex X. Liu, Eugene Chai, Karthik Sundaresan", "title": "Monitoring Browsing Behavior of Customers in Retail Stores via RFID\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose to use commercial off-the-shelf (COTS) monostatic\nRFID devices (i.e. which use a single antenna at a time for both transmitting\nand receiving RFID signals to and from the tags) to monitor browsing activity\nof customers in front of display items in places such as retail stores. To this\nend, we propose TagSee, a multi-person imaging system based on monostatic RFID\nimaging. TagSee is based on the insight that when customers are browsing the\nitems on a shelf, they stand between the tags deployed along the boundaries of\nthe shelf and the reader, which changes the multi-paths that the RFID signals\ntravel along, and both the RSS and phase values of the RFID signals that the\nreader receives change. Based on these variations observed by the reader,\nTagSee constructs a coarse grained image of the customers. Afterwards, TagSee\nidentifies the items that are being browsed by the customers by analyzing the\nconstructed images. The key novelty of this paper is on achieving browsing\nbehavior monitoring of multiple customers in front of display items by\nconstructing coarse grained images via robust, analytical model-driven deep\nlearning based, RFID imaging. To achieve this, we first mathematically\nformulate the problem of imaging humans using monostatic RFID devices and\nderive an approximate analytical imaging model that correlates the variations\ncaused by human obstructions in the RFID signals. Based on this model, we then\ndevelop a deep learning framework to robustly image customers with high\naccuracy. We implement TagSee scheme using a Impinj Speedway R420 reader and\nSMARTRAC DogBone RFID tags. TagSee can achieve a TPR of more than ~90% and a\nFPR of less than ~10% in multi-person scenarios using training data from just\n3-4 users.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:36:24 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ali", "Kamran", ""], ["Liu", "Alex X.", ""], ["Chai", "Eugene", ""], ["Sundaresan", "Karthik", ""]]}, {"id": "2007.03621", "submitter": "Ramachandra Raghavendra Prof.", "authors": "Sushma Venkatesh, Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja,\n  Naser Damer, Christoph Busch", "title": "Can GAN Generated Morphs Threaten Face Recognition Systems Equally as\n  Landmark Based Morphs? -- Vulnerability and Detection", "comments": "Accepted in IWBF 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of face morphing is to combine face images of different\ndata subjects (e.g. a malicious actor and an accomplice) to generate a face\nimage that can be equally verified for both contributing data subjects. In this\npaper, we propose a new framework for generating face morphs using a newer\nGenerative Adversarial Network (GAN) - StyleGAN. In contrast to earlier works,\nwe generate realistic morphs of both high-quality and high resolution of\n1024$\\times$1024 pixels. With the newly created morphing dataset of 2500\nmorphed face images, we pose a critical question in this work. \\textit{(i) Can\nGAN generated morphs threaten Face Recognition Systems (FRS) equally as\nLandmark based morphs?} Seeking an answer, we benchmark the vulnerability of a\nCommercial-Off-The-Shelf FRS (COTS) and a deep learning-based FRS (ArcFace).\nThis work also benchmarks the detection approaches for both GAN generated\nmorphs against the landmark based morphs using established Morphing Attack\nDetection (MAD) schemes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:52:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Venkatesh", "Sushma", ""], ["Zhang", "Haoyu", ""], ["Ramachandra", "Raghavendra", ""], ["Raja", "Kiran", ""], ["Damer", "Naser", ""], ["Busch", "Christoph", ""]]}, {"id": "2007.03626", "submitter": "Jianing Yang", "authors": "Jianing Yang, Yuying Zhu, Yongxin Wang, Ruitao Yi, Amir Zadeh,\n  Louis-Philippe Morency", "title": "What Gives the Answer Away? Question Answering Bias Analysis on Video QA\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering biases in video QA datasets can mislead multimodal model\nto overfit to QA artifacts and jeopardize the model's ability to generalize.\nUnderstanding how strong these QA biases are and where they come from helps the\ncommunity measure progress more accurately and provide researchers insights to\ndebug their models. In this paper, we analyze QA biases in popular video\nquestion answering datasets and discover pretrained language models can answer\n37-48% questions correctly without using any multimodal context information,\nfar exceeding the 20% random guess baseline for 5-choose-1 multiple-choice\nquestions. Our ablation study shows biases can come from annotators and type of\nquestions. Specifically, annotators that have been seen during training are\nbetter predicted by the model and reasoning, abstract questions incur more\nbiases than factual, direct questions. We also show empirically that using\nannotator-non-overlapping train-test splits can reduce QA biases for video QA\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:00:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Yang", "Jianing", ""], ["Zhu", "Yuying", ""], ["Wang", "Yongxin", ""], ["Yi", "Ruitao", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2007.03632", "submitter": "Reuben Dorent", "authors": "Reuben Dorent, Samuel Joutard, Jonathan Shapey, Sotirios Bisdas, Neil\n  Kitchen, Robert Bradford, Shakeel Saeed, Marc Modat, Sebastien Ourselin, Tom\n  Vercauteren", "title": "Scribble-based Domain Adaptation via Co-segmentation", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59710-8_47", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional networks have reached state-of-the-art\nperformance in many medical image segmentation tasks, they have typically\ndemonstrated poor generalisation capability. To be able to generalise from one\ndomain (e.g. one imaging modality) to another, domain adaptation has to be\nperformed. While supervised methods may lead to good performance, they require\nto fully annotate additional data which may not be an option in practice. In\ncontrast, unsupervised methods don't need additional annotations but are\nusually unstable and hard to train. In this work, we propose a novel\nweakly-supervised method. Instead of requiring detailed but time-consuming\nannotations, scribbles on the target domain are used to perform domain\nadaptation. This paper introduces a new formulation of domain adaptation based\non structured learning and co-segmentation. Our method is easy to train, thanks\nto the introduction of a regularised loss. The framework is validated on\nVestibular Schwannoma segmentation (T1 to T2 scans). Our proposed method\noutperforms unsupervised approaches and achieves comparable performance to a\nfully-supervised approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:09:30 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 15:53:03 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Dorent", "Reuben", ""], ["Joutard", "Samuel", ""], ["Shapey", "Jonathan", ""], ["Bisdas", "Sotirios", ""], ["Kitchen", "Neil", ""], ["Bradford", "Robert", ""], ["Saeed", "Shakeel", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2007.03639", "submitter": "Parth Kothari", "authors": "Parth Kothari, Sven Kreiss, Alexandre Alahi", "title": "Human Trajectory Forecasting in Crowds: A Deep Learning Perspective", "comments": "IEEE format, Layer-wise Relevance Propagation added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the past few decades, human trajectory forecasting has been a field of\nactive research owing to its numerous real-world applications: evacuation\nsituation analysis, deployment of intelligent transport systems, traffic\noperations, to name a few. Early works handcrafted this representation based on\ndomain knowledge. However, social interactions in crowded environments are not\nonly diverse but often subtle. Recently, deep learning methods have\noutperformed their handcrafted counterparts, as they learned about human-human\ninteractions in a more generic data-driven fashion. In this work, we present an\nin-depth analysis of existing deep learning-based methods for modelling social\ninteractions. We propose two knowledge-based data-driven methods to effectively\ncapture these social interactions. To objectively compare the performance of\nthese interaction-based forecasting models, we develop a large scale\ninteraction-centric benchmark TrajNet++, a significant yet missing component in\nthe field of human trajectory forecasting. We propose novel performance metrics\nthat evaluate the ability of a model to output socially acceptable\ntrajectories. Experiments on TrajNet++ validate the need for our proposed\nmetrics, and our method outperforms competitive baselines on both real-world\nand synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:19:56 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 15:49:44 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 11:02:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kothari", "Parth", ""], ["Kreiss", "Sven", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2007.03643", "submitter": "Keegan Lensink", "authors": "Keegan Lensink, Issam Laradji, Marco Law, Paolo Emilio Barbano, Savvas\n  Nicolaou, William Parker, Eldad Haber", "title": "Segmentation of Pulmonary Opacification in Chest CT Scans of COVID-19\n  Patients", "comments": "9 pages, 5 figures. Fix typo in delimiter between author names in\n  arXiv metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has rapidly\nspread into a global pandemic. A form of pneumonia, presenting as opacities\nwith in a patient's lungs, is the most common presentation associated with this\nvirus, and great attention has gone into how these changes relate to patient\nmorbidity and mortality. In this work we provide open source models for the\nsegmentation of patterns of pulmonary opacification on chest Computed\nTomography (CT) scans which have been correlated with various stages and\nseverities of infection. We have collected 663 chest CT scans of COVID-19\npatients from healthcare centers around the world, and created pixel wise\nsegmentation labels for nearly 25,000 slices that segment 6 different patterns\nof pulmonary opacification. We provide open source implementations and\npre-trained weights for multiple segmentation models trained on our dataset.\nOur best model achieves an opacity Intersection-Over-Union score of 0.76 on our\ntest set, demonstrates successful domain adaptation, and predicts the volume of\nopacification within 1.7\\% of expert radiologists. Additionally, we present an\nanalysis of the inter-observer variability inherent to this task, and propose\nmethods for appropriate probabilistic approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:32:24 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:26:06 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lensink", "Keegan", ""], ["Laradji", "Issam", ""], ["Law", "Marco", ""], ["Barbano", "Paolo Emilio", ""], ["Nicolaou", "Savvas", ""], ["Parker", "William", ""], ["Haber", "Eldad", ""]]}, {"id": "2007.03669", "submitter": "Victoria Dean", "authors": "Victoria Dean, Shubham Tulsiani, Abhinav Gupta", "title": "See, Hear, Explore: Curiosity via Audio-Visual Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration is one of the core challenges in reinforcement learning. A common\nformulation of curiosity-driven exploration uses the difference between the\nreal future and the future predicted by a learned model. However, predicting\nthe future is an inherently difficult task which can be ill-posed in the face\nof stochasticity. In this paper, we introduce an alternative form of curiosity\nthat rewards novel associations between different senses. Our approach exploits\nmultiple modalities to provide a stronger signal for more efficient\nexploration. Our method is inspired by the fact that, for humans, both sight\nand sound play a critical role in exploration. We present results on several\nAtari environments and Habitat (a photorealistic navigation simulator), showing\nthe benefits of using an audio-visual association model for intrinsically\nguiding learning agents in the absence of external rewards. For videos and\ncode, see https://vdean.github.io/audio-curiosity.html.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:56:35 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 16:26:54 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dean", "Victoria", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2007.03672", "submitter": "Zhe Cao", "authors": "Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, Jitendra\n  Malik", "title": "Long-term Human Motion Prediction with Scene Context", "comments": "ECCV 2020 Oral. Dataset & Code:\n  https://github.com/ZheC/GTA-IM-Dataset Video:\n  https://people.eecs.berkeley.edu/~zhecao/hmp/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human movement is goal-directed and influenced by the spatial layout of the\nobjects in the scene. To plan future human motion, it is crucial to perceive\nthe environment -- imagine how hard it is to navigate a new room with lights\noff. Existing works on predicting human motion do not pay attention to the\nscene context and thus struggle in long-term prediction. In this work, we\npropose a novel three-stage framework that exploits scene context to tackle\nthis task. Given a single scene image and 2D pose histories, our method first\nsamples multiple human motion goals, then plans 3D human paths towards each\ngoal, and finally predicts 3D human pose sequences following each path. For\nstable training and rigorous evaluation, we contribute a diverse synthetic\ndataset with clean annotations. In both synthetic and real datasets, our method\nshows consistent quantitative and qualitative improvements over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:59:53 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 06:22:17 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 17:23:11 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Cao", "Zhe", ""], ["Gao", "Hang", ""], ["Mangalam", "Karttikeya", ""], ["Cai", "Qi-Zhi", ""], ["Vo", "Minh", ""], ["Malik", "Jitendra", ""]]}, {"id": "2007.03730", "submitter": "Ping-Yeh Chiang", "authors": "Ping-yeh Chiang, Michael J. Curry, Ahmed Abdelkader, Aounon Kumar,\n  John Dickerson, Tom Goldstein", "title": "Detection as Regression: Certified Object Detection by Median Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the vulnerability of object detectors to adversarial attacks, very\nfew defenses are known to date. While adversarial training can improve the\nempirical robustness of image classifiers, a direct extension to object\ndetection is very expensive. This work is motivated by recent progress on\ncertified classification by randomized smoothing. We start by presenting a\nreduction from object detection to a regression problem. Then, to enable\ncertified regression, where standard mean smoothing fails, we propose median\nsmoothing, which is of independent interest. We obtain the first\nmodel-agnostic, training-free, and certified defense for object detection\nagainst $\\ell_2$-bounded attacks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:40:19 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 22:13:31 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 16:43:49 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chiang", "Ping-yeh", ""], ["Curry", "Michael J.", ""], ["Abdelkader", "Ahmed", ""], ["Kumar", "Aounon", ""], ["Dickerson", "John", ""], ["Goldstein", "Tom", ""]]}, {"id": "2007.03748", "submitter": "Edward Frady", "authors": "E. Paxon Frady, Spencer Kent, Bruno A. Olshausen, Friedrich T. Sommer", "title": "Resonator networks for factoring distributed representations of data\n  structures", "comments": "20 pages, 5 figures, to appear in Neural Computation 2020 with\n  companion paper: arXiv:1906.11684", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to encode and manipulate data structures with distributed neural\nrepresentations could qualitatively enhance the capabilities of traditional\nneural networks by supporting rule-based symbolic reasoning, a central property\nof cognition. Here we show how this may be accomplished within the framework of\nVector Symbolic Architectures (VSA) (Plate, 1991; Gayler, 1998; Kanerva, 1996),\nwhereby data structures are encoded by combining high-dimensional vectors with\noperations that together form an algebra on the space of distributed\nrepresentations. In particular, we propose an efficient solution to a hard\ncombinatorial search problem that arises when decoding elements of a VSA data\nstructure: the factorization of products of multiple code vectors. Our proposed\nalgorithm, called a resonator network, is a new type of recurrent neural\nnetwork that interleaves VSA multiplication operations and pattern completion.\nWe show in two examples -- parsing of a tree-like data structure and parsing of\na visual scene -- how the factorization problem arises and how the resonator\nnetwork can solve it. More broadly, resonator networks open the possibility to\napply VSAs to myriad artificial intelligence problems in real-world domains. A\ncompanion paper (Kent et al., 2020) presents a rigorous analysis and evaluation\nof the performance of resonator networks, showing it out-performs alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 19:24:27 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Frady", "E. Paxon", ""], ["Kent", "Spencer", ""], ["Olshausen", "Bruno A.", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "2007.03775", "submitter": "Sungho Park", "authors": "Sungho Park, Dohyung Kim, Sunhee Hwang, Hyeran Byun", "title": "README: REpresentation learning by fairness-Aware Disentangling MEthod", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fair representation learning aims to encode invariant representation with\nrespect to the protected attribute, such as gender or age. In this paper, we\ndesign Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair\nrepresentation learning. This network disentangles latent space into three\nsubspaces with a decorrelation loss that encourages each subspace to contain\nindependent information: 1) target attribute information, 2) protected\nattribute information, 3) mutual attribute information. After the\nrepresentation learning, this disentangled representation is leveraged for\nfairer downstream classification by excluding the subspace with the protected\nattribute information. We demonstrate the effectiveness of our model through\nextensive experiments on CelebA and UTK Face datasets. Our method outperforms\nthe previous state-of-the-art method by large margins in terms of equal\nopportunity and equalized odds.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:16:49 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Park", "Sungho", ""], ["Kim", "Dohyung", ""], ["Hwang", "Sunhee", ""], ["Byun", "Hyeran", ""]]}, {"id": "2007.03777", "submitter": "Huaiyi Huang", "authors": "Huaiyi Huang, Yuqi Zhang, Qingqiu Huang, Zhengkui Guo, Ziwei Liu, and\n  Dahua Lin", "title": "Placepedia: Comprehensive Place Understanding with Multi-Faceted\n  Annotations", "comments": "To appear in ECCV 2020. Dataset is available at:\n  https://hahehi.github.io/placepedia.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place is an important element in visual understanding. Given a photo of a\nbuilding, people can often tell its functionality, e.g. a restaurant or a shop,\nits cultural style, e.g. Asian or European, as well as its economic type, e.g.\nindustry oriented or tourism oriented. While place recognition has been widely\nstudied in previous work, there remains a long way towards comprehensive place\nunderstanding, which is far beyond categorizing a place with an image and\nrequires information of multiple aspects. In this work, we contribute\nPlacepedia, a large-scale place dataset with more than 35M photos from 240K\nunique places. Besides the photos, each place also comes with massive\nmulti-faceted information, e.g. GDP, population, etc., and labels at multiple\nlevels, including function, city, country, etc.. This dataset, with its large\namount of data and rich annotations, allows various studies to be conducted.\nParticularly, in our studies, we develop 1) PlaceNet, a unified framework for\nmulti-level place recognition, and 2) a method for city embedding, which can\nproduce a vector representation for a city that captures both visual and\nmulti-faceted side information. Such studies not only reveal key challenges in\nplace understanding, but also establish connections between visual observations\nand underlying socioeconomic/cultural implications.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:17:01 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 08:17:10 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 16:38:50 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 08:56:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Huang", "Huaiyi", ""], ["Zhang", "Yuqi", ""], ["Huang", "Qingqiu", ""], ["Guo", "Zhengkui", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""]]}, {"id": "2007.03778", "submitter": "Edward Smith", "authors": "Edward J. Smith, Roberto Calandra, Adriana Romero, Georgia Gkioxari,\n  David Meger, Jitendra Malik, Michal Drozdzal", "title": "3D Shape Reconstruction from Vision and Touch", "comments": "Accepted at Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a toddler is presented a new toy, their instinctual behaviour is to pick\nit upand inspect it with their hand and eyes in tandem, clearly searching over\nits surface to properly understand what they are playing with. At any instance\nhere, touch provides high fidelity localized information while vision provides\ncomplementary global context. However, in 3D shape reconstruction, the\ncomplementary fusion of visual and haptic modalities remains largely\nunexplored. In this paper, we study this problem and present an effective\nchart-based approach to multi-modal shape understanding which encourages a\nsimilar fusion vision and touch information.To do so, we introduce a dataset of\nsimulated touch and vision signals from the interaction between a robotic hand\nand a large array of 3D objects. Our results show that (1) leveraging both\nvision and touch signals consistently improves single-modality baselines; (2)\nour approach outperforms alternative modality fusion methods and strongly\nbenefits from the proposed chart-based structure; (3) there construction\nquality increases with the number of grasps provided; and (4) the touch\ninformation not only enhances the reconstruction at the touch site but also\nextrapolates to its local neighborhood.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:20:33 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:57:55 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Smith", "Edward J.", ""], ["Calandra", "Roberto", ""], ["Romero", "Adriana", ""], ["Gkioxari", "Georgia", ""], ["Meger", "David", ""], ["Malik", "Jitendra", ""], ["Drozdzal", "Michal", ""]]}, {"id": "2007.03780", "submitter": "Anpei Chen", "authors": "Anpei Chen, Ruiyang Liu, Ling Xie, Jingyi Yu", "title": "A Free Viewpoint Portrait Generator with Dynamic Styling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating portrait images from a single latent space facing the problem of\nentangled attributes, making it difficult to explicitly adjust the generation\non specific attributes, e.g., contour and viewpoint control or dynamic styling.\nTherefore, we propose to decompose the generation space into two subspaces:\ngeometric and texture space. We first encode portrait scans with a semantic\noccupancy field (SOF), which represents semantic-embedded geometry structure\nand output free-viewpoint semantic segmentation maps. Then we design a semantic\ninstance wised(SIW) StyleGAN to regionally styling the segmentation map. We\ncapture 664 3D portrait scans for our SOF training and use real capture\nphotos(FFHQ and CelebA-HQ) for SIW StyleGAN training. Adequate experiments show\nthat our representations enable appearance consistent shape, pose, regional\nstyles controlling, achieve state-of-the-art results, and generalize well in\nvarious application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:28:47 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Chen", "Anpei", ""], ["Liu", "Ruiyang", ""], ["Xie", "Ling", ""], ["Yu", "Jingyi", ""]]}, {"id": "2007.03815", "submitter": "Ping Hu", "authors": "Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin,\n  Kate Saenko, Stan Sclaroff", "title": "Real-time Semantic Segmentation with Fast Attention", "comments": "project page: https://cs-people.bu.edu/pinghu/FANet.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep CNN based models for semantic segmentation, high accuracy relies on\nrich spatial context (large receptive fields) and fine spatial details (high\nresolution), both of which incur high computational costs. In this paper, we\npropose a novel architecture that addresses both challenges and achieves\nstate-of-the-art performance for semantic segmentation of high-resolution\nimages and videos in real-time. The proposed architecture relies on our fast\nspatial attention, which is a simple yet efficient modification of the popular\nself-attention mechanism and captures the same rich spatial context at a small\nfraction of the computational cost, by changing the order of operations.\nMoreover, to efficiently process high-resolution input, we apply an additional\nspatial reduction to intermediate feature stages of the network with minimal\nloss in accuracy thanks to the use of the fast attention module to fuse\nfeatures. We validate our method with a series of experiments, and show that\nresults on multiple datasets demonstrate superior performance with better\naccuracy and speed compared to existing approaches for real-time semantic\nsegmentation. On Cityscapes, our network achieves 74.4$\\%$ mIoU at 72 FPS and\n75.5$\\%$ mIoU at 58 FPS on a single Titan X GPU, which is~$\\sim$50$\\%$ faster\nthan the state-of-the-art while retaining the same accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:37:16 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 22:44:34 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Hu", "Ping", ""], ["Perazzi", "Federico", ""], ["Heilbron", "Fabian Caba", ""], ["Wang", "Oliver", ""], ["Lin", "Zhe", ""], ["Saenko", "Kate", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2007.03817", "submitter": "Franco Matzkin", "authors": "Franco Matzkin, Virginia Newcombe, Susan Stevenson, Aneesh Khetani,\n  Tom Newman, Richard Digby, Andrew Stevens, Ben Glocker, Enzo Ferrante", "title": "Self-supervised Skull Reconstruction in Brain CT Images with\n  Decompressive Craniectomy", "comments": "Accepted for publication in MICCAI 2020. Update: Figure 1 corrected\n  to match description", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decompressive craniectomy (DC) is a common surgical procedure consisting of\nthe removal of a portion of the skull that is performed after incidents such as\nstroke, traumatic brain injury (TBI) or other events that could result in acute\nsubdural hemorrhage and/or increasing intracranial pressure. In these cases, CT\nscans are obtained to diagnose and assess injuries, or guide a certain therapy\nand intervention.\n  We propose a deep learning based method to reconstruct the skull defect\nremoved during DC performed after TBI from post-operative CT images. This\nreconstruction is useful in multiple scenarios, e.g. to support the creation of\ncranioplasty plates, accurate measurements of bone flap volume and total\nintracranial volume, important for studies that aim to relate later atrophy to\npatient outcome. We propose and compare alternative self-supervised methods\nwhere an encoder-decoder convolutional neural network (CNN) estimates the\nmissing bone flap on post-operative CTs. The self-supervised learning strategy\nonly requires images with complete skulls and avoids the need for annotated DC\nimages. For evaluation, we employ real and simulated images with DC, comparing\nthe results with other state-of-the-art approaches. The experiments show that\nthe proposed model outperforms current manual methods, enabling reconstruction\neven in highly challenging cases where big skull defects have been removed\nduring surgery.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:38:38 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 00:33:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Matzkin", "Franco", ""], ["Newcombe", "Virginia", ""], ["Stevenson", "Susan", ""], ["Khetani", "Aneesh", ""], ["Newman", "Tom", ""], ["Digby", "Richard", ""], ["Stevens", "Andrew", ""], ["Glocker", "Ben", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2007.03838", "submitter": "Wei Li", "authors": "Junhua Zou, Zhisong Pan, Junyang Qiu, Yexin Duan, Xin Liu, Yu Pan", "title": "Making Adversarial Examples More Transferable and Indistinguishable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous methods generate adversarial examples based on the fast\ngradient sign attack series. However, these methods cannot balance the\nindistinguishability and transferability due to the limitations of the basic\nsign structure. To address this problem, we propose an ADAM iterative fast\ngradient tanh method (AI-FGTM) to generate indistinguishable adversarial\nexamples with high transferability. Extensive experiments on the ImageNet\ndataset show that our method generates more indistinguishable adversarial\nexamples and achieves higher black-box attack success rates without extra\nrunning time and resource. Our best attack, TI-DI-AITM, can fool six black-box\ndefense models with an average success rate of 88.0\\%. We expect that our\nmethod will serve as a new baseline for generating adversarial examples with\nmore transferability and indistinguishability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 01:12:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Zou", "Junhua", ""], ["Pan", "Zhisong", ""], ["Qiu", "Junyang", ""], ["Duan", "Yexin", ""], ["Liu", "Xin", ""], ["Pan", "Yu", ""]]}, {"id": "2007.03844", "submitter": "Zexi Chen", "authors": "Zexi Chen, Bharathkumar Ramachandra, Ranga Raju Vatsavai", "title": "Consistency Regularization with Generative Adversarial Networks for\n  Semi-Supervised Learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) based semi-supervised learning (SSL)\napproaches are shown to improve classification performance by utilizing a large\nnumber of unlabeled samples in conjunction with limited labeled samples.\nHowever, their performance still lags behind the state-of-the-art non-GAN based\nSSL approaches. We identify that the main reason for this is the lack of\nconsistency in class probability predictions on the same image under local\nperturbations. Following the general literature, we address this issue via\nlabel consistency regularization, which enforces the class probability\npredictions for an input image to be unchanged under various\nsemantic-preserving perturbations. In this work, we introduce consistency\nregularization into the vanilla semi-GAN to address this critical limitation.\nIn particular, we present a new composite consistency regularization method\nwhich, in spirit, leverages both local consistency and interpolation\nconsistency. We demonstrate the efficacy of our approach on two SSL image\nclassification benchmark datasets, SVHN and CIFAR-10. Our experiments show that\nthis new composite consistency regularization based semi-GAN significantly\nimproves its performance and achieves new state-of-the-art performance among\nGAN-based SSL approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 01:47:10 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 08:23:06 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chen", "Zexi", ""], ["Ramachandra", "Bharathkumar", ""], ["Vatsavai", "Ranga Raju", ""]]}, {"id": "2007.03848", "submitter": "Anoop Cherian", "authors": "Shijie Geng, Peng Gao, Moitreya Chatterjee, Chiori Hori, Jonathan Le\n  Roux, Yongfeng Zhang, Hongsheng Li, Anoop Cherian", "title": "Dynamic Graph Representation Learning for Video Dialog via Multi-Modal\n  Shuffled Transformers", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an input video, its associated audio, and a brief caption, the\naudio-visual scene aware dialog (AVSD) task requires an agent to indulge in a\nquestion-answer dialog with a human about the audio-visual content. This task\nthus poses a challenging multi-modal representation learning and reasoning\nscenario, advancements into which could influence several human-machine\ninteraction applications. To solve this task, we introduce a\nsemantics-controlled multi-modal shuffled Transformer reasoning framework,\nconsisting of a sequence of Transformer modules, each taking a modality as\ninput and producing representations conditioned on the input question. Our\nproposed Transformer variant uses a shuffling scheme on their multi-head\noutputs, demonstrating better regularization. To encode fine-grained visual\ninformation, we present a novel dynamic scene graph representation learning\npipeline that consists of an intra-frame reasoning layer producing\nspatio-semantic graph representations for every frame, and an inter-frame\naggregation module capturing temporal cues. Our entire pipeline is trained\nend-to-end. We present experiments on the benchmark AVSD dataset, both on\nanswer generation and selection tasks. Our results demonstrate state-of-the-art\nperformances on all evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:00:22 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 20:04:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Geng", "Shijie", ""], ["Gao", "Peng", ""], ["Chatterjee", "Moitreya", ""], ["Hori", "Chiori", ""], ["Roux", "Jonathan Le", ""], ["Zhang", "Yongfeng", ""], ["Li", "Hongsheng", ""], ["Cherian", "Anoop", ""]]}, {"id": "2007.03851", "submitter": "Yujian Feng", "authors": "Xiaofeng Zhang, Feng Chen, Cailing Wang, Songsong Wu, Ming Tao and\n  Guoping Jiang", "title": "SiENet: Siamese Expansion Network for Image Extrapolation", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2020.3019705", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from image inpainting, image outpainting has relative less context\nin the image center to capture and more content at the image border to predict.\nTherefore, classical encoder-decoder pipeline of existing methods may not\npredict the outstretched unknown content perfectly. In this paper, a novel\ntwo-stage siamese adversarial model for image extrapolation, named Siamese\nExpansion Network (SiENet) is proposed. In two stages, a novel border sensitive\nconvolution named adaptive filling convolution is designed for allowing encoder\nto predict the unknown content, alleviating the burden of decoder. Besides, to\nintroduce prior knowledge to network and reinforce the inferring ability of\nencoder, siamese adversarial mechanism is designed to enable our network to\nmodel the distribution of covered long range feature for that of uncovered\nimage feature. The results on four datasets has demonstrated that our method\noutperforms existing state-of-the-arts and could produce realistic results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:17:22 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhang", "Xiaofeng", ""], ["Chen", "Feng", ""], ["Wang", "Cailing", ""], ["Wu", "Songsong", ""], ["Tao", "Ming", ""], ["Jiang", "Guoping", ""]]}, {"id": "2007.03858", "submitter": "Zerong Zheng", "authors": "Zerong Zheng and Tao Yu and Yebin Liu and Qionghai Dai", "title": "PaMIR: Parametric Model-Conditioned Implicit Representation for\n  Image-based Human Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling 3D humans accurately and robustly from a single image is very\nchallenging, and the key for such an ill-posed problem is the 3D representation\nof the human models. To overcome the limitations of regular 3D representations,\nwe propose Parametric Model-Conditioned Implicit Representation (PaMIR), which\ncombines the parametric body model with the free-form deep implicit function.\nIn our PaMIR-based reconstruction framework, a novel deep neural network is\nproposed to regularize the free-form deep implicit function using the semantic\nfeatures of the parametric model, which improves the generalization ability\nunder the scenarios of challenging poses and various clothing topologies.\nMoreover, a novel depth-ambiguity-aware training loss is further integrated to\nresolve depth ambiguities and enable successful surface detail reconstruction\nwith imperfect body reference. Finally, we propose a body reference\noptimization method to improve the parametric model estimation accuracy and to\nenhance the consistency between the parametric model and the implicit function.\nWith the PaMIR representation, our framework can be easily extended to\nmulti-image input scenarios without the need of multi-camera calibration and\npose synchronization. Experimental results demonstrate that our method achieves\nstate-of-the-art performance for image-based 3D human reconstruction in the\ncases of challenging poses and clothing types.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:26:19 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 12:12:03 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zheng", "Zerong", ""], ["Yu", "Tao", ""], ["Liu", "Yebin", ""], ["Dai", "Qionghai", ""]]}, {"id": "2007.03868", "submitter": "Li Xiao", "authors": "Gonglei Shi, Li Xiao, Yang Chen, S. Kevin Zhou", "title": "Marginal loss and exclusion loss for partially supervised multi-organ\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating multiple organs in medical images is both costly and\ntime-consuming; therefore, existing multi-organ datasets with labels are often\nlow in sample size and mostly partially labeled, that is, a dataset has a few\norgans labeled but not all organs. In this paper, we investigate how to learn a\nsingle multi-organ segmentation network from a union of such datasets. To this\nend, we propose two types of novel loss function, particularly designed for\nthis scenario: (i) marginal loss and (ii) exclusion loss. Because the\nbackground label for a partially labeled image is, in fact, a `merged' label of\nall unlabelled organs and `true' background (in the sense of full labels), the\nprobability of this `merged' background label is a marginal probability,\nsumming the relevant probabilities before merging. This marginal probability\ncan be plugged into any existing loss function (such as cross entropy loss,\nDice loss, etc.) to form a marginal loss. Leveraging the fact that the organs\nare non-overlapping, we propose the exclusion loss to gauge the dissimilarity\nbetween labeled organs and the estimated segmentation of unlabelled organs.\nExperiments on a union of five benchmark datasets in multi-organ segmentation\nof liver, spleen, left and right kidneys, and pancreas demonstrate that using\nour newly proposed loss functions brings a conspicuous performance improvement\nfor state-of-the-art methods without introducing any extra computation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:59:21 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Shi", "Gonglei", ""], ["Xiao", "Li", ""], ["Chen", "Yang", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2007.03874", "submitter": "Kamran Ali", "authors": "Kamran Ali, Alex X. Liu", "title": "Fine-grained Vibration Based Sensing Using a Smartphone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recognizing surfaces based on their vibration signatures is useful as it can\nenable tagging of different locations without requiring any additional hardware\nsuch as Near Field Communication (NFC) tags. However, previous vibration based\nsurface recognition schemes either use custom hardware for creating and sensing\nvibration, which makes them difficult to adopt, or use inertial (IMU) sensors\nin commercial off-the-shelf (COTS) smartphones to sense movements produced due\nto vibrations, which makes them coarse-grained because of the low sampling\nrates of IMU sensors. The mainstream COTS smartphones based schemes are also\nsusceptible to inherent hardware based irregularities in vibration mechanism of\nthe smartphones. Moreover, the existing schemes that use microphones to sense\nvibration are prone to short-term and constant background noises (e.g.\nintermittent talking, exhaust fan, etc.) because microphones not only capture\nthe sounds created by vibration but also other interfering sounds present in\nthe environment. In this paper, we propose VibroTag, a robust and practical\nvibration based sensing scheme that works with smartphones with different\nhardware, can extract fine-grained vibration signatures of different surfaces,\nand is robust to environmental noise and hardware based irregularities. We\nimplemented VibroTag on two different Android phones and evaluated in multiple\ndifferent environments where we collected data from 4 individuals for 5 to 20\nconsecutive days. Our results show that VibroTag achieves an average accuracy\nof 86.55% while recognizing 24 different locations/surfaces, even when some of\nthose surfaces were made of similar material. VibroTag's accuracy is 37% higher\nthan the average accuracy of 49.25% achieved by one of the state-of-the-art\nIMUs based schemes, which we implemented for comparison with VibroTag.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 03:18:23 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 22:34:46 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ali", "Kamran", ""], ["Liu", "Alex X.", ""]]}, {"id": "2007.03877", "submitter": "Dooseop Choi Dr", "authors": "Dooseop Choi, Seung-jun Han, Kyoungwook Min, Jeongdan Choi", "title": "PathGAN: Local Path Planning with Attentive Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve autonomous driving without high-definition maps, we present a\nmodel capable of generating multiple plausible paths from egocentric images for\nautonomous vehicles. Our generative model comprises two neural networks: the\nfeature extraction network (FEN) and path generation network (PGN). The FEN\nextracts meaningful features from an egocentric image, whereas the PGN\ngenerates multiple paths from the features, given a driving intention and\nspeed. To ensure that the paths generated are plausible and consistent with the\nintention, we introduce an attentive discriminator and train it with the PGN\nunder generative adversarial networks framework. We also devise an interaction\nmodel between the positions in the paths and the intentions hidden in the\npositions and design a novel PGN architecture that reflects the interaction\nmodel, resulting in the improvement of the accuracy and diversity of the\ngenerated paths. Finally, we introduce ETRIDriving, a dataset for autonomous\ndriving in which the recorded sensor data are labeled with discrete high-level\ndriving actions, and demonstrate the state-of-the-art performance of the\nproposed model on ETRIDriving in terms of accuracy and diversity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 03:31:58 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 22:54:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Choi", "Dooseop", ""], ["Han", "Seung-jun", ""], ["Min", "Kyoungwook", ""], ["Choi", "Jeongdan", ""]]}, {"id": "2007.03882", "submitter": "Chuang Niu", "authors": "Chuang Niu, Wenxiang Cong, Fenglei Fan, Hongming Shan, Mengzhou Li,\n  Jimin Liang, Ge Wang", "title": "Low-dimensional Manifold Constrained Disentanglement Network for Metal\n  Artifact Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based methods have achieved promising results for CT\nmetal artifact reduction (MAR), most of which use many synthesized paired\nimages for training. As synthesized metal artifacts in CT images may not\naccurately reflect the clinical counterparts, an artifact disentanglement\nnetwork (ADN) was proposed with unpaired clinical images directly, producing\npromising results on clinical datasets. However, without sufficient\nsupervision, it is difficult for ADN to recover structural details of\nartifact-affected CT images based on adversarial losses only. To overcome these\nproblems, here we propose a low-dimensional manifold (LDM) constrained\ndisentanglement network (DN), leveraging the image characteristics that the\npatch manifold is generally low-dimensional. Specifically, we design an LDM-DN\nlearning algorithm to empower the disentanglement network through optimizing\nthe synergistic network loss functions while constraining the recovered images\nto be on a low-dimensional patch manifold. Moreover, learning from both paired\nand unpaired data, an efficient hybrid optimization scheme is proposed to\nfurther improve the MAR performance on clinical datasets. Extensive experiments\ndemonstrate that the proposed LDM-DN approach can consistently improve the MAR\nperformance in paired and/or unpaired learning settings, outperforming\ncompeting methods on synthesized and clinical datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 03:47:34 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Niu", "Chuang", ""], ["Cong", "Wenxiang", ""], ["Fan", "Fenglei", ""], ["Shan", "Hongming", ""], ["Li", "Mengzhou", ""], ["Liang", "Jimin", ""], ["Wang", "Ge", ""]]}, {"id": "2007.03887", "submitter": "Yunhan Zhao", "authors": "Yunhan Zhao, Shu Kong, Charless Fowlkes", "title": "Camera Pose Matters: Improving Depth Prediction by Mitigating Pose\n  Distribution Bias", "comments": "Accepted at CVPR2021, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth predictors are typically trained on large-scale training sets\nwhich are naturally biased w.r.t the distribution of camera poses. As a result,\ntrained predictors fail to make reliable depth predictions for testing examples\ncaptured under uncommon camera poses. To address this issue, we propose two\nnovel techniques that exploit the camera pose during training and prediction.\nFirst, we introduce a simple perspective-aware data augmentation that\nsynthesizes new training examples with more diverse views by perturbing the\nexisting ones in a geometrically consistent manner. Second, we propose a\nconditional model that exploits the per-image camera pose as prior knowledge by\nencoding it as a part of the input. We show that jointly applying the two\nmethods improves depth prediction on images captured under uncommon and even\nnever-before-seen camera poses. We show that our methods improve performance\nwhen applied to a range of different predictor architectures. Lastly, we show\nthat explicitly encoding the camera pose distribution improves the\ngeneralization performance of a synthetically trained depth predictor when\nevaluated on real images.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 04:14:17 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 05:51:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Yunhan", ""], ["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "2007.03891", "submitter": "Qi Zhang", "authors": "Qi Zhang and Antoni B. Chan", "title": "Single-Frame based Deep View Synchronization for Unsynchronized\n  Multi-Camera Surveillance", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-camera surveillance has been an active research topic for understanding\nand modeling scenes. Compared to a single camera, multi-cameras provide larger\nfield-of-view and more object cues, and the related applications are multi-view\ncounting, multi-view tracking, 3D pose estimation or 3D reconstruction, etc. It\nis usually assumed that the cameras are all temporally synchronized when\ndesigning models for these multi-camera based tasks. However, this assumption\nis not always valid,especially for multi-camera systems with network\ntransmission delay and low frame-rates due to limited network bandwidth,\nresulting in desynchronization of the captured frames across cameras. To handle\nthe issue of unsynchronized multi-cameras, in this paper, we propose a\nsynchronization model that works in conjunction with existing DNN-based\nmulti-view models, thus avoiding the redesign of the whole model. Under the\nlow-fps regime, we assume that only a single relevant frame is available from\neach view, and synchronization is achieved by matching together image contents\nguided by epipolar geometry. We consider two variants of the model, based on\nwhere in the pipeline the synchronization occurs, scene-level synchronization\nand camera-level synchronization. The view synchronization step and the\ntask-specific view fusion and prediction step are unified in the same framework\nand trained in an end-to-end fashion. Our view synchronization models are\napplied to different DNNs-based multi-camera vision tasks under the\nunsynchronized setting, including multi-view counting and 3D pose estimation,\nand achieve good performance compared to baselines.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 04:39:38 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 07:23:13 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Qi", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2007.03898", "submitter": "Arash Vahdat", "authors": "Arash Vahdat, Jan Kautz", "title": "NVAE: A Deep Hierarchical Variational Autoencoder", "comments": "Neural Information Processing Systems (NeurIPS) 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows, autoregressive models, variational autoencoders (VAEs),\nand deep energy-based models are among competing likelihood-based frameworks\nfor deep generative learning. Among them, VAEs have the advantage of fast and\ntractable sampling and easy-to-access encoding networks. However, they are\ncurrently outperformed by other models such as normalizing flows and\nautoregressive models. While the majority of the research in VAEs is focused on\nthe statistical challenges, we explore the orthogonal direction of carefully\ndesigning neural architectures for hierarchical VAEs. We propose Nouveau VAE\n(NVAE), a deep hierarchical VAE built for image generation using depth-wise\nseparable convolutions and batch normalization. NVAE is equipped with a\nresidual parameterization of Normal distributions and its training is\nstabilized by spectral regularization. We show that NVAE achieves\nstate-of-the-art results among non-autoregressive likelihood-based models on\nthe MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong\nbaseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art\nfrom 2.98 to 2.91 bits per dimension, and it produces high-quality images on\nCelebA HQ. To the best of our knowledge, NVAE is the first successful VAE\napplied to natural images as large as 256$\\times$256 pixels. The source code is\navailable at https://github.com/NVlabs/NVAE .\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 04:56:56 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 17:27:38 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 03:08:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Vahdat", "Arash", ""], ["Kautz", "Jan", ""]]}, {"id": "2007.03903", "submitter": "Fangxin Liu", "authors": "Liu Fangxin, Zhao Wenbo, Wang Yanzhi, Dai Changzhi, Jiang Li", "title": "AUSN: Approximately Uniform Quantization by Adaptively Superimposing\n  Non-uniform Distribution for Deep Neural Networks", "comments": "16 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is essential to simplify DNN inference in edge applications.\nExisting uniform and non-uniform quantization methods, however, exhibit an\ninherent conflict between the representing range and representing resolution,\nand thereby result in either underutilized bit-width or significant accuracy\ndrop. Moreover, these methods encounter three drawbacks: i) the absence of a\nquantitative metric for in-depth analysis of the source of the quantization\nerrors; ii) the limited focus on the image classification tasks based on CNNs;\niii) the unawareness of the real hardware and energy consumption reduced by\nlowering the bit-width. In this paper, we first define two quantitative\nmetrics, i.e., the Clipping Error and rounding error, to analyze the\nquantization error distribution. We observe that the boundary- and rounding-\nerrors vary significantly across layers, models and tasks. Consequently, we\npropose a novel quantization method to quantize the weight and activation. The\nkey idea is to Approximate the Uniform quantization by Adaptively Superposing\nmultiple Non-uniform quantized values, namely AUSN. AUSN is consist of a\ndecoder-free coding scheme that efficiently exploits the bit-width to its\nextreme, a superposition quantization algorithm that can adapt the coding\nscheme to different DNN layers, models and tasks without extra hardware design\neffort, and a rounding scheme that can eliminate the well-known bit-width\noverflow and re-quantization issues. Theoretical analysis~(see Appendix A) and\naccuracy evaluation on various DNN models of different tasks show the\neffectiveness and generalization of AUSN. The synthesis~(see Appendix B)\nresults on FPGA show $2\\times$ reduction of the energy consumption, and\n$2\\times$ to $4\\times$ reduction of the hardware resource.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 05:10:53 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fangxin", "Liu", ""], ["Wenbo", "Zhao", ""], ["Yanzhi", "Wang", ""], ["Changzhi", "Dai", ""], ["Li", "Jiang", ""]]}, {"id": "2007.03924", "submitter": "Umair Bin Waheed", "authors": "Umair bin Waheed, Ahmed Shaheen, Mike Fehler, Ben Fulcher", "title": "Winning with Simple Learning Models: Detecting Earthquakes in Groningen,\n  the Netherlands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is fast emerging as a potential disruptive tool to tackle\nlongstanding research problems across the sciences. Notwithstanding its success\nacross disciplines, the recent trend of the overuse of deep learning is\nconcerning to many machine learning practitioners. Recently, seismologists have\nalso demonstrated the efficacy of deep learning algorithms in detecting low\nmagnitude earthquakes. Here, we revisit the problem of seismic event detection\nbut using a logistic regression model with feature extraction. We select\nwell-discriminating features from a huge database of time-series operations\ncollected from interdisciplinary time-series analysis methods. Using a simple\nlearning model with only five trainable parameters, we detect several\nlow-magnitude induced earthquakes from the Groningen gas field that are not\npresent in the catalog. We note that the added advantage of simpler models is\nthat the selected features add to our understanding of the noise and event\nclasses present in the dataset. Since simpler models are easy to maintain,\ndebug, understand, and train, through this study we underscore that it might be\na dangerous pursuit to use deep learning without carefully weighing simpler\nalternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:06:09 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Waheed", "Umair bin", ""], ["Shaheen", "Ahmed", ""], ["Fehler", "Mike", ""], ["Fulcher", "Ben", ""]]}, {"id": "2007.03938", "submitter": "Minsoo Kang", "authors": "Minsoo Kang and Bohyung Han", "title": "Operation-Aware Soft Channel Pruning using Differentiable Masks", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple but effective data-driven channel pruning algorithm,\nwhich compresses deep neural networks in a differentiable way by exploiting the\ncharacteristics of operations. The proposed approach makes a joint\nconsideration of batch normalization (BN) and rectified linear unit (ReLU) for\nchannel pruning; it estimates how likely the two successive operations\ndeactivate each feature map and prunes the channels with high probabilities. To\nthis end, we learn differentiable masks for individual channels and make soft\ndecisions throughout the optimization procedure, which facilitates to explore\nlarger search space and train more stable networks. The proposed framework\nenables us to identify compressed models via a joint learning of model\nparameters and channel pruning without an extra procedure of fine-tuning. We\nperform extensive experiments and achieve outstanding performance in terms of\nthe accuracy of output networks given the same amount of resources when\ncompared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:44:00 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 02:33:59 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kang", "Minsoo", ""], ["Han", "Bohyung", ""]]}, {"id": "2007.03943", "submitter": "Hsin-Ping Chou", "authors": "Hsin-Ping Chou, Shih-Chieh Chang, Jia-Yu Pan, Wei Wei, Da-Cheng Juan", "title": "Remix: Rebalanced Mixup", "comments": "16 pages, 4 figures, ECCV'2020 Workshop on IPCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image classifiers often perform poorly when training data are heavily\nclass-imbalanced. In this work, we propose a new regularization technique,\nRemix, that relaxes Mixup's formulation and enables the mixing factors of\nfeatures and labels to be disentangled. Specifically, when mixing two samples,\nwhile features are mixed in the same fashion as Mixup, Remix assigns the label\nin favor of the minority class by providing a disproportionately higher weight\nto the minority class. By doing so, the classifier learns to push the decision\nboundaries towards the majority classes and balance the generalization error\nbetween majority and minority classes. We have studied the state-of-the art\nregularization techniques such as Mixup, Manifold Mixup and CutMix under\nclass-imbalanced regime, and shown that the proposed Remix significantly\noutperforms these state-of-the-arts and several re-weighting and re-sampling\ntechniques, on the imbalanced datasets constructed by CIFAR-10, CIFAR-100, and\nCINIC-10. We have also evaluated Remix on a real-world large-scale imbalanced\ndataset, iNaturalist 2018. The experimental results confirmed that Remix\nprovides consistent and significant improvements over the previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:01:15 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 14:30:17 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 13:33:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chou", "Hsin-Ping", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "2007.03951", "submitter": "Chunwei Tian", "authors": "Chunwei Tian, Yong Xu, Wangmeng Zuo, Bo Du, Chia-Wen Lin and David\n  Zhang", "title": "Designing and Training of A Dual CNN for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) for image denoising have recently\nattracted increasing research interest. However, plain networks cannot recover\nfine details for a complex task, such as real noisy images. In this paper, we\npropsoed a Dual denoising Network (DudeNet) to recover a clean image.\nSpecifically, DudeNet consists of four modules: a feature extraction block, an\nenhancement block, a compression block, and a reconstruction block. The feature\nextraction block with a sparse machanism extracts global and local features via\ntwo sub-networks. The enhancement block gathers and fuses the global and local\nfeatures to provide complementary information for the latter network. The\ncompression block refines the extracted information and compresses the network.\nFinally, the reconstruction block is utilized to reconstruct a denoised image.\nThe DudeNet has the following advantages: (1) The dual networks with a parse\nmechanism can extract complementary features to enhance the generalized ability\nof denoiser. (2) Fusing global and local features can extract salient features\nto recover fine details for complex noisy images. (3) A Small-size filter is\nused to reduce the complexity of denoiser. Extensive experiments demonstrate\nthe superiority of DudeNet over existing current state-of-the-art denoising\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:16:24 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Tian", "Chunwei", ""], ["Xu", "Yong", ""], ["Zuo", "Wangmeng", ""], ["Du", "Bo", ""], ["Lin", "Chia-Wen", ""], ["Zhang", "David", ""]]}, {"id": "2007.03956", "submitter": "Tomer Yeminy", "authors": "Tomer Yeminy and Ori Katz", "title": "Guidestar-free image-guided wavefront-shaping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical imaging through scattering media is a fundamental challenge in many\napplications. Recently, substantial breakthroughs such as imaging through\nbiological tissues and looking around corners have been obtained by the use of\nwavefront-shaping approaches. However, these require an implanted guide-star\nfor determining the wavefront correction, controlled coherent illumination, and\nmost often raster scanning of the shaped focus. Alternative novel computational\napproaches that exploit speckle correlations, avoid guide-stars and wavefront\ncontrol but are limited to small two-dimensional objects contained within the\nmemory-effect correlations range. Here, we present a new concept, image-guided\nwavefront-shaping, allowing non-invasive, guidestar-free, widefield, incoherent\nimaging through highly scattering layers, without illumination control. Most\nimportantly, the wavefront-correction is found even for objects that are larger\nthan the memory-effect range, by blindly optimizing image-quality metrics. We\ndemonstrate imaging of extended objects through highly-scattering layers and\nmulti-core fibers, paving the way for non-invasive imaging in various\napplications, from microscopy to endoscopy.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:26:14 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Yeminy", "Tomer", ""], ["Katz", "Ori", ""]]}, {"id": "2007.03979", "submitter": "Kun Li", "authors": "Kun Li, Jing Yang, Nianhong Jiao, Jinsong Zhang, and Yu-Kun Lai", "title": "Adaptive 3D Face Reconstruction from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction from a single image is a challenging problem,\nespecially under partial occlusions and extreme poses. This is because the\nuncertainty of the estimated 2D landmarks will affect the quality of face\nreconstruction. In this paper, we propose a novel joint 2D and 3D optimization\nmethod to adaptively reconstruct 3D face shapes from a single image, which\ncombines the depths of 3D landmarks to solve the uncertain detections of\ninvisible landmarks. The strategy of our method involves two aspects: a\ncoarse-to-fine pose estimation using both 2D and 3D landmarks, and an adaptive\n2D and 3D re-weighting based on the refined pose parameter to recover accurate\n3D faces. Experimental results on multiple datasets demonstrate that our method\ncan generate high-quality reconstruction from a single color image and is\nrobust for self-occlusion and large poses.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 09:35:26 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 07:29:24 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Li", "Kun", ""], ["Yang", "Jing", ""], ["Jiao", "Nianhong", ""], ["Zhang", "Jinsong", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2007.03995", "submitter": "Nabeel Seedat", "authors": "Nabeel Seedat", "title": "MCU-Net: A framework towards uncertainty representations for decision\n  support system patient referrals in healthcare contexts", "comments": "4 pages, 4 figures,Spotlight Talk at KDD 2020 - Applied Data Science\n  for Healthcare Workshop & presented at ICML 2020: Uncertainty and Robustness\n  in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating a human-in-the-loop system when deploying automated decision\nsupport is critical in healthcare contexts to create trust, as well as provide\nreliable performance on a patient-to-patient basis. Deep learning methods while\nhaving high performance, do not allow for this patient-centered approach due to\nthe lack of uncertainty representation. Thus, we present a framework of\nuncertainty representation evaluated for medical image segmentation, using\nMCU-Net which combines a U-Net with Monte Carlo Dropout, evaluated with four\ndifferent uncertainty metrics. The framework augments this by adding a\nhuman-in-the-loop aspect based on an uncertainty threshold for automated\nreferral of uncertain cases to a medical professional. We demonstrate that\nMCU-Net combined with epistemic uncertainty and an uncertainty threshold tuned\nfor this application maximizes automated performance on an individual patient\nlevel, yet refers truly uncertain cases. This is a step towards uncertainty\nrepresentations when deploying machine learning based decision support in\nhealthcare settings.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 09:54:56 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:52:00 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 11:12:16 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Seedat", "Nabeel", ""]]}, {"id": "2007.04023", "submitter": "Dan Levi", "authors": "Noa Garnett, Roy Uziel, Netalee Efrat, Dan Levi", "title": "Synthetic-to-Real Domain Adaptation for Lane Detection", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate lane detection, a crucial enabler for autonomous driving, currently\nrelies on obtaining a large and diverse labeled training dataset. In this work,\nwe explore learning from abundant, randomly generated synthetic data, together\nwith unlabeled or partially labeled target domain data, instead. Randomly\ngenerated synthetic data has the advantage of controlled variability in the\nlane geometry and lighting, but it is limited in terms of photo-realism. This\nposes the challenge of adapting models learned on the unrealistic synthetic\ndomain to real images. To this end we develop a novel autoencoder-based\napproach that uses synthetic labels unaligned with particular images for\nadapting to target domain data. In addition, we explore existing domain\nadaptation approaches, such as image translation and self-supervision, and\nadjust them to the lane detection task. We test all approaches in the\nunsupervised domain adaptation setting in which no target domain labels are\navailable and in the semi-supervised setting in which a small portion of the\ntarget images are labeled. In extensive experiments using three different\ndatasets, we demonstrate the possibility to save costly target domain labeling\nefforts. For example, using our proposed autoencoder approach on the llamas and\ntuSimple lane datasets, we can almost recover the fully supervised accuracy\nwith only 10% of the labeled data. In addition, our autoencoder approach\noutperforms all other methods in the semi-supervised domain adaptation\nscenario.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:54:21 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 12:52:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Garnett", "Noa", ""], ["Uziel", "Roy", ""], ["Efrat", "Netalee", ""], ["Levi", "Dan", ""]]}, {"id": "2007.04039", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Maryam Mirsadeghi, Timoth\\'ee Masquelier", "title": "BS4NN: Binarized Spiking Neural Networks with Temporal Coding and\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently proposed the S4NN algorithm, essentially an adaptation of\nbackpropagation to multilayer spiking neural networks that use simple non-leaky\nintegrate-and-fire neurons and a form of temporal coding known as\ntime-to-first-spike coding. With this coding scheme, neurons fire at most once\nper stimulus, but the firing order carries information. Here, we introduce\nBS4NN, a modification of S4NN in which the synaptic weights are constrained to\nbe binary (+1 or -1), in order to decrease memory and computation footprints.\nThis was done using two sets of weights: firstly, real-valued weights, updated\nby gradient descent, and used in the backward pass of backpropagation, and\nsecondly, their signs, used in the forward pass. Similar strategies have been\nused to train (non-spiking) binarized neural networks. The main difference is\nthat BS4NN operates in the time domain: spikes are propagated sequentially, and\ndifferent neurons may reach their threshold at different times, which increases\ncomputational power. We validated BS4NN on two popular benchmarks, MNIST and\nFashion MNIST, and obtained state-of-the-art accuracies for this sort of\nnetworks (97.0% and 87.3% respectively) with a negligible accuracy drop with\nrespect to real-valued weights (0.4% and 0.7%, respectively). We also\ndemonstrated that BS4NN outperforms a simple BNN with the same architectures on\nthose two datasets (by 0.2% and 0.9% respectively), presumably because it\nleverages the temporal dimension.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:31:32 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Mirsadeghi", "Maryam", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "2007.04101", "submitter": "Peng Xu", "authors": "Peng Xu, Yongye Huang, Tongtong Yuan, Tao Xiang, Timothy M.\n  Hospedales, Yi-Zhe Song, Liang Wang", "title": "On Learning Semantic Representations for Million-Scale Free-Hand\n  Sketches", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.01401", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study learning semantic representations for million-scale\nfree-hand sketches. This is highly challenging due to the domain-unique traits\nof sketches, e.g., diverse, sparse, abstract, noisy. We propose a dual-branch\nCNNRNN network architecture to represent sketches, which simultaneously encodes\nboth the static and temporal patterns of sketch strokes. Based on this\narchitecture, we further explore learning the sketch-oriented semantic\nrepresentations in two challenging yet practical settings, i.e., hashing\nretrieval and zero-shot recognition on million-scale sketches. Specifically, we\nuse our dual-branch architecture as a universal representation framework to\ndesign two sketch-specific deep models: (i) We propose a deep hashing model for\nsketch retrieval, where a novel hashing loss is specifically designed to\naccommodate both the abstract and messy traits of sketches. (ii) We propose a\ndeep embedding model for sketch zero-shot recognition, via collecting a\nlarge-scale edge-map dataset and proposing to extract a set of semantic vectors\nfrom edge-maps as the semantic knowledge for sketch zero-shot domain alignment.\nBoth deep models are evaluated by comprehensive experiments on million-scale\nsketches and outperform the state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:23:22 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Xu", "Peng", ""], ["Huang", "Yongye", ""], ["Yuan", "Tongtong", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Song", "Yi-Zhe", ""], ["Wang", "Liang", ""]]}, {"id": "2007.04108", "submitter": "Matteo Dunnhofer", "authors": "Matteo Dunnhofer, Niki Martinel, Christian Micheloni", "title": "Tracking-by-Trackers with a Distilled and Reinforced Model", "comments": "Asian Conference on Computer Vision (ACCV) 2020", "journal-ref": null, "doi": "10.1007/978-3-030-69532-3_38", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking was generally tackled by reasoning independently on\nfast processing algorithms, accurate online adaptation methods, and fusion of\ntrackers. In this paper, we unify such goals by proposing a novel tracking\nmethodology that takes advantage of other visual trackers, offline and online.\nA compact student model is trained via the marriage of knowledge distillation\nand reinforcement learning. The first allows to transfer and compress tracking\nknowledge of other trackers. The second enables the learning of evaluation\nmeasures which are then exploited online. After learning, the student can be\nultimately used to build (i) a very fast single-shot tracker, (ii) a tracker\nwith a simple and effective online adaptation mechanism, (iii) a tracker that\nperforms fusion of other trackers. Extensive validation shows that the proposed\nalgorithms compete with real-time state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 13:24:04 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 13:42:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Dunnhofer", "Matteo", ""], ["Martinel", "Niki", ""], ["Micheloni", "Christian", ""]]}, {"id": "2007.04118", "submitter": "ShawnXY Yang", "authors": "Xiao Yang, Dingcheng Yang, Yinpeng Dong, Wenjian Yu, Hang Su, Jun Zhu", "title": "Delving into the Adversarial Robustness on Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has recently made substantial progress and achieved high\naccuracy on standard benchmarks based on the development of deep convolutional\nneural networks (CNNs). However, the lack of robustness in deep CNNs to\nadversarial examples has raised security concerns to enormous face recognition\napplications. To facilitate a better understanding of the adversarial\nvulnerability of the existing face recognition models, in this paper we perform\ncomprehensive robustness evaluations, which can be applied as reference for\nevaluating the robustness of subsequent works on face recognition. We\ninvestigate 15 popular face recognition models and evaluate their robustness by\nusing various adversarial attacks as an important surrogate. These evaluations\nare conducted under diverse adversarial settings, including dodging and\nimpersonation attacks, $\\ell_2$ and $\\ell_\\infty$ attacks, white-box and\nblack-box attacks. We further propose a landmark-guided cutout (LGC) attack\nmethod to improve the transferability of adversarial examples for black-box\nattacks, by considering the special characteristics of face recognition. Based\non our evaluations, we draw several important findings, which are crucial for\nunderstanding the adversarial robustness and providing insights for future\nresearch on face recognition. Code is available at\n\\url{https://github.com/ShawnXYang/Face-Robustness-Benchmark}.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 13:39:22 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Yang", "Xiao", ""], ["Yang", "Dingcheng", ""], ["Dong", "Yinpeng", ""], ["Yu", "Wenjian", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2007.04134", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Stavros Petridis, Maja Pantic", "title": "Learning Speech Representations from Raw Audio by Joint Audiovisual\n  Self-Supervision", "comments": "Accepted at the Workshop on Self-supervision in Audio and Speech at\n  ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intuitive interaction between the audio and visual modalities is valuable\nfor cross-modal self-supervised learning. This concept has been demonstrated\nfor generic audiovisual tasks like video action recognition and acoustic scene\nclassification. However, self-supervision remains under-explored for\naudiovisual speech. We propose a method to learn self-supervised speech\nrepresentations from the raw audio waveform. We train a raw audio encoder by\ncombining audio-only self-supervision (by predicting informative audio\nattributes) with visual self-supervision (by generating talking faces from\naudio). The visual pretext task drives the audio representations to capture\ninformation related to lip movements. This enriches the audio encoder with\nvisual information and the encoder can be used for evaluation without the\nvisual modality. Our method attains competitive performance with respect to\nexisting self-supervised audio features on established isolated word\nclassification benchmarks, and significantly outperforms other methods at\nlearning from fewer labels. Notably, our method also outperforms fully\nsupervised training, thus providing a strong initialization for speech related\ntasks. Our results demonstrate the potential of multimodal self-supervision in\naudiovisual speech for learning good audio representations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:07:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shukla", "Abhinav", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2007.04137", "submitter": "Giulio Lovisotto", "authors": "Giulio Lovisotto, Henry Turner, Ivo Sluganovic, Martin Strohmeier,\n  Ivan Martinovic", "title": "SLAP: Improving Physical Adversarial Examples with Short-Lived\n  Adversarial Perturbations", "comments": "13 pages, to be published in Usenix Security 2021, project page\n  https://github.com/ssloxford/short-lived-adversarial-perturbations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into adversarial examples (AE) has developed rapidly, yet static\nadversarial patches are still the main technique for conducting attacks in the\nreal world, despite being obvious, semi-permanent and unmodifiable once\ndeployed.\n  In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a\nnovel technique that allows adversaries to realize physically robust real-world\nAE by using a light projector. Attackers can project a specifically crafted\nadversarial perturbation onto a real-world object, transforming it into an AE.\nThis allows the adversary greater control over the attack compared to\nadversarial patches: (i) projections can be dynamically turned on and off or\nmodified at will, (ii) projections do not suffer from the locality constraint\nimposed by patches, making them harder to detect.\n  We study the feasibility of SLAP in the self-driving scenario, targeting both\nobject detector and traffic sign recognition tasks, focusing on the detection\nof stop signs. We conduct experiments in a variety of ambient light conditions,\nincluding outdoors, showing how in non-bright settings the proposed method\ngenerates AE that are extremely robust, causing misclassifications on\nstate-of-the-art networks with up to 99% success rate for a variety of angles\nand distances. We also demostrate that SLAP-generated AE do not present\ndetectable behaviours seen in adversarial patches and therefore bypass\nSentiNet, a physical AE detection method. We evaluate other defences including\nan adaptive defender using adversarial learning which is able to thwart the\nattack effectiveness up to 80% even in favourable attacker conditions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:11:21 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 13:14:36 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 16:17:39 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Lovisotto", "Giulio", ""], ["Turner", "Henry", ""], ["Sluganovic", "Ivo", ""], ["Strohmeier", "Martin", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2007.04171", "submitter": "Jian Liang", "authors": "Jian Liang and Dapeng Hu and Jiashi Feng", "title": "Domain Adaptation with Auxiliary Target Domain-Oriented Classifier", "comments": "CVPR 2021 camera ready. Code is available at\n  https://github.com/tim-learn/ATDOC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) aims to transfer knowledge from a label-rich but\nheterogeneous domain to a label-scare domain, which alleviates the labeling\nefforts and attracts considerable attention. Different from previous methods\nfocusing on learning domain-invariant feature representations, some recent\nmethods present generic semi-supervised learning (SSL) techniques and directly\napply them to DA tasks, even achieving competitive performance. One of the most\npopular SSL techniques is pseudo-labeling that assigns pseudo labels for each\nunlabeled data via the classifier trained by labeled data. However, it ignores\nthe distribution shift in DA problems and is inevitably biased to source data.\nTo address this issue, we propose a new pseudo-labeling framework called\nAuxiliary Target Domain-Oriented Classifier (ATDOC). ATDOC alleviates the\nclassifier bias by introducing an auxiliary classifier for target data only, to\nimprove the quality of pseudo labels. Specifically, we employ the memory\nmechanism and develop two types of non-parametric classifiers, i.e. the nearest\ncentroid classifier and neighborhood aggregation, without introducing any\nadditional network parameters. Despite its simplicity in a pseudo\nclassification objective, ATDOC with neighborhood aggregation significantly\noutperforms domain alignment techniques and prior SSL techniques on a large\nvariety of DA benchmarks and even scare-labeled SSL tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:01:35 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:27:55 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 13:52:35 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Liang", "Jian", ""], ["Hu", "Dapeng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2007.04174", "submitter": "Angelo Porrello", "authors": "Angelo Porrello, Luca Bergamini, Simone Calderara", "title": "Robust Re-Identification by Multiple Views Knowledge Distillation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve robustness in Re-Identification, standard methods leverage\ntracking information in a Video-To-Video fashion. However, these solutions face\na large drop in performance for single image queries (e.g., Image-To-Video\nsetting). Recent works address this severe degradation by transferring temporal\ninformation from a Video-based network to an Image-based one. In this work, we\ndevise a training strategy that allows the transfer of a superior knowledge,\narising from a set of views depicting the target object. Our proposal - Views\nKnowledge Distillation (VKD) - pins this visual variety as a supervision signal\nwithin a teacher-student framework, where the teacher educates a student who\nobserves fewer views. As a result, the student outperforms not only its teacher\nbut also the current state-of-the-art in Image-To-Video by a wide margin (6.3%\nmAP on MARS, 8.6% on Duke-Video-ReId and 5% on VeRi-776). A thorough analysis -\non Person, Vehicle and Animal Re-ID - investigates the properties of VKD from a\nqualitatively and quantitatively perspective. Code is available at\nhttps://github.com/aimagelab/VKD.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:04:41 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Porrello", "Angelo", ""], ["Bergamini", "Luca", ""], ["Calderara", "Simone", ""]]}, {"id": "2007.04178", "submitter": "Junsuk Choe Dr.", "authors": "Junsuk Choe, Seong Joon Oh, Sanghyuk Chun, Zeynep Akata, Hyunjung Shim", "title": "Evaluation for Weakly Supervised Object Localization: Protocol, Metrics,\n  and Datasets", "comments": "TPAMI submission. First two authors contributed equally. This is a\n  journal extension of our CVPR 2020 paper arXiv:2001.07437. Code:\n  https://github.com/clovaai/wsolevaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object localization (WSOL) has gained popularity over the\nlast years for its promise to train localization models with only image-level\nlabels. Since the seminal WSOL work of class activation mapping (CAM), the\nfield has focused on how to expand the attention regions to cover objects more\nbroadly and localize them better. However, these strategies rely on full\nlocalization supervision for validating hyperparameters and model selection,\nwhich is in principle prohibited under the WSOL setup. In this paper, we argue\nthat WSOL task is ill-posed with only image-level labels, and propose a new\nevaluation protocol where full supervision is limited to only a small held-out\nset not overlapping with the test set. We observe that, under our protocol, the\nfive most recent WSOL methods have not made a major improvement over the CAM\nbaseline. Moreover, we report that existing WSOL methods have not reached the\nfew-shot learning baseline, where the full-supervision at validation time is\nused for model training instead. Based on our findings, we discuss some future\ndirections for WSOL.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:09:16 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Choe", "Junsuk", ""], ["Oh", "Seong Joon", ""], ["Chun", "Sanghyuk", ""], ["Akata", "Zeynep", ""], ["Shim", "Hyunjung", ""]]}, {"id": "2007.04198", "submitter": "Yusheng Xiang", "authors": "Yusheng Xiang, Hongzhe Wang, Tianqing Su, Ruoyu Li, Christine Brach,\n  Samuel S. Mao, Marcus Geimer", "title": "KIT MOMA: A Mobile Machines Dataset", "comments": "15 pages; 17 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile machines typically working in a closed site, have a high potential to\nutilize autonomous driving technology. However, vigorously thriving development\nand innovation are happening mostly in the area of passenger cars. In contrast,\nalthough there are also many research pieces about autonomous driving or\nworking in mobile machines, a consensus about the SOTA solution is still not\nachieved. We believe that the most urgent problem that should be solved is the\nabsence of a public and challenging visual dataset, which makes the results\nfrom different researches comparable. To address the problem, we publish the\nKIT MOMA dataset, including eight classes of commonly used mobile machines,\nwhich can be used as a benchmark to evaluate the SOTA algorithms to detect\nmobile construction machines. The view of the gathered images is outside of the\nmobile machines since we believe fixed cameras on the ground are more suitable\nif all the interesting machines are working in a closed site. Most of the\nimages in KIT MOMA are in a real scene, whereas some of the images are from the\nofficial website of top construction machine companies. Also, we have evaluated\nthe performance of YOLO v3 on our dataset, indicating that the SOTA computer\nvision algorithms already show an excellent performance for detecting the\nmobile machines in a specific working site. Together with the dataset, we also\nupload the trained weights, which can be directly used by engineers from the\nconstruction machine industry. The dataset, trained weights, and updates can be\nfound on our Github. Moreover, the demo can be found on our Youtube.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:39:04 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Xiang", "Yusheng", ""], ["Wang", "Hongzhe", ""], ["Su", "Tianqing", ""], ["Li", "Ruoyu", ""], ["Brach", "Christine", ""], ["Mao", "Samuel S.", ""], ["Geimer", "Marcus", ""]]}, {"id": "2007.04226", "submitter": "David Wood", "authors": "David A. Wood, Sina Kafiabadi, Aisha Al Busaidi, Emily Guilhem, Jeremy\n  Lynch, Matthew Townend, Antanas Montvila, Juveria Siddiqui, Naveen Gadapa,\n  Matthew Benger, Gareth Barker, Sebastian Ourselin, James H. Cole, Thomas C.\n  Booth", "title": "Labelling imaging datasets on the basis of neuroradiology reports: a\n  validation study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) shows promise as a means to automate the\nlabelling of hospital-scale neuroradiology magnetic resonance imaging (MRI)\ndatasets for computer vision applications. To date, however, there has been no\nthorough investigation into the validity of this approach, including\ndetermining the accuracy of report labels compared to image labels as well as\nexamining the performance of non-specialist labellers. In this work, we draw on\nthe experience of a team of neuroradiologists who labelled over 5000 MRI\nneuroradiology reports as part of a project to build a dedicated deep\nlearning-based neuroradiology report classifier. We show that, in our\nexperience, assigning binary labels (i.e. normal vs abnormal) to images from\nreports alone is highly accurate. In contrast to the binary labels, however,\nthe accuracy of more granular labelling is dependent on the category, and we\nhighlight reasons for this discrepancy. We also show that downstream model\nperformance is reduced when labelling of training reports is performed by a\nnon-specialist. To allow other researchers to accelerate their research, we\nmake our refined abnormality definitions and labelling rules available, as well\nas our easy-to-use radiology report labelling app which helps streamline this\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:12:10 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 22:36:08 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 11:04:19 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 14:03:27 GMT"}, {"version": "v5", "created": "Tue, 9 Mar 2021 01:39:06 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wood", "David A.", ""], ["Kafiabadi", "Sina", ""], ["Busaidi", "Aisha Al", ""], ["Guilhem", "Emily", ""], ["Lynch", "Jeremy", ""], ["Townend", "Matthew", ""], ["Montvila", "Antanas", ""], ["Siddiqui", "Juveria", ""], ["Gadapa", "Naveen", ""], ["Benger", "Matthew", ""], ["Barker", "Gareth", ""], ["Ourselin", "Sebastian", ""], ["Cole", "James H.", ""], ["Booth", "Thomas C.", ""]]}, {"id": "2007.04234", "submitter": "Xingyi Yang", "authors": "Xingyi Yang, Xuehai He, Yuxiao Liang, Yue Yang, Shanghang Zhang,\n  Pengtao Xie", "title": "Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining\n  Paradigms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining has become a standard technique in computer vision and natural\nlanguage processing, which usually helps to improve performance substantially.\nPreviously, the most dominant pretraining method is transfer learning (TL),\nwhich uses labeled data to learn a good representation network. Recently, a new\npretraining approach -- self-supervised learning (SSL) -- has demonstrated\npromising results on a wide range of applications. SSL does not require\nannotated labels. It is purely conducted on input data by solving auxiliary\ntasks defined on the input data examples. The current reported results show\nthat in certain applications, SSL outperforms TL and the other way around in\nother applications. There has not been a clear understanding on what properties\nof data and tasks render one approach outperforms the other. Without an\ninformed guideline, ML researchers have to try both methods to find out which\none is better empirically. It is usually time-consuming to do so. In this work,\nwe aim to address this problem. We perform a comprehensive comparative study\nbetween SSL and TL regarding which one works better under different properties\nof data and tasks, including domain difference between source and target tasks,\nthe amount of pretraining data, class imbalance in source data, and usage of\ntarget data for additional pretraining, etc. The insights distilled from our\ncomparative studies can help ML researchers decide which method to use based on\nthe properties of their applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 05:21:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Yang", "Xingyi", ""], ["He", "Xuehai", ""], ["Liang", "Yuxiao", ""], ["Yang", "Yue", ""], ["Zhang", "Shanghang", ""], ["Xie", "Pengtao", ""]]}, {"id": "2007.04238", "submitter": "Myriam Bontonou", "authors": "Myriam Bontonou, Louis B\\'ethune, Vincent Gripon", "title": "Predicting the Accuracy of a Few-Shot Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of few-shot learning, one cannot measure the generalization\nability of a trained classifier using validation sets, due to the small number\nof labeled samples. In this paper, we are interested in finding alternatives to\nanswer the question: is my classifier generalizing well to previously unseen\ndata? We first analyze the reasons for the variability of generalization\nperformances. We then investigate the case of using transfer-based solutions,\nand consider three settings: i) supervised where we only have access to a few\nlabeled samples, ii) semi-supervised where we have access to both a few labeled\nsamples and a set of unlabeled samples and iii) unsupervised where we only have\naccess to unlabeled samples. For each setting, we propose reasonable measures\nthat we empirically demonstrate to be correlated with the generalization\nability of considered classifiers. We also show that these simple measures can\nbe used to predict generalization up to a certain confidence. We conduct our\nexperiments on standard few-shot vision datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:31:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bontonou", "Myriam", ""], ["B\u00e9thune", "Louis", ""], ["Gripon", "Vincent", ""]]}, {"id": "2007.04242", "submitter": "Zhuo Su", "authors": "Zhuo Su, Linpu Fang, Wenxiong Kang, Dewen Hu, Matti Pietik\\\"ainen, Li\n  Liu", "title": "Dynamic Group Convolution for Accelerating Convolutional Neural Networks", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replacing normal convolutions with group convolutions can significantly\nincrease the computational efficiency of modern deep convolutional networks,\nwhich has been widely adopted in compact network architecture designs. However,\nexisting group convolutions undermine the original network structures by\ncutting off some connections permanently resulting in significant accuracy\ndegradation. In this paper, we propose dynamic group convolution (DGC) that\nadaptively selects which part of input channels to be connected within each\ngroup for individual samples on the fly. Specifically, we equip each group with\na small feature selector to automatically select the most important input\nchannels conditioned on the input images. Multiple groups can adaptively\ncapture abundant and complementary visual/semantic features for each input\nimage. The DGC preserves the original network structure and has similar\ncomputational efficiency as the conventional group convolution simultaneously.\nExtensive experiments on multiple image classification benchmarks including\nCIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority over the existing\ngroup convolution techniques and dynamic execution methods. The code is\navailable at https://github.com/zhuogege1943/dgc.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:35:44 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 17:24:31 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Su", "Zhuo", ""], ["Fang", "Linpu", ""], ["Kang", "Wenxiong", ""], ["Hu", "Dewen", ""], ["Pietik\u00e4inen", "Matti", ""], ["Liu", "Li", ""]]}, {"id": "2007.04245", "submitter": "Ka Chun Lam", "authors": "Ka Chun Lam, Francisco Pereira, Maryam Vaziri-Pashkam, Kristin\n  Woodard, Emalie McMahon", "title": "Mental representations of objects reflect the ways in which we interact\n  with them", "comments": "17 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to interact with objects in our environment, humans rely on an\nunderstanding of the actions that can be performed on them, as well as their\nproperties. When considering concrete motor actions, this knowledge has been\ncalled the object affordance. Can this notion be generalized to any type of\ninteraction that one can have with an object? In this paper we introduce a\nmethod to represent objects in a space where each dimension corresponds to a\nbroad mode of interaction, based on verb selectional preferences in text\ncorpora. This object embedding makes it possible to predict human judgments of\nverb applicability to objects better than a variety of alternative approaches.\nFurthermore, we show that the dimensions in this space can be used to predict\ncategorical and functional dimensions in a state-of-the-art mental\nrepresentation of objects, derived solely from human judgements of object\nsimilarity. These results suggest that interaction knowledge accounts for a\nlarge part of mental representations of objects.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 22:05:56 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 13:06:41 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lam", "Ka Chun", ""], ["Pereira", "Francisco", ""], ["Vaziri-Pashkam", "Maryam", ""], ["Woodard", "Kristin", ""], ["McMahon", "Emalie", ""]]}, {"id": "2007.04250", "submitter": "Tianshi Cao", "authors": "Tianshi Cao, Chin-Wei Huang, David Yu-Tung Hui, Joseph Paul Cohen", "title": "A Benchmark of Medical Out of Distribution Detection", "comments": "Submitted to Machine Learning for Biomedical Imaging Journal (MELBA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Deep learning models deployed for use on medical tasks can be\nequipped with Out-of-Distribution Detection (OoDD) methods in order to avoid\nerroneous predictions. However it is unclear which OoDD method should be used\nin practice. Specific Problem: Systems trained for one particular domain of\nimages cannot be expected to perform accurately on images of a different\ndomain. These images should be flagged by an OoDD method prior to diagnosis.\nOur approach: This paper defines 3 categories of OoD examples and benchmarks\npopular OoDD methods in three domains of medical imaging: chest X-ray, fundus\nimaging, and histology slides. Results: Our experiments show that despite\nmethods yielding good results on some categories of out-of-distribution\nsamples, they fail to recognize images close to the training distribution.\nConclusion: We find a simple binary classifier on the feature representation\nhas the best accuracy and AUPRC on average. Users of diagnostic tools which\nemploy these OoDD methods should still remain vigilant that images very close\nto the training distribution yet not in it could yield unexpected results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:39:34 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 02:05:43 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Cao", "Tianshi", ""], ["Huang", "Chin-Wei", ""], ["Hui", "David Yu-Tung", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "2007.04251", "submitter": "Zheyuan Xu", "authors": "Zheyuan Xu, Hongche Yin, Jian Yao", "title": "Deformable spatial propagation network for depth completion", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion has attracted extensive attention recently due to the\ndevelopment of autonomous driving, which aims to recover dense depth map from\nsparse depth measurements. Convolutional spatial propagation network (CSPN) is\none of the state-of-the-art methods in this task, which adopt a linear\npropagation model to refine coarse depth maps with local context. However, the\npropagation of each pixel occurs in a fixed receptive field. This may not be\nthe optimal for refinement since different pixel needs different local context.\nTo tackle this issue, in this paper, we propose a deformable spatial\npropagation network (DSPN) to adaptively generates different receptive field\nand affinity matrix for each pixel. It allows the network obtain information\nwith much fewer but more relevant pixels for propagation. Experimental results\non KITTI depth completion benchmark demonstrate that our proposed method\nachieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:39:50 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 09:52:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Xu", "Zheyuan", ""], ["Yin", "Hongche", ""], ["Yao", "Jian", ""]]}, {"id": "2007.04257", "submitter": "Felipe Belem", "authors": "F.C. Belem and S.J.F. Guimaraes and A.X. Falcao", "title": "Superpixel Segmentation using Dynamic and Iterative Spanning Forest", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2020.3015433", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As constituent parts of image objects, superpixels can improve several\nhigher-level operations. However, image segmentation methods might have their\naccuracy seriously compromised for reduced numbers of superpixels. We have\ninvestigated a solution based on the Iterative Spanning Forest (ISF) framework.\nIn this work, we present Dynamic ISF (DISF) -- a method based on the following\nsteps. (a) It starts from an image graph and a seed set with considerably more\npixels than the desired number of superpixels. (b) The seeds compete among\nthemselves, and each seed conquers its most closely connected pixels, resulting\nin an image partition (spanning forest) with connected superpixels. In step\n(c), DISF assigns relevance values to seeds based on superpixel analysis and\nremoves the most irrelevant ones. Steps (b) and (c) are repeated until the\ndesired number of superpixels is reached. DISF has the chance to reconstruct\nrelevant edges after each iteration, when compared to region merging\nalgorithms. As compared to other seed-based superpixel methods, DISF is more\nlikely to find relevant seeds. It also introduces dynamic arc-weight estimation\nin the ISF framework for more effective superpixel delineation, and we\ndemonstrate all results on three datasets with distinct object properties.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:46:58 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Belem", "F. C.", ""], ["Guimaraes", "S. J. F.", ""], ["Falcao", "A. X.", ""]]}, {"id": "2007.04258", "submitter": "Florin-Cristian Ghesu", "authors": "Florin C. Ghesu, Bogdan Georgescu, Awais Mansoor, Youngjin Yoo, Eli\n  Gibson, R.S. Vishwanath, Abishek Balachandran, James M. Balter, Yue Cao,\n  Ramandeep Singh, Subba R. Digumarthy, Mannudeep K. Kalra, Sasa Grbic, Dorin\n  Comaniciu", "title": "Quantifying and Leveraging Predictive Uncertainty for Medical Image\n  Assessment", "comments": "Under review at Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of medical images is a challenging task, often complicated\nby the presence of artifacts, occlusions, limited contrast and more. Most\nnotable is the case of chest radiography, where there is a high inter-rater\nvariability in the detection and classification of abnormalities. This is\nlargely due to inconclusive evidence in the data or subjective definitions of\ndisease appearance. An additional example is the classification of anatomical\nviews based on 2D Ultrasound images. Often, the anatomical context captured in\na frame is not sufficient to recognize the underlying anatomy. Current machine\nlearning solutions for these problems are typically limited to providing\nprobabilistic predictions, relying on the capacity of underlying models to\nadapt to limited information and the high degree of label noise. In practice,\nhowever, this leads to overconfident systems with poor generalization on unseen\ndata. To account for this, we propose a system that learns not only the\nprobabilistic estimate for classification, but also an explicit uncertainty\nmeasure which captures the confidence of the system in the predicted output. We\nargue that this approach is essential to account for the inherent ambiguity\ncharacteristic of medical images from different radiologic exams including\ncomputed radiography, ultrasonography and magnetic resonance imaging. In our\nexperiments we demonstrate that sample rejection based on the predicted\nuncertainty can significantly improve the ROC-AUC for various tasks, e.g., by\n8% to 0.91 with an expected rejection rate of under 25% for the classification\nof different abnormalities in chest radiographs. In addition, we show that\nusing uncertainty-driven bootstrapping to filter the training data, one can\nachieve a significant increase in robustness and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:47:55 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ghesu", "Florin C.", ""], ["Georgescu", "Bogdan", ""], ["Mansoor", "Awais", ""], ["Yoo", "Youngjin", ""], ["Gibson", "Eli", ""], ["Vishwanath", "R. S.", ""], ["Balachandran", "Abishek", ""], ["Balter", "James M.", ""], ["Cao", "Yue", ""], ["Singh", "Ramandeep", ""], ["Digumarthy", "Subba R.", ""], ["Kalra", "Mannudeep K.", ""], ["Grbic", "Sasa", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2007.04259", "submitter": "Tao Wang", "authors": "Tao Wang and Yuanzheng Cai and Lingyu Liang and Dongyi Ye", "title": "A Multi-Level Approach to Waste Object Segmentation", "comments": "Paper appears in Sensors 2020, 20(14), 3816", "journal-ref": null, "doi": "10.3390/s20143816", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of localizing waste objects from a color image and an\noptional depth image, which is a key perception component for robotic\ninteraction with such objects. Specifically, our method integrates the\nintensity and depth information at multiple levels of spatial granularity.\nFirstly, a scene-level deep network produces an initial coarse segmentation,\nbased on which we select a few potential object regions to zoom in and perform\nfine segmentation. The results of the above steps are further integrated into a\ndensely connected conditional random field that learns to respect the\nappearance, depth, and spatial affinities with pixel-level accuracy. In\naddition, we create a new RGBD waste object segmentation dataset, MJU-Waste,\nthat is made public to facilitate future research in this area. The efficacy of\nour method is validated on both MJU-Waste and the Trash Annotation in Context\n(TACO) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:49:25 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wang", "Tao", ""], ["Cai", "Yuanzheng", ""], ["Liang", "Lingyu", ""], ["Ye", "Dongyi", ""]]}, {"id": "2007.04269", "submitter": "Yuhui Yuan", "authors": "Yuhui Yuan, Jingyi Xie, Xilin Chen, Jingdong Wang", "title": "SegFix: Model-Agnostic Boundary Refinement for Segmentation", "comments": "ECCV 2020. Project Page:\n  https://github.com/openseg-group/openseg.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model-agnostic post-processing scheme to improve the boundary\nquality for the segmentation result that is generated by any existing\nsegmentation model. Motivated by the empirical observation that the label\npredictions of interior pixels are more reliable, we propose to replace the\noriginally unreliable predictions of boundary pixels by the predictions of\ninterior pixels. Our approach processes only the input image through two steps:\n(i) localize the boundary pixels and (ii) identify the corresponding interior\npixel for each boundary pixel. We build the correspondence by learning a\ndirection away from the boundary pixel to an interior pixel. Our method\nrequires no prior information of the segmentation models and achieves nearly\nreal-time speed. We empirically verify that our SegFix consistently reduces the\nboundary errors for segmentation results generated from various\nstate-of-the-art models on Cityscapes, ADE20K and GTA5. Code is available at:\nhttps://github.com/openseg-group/openseg.pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:08:08 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 16:02:08 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 08:14:04 GMT"}, {"version": "v4", "created": "Thu, 27 Aug 2020 09:45:58 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yuan", "Yuhui", ""], ["Xie", "Jingyi", ""], ["Chen", "Xilin", ""], ["Wang", "Jingdong", ""]]}, {"id": "2007.04295", "submitter": "Denys Malyshev", "authors": "Mariia Drozdova, Anton Broilovskiy, Andrey Ustyuzhanin, Denys Malyshev", "title": "A study of Neural networks point source extraction on simulated\n  Fermi/LAT Telescope images", "comments": "Accepted to Astronomische Nachrichten", "journal-ref": null, "doi": "10.1002/asna.202013788", "report-no": null, "categories": "cs.CV astro-ph.HE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrophysical images in the GeV band are challenging to analyze due to the\nstrong contribution of the background and foreground astrophysical diffuse\nemission and relatively broad point spread function of modern space-based\ninstruments. In certain cases, even finding of point sources on the image\nbecomes a non-trivial task. We present a method for point sources extraction\nusing a convolution neural network (CNN) trained on our own artificial data set\nwhich imitates images from the Fermi Large Area Telescope. These images are raw\ncount photon maps of 10x10 degrees covering energies from 1 to 10 GeV. We\ncompare different CNN architectures that demonstrate accuracy increase by ~15%\nand reduces the inference time by at least the factor of 4 accuracy improvement\nwith respect to a similar state of the art models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:47:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Drozdova", "Mariia", ""], ["Broilovskiy", "Anton", ""], ["Ustyuzhanin", "Andrey", ""], ["Malyshev", "Denys", ""]]}, {"id": "2007.04309", "submitter": "Nicklas Hansen", "authors": "Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\\`a, Pieter\n  Abbeel, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang", "title": "Self-Supervised Policy Adaptation during Deployment", "comments": "Website: https://nicklashansen.github.io/PAD/ Code:\n  https://github.com/nicklashansen/policy-adaptation-during-deployment ICLR\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real world scenarios, a policy trained by reinforcement learning in\none environment needs to be deployed in another, potentially quite different\nenvironment. However, generalization across different environments is known to\nbe hard. A natural solution would be to keep training after deployment in the\nnew environment, but this cannot be done if the new environment offers no\nreward signal. Our work explores the use of self-supervision to allow the\npolicy to continue training after deployment without using any rewards. While\nprevious methods explicitly anticipate changes in the new environment, we\nassume no prior knowledge of those changes yet still obtain significant\nimprovements. Empirical evaluations are performed on diverse simulation\nenvironments from DeepMind Control suite and ViZDoom, as well as real robotic\nmanipulation tasks in continuously changing environments, taking observations\nfrom an uncalibrated camera. Our method improves generalization in 31 out of 36\nenvironments across various tasks and outperforms domain randomization on a\nmajority of environments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:56:27 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:01:06 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 02:47:39 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hansen", "Nicklas", ""], ["Jangir", "Rishabh", ""], ["Sun", "Yu", ""], ["Aleny\u00e0", "Guillem", ""], ["Abbeel", "Pieter", ""], ["Efros", "Alexei A.", ""], ["Pinto", "Lerrel", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2007.04316", "submitter": "Hugo Proen\\c{c}a", "authors": "Hugo Proen\\c{c}a", "title": "The UU-Net: Reversible Face De-Identification for Visual Surveillance\n  Video Footage", "comments": "12 pages, 4 tables, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reversible face de-identification method for low resolution\nvideo data, where landmark-based techniques cannot be reliably used. Our\nsolution is able to generate a photo realistic de-identified stream that meets\nthe data protection regulations and can be publicly released under minimal\nprivacy constraints. Notably, such stream encapsulates all the information\nrequired to later reconstruct the original scene, which is useful for\nscenarios, such as crime investigation, where the identification of the\nsubjects is of most importance. We describe a learning process that jointly\noptimizes two main components: 1) a public module, that receives the raw data\nand generates the de-identified stream, where the ID information is surrogated\nin a photo-realistic and seamless way; and 2) a private module, designed for\nlegal/security authorities, that analyses the public stream and reconstructs\nthe original scene, disclosing the actual IDs of all the subjects in the scene.\nThe proposed solution is landmarks-free and uses a conditional generative\nadversarial network to generate synthetic faces that preserve pose, lighting,\nbackground information and even facial expressions. Also, we enable full\ncontrol over the set of soft facial attributes that should be preserved between\nthe raw and de-identified data, which broads the range of applications for this\nsolution. Our experiments were conducted in three different visual surveillance\ndatasets (BIODI, MARS and P-DESTRE) and showed highly encouraging results. The\nsource code is available at https://github.com/hugomcp/uu-net.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:34:25 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Proen\u00e7a", "Hugo", ""]]}, {"id": "2007.04344", "submitter": "Chunwei Tian", "authors": "Chunwei Tian, Ruibin Zhuge, Zhihao Wu, Yong Xu, Wangmeng Zuo, Chen\n  Chen, Chia-Wen Lin", "title": "Lightweight image super-resolution with enhanced CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) with strong expressive ability have\nachieved impressive performances on single image super-resolution (SISR).\nHowever, their excessive amounts of convolutions and parameters usually consume\nhigh computational cost and more memory storage for training a SR model, which\nlimits their applications to SR with resource-constrained devices in real\nworld. To resolve these problems, we propose a lightweight enhanced SR CNN\n(LESRCNN) with three successive sub-blocks, an information extraction and\nenhancement block (IEEB), a reconstruction block (RB) and an information\nrefinement block (IRB). Specifically, the IEEB extracts hierarchical\nlow-resolution (LR) features and aggregates the obtained features step-by-step\nto increase the memory ability of the shallow layers on deep layers for SISR.\nTo remove redundant information obtained, a heterogeneous architecture is\nadopted in the IEEB. After that, the RB converts low-frequency features into\nhigh-frequency features by fusing global and local features, which is\ncomplementary with the IEEB in tackling the long-term dependency problem.\nFinally, the IRB uses coarse high-frequency features from the RB to learn more\naccurate SR features and construct a SR image. The proposed LESRCNN can obtain\na high-quality image by a model for different scales. Extensive experiments\ndemonstrate that the proposed LESRCNN outperforms state-of-the-arts on SISR in\nterms of qualitative and quantitative evaluation. The code of LESRCNN is\naccessible on https://github.com/hellloxiaotian/LESRCNN.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:03:40 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 03:03:38 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 12:46:15 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Tian", "Chunwei", ""], ["Zhuge", "Ruibin", ""], ["Wu", "Zhihao", ""], ["Xu", "Yong", ""], ["Zuo", "Wangmeng", ""], ["Chen", "Chen", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2007.04349", "submitter": "Sophia Bano", "authors": "Sophia Bano, Francisco Vasconcelos, Luke M. Shepherd, Emmanuel Vander\n  Poorten, Tom Vercauteren, Sebastien Ourselin, Anna L. David, Jan Deprest and\n  Danail Stoyanov", "title": "Deep Placental Vessel Segmentation for Fetoscopic Mosaicking", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59716-0_73", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During fetoscopic laser photocoagulation, a treatment for twin-to-twin\ntransfusion syndrome (TTTS), the clinician first identifies abnormal placental\nvascular connections and laser ablates them to regulate blood flow in both\nfetuses. The procedure is challenging due to the mobility of the environment,\npoor visibility in amniotic fluid, occasional bleeding, and limitations in the\nfetoscopic field-of-view and image quality. Ideally, anastomotic placental\nvessels would be automatically identified, segmented and registered to create\nexpanded vessel maps to guide laser ablation, however, such methods have yet to\nbe clinically adopted. We propose a solution utilising the U-Net architecture\nfor performing placental vessel segmentation in fetoscopic videos. The obtained\nvessel probability maps provide sufficient cues for mosaicking alignment by\nregistering consecutive vessel maps using the direct intensity-based technique.\nExperiments on 6 different in vivo fetoscopic videos demonstrate that the\nvessel intensity-based registration outperformed image intensity-based\nregistration approaches showing better robustness in qualitative and\nquantitative comparison. We additionally reduce drift accumulation to\nnegligible even for sequences with up to 400 frames and we incorporate a scheme\nfor quantifying drift error in the absence of the ground-truth. Our paper\nprovides a benchmark for fetoscopy placental vessel segmentation and\nregistration by contributing the first in vivo vessel segmentation and\nfetoscopic videos dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:09:40 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Bano", "Sophia", ""], ["Vasconcelos", "Francisco", ""], ["Shepherd", "Luke M.", ""], ["Poorten", "Emmanuel Vander", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "Sebastien", ""], ["David", "Anna L.", ""], ["Deprest", "Jan", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2007.04350", "submitter": "Ziran Wang", "authors": "Yongkang Liu, Ziran Wang, Kyungtae Han, Zhenyu Shou, Prashant Tiwari,\n  and John H. L. Hansen", "title": "Sensor Fusion of Camera and Cloud Digital Twin Information for\n  Intelligent Vehicles", "comments": "Accepted by the 31st IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of intelligent vehicles and Advanced Driving\nAssistance Systems (ADAS), a mixed level of human driver engagements is\ninvolved in the transportation system. Visual guidance for drivers is essential\nunder this situation to prevent potential risks. To advance the development of\nvisual guidance systems, we introduce a novel sensor fusion methodology,\nintegrating camera image and Digital Twin knowledge from the cloud. Target\nvehicle bounding box is drawn and matched by combining results of object\ndetector running on ego vehicle and position information from the cloud. The\nbest matching result, with a 79.2% accuracy under 0.7 Intersection over Union\n(IoU) threshold, is obtained with depth image served as an additional feature\nsource. Game engine-based simulation results also reveal that the visual\nguidance system could improve driving safety significantly cooperate with the\ncloud Digital Twin system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:09:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Liu", "Yongkang", ""], ["Wang", "Ziran", ""], ["Han", "Kyungtae", ""], ["Shou", "Zhenyu", ""], ["Tiwari", "Prashant", ""], ["Hansen", "John H. L.", ""]]}, {"id": "2007.04356", "submitter": "Royson Lee", "authors": "Royson Lee, {\\L}ukasz Dudziak, Mohamed Abdelfattah, Stylianos I.\n  Venieris, Hyeji Kim, Hongkai Wen, Nicholas D. Lane", "title": "Journey Towards Tiny Perceptual Super-Resolution", "comments": "Accepted at the 16th European Conference on Computer Vision (ECCV),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in single-image perceptual super-resolution (SR) have\ndemonstrated unprecedented performance in generating realistic textures by\nmeans of deep convolutional networks. However, these convolutional models are\nexcessively large and expensive, hindering their effective deployment to end\ndevices. In this work, we propose a neural architecture search (NAS) approach\nthat integrates NAS and generative adversarial networks (GANs) with recent\nadvances in perceptual SR and pushes the efficiency of small perceptual SR\nmodels to facilitate on-device execution. Specifically, we search over the\narchitectures of both the generator and the discriminator sequentially,\nhighlighting the unique challenges and key observations of searching for an\nSR-optimized discriminator and comparing them with existing discriminator\narchitectures in the literature. Our tiny perceptual SR (TPSR) models\noutperform SRGAN and EnhanceNet on both full-reference perceptual metric\n(LPIPS) and distortion metric (PSNR) while being up to 26.4$\\times$ more memory\nefficient and 33.6$\\times$ more compute efficient respectively.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:24:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lee", "Royson", ""], ["Dudziak", "\u0141ukasz", ""], ["Abdelfattah", "Mohamed", ""], ["Venieris", "Stylianos I.", ""], ["Kim", "Hyeji", ""], ["Wen", "Hongkai", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2007.04364", "submitter": "Anamaria Radoi", "authors": "Andreea Birhala, Catalin Nicolae Ristea, Anamaria Radoi, Liviu\n  Cristian Dutu", "title": "Temporal aggregation of audio-visual modalities for emotion recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion recognition has a pivotal role in affective computing and in\nhuman-computer interaction. The current technological developments lead to\nincreased possibilities of collecting data about the emotional state of a\nperson. In general, human perception regarding the emotion transmitted by a\nsubject is based on vocal and visual information collected in the first seconds\nof interaction with the subject. As a consequence, the integration of verbal\n(i.e., speech) and non-verbal (i.e., image) information seems to be the\npreferred choice in most of the current approaches towards emotion recognition.\nIn this paper, we propose a multimodal fusion technique for emotion recognition\nbased on combining audio-visual modalities from a temporal window with\ndifferent temporal offsets for each modality. We show that our proposed method\noutperforms other methods from the literature and human accuracy rating. The\nexperiments are conducted over the open-access multimodal dataset CREMA-D.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:44:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Birhala", "Andreea", ""], ["Ristea", "Catalin Nicolae", ""], ["Radoi", "Anamaria", ""], ["Dutu", "Liviu Cristian", ""]]}, {"id": "2007.04383", "submitter": "Azmi Can \\\"Ozgen", "authors": "Azmi Can \\\"Ozgen, Haz{\\i}m Kemal Ekenel", "title": "Words as Art Materials: Generating Paintings with Sequential GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Converting text descriptions into images using Generative Adversarial\nNetworks has become a popular research area. Visually appealing images have\nbeen generated successfully in recent years. Inspired by these studies, we\ninvestigated the generation of artistic images on a large variance dataset.\nThis dataset includes images with variations, for example, in shape, color, and\ncontent. These variations in images provide originality which is an important\nfactor for artistic essence. One major characteristic of our work is that we\nused keywords as image descriptions, instead of sentences. As the network\narchitecture, we proposed a sequential Generative Adversarial Network model.\nThe first stage of this sequential model processes the word vectors and creates\na base image whereas the next stages focus on creating high-resolution\nartistic-style images without working on word vectors. To deal with the\nunstable nature of GANs, we proposed a mixture of techniques like Wasserstein\nloss, spectral normalization, and minibatch discrimination. Ultimately, we were\nable to generate painting images, which have a variety of styles. We evaluated\nour results by using the Fr\\'echet Inception Distance score and conducted a\nuser study with 186 participants.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:17:14 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["\u00d6zgen", "Azmi Can", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "2007.04389", "submitter": "Bar{\\i}\\c{s} \\\"Ozcan", "authors": "Bar{\\i}\\c{s} \\\"Ozcan, Furkan K{\\i}nl{\\i}, Furkan K{\\i}ra\\c{c}", "title": "Quaternion Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsules are grouping of neurons that allow to represent sophisticated\ninformation of a visual entity such as pose and features. In the view of this\nproperty, Capsule Networks outperform CNNs in challenging tasks like object\nrecognition in unseen viewpoints, and this is achieved by learning the\ntransformations between the object and its parts with the help of high\ndimensional representation of pose information. In this paper, we present\nQuaternion Capsules (QCN) where pose information of capsules and their\ntransformations are represented by quaternions. Quaternions are immune to the\ngimbal lock, have straightforward regularization of the rotation representation\nfor capsules, and require less number of parameters than matrices. The\nexperimental results show that QCNs generalize better to novel viewpoints with\nfewer parameters, and also achieve on-par or better performances with the\nstate-of-the-art Capsule architectures on well-known benchmarking datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:33:18 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["\u00d6zcan", "Bar\u0131\u015f", ""], ["K\u0131nl\u0131", "Furkan", ""], ["K\u0131ra\u00e7", "Furkan", ""]]}, {"id": "2007.04422", "submitter": "Vatsal Goel", "authors": "Vatsal Goel, Mohit Chandak, Ashish Anand and Prithwijit Guha", "title": "IQ-VQA: Intelligent Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though there has been tremendous progress in the field of Visual\nQuestion Answering, models today still tend to be inconsistent and brittle. To\nthis end, we propose a model-independent cyclic framework which increases\nconsistency and robustness of any VQA architecture. We train our models to\nanswer the original question, generate an implication based on the answer and\nthen also learn to answer the generated implication correctly. As a part of the\ncyclic framework, we propose a novel implication generator which can generate\nimplied questions from any question-answer pair. As a baseline for future works\non consistency, we provide a new human annotated VQA-Implications dataset. The\ndataset consists of ~30k questions containing implications of 3 types - Logical\nEquivalence, Necessary Condition and Mutual Exclusion - made from the VQA v2.0\nvalidation dataset. We show that our framework improves consistency of VQA\nmodels by ~15% on the rule-based dataset, ~7% on VQA-Implications dataset and\nrobustness by ~2%, without degrading their performance. In addition, we also\nquantitatively show improvement in attention maps which highlights better\nmulti-modal understanding of vision and language.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 20:41:52 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Goel", "Vatsal", ""], ["Chandak", "Mohit", ""], ["Anand", "Ashish", ""], ["Guha", "Prithwijit", ""]]}, {"id": "2007.04449", "submitter": "Daniil Pakhomov", "authors": "Daniil Pakhomov, Nassir Navab", "title": "Searching for Efficient Architecture for Instrument Segmentation in\n  Robotic Surgery", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of surgical instruments is an important problem in\nrobot-assisted surgery: it is a crucial step towards full instrument pose\nestimation and is directly used for masking of augmented reality overlays\nduring surgical procedures. Most applications rely on accurate real-time\nsegmentation of high-resolution surgical images. While previous research\nfocused primarily on methods that deliver high accuracy segmentation masks,\nmajority of them can not be used for real-time applications due to their\ncomputational cost. In this work, we design a light-weight and highly-efficient\ndeep residual architecture which is tuned to perform real-time inference of\nhigh-resolution images. To account for reduced accuracy of the discovered\nlight-weight deep residual network and avoid adding any additional\ncomputational burden, we perform a differentiable search over dilation rates\nfor residual units of our network. We test our discovered architecture on the\nEndoVis 2017 Robotic Instruments dataset and verify that our model is the\nstate-of-the-art in terms of speed and accuracy tradeoff with a speed of up to\n125 FPS on high resolution images.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:38:29 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Pakhomov", "Daniil", ""], ["Navab", "Nassir", ""]]}, {"id": "2007.04480", "submitter": "Richard Droste", "authors": "Richard Droste, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble", "title": "Automatic Probe Movement Guidance for Freehand Obstetric Ultrasound", "comments": "Accepted at the 23rd International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first system that provides real-time probe movement guidance\nfor acquiring standard planes in routine freehand obstetric ultrasound\nscanning. Such a system can contribute to the worldwide deployment of obstetric\nultrasound scanning by lowering the required level of operator expertise. The\nsystem employs an artificial neural network that receives the ultrasound video\nsignal and the motion signal of an inertial measurement unit (IMU) that is\nattached to the probe, and predicts a guidance signal. The network termed\nUS-GuideNet predicts either the movement towards the standard plane position\n(goal prediction), or the next movement that an expert sonographer would\nperform (action prediction). While existing models for other ultrasound\napplications are trained with simulations or phantoms, we train our model with\nreal-world ultrasound video and probe motion data from 464 routine clinical\nscans by 17 accredited sonographers. Evaluations for 3 standard plane types\nshow that the model provides a useful guidance signal with an accuracy of 88.8%\nfor goal prediction and 90.9% for action prediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 23:58:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Droste", "Richard", ""], ["Drukker", "Lior", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "2007.04505", "submitter": "Daniil Pakhomov", "authors": "Daniil Pakhomov, Wei Shen, Nassir Navab", "title": "Towards Unsupervised Learning for Instrument Segmentation in Robotic\n  Surgery with Cycle-Consistent Adversarial Networks", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical tool segmentation in endoscopic images is an important problem: it\nis a crucial step towards full instrument pose estimation and it is used for\nintegration of pre- and intra-operative images into the endoscopic view. While\nmany recent approaches based on convolutional neural networks have shown great\nresults, a key barrier to progress lies in the acquisition of a large number of\nmanually-annotated images which is necessary for an algorithm to generalize and\nwork well in diverse surgical scenarios. Unlike the surgical image data itself,\nannotations are difficult to acquire and may be of variable quality. On the\nother hand, synthetic annotations can be automatically generated by using\nforward kinematic model of the robot and CAD models of tools by projecting them\nonto an image plane. Unfortunately, this model is very inaccurate and cannot be\nused for supervised learning of image segmentation models. Since generated\nannotations will not directly correspond to endoscopic images due to errors, we\nformulate the problem as an unpaired image-to-image translation where the goal\nis to learn the mapping between an input endoscopic image and a corresponding\nannotation using an adversarial model. Our approach allows to train image\nsegmentation models without the need to acquire expensive annotations and can\npotentially exploit large unlabeled endoscopic image collection outside the\nannotated distributions of image/annotation data. We test our proposed method\non Endovis 2017 challenge dataset and show that it is competitive with\nsupervised segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 01:39:39 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Pakhomov", "Daniil", ""], ["Shen", "Wei", ""], ["Navab", "Nassir", ""]]}, {"id": "2007.04514", "submitter": "Rui Zhao", "authors": "Rui Zhao, Tianshan Liu, Jun Xiao, Daniel P.K. Lun, Kin-Man Lam", "title": "Deep Multi-task Learning for Facial Expression Recognition and Synthesis\n  Based on Selective Feature Sharing", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is an effective learning strategy for deep-learning-based\nfacial expression recognition tasks. However, most existing methods take into\nlimited consideration the feature selection, when transferring information\nbetween different tasks, which may lead to task interference when training the\nmulti-task networks. To address this problem, we propose a novel selective\nfeature-sharing method, and establish a multi-task network for facial\nexpression recognition and facial expression synthesis. The proposed method can\neffectively transfer beneficial features between different tasks, while\nfiltering out useless and harmful information. Moreover, we employ the facial\nexpression synthesis task to enlarge and balance the training dataset to\nfurther enhance the generalization ability of the proposed method. Experimental\nresults show that the proposed method achieves state-of-the-art performance on\nthose commonly used facial expression recognition benchmarks, which makes it a\npotential solution to real-world facial expression recognition problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:29:34 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Zhao", "Rui", ""], ["Liu", "Tianshan", ""], ["Xiao", "Jun", ""], ["Lun", "Daniel P. K.", ""], ["Lam", "Kin-Man", ""]]}, {"id": "2007.04515", "submitter": "Senthil Purushwalkam", "authors": "Senthil Purushwalkam, Tian Ye, Saurabh Gupta, Abhinav Gupta", "title": "Aligning Videos in Space and Time", "comments": "To appear at the European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the task of extracting visual correspondences\nacross videos. Given a query video clip from an action class, we aim to align\nit with training videos in space and time. Obtaining training data for such a\nfine-grained alignment task is challenging and often ambiguous. Hence, we\npropose a novel alignment procedure that learns such correspondence in space\nand time via cross video cycle-consistency. During training, given a pair of\nvideos, we compute cycles that connect patches in a given frame in the first\nvideo by matching through frames in the second video. Cycles that connect\noverlapping patches together are encouraged to score higher than cycles that\nconnect non-overlapping patches. Our experiments on the Penn Action and Pouring\ndatasets demonstrate that the proposed method can successfully learn to\ncorrespond semantically similar patches across videos, and learns\nrepresentations that are sensitive to object and action states.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:30:48 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Purushwalkam", "Senthil", ""], ["Ye", "Tian", ""], ["Gupta", "Saurabh", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2007.04525", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Kaveh Hassani, Pradeep Kumar Jayaraman, Amir\n  Hosein Khasahmadi, Tonya Custis", "title": "PointMask: Towards Interpretable and Bias-Resilient Point Cloud\n  Processing", "comments": "Accepted to ICML 2020 WHI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep classifiers tend to associate a few discriminative input variables with\ntheir objective function, which in turn, may hurt their generalization\ncapabilities. To address this, one can design systematic experiments and/or\ninspect the models via interpretability methods. In this paper, we investigate\nboth of these strategies on deep models operating on point clouds. We propose\nPointMask, a model-agnostic interpretable information-bottleneck approach for\nattribution in point cloud models. PointMask encourages exploring the majority\nof variation factors in the input space while gradually converging to a general\nsolution. More specifically, PointMask introduces a regularization term that\nminimizes the mutual information between the input and the latent features used\nto masks out irrelevant variables. We show that coupling a PointMask layer with\nan arbitrary model can discern the points in the input space which contribute\nthe most to the prediction score, thereby leading to interpretability. Through\ndesigned bias experiments, we also show that thanks to its gradual masking\nfeature, our proposed method is effective in handling data bias.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:06:06 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Hassani", "Kaveh", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Khasahmadi", "Amir Hosein", ""], ["Custis", "Tonya", ""]]}, {"id": "2007.04536", "submitter": "Xiaosheng Hu", "authors": "Jianrong Wang, Xiaosheng Hu, Li Liu, Wei Liu, Mei Yu, Tianyi Xu", "title": "Attention-based Residual Speech Portrait Model for Speech to Face\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a speaker's speech, it is interesting to see if it is possible to\ngenerate this speaker's face. One main challenge in this task is to alleviate\nthe natural mismatch between face and speech. To this end, in this paper, we\npropose a novel Attention-based Residual Speech Portrait Model (AR-SPM) by\nintroducing the ideal of the residual into a hybrid encoder-decoder\narchitecture, where face prior features are merged with the output of speech\nencoder to form the final face feature. In particular, we innovatively\nestablish a tri-item loss function, which is a weighted linear combination of\nthe L2-norm, L1-norm and negative cosine loss, to train our model by comparing\nthe final face feature and true face feature. Evaluation on AVSpeech dataset\nshows that our proposed model accelerates the convergence of training,\noutperforms the state-of-the-art in terms of quality of the generated face, and\nachieves superior recognition accuracy of gender and age compared with the\nground truth.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:31:33 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Wang", "Jianrong", ""], ["Hu", "Xiaosheng", ""], ["Liu", "Li", ""], ["Liu", "Wei", ""], ["Yu", "Mei", ""], ["Xu", "Tianyi", ""]]}, {"id": "2007.04537", "submitter": "Junming Zhang", "authors": "Junming Zhang, Weijia Chen, Yuping Wang, Ram Vasudevan, Matthew\n  Johnson-Roberson", "title": "Point Set Voting for Partial Point Cloud Analysis", "comments": "IEEE Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2020.3048658", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continual improvement of 3D sensors has driven the development of\nalgorithms to perform point cloud analysis. In fact, techniques for point cloud\nclassification and segmentation have in recent years achieved incredible\nperformance driven in part by leveraging large synthetic datasets.\nUnfortunately these same state-of-the-art approaches perform poorly when\napplied to incomplete point clouds. This limitation of existing algorithms is\nparticularly concerning since point clouds generated by 3D sensors in the real\nworld are usually incomplete due to perspective view or occlusion by other\nobjects. This paper proposes a general model for partial point clouds analysis\nwherein the latent feature encoding a complete point clouds is inferred by\napplying a local point set voting strategy. In particular, each local point set\nconstructs a vote that corresponds to a distribution in the latent space, and\nthe optimal latent feature is the one with the highest probability. This\napproach ensures that any subsequent point cloud analysis is robust to partial\nobservation while simultaneously guaranteeing that the proposed model is able\nto output multiple possible results. This paper illustrates that this proposed\nmethod achieves state-of-the-art performance on shape classification, part\nsegmentation and point cloud completion.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:37:31 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 17:37:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Junming", ""], ["Chen", "Weijia", ""], ["Wang", "Yuping", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "2007.04538", "submitter": "Kunyuan Li", "authors": "Kunyuan Li, Jun Zhang, Rui Sun, Xudong Zhang, Jun Gao", "title": "EPI-based Oriented Relation Networks for Light Field Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras record not only the spatial information of observed\nscenes but also the directions of all incoming light rays. The spatial and\nangular information implicitly contain geometrical characteristics such as\nmulti-view or epipolar geometry, which can be exploited to improve the\nperformance of depth estimation. An Epipolar Plane Image (EPI), the unique 2D\nspatial-angular slice of the light field, contains patterns of oriented lines.\nThe slope of these lines is associated with the disparity. Benefiting from this\nproperty of EPIs, some representative methods estimate depth maps by analyzing\nthe disparity of each line in EPIs. However, these methods often extract the\noptimal slope of the lines from EPIs while ignoring the relationship between\nneighboring pixels, which leads to inaccurate depth map predictions. Based on\nthe observation that an oriented line and its neighboring pixels in an EPI\nshare a similar linear structure, we propose an end-to-end fully convolutional\nnetwork (FCN) to estimate the depth value of the intersection point on the\nhorizontal and vertical EPIs. Specifically, we present a new feature-extraction\nmodule, called Oriented Relation Module (ORM), that constructs the relationship\nbetween the line orientations. To facilitate training, we also propose a\nrefocusing-based data augmentation method to obtain different slopes from EPIs\nof the same scene point. Extensive experiments verify the efficacy of learning\nrelations and show that our approach is competitive to other state-of-the-art\nmethods. The code and the trained models are available at\nhttps://github.com/lkyahpu/EPI_ORM.git.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:39:09 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 07:10:04 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Li", "Kunyuan", ""], ["Zhang", "Jun", ""], ["Sun", "Rui", ""], ["Zhang", "Xudong", ""], ["Gao", "Jun", ""]]}, {"id": "2007.04543", "submitter": "SungKwon An", "authors": "Sungkwon An, Hyungmin Roh and Myungjoo Kang", "title": "Blur Invariant Kernel-Adaptive Network for Single Image Blind deblurring", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, blind, single image deblurring method that utilizes\ninformation regarding blur kernels. Our model solves the deblurring problem by\ndividing it into two successive tasks: (1) blur kernel estimation and (2) sharp\nimage restoration. We first introduce a kernel estimation network that produces\nadaptive blur kernels based on the analysis of the blurred image. The network\nlearns the blur pattern of the input image and trains to generate the\nestimation of image-specific blur kernels. Subsequently, we propose a\ndeblurring network that restores sharp images using the estimated blur kernel.\nTo use the kernel efficiently, we propose a kernel-adaptive AE block that\nencodes features from both blurred images and blur kernels into a low\ndimensional space and then decodes them simultaneously to obtain an\nappropriately synthesized feature representation. We evaluate our model on\nREDS, GOPRO and Flickr2K datasets using various Gaussian blur kernels.\nExperiments show that our model can achieve state-of-the-art results on each\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:53:33 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 04:47:11 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 07:28:07 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["An", "Sungkwon", ""], ["Roh", "Hyungmin", ""], ["Kang", "Myungjoo", ""]]}, {"id": "2007.04546", "submitter": "Mengye Ren", "authors": "Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard S. Zemel", "title": "Wandering Within a World: Online Contextualized Few-Shot Learning", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to bridge the gap between typical human and machine-learning\nenvironments by extending the standard framework of few-shot learning to an\nonline, continual setting. In this setting, episodes do not have separate\ntraining and testing phases, and instead models are evaluated online while\nlearning novel classes. As in the real world, where the presence of\nspatiotemporal context helps us retrieve learned skills in the past, our online\nfew-shot learning setting also features an underlying context that changes\nthroughout time. Object classes are correlated within a context and inferring\nthe correct context can lead to better performance. Building upon this setting,\nwe propose a new few-shot learning dataset based on large scale indoor imagery\nthat mimics the visual experience of an agent wandering within a world.\nFurthermore, we convert popular few-shot learning approaches into online\nversions and we also propose a new contextual prototypical memory model that\ncan make use of spatiotemporal contextual information from the recent past.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:05:04 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 02:34:57 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 20:15:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ren", "Mengye", ""], ["Iuzzolino", "Michael L.", ""], ["Mozer", "Michael C.", ""], ["Zemel", "Richard S.", ""]]}, {"id": "2007.04557", "submitter": "Aly Magassouba", "authors": "Tadashi Ogura, Aly Magassouba, Komei Sugiura, Tsubasa Hirakawa,\n  Takayoshi Yamashita, Hironobu Fujiyoshi, and Hisashi Kawai", "title": "Alleviating the Burden of Labeling: Sentence Generation by Attention\n  Branch Encoder-Decoder Network", "comments": "9 pages, 8 figures. accepted for IEEE Robotics and Automation Letters\n  (RA-L) with presentation at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domestic service robots (DSRs) are a promising solution to the shortage of\nhome care workers. However, one of the main limitations of DSRs is their\ninability to interact naturally through language. Recently, data-driven\napproaches have been shown to be effective for tackling this limitation;\nhowever, they often require large-scale datasets, which is costly. Based on\nthis background, we aim to perform automatic sentence generation of fetching\ninstructions: for example, \"Bring me a green tea bottle on the table.\" This is\nparticularly challenging because appropriate expressions depend on the target\nobject, as well as its surroundings. In this paper, we propose the attention\nbranch encoder--decoder network (ABEN), to generate sentences from visual\ninputs. Unlike other approaches, the ABEN has multimodal attention branches\nthat use subword-level attention and generate sentences based on subword\nembeddings. In experiments, we compared the ABEN with a baseline method using\nfour standard metrics in image captioning. Results show that the ABEN\noutperformed the baseline in terms of these metrics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 05:02:23 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ogura", "Tadashi", ""], ["Magassouba", "Aly", ""], ["Sugiura", "Komei", ""], ["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Fujiyoshi", "Hironobu", ""], ["Kawai", "Hisashi", ""]]}, {"id": "2007.04561", "submitter": "Joel Ye", "authors": "Joel Ye, Dhruv Batra, Erik Wijmans, Abhishek Das", "title": "Auxiliary Tasks Speed Up Learning PointGoal Navigation", "comments": "8 pages. Accepted to CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PointGoal Navigation is an embodied task that requires agents to navigate to\na specified point in an unseen environment. Wijmans et al. showed that this\ntask is solvable but their method is computationally prohibitive, requiring 2.5\nbillion frames and 180 GPU-days. In this work, we develop a method to\nsignificantly increase sample and time efficiency in learning PointNav using\nself-supervised auxiliary tasks (e.g. predicting the action taken between two\negocentric observations, predicting the distance between two observations from\na trajectory,etc.).We find that naively combining multiple auxiliary tasks\nimproves sample efficiency,but only provides marginal gains beyond a point. To\novercome this, we use attention to combine representations learnt from\nindividual auxiliary tasks. Our best agent is 5.5x faster to reach the\nperformance of the previous state-of-the-art, DD-PPO, at 40M frames, and\nimproves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is\npublicly available at https://github.com/joel99/habitat-pointnav-aux.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 05:22:40 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 20:29:06 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Ye", "Joel", ""], ["Batra", "Dhruv", ""], ["Wijmans", "Erik", ""], ["Das", "Abhishek", ""]]}, {"id": "2007.04564", "submitter": "Darpan Kumar Yadav", "authors": "Darpan Kumar Yadav, Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay\n  and Indra Narayan Kar", "title": "Efficient detection of adversarial images", "comments": "10 pages, 3 figures, 3 algorithms, 8 tables. Extension of the\n  Conference paper:- Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, and\n  Indra Narayan Kar. Adversarial image detection in cyber-physical systems. In\n  Proceedings of the 1st ACM Workshop on Autonomous and Intelligent Mobile\n  Systems, pages 1-5, 2020. Can be found at\n  https://dl.acm.org/doi/abs/10.1145/3377283.3377285", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, detection of deception attack on deep neural network (DNN)\nbased image classification in autonomous and cyber-physical systems is\nconsidered. Several studies have shown the vulnerability of DNN to malicious\ndeception attacks. In such attacks, some or all pixel values of an image are\nmodified by an external attacker, so that the change is almost invisible to the\nhuman eye but significant enough for a DNN-based classifier to misclassify it.\nThis paper first proposes a novel pre-processing technique that facilitates the\ndetection of such modified images under any DNN-based image classifier as well\nas the attacker model. The proposed pre-processing algorithm involves a certain\ncombination of principal component analysis (PCA)-based decomposition of the\nimage, and random perturbation based detection to reduce computational\ncomplexity. Next, an adaptive version of this algorithm is proposed where a\nrandom number of perturbations are chosen adaptively using a doubly-threshold\npolicy, and the threshold values are learnt via stochastic approximation in\norder to minimize the expected number of perturbations subject to constraints\non the false alarm and missed detection probabilities. Numerical experiments\nshow that the proposed detection scheme outperforms a competing algorithm while\nachieving reasonably low computational complexity.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 05:35:49 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yadav", "Darpan Kumar", ""], ["Mundra", "Kartik", ""], ["Modpur", "Rahul", ""], ["Chattopadhyay", "Arpan", ""], ["Kar", "Indra Narayan", ""]]}, {"id": "2007.04574", "submitter": "Haojie Liu", "authors": "Haojie Liu, Ming Lu, Zhan Ma, Fan Wang, Zhihuang Xie, Xun Cao, Yao\n  Wang", "title": "Neural Video Coding using Multiscale Motion Compensation and\n  Spatiotemporal Context Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, traditional block-based video coding has made\nremarkable progress and spawned a series of well-known standards such as\nMPEG-4, H.264/AVC and H.265/HEVC. On the other hand, deep neural networks\n(DNNs) have shown their powerful capacity for visual content understanding,\nfeature extraction and compact representation. Some previous works have\nexplored the learnt video coding algorithms in an end-to-end manner, which show\nthe great potential compared with traditional methods. In this paper, we\npropose an end-to-end deep neural video coding framework (NVC), which uses\nvariational autoencoders (VAEs) with joint spatial and temporal prior\naggregation (PA) to exploit the correlations in intra-frame pixels, inter-frame\nmotions and inter-frame compensation residuals, respectively. Novel features of\nNVC include: 1) To estimate and compensate motion over a large range of\nmagnitudes, we propose an unsupervised multiscale motion compensation network\n(MS-MCN) together with a pyramid decoder in the VAE for coding motion features\nthat generates multiscale flow fields, 2) we design a novel adaptive\nspatiotemporal context model for efficient entropy coding for motion\ninformation, 3) we adopt nonlocal attention modules (NLAM) at the bottlenecks\nof the VAEs for implicit adaptive feature extraction and activation, leveraging\nits high transformation capacity and unequal weighting with joint global and\nlocal information, and 4) we introduce multi-module optimization and a\nmulti-frame training strategy to minimize the temporal error propagation among\nP-frames. NVC is evaluated for the low-delay causal settings and compared with\nH.265/HEVC, H.264/AVC and the other learnt video compression methods following\nthe common test conditions, demonstrating consistent gains across all popular\ntest sequences for both PSNR and MS-SSIM distortion metrics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 06:15:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Liu", "Haojie", ""], ["Lu", "Ming", ""], ["Ma", "Zhan", ""], ["Wang", "Fan", ""], ["Xie", "Zhihuang", ""], ["Cao", "Xun", ""], ["Wang", "Yao", ""]]}, {"id": "2007.04584", "submitter": "Dazhen Deng", "authors": "Dazhen Deng, Yihong Wu, Xinhuan Shu, Jiang Wu, Mengye Xu, Siwei Fu,\n  Weiwei Cui, Yingcai Wu", "title": "VisImages: a Corpus of Visualizations in the Images of Visualization\n  Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images in visualization publications contain rich information, e.g., novel\nvisualization designs and common combinations of visualizations. A systematic\ncollection of these images can contribute to the community in many aspects,\nsuch as literature analysis and automated tasks for visualization. In this\npaper, we build and make public a dataset, VisImages, which collects 12,267\nimages with captions from 1,397 papers in IEEE InfoVis and VAST. Based on a\nrefined taxonomy for visualizations in publications, the dataset includes\n35,096 annotated visualizations, as well as their positions. We demonstrate the\nusefulness of VisImages through three use cases: 1) exploring and analyzing the\nevolution of visualizations with VisImages Explorer, 2) training and\nbenchmarking models for visualization classification, and 3) localizing and\nrecognizing visualizations in the images automatically.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 06:47:49 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 02:22:28 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 03:44:16 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 11:29:09 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Deng", "Dazhen", ""], ["Wu", "Yihong", ""], ["Shu", "Xinhuan", ""], ["Wu", "Jiang", ""], ["Xu", "Mengye", ""], ["Fu", "Siwei", ""], ["Cui", "Weiwei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2007.04589", "submitter": "Kwot Sin Lee", "authors": "Kwot Sin Lee, Ngoc-Trung Tran, Ngai-Man Cheung", "title": "InfoMax-GAN: Improved Adversarial Image Generation via Information\n  Maximization and Contrastive Learning", "comments": "Accepted to WACV 2021. An initial version was accepted to NeurIPS\n  2019 Workshop on Information Theory and Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) are fundamental to many\ngenerative modelling applications, they suffer from numerous issues. In this\nwork, we propose a principled framework to simultaneously mitigate two\nfundamental issues in GANs: catastrophic forgetting of the discriminator and\nmode collapse of the generator. We achieve this by employing for GANs a\ncontrastive learning and mutual information maximization approach, and perform\nextensive analyses to understand sources of improvements. Our approach\nsignificantly stabilizes GAN training and improves GAN performance for image\nsynthesis across five datasets under the same training and evaluation\nconditions against state-of-the-art works. In particular, compared to the\nstate-of-the-art SSGAN, our approach does not suffer from poorer performance on\nimage domains such as faces, and instead improves performance significantly.\nOur approach is simple to implement and practical: it involves only one\nauxiliary objective, has a low computational cost, and performs robustly across\na wide range of training settings and datasets without any hyperparameter\ntuning. For reproducibility, our code is available in Mimicry:\nhttps://github.com/kwotsin/mimicry.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 06:56:11 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 06:16:53 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 08:15:42 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 20:19:00 GMT"}, {"version": "v5", "created": "Tue, 10 Nov 2020 05:00:07 GMT"}, {"version": "v6", "created": "Sun, 22 Nov 2020 18:40:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lee", "Kwot Sin", ""], ["Tran", "Ngoc-Trung", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "2007.04592", "submitter": "Bahram Zonooz", "authors": "Hemang Chawla, Matti Jukola, Elahe Arani, and Bahram Zonooz", "title": "Monocular Vision based Crowdsourced 3D Traffic Sign Positioning with\n  Unknown Camera Intrinsics and Distortion Coefficients", "comments": "Accepted at 2020 IEEE 23rd International Conference on Intelligent\n  Transportation Systems (ITSC)", "journal-ref": null, "doi": "10.1109/ITSC45102.2020.9294445", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles and driver assistance systems utilize maps of 3D semantic\nlandmarks for improved decision making. However, scaling the mapping process as\nwell as regularly updating such maps come with a huge cost. Crowdsourced\nmapping of these landmarks such as traffic sign positions provides an appealing\nalternative. The state-of-the-art approaches to crowdsourced mapping use ground\ntruth camera parameters, which may not always be known or may change over time.\nIn this work, we demonstrate an approach to computing 3D traffic sign positions\nwithout knowing the camera focal lengths, principal point, and distortion\ncoefficients a priori. We validate our proposed approach on a public dataset of\ntraffic signs in KITTI. Using only a monocular color camera and GPS, we achieve\nan average single journey relative and absolute positioning accuracy of 0.26 m\nand 1.38 m, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 07:03:17 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chawla", "Hemang", ""], ["Jukola", "Matti", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2007.04639", "submitter": "Mohbat Tharani", "authors": "Mohbat Tharani, Abdul Wahab Amin, Mohammad Maaz and Murtaza Taj", "title": "Attention Neural Network for Trash Detection on Water Channels", "comments": "Object Detection, Trash Detection, Water Quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rivers and canals flowing through cities are often used illegally for dumping\nthe trash. This contaminates freshwater channels as well as causes blockage in\nsewerage resulting in urban flooding. When this contaminated water reaches\nagricultural fields, it results in degradation of soil and poses critical\nenvironmental as well as economic threats. The dumped trash is often found\nfloating on the water surface. The trash could be disfigured, partially\nsubmerged, decomposed into smaller pieces, clumped together with other objects\nwhich obscure its shape and creates a challenging detection problem. This paper\nproposes a method for the detection of visible trash floating on the water\nsurface of the canals in urban areas. We also provide a large dataset, first of\nits kind, trash in water channels that contains object-level annotations. A\nnovel attention layer is proposed that improves the detection of smaller\nobjects. Towards the end of this paper, we provide a detailed comparison of our\nmethod with state-of-the-art object detectors and show that our method\nsignificantly improves the detection of smaller objects. The dataset will be\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:41:30 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Tharani", "Mohbat", ""], ["Amin", "Abdul Wahab", ""], ["Maaz", "Mohammad", ""], ["Taj", "Murtaza", ""]]}, {"id": "2007.04644", "submitter": "Yin Zhao", "authors": "Chaoping Tu, Yin Zhao, Longjun Cai", "title": "ESA-ReID: Entropy-Based Semantic Feature Alignment for Person re-ID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is a challenging task in real-world. Besides\nthe typical application in surveillance system, re-ID also has significant\nvalues to improve the recall rate of people identification in content video (TV\nor Movies). However, the occlusion, shot angle variations and complicated\nbackground make it far away from application, especially in content video. In\nthis paper we propose an entropy based semantic feature alignment model, which\ntakes advantages of the detailed information of the human semantic feature.\nConsidering the uncertainty of semantic segmentation, we introduce a semantic\nalignment with an entropy-based mask which can reduce the negative effects of\nmask segmentation errors. We construct a new re-ID dataset based on content\nvideos with many cases of occlusion and body part missing, which will be\nreleased in future. Extensive studies on both existing datasets and the new\ndataset demonstrate the superior performance of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:56:28 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Tu", "Chaoping", ""], ["Zhao", "Yin", ""], ["Cai", "Longjun", ""]]}, {"id": "2007.04645", "submitter": "Prem Raj", "authors": "Prem Raj, Vinay P. Namboodiri and L. Behera", "title": "Learning to Switch CNNs with Model Agnostic Meta Learning for Fine\n  Precision Visual Servoing", "comments": "Accepted in IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS-2020). For video visit - https://youtu.be/GSG20lmWDUo", "journal-ref": "2020 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 10210-10217", "doi": "10.1109/IROS45743.2020.9341756", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been successfully applied for\nrelative camera pose estimation from labeled image-pair data, without requiring\nany hand-engineered features, camera intrinsic parameters or depth information.\nThe trained CNN can be utilized for performing pose based visual servo control\n(PBVS). One of the ways to improve the quality of visual servo output is to\nimprove the accuracy of the CNN for estimating the relative pose estimation.\nWith a given state-of-the-art CNN for relative pose regression, how can we\nachieve an improved performance for visual servo control? In this paper, we\nexplore switching of CNNs to improve the precision of visual servo control. The\nidea of switching a CNN is due to the fact that the dataset for training a\nrelative camera pose regressor for visual servo control must contain variations\nin relative pose ranging from a very small scale to eventually a larger scale.\nWe found that, training two different instances of the CNN, one for\nlarge-scale-displacements (LSD) and another for small-scale-displacements (SSD)\nand switching them during the visual servo execution yields better results than\ntraining a single CNN with the combined LSD+SSD data. However, it causes extra\nstorage overhead and switching decision is taken by a manually set threshold\nwhich may not be optimal for all the scenes. To eliminate these drawbacks, we\npropose an efficient switching strategy based on model agnostic meta learning\n(MAML) algorithm. In this, a single model is trained to learn parameters which\nare simultaneously good for multiple tasks, namely a binary classification for\nswitching decision, a 6DOF pose regression for LSD data and also a 6DOF pose\nregression for SSD data. The proposed approach performs far better than the\nnaive approach, while storage and run-time overheads are almost negligible.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:56:53 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Raj", "Prem", ""], ["Namboodiri", "Vinay P.", ""], ["Behera", "L.", ""]]}, {"id": "2007.04646", "submitter": "Linpu Fang", "authors": "Linpu Fang, Xingyan Liu, Li Liu, Hang Xu, and Wenxiong Kang", "title": "JGR-P2O: Joint Graph Reasoning based Pixel-to-Offset Prediction Network\n  for 3D Hand Pose Estimation from a Single Depth Image", "comments": "Accepted by ECCV2020 as a Spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art single depth image-based 3D hand pose estimation methods are\nbased on dense predictions, including voxel-to-voxel predictions,\npoint-to-point regression, and pixel-wise estimations. Despite the good\nperformance, those methods have a few issues in nature, such as the poor\ntrade-off between accuracy and efficiency, and plain feature representation\nlearning with local convolutions. In this paper, a novel pixel-wise\nprediction-based method is proposed to address the above issues. The key ideas\nare two-fold: a) explicitly modeling the dependencies among joints and the\nrelations between the pixels and the joints for better local feature\nrepresentation learning; b) unifying the dense pixel-wise offset predictions\nand direct joint regression for end-to-end training. Specifically, we first\npropose a graph convolutional network (GCN) based joint graph reasoning module\nto model the complex dependencies among joints and augment the representation\ncapability of each pixel. Then we densely estimate all pixels' offsets to\njoints in both image plane and depth space and calculate the joints' positions\nby a weighted average over all pixels' predictions, totally discarding the\ncomplex postprocessing operations. The proposed model is implemented with an\nefficient 2D fully convolutional network (FCN) backbone and has only about 1.4M\nparameters. Extensive experiments on multiple 3D hand pose estimation\nbenchmarks demonstrate that the proposed method achieves new state-of-the-art\naccuracy while running very efficiently with around a speed of 110fps on a\nsingle NVIDIA 1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:57:19 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 03:49:36 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Fang", "Linpu", ""], ["Liu", "Xingyan", ""], ["Liu", "Li", ""], ["Xu", "Hang", ""], ["Kang", "Wenxiong", ""]]}, {"id": "2007.04651", "submitter": "Changxu Cheng", "authors": "Changxu Cheng, Wuheng Xu, Xiang Bai, Bin Feng, and Wenyu Liu", "title": "Maximum Entropy Regularization and Chinese Text Recognition", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese text recognition is more challenging than Latin text due to the large\namount of fine-grained Chinese characters and the great imbalance over classes,\nwhich causes a serious overfitting problem. We propose to apply Maximum Entropy\nRegularization to regularize the training process, which is to simply add a\nnegative entropy term to the canonical cross-entropy loss without any\nadditional parameters and modification of a model. We theoretically give the\nconvergence probability distribution and analyze how the regularization\ninfluence the learning process. Experiments on Chinese character recognition,\nChinese text line recognition and fine-grained image classification achieve\nconsistent improvement, proving that the regularization is beneficial to\ngeneralization and robustness of a recognition model.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:19:56 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Cheng", "Changxu", ""], ["Xu", "Wuheng", ""], ["Bai", "Xiang", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2007.04655", "submitter": "Jennifer Maier", "authors": "Jennifer Maier, Marlies Nitschke, Jang-Hwan Choi, Garry Gold, Rebecca\n  Fahrig, Bjoern M. Eskofier, Andreas Maier", "title": "Inertial Measurements for Motion Compensation in Weight-bearing\n  Cone-beam CT of the Knee", "comments": "10 pages, 2 figures, 2 tables, accepted at MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59716-0_2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Involuntary motion during weight-bearing cone-beam computed tomography (CT)\nscans of the knee causes artifacts in the reconstructed volumes making them\nunusable for clinical diagnosis. Currently, image-based or marker-based methods\nare applied to correct for this motion, but often require long execution or\npreparation times. We propose to attach an inertial measurement unit (IMU)\ncontaining an accelerometer and a gyroscope to the leg of the subject in order\nto measure the motion during the scan and correct for it. To validate this\napproach, we present a simulation study using real motion measured with an\noptical 3D tracking system. With this motion, an XCAT numerical knee phantom is\nnon-rigidly deformed during a simulated CT scan creating motion corrupted\nprojections. A biomechanical model is animated with the same tracked motion in\norder to generate measurements of an IMU placed below the knee. In our proposed\nmulti-stage algorithm, these signals are transformed to the global coordinate\nsystem of the CT scan and applied for motion compensation during\nreconstruction. Our proposed approach can effectively reduce motion artifacts\nin the reconstructed volumes. Compared to the motion corrupted case, the\naverage structural similarity index and root mean squared error with respect to\nthe no-motion case improved by 13-21% and 68-70%, respectively. These results\nare qualitatively and quantitatively on par with a state-of-the-art\nmarker-based method we compared our approach to. The presented study shows the\nfeasibility of this novel approach, and yields promising results towards a\npurely IMU-based motion compensation in C-arm CT.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:26:27 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Maier", "Jennifer", ""], ["Nitschke", "Marlies", ""], ["Choi", "Jang-Hwan", ""], ["Gold", "Garry", ""], ["Fahrig", "Rebecca", ""], ["Eskofier", "Bjoern M.", ""], ["Maier", "Andreas", ""]]}, {"id": "2007.04657", "submitter": "Timothy Callemein", "authors": "Timothy Callemein, Wiebe Van Ranst and Toon Goedem\\'e", "title": "The autonomous hidden camera crew", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": "10.23919/MVA.2017.7986769", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reality TV shows that follow people in their day-to-day lives are not a new\nconcept. However, the traditional methods used in the industry require a lot of\nmanual labour and need the presence of at least one physical camera man.\nBecause of this, the subjects tend to behave differently when they are aware of\nbeing recorded. This paper will present an approach to follow people in their\nday-to-day lives, for long periods of time (months to years), while being as\nunobtrusive as possible. To do this, we use unmanned cinematographically-aware\ncameras hidden in people's houses. Our contribution in this paper is twofold:\nFirst, we create a system to limit the amount of recorded data by intelligently\ncontrolling a video switch matrix, in combination with a multi-channel\nrecorder. Second, we create a virtual camera man by controlling a PTZ camera to\nautomatically make cinematographically pleasing shots. Throughout this paper,\nwe worked closely with a real camera crew. This enabled us to compare the\nresults of our system to the work of trained professionals.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:27:34 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Callemein", "Timothy", ""], ["Van Ranst", "Wiebe", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2007.04666", "submitter": "Timothy Callemein", "authors": "Steven Puttemans, Timothy Callemein and Toon Goedem\\'e", "title": "Building Robust Industrial Applicable Object Detection Models Using\n  Transfer Learning and Single Pass Deep Learning Architectures", "comments": null, "journal-ref": null, "doi": "10.5220/0006562002090217", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uprising trend of deep learning in computer vision and artificial\nintelligence can simply not be ignored. On the most diverse tasks, from\nrecognition and detection to segmentation, deep learning is able to obtain\nstate-of-the-art results, reaching top notch performance. In this paper we\nexplore how deep convolutional neural networks dedicated to the task of object\ndetection can improve our industrial-oriented object detection pipelines, using\nstate-of-the-art open source deep learning frameworks, like Darknet. By using a\ndeep learning architecture that integrates region proposals, classification and\nprobability estimation in a single run, we aim at obtaining real-time\nperformance. We focus on reducing the needed amount of training data\ndrastically by exploring transfer learning, while still maintaining a high\naverage precision. Furthermore we apply these algorithms to two industrially\nrelevant applications, one being the detection of promotion boards in eye\ntracking data and the other detecting and recognizing packages of warehouse\nproducts for augmented advertisements.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:50:45 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Puttemans", "Steven", ""], ["Callemein", "Timothy", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2007.04670", "submitter": "Xiangru Tang", "authors": "Xiangru Tang, Haoyuan Wang, Xiang Pan, Jiyang Qi", "title": "Multi-Granularity Modularized Network for Abstract Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract visual reasoning connects mental abilities to the physical world,\nwhich is a crucial factor in cognitive development. Most toddlers display\nsensitivity to this skill, but it is not easy for machines. Aimed at it, we\nfocus on the Raven Progressive Matrices Test, designed to measure cognitive\nreasoning. Recent work designed some black-boxes to solve it in an end-to-end\nfashion, but they are incredibly complicated and difficult to explain. Inspired\nby cognitive studies, we propose a Multi-Granularity Modularized Network (MMoN)\nto bridge the gap between the processing of raw sensory information and\nsymbolic reasoning. Specifically, it learns modularized reasoning functions to\nmodel the semantic rule from the visual grounding in a neuro-symbolic and\nsemi-supervision way. To comprehensively evaluate MMoN, our experiments are\nconducted on the dataset of both seen and unseen reasoning rules. The result\nshows that MMoN is well suited for abstract visual reasoning and also\nexplainable on the generalization test.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 09:54:05 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 02:32:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Tang", "Xiangru", ""], ["Wang", "Haoyuan", ""], ["Pan", "Xiang", ""], ["Qi", "Jiyang", ""]]}, {"id": "2007.04671", "submitter": "Timothy Callemein", "authors": "Timothy Callemein, Kristof Van Beeck, Geert Br\\^one, Toon Goedem\\'e", "title": "Automated analysis of eye-tracker-based human-human interaction studies", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-13-1056-0_50", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile eye-tracking systems have been available for about a decade now and\nare becoming increasingly popular in different fields of application, including\nmarketing, sociology, usability studies and linguistics. While the\nuser-friendliness and ergonomics of the hardware are developing at a rapid\npace, the software for the analysis of mobile eye-tracking data in some points\nstill lacks robustness and functionality. With this paper, we investigate which\nstate-of-the-art computer vision algorithms may be used to automate the\npost-analysis of mobile eye-tracking data. For the case study in this paper, we\nfocus on mobile eye-tracker recordings made during human-human face-to-face\ninteractions. We compared two recent publicly available frameworks (YOLOv2 and\nOpenPose) to relate the gaze location generated by the eye-tracker to the head\nand hands visible in the scene camera data. In this paper we will show that the\nuse of this single-pipeline framework provides robust results, which are both\nmore accurate and faster than previous work in the field. Moreover, our\napproach does not rely on manual interventions during this process.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:00:03 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Callemein", "Timothy", ""], ["Van Beeck", "Kristof", ""], ["Br\u00f4ne", "Geert", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2007.04678", "submitter": "Timothy Callemein", "authors": "Timothy Callemein, Kristof Van Beeck, and Toon Goedem\\'e", "title": "How low can you go? Privacy-preserving people detection with an\n  omni-directional camera", "comments": null, "journal-ref": null, "doi": "10.5220/0007573206300637", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use a ceiling-mounted omni-directional camera to detect\npeople in a room. This can be used as a sensor to measure the occupancy of\nmeeting rooms and count the amount of flex-desk working spaces available. If\nthese devices can be integrated in an embedded low-power sensor, it would form\nan ideal extension of automated room reservation systems in office\nenvironments. The main challenge we target here is ensuring the privacy of the\npeople filmed. The approach we propose is going to extremely low image\nresolutions, such that it is impossible to recognise people or read potentially\nconfidential documents. Therefore, we retrained a single-shot low-resolution\nperson detection network with automatically generated ground truth. In this\npaper, we prove the functionality of this approach and explore how low we can\ngo in resolution, to determine the optimal trade-off between recognition\naccuracy and privacy preservation. Because of the low resolution, the result is\na lightweight network that can potentially be deployed on embedded hardware.\nSuch embedded implementation enables the development of a decentralised smart\ncamera which only outputs the required meta-data (i.e. the number of persons in\nthe meeting room).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:10:23 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Callemein", "Timothy", ""], ["Van Beeck", "Kristof", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2007.04687", "submitter": "Peng Wu", "authors": "Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu,\n  Zhiwei Yang", "title": "Not only Look, but also Listen: Learning Multimodal Violence Detection\n  under Weak Supervision", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Violence detection has been studied in computer vision for years. However,\nprevious work are either superficial, e.g., classification of short-clips, and\nthe single scenario, or undersupplied, e.g., the single modality, and\nhand-crafted features based multimodality. To address this problem, in this\nwork we first release a large-scale and multi-scene dataset named XD-Violence\nwith a total duration of 217 hours, containing 4754 untrimmed videos with audio\nsignals and weak labels. Then we propose a neural network containing three\nparallel branches to capture different relations among video snippets and\nintegrate features, where holistic branch captures long-range dependencies\nusing similarity prior, localized branch captures local positional relation\nusing proximity prior, and score branch dynamically captures the closeness of\npredicted score. Besides, our method also includes an approximator to meet the\nneeds of online detection. Our method outperforms other state-of-the-art\nmethods on our released dataset and other existing benchmark. Moreover,\nextensive experimental results also show the positive effect of multimodal\n(audio-visual) input and modeling relationships. The code and dataset will be\nreleased in https://roc-ng.github.io/XD-Violence/.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:29:31 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 04:16:22 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wu", "Peng", ""], ["Liu", "Jing", ""], ["Shi", "Yujia", ""], ["Sun", "Yujia", ""], ["Shao", "Fangtao", ""], ["Wu", "Zhaoyang", ""], ["Yang", "Zhiwei", ""]]}, {"id": "2007.04690", "submitter": "Alessandro Ortis", "authors": "Sebastiano Battiato, Alessandro Ortis, Francesca Trenta, Lorenzo\n  Ascari, Mara Politi, Consolata Siniscalco", "title": "Pollen13K: A Large Scale Microscope Pollen Grain Image Dataset", "comments": "This paper is a preprint of a paper accepted at the IEEE\n  International Conference on Image Processing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pollen grain classification has a remarkable role in many fields from\nmedicine to biology and agronomy. Indeed, automatic pollen grain classification\nis an important task for all related applications and areas. This work presents\nthe first large-scale pollen grain image dataset, including more than 13\nthousands objects. After an introduction to the problem of pollen grain\nclassification and its motivations, the paper focuses on the employed data\nacquisition steps, which include aerobiological sampling, microscope image\nacquisition, object detection, segmentation and labelling. Furthermore, a\nbaseline experimental assessment for the task of pollen classification on the\nbuilt dataset, together with discussion on the achieved results, is presented.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:33:31 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Battiato", "Sebastiano", ""], ["Ortis", "Alessandro", ""], ["Trenta", "Francesca", ""], ["Ascari", "Lorenzo", ""], ["Politi", "Mara", ""], ["Siniscalco", "Consolata", ""]]}, {"id": "2007.04717", "submitter": "Francesco Guarnera", "authors": "Oliver Giudice (1 and 2), Dario Allegra (1), Francesco Guarnera (1 and\n  2), Filippo Stanco (1), Sebastiano Battiato (1 and 2) ((1) University of\n  Catania, (2) iCTLab s.r.l. - Spin-off of University of Catania)", "title": "Animated GIF optimization by adaptive color local table management", "comments": null, "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP)", "doi": "10.1109/ICIP40778.2020.9190967", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After thirty years of the GIF file format, today is becoming more popular\nthan ever: being a great way of communication for friends and communities on\nInstant Messengers and Social Networks. While being so popular, the original\ncompression method to encode GIF images have not changed a bit. On the other\nhand popularity means that storage saving becomes an issue for hosting\nplatforms. In this paper a parametric optimization technique for animated GIFs\nwill be presented. The proposed technique is based on Local Color Table\nselection and color remapping in order to create optimized animated GIFs while\npreserving the original format. The technique achieves good results in terms of\nbyte reduction with limited or no loss of perceived color quality. Tests\ncarried out on 1000 GIF files demonstrate the effectiveness of the proposed\noptimization strategy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:36:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Giudice", "Oliver", "", "1 and 2"], ["Allegra", "Dario", "", "1 and\n  2"], ["Guarnera", "Francesco", "", "1 and\n  2"], ["Stanco", "Filippo", "", "1 and 2"], ["Battiato", "Sebastiano", "", "1 and 2"]]}, {"id": "2007.04734", "submitter": "Nan Wang", "authors": "Nan Wang, Chengwei Chen, Yuan Xie, Lizhuang Ma", "title": "Brain Tumor Anomaly Detection via Latent Regularized Adversarial Network", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of medical imaging technology, medical images have\nbecome an important basis for doctors to diagnose patients. The brain structure\nin the collected data is complicated, thence, doctors are required to spend\nplentiful energy when diagnosing brain abnormalities. Aiming at the imbalance\nof brain tumor data and the rare amount of labeled data, we propose an\ninnovative brain tumor abnormality detection algorithm. The semi-supervised\nanomaly detection model is proposed in which only healthy (normal) brain images\nare trained. Model capture the common pattern of the normal images in the\ntraining process and detect anomalies based on the reconstruction error of\nlatent space. Furthermore, the method first uses singular value to constrain\nthe latent space and jointly optimizes the image space through multiple loss\nfunctions, which make normal samples and abnormal samples more separable in the\nfeature-level. This paper utilizes BraTS, HCP, MNIST, and CIFAR-10 datasets to\ncomprehensively evaluate the effectiveness and practicability. Extensive\nexperiments on intra- and cross-dataset tests prove that our semi-supervised\nmethod achieves outperforms or comparable results to state-of-the-art\nsupervised techniques.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 12:12:16 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Wang", "Nan", ""], ["Chen", "Chengwei", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2007.04754", "submitter": "Mayank Patwari", "authors": "Mayank Patwari, Ralf Gutjahr, Rainer Raupach, Andreas Maier", "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering", "comments": "10 pages, 4 figures, 1 table. Accepted at MICCAI2020", "journal-ref": null, "doi": "10.1007/978-3-030-59713-9_49", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown great success in low dose CT denoising.\nHowever, most of these deep neural networks have several hundred thousand\ntrainable parameters. This, combined with the inherent non-linearity of the\nneural network, makes the deep neural network diffcult to understand with low\naccountability. In this study we introduce JBFnet, a neural network for low\ndose CT denoising. The architecture of JBFnet implements iterative bilateral\nfiltering. The filter functions of the Joint Bilateral Filter (JBF) are learned\nvia shallow convolutional networks. The guidance image is estimated by a deep\nneural network. JBFnet is split into four filtering blocks, each of which\nperforms Joint Bilateral Filtering. Each JBF block consists of 112 trainable\nparameters, making the noise removal process comprehendable. The Noise Map (NM)\nis added after filtering to preserve high level features. We train JBFnet with\nthe data from the body scans of 10 patients, and test it on the AAPM low dose\nCT Grand Challenge dataset. We compare JBFnet with state-of-the-art deep\nlearning networks. JBFnet outperforms CPCE3D, GAN and deep GFnet on the test\ndataset in terms of noise removal while preserving structures. We conduct\nseveral ablation studies to test the performance of our network architecture\nand training method. Our current setup achieves the best performance, while\nstill maintaining behavioural accountability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 12:59:28 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Patwari", "Mayank", ""], ["Gutjahr", "Ralf", ""], ["Raupach", "Rainer", ""], ["Maier", "Andreas", ""]]}, {"id": "2007.04755", "submitter": "Yongqin Xian", "authors": "Yongqin Xian, Bruno Korbar, Matthijs Douze, Bernt Schiele, Zeynep\n  Akata, Lorenzo Torresani", "title": "Generalized Many-Way Few-Shot Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning methods operate in low data regimes. The aim is to learn\nwith few training examples per class. Although significant progress has been\nmade in few-shot image classification, few-shot video recognition is relatively\nunexplored and methods based on 2D CNNs are unable to learn temporal\ninformation. In this work we thus develop a simple 3D CNN baseline, surpassing\nexisting methods by a large margin. To circumvent the need of labeled examples,\nwe propose to leverage weakly-labeled videos from a large dataset using tag\nretrieval followed by selecting the best clips with visual similarities,\nyielding further improvement. Our results saturate current 5-way benchmarks for\nfew-shot video classification and therefore we propose a new challenging\nbenchmark involving more classes and a mixture of classes with varying\nsupervision.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:05:32 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Xian", "Yongqin", ""], ["Korbar", "Bruno", ""], ["Douze", "Matthijs", ""], ["Schiele", "Bernt", ""], ["Akata", "Zeynep", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2007.04756", "submitter": "Manas Gupta", "authors": "Manas Gupta, Siddharth Aravindan, Aleksandra Kalisz, Vijay\n  Chandrasekhar, Lin Jie", "title": "Learning to Prune Deep Neural Networks via Reinforcement Learning", "comments": "Accepted at the ICML 2020 Workshop on Automated Machine Learning\n  (AutoML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes PuRL - a deep reinforcement learning (RL) based algorithm\nfor pruning neural networks. Unlike current RL based model compression\napproaches where feedback is given only at the end of each episode to the\nagent, PuRL provides rewards at every pruning step. This enables PuRL to\nachieve sparsity and accuracy comparable to current state-of-the-art methods,\nwhile having a much shorter training cycle. PuRL achieves more than 80%\nsparsity on the ResNet-50 model while retaining a Top-1 accuracy of 75.37% on\nthe ImageNet dataset. Through our experiments we show that PuRL is also able to\nsparsify already efficient architectures like MobileNet-V2. In addition to\nperformance characterisation experiments, we also provide a discussion and\nanalysis of the various RL design choices that went into the tuning of the\nMarkov Decision Process underlying PuRL. Lastly, we point out that PuRL is\nsimple to use and can be easily adapted for various architectures.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:06:07 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gupta", "Manas", ""], ["Aravindan", "Siddharth", ""], ["Kalisz", "Aleksandra", ""], ["Chandrasekhar", "Vijay", ""], ["Jie", "Lin", ""]]}, {"id": "2007.04768", "submitter": "Mayank Patwari", "authors": "Mayank Patwari, Ralf Gutjahr, Rainer Raupach, Andreas Maier", "title": "Low Dose CT Denoising via Joint Bilateral Filtering and Intelligent\n  Parameter Optimization", "comments": "4 pages, 5 figures, 1 table. Accepted at CT Meeting 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising of clinical CT images is an active area for deep learning research.\nCurrent clinically approved methods use iterative reconstruction methods to\nreduce the noise in CT images. Iterative reconstruction techniques require\nmultiple forward and backward projections, which are time-consuming and\ncomputationally expensive. Recently, deep learning methods have been\nsuccessfully used to denoise CT images. However, conventional deep learning\nmethods suffer from the 'black box' problem. They have low accountability,\nwhich is necessary for use in clinical imaging situations. In this paper, we\nuse a Joint Bilateral Filter (JBF) to denoise our CT images. The guidance image\nof the JBF is estimated using a deep residual convolutional neural network\n(CNN). The range smoothing and spatial smoothing parameters of the JBF are\ntuned by a deep reinforcement learning task. Our actor first chooses a\nparameter, and subsequently chooses an action to tune the value of the\nparameter. A reward network is designed to direct the reinforcement learning\ntask. Our denoising method demonstrates good denoising performance, while\nretaining structural information. Our method significantly outperforms state of\nthe art deep neural networks. Moreover, our method has only two parameters,\nwhich makes it significantly more interpretable and reduces the 'black box'\nproblem. We experimentally measure the impact of our intelligent parameter\noptimization and our reward network. Our studies show that our current setup\nyields the best results in terms of structural preservation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:17:36 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Patwari", "Mayank", ""], ["Gutjahr", "Ralf", ""], ["Raupach", "Rainer", ""], ["Maier", "Andreas", ""]]}, {"id": "2007.04774", "submitter": "Dominik M\\\"uller", "authors": "Dominik M\\\"uller, I\\~naki Soto Rey, Frank Kramer", "title": "Automated Chest CT Image Segmentation of COVID-19 Lung Infection based\n  on 3D U-Net", "comments": "Code repository: https://github.com/frankkramer-lab/covid19.MIScnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease 2019 (COVID-19) affects billions of lives around the\nworld and has a significant impact on public healthcare. Due to rising\nskepticism towards the sensitivity of RT-PCR as screening method, medical\nimaging like computed tomography offers great potential as alternative. For\nthis reason, automated image segmentation is highly desired as clinical\ndecision support for quantitative assessment and disease monitoring. However,\npublicly available COVID-19 imaging data is limited which leads to overfitting\nof traditional approaches. To address this problem, we propose an innovative\nautomated segmentation pipeline for COVID-19 infected regions, which is able to\nhandle small datasets by utilization as variant databases. Our method focuses\non on-the-fly generation of unique and random image patches for training by\nperforming several preprocessing methods and exploiting extensive data\naugmentation. For further reduction of the overfitting risk, we implemented a\nstandard 3D U-Net architecture instead of new or computational complex neural\nnetwork architectures. Through a 5-fold cross-validation on 20 CT scans of\nCOVID-19 patients, we were able to develop a highly accurate as well as robust\nsegmentation model for lungs and COVID-19 infected regions without overfitting\non the limited data. Our method achieved Dice similarity coefficients of 0.956\nfor lungs and 0.761 for infection. We demonstrated that the proposed method\noutperforms related approaches, advances the state-of-the-art for COVID-19\nsegmentation and improves medical image analysis with limited data. The code\nand model are available under the following link:\nhttps://github.com/frankkramer-lab/covid19.MIScnn\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:29:26 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["M\u00fcller", "Dominik", ""], ["Rey", "I\u00f1aki Soto", ""], ["Kramer", "Frank", ""]]}, {"id": "2007.04780", "submitter": "Anna Volokitin", "authors": "Anna Volokitin, Ertunc Erdil, Neerav Karani, Kerem Can Tezcan, Xiaoran\n  Chen, Luc Van Gool, Ender Konukoglu", "title": "Modelling the Distribution of 3D Brain MRI using a 2D Slice VAE", "comments": "accepted for publication at MICCAI 2020. Code available\n  https://github.com/voanna/slices-to-3d-brain-vae/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modelling has been an essential tool in medical image analysis,\nespecially for analyzing brain Magnetic Resonance Images (MRI). Recent deep\nlearning techniques for estimating high-dimensional distributions, in\nparticular Variational Autoencoders (VAEs), opened up new avenues for\nprobabilistic modeling. Modelling of volumetric data has remained a challenge,\nhowever, because constraints on available computation and training data make it\ndifficult effectively leverage VAEs, which are well-developed for 2D images. We\npropose a method to model 3D MR brain volumes distribution by combining a 2D\nslice VAE with a Gaussian model that captures the relationships between slices.\nWe do so by estimating the sample mean and covariance in the latent space of\nthe 2D model over the slice direction. This combined model lets us sample new\ncoherent stacks of latent variables to decode into slices of a volume. We also\nintroduce a novel evaluation method for generated volumes that quantifies how\nwell their segmentations match those of true brain anatomy. We demonstrate that\nour proposed model is competitive in generating high quality volumes at high\nresolutions according to both traditional metrics and our proposed evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:23:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Volokitin", "Anna", ""], ["Erdil", "Ertunc", ""], ["Karani", "Neerav", ""], ["Tezcan", "Kerem Can", ""], ["Chen", "Xiaoran", ""], ["Van Gool", "Luc", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2007.04782", "submitter": "Igor Andr\\'e Pegoraro Santana", "authors": "Igor Andr\\'e Pegoraro Santana, Marcos Aurelio Domingues", "title": "A Systematic Review on Context-Aware Recommender Systems using Deep\n  Learning and Embeddings", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are tools that improve how users find relevant\ninformation in web systems, so they do not face too much information. In order\nto generate better recommendations, the context of information should be used\nin the recommendation process. Context-Aware Recommender Systems were created,\naccomplishing state-of-the-art results and improving traditional recommender\nsystems. There are many approaches to build recommender systems, and two of the\nmost prominent advances in area have been the use of Embeddings to represent\nthe data in the recommender system, and the use of Deep Learning architectures\nto generate the recommendations to the user. A systematic review adopts a\nformal and systematic method to perform a bibliographic review, and it is used\nto identify and evaluate all the research in certain area of study, by\nanalyzing the relevant research published. A systematic review was conducted to\nunderstand how the Deep Learning and Embeddings techniques are being applied to\nimprove Context-Aware Recommender Systems. We summarized the architectures that\nare used to create those and the domains that they are used.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:23:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Santana", "Igor Andr\u00e9 Pegoraro", ""], ["Domingues", "Marcos Aurelio", ""]]}, {"id": "2007.04785", "submitter": "Renqian Luo", "authors": "Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu", "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search", "comments": "Code is available at https://github.com/renqianluo/GBDT-NAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided at https://github.com/renqianluo/GBDT-NAS.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:28:49 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 11:43:02 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 07:31:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Luo", "Renqian", ""], ["Tan", "Xu", ""], ["Wang", "Rui", ""], ["Qin", "Tao", ""], ["Chen", "Enhong", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2007.04793", "submitter": "Tom Needham", "authors": "Xiaoyang Guo, Aditi Basu Bal, Tom Needham, Anuj Srivastava", "title": "Statistical shape analysis of brain arterial networks (BAN)", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.00287", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structures of brain arterial networks (BANs) - that are complex arrangements\nof individual arteries, their branching patterns, and inter-connectivities -\nplay an important role in characterizing and understanding brain physiology.\nOne would like tools for statistically analyzing the shapes of BANs, i.e.\nquantify shape differences, compare population of subjects, and study the\neffects of covariates on these shapes. This paper mathematically represents and\nstatistically analyzes BAN shapes as elastic shape graphs. Each elastic shape\ngraph is made up of nodes that are connected by a number of 3D curves, and\nedges, with arbitrary shapes. We develop a mathematical representation, a\nRiemannian metric and other geometrical tools, such as computations of\ngeodesics, means and covariances, and PCA for analyzing elastic graphs and\nBANs. This analysis is applied to BANs after separating them into four\ncomponents -- top, bottom, left, and right. This framework is then used to\ngenerate shape summaries of BANs from 92 subjects, and to study the effects of\nage and gender on shapes of BAN components. We conclude that while gender\neffects require further investigation, the age has a clear, quantifiable effect\non BAN shapes. Specifically, we find an increased variance in BAN shapes as age\nincreases.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:46:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Bal", "Aditi Basu", ""], ["Needham", "Tom", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2007.04806", "submitter": "Rasmus Malik Thaarup H{\\o}egh", "authors": "Laura Rieger, Rasmus M. Th. H{\\o}egh, and Lars K. Hansen", "title": "Client Adaptation improves Federated Learning with Simulated Non-IID\n  Clients", "comments": "11 pages, 11 figures. To appear at International Workshop on\n  Federated Learning for User Privacy and Data Confidentiality in Conjunction\n  with ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a federated learning approach for learning a client adaptable,\nrobust model when data is non-identically and non-independently distributed\n(non-IID) across clients. By simulating heterogeneous clients, we show that\nadding learned client-specific conditioning improves model performance, and the\napproach is shown to work on balanced and imbalanced data set from both audio\nand image domains. The client adaptation is implemented by a conditional gated\nactivation unit and is particularly beneficial when there are large differences\nbetween the data distribution for each client, a common scenario in federated\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:48:39 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Rieger", "Laura", ""], ["H\u00f8egh", "Rasmus M. Th.", ""], ["Hansen", "Lars K.", ""]]}, {"id": "2007.04807", "submitter": "Hongxu Yang", "authors": "Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With", "title": "Medical Instrument Detection in Ultrasound-Guided Interventions: A\n  Review", "comments": "Draft paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical instrument detection is essential for computer-assisted interventions\nsince it would facilitate the surgeons to find the instrument efficiently with\na better interpretation, which leads to a better outcome. This article reviews\nmedical instrument detection methods in the ultrasound-guided intervention.\nFirst, we present a comprehensive review of instrument detection methodologies,\nwhich include traditional non-data-driven methods and data-driven methods. The\nnon-data-driven methods were extensively studied prior to the era of machine\nlearning, i.e. data-driven approaches. We discuss the main clinical\napplications of medical instrument detection in ultrasound, including\nanesthesia, biopsy, prostate brachytherapy, and cardiac catheterization, which\nwere validated on clinical datasets. Finally, we selected several principal\npublications to summarize the key issues and potential research directions for\nthe computer-assisted intervention community.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:50:18 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:32:12 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yang", "Hongxu", ""], ["Shan", "Caifeng", ""], ["Kolen", "Alexander F.", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2007.04865", "submitter": "Jonghye Woo", "authors": "Jonghye Woo, Fangxu Xing, Jerry L. Prince, Maureen Stone, Arnold\n  Gomez, Timothy G. Reese, Van J. Wedeen, Georges El Fakhri", "title": "A Deep Joint Sparse Non-negative Matrix Factorization Framework for\n  Identifying the Common and Subject-specific Functional Units of Tongue Motion\n  During Speech", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligible speech is produced by creating varying internal local muscle\ngroupings -- i.e., functional units -- that are generated in a systematic and\ncoordinated manner. There are two major challenges in characterizing and\nanalyzing functional units.~First, due to the complex and convoluted nature of\ntongue structure and function, it is of great importance to develop a method\nthat can accurately decode complex muscle coordination patterns during speech.\nSecond, it is challenging to keep identified functional units across subjects\ncomparable due to their substantial variability. In this work, to address these\nchallenges, we develop a new deep learning framework to identify common and\nsubject-specific functional units of tongue motion during speech.~Our framework\nhinges on joint deep graph-regularized sparse non-negative matrix factorization\n(NMF) using motion quantities derived from displacements by tagged Magnetic\nResonance Imaging. More specifically, we transform NMF with sparse and graph\nregularizations into modular architectures akin to deep neural networks by\nmeans of unfolding the Iterative Shrinkage-Thresholding Algorithm to learn\ninterpretable building blocks and associated weighting map. We then apply\nspectral clustering to common and subject-specific weighting maps from which we\njointly determine the common and subject-specific functional units. Experiments\ncarried out with simulated datasets show that the proposed method achieved on\npar or better clustering performance over the comparison methods. Experiments\ncarried out with in vivo tongue motion data show that the proposed method can\ndetermine the common and subject-specific functional units with increased\ninterpretability and decreased size variability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:05:44 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 23:10:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Woo", "Jonghye", ""], ["Xing", "Fangxu", ""], ["Prince", "Jerry L.", ""], ["Stone", "Maureen", ""], ["Gomez", "Arnold", ""], ["Reese", "Timothy G.", ""], ["Wedeen", "Van J.", ""], ["Fakhri", "Georges El", ""]]}, {"id": "2007.04873", "submitter": "Yuming Shen", "authors": "Yuming Shen, Jie Qin, Lei Huang", "title": "Invertible Zero-Shot Recognition Flows", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been successfully applied to Zero-Shot Learning\n(ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the\nhardness of training with ZSL-oriented regularizers and the limited generation\nquality) hinder the existing generative ZSL models from fully bypassing the\nseen-unseen bias. To tackle the above limitations, for the first time, this\nwork incorporates a new family of generative models (i.e., flow-based models)\ninto ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data\nembeddings (i.e., the semantic factors and the non-semantic ones) with the\nforward pass of an invertible flow network, while the reverse pass generates\ndata samples. This procedure theoretically extends conventional generative\nflows to a factorized conditional scheme. To explicitly solve the bias problem,\nour model enlarges the seen-unseen distributional discrepancy based on negative\nsample-based distance measurement. Notably, IZF works flexibly with either a\nnaive Bayesian classifier or a held-out trainable one for zero-shot\nrecognition. Experiments on widely-adopted ZSL benchmarks demonstrate the\nsignificant performance gain of IZF over existing methods, in both classic and\ngeneralized settings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:21:28 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Shen", "Yuming", ""], ["Qin", "Jie", ""], ["Huang", "Lei", ""]]}, {"id": "2007.04883", "submitter": "Xiaogang Wang", "authors": "Xiaogang Wang, Yuelang Xu, Kai Xu, Andrea Tagliasacchi, Bin Zhou, Ali\n  Mahdavi-Amiri, Hao Zhang", "title": "PIE-NET: Parametric Inference of Point Cloud Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an end-to-end learnable technique to robustly identify feature\nedges in 3D point cloud data. We represent these edges as a collection of\nparametric curves (i.e.,lines, circles, and B-splines). Accordingly, our deep\nneural network, coined PIE-NET, is trained for parametric inference of edges.\nThe network relies on a \"region proposal\" architecture, where a first module\nproposes an over-complete collection of edge and corner points, and a second\nmodule ranks each proposal to decide whether it should be considered. We train\nand evaluate our method on the ABC dataset, a large dataset of CAD models, and\ncompare our results to those produced by traditional (non-learning) processing\npipelines, as well as a recent deep learning based edge detector (EC-NET). Our\nresults significantly improve over the state-of-the-art from both a\nquantitative and qualitative standpoint.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:35:10 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 15:25:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Xiaogang", ""], ["Xu", "Yuelang", ""], ["Xu", "Kai", ""], ["Tagliasacchi", "Andrea", ""], ["Zhou", "Bin", ""], ["Mahdavi-Amiri", "Ali", ""], ["Zhang", "Hao", ""]]}, {"id": "2007.04901", "submitter": "Gongyang Li", "authors": "Gongyang Li, Zhi Liu, Linwei Ye, Yang Wang, Haibin Ling", "title": "Cross-Modal Weighting Network for RGB-D Salient Object Detection", "comments": "Accepted in ECCV2020. Code: https://github.com/MathLee/CMWNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth maps contain geometric clues for assisting Salient Object Detection\n(SOD). In this paper, we propose a novel Cross-Modal Weighting (CMW) strategy\nto encourage comprehensive interactions between RGB and depth channels for\nRGB-D SOD. Specifically, three RGB-depth interaction modules, named CMW-L,\nCMW-M and CMW-H, are developed to deal with respectively low-, middle- and\nhigh-level cross-modal information fusion. These modules use Depth-to-RGB\nWeighing (DW) and RGB-to-RGB Weighting (RW) to allow rich cross-modal and\ncross-scale interactions among feature layers generated by different network\nblocks. To effectively train the proposed Cross-Modal Weighting Network\n(CMWNet), we design a composite loss function that summarizes the errors\nbetween intermediate predictions and ground truth over different scales. With\nall these novel components working together, CMWNet effectively fuses\ninformation from RGB and depth channels, and meanwhile explores object\nlocalization and details across scales. Thorough evaluations demonstrate CMWNet\nconsistently outperforms 15 state-of-the-art RGB-D SOD methods on seven popular\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 16:01:44 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Li", "Gongyang", ""], ["Liu", "Zhi", ""], ["Ye", "Linwei", ""], ["Wang", "Yang", ""], ["Ling", "Haibin", ""]]}, {"id": "2007.04905", "submitter": "Lukasz Wandzik", "authors": "Lukasz Wandzik, Raul Vicente Garcia, J\\\"org Kr\\\"uger", "title": "Uncertainty Quantification in Deep Residual Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification is an important and challenging problem in deep\nlearning. Previous methods rely on dropout layers which are not present in\nmodern deep architectures or batch normalization which is sensitive to batch\nsizes. In this work, we address the problem of uncertainty quantification in\ndeep residual networks by using a regularization technique called stochastic\ndepth. We show that training residual networks using stochastic depth can be\ninterpreted as a variational approximation to the intractable posterior over\nthe weights in Bayesian neural networks. We demonstrate that by sampling from a\ndistribution of residual networks with varying depth and shared weights,\nmeaningful uncertainty estimates can be obtained. Moreover, compared to the\noriginal formulation of residual networks, our method produces well-calibrated\nsoftmax probabilities with only minor changes to the network's structure. We\nevaluate our approach on popular computer vision datasets and measure the\nquality of uncertainty estimates. We also test the robustness to domain shift\nand show that our method is able to express higher predictive uncertainty on\nout-of-distribution samples. Finally, we demonstrate how the proposed approach\ncould be used to obtain uncertainty estimates in facial verification\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 16:05:37 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Wandzik", "Lukasz", ""], ["Garcia", "Raul Vicente", ""], ["Kr\u00fcger", "J\u00f6rg", ""]]}, {"id": "2007.04928", "submitter": "Sontje Ihler MSc", "authors": "Sontje Ihler and Max-Heinrich Laves and Tobias Ortmaier", "title": "Patient-Specific Domain Adaptation for Fast Optical Flow Based on\n  Teacher-Student Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast motion feedback is crucial in computer-aided surgery (CAS) on moving\ntissue. Image-assistance in safety-critical vision applications requires a\ndense tracking of tissue motion. This can be done using optical flow (OF).\nAccurate motion predictions at high processing rates lead to higher patient\nsafety. Current deep learning OF models show the common speed vs. accuracy\ntrade-off. To achieve high accuracy at high processing rates, we propose\npatient-specific fine-tuning of a fast model. This minimizes the domain gap\nbetween training and application data, while reducing the target domain to the\ncapability of the lower complex, fast model. We propose to obtain training\nsequences pre-operatively in the operation room. We handle missing ground\ntruth, by employing teacher-student learning. Using flow estimations from\nteacher model FlowNet2 we specialize a fast student model FlowNet2S on the\npatient-specific domain. Evaluation is performed on sequences from the Hamlyn\ndataset. Our student model shows very good performance after fine-tuning.\nTracking accuracy is comparable to the teacher model at a speed up of factor\nsix. Fine-tuning can be performed within minutes, making it feasible for the\noperation room. Our method allows to use a real-time capable model that was\npreviously not suited for this task. This method is laying the path for\nimproved patient-specific motion estimation in CAS.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:01:08 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ihler", "Sontje", ""], ["Laves", "Max-Heinrich", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "2007.04931", "submitter": "Mattia Litrico", "authors": "Oliver Giudice (1), Mattia Litrico (1), Sebastiano Battiato (1 and 2)\n  ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University of\n  Catania)", "title": "Single architecture and multiple task deep neural network for altered\n  fingerprint analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprints are one of the most copious evidence in a crime scene and, for\nthis reason, they are frequently used by law enforcement for identification of\nindividuals. But fingerprints can be altered. \"Altered fingerprints\", refers to\nintentionally damage of the friction ridge pattern and they are often used by\nsmart criminals in hope to evade law enforcement. We use a deep neural network\napproach training an Inception-v3 architecture. This paper proposes a method\nfor detection of altered fingerprints, identification of types of alterations\nand recognition of gender, hand and fingers. We also produce activation maps\nthat show which part of a fingerprint the neural network has focused on, in\norder to detect where alterations are positioned. The proposed approach\nachieves an accuracy of 98.21%, 98.46%, 92.52%, 97.53% and 92,18% for the\nclassification of fakeness, alterations, gender, hand and fingers, respectively\non the SO.CO.FING. dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:02:09 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Giudice", "Oliver", "", "1 and 2"], ["Litrico", "Mattia", "", "1 and 2"], ["Battiato", "Sebastiano", "", "1 and 2"]]}, {"id": "2007.04934", "submitter": "Timothy Callemein", "authors": "Timothy Callemein, Kristof Van Beeck and Toon Goedem\\'e", "title": "Anyone here? Smart embedded low-resolution omnidirectional video sensor\n  to measure room occupancy", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2019.00319", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a room occupancy sensing solution with unique\nproperties: (i) It is based on an omnidirectional vision camera, capturing rich\nscene info over a wide angle, enabling to count the number of people in a room\nand even their position. (ii) Although it uses a camera-input, no privacy\nissues arise because its extremely low image resolution, rendering people\nunrecognisable. (iii) The neural network inference is running entirely on a\nlow-cost processing platform embedded in the sensor, reducing the privacy risk\neven further. (iv) Limited manual data annotation is needed, because of the\nself-training scheme we propose. Such a smart room occupancy rate sensor can be\nused in e.g. meeting rooms and flex-desks. Indeed, by encouraging flex-desking,\nthe required office space can be reduced significantly. In some cases, however,\na flex-desk that has been reserved remains unoccupied without an update in the\nreservation system. A similar problem occurs with meeting rooms, which are\noften under-occupied. By optimising the occupancy rate a huge reduction in\ncosts can be achieved. Therefore, in this paper, we develop such system which\ndetermines the number of people present in office flex-desks and meeting rooms.\nUsing an omnidirectional camera mounted in the ceiling, combined with a person\ndetector, the company can intelligently update the reservation system based on\nthe measured occupancy. Next to the optimisation and embedded implementation of\nsuch a self-training omnidirectional people detection algorithm, in this work\nwe propose a novel approach that combines spatial and temporal image data,\nimproving performance of our system on extreme low-resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:05:32 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Callemein", "Timothy", ""], ["Van Beeck", "Kristof", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2007.04940", "submitter": "Jingjing Shen", "authors": "Jingjing Shen, Thomas J. Cashman, Qi Ye, Tim Hutton, Toby Sharp,\n  Federica Bogo, Andrew William Fitzgibbon, Jamie Shotton", "title": "The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization", "comments": null, "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realtime perceptual and interaction capabilities in mixed reality require a\nrange of 3D tracking problems to be solved at low latency on\nresource-constrained hardware such as head-mounted devices. Indeed, for devices\nsuch as HoloLens 2 where the CPU and GPU are left available for applications,\nmultiple tracking subsystems are required to run on a continuous, real-time\nbasis while sharing a single Digital Signal Processor. To solve model-fitting\nproblems for HoloLens 2 hand tracking, where the computational budget is\napproximately 100 times smaller than an iPhone 7, we introduce a new surface\nmodel: the `Phong surface'. Using ideas from computer graphics, the Phong\nsurface describes the same 3D shape as a triangulated mesh model, but with\ncontinuous surface normals which enable the use of lifting-based optimization,\nproviding significant efficiency gains over ICP-based methods. We show that\nPhong surfaces retain the convergence benefits of smoother surface models,\nwhile triangle meshes do not.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:10:11 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Shen", "Jingjing", ""], ["Cashman", "Thomas J.", ""], ["Ye", "Qi", ""], ["Hutton", "Tim", ""], ["Sharp", "Toby", ""], ["Bogo", "Federica", ""], ["Fitzgibbon", "Andrew William", ""], ["Shotton", "Jamie", ""]]}, {"id": "2007.04942", "submitter": "Timothy Callemein", "authors": "Robin Schrijvers, Steven Puttemans, Timothy Callemein and Toon\n  Goedem\\'e", "title": "Real-time Embedded Person Detection and Tracking for Shopping Behaviour\n  Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-40605-9_46", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shopping behaviour analysis through counting and tracking of people in\nshop-like environments offers valuable information for store operators and\nprovides key insights in the stores layout (e.g. frequently visited spots).\nInstead of using extra staff for this, automated on-premise solutions are\npreferred. These automated systems should be cost-effective, preferably on\nlightweight embedded hardware, work in very challenging situations (e.g.\nhandling occlusions) and preferably work real-time. We solve this challenge by\nimplementing a real-time TensorRT optimized YOLOv3-based pedestrian detector,\non a Jetson TX2 hardware platform. By combining the detector with a sparse\noptical flow tracker we assign a unique ID to each customer and tackle the\nproblem of loosing partially occluded customers. Our detector-tracker based\nsolution achieves an average precision of 81.59% at a processing speed of 10\nFPS. Besides valuable statistics, heat maps of frequently visited spots are\nextracted and used as an overlay on the video stream.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:14:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Schrijvers", "Robin", ""], ["Puttemans", "Steven", ""], ["Callemein", "Timothy", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "2007.04950", "submitter": "Nitish Bhardwaj", "authors": "Alpana Dubey, Nitish Bhardwaj, Kumar Abhinav, Suma Mani Kuriakose,\n  Sakshi Jain and Veenu Arora", "title": "AI Assisted Apparel Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is a fast-changing industry where designs are refreshed at large\nscale every season. Moreover, it faces huge challenge of unsold inventory as\nnot all designs appeal to customers. This puts designers under significant\npressure. Firstly, they need to create innumerous fresh designs. Secondly, they\nneed to create designs that appeal to customers. Although we see advancements\nin approaches to help designers analyzing consumers, often such insights are\ntoo many. Creating all possible designs with those insights is time consuming.\nIn this paper, we propose a system of AI assistants that assists designers in\ntheir design journey. The proposed system assists designers in analyzing\ndifferent selling/trending attributes of apparels. We propose two design\ngeneration assistants namely Apparel-Style-Merge and Apparel-Style-Transfer.\nApparel-Style-Merge generates new designs by combining high level components of\napparels whereas Apparel-Style-Transfer generates multiple customization of\napparels by applying different styles, colors and patterns. We compose a new\ndataset, named DeepAttributeStyle, with fine-grained annotation of landmarks of\ndifferent apparel components such as neck, sleeve etc. The proposed system is\nevaluated on a user group consisting of people with and without design\nbackground. Our evaluation result demonstrates that our approach generates high\nquality designs that can be easily used in fabrication. Moreover, the suggested\ndesigns aid to the designers creativity.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:24:40 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 17:14:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Dubey", "Alpana", ""], ["Bhardwaj", "Nitish", ""], ["Abhinav", "Kumar", ""], ["Kuriakose", "Suma Mani", ""], ["Jain", "Sakshi", ""], ["Arora", "Veenu", ""]]}, {"id": "2007.04954", "submitter": "Chuang Gan", "authors": "Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer,\n  Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi\n  Sano, Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis,\n  Kevin Feigelis, Daniel M. Bear, Dan Gutfreund, David Cox, James J. DiCarlo,\n  Josh McDermott, Joshua B. Tenenbaum, Daniel L.K. Yamins", "title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation", "comments": "Project page: http://www.threedworld.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ThreeDWorld (TDW), a platform for interactive multi-modal\nphysical simulation. With TDW, users can simulate high-fidelity sensory data\nand physical interactions between mobile agents and objects in a wide variety\nof rich 3D environments. TDW has several unique properties: 1) realtime near\nphoto-realistic image rendering quality; 2) a library of objects and\nenvironments with materials for high-quality rendering, and routines enabling\nuser customization of the asset library; 3) generative procedures for\nefficiently building classes of new environments 4) high-fidelity audio\nrendering; 5) believable and realistic physical interactions for a wide variety\nof material types, including cloths, liquid, and deformable objects; 6) a range\nof \"avatar\" types that serve as embodiments of AI agents, with the option for\nuser avatar customization; and 7) support for human interactions with VR\ndevices. TDW also provides a rich API enabling multiple agents to interact\nwithin a simulation and return a range of sensor and physics data representing\nthe state of the world. We present initial experiments enabled by the platform\naround emerging research directions in computer vision, machine learning, and\ncognitive science, including multi-modal physical scene understanding,\nmulti-agent interactions, models that \"learn like a child\", and attention\nstudies in humans and neural networks. The simulation platform will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:33:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gan", "Chuang", ""], ["Schwartz", "Jeremy", ""], ["Alter", "Seth", ""], ["Schrimpf", "Martin", ""], ["Traer", "James", ""], ["De Freitas", "Julian", ""], ["Kubilius", "Jonas", ""], ["Bhandwaldar", "Abhishek", ""], ["Haber", "Nick", ""], ["Sano", "Megumi", ""], ["Kim", "Kuno", ""], ["Wang", "Elias", ""], ["Mrowca", "Damian", ""], ["Lingelbach", "Michael", ""], ["Curtis", "Aidan", ""], ["Feigelis", "Kevin", ""], ["Bear", "Daniel M.", ""], ["Gutfreund", "Dan", ""], ["Cox", "David", ""], ["DiCarlo", "James J.", ""], ["McDermott", "Josh", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "2007.04964", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay and Yedid Hoshen", "title": "Improving Style-Content Disentanglement in Image-to-Image Translation", "comments": "Project page:\n  http://www.vision.huji.ac.il/style-content-disentanglement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation methods have achieved tremendous\nsuccess in recent years. However, it can be easily observed that their models\ncontain significant entanglement which often hurts the translation performance.\nIn this work, we propose a principled approach for improving style-content\ndisentanglement in image-to-image translation. By considering the information\nflow into each of the representations, we introduce an additional loss term\nwhich serves as a content-bottleneck. We show that the results of our method\nare significantly more disentangled than those produced by current methods,\nwhile further improving the visual quality and translation diversity.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:51:56 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gabbay", "Aviv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2007.04976", "submitter": "Deepak Pathak", "authors": "Wenlong Huang, Igor Mordatch, Deepak Pathak", "title": "One Policy to Control Them All: Shared Modular Policies for\n  Agent-Agnostic Control", "comments": "Accepted at ICML 2020. Videos and code at\n  https://huangwl18.github.io/modular-rl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is typically concerned with learning control policies\ntailored to a particular agent. We investigate whether there exists a single\nglobal policy that can generalize to control a wide variety of agent\nmorphologies -- ones in which even dimensionality of state and action spaces\nchanges. We propose to express this global policy as a collection of identical\nmodular neural networks, dubbed as Shared Modular Policies (SMP), that\ncorrespond to each of the agent's actuators. Every module is only responsible\nfor controlling its corresponding actuator and receives information from only\nits local sensors. In addition, messages are passed between modules,\npropagating information between distant modules. We show that a single modular\npolicy can successfully generate locomotion behaviors for several planar agents\nwith different skeletal structures such as monopod hoppers, quadrupeds, bipeds,\nand generalize to variants not seen during training -- a process that would\nnormally require training and manual hyperparameter tuning for each morphology.\nWe observe that a wide variety of drastically diverse locomotion styles across\nmorphologies as well as centralized coordination emerges via message passing\nbetween decentralized modules purely from the reinforcement learning objective.\nVideos and code at https://huangwl18.github.io/modular-rl/\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:59:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Huang", "Wenlong", ""], ["Mordatch", "Igor", ""], ["Pathak", "Deepak", ""]]}, {"id": "2007.04978", "submitter": "Elsa Angelini", "authors": "Jie Yang, Elsa D. Angelini, Pallavi P. Balte, Eric A. Hoffman, John\n  H.M. Austin, Benjamin M. Smith, R. Graham Barr, and Andrew F. Laine", "title": "Novel Subtypes of Pulmonary Emphysema Based on Spatially-Informed Lung\n  Texture Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary emphysema overlaps considerably with chronic obstructive pulmonary\ndisease (COPD), and is traditionally subcategorized into three subtypes\npreviously identified on autopsy. Unsupervised learning of emphysema subtypes\non computed tomography (CT) opens the way to new definitions of emphysema\nsubtypes and eliminates the need of thorough manual labeling. However, CT-based\nemphysema subtypes have been limited to texture-based patterns without\nconsidering spatial location. In this work, we introduce a standardized spatial\nmapping of the lung for quantitative study of lung texture location, and\npropose a novel framework for combining spatial and texture information to\ndiscover spatially-informed lung texture patterns (sLTPs) that represent novel\nemphysema subtypes. Exploiting two cohorts of full-lung CT scans from the MESA\nCOPD and EMCAP studies, we first show that our spatial mapping enables\npopulation-wide study of emphysema spatial location. We then evaluate the\ncharacteristics of the sLTPs discovered on MESA COPD, and show that they are\nreproducible, able to encode standard emphysema subtypes, and associated with\nphysiological symptoms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:59:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yang", "Jie", ""], ["Angelini", "Elsa D.", ""], ["Balte", "Pallavi P.", ""], ["Hoffman", "Eric A.", ""], ["Austin", "John H. M.", ""], ["Smith", "Benjamin M.", ""], ["Barr", "R. Graham", ""], ["Laine", "Andrew F.", ""]]}, {"id": "2007.04979", "submitter": "Unnat Jain", "authors": "Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik,\n  Aniruddha Kembhavi, Alexander Schwing", "title": "A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied\n  Tasks", "comments": "Accepted to ECCV 2020 (spotlight); Project page:\n  https://unnat.github.io/cordial-sync", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents must learn to collaborate. It is not scalable to develop a\nnew centralized agent every time a task's difficulty outpaces a single agent's\nabilities. While multi-agent collaboration research has flourished in\ngridworld-like environments, relatively little work has considered visually\nrich domains. Addressing this, we introduce the novel task FurnMove in which\nagents work together to move a piece of furniture through a living room to a\ngoal. Unlike existing tasks, FurnMove requires agents to coordinate at every\ntimestep. We identify two challenges when training agents to complete FurnMove:\nexisting decentralized action sampling procedures do not permit expressive\njoint action policies and, in tasks requiring close coordination, the number of\nfailed actions dominates successful actions. To confront these challenges we\nintroduce SYNC-policies (synchronize your actions coherently) and CORDIAL\n(coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58%\ncompletion rate on FurnMove, an impressive absolute gain of 25 percentage\npoints over competitive decentralized baselines. Our dataset, code, and\npretrained models are available at https://unnat.github.io/cordial-sync .\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:59:57 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jain", "Unnat", ""], ["Weihs", "Luca", ""], ["Kolve", "Eric", ""], ["Farhadi", "Ali", ""], ["Lazebnik", "Svetlana", ""], ["Kembhavi", "Aniruddha", ""], ["Schwing", "Alexander", ""]]}, {"id": "2007.05008", "submitter": "Pietro Antonio Cicalese", "authors": "Pietro Antonio Cicalese, Aryan Mobiny, Pengyu Yuan, Jan Becker,\n  Chandra Mohan, Hien Van Nguyen", "title": "StyPath: Style-Transfer Data Augmentation For Robust Histology Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of Antibody Mediated Rejection (AMR) in kidney transplant\nremains challenging even for experienced nephropathologists; this is partly\nbecause histological tissue stain analysis is often characterized by low\ninter-observer agreement and poor reproducibility. One of the implicated causes\nfor inter-observer disagreement is the variability of tissue stain quality\nbetween (and within) pathology labs, coupled with the gradual fading of\narchival sections. Variations in stain colors and intensities can make tissue\nevaluation difficult for pathologists, ultimately affecting their ability to\ndescribe relevant morphological features. Being able to accurately predict the\nAMR status based on kidney histology images is crucial for improving patient\ntreatment and care. We propose a novel pipeline to build robust deep neural\nnetworks for AMR classification based on StyPath, a histological data\naugmentation technique that leverages a light weight style-transfer algorithm\nas a means to reduce sample-specific bias. Each image was generated in 1.84 +-\n0.03 seconds using a single GTX TITAN V gpu and pytorch, making it faster than\nother popular histological data augmentation techniques. We evaluated our model\nusing a Monte Carlo (MC) estimate of Bayesian performance and generate an\nepistemic measure of uncertainty to compare both the baseline and StyPath\naugmented models. We also generated Grad-CAM representations of the results\nwhich were assessed by an experienced nephropathologist; we used this\nqualitative analysis to elucidate on the assumptions being made by each model.\nOur results imply that our style-transfer augmentation technique improves\nhistological classification performance (reducing error from 14.8% to 11.5%)\nand generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:02:49 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Cicalese", "Pietro Antonio", ""], ["Mobiny", "Aryan", ""], ["Yuan", "Pengyu", ""], ["Becker", "Jan", ""], ["Mohan", "Chandra", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "2007.05009", "submitter": "Pengyu Yuan", "authors": "Pengyu Yuan, Aryan Mobiny, Jahandar Jahanipour, Xiaoyang Li, Pietro\n  Antonio Cicalese, Badrinath Roysam, Vishal Patel, Maric Dragan, and Hien Van\n  Nguyen", "title": "Few Is Enough: Task-Augmented Active Meta-Learning for Brain Cell\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (or DNNs) must constantly cope with distribution changes\nin the input data when the task of interest or the data collection protocol\nchanges. Retraining a network from scratch to combat this issue poses a\nsignificant cost. Meta-learning aims to deliver an adaptive model that is\nsensitive to these underlying distribution changes, but requires many tasks\nduring the meta-training process. In this paper, we propose a tAsk-auGmented\nactIve meta-LEarning (AGILE) method to efficiently adapt DNNs to new tasks by\nusing a small number of training examples. AGILE combines a meta-learning\nalgorithm with a novel task augmentation technique which we use to generate an\ninitial adaptive model. It then uses Bayesian dropout uncertainty estimates to\nactively select the most difficult samples when updating the model to a new\ntask. This allows AGILE to learn with fewer tasks and a few informative\nsamples, achieving high performance with a limited dataset. We perform our\nexperiments using the brain cell classification task and compare the results to\na plain meta-learning model trained from scratch. We show that the proposed\ntask-augmented meta-learning framework can learn to classify new cell types\nafter a single gradient step with a limited number of training samples. We show\nthat active learning with Bayesian uncertainty can further improve the\nperformance when the number of training samples is extremely small. Using only\n1% of the training data and a single update step, we achieved 90% accuracy on\nthe new cell type classification task, a 50% points improvement over a\nstate-of-the-art meta-learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:03:12 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Yuan", "Pengyu", ""], ["Mobiny", "Aryan", ""], ["Jahanipour", "Jahandar", ""], ["Li", "Xiaoyang", ""], ["Cicalese", "Pietro Antonio", ""], ["Roysam", "Badrinath", ""], ["Patel", "Vishal", ""], ["Dragan", "Maric", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "2007.05028", "submitter": "Li Wang", "authors": "Li Wang and Ren-Cang Li and Wen-Wei", "title": "Multi-view Orthonormalized Partial Least Squares: Regularizations and\n  Deep Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a family of subspace-based learning method for multi-view\nlearning using the least squares as the fundamental basis. Specifically, we\ninvestigate orthonormalized partial least squares (OPLS) and study its\nimportant properties for both multivariate regression and classification.\nBuilding on the least squares reformulation of OPLS, we propose a unified\nmulti-view learning framework to learn a classifier over a common latent space\nshared by all views. The regularization technique is further leveraged to\nunleash the power of the proposed framework by providing three generic types of\nregularizers on its inherent ingredients including model parameters, decision\nvalues and latent projected points. We instantiate a set of regularizers in\nterms of various priors. The proposed framework with proper choices of\nregularizers not only can recast existing methods, but also inspire new models.\nTo further improve the performance of the proposed framework on complex real\nproblems, we propose to learn nonlinear transformations parameterized by deep\nnetworks. Extensive experiments are conducted to compare various methods on\nnine data sets with different numbers of views in terms of both feature\nextraction and cross-modal retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:00:39 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wang", "Li", ""], ["Li", "Ren-Cang", ""], ["Wen-Wei", "", ""]]}, {"id": "2007.05033", "submitter": "Adarsh K. Jeewajee", "authors": "Adarsh K. Jeewajee, Leslie P. Kaelbling", "title": "Adversarially-learned Inference via an Ensemble of Discrete Undirected\n  Graphical Models", "comments": "17 pages, 5 figures, 5 tables. NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are compact representations of joint probability\ndistributions over random variables. To solve inference tasks of interest,\ngraphical models of arbitrary topology can be trained using empirical risk\nminimization. However, to solve inference tasks that were not seen during\ntraining, these models (EGMs) often need to be re-trained. Instead, we propose\nan inference-agnostic adversarial training framework which produces an\ninfinitely-large ensemble of graphical models (AGMs). The ensemble is optimized\nto generate data within the GAN framework, and inference is performed using a\nfinite subset of these models. AGMs perform comparably with EGMs on inference\ntasks that the latter were specifically optimized for. Most importantly, AGMs\nshow significantly better generalization to unseen inference tasks compared to\nEGMs, as well as deep neural architectures like GibbsNet and VAEAC which allow\narbitrary conditioning. Finally, AGMs allow fast data sampling, competitive\nwith Gibbs sampling from EGMs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:13:36 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 22:51:27 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 05:05:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Jeewajee", "Adarsh K.", ""], ["Kaelbling", "Leslie P.", ""]]}, {"id": "2007.05038", "submitter": "Dr. Mohammed Javed", "authors": "Mohammed Javed and MD Meraz and Pavan Chakraborty", "title": "A Quick Review on Recent Trends in 3D Point Cloud Data Compression\n  Techniques and the Challenges of Direct Processing in 3D Compressed Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic processing of 3D Point Cloud data for object detection, tracking\nand segmentation is the latest trending research in the field of AI and Data\nScience, which is specifically aimed at solving different challenges of\nautonomous driving cars and getting real time performance. However, the amount\nof data that is being produced in the form of 3D point cloud (with LiDAR) is\nvery huge, due to which the researchers are now on the way inventing new data\ncompression algorithms to handle huge volumes of data thus generated. However,\ncompression on one hand has an advantage in overcoming space requirements, but\non the other hand, its processing gets expensive due to the decompression,\nwhich indents additional computing resources. Therefore, it would be novel to\nthink of developing algorithms that can operate/analyse directly with the\ncompressed data without involving the stages of decompression and recompression\n(required as many times, the compressed data needs to be operated or analyzed).\nThis research field is termed as Compressed Domain Processing. In this paper,\nwe will quickly review few of the recent state-of-the-art developments in the\narea of LiDAR generated 3D point cloud data compression, and highlight the\nfuture challenges of compressed domain processing of 3D point cloud data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:56:58 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Javed", "Mohammed", ""], ["Meraz", "MD", ""], ["Chakraborty", "Pavan", ""]]}, {"id": "2007.05056", "submitter": "Ali Reza Feizi Derakhshi", "authors": "Aidin Zehtab-Salmasi, Ali-Reza Feizi-Derakhshi, Narjes\n  Nikzad-Khasmakhi, Meysam Asgari-Chenaghlu, Saeideh Nabipour", "title": "Multimodal price prediction", "comments": "This is a preprint of an article published in \"Annals of Data\n  Science\". The final authenticated version is available online at:\n  https://link.springer.com/article/10.1007/s40745-021-00326-z", "journal-ref": null, "doi": "10.1007/s40745-021-00326-z", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Price prediction is one of the examples related to forecasting tasks and is a\nproject based on data science. Price prediction analyzes data and predicts the\ncost of new products. The goal of this research is to achieve an arrangement to\npredict the price of a cellphone based on its specifications. So, five deep\nlearning models are proposed to predict the price range of a cellphone, one\nunimodal and four multimodal approaches. The multimodal methods predict the\nprices based on the graphical and non-graphical features of cellphones that\nhave an important effect on their valorizations. Also, to evaluate the\nefficiency of the proposed methods, a cellphone dataset has been gathered from\nGSMArena. The experimental results show 88.3% F1-score, which confirms that\nmultimodal learning leads to more accurate predictions than state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 20:46:13 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 04:53:23 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 11:57:24 GMT"}, {"version": "v4", "created": "Fri, 2 Apr 2021 05:19:39 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zehtab-Salmasi", "Aidin", ""], ["Feizi-Derakhshi", "Ali-Reza", ""], ["Nikzad-Khasmakhi", "Narjes", ""], ["Asgari-Chenaghlu", "Meysam", ""], ["Nabipour", "Saeideh", ""]]}, {"id": "2007.05059", "submitter": "Taylor Webb", "authors": "Taylor W. Webb, Zachary Dulberg, Steven M. Frankland, Alexander A.\n  Petrov, Randall C. O'Reilly, Jonathan D. Cohen", "title": "Learning Representations that Support Extrapolation", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrapolation -- the ability to make inferences that go beyond the scope of\none's experiences -- is a hallmark of human intelligence. By contrast, the\ngeneralization exhibited by contemporary neural network algorithms is largely\nlimited to interpolation between data points in their training corpora. In this\npaper, we consider the challenge of learning representations that support\nextrapolation. We introduce a novel visual analogy benchmark that allows the\ngraded evaluation of extrapolation as a function of distance from the convex\ndomain defined by the training data. We also introduce a simple technique,\ntemporal context normalization, that encourages representations that emphasize\nthe relations between objects. We find that this technique enables a\nsignificant improvement in the ability to extrapolate, considerably\noutperforming a number of competitive techniques.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 20:53:45 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 22:36:46 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Webb", "Taylor W.", ""], ["Dulberg", "Zachary", ""], ["Frankland", "Steven M.", ""], ["Petrov", "Alexander A.", ""], ["O'Reilly", "Randall C.", ""], ["Cohen", "Jonathan D.", ""]]}, {"id": "2007.05079", "submitter": "Sanaz Aliari", "authors": "Sanaz Aliari, Kaveh F. Sadabadi", "title": "Automatic Detection of Major Freeway Congestion Events Using Wireless\n  Traffic Sensor Data: A Machine Learning Approach", "comments": "12 pages, 3 figures", "journal-ref": "Transportation Research Record 2673.7 (2019): 436-442", "doi": "10.1177/0361198119843859", "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the dynamics of traffic in major corridors can provide invaluable\ninsight for traffic planning purposes. An important requirement for this\nmonitoring is the availability of methods to automatically detect major traffic\nevents and to annotate the abundance of travel data. This paper introduces a\nmachine learning based approach for reliable detection and characterization of\nhighway traffic congestion events from hundreds of hours of traffic speed data.\nIndeed, the proposed approach is a generic approach for detection of changes in\nany given time series, which is the wireless traffic sensor data in the present\nstudy. The speed data is initially time-windowed by a ten-hour long sliding\nwindow and fed into three Neural Networks that are used to detect the existence\nand duration of congestion events (slowdowns) in each window. The sliding\nwindow captures each slowdown event multiple times and results in increased\nconfidence in congestion detection. The training and parameter tuning are\nperformed on 17,483 hours of data that includes 168 slowdown events. This data\nis collected and labeled as part of the ongoing probe data validation studies\nat the Center for Advanced Transportation Technologies (CATT) at the University\nof Maryland. The Neural networks are carefully trained to reduce the chances of\nover-fitting to the training data. The experimental results show that this\napproach is able to successfully detect most of the congestion events, while\nsignificantly outperforming a heuristic rule-based approach. Moreover, the\nproposed approach is shown to be more accurate in estimation of the start-time\nand end-time of the congestion events.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:38:45 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Aliari", "Sanaz", ""], ["Sadabadi", "Kaveh F.", ""]]}, {"id": "2007.05080", "submitter": "Furkan K{\\i}nl{\\i}", "authors": "Furkan K{\\i}nl{\\i}, Bar{\\i}\\c{s} \\\"Ozcan, Furkan K{\\i}ra\\c{c}", "title": "A Benchmark for Inpainting of Clothing Images with Irregular Holes", "comments": "Accepted to AIM2020: Advanced Image Manipulation workshop and\n  challenges at ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion image understanding is an active research field with a large number\nof practical applications for the industry. Despite its practical impacts on\nintelligent fashion analysis systems, clothing image inpainting has not been\nextensively examined yet. For that matter, we present an extensive benchmark of\nclothing image inpainting on well-known fashion datasets. Furthermore, we\nintroduce the use of a dilated version of partial convolutions, which\nefficiently derive the mask update step, and empirically show that the proposed\nmethod reduces the required number of layers to form fully-transparent masks.\nExperiments show that dilated partial convolutions (DPConv) improve the\nquantitative inpainting performance when compared to the other inpainting\nstrategies, especially it performs better when the mask size is 20% or more of\nthe image. \\keywords{image inpainting, fashion image understanding, dilated\nconvolutions, partial convolutions\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:44:08 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 12:21:54 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 17:43:38 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["K\u0131nl\u0131", "Furkan", ""], ["\u00d6zcan", "Bar\u0131\u015f", ""], ["K\u0131ra\u00e7", "Furkan", ""]]}, {"id": "2007.05099", "submitter": "Xu Ma", "authors": "Xu Ma, Jingda Guo, Sihai Tang, Zhinan Qiao, Qi Chen, Qing Yang, Song\n  Fu", "title": "DCANet: Learning Connected Attentions for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While self-attention mechanism has shown promising results for many vision\ntasks, it only considers the current features at a time. We show that such a\nmanner cannot take full advantage of the attention mechanism. In this paper, we\npresent Deep Connected Attention Network (DCANet), a novel design that boosts\nattention modules in a CNN model without any modification of the internal\nstructure. To achieve this, we interconnect adjacent attention blocks, making\ninformation flow among attention blocks possible. With DCANet, all attention\nblocks in a CNN model are trained jointly, which improves the ability of\nattention learning. Our DCANet is generic. It is not limited to a specific\nattention module or base network architecture. Experimental results on ImageNet\nand MS COCO benchmarks show that DCANet consistently outperforms the\nstate-of-the-art attention modules with a minimal additional computational\noverhead in all test cases. All code and models are made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 22:37:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Ma", "Xu", ""], ["Guo", "Jingda", ""], ["Tang", "Sihai", ""], ["Qiao", "Zhinan", ""], ["Chen", "Qi", ""], ["Yang", "Qing", ""], ["Fu", "Song", ""]]}, {"id": "2007.05103", "submitter": "Dmitry V. Dylov", "authors": "Elizaveta Lazareva, Oleg Rogov, Olga Shegai, Denis Larionov, Dmitry V.\n  Dylov", "title": "LORCK: Learnable Object-Resembling Convolution Kernels", "comments": "18 pages total. Main: 12 figures and 3 tables (main and\n  supplemental). D.V.D is corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of certain hollow organs, such as the bladder, is especially\nhard to automate due to their complex geometry, vague intensity gradients in\nthe soft tissues, and a tedious manual process of the data annotation routine.\nYet, accurate localization of the walls and the cancer regions in the\nradiologic images of such organs is an essential step in oncology. To address\nthis issue, we propose a new class of hollow kernels that learn to 'mimic' the\ncontours of the segmented organ, effectively replicating its shape and\nstructural complexity. We train a series of the U-Net-like neural networks\nusing the proposed kernels and demonstrate the superiority of the idea in\nvarious spatio-temporal convolution scenarios. Specifically, the dilated\nhollow-kernel architecture outperforms state-of-the-art spatial segmentation\nmodels, whereas the addition of temporal blocks with, e.g., Bi-LSTM,\nestablishes a new multi-class baseline for the bladder segmentation challenge.\nOur spatio-temporal model based on the hollow kernels reaches the mean dice\nscores of 0.936, 0.736, and 0.712 for the bladder's inner wall, the outer wall,\nand the tumor regions, respectively. The results pave the way towards other\ndomain-specific deep learning applications where the shape of the segmented\nobject could be used to form a proper convolution kernel for boosting the\nsegmentation outcome.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 23:17:40 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 12:51:31 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lazareva", "Elizaveta", ""], ["Rogov", "Oleg", ""], ["Shegai", "Olga", ""], ["Larionov", "Denis", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2007.05104", "submitter": "Yan Luo", "authors": "Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, and Qi Zhao", "title": "$n$-Reference Transfer Learning for Saliency Prediction", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from deep learning research and large-scale datasets, saliency\nprediction has achieved significant success in the past decade. However, it\nstill remains challenging to predict saliency maps on images in new domains\nthat lack sufficient data for data-hungry models. To solve this problem, we\npropose a few-shot transfer learning paradigm for saliency prediction, which\nenables efficient transfer of knowledge learned from the existing large-scale\nsaliency datasets to a target domain with limited labeled examples.\nSpecifically, very few target domain examples are used as the reference to\ntrain a model with a source domain dataset such that the training process can\nconverge to a local minimum in favor of the target domain. Then, the learned\nmodel is further fine-tuned with the reference. The proposed framework is\ngradient-based and model-agnostic. We conduct comprehensive experiments and\nablation study on various source domain and target domain pairs. The results\nshow that the proposed framework achieves a significant performance\nimprovement. The code is publicly available at\n\\url{https://github.com/luoyan407/n-reference}.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 23:20:44 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Luo", "Yan", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan S.", ""], ["Zhao", "Qi", ""]]}, {"id": "2007.05113", "submitter": "Xugong Qin", "authors": "Xugong Qin, Yu Zhou, Dayan Wu, Yinliang Yue, Weiping Wang", "title": "FC2RN: A Fully Convolutional Corner Refinement Network for Accurate\n  Multi-Oriented Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent scene text detection works mainly focus on curve text detection.\nHowever, in real applications, the curve texts are more scarce than the\nmulti-oriented ones. Accurate detection of multi-oriented text with large\nvariations of scales, orientations, and aspect ratios is of great significance.\nAmong the multi-oriented detection methods, direct regression for the geometry\nof scene text shares a simple yet powerful pipeline and gets popular in\nacademic and industrial communities, but it may produce imperfect detections,\nespecially for long texts due to the limitation of the receptive field. In this\nwork, we aim to improve this while keeping the pipeline simple. A fully\nconvolutional corner refinement network (FC2RN) is proposed for accurate\nmulti-oriented text detection, in which an initial corner prediction and a\nrefined corner prediction are obtained at one pass. With a novel quadrilateral\nRoI convolution operation tailed for multi-oriented scene text, the initial\nquadrilateral prediction is encoded into the feature maps which can be further\nused to predict offset between the initial prediction and the ground-truth as\nwell as output a refined confidence score. Experimental results on four public\ndatasets including MSRA-TD500, ICDAR2017-RCTW, ICDAR2015, and COCO-Text\ndemonstrate that FC2RN can outperform the state-of-the-art methods. The\nablation study shows the effectiveness of corner refinement and scoring for\naccurate text localization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:04:24 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Qin", "Xugong", ""], ["Zhou", "Yu", ""], ["Wu", "Dayan", ""], ["Yue", "Yinliang", ""], ["Wang", "Weiping", ""]]}, {"id": "2007.05123", "submitter": "Tuan Anh Bui", "authors": "Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas\n  Abraham, Dinh Phung", "title": "Improving Adversarial Robustness by Enforcing Local and Global\n  Compactness", "comments": "Proceeding of the European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fact that deep neural networks are susceptible to crafted perturbations\nseverely impacts the use of deep learning in certain domains of application.\nAmong many developed defense models against such attacks, adversarial training\nemerges as the most successful method that consistently resists a wide range of\nattacks. In this work, based on an observation from a previous study that the\nrepresentations of a clean data example and its adversarial examples become\nmore divergent in higher layers of a deep neural net, we propose the Adversary\nDivergence Reduction Network which enforces local/global compactness and the\nclustering assumption over an intermediate layer of a deep neural network. We\nconduct comprehensive experiments to understand the isolating behavior of each\ncomponent (i.e., local/global compactness and the clustering assumption) and\ncompare our proposed model with state-of-the-art adversarial training methods.\nThe experimental results demonstrate that augmenting adversarial training with\nour proposed components can further improve the robustness of the network,\nleading to higher unperturbed and adversarial predictive performances.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:43:06 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bui", "Anh", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Montague", "Paul", ""], ["deVel", "Olivier", ""], ["Abraham", "Tamas", ""], ["Phung", "Dinh", ""]]}, {"id": "2007.05146", "submitter": "Xinghao Chen", "authors": "Xinghao Chen, Yiman Zhang, Yunhe Wang, Han Shu, Chunjing Xu, Chang Xu", "title": "Optical Flow Distillation: Towards Efficient and Stable Video Style\n  Transfer", "comments": null, "journal-ref": "ECCV 2020", "doi": "10.1007/978-3-030-58539-6_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video style transfer techniques inspire many exciting applications on mobile\ndevices. However, their efficiency and stability are still far from\nsatisfactory. To boost the transfer stability across frames, optical flow is\nwidely adopted, despite its high computational complexity, e.g. occupying over\n97% inference time. This paper proposes to learn a lightweight video style\ntransfer network via knowledge distillation paradigm. We adopt two teacher\nnetworks, one of which takes optical flow during inference while the other does\nnot. The output difference between these two teacher networks highlights the\nimprovements made by optical flow, which is then adopted to distill the target\nstudent network. Furthermore, a low-rank distillation loss is employed to\nstabilize the output of student network by mimicking the rank of input videos.\nExtensive experiments demonstrate that our student network without an optical\nflow module is still able to generate stable video and runs much faster than\nthe teacher network.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 03:00:33 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 07:24:29 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chen", "Xinghao", ""], ["Zhang", "Yiman", ""], ["Wang", "Yunhe", ""], ["Shu", "Han", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "2007.05149", "submitter": "Yijun Zhao", "authors": "Yijun Zhao, Jacek Ossowski, Xuming Wang, Shangjin Li, Orrin Devinsky,\n  Samantha P. Martin, and Heath R. Pardoe", "title": "Localized Motion Artifact Reduction on Brain MRI Using Deep Learning\n  with Effective Data Augmentation Techniques", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-scanner motion degrades the quality of magnetic resonance imaging (MRI)\nthereby reducing its utility in the detection of clinically relevant\nabnormalities. We introduce a deep learning-based MRI artifact reduction model\n(DMAR) to localize and correct head motion artifacts in brain MRI scans. Our\napproach integrates the latest advances in object detection and noise reduction\nin Computer Vision. Specifically, DMAR employs a two-stage approach: in the\nfirst, degraded regions are detected using the Single Shot Multibox Detector\n(SSD), and in the second, the artifacts within the found regions are reduced\nusing a convolutional autoencoder (CAE). We further introduce a set of novel\ndata augmentation techniques to address the high dimensionality of MRI images\nand the scarcity of available data. As a result, our model was trained on a\nlarge synthetic dataset of 225,000 images generated from 375 whole brain\nT1-weighted MRI scans. DMAR visibly reduces image artifacts when applied to\nboth synthetic test images and 55 real-world motion-affected slices from 18\nsubjects from the multi-center Autism Brain Imaging Data Exchange (ABIDE)\nstudy. Quantitatively, depending on the level of degradation, our model\nachieves a 27.8%-48.1% reduction in RMSE and a 2.88--5.79 dB gain in PSNR on a\n5000-sample set of synthetic images. For real-world artifact-affected scans\nfrom ABIDE, our model reduced the variance of image voxel intensity within\nartifact-affected brain regions (p = 0.014).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 03:30:27 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 18:28:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhao", "Yijun", ""], ["Ossowski", "Jacek", ""], ["Wang", "Xuming", ""], ["Li", "Shangjin", ""], ["Devinsky", "Orrin", ""], ["Martin", "Samantha P.", ""], ["Pardoe", "Heath R.", ""]]}, {"id": "2007.05166", "submitter": "Ifigeneia Apostolopoulou Ms", "authors": "Ifigeneia Apostolopoulou, Elan Rosenfeld, Artur Dubrawski", "title": "Self-Reflective Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Variational Autoencoder (VAE) is a powerful framework for learning\nprobabilistic latent variable generative models. However, typical assumptions\non the approximate posterior distribution of the encoder and/or the prior,\nseriously restrict its capacity for inference and generative modeling.\nVariational inference based on neural autoregressive models respects the\nconditional dependencies of the exact posterior, but this flexibility comes at\na cost: such models are expensive to train in high-dimensional regimes and can\nbe slow to produce samples. In this work, we introduce an orthogonal solution,\nwhich we call self-reflective inference. By redesigning the hierarchical\nstructure of existing VAE architectures, self-reflection ensures that the\nstochastic flow preserves the factorization of the exact posterior,\nsequentially updating the latent codes in a recurrent manner consistent with\nthe generative model. We empirically demonstrate the clear advantages of\nmatching the variational posterior to the exact posterior - on binarized MNIST,\nself-reflective inference achieves state-of-the art performance without\nresorting to complex, computationally expensive components such as\nautoregressive layers. Moreover, we design a variational normalizing flow that\nemploys the proposed architecture, yielding predictive benefits compared to its\npurely generative counterpart. Our proposed modification is quite general and\ncomplements the existing literature; self-reflective inference can naturally\nleverage advances in distribution estimation and generative modeling to improve\nthe capacity of each layer in the hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:05:26 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Apostolopoulou", "Ifigeneia", ""], ["Rosenfeld", "Elan", ""], ["Dubrawski", "Artur", ""]]}, {"id": "2007.05167", "submitter": "Manoranjan Paul PhD", "authors": "Muhammad Rafiqul Islam, Manoranjan Paul", "title": "Rain Streak Removal in a Video to Improve Visibility by TAWL Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision applications, the visibility of the video content is\ncrucial to perform analysis for better accuracy. The visibility can be affected\nby several atmospheric interferences in challenging weather-one of them is the\nappearance of rain streak. In recent time, rain streak removal achieves lots of\ninterest to the researchers as it has some exciting applications such as\nautonomous car, intelligent traffic monitoring system, multimedia, etc. In this\npaper, we propose a novel and simple method by combining three novel extracted\nfeatures focusing on temporal appearance, wide shape and relative location of\nthe rain streak and we called it TAWL (Temporal Appearance, Width, and\nLocation) method. The proposed TAWL method adaptively uses features from\ndifferent resolutions and frame rates. Moreover, it progressively processes\nfeatures from the up-coming frames so that it can remove rain in the real-time.\nThe experiments have been conducted using video sequences with both real rains\nand synthetic rains to compare the performance of the proposed method against\nthe relevant state-of-the-art methods. The experimental results demonstrate\nthat the proposed method outperforms the state-of-the-art methods by removing\nmore rain streaks while keeping other moving regions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:07:59 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Islam", "Muhammad Rafiqul", ""], ["Paul", "Manoranjan", ""]]}, {"id": "2007.05168", "submitter": "John Yang", "authors": "John Yang, Hyung Jin Chang, Seungeui Lee, Nojun Kwak", "title": "SeqHAND:RGB-Sequence-Based 3D Hand Pose and Shape Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose estimation based on RGB images has been studied for a long time.\nMost of the studies, however, have performed frame-by-frame estimation based on\nindependent static images. In this paper, we attempt to not only consider the\nappearance of a hand but incorporate the temporal movement information of a\nhand in motion into the learning framework for better 3D hand pose estimation\nperformance, which leads to the necessity of a large scale dataset with\nsequential RGB hand images. We propose a novel method that generates a\nsynthetic dataset that mimics natural human hand movements by re-engineering\nannotations of an extant static hand pose dataset into pose-flows. With the\ngenerated dataset, we train a newly proposed recurrent framework, exploiting\nvisuo-temporal features from sequential images of synthetic hands in motion and\nemphasizing temporal smoothness of estimations with a temporal consistency\nconstraint. Our novel training strategy of detaching the recurrent layer of the\nframework during domain finetuning from synthetic to real allows preservation\nof the visuo-temporal features learned from sequential synthetic hand images.\nHand poses that are sequentially estimated consequently produce natural and\nsmooth hand movements which lead to more robust estimations. We show that\nutilizing temporal information for 3D hand pose estimation significantly\nenhances general pose estimations by outperforming state-of-the-art methods in\nexperiments on hand pose estimation benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:11:14 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Yang", "John", ""], ["Chang", "Hyung Jin", ""], ["Lee", "Seungeui", ""], ["Kwak", "Nojun", ""]]}, {"id": "2007.05175", "submitter": "He-Feng Yin", "authors": "He-Feng Yin, Xiao-Jun Wu, Zhen-Hua Feng and Josef Kittler", "title": "Affine Non-negative Collaborative Representation Based Pattern\n  Classification", "comments": "submitted to the 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, representation-based classification methods have\nreceived considerable attention in pattern recognition. In particular, the\nrecently proposed non-negative representation based classification (NRC) method\nhas been reported to achieve promising results in a wide range of\nclassification tasks. However, NRC has two major drawbacks. First, there is no\nregularization term in the formulation of NRC, which may result in unstable\nsolution and misclassification. Second, NRC ignores the fact that data usually\nlies in a union of multiple affine subspaces, rather than linear subspaces in\npractical applications. To address the above issues, this paper presents an\naffine non-negative collaborative representation (ANCR) model for pattern\nclassification. To be more specific, ANCR imposes a regularization term on the\ncoding vector. Moreover, ANCR introduces an affine constraint to better\nrepresent the data from affine subspaces. The experimental results on several\nbenchmarking datasets demonstrate the merits of the proposed ANCR method. The\nsource code of our ANCR is publicly available at\nhttps://github.com/yinhefeng/ANCR.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:48:54 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Yin", "He-Feng", ""], ["Wu", "Xiao-Jun", ""], ["Feng", "Zhen-Hua", ""], ["Kittler", "Josef", ""]]}, {"id": "2007.05197", "submitter": "Manoranjan Paul PhD", "authors": "Manoranjan Paul, Sourabhi Debnath, Tanmoy Debnath, Suzy Rogiers, Tintu\n  Baby, DM Motiur Rahaman, Lihong Zheng, Leigh Schmidtke", "title": "Hyperspectral Imaging to detect Age, Defects and Individual Nutrient\n  Deficiency in Grapevine Leaves", "comments": "24 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral (HS) imaging was successfully employed in the 380 nm to 1000 nm\nwavelength range to investigate the efficacy of detecting age, healthiness and\nindividual nutrient deficiency of grapevine leaves collected from vineyards\nlocated in central west NSW, Australia. For age detection, the appearance of\nmany healthy grapevine leaves has been examined. Then visually defective leaves\nwere compared with healthy leaves. Control leaves and individual\nnutrient-deficient leaves (e.g. N, K and Mg) were also analysed. Several\nfeatures were employed at various stages in the Ultraviolet (UV), Visible (VIS)\nand Near Infrared (NIR) regions to evaluate the experimental data: mean\nbrightness, mean 1st derivative brightness, variation index, mean spectral\nratio, normalised difference vegetation index (NDVI) and standard deviation\n(SD). Experiment results demonstrate that these features could be utilised with\na high degree of effectiveness to compare age, identify unhealthy samples and\nnot only to distinguish from control and nutrient deficiency but also to\nidentify individual nutrient defects. Therefore, our work corroborated that HS\nimaging has excellent potential as a non-destructive as well as a non-contact\nmethod to detect age, healthiness and individual nutrient deficiencies of\ngrapevine leaves\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:42:24 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Paul", "Manoranjan", ""], ["Debnath", "Sourabhi", ""], ["Debnath", "Tanmoy", ""], ["Rogiers", "Suzy", ""], ["Baby", "Tintu", ""], ["Rahaman", "DM Motiur", ""], ["Zheng", "Lihong", ""], ["Schmidtke", "Leigh", ""]]}, {"id": "2007.05201", "submitter": "Huaying Hao", "authors": "Yuhui Ma and Huaying Hao and Huazhu Fu and Jiong Zhang and Jianlong\n  Yang and Jiang Liu and Yalin Zheng and Yitian Zhao", "title": "ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New\n  Model", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography Angiography (OCT-A) is a non-invasive imaging\ntechnique, and has been increasingly used to image the retinal vasculature at\ncapillary level resolution. However, automated segmentation of retinal vessels\nin OCT-A has been under-studied due to various challenges such as low capillary\nvisibility and high vessel complexity, despite its significance in\nunderstanding many eye-related diseases. In addition, there is no publicly\navailable OCT-A dataset with manually graded vessels for training and\nvalidation. To address these issues, for the first time in the field of retinal\nimage analysis we construct a dedicated Retinal OCT-A SEgmentation dataset\n(ROSE), which consists of 229 OCT-A images with vessel annotations at either\ncenterline-level or pixel level. This dataset has been released for public\naccess to assist researchers in the community in undertaking research in\nrelated topics. Secondly, we propose a novel Split-based Coarse-to-Fine vessel\nsegmentation network (SCF-Net), with the ability to detect thick and thin\nvessels separately. In the SCF-Net, a split-based coarse segmentation (SCS)\nmodule is first introduced to produce a preliminary confidence map of vessels,\nand a split-based refinement (SRN) module is then used to optimize the\nshape/contour of the retinal microvasculature. Thirdly, we perform a thorough\nevaluation of the state-of-the-art vessel segmentation models and our SCF-Net\non the proposed ROSE dataset. The experimental results demonstrate that our\nSCF-Net yields better vessel segmentation performance in OCT-A than both\ntraditional methods and other deep learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:54:19 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 07:45:51 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ma", "Yuhui", ""], ["Hao", "Huaying", ""], ["Fu", "Huazhu", ""], ["Zhang", "Jiong", ""], ["Yang", "Jianlong", ""], ["Liu", "Jiang", ""], ["Zheng", "Yalin", ""], ["Zhao", "Yitian", ""]]}, {"id": "2007.05205", "submitter": "Jong Chul Ye", "authors": "Jaeyoung Huh, Shujaat Khan, and Jong Chul Ye", "title": "OT-driven Multi-Domain Unsupervised Ultrasound Image Artifact Removal\n  using a Single CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging (US) often suffers from distinct image artifacts from\nvarious sources. Classic approaches for solving these problems are usually\nmodel-based iterative approaches that have been developed specifically for each\ntype of artifact, which are often computationally intensive. Recently, deep\nlearning approaches have been proposed as computationally efficient and high\nperformance alternatives. Unfortunately, in the current deep learning\napproaches, a dedicated neural network should be trained with matched training\ndata for each specific artifact type. This poses a fundamental limitation in\nthe practical use of deep learning for US, since large number of models should\nbe stored to deal with various US image artifacts. Inspired by the recent\nsuccess of multi-domain image transfer, here we propose a novel, unsupervised,\ndeep learning approach in which a single neural network can be used to deal\nwith different types of US artifacts simply by changing a mask vector that\nswitches between different target domains. Our algorithm is rigorously derived\nusing an optimal transport (OT) theory for cascaded probability measures.\nExperimental results using phantom and in vivo data demonstrate that the\nproposed method can generate high quality image by removing distinct artifacts,\nwhich are comparable to those obtained by separately trained multiple neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:11:04 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Huh", "Jaeyoung", ""], ["Khan", "Shujaat", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2007.05220", "submitter": "Chen Liu", "authors": "Chen Liu, Jiaqi Fan, Guosheng Yin", "title": "Efficient Unpaired Image Dehazing with Cyclic Perceptual-Depth\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing without paired haze-free images is of immense importance, as\nacquiring paired images often entails significant cost. However, we observe\nthat previous unpaired image dehazing approaches tend to suffer from\nperformance degradation near depth borders, where depth tends to vary abruptly.\nHence, we propose to anneal the depth border degradation in unpaired image\ndehazing with cyclic perceptual-depth supervision. Coupled with the dual-path\nfeature re-using backbones of the generators and discriminators, our model\nachieves $\\mathbf{20.36}$ Peak Signal-to-Noise Ratio (PSNR) on NYU Depth V2\ndataset, significantly outperforming its predecessors with reduced Floating\nPoint Operations (FLOPs).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:50:04 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liu", "Chen", ""], ["Fan", "Jiaqi", ""], ["Yin", "Guosheng", ""]]}, {"id": "2007.05223", "submitter": "Jianming Ye", "authors": "Jianming Ye, Shiliang Zhang, Jingdong Wang", "title": "Distillation Guided Residual Learning for Binary Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to bridge the performance gap between Binary CNN (BCNN) and\nFloating point CNN (FCNN). We observe that, this performance gap leads to\nsubstantial residuals between intermediate feature maps of BCNN and FCNN. To\nminimize the performance gap, we enforce BCNN to produce similar intermediate\nfeature maps with the ones of FCNN. This training strategy, i.e., optimizing\neach binary convolutional block with block-wise distillation loss derived from\nFCNN, leads to a more effective optimization to BCNN. It also motivates us to\nupdate the binary convolutional block architecture to facilitate the\noptimization of block-wise distillation loss. Specifically, a lightweight\nshortcut branch is inserted into each binary convolutional block to complement\nresiduals at each block. Benefited from its Squeeze-and-Interaction (SI)\nstructure, this shortcut branch introduces a fraction of parameters, e.g., 10\\%\noverheads, but effectively complements the residuals. Extensive experiments on\nImageNet demonstrate the superior performance of our method in both\nclassification efficiency and accuracy, e.g., BCNN trained with our methods\nachieves the accuracy of 60.45\\% on ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:55:39 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 01:21:29 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ye", "Jianming", ""], ["Zhang", "Shiliang", ""], ["Wang", "Jingdong", ""]]}, {"id": "2007.05224", "submitter": "Zhongqiang Liu", "authors": "Zhongqiang Liu", "title": "Automatic Segmentation of Non-Tumor Tissues in Glioma MR Brain Images\n  Using Deformable Registration with Partial Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brain tumor diagnosis and surgical planning, segmentation of tumor regions\nand accurate analysis of surrounding normal tissues are necessary for\nphysicians. Pathological variability often renders difficulty to register a\nwell-labeled normal atlas to such images and to automatic segment/label\nsurrounding normal brain tissues. In this paper, we propose a new registration\napproach that first segments brain tumor using a U-Net and then simulates\nmissed normal tissues within the tumor region using a partial convolutional\nnetwork. Then, a standard normal brain atlas image is registered onto such\ntumor-removed images in order to segment/label the normal brain tissues. In\nthis way, our new approach greatly reduces the effects of pathological\nvariability in deformable registration and segments the normal tissues\nsurrounding brain tumor well. In experiments, we used MICCAI BraTS2018 T1 tumor\nimages to evaluate the proposed algorithm. By comparing direct registration\nwith the proposed algorithm, the results showed that the Dice coefficient for\ngray matters was significantly improved for surrounding normal brain tissues.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:58:23 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liu", "Zhongqiang", ""]]}, {"id": "2007.05225", "submitter": "Qingsong Yao", "authors": "Qingsong Yao, Zecheng He, Hu Han and S. Kevin Zhou", "title": "Miss the Point: Targeted Adversarial Attack on Multiple Landmark\n  Detection", "comments": "accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent methods in multiple landmark detection based on deep convolutional\nneural networks (CNNs) reach high accuracy and improve traditional clinical\nworkflow. However, the vulnerability of CNNs to adversarial-example attacks can\nbe easily exploited to break classification and segmentation tasks. This paper\nis the first to study how fragile a CNN-based model on multiple landmark\ndetection to adversarial perturbations. Specifically, we propose a novel\nAdaptive Targeted Iterative FGSM (ATI-FGSM) attack against the state-of-the-art\nmodels in multiple landmark detection. The attacker can use ATI-FGSM to\nprecisely control the model predictions of arbitrarily selected landmarks,\nwhile keeping other stationary landmarks still, by adding imperceptible\nperturbations to the original image. A comprehensive evaluation on a public\ndataset for cephalometric landmark detection demonstrates that the adversarial\nexamples generated by ATI-FGSM break the CNN-based network more effectively and\nefficiently, compared with the original Iterative FGSM attack. Our work reveals\nserious threats to patients' health. Furthermore, we discuss the limitations of\nour method and provide potential defense directions, by investigating the\ncoupling effect of nearby landmarks, i.e., a major source of divergence in our\nexperiments. Our source code is available at\nhttps://github.com/qsyao/attack_landmark_detection.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:58:35 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Yao", "Qingsong", ""], ["He", "Zecheng", ""], ["Han", "Hu", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2007.05230", "submitter": "Jing Yao", "authors": "Jing Yao, Danfeng Hong, Jocelyn Chanussot, Deyu Meng, Xiaoxiang Zhu,\n  Zongben Xu", "title": "Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advancement of deep learning techniques has made great progress on\nhyperspectral image super-resolution (HSI-SR). Yet the development of\nunsupervised deep networks remains challenging for this task. To this end, we\npropose a novel coupled unmixing network with a cross-attention mechanism,\nCUCaNet for short, to enhance the spatial resolution of HSI by means of\nhigher-spatial-resolution multispectral image (MSI). Inspired by coupled\nspectral unmixing, a two-stream convolutional autoencoder framework is taken as\nbackbone to jointly decompose MS and HS data into a spectrally meaningful basis\nand corresponding coefficients. CUCaNet is capable of adaptively learning\nspectral and spatial response functions from HS-MS correspondences by enforcing\nreasonable consistency assumptions on the networks. Moreover, a cross-attention\nmodule is devised to yield more effective spatial-spectral information transfer\nin networks. Extensive experiments are conducted on three widely-used HS-MS\ndatasets in comparison with state-of-the-art HSI-SR models, demonstrating the\nsuperiority of the CUCaNet in the HSI-SR application. Furthermore, the codes\nand datasets will be available at:\nhttps://github.com/danfenghong/ECCV2020_CUCaNet.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 08:08:20 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 14:42:31 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 12:48:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yao", "Jing", ""], ["Hong", "Danfeng", ""], ["Chanussot", "Jocelyn", ""], ["Meng", "Deyu", ""], ["Zhu", "Xiaoxiang", ""], ["Xu", "Zongben", ""]]}, {"id": "2007.05233", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Alessio Tonioni, Fabio Tosi, Stefano Mattoccia, Luigi Di\n  Stefano", "title": "Continual Adaptation for Deep Stereo", "comments": "Extended version of CVPR 2019 paper \"Real-time self-adaptive deep\n  stereo\" - Accepted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from stereo images is carried out with unmatched results by\nconvolutional neural networks trained end-to-end to regress dense disparities.\nLike for most tasks, this is possible if large amounts of labelled samples are\navailable for training, possibly covering the whole data distribution\nencountered at deployment time. Being such an assumption systematically unmet\nin real applications, the capacity of adapting to any unseen setting becomes of\nparamount importance. Purposely, we propose a continual adaptation paradigm for\ndeep stereo networks designed to deal with challenging and ever-changing\nenvironments. We design a lightweight and modular architecture, Modularly\nADaptive Network (MADNet), and formulate Modular ADaptation algorithms (MAD,\nMAD++) which permit efficient optimization of independent sub-portions of the\nentire network. In our paradigm, the learning signals needed to continuously\nadapt models online can be sourced from self-supervision via right-to-left\nimage warping or from traditional stereo algorithms. With both sources, no\nother data than the input images being gathered at deployment time are needed.\nThus, our network architecture and adaptation algorithms realize the first\nreal-time self-adaptive deep stereo system and pave the way for a new paradigm\nthat can facilitate practical deployment of end-to-end architectures for dense\ndisparity regression.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 08:15:58 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 06:48:23 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 07:53:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Poggi", "Matteo", ""], ["Tonioni", "Alessio", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "2007.05280", "submitter": "Florian Kraus", "authors": "Florian Kraus, Nicolas Scheiner, Werner Ritter, Klaus Dietmayer", "title": "Using Machine Learning to Detect Ghost Images in Automotive Radar", "comments": null, "journal-ref": "IEEE 23rd International Conference on Intelligent Transportation\n  Systems (ITSC), Rhodes, Greece, 2020", "doi": "10.1109/ITSC45102.2020.9294631", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar sensors are an important part of driver assistance systems and\nintelligent vehicles due to their robustness against all kinds of adverse\nconditions, e.g., fog, snow, rain, or even direct sunlight. This robustness is\nachieved by a substantially larger wavelength compared to light-based sensors\nsuch as cameras or lidars. As a side effect, many surfaces act like mirrors at\nthis wavelength, resulting in unwanted ghost detections. In this article, we\npresent a novel approach to detect these ghost objects by applying data-driven\nmachine learning algorithms. For this purpose, we use a large-scale automotive\ndata set with annotated ghost objects. We show that we can use a\nstate-of-the-art automotive radar classifier in order to detect ghost objects\nalongside real objects. Furthermore, we are able to reduce the amount of false\npositive detections caused by ghost images in some settings.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:51:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kraus", "Florian", ""], ["Scheiner", "Nicolas", ""], ["Ritter", "Werner", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2007.05295", "submitter": "Julia Noothout", "authors": "Julia M. H. Noothout, Bob D. de Vos, Jelmer M. Wolterink, Elbrich M.\n  Postma, Paul A. M. Smeets, Richard A. P. Takx, Tim Leiner, Max A. Viergever\n  and Ivana I\\v{s}gum", "title": "Deep Learning-Based Regression and Classification for Automatic Landmark\n  Localization in Medical Images", "comments": "12 pages, accepted at IEEE transactions in Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2020.3009002", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a fast and accurate method to automatically\nlocalize anatomical landmarks in medical images. We employ a global-to-local\nlocalization approach using fully convolutional neural networks (FCNNs). First,\na global FCNN localizes multiple landmarks through the analysis of image\npatches, performing regression and classification simultaneously. In\nregression, displacement vectors pointing from the center of image patches\ntowards landmark locations are determined. In classification, presence of\nlandmarks of interest in the patch is established. Global landmark locations\nare obtained by averaging the predicted displacement vectors, where the\ncontribution of each displacement vector is weighted by the posterior\nclassification probability of the patch that it is pointing from. Subsequently,\nfor each landmark localized with global localization, local analysis is\nperformed. Specialized FCNNs refine the global landmark locations by analyzing\nlocal sub-images in a similar manner, i.e. by performing regression and\nclassification simultaneously and combining the results. Evaluation was\nperformed through localization of 8 anatomical landmarks in CCTA scans, 2\nlandmarks in olfactory MR scans, and 19 landmarks in cephalometric X-rays. We\ndemonstrate that the method performs similarly to a second observer and is able\nto localize landmarks in a diverse set of medical images, differing in image\nmodality, image dimensionality, and anatomical coverage.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 10:46:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Noothout", "Julia M. H.", ""], ["de Vos", "Bob D.", ""], ["Wolterink", "Jelmer M.", ""], ["Postma", "Elbrich M.", ""], ["Smeets", "Paul A. M.", ""], ["Takx", "Richard A. P.", ""], ["Leiner", "Tim", ""], ["Viergever", "Max A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "2007.05299", "submitter": "Zakaria Laskar", "authors": "Zakaria Laskar, Juho Kannala", "title": "Data-Efficient Ranking Distillation for Image Retrieval", "comments": "10 pages, 2 figures. Edited figure 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning has lead to rapid developments in the field\nof image retrieval. However, the best performing architectures incur\nsignificant computational cost. Recent approaches tackle this issue using\nknowledge distillation to transfer knowledge from a deeper and heavier\narchitecture to a much smaller network. In this paper we address knowledge\ndistillation for metric learning problems. Unlike previous approaches, our\nproposed method jointly addresses the following constraints i) limited queries\nto teacher model, ii) black box teacher model with access to the final output\nrepresentation, and iii) small fraction of original training data without any\nground-truth labels. In addition, the distillation method does not require the\nstudent and teacher to have same dimensionality. Addressing these constraints\nreduces computation requirements, dependency on large-scale training datasets\nand addresses practical scenarios of limited or partial access to private data\nsuch as teacher models or the corresponding training data/labels. The key idea\nis to augment the original training set with additional samples by performing\nlinear interpolation in the final output representation space. Distillation is\nthen performed in the joint space of original and augmented teacher-student\nsample representations. Results demonstrate that our approach can match\nbaseline models trained with full supervision. In low training sample settings,\nour approach outperforms the fully supervised approach on two challenging image\nretrieval datasets, ROxford5k and RParis6k \\cite{Roxf} with the least possible\nteacher supervision.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 10:59:16 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 10:51:04 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Laskar", "Zakaria", ""], ["Kannala", "Juho", ""]]}, {"id": "2007.05307", "submitter": "Yushan Liu", "authors": "Yushan Liu, Markus M. Geipel, Christoph Tietz, Florian Buettner", "title": "TIMELY: Improving Labeling Consistency in Medical Imaging for Cell Type\n  Classification", "comments": "Accepted at ECAI 2020 (24th European Conference on Artificial\n  Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing diseases such as leukemia or anemia requires reliable counts of\nblood cells. Hematologists usually label and count microscopy images of blood\ncells manually. In many cases, however, cells in different maturity states are\ndifficult to distinguish, and in combination with image noise and subjectivity,\nhumans are prone to make labeling mistakes. This results in labels that are\noften not reproducible, which can directly affect the diagnoses. We introduce\nTIMELY, a probabilistic model that combines pseudotime inference methods with\ninhomogeneous hidden Markov trees, which addresses this challenge of label\ninconsistency. We show first on simulation data that TIMELY is able to identify\nand correct wrong labels with higher precision and recall than baseline methods\nfor labeling correction. We then apply our method to two real-world datasets of\nblood cell data and show that TIMELY successfully finds inconsistent labels,\nthereby improving the quality of human-generated labels.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:13:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liu", "Yushan", ""], ["Geipel", "Markus M.", ""], ["Tietz", "Christoph", ""], ["Buettner", "Florian", ""]]}, {"id": "2007.05324", "submitter": "Johannes C. Paetzold", "authors": "Stefan Gerl, Johannes C. Paetzold, Hailong He, Ivan Ezhov, Suprosanna\n  Shit, Florian Kofler, Amirhossein Bayat, Giles Tetteh, Vasilis Ntziachristos,\n  Bjoern Menze", "title": "A distance-based loss for smooth and continuous skin layer segmentation\n  in optoacoustic images", "comments": "Accepted at International Conference on Medical Image Computing and\n  Computer-Assisted Intervention (MICCAI) 2020", "journal-ref": "Medical Image Computing and Computer Assisted Intervention MICCAI\n  2020. MICCAI 2020. Lecture Notes in Computer Science, vol 12266. Springer", "doi": "10.1007/978-3-030-59725-2_30", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raster-scan optoacoustic mesoscopy (RSOM) is a powerful, non-invasive optical\nimaging technique for functional, anatomical, and molecular skin and tissue\nanalysis. However, both the manual and the automated analysis of such images\nare challenging, because the RSOM images have very low contrast, poor signal to\nnoise ratio, and systematic overlaps between the absorption spectra of melanin\nand hemoglobin. Nonetheless, the segmentation of the epidermis layer is a\ncrucial step for many downstream medical and diagnostic tasks, such as vessel\nsegmentation or monitoring of cancer progression. We propose a novel,\nshape-specific loss function that overcomes discontinuous segmentations and\nachieves smooth segmentation surfaces while preserving the same volumetric Dice\nand IoU. Further, we validate our epidermis segmentation through the\nsensitivity of vessel segmentation. We found a 20 $\\%$ improvement in Dice for\nvessel segmentation tasks when the epidermis mask is provided as additional\ninformation to the vessel segmentation network.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 12:02:57 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gerl", "Stefan", ""], ["Paetzold", "Johannes C.", ""], ["He", "Hailong", ""], ["Ezhov", "Ivan", ""], ["Shit", "Suprosanna", ""], ["Kofler", "Florian", ""], ["Bayat", "Amirhossein", ""], ["Tetteh", "Giles", ""], ["Ntziachristos", "Vasilis", ""], ["Menze", "Bjoern", ""]]}, {"id": "2007.05343", "submitter": "Aryan Mobiny", "authors": "Aryan Mobiny, Pengyu Yuan, Pietro Antonio Cicalese, Hien Van Nguyen", "title": "DECAPS: Detail-Oriented Capsule Networks", "comments": "arXiv admin note: text overlap with arXiv:2004.07407", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks (CapsNets) have demonstrated to be a promising alternative\nto Convolutional Neural Networks (CNNs). However, they often fall short of\nstate-of-the-art accuracies on large-scale high-dimensional datasets. We\npropose a Detail-Oriented Capsule Network (DECAPS) that combines the strength\nof CapsNets with several novel techniques to boost its classification\naccuracies. First, DECAPS uses an Inverted Dynamic Routing (IDR) mechanism to\ngroup lower-level capsules into heads before sending them to higher-level\ncapsules. This strategy enables capsules to selectively attend to small but\ninformative details within the data which may be lost during pooling operations\nin CNNs. Second, DECAPS employs a Peekaboo training procedure, which encourages\nthe network to focus on fine-grained information through a second-level\nattention scheme. Finally, the distillation process improves the robustness of\nDECAPS by averaging over the original and attended image region predictions. We\nprovide extensive experiments on the CheXpert and RSNA Pneumonia datasets to\nvalidate the effectiveness of DECAPS. Our networks achieve state-of-the-art\naccuracies not only in classification (increasing the average area under ROC\ncurves from 87.24% to 92.82% on the CheXpert dataset) but also in the\nweakly-supervised localization of diseased areas (increasing average precision\nfrom 41.7% to 80% for the RSNA Pneumonia detection dataset).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 01:48:22 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Mobiny", "Aryan", ""], ["Yuan", "Pengyu", ""], ["Cicalese", "Pietro Antonio", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "2007.05351", "submitter": "Marc Aubreville", "authors": "Christof A. Bertram and Mitko Veta and Christian Marzahl and Nikolas\n  Stathonikos and Andreas Maier and Robert Klopfleisch and Marc Aubreville", "title": "Are pathologist-defined labels reproducible? Comparison of the TUPAC16\n  mitotic figure dataset with an alternative set of labels", "comments": "10 pages, submitted to LABELS@MICCAI 2020", "journal-ref": "In: Cardoso J. et al. (eds) Interpretable and Annotation-Efficient\n  Learning for Medical Image Computing. IMIMIC 2020, MIL3ID 2020, LABELS 2020.\n  Lecture Notes in Computer Science, vol 12446. Springer, Cham", "doi": "10.1007/978-3-030-61166-8_22", "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathologist-defined labels are the gold standard for histopathological data\nsets, regardless of well-known limitations in consistency for some tasks. To\ndate, some datasets on mitotic figures are available and were used for\ndevelopment of promising deep learning-based algorithms. In order to assess\nrobustness of those algorithms and reproducibility of their methods it is\nnecessary to test on several independent datasets. The influence of different\nlabeling methods of these available datasets is currently unknown. To tackle\nthis, we present an alternative set of labels for the images of the auxiliary\nmitosis dataset of the TUPAC16 challenge. Additional to manual mitotic figure\nscreening, we used a novel, algorithm-aided labeling process, that allowed to\nminimize the risk of missing rare mitotic figures in the images. All potential\nmitotic figures were independently assessed by two pathologists. The novel,\npublicly available set of labels contains 1,999 mitotic figures (+28.80%) and\nadditionally includes 10,483 labels of cells with high similarities to mitotic\nfigures (hard examples). We found significant difference comparing F_1 scores\nbetween the original label set (0.549) and the new alternative label set\n(0.735) using a standard deep learning object detection architecture. The\nmodels trained on the alternative set showed higher overall confidence values,\nsuggesting a higher overall label consistency. Findings of the present study\nshow that pathologists-defined labels may vary significantly resulting in\nnotable difference in the model performance. Comparison of deep learning-based\nalgorithms between independent datasets with different labeling methods should\nbe done with caution.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 12:44:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Bertram", "Christof A.", ""], ["Veta", "Mitko", ""], ["Marzahl", "Christian", ""], ["Stathonikos", "Nikolas", ""], ["Maier", "Andreas", ""], ["Klopfleisch", "Robert", ""], ["Aubreville", "Marc", ""]]}, {"id": "2007.05355", "submitter": "WanHong Huang", "authors": "Wanhong Huang, Chunxi Yang, TianHong Hou", "title": "Spine Landmark Localization with combining of Heatmap Regression and\n  Direct Coordinate Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark Localization plays a very important role in processing medical\nimages as well as in disease identification. However, In medical field, it's a\nchallenging task because of the complexity of medical images and the high\nrequirement of accuracy for disease identification and treatment.There are two\ndominant ways to regress landmark coordination, one using the full\nconvolutional network to regress the heatmaps of landmarks , which is a complex\nway and heatmap post-process strategies are needed, and the other way is to\nregress the coordination using CNN + Full Connective Network directly, which is\nvery simple and faster training , but larger dataset and deeper model are\nneeded to achieve higher accuracy. Though with data augmentation and deeper\nnetwork it can reach a reasonable accuracy, but the accuracy still not reach\nthe requirement of medical field. In addition, a deeper networks also means\nlarger space consumption. To achieve a higher accuracy, we contrived a new\nlandmark regression method which combing heatmap regression and direct\ncoordinate regression base on probability methods and system control theory.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 12:47:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Huang", "Wanhong", ""], ["Yang", "Chunxi", ""], ["Hou", "TianHong", ""]]}, {"id": "2007.05361", "submitter": "Le Hui", "authors": "Le Hui, Rui Xu, Jin Xie, Jianjun Qian, Jian Yang", "title": "Progressive Point Cloud Deconvolution Generation Network", "comments": "Accepted to ECCV 2020; Project page: https://github.com/fpthink/PDGN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective point cloud generation method, which\ncan generate multi-resolution point clouds of the same shape from a latent\nvector. Specifically, we develop a novel progressive deconvolution network with\nthe learning-based bilateral interpolation. The learning-based bilateral\ninterpolation is performed in the spatial and feature spaces of point clouds so\nthat local geometric structure information of point clouds can be exploited.\nStarting from the low-resolution point clouds, with the bilateral interpolation\nand max-pooling operations, the deconvolution network can progressively output\nhigh-resolution local and global feature maps. By concatenating different\nresolutions of local and global feature maps, we employ the multi-layer\nperceptron as the generation network to generate multi-resolution point clouds.\nIn order to keep the shapes of different resolutions of point clouds\nconsistent, we propose a shape-preserving adversarial loss to train the point\ncloud deconvolution generation network. Experimental results demonstrate the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:07:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Hui", "Le", ""], ["Xu", "Rui", ""], ["Xie", "Jin", ""], ["Qian", "Jianjun", ""], ["Yang", "Jian", ""]]}, {"id": "2007.05363", "submitter": "Krishna Chaitanya", "authors": "Krishna Chaitanya, Neerav Karani, Christian F. Baumgartner, Ertunc\n  Erdil, Anton Becker, Olivio Donati, Ender Konukoglu", "title": "Semi-supervised Task-driven Data Augmentation for Medical Image\n  Segmentation", "comments": "15 pages, 11 Figures, 3 tables. Accepted at Medical Image Analysis,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning-based segmentation methods typically require a large\nnumber of annotated training data to generalize well at test time. In medical\napplications, curating such datasets is not a favourable option because\nacquiring a large number of annotated samples from experts is time-consuming\nand expensive. Consequently, numerous methods have been proposed in the\nliterature for learning with limited annotated examples. Unfortunately, the\nproposed approaches in the literature have not yet yielded significant gains\nover random data augmentation for image segmentation, where random\naugmentations themselves do not yield high accuracy. In this work, we propose a\nnovel task-driven data augmentation method for learning with limited labeled\ndata where the synthetic data generator, is optimized for the segmentation\ntask. The generator of the proposed method models intensity and shape\nvariations using two sets of transformations, as additive intensity\ntransformations and deformation fields. Both transformations are optimized\nusing labeled as well as unlabeled examples in a semi-supervised framework. Our\nexperiments on three medical datasets, namely cardic, prostate and pancreas,\nshow that the proposed approach significantly outperforms standard augmentation\nand semi-supervised approaches for image segmentation in the limited annotation\nsetting. The code is made publicly available at\nhttps://github.com/krishnabits001/task$\\_$driven$\\_$data$\\_$augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:05:42 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 17:34:51 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chaitanya", "Krishna", ""], ["Karani", "Neerav", ""], ["Baumgartner", "Christian F.", ""], ["Erdil", "Ertunc", ""], ["Becker", "Anton", ""], ["Donati", "Olivio", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2007.05393", "submitter": "Shen Wang", "authors": "Shen Wang, Kongming Liang, Yiming Li, Yizhou Yu, Yizhou Wang", "title": "Context-Aware Refinement Network Incorporating Structural Connectivity\n  Prior for Brain Midline Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain midline delineation can facilitate the clinical evaluation of brain\nmidline shift, which plays an important role in the diagnosis and prognosis of\nvarious brain pathology. Nevertheless, there are still great challenges with\nbrain midline delineation, such as the largely deformed midline caused by the\nmass effect and the possible morphological failure that the predicted midline\nis not a connected curve. To address these challenges, we propose a\ncontext-aware refinement network (CAR-Net) to refine and integrate the feature\npyramid representation generated by the UNet. Consequently, the proposed\nCAR-Net explores more discriminative contextual features and a larger receptive\nfield, which is of great importance to predict largely deformed midline. For\nkeeping the structural connectivity of the brain midline, we introduce a novel\nconnectivity regular loss (CRL) to punish the disconnectivity between adjacent\ncoordinates. Moreover, we address the ignored prerequisite of previous\nregression-based methods that the brain CT image must be in the standard pose.\nA simple pose rectification network is presented to align the source input\nimage to the standard pose image. Extensive experimental results on the CQ\ndataset and one inhouse dataset show that the proposed method requires fewer\nparameters and outperforms three state-of-the-art methods in terms of four\nevaluation metrics. Code is available at\nhttps://github.com/ShawnBIT/Brain-Midline-Detection.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:01:20 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wang", "Shen", ""], ["Liang", "Kongming", ""], ["Li", "Yiming", ""], ["Yu", "Yizhou", ""], ["Wang", "Yizhou", ""]]}, {"id": "2007.05397", "submitter": "Adithya Ranga", "authors": "Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel,\n  Patrick P\\'erez, Tuan-Hung Vu and Xavier Perrotton", "title": "VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable\n  Road Users", "comments": "This paper is reprinted from, \"VRUNet: Multi-Task Learning Model for\n  Intent Prediction of Vulnerable Road Users, IS&T Electronic Imaging:\n  Autonomous Vehicles and Machines 2020 Proceedings, (IS&T, Springfield, VA,\n  2020) page 109-1-10. DOI: 10.2352/ISSN.2470-1173.2020.16.AVM-109.\" Reprinted\n  with permission of The Society for Imaging Science and Technology, holders of\n  the 2020 copyright", "journal-ref": null, "doi": "10.2352/ISSN.2470-1173.2020.16.AVM-109", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced perception and path planning are at the core for any self-driving\nvehicle. Autonomous vehicles need to understand the scene and intentions of\nother road users for safe motion planning. For urban use cases it is very\nimportant to perceive and predict the intentions of pedestrians, cyclists,\nscooters, etc., classified as vulnerable road users (VRU). Intent is a\ncombination of pedestrian activities and long term trajectories defining their\nfuture motion. In this paper we propose a multi-task learning model to predict\npedestrian actions, crossing intent and forecast their future path from video\nsequences. We have trained the model on naturalistic driving open-source JAAD\ndataset, which is rich in behavioral annotations and real world scenarios.\nExperimental results show state-of-the-art performance on JAAD dataset and how\nwe can benefit from jointly learning and predicting actions and trajectories\nusing 2D human pose features and scene context.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:02:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Ranga", "Adithya", ""], ["Giruzzi", "Filippo", ""], ["Bhanushali", "Jagdish", ""], ["Wirbel", "Emilie", ""], ["P\u00e9rez", "Patrick", ""], ["Vu", "Tuan-Hung", ""], ["Perrotton", "Xavier", ""]]}, {"id": "2007.05405", "submitter": "Chinedu Nwoye", "authors": "Chinedu Innocent Nwoye, Cristians Gonzalez, Tong Yu, Pietro Mascagni,\n  Didier Mutter, Jacques Marescaux and Nicolas Padoy", "title": "Recognition of Instrument-Tissue Interactions in Endoscopic Videos via\n  Action Triplets", "comments": "13 pages, 4 figures, 6 tables. Accepted and to be published in MICCAI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recognition of surgical activity is an essential component to develop\ncontext-aware decision support for the operating room. In this work, we tackle\nthe recognition of fine-grained activities, modeled as action triplets\n<instrument, verb, target> representing the tool activity. To this end, we\nintroduce a new laparoscopic dataset, CholecT40, consisting of 40 videos from\nthe public dataset Cholec80 in which all frames have been annotated using 128\ntriplet classes. Furthermore, we present an approach to recognize these\ntriplets directly from the video data. It relies on a module called Class\nActivation Guide (CAG), which uses the instrument activation maps to guide the\nverb and target recognition. To model the recognition of multiple triplets in\nthe same frame, we also propose a trainable 3D Interaction Space, which\ncaptures the associations between the triplet components. Finally, we\ndemonstrate the significance of these contributions via several ablation\nstudies and comparisons to baselines on CholecT40.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:17:10 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Nwoye", "Chinedu Innocent", ""], ["Gonzalez", "Cristians", ""], ["Yu", "Tong", ""], ["Mascagni", "Pietro", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2007.05428", "submitter": "Duong Hung Pham", "authors": "Duong-Hung Pham, Adrian Basarab, Ilyess Zemmoura, Jean-Pierre\n  Remenieras and Denis Kouame", "title": "Joint Blind Deconvolution and Robust Principal Component Analysis for\n  Blood Flow Estimation in Medical Ultrasound Imaging", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of high-resolution Doppler blood flow\nestimation from an ultrafast sequence of ultrasound images. Formulating the\nseparation of clutter and blood components as an inverse problem has been shown\nin the literature to be a good alternative to spatio-temporal singular value\ndecomposition (SVD)-based clutter filtering. In particular, a deconvolution\nstep has recently been embedded in such a problem to mitigate the influence of\nthe experimentally measured point spread function (PSF) of the imaging system.\nDeconvolution was shown in this context to improve the accuracy of the blood\nflow reconstruction. However, measuring the PSF requires non-trivial\nexperimental setups. To overcome this limitation, we propose herein a blind\ndeconvolution method able to estimate both the blood component and the PSF from\nDoppler data. Numerical experiments conducted on simulated and in vivo data\ndemonstrate qualitatively and quantitatively the effectiveness of the proposed\napproach in comparison with the previous method based on experimentally\nmeasured PSF and two other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:03:33 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Pham", "Duong-Hung", ""], ["Basarab", "Adrian", ""], ["Zemmoura", "Ilyess", ""], ["Remenieras", "Jean-Pierre", ""], ["Kouame", "Denis", ""]]}, {"id": "2007.05441", "submitter": "Gongfan Fang", "authors": "Gongfan Fang, Xinchao Wang, Haofei Zhang, Jie Song, Mingli Song", "title": "Impression Space from Deep Template Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an innate ability for humans to imagine something only according to\ntheir impression, without having to memorize all the details of what they have\nseen. In this work, we would like to demonstrate that a trained convolutional\nneural network also has the capability to \"remember\" its input images. To\nachieve this, we propose a simple but powerful framework to establish an\n{\\emph{Impression Space}} upon an off-the-shelf pretrained network. This\nnetwork is referred to as the {\\emph{Template Network}} because its filters\nwill be used as templates to reconstruct images from the impression. In our\nframework, the impression space and image space are bridged by a layer-wise\nencoding and iterative decoding process. It turns out that the impression space\nindeed captures the salient features from images, and it can be directly\napplied to tasks such as unpaired image translation and image synthesis through\nimpression matching without further network training. Furthermore, the\nimpression naturally constructs a high-level common space for different data.\nBased on this, we propose a mechanism to model the data relations inside the\nimpression space, which is able to reveal the feature similarity between\nimages. Our code will be released.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:29:33 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Fang", "Gongfan", ""], ["Wang", "Xinchao", ""], ["Zhang", "Haofei", ""], ["Song", "Jie", ""], ["Song", "Mingli", ""]]}, {"id": "2007.05446", "submitter": "Prasitthichai Naronglerdrit", "authors": "Prasitthichai Naronglerdrit, Iosif Mporas", "title": "Evaluation of Big Data based CNN Models in Classification of Skin\n  Lesions with Melanoma", "comments": "Series Title: Studies in Computational Intelligence, Book Title: Deep\n  Learning for Cancer Diagnosis, Series Volume: 908, DOI:\n  10.1007/978-981-15-6321-8, eBook ISBN: 978-981-15-6321-8", "journal-ref": null, "doi": "10.1007/978-981-15-6321-8", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter presents a methodology for diagnosis of pigmented skin lesions\nusing convolutional neural networks. The architecture is based on\nconvolu-tional neural networks and it is evaluated using new CNN models as well\nas re-trained modification of pre-existing CNN models were used. The\nexperi-mental results showed that CNN models pre-trained on big datasets for\ngen-eral purpose image classification when re-trained in order to identify skin\nle-sion types offer more accurate results when compared to convolutional neural\nnetwork models trained explicitly from the dermatoscopic images. The best\nperformance was achieved by re-training a modified version of ResNet-50\nconvolutional neural network with accuracy equal to 93.89%. Analysis on skin\nlesion pathology type was also performed with classification accuracy for\nmelanoma and basal cell carcinoma being equal to 79.13% and 82.88%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:39:32 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Naronglerdrit", "Prasitthichai", ""], ["Mporas", "Iosif", ""]]}, {"id": "2007.05448", "submitter": "Hui Qu", "authors": "Hui Qu, Pengxiang Wu, Qiaoying Huang, Jingru Yi, Zhennan Yan, Kang Li,\n  Gregory M. Riedlinger, Subhajyoti De, Shaoting Zhang, Dimitris N. Metaxas", "title": "Weakly Supervised Deep Nuclei Segmentation Using Partial Points\n  Annotation in Histopathology Images", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/TMI.2020.3002244", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclei segmentation is a fundamental task in histopathology image analysis.\nTypically, such segmentation tasks require significant effort to manually\ngenerate accurate pixel-wise annotations for fully supervised training. To\nalleviate such tedious and manual effort, in this paper we propose a novel\nweakly supervised segmentation framework based on partial points annotation,\ni.e., only a small portion of nuclei locations in each image are labeled. The\nframework consists of two learning stages. In the first stage, we design a\nsemi-supervised strategy to learn a detection model from partially labeled\nnuclei locations. Specifically, an extended Gaussian mask is designed to train\nan initial model with partially labeled data. Then, selftraining with\nbackground propagation is proposed to make use of the unlabeled regions to\nboost nuclei detection and suppress false positives. In the second stage, a\nsegmentation model is trained from the detected nuclei locations in a\nweakly-supervised fashion. Two types of coarse labels with complementary\ninformation are derived from the detected points and are then utilized to train\na deep neural network. The fully-connected conditional random field loss is\nutilized in training to further refine the model without introducing extra\ncomputational complexity during inference. The proposed method is extensively\nevaluated on two nuclei segmentation datasets. The experimental results\ndemonstrate that our method can achieve competitive performance compared to the\nfully supervised counterpart and the state-of-the-art methods while requiring\nsignificantly less annotation effort.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:41:29 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Qu", "Hui", ""], ["Wu", "Pengxiang", ""], ["Huang", "Qiaoying", ""], ["Yi", "Jingru", ""], ["Yan", "Zhennan", ""], ["Li", "Kang", ""], ["Riedlinger", "Gregory M.", ""], ["De", "Subhajyoti", ""], ["Zhang", "Shaoting", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "2007.05454", "submitter": "Maria Escobar", "authors": "Cristina Gonz\\'alez and Mar\\'ia Escobar and Laura Daza and Felipe\n  Torres and Gustavo Triana and Pablo Arbel\\'aez", "title": "SIMBA: Specific Identity Markers for Bone Age Assessment", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone Age Assessment (BAA) is a task performed by radiologists to diagnose\nabnormal growth in a child. In manual approaches, radiologists take into\naccount different identity markers when calculating bone age, i.e.,\nchronological age and gender. However, the current automated Bone Age\nAssessment methods do not completely exploit the information present in the\npatient's metadata. With this lack of available methods as motivation, we\npresent SIMBA: Specific Identity Markers for Bone Age Assessment. SIMBA is a\nnovel approach for the task of BAA based on the use of identity markers. For\nthis purpose, we build upon the state-of-the-art model, fusing the information\npresent in the identity markers with the visual features created from the\noriginal hand radiograph. We then use this robust representation to estimate\nthe patient's relative bone age: the difference between chronological age and\nbone age. We validate SIMBA on the Radiological Hand Pose Estimation dataset\nand find that it outperforms previous state-of-the-art methods. SIMBA sets a\ntrend of a new wave of Computer-aided Diagnosis methods that incorporate all of\nthe data that is available regarding a patient. To promote further research in\nthis area and ensure reproducibility we will provide the source code as well as\nthe pre-trained models of SIMBA.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:48:48 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 16:30:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gonz\u00e1lez", "Cristina", ""], ["Escobar", "Mar\u00eda", ""], ["Daza", "Laura", ""], ["Torres", "Felipe", ""], ["Triana", "Gustavo", ""], ["Arbel\u00e1ez", "Pablo", ""]]}, {"id": "2007.05461", "submitter": "Varun Aggarwal", "authors": "Abhishek Singhania, Abhishek Unnam and Varun Aggarwal", "title": "Grading video interviews with fairness considerations", "comments": "Submitted to NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable interest in predicting human emotions and traits\nusing facial images and videos. Lately, such work has come under criticism for\npoor labeling practices, inconclusive prediction results and fairness\nconsiderations. We present a careful methodology to automatically derive social\nskills of candidates based on their video response to interview questions. We,\nfor the first time, include video data from multiple countries encompassing\nmultiple ethnicities. Also, the videos were rated by individuals from multiple\nracial backgrounds, following several best practices, to achieve a consensus\nand unbiased measure of social skills. We develop two machine-learning models\nto predict social skills. The first model employs expert-guidance to use\nplausibly causal features. The second uses deep learning and depends solely on\nthe empirical correlations present in the data. We compare errors of both these\nmodels, study the specificity of the models and make recommendations. We\nfurther analyze fairness by studying the errors of models by race and gender.\nWe verify the usefulness of our models by determining how well they predict\ninterview outcomes for candidates. Overall, the study provides strong support\nfor using artificial intelligence for video interview scoring, while taking\ncare of fairness and ethical considerations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:06:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Singhania", "Abhishek", ""], ["Unnam", "Abhishek", ""], ["Aggarwal", "Varun", ""]]}, {"id": "2007.05471", "submitter": "Xiao-Chang Liu", "authors": "Xiao-Chang Liu, Xuan-Yi Li, Ming-Ming Cheng, Peter Hall", "title": "Geometric Style Transfer", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer (NST), where an input image is rendered in the style of\nanother image, has been a topic of considerable progress in recent years.\nResearch over that time has been dominated by transferring aspects of color and\ntexture, yet these factors are only one component of style. Other factors of\nstyle include composition, the projection system used, and the way in which\nartists warp and bend objects. Our contribution is to introduce a neural\narchitecture that supports transfer of geometric style. Unlike recent work in\nthis area, we are unique in being general in that we are not restricted by\nsemantic content. This new architecture runs prior to a network that transfers\ntexture style, enabling us to transfer texture to a warped image. This form of\nnetwork supports a second novelty: we extend the NST input paradigm. Users can\ninput content/style pair as is common, or they can chose to input a\ncontent/texture-style/geometry-style triple. This three image input paradigm\ndivides style into two parts and so provides significantly greater versatility\nto the output we can produce. We provide user studies that show the quality of\nour output, and quantify the importance of geometric style transfer to style\nrecognition by humans.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:33:23 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liu", "Xiao-Chang", ""], ["Li", "Xuan-Yi", ""], ["Cheng", "Ming-Ming", ""], ["Hall", "Peter", ""]]}, {"id": "2007.05481", "submitter": "Pierre Godet", "authors": "Pierre Godet, Alexandre Boulch, Aur\\'elien Plyer and Guy Le Besnerais", "title": "STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame\n  Optical Flow Estimation", "comments": "9 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new lightweight CNN-based algorithm for multi-frame optical flow\nestimation. Our solution introduces a double recurrence over spatial scale and\ntime through repeated use of a generic \"STaR\" (SpatioTemporal Recurrent) cell.\nIt includes (i) a temporal recurrence based on conveying learned features\nrather than optical flow estimates; (ii) an occlusion detection process which\nis coupled with optical flow estimation and therefore uses a very limited\nnumber of extra parameters. The resulting STaRFlow algorithm gives\nstate-of-the-art performances on MPI Sintel and Kitti2015 and involves\nsignificantly less parameters than all other methods with comparable results.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:01:34 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Godet", "Pierre", ""], ["Boulch", "Alexandre", ""], ["Plyer", "Aur\u00e9lien", ""], ["Besnerais", "Guy Le", ""]]}, {"id": "2007.05490", "submitter": "Julie Stephany Berrio Perez", "authors": "Julie Stephany Berrio, Mao Shan, Stewart Worrall, Eduardo Nebot", "title": "Camera-Lidar Integration: Probabilistic sensor fusion for semantic\n  mapping", "comments": "15 pages. arXiv admin note: text overlap with arXiv:2003.01871", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated vehicle operating in an urban environment must be able to\nperceive and recognise object/obstacles in a three-dimensional world while\nnavigating in a constantly changing environment. In order to plan and execute\naccurate sophisticated driving maneuvers, a high-level contextual understanding\nof the surroundings is essential. Due to the recent progress in image\nprocessing, it is now possible to obtain high definition semantic information\nin 2D from monocular cameras, though cameras cannot reliably provide the highly\naccurate 3D information provided by lasers. The fusion of these two sensor\nmodalities can overcome the shortcomings of each individual sensor, though\nthere are a number of important challenges that need to be addressed in a\nprobabilistic manner. In this paper, we address the common, yet challenging,\nlidar/camera/semantic fusion problems which are seldom approached in a wholly\nprobabilistic manner. Our approach is capable of using a multi-sensor platform\nto build a three-dimensional semantic voxelized map that considers the\nuncertainty of all of the processes involved. We present a probabilistic\npipeline that incorporates uncertainties from the sensor readings (cameras,\nlidar, IMU and wheel encoders), compensation for the motion of the vehicle, and\nheuristic label probabilities for the semantic images. We also present a novel\nand efficient viewpoint validation algorithm to check for occlusions from the\ncamera frames. A probabilistic projection is performed from the camera images\nto the lidar point cloud. Each labelled lidar scan then feeds into an octree\nmap building algorithm that updates the class probabilities of the map voxels\nevery time a new observation is available. We validate our approach using a set\nof qualitative and quantitative experimental tests on the USyd Dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 07:59:39 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Berrio", "Julie Stephany", ""], ["Shan", "Mao", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2007.05494", "submitter": "C\\'esar Soares", "authors": "Lucas P. Soares and Cesar P. Soares", "title": "Automatic Detection of COVID-19 Cases on X-ray images Using\n  Convolutional Neural Networks", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent months the world has been surprised by the rapid advance of\nCOVID-19. In order to face this disease and minimize its socio-economic\nimpacts, in addition to surveillance and treatment, diagnosis is a crucial\nprocedure. However, the realization of this is hampered by the delay and the\nlimited access to laboratory tests, demanding new strategies to carry out case\ntriage. In this scenario, deep learning models are being proposed as a possible\noption to assist the diagnostic process based on chest X-ray and computed\ntomography images. Therefore, this research aims to automate the process of\ndetecting COVID-19 cases from chest images, using convolutional neural networks\n(CNN) through deep learning techniques. The results can contribute to expand\naccess to other forms of detection of COVID-19 and to speed up the process of\nidentifying this disease. All databases used, the codes built, and the results\nobtained from the models' training are available for open access. This action\nfacilitates the involvement of other researchers in enhancing these models\nsince this can contribute to the improvement of results and, consequently, the\nprogress in confronting COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 00:46:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Soares", "Lucas P.", ""], ["Soares", "Cesar P.", ""]]}, {"id": "2007.05500", "submitter": "Subhashini Venugopalan", "authors": "Arunachalam Narayanaswamy, Subhashini Venugopalan, Dale R. Webster,\n  Lily Peng, Greg Corrado, Paisan Ruamviboonsuk, Pinal Bavishi, Rory Sayres,\n  Abigail Huang, Siva Balasubramanian, Michael Brenner, Philip Nelson, and\n  Avinash V. Varadarajan", "title": "Scientific Discovery by Generating Counterfactuals using Image\n  Translation", "comments": "Accepted at MICCAI 2020. This version combines camera-ready and\n  supplement", "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model explanation techniques play a critical role in understanding the source\nof a model's performance and making its decisions transparent. Here we\ninvestigate if explanation techniques can also be used as a mechanism for\nscientific discovery. We make three contributions: first, we propose a\nframework to convert predictions from explanation techniques to a mechanism of\ndiscovery. Second, we show how generative models in combination with black-box\npredictors can be used to generate hypotheses (without human priors) that can\nbe critically examined. Third, with these techniques we study classification\nmodels for retinal images predicting Diabetic Macular Edema (DME), where recent\nwork showed that a CNN trained on these images is likely learning novel\nfeatures in the image. We demonstrate that the proposed framework is able to\nexplain the underlying scientific mechanism, thus bridging the gap between the\nmodel's performance and human understanding.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:25:52 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 23:38:22 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Narayanaswamy", "Arunachalam", ""], ["Venugopalan", "Subhashini", ""], ["Webster", "Dale R.", ""], ["Peng", "Lily", ""], ["Corrado", "Greg", ""], ["Ruamviboonsuk", "Paisan", ""], ["Bavishi", "Pinal", ""], ["Sayres", "Rory", ""], ["Huang", "Abigail", ""], ["Balasubramanian", "Siva", ""], ["Brenner", "Michael", ""], ["Nelson", "Philip", ""], ["Varadarajan", "Avinash V.", ""]]}, {"id": "2007.05515", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "AViD Dataset: Anonymized Videos from Diverse Countries", "comments": "https://github.com/piergiaj/AViD", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new public video dataset for action recognition: Anonymized\nVideos from Diverse countries (AViD). Unlike existing public video datasets,\nAViD is a collection of action videos from many different countries. The\nmotivation is to create a public dataset that would benefit training and\npretraining of action recognition models for everybody, rather than making it\nuseful for limited countries. Further, all the face identities in the AViD\nvideos are properly anonymized to protect their privacy. It also is a static\ndataset where each video is licensed with the creative commons license. We\nconfirm that most of the existing video datasets are statistically biased to\nonly capture action videos from a limited number of countries. We\nexperimentally illustrate that models trained with such biased datasets do not\ntransfer perfectly to action videos from the other countries, and show that\nAViD addresses such problem. We also confirm that the new AViD dataset could\nserve as a good dataset for pretraining the models, performing comparably or\nbetter than prior datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:50:38 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 15:23:54 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 15:10:44 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2007.05533", "submitter": "Laura Bravo", "authors": "Cristina Gonz\\'alez (1), Laura Bravo-S\\'anchez (1), Pablo Arbelaez (1)\n  ((1) Center for Research and Formation in Artificial Intelligence,\n  Universidad de los Andes, Colombia)", "title": "ISINet: An Instance-Based Approach for Surgical Instrument Segmentation", "comments": "Accepted at MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of semantic segmentation of surgical instruments in\nrobotic-assisted surgery scenes. We propose the Instance-based Surgical\nInstrument Segmentation Network (ISINet), a method that addresses this task\nfrom an instance-based segmentation perspective. Our method includes a temporal\nconsistency module that takes into account the previously overlooked and\ninherent temporal information of the problem. We validate our approach on the\nexisting benchmark for the task, the Endoscopic Vision 2017 Robotic Instrument\nSegmentation Dataset, and on the 2018 version of the dataset, whose annotations\nwe extended for the fine-grained version of instrument segmentation. Our\nresults show that ISINet significantly outperforms state-of-the-art methods,\nwith our baseline version duplicating the Intersection over Union (IoU) of\nprevious methods and our complete model triplicating the IoU.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:20:56 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gonz\u00e1lez", "Cristina", ""], ["Bravo-S\u00e1nchez", "Laura", ""], ["Arbelaez", "Pablo", ""]]}, {"id": "2007.05534", "submitter": "Wentao Zhu", "authors": "Liyue Shen, Wentao Zhu, Xiaosong Wang, Lei Xing, John M. Pauly, Baris\n  Turkbey, Stephanie Anne Harmon, Thomas Hogue Sanford, Sherif Mehralivand,\n  Peter Choyke, Bradford Wood, Daguang Xu", "title": "Multi-Domain Image Completion for Random Missing Input Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-domain data are widely leveraged in vision applications taking\nadvantage of complementary information from different modalities, e.g., brain\ntumor segmentation from multi-parametric magnetic resonance imaging (MRI).\nHowever, due to possible data corruption and different imaging protocols, the\navailability of images for each domain could vary amongst multiple data sources\nin practice, which makes it challenging to build a universal model with a\nvaried set of input data. To tackle this problem, we propose a general approach\nto complete the random missing domain(s) data in real applications.\nSpecifically, we develop a novel multi-domain image completion method that\nutilizes a generative adversarial network (GAN) with a representational\ndisentanglement scheme to extract shared skeleton encoding and separate flesh\nencoding across multiple domains. We further illustrate that the learned\nrepresentation in multi-domain image completion could be leveraged for\nhigh-level tasks, e.g., segmentation, by introducing a unified framework\nconsisting of image completion and segmentation with a shared content encoder.\nThe experiments demonstrate consistent performance improvement on three\ndatasets for brain tumor segmentation, prostate segmentation, and facial\nexpression image completion respectively.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:38:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shen", "Liyue", ""], ["Zhu", "Wentao", ""], ["Wang", "Xiaosong", ""], ["Xing", "Lei", ""], ["Pauly", "John M.", ""], ["Turkbey", "Baris", ""], ["Harmon", "Stephanie Anne", ""], ["Sanford", "Thomas Hogue", ""], ["Mehralivand", "Sherif", ""], ["Choyke", "Peter", ""], ["Wood", "Bradford", ""], ["Xu", "Daguang", ""]]}, {"id": "2007.05573", "submitter": "Yutong Gao", "authors": "Yutong Gao, Yi Pan", "title": "Improved Detection of Adversarial Images Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are immensely deployed in both industry and\nacademy. Recent studies indicate that machine learning models used for\nclassification tasks are vulnerable to adversarial examples, which limits the\nusage of applications in the fields with high precision requirements. We\npropose a new approach called Feature Map Denoising to detect the adversarial\ninputs and show the performance of detection on the mixed dataset consisting of\nadversarial examples generated by different attack algorithms, which can be\nused to associate with any pre-trained DNNs at a low cost. Wiener filter is\nalso introduced as the denoise algorithm to the defense model, which can\nfurther improve performance. Experimental results indicate that good accuracy\nof detecting the adversarial examples can be achieved through our Feature Map\nDenoising algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 19:02:24 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gao", "Yutong", ""], ["Pan", "Yi", ""]]}, {"id": "2007.05592", "submitter": "Boyi Liu", "authors": "Boyi Liu, Bingjie Yan, Yize Zhou, Yifan Yang, Yixian Zhang", "title": "Experiments of Federated Learning for COVID-19 Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI plays an important role in COVID-19 identification. Computer vision and\ndeep learning techniques can assist in determining COVID-19 infection with\nChest X-ray Images. However, for the protection and respect of the privacy of\npatients, the hospital's specific medical-related data did not allow leakage\nand sharing without permission. Collecting such training data was a major\nchallenge. To a certain extent, this has caused a lack of sufficient data\nsamples when performing deep learning approaches to detect COVID-19. Federated\nLearning is an available way to address this issue. It can effectively address\nthe issue of data silos and get a shared model without obtaining local data. In\nthe work, we propose the use of federated learning for COVID-19 data training\nand deploy experiments to verify the effectiveness. And we also compare\nperformances of four popular models (MobileNet, ResNet18, MoblieNet, and\nCOVID-Net) with the federated learning framework and without the framework.\nThis work aims to inspire more researches on federated learning about COVID-19.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 08:25:37 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Boyi", ""], ["Yan", "Bingjie", ""], ["Zhou", "Yize", ""], ["Yang", "Yifan", ""], ["Zhang", "Yixian", ""]]}, {"id": "2007.05593", "submitter": "Hong Xu", "authors": "Hong Xu, David E. Timm, Shireen Y. Elhabian", "title": "Attention-guided Quality Assessment for Automated Cryo-EM Grid Screening", "comments": "Accepted for publication in MICCAI 2020, the 23rd International\n  Conference on Medical Image Computing and Computer Assisted Intervention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryogenic electron microscopy (cryo-EM) has become an enabling technology in\ndrug discovery and in understanding molecular bases of disease by producing\nnear-atomic resolution (less than 0.4 nm) 3D reconstructions of biological\nmacromolecules. The imaging process required for 3D reconstructions involves a\nhighly iterative and empirical screening process, starting with the acquisition\nof low magnification images of the cryo-EM grids. These images are inspected\nfor squares that are likely to contain useful molecular signals. Potentially\nuseful squares within the grid are then imaged at progressively higher\nmagnifications, with the goal of identifying sub-micron areas within circular\nholes (bounded by the squares) for imaging at high magnification. This arduous,\nmulti-step data acquisition process represents a bottleneck for obtaining a\nhigh throughput data collection. Here, we focus on automating the early\ndecision making for the microscope operator, scoring low magnification images\nof squares, and proposing the first deep learning framework, XCryoNet, for\nautomated cryo-EM grid screening. XCryoNet is a semi-supervised,\nattention-guided deep learning approach that provides explainable scoring of\nautomatically extracted square images using limited amounts of labeled data.\nResults show up to 8% and 37% improvements over a fully supervised and a\nno-attention solution, respectively, when labeled data is scarce.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 20:11:43 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 21:55:17 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xu", "Hong", ""], ["Timm", "David E.", ""], ["Elhabian", "Shireen Y.", ""]]}, {"id": "2007.05597", "submitter": "Siddharth Biswal", "authors": "Siddharth Biswal, Peiye Zhuang, Ayis Pyrros, Nasir Siddiqui, Sanmi\n  Koyejo, Jimeng Sun", "title": "EMIXER: End-to-end Multimodal X-ray Generation via Self-supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have enabled the automated synthesis of high-quality\ndata for diverse applications. However, the most effective generative models\nare specialized to data from a single domain (e.g., images or text). Real-world\napplications such as healthcare require multi-modal data from multiple domains\n(e.g., both images and corresponding text), which are difficult to acquire due\nto limited availability and privacy concerns and are much harder to synthesize.\nTo tackle this joint synthesis challenge, we propose an End-to-end MultImodal\nX-ray genERative model (EMIXER) for jointly synthesizing x-ray images and\ncorresponding free-text reports, all conditional on diagnosis labels. EMIXER is\nan conditional generative adversarial model by 1) generating an image based on\na label, 2) encoding the image to a hidden embedding, 3) producing the\ncorresponding text via a hierarchical decoder from the image embedding, and 4)\na joint discriminator for assessing both the image and the corresponding text.\nEMIXER also enables self-supervision to leverage vast amount of unlabeled data.\nExtensive experiments with real X-ray reports data illustrate how data\naugmentation using synthesized multimodal samples can improve the performance\nof a variety of supervised tasks including COVID-19 X-ray classification with\nvery limited samples. The quality of generated images and reports are also\nconfirmed by radiologists. We quantitatively show that EMIXER generated\nsynthetic datasets can augment X-ray image classification, report generation\nmodels to achieve 5.94% and 6.9% improvement on models trained only on real\ndata samples. Taken together, our results highlight the promise of state of\ngenerative models to advance clinical machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 20:19:01 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 19:07:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Biswal", "Siddharth", ""], ["Zhuang", "Peiye", ""], ["Pyrros", "Ayis", ""], ["Siddiqui", "Nasir", ""], ["Koyejo", "Sanmi", ""], ["Sun", "Jimeng", ""]]}, {"id": "2007.05606", "submitter": "Philippe Reiter", "authors": "Philippe Reiter, Geet Rose Jose, Spyridon Bizmpikis, Ionela-Ancu\\c{t}a\n  C\\^irjil\\u{a}", "title": "Neuromorphic Processing and Sensing: Evolutionary Progression of AI to\n  Spiking", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing rise in machine learning and deep learning applications is\nrequiring ever more computational resources to successfully meet the growing\ndemands of an always-connected, automated world. Neuromorphic technologies\nbased on Spiking Neural Network algorithms hold the promise to implement\nadvanced artificial intelligence using a fraction of the computations and power\nrequirements by modeling the functioning, and spiking, of the human brain. With\nthe proliferation of tools and platforms aiding data scientists and machine\nlearning engineers to develop the latest innovations in artificial and deep\nneural networks, a transition to a new paradigm will require building from the\ncurrent well-established foundations. This paper explains the theoretical\nworkings of neuromorphic technologies based on spikes, and overviews the\nstate-of-art in hardware processors, software platforms and neuromorphic\nsensing devices. A progression path is paved for current machine learning\nspecialists to update their skillset, as well as classification or predictive\nmodels from the current generation of deep neural networks to SNNs. This can be\nachieved by leveraging existing, specialized hardware in the form of SpiNNaker\nand the Nengo migration toolkit. First-hand, experimental results of converting\na VGG-16 neural network to an SNN are shared. A forward gaze into industrial,\nmedical and commercial applications that can readily benefit from SNNs wraps up\nthis investigation into the neuromorphic computing future.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 20:54:42 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Reiter", "Philippe", ""], ["Jose", "Geet Rose", ""], ["Bizmpikis", "Spyridon", ""], ["C\u00eerjil\u0103", "Ionela-Ancu\u0163a", ""]]}, {"id": "2007.05608", "submitter": "Jean Oh", "authors": "Junjiao Tian and Jean Oh", "title": "Image Captioning with Compositional Neural Module Networks", "comments": "International Joint Conference on Artificial Intelligence (IJCAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image captioning where fluency is an important factor in evaluation, e.g.,\n$n$-gram metrics, sequential models are commonly used; however, sequential\nmodels generally result in overgeneralized expressions that lack the details\nthat may be present in an input image. Inspired by the idea of the\ncompositional neural module networks in the visual question answering task, we\nintroduce a hierarchical framework for image captioning that explores both\ncompositionality and sequentiality of natural language. Our algorithm learns to\ncompose a detail-rich sentence by selectively attending to different modules\ncorresponding to unique aspects of each object detected in an input image to\ninclude specific descriptions such as counts and color. In a set of experiments\non the MSCOCO dataset, the proposed model outperforms a state-of-the art model\nacross multiple evaluation metrics, more importantly, presenting visually\ninterpretable results. Furthermore, the breakdown of subcategories $f$-scores\nof the SPICE metric and human evaluation on Amazon Mechanical Turk show that\nour compositional module networks effectively generate accurate and detailed\ncaptions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 20:58:04 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tian", "Junjiao", ""], ["Oh", "Jean", ""]]}, {"id": "2007.05610", "submitter": "Mark Crowley", "authors": "Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H.R.\n  Tizhoosh", "title": "Batch-Incremental Triplet Sampling for Training Triplet Networks Using\n  Bayesian Updating Theorem", "comments": "Accepted for presentation at the 25th International Conference on\n  Pattern Recognition (ICPR), IEEE, 2020. The first two authors contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variants of Triplet networks are robust entities for learning a\ndiscriminative embedding subspace. There exist different triplet mining\napproaches for selecting the most suitable training triplets. Some of these\nmining methods rely on the extreme distances between instances, and some others\nmake use of sampling. However, sampling from stochastic distributions of data\nrather than sampling merely from the existing embedding instances can provide\nmore discriminative information. In this work, we sample triplets from\ndistributions of data rather than from existing instances. We consider a\nmultivariate normal distribution for the embedding of each class. Using\nBayesian updating and conjugate priors, we update the distributions of classes\ndynamically by receiving the new mini-batches of training data. The proposed\ntriplet mining with Bayesian updating can be used with any triplet-based loss\nfunction, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss.\nAccordingly, Our triplet mining approaches are called Bayesian Updating Triplet\n(BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is\nbeing used. Experimental results on two public datasets, namely MNIST and\nhistopathology colorectal cancer (CRC), substantiate the effectiveness of the\nproposed triplet mining method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 21:07:51 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 14:13:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Sikaroudi", "Milad", ""], ["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2007.05615", "submitter": "Austin McEver", "authors": "R. Austin McEver and B.S. Manjunath", "title": "PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state of the art methods for generating semantic segmentation rely\nheavily on a large set of images that have each pixel labeled with a class of\ninterest label or background. Coming up with such labels, especially in domains\nthat require an expert to do annotations, comes at a heavy cost in time and\nmoney. Several methods have shown that we can learn semantic segmentation from\nless expensive image-level labels, but the effectiveness of point level labels,\na healthy compromise between all pixels labelled and none, still remains\nlargely unexplored. This paper presents a novel procedure for producing\nsemantic segmentation from images given some point level annotations. This\nmethod includes point annotations in the training of a convolutional neural\nnetwork (CNN) for producing improved localization and class activation maps.\nThen, we use another CNN for predicting semantic affinities in order to\npropagate rough class labels and create pseudo semantic segmentation labels.\nFinally, we propose training a CNN that is normally fully supervised using our\npseudo labels in place of ground truth labels, which further improves\nperformance and simplifies the inference process by requiring just one CNN\nduring inference rather than two. Our method achieves state of the art results\nfor point supervised semantic segmentation on the PASCAL VOC 2012 dataset\n\\cite{everingham2010pascal}, even outperforming state of the art methods for\nstronger bounding box and squiggle supervision.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 21:25:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["McEver", "R. Austin", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2007.05617", "submitter": "Yuhao Chen", "authors": "Yuhao Chen and Yifan Wu and Linlin Xu and Alexander Wong", "title": "Quantization in Relative Gradient Angle Domain For Building Polygon\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building footprint extraction in remote sensing data benefits many important\napplications, such as urban planning and population estimation. Recently, rapid\ndevelopment of Convolutional Neural Networks (CNNs) and open-sourced high\nresolution satellite building image datasets have pushed the performance\nboundary further for automated building extractions. However, CNN approaches\noften generate imprecise building morphologies including noisy edges and round\ncorners. In this paper, we leverage the performance of CNNs, and propose a\nmodule that uses prior knowledge of building corners to create angular and\nconcise building polygons from CNN segmentation outputs. We describe a new\ntransform, Relative Gradient Angle Transform (RGA Transform) that converts\nobject contours from time vs. space to time vs. angle. We propose a new shape\ndescriptor, Boundary Orientation Relation Set (BORS), to describe angle\nrelationship between edges in RGA domain, such as orthogonality and\nparallelism. Finally, we develop an energy minimization framework that makes\nuse of the angle relationship in BORS to straighten edges and reconstruct sharp\ncorners, and the resulting corners create a polygon. Experimental results\ndemonstrate that our method refines CNN output from a rounded approximation to\na more clear-cut angular shape of the building footprint.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 21:33:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chen", "Yuhao", ""], ["Wu", "Yifan", ""], ["Xu", "Linlin", ""], ["Wong", "Alexander", ""]]}, {"id": "2007.05643", "submitter": "Lucas Ribas", "authors": "Lucas C. Ribas, Leonardo F. S. Scabini, Jarbas Joaci de Mesquita S\\'a\n  Junior and Odemir M. Bruno", "title": "Learning Local Complex Features using Randomized Neural Networks for\n  Texture Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is a visual attribute largely used in many problems of image\nanalysis. Currently, many methods that use learning techniques have been\nproposed for texture discrimination, achieving improved performance over\nprevious handcrafted methods. In this paper, we present a new approach that\ncombines a learning technique and the Complex Network (CN) theory for texture\nanalysis. This method takes advantage of the representation capacity of CN to\nmodel a texture image as a directed network and uses the topological\ninformation of vertices to train a randomized neural network. This neural\nnetwork has a single hidden layer and uses a fast learning algorithm, which is\nable to learn local CN patterns for texture characterization. Thus, we use the\nweighs of the trained neural network to compose a feature vector. These feature\nvectors are evaluated in a classification experiment in four widely used image\ndatabases. Experimental results show a high classification performance of the\nproposed method when compared to other methods, indicating that our approach\ncan be used in many image analysis problems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 23:18:01 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 18:51:30 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ribas", "Lucas C.", ""], ["Scabini", "Leonardo F. S.", ""], ["Junior", "Jarbas Joaci de Mesquita S\u00e1", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "2007.05655", "submitter": "Zhiwei Deng", "authors": "Zhiwei Deng, Karthik Narasimhan, Olga Russakovsky", "title": "Evolving Graphical Planner: Contextual Global Planning for\n  Vision-and-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform effective planning is crucial for building an\ninstruction-following agent. When navigating through a new environment, an\nagent is challenged with (1) connecting the natural language instructions with\nits progressively growing knowledge of the world; and (2) performing long-range\nplanning and decision making in the form of effective exploration and error\ncorrection. Current methods are still limited on both fronts despite extensive\nefforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a\nmodel that performs global planning for navigation based on raw sensory input.\nThe model dynamically constructs a graphical representation, generalizes the\naction space to allow for more flexible decision making, and performs efficient\nplanning on a proxy graph representation. We evaluate our model on a\nchallenging Vision-and-Language Navigation (VLN) task with photorealistic\nimages and achieve superior performance compared to previous navigation\narchitectures. For instance, we achieve a 53% success rate on the test split of\nthe Room-to-Room navigation task through pure imitation learning, outperforming\nprevious navigation architectures by up to 5%.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 00:21:05 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Deng", "Zhiwei", ""], ["Narasimhan", "Karthik", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2007.05661", "submitter": "Xuequan Lu", "authors": "Dongbo Zhang, Zheng Fang, Xuequan Lu, Hong Qin, Antonio Robles-Kelly,\n  Chao Zhang, Ying He", "title": "Deep Patch-based Human Segmentation", "comments": "submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human segmentation has seen noticeable progress in re-cent years. It,\nhowever, still remains a challenge to date. In this paper, weintroduce a deep\npatch-based method for 3D human segmentation. Wefirst extract a local surface\npatch for each vertex and then parameterizeit into a 2D grid (or image). We\nthen embed identified shape descriptorsinto the 2D grids which are further fed\ninto the powerful 2D Convolu-tional Neural Network for regressing corresponding\nsemantic labels (e.g.,head, torso). Experiments demonstrate that our method is\neffective inhuman segmentation, and achieves state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 01:51:23 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhang", "Dongbo", ""], ["Fang", "Zheng", ""], ["Lu", "Xuequan", ""], ["Qin", "Hong", ""], ["Robles-Kelly", "Antonio", ""], ["Zhang", "Chao", ""], ["He", "Ying", ""]]}, {"id": "2007.05667", "submitter": "Sara Elkerdawy", "authors": "Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang and\n  Nilanjan Ray", "title": "To Filter Prune, or to Layer Prune, That Is The Question", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in pruning of neural networks have made it possible to remove\na large number of filters or weights without any perceptible drop in accuracy.\nThe number of parameters and that of FLOPs are usually the reported metrics to\nmeasure the quality of the pruned models. However, the gain in speed for these\npruned models is often overlooked in the literature due to the complex nature\nof latency measurements. In this paper, we show the limitation of filter\npruning methods in terms of latency reduction and propose LayerPrune framework.\nLayerPrune presents a set of layer pruning methods based on different criteria\nthat achieve higher latency reduction than filter pruning methods on similar\naccuracy. The advantage of layer pruning over filter pruning in terms of\nlatency reduction is a result of the fact that the former is not constrained by\nthe original model's depth and thus allows for a larger range of latency\nreduction. For each filter pruning method we examined, we use the same filter\nimportance criterion to calculate a per-layer importance score in one-shot. We\nthen prune the least important layers and fine-tune the shallower model which\nobtains comparable or better accuracy than its filter-based pruning\ncounterpart. This one-shot process allows to remove layers from single path\nnetworks like VGG before fine-tuning, unlike in iterative filter pruning, a\nminimum number of filters per layer is required to allow for data flow which\nconstraint the search space. To the best of our knowledge, we are the first to\nexamine the effect of pruning methods on latency metric instead of FLOPs for\nmultiple networks, datasets and hardware targets. LayerPrune also outperforms\nhandcrafted architectures such as Shufflenet, MobileNet, MNASNet and ResNet18\nby 7.3%, 4.6%, 2.8% and 0.5% respectively on similar latency budget on ImageNet\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 02:51:40 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 20:53:28 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 17:48:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Elkerdawy", "Sara", ""], ["Elhoushi", "Mostafa", ""], ["Singh", "Abhineet", ""], ["Zhang", "Hong", ""], ["Ray", "Nilanjan", ""]]}, {"id": "2007.05675", "submitter": "Jinhai Yang", "authors": "Jinhai Yang, Hua Yang, Lin Chen", "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine\n  Pseudo-Labeling with Visual-Semantic Meta-Embedding", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": "10.1145/3474085.3475200", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 03:44:21 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 02:46:54 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 12:39:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yang", "Jinhai", ""], ["Yang", "Hua", ""], ["Chen", "Lin", ""]]}, {"id": "2007.05676", "submitter": "Brent Griffin Dr", "authors": "Brent A. Griffin and Jason J. Corso", "title": "Learning Object Depth from Camera Motion and Video Object Segmentation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation, i.e., the separation of a target object from\nbackground in video, has made significant progress on real and challenging\nvideos in recent years. To leverage this progress in 3D applications, this\npaper addresses the problem of learning to estimate the depth of segmented\nobjects given some measurement of camera motion (e.g., from robot kinematics or\nvehicle odometry). We achieve this by, first, introducing a diverse, extensible\ndataset and, second, designing a novel deep network that estimates the depth of\nobjects using only segmentation masks and uncalibrated camera movement. Our\ndata-generation framework creates artificial object segmentations that are\nscaled for changes in distance between the camera and object, and our network\nlearns to estimate object depth even with segmentation errors. We demonstrate\nour approach across domains using a robot camera to locate objects from the YCB\ndataset and a vehicle camera to locate obstacles while driving.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 03:50:57 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 23:54:19 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 17:43:07 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Griffin", "Brent A.", ""], ["Corso", "Jason J.", ""]]}, {"id": "2007.05683", "submitter": "Zheda Mai", "authors": "Zheda Mai, Hyunwoo Kim, Jihwan Jeong, Scott Sanner", "title": "Batch-level Experience Replay with Review for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is a branch of deep learning that seeks to strike a\nbalance between learning stability and plasticity. The CVPR 2020 CLVision\nContinual Learning for Computer Vision challenge is dedicated to evaluating and\nadvancing the current state-of-the-art continual learning methods using the\nCORe50 dataset with three different continual learning scenarios. This paper\npresents our approach, called Batch-level Experience Replay with Review, to\nthis challenge. Our team achieved the 1'st place in all three scenarios out of\n79 participated teams. The codebase of our implementation is publicly available\nat https://github.com/RaptorMai/CVPR20_CLVision_challenge\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 05:20:09 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mai", "Zheda", ""], ["Kim", "Hyunwoo", ""], ["Jeong", "Jihwan", ""], ["Sanner", "Scott", ""]]}, {"id": "2007.05684", "submitter": "Yunxia Zhao", "authors": "Yunxia Zhao", "title": "Fast Real-time Counterfactual Explanations", "comments": "This paper has been accepted by ICML workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual explanations are considered, which is to answer {\\it why the\nprediction is class A but not B.} Different from previous optimization based\nmethods, an optimization-free Fast ReAl-time Counterfactual Explanation (FRACE)\nalgorithm is proposed benefiting from the development of multi-domain image to\nimage translation algorithms. Built from starGAN, a transformer is trained as a\nresidual generator conditional on a classifier constrained under a proposal\nperturbation loss which maintains the content information of the query image,\nbut just the class-specific semantic information is changed. The transformer\ncan transfer the query image to any counterfactual class, and during inference,\nour explanation can be generated by it only within a forward time. It is fast\nand can satisfy the real-time practical application. Because of the adversarial\ntraining of GAN, our explanation is also more realistic compared to other\ncounterparts. The experimental results demonstrate that our proposal is better\nthan the existing state of the art in terms of quality and speed.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 05:29:28 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 06:05:21 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhao", "Yunxia", ""]]}, {"id": "2007.05687", "submitter": "Xuhua Huang", "authors": "Xuhua Huang, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang", "title": "Fast Video Object Segmentation With Temporal Aggregation Network and\n  Dynamic Template Matching", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in Video Object Segmentation (VOS), the\nvideo object tracking task in its finest level. While the VOS task can be\nnaturally decoupled into image semantic segmentation and video object tracking,\nsignificantly much more research effort has been made in segmentation than\ntracking. In this paper, we introduce \"tracking-by-detection\" into VOS which\ncan coherently integrate segmentation into tracking, by proposing a new\ntemporal aggregation network and a novel dynamic time-evolving template\nmatching mechanism to achieve significantly improved performance. Notably, our\nmethod is entirely online and thus suitable for one-shot learning, and our\nend-to-end trainable model allows multiple object segmentation in one forward\npass. We achieve new state-of-the-art performance on the DAVIS benchmark\nwithout complicated bells and whistles in both speed and accuracy, with a speed\nof 0.14 second per frame and J&F measure of 75.9% respectively.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 05:44:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Huang", "Xuhua", ""], ["Xu", "Jiarui", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2007.05706", "submitter": "Zhi Chen", "authors": "Zhi Chen and Fan Yang and Wenbing Tao", "title": "Cascade Network with Guided Loss and Hybrid Attention for Two-view\n  Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are committed to designing a high-performance network for\ntwo-view geometry. We first propose a Guided Loss and theoretically establish\nthe direct negative correlation between the loss and Fn-measure by dynamically\nadjusting the weights of positive and negative classes during training, so that\nthe network is always trained towards the direction of increasing Fn-measure.\nBy this way, the network can maintain the advantage of the cross-entropy loss\nwhile maximizing the Fn-measure. We then propose a hybrid attention block to\nextract feature, which integrates the bayesian attentive context normalization\n(BACN) and channel-wise attention (CA). BACN can mine the prior information to\nbetter exploit global context and CA can capture complex channel context to\nenhance the channel awareness of the network. Finally, based on our Guided Loss\nand hybrid attention block, a cascade network is designed to gradually optimize\nthe result for more superior performance. Experiments have shown that our\nnetwork achieves the state-of-the-art performance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 07:44:04 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 03:03:22 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Zhi", ""], ["Yang", "Fan", ""], ["Tao", "Wenbing", ""]]}, {"id": "2007.05719", "submitter": "Yuexin Ma", "authors": "Yuexin Ma, Xinge ZHU, Xinjing Cheng, Ruigang Yang, Jiming Liu, Dinesh\n  Manocha", "title": "AutoTrajectory: Label-free Trajectory Extraction and Prediction from\n  Videos using Dynamic Points", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for trajectory prediction operate in supervised manners, and\ntherefore require vast quantities of corresponding ground truth data for\ntraining. In this paper, we present a novel, label-free algorithm,\nAutoTrajectory, for trajectory extraction and prediction to use raw videos\ndirectly. To better capture the moving objects in videos, we introduce dynamic\npoints. We use them to model dynamic motions by using a forward-backward\nextractor to keep temporal consistency and using image reconstruction to keep\nspatial consistency in an unsupervised manner. Then we aggregate dynamic points\nto instance points, which stand for moving objects such as pedestrians in\nvideos. Finally, we extract trajectories by matching instance points for\nprediction training. To the best of our knowledge, our method is the first to\nachieve unsupervised learning of trajectory extraction and prediction. We\nevaluate the performance on well-known trajectory datasets and show that our\nmethod is effective for real-world videos and can use raw videos to further\nimprove the performance of existing models.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 08:43:34 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ma", "Yuexin", ""], ["ZHU", "Xinge", ""], ["Cheng", "Xinjing", ""], ["Yang", "Ruigang", ""], ["Liu", "Jiming", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2007.05720", "submitter": "Fu Xiong", "authors": "Fu Xiong, Yang Xiao, Zhiguo Cao, Yancheng Wang, Joey Tianyi Zhou and\n  Jianxi Wu", "title": "ECML: An Ensemble Cascade Metric Learning Mechanism towards Face\n  Verification", "comments": "Accepted to IEEE Transaction on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification can be regarded as a 2-class fine-grained visual\nrecognition problem. Enhancing the feature's discriminative power is one of the\nkey problems to improve its performance. Metric learning technology is often\napplied to address this need, while achieving a good tradeoff between\nunderfitting and overfitting plays the vital role in metric learning. Hence, we\npropose a novel ensemble cascade metric learning (ECML) mechanism. In\nparticular, hierarchical metric learning is executed in the cascade way to\nalleviate underfitting. Meanwhile, at each learning level, the features are\nsplit into non-overlapping groups. Then, metric learning is executed among the\nfeature groups in the ensemble manner to resist overfitting. Considering the\nfeature distribution characteristics of faces, a robust Mahalanobis metric\nlearning method (RMML) with closed-form solution is additionally proposed. It\ncan avoid the computation failure issue on inverse matrix faced by some\nwell-known metric learning approaches (e.g., KISSME). Embedding RMML into the\nproposed ECML mechanism, our metric learning paradigm (EC-RMML) can run in the\none-pass learning manner. Experimental results demonstrate that EC-RMML is\nsuperior to state-of-the-art metric learning methods for face verification.\nAnd, the proposed ensemble cascade metric learning mechanism is also applicable\nto other metric learning approaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 08:47:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xiong", "Fu", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""], ["Wang", "Yancheng", ""], ["Zhou", "Joey Tianyi", ""], ["Wu", "Jianxi", ""]]}, {"id": "2007.05722", "submitter": "Takashi Oya", "authors": "Takashi Oya, Shohei Iwase, Ryota Natsume, Takahiro Itazuri, Shugo\n  Yamaguchi, Shigeo Morishima", "title": "Do We Need Sound for Sound Source Localization?", "comments": "Paper: 14 pages, 6 figures. Supplementary Material: 6 pages, 3\n  figures. Videos and Codes will be released later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the performance of sound source localization which uses both visual\nand aural information, it presently remains unclear how much either image or\nsound modalities contribute to the result, i.e. do we need both image and sound\nfor sound source localization? To address this question, we develop an\nunsupervised learning system that solves sound source localization by\ndecomposing this task into two steps: (i) \"potential sound source\nlocalization\", a step that localizes possible sound sources using only visual\ninformation (ii) \"object selection\", a step that identifies which objects are\nactually sounding using aural information. Our overall system achieves\nstate-of-the-art performance in sound source localization, and more\nimportantly, we find that despite the constraint on available information, the\nresults of (i) achieve similar performance. From this observation and further\nexperiments, we show that visual information is dominant in \"sound\" source\nlocalization when evaluated with the currently adopted benchmark dataset.\nMoreover, we show that the majority of sound-producing objects within the\nsamples in this dataset can be inherently identified using only visual\ninformation, and thus that the dataset is inadequate to evaluate a system's\ncapability to leverage aural information. As an alternative, we present an\nevaluation protocol that enforces both visual and aural information to be\nleveraged, and verify this property through several experiments.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 08:57:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Oya", "Takashi", ""], ["Iwase", "Shohei", ""], ["Natsume", "Ryota", ""], ["Itazuri", "Takahiro", ""], ["Yamaguchi", "Shugo", ""], ["Morishima", "Shigeo", ""]]}, {"id": "2007.05729", "submitter": "Koushik Nagasubramanian", "authors": "Koushik Nagasubramanian, Asheesh K. Singh, Arti Singh, Soumik Sarkar,\n  Baskar Ganapathysubramanian", "title": "Usefulness of interpretability methods to explain deep learning based\n  plant stress phenotyping", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been successfully deployed for automating plant\nstress identification and quantification. In recent years, there is a growing\npush towards training models that are interpretable -i.e. that justify their\nclassification decisions by visually highlighting image features that were\ncrucial for classification decisions. The expectation is that trained network\nmodels utilize image features that mimic visual cues used by plant\npathologists. In this work, we compare some of the most popular\ninterpretability methods: Saliency Maps, SmoothGrad, Guided Backpropogation,\nDeep Taylor Decomposition, Integrated Gradients, Layer-wise Relevance\nPropagation and Gradient times Input, for interpreting the deep learning model.\nWe train a DenseNet-121 network for the classification of eight different\nsoybean stresses (biotic and abiotic). Using a dataset consisting of 16,573 RGB\nimages of healthy and stressed soybean leaflets captured under controlled\nconditions, we obtained an overall classification accuracy of 95.05 \\%. For a\ndiverse subset of the test data, we compared the important features with those\nidentified by a human expert. We observed that most interpretability methods\nidentify the infected regions of the leaf as important features for some -- but\nnot all -- of the correctly classified images. For some images, the output of\nthe interpretability methods indicated that spurious feature correlations may\nhave been used to correctly classify them. Although the output explanation maps\nof these interpretability methods may be different from each other for a given\nimage, we advocate the use of these interpretability methods as `hypothesis\ngeneration' mechanisms that can drive scientific insight.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 09:28:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nagasubramanian", "Koushik", ""], ["Singh", "Asheesh K.", ""], ["Singh", "Arti", ""], ["Sarkar", "Soumik", ""], ["Ganapathysubramanian", "Baskar", ""]]}, {"id": "2007.05740", "submitter": "Narinder Punn", "authors": "Uppala Sumanth, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "Enhanced Behavioral Cloning Based self-driving Car Using Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing phase of artificial intelligence and autonomous learning,\nthe self-driving car is one of the promising area of research and emerging as a\ncenter of focus for automobile industries. Behavioral cloning is the process of\nreplicating human behavior via visuomotor policies by means of machine learning\nalgorithms. In recent years, several deep learning-based behavioral cloning\napproaches have been developed in the context of self-driving cars specifically\nbased on the concept of transfer learning. Concerning the same, the present\npaper proposes a transfer learning approach using VGG16 architecture, which is\nfine tuned by retraining the last block while keeping other blocks as\nnon-trainable. The performance of proposed architecture is further compared\nwith existing NVIDIA architecture and its pruned variants (pruned by 22.2% and\n33.85% using 1x1 filter to decrease the total number of parameters).\nExperimental results show that the VGG16 with transfer learning architecture\nhas outperformed other discussed approaches with faster convergence.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 10:44:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sumanth", "Uppala", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2007.05742", "submitter": "Zhao Kang", "authors": "Zhao Kang and Xiao Lu and Jian Liang and Kun Bai and Zenglin Xu", "title": "Relation-Guided Representation Learning", "comments": "Appear in Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep auto-encoders (DAEs) have achieved great success in learning data\nrepresentations via the powerful representability of neural networks. But most\nDAEs only focus on the most dominant structures which are able to reconstruct\nthe data from a latent space and neglect rich latent structural information. In\nthis work, we propose a new representation learning method that explicitly\nmodels and leverages sample relations, which in turn is used as supervision to\nguide the representation learning. Different from previous work, our framework\nwell preserves the relations between samples. Since the prediction of pairwise\nrelations themselves is a fundamental problem, our model adaptively learns them\nfrom data. This provides much flexibility to encode real data manifold. The\nimportant role of relation and representation learning is evaluated on the\nclustering task. Extensive experiments on benchmark data sets demonstrate the\nsuperiority of our approach. By seeking to embed samples into subspace, we\nfurther show that our method can address the large-scale and out-of-sample\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 10:57:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Liang", "Jian", ""], ["Bai", "Kun", ""], ["Xu", "Zenglin", ""]]}, {"id": "2007.05743", "submitter": "Manik Sharma", "authors": "Manik Sharma and Ganapathy Krishnamurthi", "title": "Distangling Biological Noise in Cellular Images with a focus on\n  Explainability", "comments": "13 Pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of some drugs and medical treatments has risen in recent years that\nmany patients are having to go without. A classification project could make\nresearchers more efficient.\n  One of the more surprising reasons behind the cost is how long it takes to\nbring new treatments to market. Despite improvements in technology and science,\nresearch and development continues to lag. In fact, finding new treatment\ntakes, on average, more than 10 years and costs hundreds of millions of\ndollars. In turn, greatly decreasing the cost of treatments can make ensure\nthese treatments get to patients faster. This work aims at solving a part of\nthis problem by creating a cellular image classification model which can\ndecipher the genetic perturbations in cell (occurring naturally or\nartificially). Another interesting question addressed is what makes the\ndeep-learning model decide in a particular fashion, which can further help in\ndemystifying the mechanism of action of certain perturbations and paves a way\ntowards the explainability of the deep-learning model.\n  We show the results of Grad-CAM visualizations and make a case for the\nsignificance of certain features over others. Further we discuss how these\nsignificant features are pivotal in extracting useful diagnostic information\nfrom the deep-learning model.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 11:04:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sharma", "Manik", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "2007.05751", "submitter": "Cheng Gong", "authors": "Zirui Li, Chao Lu, Cheng Gong, Cheng Gong, Jinghang Li, Lianzhen Wei", "title": "Driver Behavior Modelling at the Urban Intersection via Canonical\n  Correlation Analysis", "comments": "2020 3rd IEEE International Conference on Unmanned Systems (ICUS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The urban intersection is a typically dynamic and complex scenario for\nintelligent vehicles, which exists a variety of driving behaviors and traffic\nparticipants. Accurately modelling the driver behavior at the intersection is\nessential for intelligent transportation systems (ITS). Previous researches\nmainly focus on using attention mechanism to model the degree of correlation.\nIn this research, a canonical correlation analysis (CCA)-based framework is\nproposed. The value of canonical correlation is used for feature selection.\nGaussian mixture model and Gaussian process regression are applied for driver\nbehavior modelling. Two experiments using simulated and naturalistic driving\ndata are designed for verification. Experimental results are consistent with\nthe driver's judgment. Comparative studies show that the proposed framework can\nobtain a better performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 11:34:22 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Zirui", ""], ["Lu", "Chao", ""], ["Gong", "Cheng", ""], ["Gong", "Cheng", ""], ["Li", "Jinghang", ""], ["Wei", "Lianzhen", ""]]}, {"id": "2007.05756", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Harm de Vries, C\\u{a}t\\u{a}lina Cangea, Graham W.\n  Taylor, Aaron Courville, Eugene Belilovsky", "title": "Generative Compositional Augmentations for Scene Graph Prediction", "comments": "extended version with major updates; 19 pages; the code is available\n  at https://github.com/bknyaz/sgg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring objects and their relationships from an image in the form of a\nscene graph is useful in many applications at the intersection of vision and\nlanguage. In this work, we consider a challenging problem of compositional\ngeneralization that emerges in this task due to a long tail data distribution.\nCurrent scene graph generation models are trained on a tiny fraction of the\ndistribution corresponding to the most frequent compositions, e.g. <cup, on,\ntable>. However, test images might contain zero- and few-shot compositions of\nobjects and relationships, e.g. <cup, on, surfboard>. Despite each of the\nobject categories and the predicate (e.g. 'on') being frequent in the training\ndata, the models often fail to properly understand such unseen or rare\ncompositions. To improve generalization, it is natural to attempt increasing\nthe diversity of the training distribution. However, in the graph domain this\nis non-trivial. To that end, we propose a method to synthesize rare yet\nplausible scene graphs by perturbing real ones. We then propose and empirically\nstudy a model based on conditional generative adversarial networks (GANs) that\nallows us to generate visual features of perturbed scene graphs and learn from\nthem in a joint fashion. When evaluated on the Visual Genome dataset, our\napproach yields marginal, but consistent improvements in zero- and few-shot\nmetrics. We analyze the limitations of our approach indicating promising\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 12:11:53 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:42:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Knyazev", "Boris", ""], ["de Vries", "Harm", ""], ["Cangea", "C\u0103t\u0103lina", ""], ["Taylor", "Graham W.", ""], ["Courville", "Aaron", ""], ["Belilovsky", "Eugene", ""]]}, {"id": "2007.05779", "submitter": "Junhao Cheng", "authors": "Junhao Cheng, Zhuojun Chen, XinYu Zhang, Yizhou Li, Xiaoyuan Jing", "title": "Exploit the potential of Multi-column architecture for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is an important yet challenging task in computer vision due to\nserious occlusions, complex background and large scale variations, etc.\nMulti-column architecture is widely adopted to overcome these challenges,\nyielding state-of-the-art performance in many public benchmarks. However, there\nstill are two issues in such design: scale limitation and feature similarity.\nFurther performance improvements are thus restricted. In this paper, we propose\na novel crowd counting framework called Pyramid Scale Network (PSNet) to\nexplicitly address these issues. Specifically, for scale limitation, we adopt\nthree Pyramid Scale Modules (PSM) to efficiently capture multi-scale features,\nwhich integrate a message passing mechanism and an attention mechanism into\nmulti-column architecture. Moreover, for feature similarity, a novel loss\nfunction named Multi-column variance loss is introduced to make the features\nlearned by each column in PSM appropriately different from each other. To the\nbest of our knowledge, PSNet is the first work to explicitly address scale\nlimitation and feature similarity in multi-column design. Extensive experiments\non five benchmark datasets demonstrate the effectiveness of the proposed\ninnovations as well as the superior performance over the state-of-the-art. Our\ncode is publicly available at: https://github.com/oahunc/Pyramid_Scale_Network\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 14:08:25 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 09:52:38 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Cheng", "Junhao", ""], ["Chen", "Zhuojun", ""], ["Zhang", "XinYu", ""], ["Li", "Yizhou", ""], ["Jing", "Xiaoyuan", ""]]}, {"id": "2007.05785", "submitter": "Wei Fang", "authors": "Wei Fang, Zhaofei Yu, Yanqi Chen, Timothee Masquelier, Tiejun Huang,\n  Yonghong Tian", "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of\n  Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have attracted enormous research interest due\nto temporal information processing capability, low power consumption, and high\nbiological plausibility. However, the formulation of efficient and\nhigh-performance learning algorithms for SNNs is still challenging. Most\nexisting learning methods learn the synaptic-related parameters only, and\nrequire manual tuning of the membrane-related parameters that determine the\ndynamics of single spiking neurons. These parameters are typically chosen to be\nthe same for all neurons, which limits the diversity of neurons and thus the\nexpressiveness of the resulting SNNs. In this paper, we take inspiration from\nthe observation that membrane-related parameters are different across brain\nregions, and propose a training algorithm that is capable to learn not only the\nsynaptic weights but also the membrane time constants of SNN. We show that\nincorporating learnable membrane time constants can make the network less\nsensitive to initial values and can speed up learning. In addition, we\nreevaluate the pooling methods in SNNs and find that max-pooling is able to\nincrease the fitting capacity of SNNs in temporal tasks, as well as reduce the\ncomputation cost. We evaluate the proposed method for image classification\ntasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and\nneuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment\nresults show that the proposed method outperforms the state-of-the-art accuracy\non nearly all datasets, using fewer time-steps.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 14:35:42 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 16:08:36 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 12:47:40 GMT"}, {"version": "v4", "created": "Fri, 27 Nov 2020 05:23:29 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Fang", "Wei", ""], ["Yu", "Zhaofei", ""], ["Chen", "Yanqi", ""], ["Masquelier", "Timothee", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "2007.05786", "submitter": "Nazanin Mashhaditafreshi", "authors": "Nazanin Mashhaditafreshi, Amara Tariq, Judy Wawira Gichoya, Imon\n  Banerjee", "title": "Generalization of Deep Convolutional Neural Networks -- A Case-study on\n  Open-source Chest Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have attracted extensive attention\nand been applied in many areas, including medical image analysis and clinical\ndiagnosis. One major challenge is to conceive a DCNN model with remarkable\nperformance on both internal and external data. We demonstrate that DCNNs may\nnot generalize to new data, but increasing the quality and heterogeneity of the\ntraining data helps to improve the generalizibility factor. We use\nInceptionResNetV2 and DenseNet121 architectures to predict the risk of 5 common\nchest pathologies. The experiments were conducted on three publicly available\ndatabases: CheXpert, ChestX-ray14, and MIMIC Chest Xray JPG. The results show\nthe internal performance of each of the 5 pathologies outperformed external\nperformance on both of the models. Moreover, our strategy of exposing the\nmodels to a mix of different datasets during the training phase helps to\nimprove model performance on the external dataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 14:37:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Mashhaditafreshi", "Nazanin", ""], ["Tariq", "Amara", ""], ["Gichoya", "Judy Wawira", ""], ["Banerjee", "Imon", ""]]}, {"id": "2007.05791", "submitter": "Yue Liu", "authors": "Yue Liu, Hossein Azizpour, Fredrik Strand, Kevin Smith", "title": "Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast\n  Cancer Risk Models", "comments": "Accepted by MICCAI 2020 (Medical Image Computing and Computer\n  Assisted Intervention)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately estimate risk of developing breast cancer would be\ninvaluable for clinical decision-making. One promising new approach is to\nintegrate image-based risk models based on deep neural networks. However, one\nmust take care when using such models, as selection of training data influences\nthe patterns the network will learn to identify. With this in mind, we trained\nnetworks using three different criteria to select the positive training data\n(i.e. images from patients that will develop cancer): an inherent risk model\ntrained on images with no visible signs of cancer, a cancer signs model trained\non images containing cancer or early signs of cancer, and a conflated model\ntrained on all images from patients with a cancer diagnosis. We find that these\nthree models learn distinctive features that focus on different patterns, which\ntranslates to contrasts in performance. Short-term risk is best estimated by\nthe cancer signs model, whilst long-term risk is best estimated by the inherent\nrisk model. Carelessly training with all images conflates inherent risk with\nearly cancer signs, and yields sub-optimal estimates in both regimes. As a\nconsequence, conflated models may lead physicians to recommend preventative\naction when early cancer signs are already visible.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 15:04:28 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 12:24:21 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 08:01:14 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 12:33:52 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Liu", "Yue", ""], ["Azizpour", "Hossein", ""], ["Strand", "Fredrik", ""], ["Smith", "Kevin", ""]]}, {"id": "2007.05828", "submitter": "Ka-Ho Chow", "authors": "Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei,\n  Yanzhao Wu", "title": "Understanding Object Detection Through An Adversarial Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks based object detection models have revolutionized\ncomputer vision and fueled the development of a wide range of visual\nrecognition applications. However, recent studies have revealed that deep\nobject detectors can be compromised under adversarial attacks, causing a victim\ndetector to detect no object, fake objects, or mislabeled objects. With object\ndetection being used pervasively in many security-critical applications, such\nas autonomous vehicles and smart cities, we argue that a holistic approach for\nan in-depth understanding of adversarial attacks and vulnerabilities of deep\nobject detection systems is of utmost importance for the research community to\ndevelop robust defense mechanisms. This paper presents a framework for\nanalyzing and evaluating vulnerabilities of the state-of-the-art object\ndetectors under an adversarial lens, aiming to analyze and demystify the attack\nstrategies, adverse effects, and costs, as well as the cross-model and\ncross-resolution transferability of attacks. Using a set of quantitative\nmetrics, extensive experiments are performed on six representative deep object\ndetectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two\nbenchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed\nframework can serve as a methodical benchmark for analyzing adversarial\nbehaviors and risks in real-time object detection systems. We conjecture that\nthis framework can also serve as a tool to assess the security risks and the\nadversarial robustness of deep object detectors to be deployed in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 18:41:47 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chow", "Ka-Ho", ""], ["Liu", "Ling", ""], ["Gursoy", "Mehmet Emre", ""], ["Truex", "Stacey", ""], ["Wei", "Wenqi", ""], ["Wu", "Yanzhao", ""]]}, {"id": "2007.05830", "submitter": "M. F. Mridha", "authors": "Abu Quwsar Ohi, M. F. Mridha, Farisa Benta Safir, Md. Abdul Hamid,\n  Muhammad Mostafa Monowar", "title": "AutoEmbedder: A semi-supervised DNN embedding system for clustering", "comments": "The manuscript is accepted and published in Knowledge-Based System", "journal-ref": "Knowledge-Based Systems, p.106190 (2020)", "doi": "10.1016/j.knosys.2020.106190", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is widely used in unsupervised learning method that deals with\nunlabeled data. Deep clustering has become a popular study area that relates\nclustering with Deep Neural Network (DNN) architecture. Deep clustering method\ndownsamples high dimensional data, which may also relate clustering loss. Deep\nclustering is also introduced in semi-supervised learning (SSL). Most SSL\nmethods depend on pairwise constraint information, which is a matrix containing\nknowledge if data pairs can be in the same cluster or not. This paper\nintroduces a novel embedding system named AutoEmbedder, that downsamples higher\ndimensional data to clusterable embedding points. To the best of our knowledge,\nthis is the first research endeavor that relates to traditional classifier DNN\narchitecture with a pairwise loss reduction technique. The training process is\nsemi-supervised and uses Siamese network architecture to compute pairwise\nconstraint loss in the feature learning phase. The AutoEmbedder outperforms\nmost of the existing DNN based semi-supervised methods tested on famous\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:00:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""], ["Safir", "Farisa Benta", ""], ["Hamid", "Md. Abdul", ""], ["Monowar", "Muhammad Mostafa", ""]]}, {"id": "2007.05835", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Sourav Bairagya, Sutanu Bera, Siddhant Haldar, Prabir\n  Kumar Biswas", "title": "Lightweight Modules for Efficient Deep Learning based Image Restoration", "comments": "Accepted at: IEEE Transactions on Circuits and Systems for Video\n  Technology (Early Access Print) | |Codes Available at:\n  https://github.com/avisekiit/TCSVT-LightWeight-CNNs | Supplementary Document\n  at:\n  https://drive.google.com/file/d/1BQhkh33Sen-d0qOrjq5h8ahw2VCUIVLg/view?usp=sharing", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3007723", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low level image restoration is an integral component of modern artificial\nintelligence (AI) driven camera pipelines. Most of these frameworks are based\non deep neural networks which present a massive computational overhead on\nresource constrained platform like a mobile phone. In this paper, we propose\nseveral lightweight low-level modules which can be used to create a\ncomputationally low cost variant of a given baseline model. Recent works for\nefficient neural networks design have mainly focused on classification.\nHowever, low-level image processing falls under the image-to-image' translation\ngenre which requires some additional computational modules not present in\nclassification. This paper seeks to bridge this gap by designing generic\nefficient modules which can replace essential components used in contemporary\ndeep learning based image restoration networks. We also present and analyse our\nresults highlighting the drawbacks of applying depthwise separable\nconvolutional kernel (a popular method for efficient classification network)\nfor sub-pixel convolution based upsampling (a popular upsampling strategy for\nlow-level vision applications). This shows that concepts from domain of\nclassification cannot always be seamlessly integrated into image-to-image\ntranslation tasks. We extensively validate our findings on three popular tasks\nof image inpainting, denoising and super-resolution. Our results show that\nproposed networks consistently output visually similar reconstructions compared\nto full capacity baselines with significant reduction of parameters, memory\nfootprint and execution speeds on contemporary mobile devices.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:35:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lahiri", "Avisek", ""], ["Bairagya", "Sourav", ""], ["Bera", "Sutanu", ""], ["Haldar", "Siddhant", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "2007.05836", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, Ilkay Ulusoy", "title": "Meta Soft Label Generation for Noisy Labels", "comments": "Accepted by ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of noisy labels in the dataset causes significant performance\ndegradation for deep neural networks (DNNs). To address this problem, we\npropose a Meta Soft Label Generation algorithm called MSLG, which can jointly\ngenerate soft labels using meta-learning techniques and learn DNN parameters in\nan end-to-end fashion. Our approach adapts the meta-learning paradigm to\nestimate optimal label distribution by checking gradient directions on both\nnoisy training data and noise-free meta-data. In order to iteratively update\nsoft labels, meta-gradient descent step is performed on estimated labels, which\nwould minimize the loss of noise-free meta samples. In each iteration, the base\nclassifier is trained on estimated meta labels. MSLG is model-agnostic and can\nbe added on top of any existing model at hand with ease. We performed extensive\nexperiments on CIFAR10, Clothing1M and Food101N datasets. Results show that our\napproach outperforms other state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:37:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 09:59:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "Ilkay", ""]]}, {"id": "2007.05840", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Shuchin Aeron", "title": "Representation Learning via Adversarially-Contrastive Optimal Transport", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning compact (low-dimensional)\nrepresentations for sequential data that captures its implicit spatio-temporal\ncues. To maximize extraction of such informative cues from the data, we set the\nproblem within the context of contrastive representation learning and to that\nend propose a novel objective via optimal transport. Specifically, our\nformulation seeks a low-dimensional subspace representation of the data that\njointly (i) maximizes the distance of the data (embedded in this subspace) from\nan adversarial data distribution under the optimal transport, a.k.a. the\nWasserstein distance, (ii) captures the temporal order, and (iii) minimizes the\ndata distortion. To generate the adversarial distribution, we propose a novel\nframework connecting Wasserstein GANs with a classifier, allowing a principled\nmechanism for producing good negative distributions for contrastive learning,\nwhich is currently a challenging problem. Our full objective is cast as a\nsubspace learning problem on the Grassmann manifold and solved via Riemannian\noptimization. To empirically study our formulation, we provide experiments on\nthe task of human action recognition in video sequences. Our results\ndemonstrate competitive performance against challenging baselines.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:46:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Cherian", "Anoop", ""], ["Aeron", "Shuchin", ""]]}, {"id": "2007.05853", "submitter": "Rajesh Kumar Muthu", "authors": "Ritin Raveendran, Aviral Singh, Rajesh Kumar M", "title": "Complex Wavelet SSIM based Image Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest problems in neural learning networks is the lack of\ntraining data available to train the network. Data augmentation techniques over\nthe past few years, have therefore been developed, aiming to increase the\namount of artificial training data with the limited number of real world\nsamples. In this paper, we look particularly at the MNIST handwritten dataset\nan image dataset used for digit recognition, and the methods of data\naugmentation done on this data set. We then take a detailed look into one of\nthe most popular augmentation techniques used for this data set elastic\ndeformation; and highlight its demerit of degradation in the quality of data,\nwhich introduces irrelevant data to the training set. To decrease this\nirrelevancy, we propose to use a similarity measure called Complex Wavelet\nStructural Similarity Index Measure (CWSSIM) to selectively filter out the\nirrelevant data before we augment the data set. We compare our observations\nwith the existing augmentation technique and find our proposed method works\nyields better results than the existing technique.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:11:46 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Raveendran", "Ritin", ""], ["Singh", "Aviral", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "2007.05854", "submitter": "Bapireddy Karri", "authors": "Bapireddy Karri", "title": "Efficient resource management in UAVs for Visual Assistance", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increased interest in the use of Unmanned Aerial Vehicles (UAVs)\nfor agriculture, military, disaster management and aerial photography around\nthe world. UAVs are scalable, flexible and are useful in various environments\nwhere direct human intervention is difficult. In general, the use of UAVs with\ncameras mounted to them has increased in number due to their wide range of\napplications in real life scenarios. With the advent of deep learning models in\ncomputer vision many models have shown great success in visual tasks. But most\nof evaluation models are done on high end CPUs and GPUs. One of major\nchallenges in using UAVs for Visual Assistance tasks in real time is managing\nthe memory usage and power consumption of the these tasks which are\ncomputationally intensive and are difficult to be performed on low end\nprocessor board of the UAV. This projects describes a novel method to optimize\nthe general image processing tasks like object tracking and object detection\nfor UAV hardware in real time scenarios without affecting the flight time and\nnot tampering the latency and accuracy of these models.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:12:24 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 11:15:19 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 17:12:26 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Karri", "Bapireddy", ""]]}, {"id": "2007.05856", "submitter": "Yashasvi Baweja", "authors": "Yashasvi Baweja, Poojan Oza, Pramuditha Perera and Vishal M. Patel", "title": "Anomaly Detection-Based Unknown Face Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection-based spoof attack detection is a recent development in\nface Presentation Attack Detection (fPAD), where a spoof detector is learned\nusing only non-attacked images of users. These detectors are of practical\nimportance as they are shown to generalize well to new attack types. In this\npaper, we present a deep-learning solution for anomaly detection-based spoof\nattack detection where both classifier and feature representations are learned\ntogether end-to-end. First, we introduce a pseudo-negative class during\ntraining in the absence of attacked images. The pseudo-negative class is\nmodeled using a Gaussian distribution whose mean is calculated by a weighted\nrunning mean. Secondly, we use pairwise confusion loss to further regularize\nthe training process. The proposed approach benefits from the representation\nlearning power of the CNNs and learns better features for fPAD task as shown in\nour ablation study. We perform extensive experiments on four publicly available\ndatasets: Replay-Attack, Rose-Youtu, OULU-NPU and Spoof in Wild to show the\neffectiveness of the proposed approach over the previous methods. Code is\navailable at: \\url{https://github.com/yashasvi97/IJCB2020_anomaly}\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:20:55 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Baweja", "Yashasvi", ""], ["Oza", "Poojan", ""], ["Perera", "Pramuditha", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2007.05887", "submitter": "Feiyu Yang", "authors": "Feiyu Yang, Zhan Song, Zhenzhong Xiao, Yu Chen, Zhe Pan, Min Zhang,\n  Min Xue, Yaoyang Mo, Yao Zhang, Guoxiong Guan, Beibei Qian", "title": "Train Your Data Processor: Distribution-Aware and Error-Compensation\n  Coordinate Decoding for Human Pose Estimation", "comments": "Improve the state-of-the-art of COCO keypoint detection challenge by\n  1-2 AP. Project page: https://github.com/fyang235/DAEC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the leading performance of human pose estimation is dominated by\nheatmap based methods. While being a fundamental component of heatmap\nprocessing, heatmap decoding (i.e. transforming heatmaps to coordinates)\nreceives only limited investigations, to our best knowledge. This work fills\nthe gap by studying the heatmap decoding processing with a particular focus on\nthe errors introduced throughout the prediction process. We found that the\nerrors of heatmap based methods are surprisingly significant, which\nnevertheless was universally ignored before. In view of the discovered\nimportance, we further reveal the intrinsic limitations of the previous widely\nused heatmap decoding methods and thereout propose a Distribution-Aware and\nError-Compensation Coordinate Decoding (DAEC). Serving as a model-agnostic\nplug-in, DAEC learns its decoding strategy from training data and remarkably\nimproves the performance of a variety of state-of-the-art human pose estimation\nmodels with negligible extra computation. Specifically, equipped with DAEC, the\nSimpleBaseline-ResNet152-256x192 and HRNet-W48-256x192 are significantly\nimproved by 2.6 AP and 2.9 AP achieving 72.6 AP and 75.7 AP on COCO,\nrespectively. Moreover, the HRNet-W32-256x256 and ResNet-152-256x256 frameworks\nenjoy even more dramatic promotions of 8.4% and 7.8% on MPII with PCKh0.1\nmetric. Extensive experiments performed on these two common benchmarks,\ndemonstrates that DAEC exceeds its competitors by considerable margins, backing\nup the rationality and generality of our novel heatmap decoding idea. The\nproject is available at https://github.com/fyang235/DAEC.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 02:17:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 05:55:57 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 03:18:08 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 04:03:25 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yang", "Feiyu", ""], ["Song", "Zhan", ""], ["Xiao", "Zhenzhong", ""], ["Chen", "Yu", ""], ["Pan", "Zhe", ""], ["Zhang", "Min", ""], ["Xue", "Min", ""], ["Mo", "Yaoyang", ""], ["Zhang", "Yao", ""], ["Guan", "Guoxiong", ""], ["Qian", "Beibei", ""]]}, {"id": "2007.05892", "submitter": "Zhenliang He", "authors": "Zhenliang He, Meina Kan, Jichao Zhang, Shiguang Shan", "title": "PA-GAN: Progressive Attention Generative Adversarial Network for Facial\n  Attribute Editing", "comments": "Code: https://github.com/LynnHo/PA-GAN-Tensorflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute editing aims to manipulate attributes on the human face,\ne.g., adding a mustache or changing the hair color. Existing approaches suffer\nfrom a serious compromise between correct attribute generation and preservation\nof the other information such as identity and background, because they edit the\nattributes in the imprecise area. To resolve this dilemma, we propose a\nprogressive attention GAN (PA-GAN) for facial attribute editing. In our\napproach, the editing is progressively conducted from high to low feature level\nwhile being constrained inside a proper attribute area by an attention mask at\neach level. This manner prevents undesired modifications to the irrelevant\nregions from the beginning, and then the network can focus more on correctly\ngenerating the attributes within a proper boundary at each level. As a result,\nour approach achieves correct attribute editing with irrelevant details much\nbetter preserved compared with the state-of-the-arts. Codes are released at\nhttps://github.com/LynnHo/PA-GAN-Tensorflow.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 03:04:12 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["He", "Zhenliang", ""], ["Kan", "Meina", ""], ["Zhang", "Jichao", ""], ["Shan", "Shiguang", ""]]}, {"id": "2007.05906", "submitter": "Khawar Islam Mr", "authors": "Khawar Islam, Uzma Afzal", "title": "Framework for Passenger Seat Availability Using Face Detection in\n  Passenger Bus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advancements in Intelligent Transportation System (IES) improve passenger\ntraveling by providing information systems for bus arrival time and counting\nthe number of passengers and buses in cities. Passengers still face bus waiting\nand seat unavailability issues which have adverse effects on traffic management\nand controlling authority. We propose a Face Detection based Framework (FDF) to\ndetermine passenger seat availability in a camera-equipped bus through face\ndetection which is based on background subtraction to count empty, filled, and\ntotal seats. FDF has an integrated smartphone Passenger Application (PA) to\nidentify the nearest bus stop. We evaluate FDF in a live test environment and\nresults show that it gives 90% accuracy. We believe our results have the\npotential to address traffic management concerns and assist passengers to save\ntheir valuable time\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 04:31:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Islam", "Khawar", ""], ["Afzal", "Uzma", ""]]}, {"id": "2007.05914", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data\n  Analysis", "comments": "Accepted for Publication at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the analysis of imagery of the Gastrointestinal (GI) tract\ncaptured during endoscopy procedures has substantial potential benefits for\npatients, as it can provide diagnostic support to medical practitioners and\nreduce mistakes via human error. To further the development of such methods, we\npropose a two-stream model for endoscopic image analysis. Our model fuses two\nstreams of deep feature inputs by mapping their inherent relations through a\nnovel relational network model, to better model symptoms and classify the\nimage. In contrast to handcrafted feature-based models, our proposed network is\nable to learn features automatically and outperforms existing state-of-the-art\nmethods on two public datasets: KVASIR and Nerthus. Our extensive evaluations\nillustrate the importance of having two streams of inputs instead of a single\nstream and also demonstrates the merits of the proposed relational network\narchitecture to combine those streams.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 05:24:08 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2007.05932", "submitter": "Liang Guang", "authors": "Guang Liang, Shangfei Wang, Can Wang", "title": "Pose-aware Adversarial Domain Adaptation for Personalized Facial\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current facial expression recognition methods fail to simultaneously cope\nwith pose and subject variations.\n  In this paper, we propose a novel unsupervised adversarial domain adaptation\nmethod which can alleviate both variations at the same time. Specially, our\nmethod consists of three learning strategies: adversarial domain adaptation\nlearning, cross adversarial feature learning, and reconstruction learning. The\nfirst aims to learn pose- and expression-related feature representations in the\nsource domain and adapt both feature distributions to that of the target domain\nby imposing adversarial learning. By using personalized adversarial domain\nadaptation, this learning strategy can alleviate subject variations and exploit\ninformation from the source domain to help learning in the target domain.\n  The second serves to perform feature disentanglement between pose- and\nexpression-related feature representations by impulsing pose-related feature\nrepresentations expression-undistinguished and the expression-related feature\nrepresentations pose-undistinguished.\n  The last can further boost feature learning by applying face image\nreconstructions so that the learned expression-related feature representations\nare more pose- and identity-robust.\n  Experimental results on four benchmark datasets demonstrate the effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 07:58:31 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liang", "Guang", ""], ["Wang", "Shangfei", ""], ["Wang", "Can", ""]]}, {"id": "2007.05934", "submitter": "Chenyang Si", "authors": "Chenyang Si, Xuecheng Nie, Wei Wang, Liang Wang, Tieniu Tan, Jiashi\n  Feng", "title": "Adversarial Self-Supervised Learning for Semi-Supervised 3D Action\n  Recognition", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of semi-supervised 3D action recognition which has\nbeen rarely explored before. Its major challenge lies in how to effectively\nlearn motion representations from unlabeled data. Self-supervised learning\n(SSL) has been proved very effective at learning representations from unlabeled\ndata in the image domain. However, few effective self-supervised approaches\nexist for 3D action recognition, and directly applying SSL for semi-supervised\nlearning suffers from misalignment of representations learned from SSL and\nsupervised learning tasks. To address these issues, we present Adversarial\nSelf-Supervised Learning (ASSL), a novel framework that tightly couples SSL and\nthe semi-supervised scheme via neighbor relation exploration and adversarial\nlearning. Specifically, we design an effective SSL scheme to improve the\ndiscrimination capability of learned representations for 3D action recognition,\nthrough exploring the data relations within a neighborhood. We further propose\nan adversarial regularization to align the feature distributions of labeled and\nunlabeled samples. To demonstrate effectiveness of the proposed ASSL in\nsemi-supervised 3D action recognition, we conduct extensive experiments on NTU\nand N-UCLA datasets. The results confirm its advantageous performance over\nstate-of-the-art semi-supervised methods in the few label regime for 3D action\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 08:01:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Si", "Chenyang", ""], ["Nie", "Xuecheng", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""], ["Feng", "Jiashi", ""]]}, {"id": "2007.05942", "submitter": "Narinder Punn", "authors": "Mohit Dandekar, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "Fruit classification using deep feature maps in the presence of\n  deceptive similar classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous detection and classification of objects are admired area of\nresearch in many industrial applications. Though, humans can distinguish\nobjects with high multi-granular similarities very easily; but for the\nmachines, it is a very challenging task. The convolution neural networks (CNN)\nhave illustrated efficient performance in multi-level representations of\nobjects for classification. Conventionally, the existing deep learning models\nutilize the transformed features generated by the rearmost layer for training\nand testing. However, it is evident that this does not work well with\nmulti-granular data, especially, in presence of deceptive similar classes\n(almost similar but different classes). The objective of the present research\nis to address the challenge of classification of deceptively similar\nmulti-granular objects with an ensemble approach thfat utilizes activations\nfrom multiple layers of CNN (deep features). These multi-layer activations are\nfurther utilized to build multiple deep decision trees (known as Random forest)\nfor classification of objects with similar appearance. The Fruits-360 dataset\nis utilized for evaluation of the proposed approach. With extensive trials it\nwas observed that the proposed model outperformed over the conventional deep\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 09:01:57 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dandekar", "Mohit", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2007.05946", "submitter": "Zongsheng Yue", "authors": "Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng", "title": "Dual Adversarial Network: Toward Real-world Noise Removal and Noise\n  Generation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-world image noise removal is a long-standing yet very challenging task\nin computer vision. The success of deep neural network in denoising stimulates\nthe research of noise generation, aiming at synthesizing more clean-noisy image\npairs to facilitate the training of deep denoisers. In this work, we propose a\nnovel unified framework to simultaneously deal with the noise removal and noise\ngeneration tasks. Instead of only inferring the posteriori distribution of the\nlatent clean image conditioned on the observed noisy image in traditional MAP\nframework, our proposed method learns the joint distribution of the clean-noisy\nimage pairs. Specifically, we approximate the joint distribution with two\ndifferent factorized forms, which can be formulated as a denoiser mapping the\nnoisy image to the clean one and a generator mapping the clean image to the\nnoisy one. The learned joint distribution implicitly contains all the\ninformation between the noisy and clean images, avoiding the necessity of\nmanually designing the image priors and noise assumptions as traditional.\nBesides, the performance of our denoiser can be further improved by augmenting\nthe original training dataset with the learned generator. Moreover, we propose\ntwo metrics to assess the quality of the generated noisy image, for which, to\nthe best of our knowledge, such metrics are firstly proposed along this\nresearch line. Extensive experiments have been conducted to demonstrate the\nsuperiority of our method over the state-of-the-arts both in the real noise\nremoval and generation tasks. The training and testing code is available at\nhttps://github.com/zsyOAOA/DANet.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 09:16:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yue", "Zongsheng", ""], ["Zhao", "Qian", ""], ["Zhang", "Lei", ""], ["Meng", "Deyu", ""]]}, {"id": "2007.05950", "submitter": "Hengli Wang", "authors": "Hengli Wang, Yuxiang Sun, Ming Liu", "title": "Self-Supervised Drivable Area and Road Anomaly Segmentation using RGB-D\n  Data for Robotic Wheelchairs", "comments": "Published in IEEE Robotics and Automation Letters (RA-L); 8 pages, 8\n  figures and 3 tables", "journal-ref": null, "doi": "10.1109/LRA.2019.2932874", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of drivable areas and road anomalies are critical\ncapabilities to achieve autonomous navigation for robotic wheelchairs. The\nrecent progress of semantic segmentation using deep learning techniques has\npresented effective results. However, the acquisition of large-scale datasets\nwith hand-labeled ground truth is time-consuming and labor-intensive, making\nthe deep learning-based methods often hard to implement in practice. We\ncontribute to the solution of this problem for the task of drivable area and\nroad anomaly segmentation by proposing a self-supervised learning approach. We\ndevelop a pipeline that can automatically generate segmentation labels for\ndrivable areas and road anomalies. Then, we train RGB-D data-based semantic\nsegmentation neural networks and get predicted labels. Experimental results\nshow that our proposed automatic labeling pipeline achieves an impressive\nspeed-up compared to manual labeling. In addition, our proposed self-supervised\napproach exhibits more robust and accurate results than the state-of-the-art\ntraditional algorithms as well as the state-of-the-art self-supervised\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 10:12:46 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Hengli", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2007.05981", "submitter": "Zhen Li", "authors": "Di Xu, Zhen Li, Yanning Zhang, Qi Cao", "title": "IllumiNet: Transferring Illumination from Planar Surfaces to Virtual\n  Objects in Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an illumination estimation method for virtual objects in\nreal environment by learning. While previous works tackled this problem by\nreconstructing high dynamic range (HDR) environment maps or the corresponding\nspherical harmonics, we do not seek to recover the lighting environment of the\nentire scene. Given a single RGB image, our method directly infers the relit\nvirtual object by transferring the illumination features extracted from planar\nsurfaces in the scene to the desired geometries. Compared to previous works,\nour approach is more robust as it works in both indoor and outdoor environments\nwith spatially-varying illumination. Experiments and evaluation results show\nthat our approach outperforms the state-of-the-art quantitatively and\nqualitatively, achieving realistic augmented experience.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 13:11:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xu", "Di", ""], ["Li", "Zhen", ""], ["Zhang", "Yanning", ""], ["Cao", "Qi", ""]]}, {"id": "2007.05993", "submitter": "Chen Qin", "authors": "Chen Qin, Jo Schlemper, Kerstin Hammernik, Jinming Duan, Ronald M\n  Summers, and Daniel Rueckert", "title": "Deep Network Interpolation for Accelerated Parallel MR Image\n  Reconstruction", "comments": "Presented at 2020 ISMRM Conference & Exhibition (Abstract #4958)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep network interpolation strategy for accelerated parallel MR\nimage reconstruction. In particular, we examine the network interpolation in\nparameter space between a source model that is formulated in an unrolled scheme\nwith L1 and SSIM losses and its counterpart that is trained with an adversarial\nloss. We show that by interpolating between the two different models of the\nsame network structure, the new interpolated network can model a trade-off\nbetween perceptual quality and fidelity.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 13:58:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Qin", "Chen", ""], ["Schlemper", "Jo", ""], ["Hammernik", "Kerstin", ""], ["Duan", "Jinming", ""], ["Summers", "Ronald M", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2007.05996", "submitter": "Suren Jayasuriya", "authors": "John Janiczek, Parth Thaker, Gautam Dasarathy, Christopher S. Edwards,\n  Philip Christensen, Suren Jayasuriya", "title": "Differentiable Programming for Hyperspectral Unmixing using a\n  Physics-based Dispersion Model", "comments": "36 pages, 11 figures. Accepted to European Conference on Computer\n  Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing is an important remote sensing task with applications\nincluding material identification and analysis. Characteristic spectral\nfeatures make many pure materials identifiable from their visible-to-infrared\nspectra, but quantifying their presence within a mixture is a challenging task\ndue to nonlinearities and factors of variation. In this paper, spectral\nvariation is considered from a physics-based approach and incorporated into an\nend-to-end spectral unmixing algorithm via differentiable programming. The\ndispersion model is introduced to simulate realistic spectral variation, and an\nefficient method to fit the parameters is presented. Then, this dispersion\nmodel is utilized as a generative model within an analysis-by-synthesis\nspectral unmixing algorithm. Further, a technique for inverse rendering using a\nconvolutional neural network to predict parameters of the generative model is\nintroduced to enhance performance and speed when training data is available.\nResults achieve state-of-the-art on both infrared and visible-to-near-infrared\n(VNIR) datasets, and show promise for the synergy between physics-based models\nand deep learning in hyperspectral unmixing in the future.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 14:16:35 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Janiczek", "John", ""], ["Thaker", "Parth", ""], ["Dasarathy", "Gautam", ""], ["Edwards", "Christopher S.", ""], ["Christensen", "Philip", ""], ["Jayasuriya", "Suren", ""]]}, {"id": "2007.06002", "submitter": "Yige Peng", "authors": "Yige Peng, Lei Bi, Michael Fulham, Dagan Feng, and Jinman Kim", "title": "Multi-Modality Information Fusion for Radiomics-based Neural\n  Architecture Search", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Radiomics' is a method that extracts mineable quantitative features from\nradiographic images. These features can then be used to determine prognosis,\nfor example, predicting the development of distant metastases (DM). Existing\nradiomics methods, however, require complex manual effort including the design\nof hand-crafted radiomic features and their extraction and selection. Recent\nradiomics methods, based on convolutional neural networks (CNNs), also require\nmanual input in network architecture design and hyper-parameter tuning.\nRadiomic complexity is further compounded when there are multiple imaging\nmodalities, for example, combined positron emission tomography - computed\ntomography (PET-CT) where there is functional information from PET and\ncomplementary anatomical localization information from computed tomography\n(CT). Existing multi-modality radiomics methods manually fuse the data that are\nextracted separately. Reliance on manual fusion often results in sub-optimal\nfusion because they are dependent on an 'expert's' understanding of medical\nimages. In this study, we propose a multi-modality neural architecture search\nmethod (MM-NAS) to automatically derive optimal multi-modality image features\nfor radiomics and thus negate the dependence on a manual process. We evaluated\nour MM-NAS on the ability to predict DM using a public PET-CT dataset of\npatients with soft-tissue sarcomas (STSs). Our results show that our MM-NAS had\na higher prediction accuracy when compared to state-of-the-art radiomics\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 14:35:13 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Peng", "Yige", ""], ["Bi", "Lei", ""], ["Fulham", "Michael", ""], ["Feng", "Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "2007.06013", "submitter": "Johann Li", "authors": "Liang Zhang, Johann Li, Ping Li, Xiaoyuan Lu, Peiyi Shen, Guangming\n  Zhu, Syed Afaq Shah, Mohammed Bennarmoun, Kun Qian, Bj\\\"orn W. Schuller", "title": "MeDaS: An open-source platform as service to help break the walls\n  between medicine and informatics", "comments": "layout error fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past decade, deep learning (DL) has achieved unprecedented success in\nnumerous fields including computer vision, natural language processing, and\nhealthcare. In particular, DL is experiencing an increasing development in\napplications for advanced medical image analysis in terms of analysis,\nsegmentation, classification, and furthermore. On the one hand, tremendous\nneeds that leverage the power of DL for medical image analysis are arising from\nthe research community of a medical, clinical, and informatics background to\njointly share their expertise, knowledge, skills, and experience. On the other\nhand, barriers between disciplines are on the road for them often hampering a\nfull and efficient collaboration. To this end, we propose our novel open-source\nplatform, i.e., MeDaS -- the MeDical open-source platform as Service. To the\nbest of our knowledge, MeDaS is the first open-source platform proving a\ncollaborative and interactive service for researchers from a medical background\neasily using DL related toolkits, and at the same time for scientists or\nengineers from information sciences to understand the medical knowledge side.\nBased on a series of toolkits and utilities from the idea of RINV (Rapid\nImplementation aNd Verification), our proposed MeDaS platform can implement\npre-processing, post-processing, augmentation, visualization, and other phases\nneeded in medical image analysis. Five tasks including the subjects of lung,\nliver, brain, chest, and pathology, are validated and demonstrated to be\nefficiently realisable by using MeDaS.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 15:17:00 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 01:59:08 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhang", "Liang", ""], ["Li", "Johann", ""], ["Li", "Ping", ""], ["Lu", "Xiaoyuan", ""], ["Shen", "Peiyi", ""], ["Zhu", "Guangming", ""], ["Shah", "Syed Afaq", ""], ["Bennarmoun", "Mohammed", ""], ["Qian", "Kun", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2007.06032", "submitter": "Hatem Hajri", "authors": "Th\\'eo Combey, Ant\\'onio Loison, Maxime Faucher and Hatem Hajri", "title": "Probabilistic Jacobian-based Saliency Maps Attacks", "comments": "Journal Machine Learning and Knowledge Extraction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network classifiers (NNCs) are known to be vulnerable to malicious\nadversarial perturbations of inputs including those modifying a small fraction\nof the input features named sparse or $L_0$ attacks. Effective and fast $L_0$\nattacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are\npractical to fool NNCs but also to improve their robustness. In this paper, we\nshow that penalising saliency maps of JSMA by the output probabilities and the\ninput features of the NNC allows to obtain more powerful attack algorithms that\nbetter take into account each input's characteristics. This leads us to\nintroduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor\nJSMA (TJSMA), and demonstrate through a variety of white-box and black-box\nexperiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they\nare both significantly faster and more efficient than the original targeted and\nnon-targeted versions of JSMA. Experiments also demonstrate, in some cases,\nvery competitive results of our attacks in comparison with the Carlini-Wagner\n(CW) $L_0$ attack, while remaining, like JSMA, significantly faster (WJSMA and\nTJSMA are more than 50 times faster than CW $L_0$ on CIFAR-10). Therefore, our\nnew attacks provide good trade-offs between JSMA and CW for $L_0$ real-time\nadversarial testing on datasets such as the ones previously cited. Codes are\npublicly available through the link\nhttps://github.com/probabilistic-jsmas/probabilistic-jsmas.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 16:32:26 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 10:34:10 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 15:06:02 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 11:19:11 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Combey", "Th\u00e9o", ""], ["Loison", "Ant\u00f3nio", ""], ["Faucher", "Maxime", ""], ["Hajri", "Hatem", ""]]}, {"id": "2007.06041", "submitter": "Michel Meneses", "authors": "Michel Meneses, Leonardo Matos, Bruno Prado, Andr\\'e de Carvalho and\n  Hendrik Macedo", "title": "Learning to associate detections for real-time multiple object tracking", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the recent advances in the object detection research field,\ntracking-by-detection has become the leading paradigm adopted by multi-object\ntracking algorithms. By extracting different features from detected objects,\nthose algorithms can estimate the objects' similarities and association\npatterns along successive frames. However, since similarity functions applied\nby tracking algorithms are handcrafted, it is difficult to employ them in new\ncontexts. In this study, it is investigated the use of artificial neural\nnetworks to learning a similarity function that can be used among detections.\nDuring training, the networks were introduced to correct and incorrect\nassociation patterns, sampled from a pedestrian tracking data set. For such,\ndifferent motion and appearance features combinations have been explored.\nFinally, a trained network has been inserted into a multiple-object tracking\nframework, which has been assessed on the MOT Challenge benchmark. Throughout\nthe experiments, the proposed tracker matched the results obtained by\nstate-of-the-art methods, it has run 58\\% faster than a recent and similar\nmethod, used as baseline.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:08:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Meneses", "Michel", ""], ["Matos", "Leonardo", ""], ["Prado", "Bruno", ""], ["de Carvalho", "Andr\u00e9", ""], ["Macedo", "Hendrik", ""]]}, {"id": "2007.06059", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Evan Shelhamer, William T. Freeman", "title": "It Is Likely That Your Loss Should be a Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many common loss functions such as mean-squared-error, cross-entropy, and\nreconstruction loss are unnecessarily rigid. Under a probabilistic\ninterpretation, these common losses correspond to distributions with fixed\nshapes and scales. We instead argue for optimizing full likelihoods that\ninclude parameters like the normal variance and softmax temperature. Joint\noptimization of these \"likelihood parameters\" with model parameters can\nadaptively tune the scales and shapes of losses in addition to the strength of\nregularization. We explore and systematically evaluate how to parameterize and\napply likelihood parameters for robust modeling, outlier-detection, and\nre-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$\nweights by fitting the scale parameters of normal and Laplace priors and\nintroduce more flexible element-wise regularizers.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:25:17 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 14:39:37 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Hamilton", "Mark", ""], ["Shelhamer", "Evan", ""], ["Freeman", "William T.", ""]]}, {"id": "2007.06063", "submitter": "Baihong Jin", "authors": "Yingshui Tan, Baihong Jin, Xiangyu Yue, Yuxin Chen, Alberto\n  Sangiovanni Vincentelli", "title": "Exploiting Uncertainties from Ensemble Learners to Improve\n  Decision-Making in Healthcare AI", "comments": "Preprint of submission to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning is widely applied in Machine Learning (ML) to improve model\nperformance and to mitigate decision risks. In this approach, predictions from\na diverse set of learners are combined to obtain a joint decision. Recently,\nvarious methods have been explored in literature for estimating decision\nuncertainties using ensemble learning; however, determining which metrics are a\nbetter fit for certain decision-making applications remains a challenging task.\nIn this paper, we study the following key research question in the selection of\nuncertainty metrics: when does an uncertainty metric outperforms another? We\nanswer this question via a rigorous analysis of two commonly used uncertainty\nmetrics in ensemble learning, namely ensemble mean and ensemble variance. We\nshow that, under mild assumptions on the ensemble learners, ensemble mean is\npreferable with respect to ensemble variance as an uncertainty metric for\ndecision making. We empirically validate our assumptions and theoretical\nresults via an extensive case study: the diagnosis of referable diabetic\nretinopathy.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:33:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tan", "Yingshui", ""], ["Jin", "Baihong", ""], ["Yue", "Xiangyu", ""], ["Chen", "Yuxin", ""], ["Vincentelli", "Alberto Sangiovanni", ""]]}, {"id": "2007.06068", "submitter": "Bilal Alsallakh", "authors": "Bilal Alsallakh and Zhixin Yan and Shabnam Ghaffarzadegan and Zeng Dai\n  and Liu Ren", "title": "Visualizing Classification Structure of Large-Scale Classifiers", "comments": "2020 ICML Workshop on Human Interpretability in Machine Learning (WHI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a measure to compute class similarity in large-scale\nclassification based on prediction scores. Such measure has not been formally\npro-posed in the literature. We show how visualizing the class similarity\nmatrix can reveal hierarchical structures and relationships that govern the\nclasses. Through examples with various classifiers, we demonstrate how such\nstructures can help in analyzing the classification behavior and in inferring\npotential corner cases. The source code for one example is available as a\nnotebook at https://github.com/bilalsal/blocks\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:55:31 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 01:58:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Alsallakh", "Bilal", ""], ["Yan", "Zhixin", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Dai", "Zeng", ""], ["Ren", "Liu", ""]]}, {"id": "2007.06071", "submitter": "Krushi Patel", "authors": "Krushi Patel, Kaidong Li, Ke Tao, Quan Wang, Ajay Bansal, Amit\n  Rastogi, Guanghui Wang", "title": "A Comparative Study on Polyp Classification using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0236452", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer is the third most common cancer diagnosed in both men and\nwomen in the United States. Most colorectal cancers start as a growth on the\ninner lining of the colon or rectum, called 'polyp'. Not all polyps are\ncancerous, but some can develop into cancer. Early detection and recognition of\nthe type of polyps is critical to prevent cancer and change outcomes. However,\nvisual classification of polyps is challenging due to varying illumination\nconditions of endoscopy, variant texture, appearance, and overlapping\nmorphology between polyps. More importantly, evaluation of polyp patterns by\ngastroenterologists is subjective leading to a poor agreement among observers.\nDeep convolutional neural networks have proven very successful in object\nclassification across various object categories. In this work, we compare the\nperformance of the state-of-the-art general object classification models for\npolyp classification. We trained a total of six CNN models end-to-end using a\ndataset of 157 video sequences composed of two types of polyps: hyperplastic\nand adenomatous. Our results demonstrate that the state-of-the-art CNN models\ncan successfully classify polyps with an accuracy comparable or better than\nreported among gastroenterologists. The results of this study can guide future\nresearch in polyp classification.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 19:16:19 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Patel", "Krushi", ""], ["Li", "Kaidong", ""], ["Tao", "Ke", ""], ["Wang", "Quan", ""], ["Bansal", "Ajay", ""], ["Rastogi", "Amit", ""], ["Wang", "Guanghui", ""]]}, {"id": "2007.06077", "submitter": "Aditya Mogadala", "authors": "Aditya Mogadala and Marius Mosbach and Dietrich Klakow", "title": "Sparse Graph to Sequence Learning for Vision Conditioned Long Textual\n  Sequence Generation", "comments": "International Conference on Machine Learning (ICML) 2020 Workshop\n  (https://logicalreasoninggnn.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating longer textual sequences when conditioned on the visual\ninformation is an interesting problem to explore. The challenge here\nproliferate over the standard vision conditioned sentence-level generation\n(e.g., image or video captioning) as it requires to produce a brief and\ncoherent story describing the visual content. In this paper, we mask this\nVision-to-Sequence as Graph-to-Sequence learning problem and approach it with\nthe Transformer architecture. To be specific, we introduce Sparse\nGraph-to-Sequence Transformer (SGST) for encoding the graph and decoding a\nsequence. The encoder aims to directly encode graph-level semantics, while the\ndecoder is used to generate longer sequences. Experiments conducted with the\nbenchmark image paragraph dataset show that our proposed achieve 13.3%\nimprovement on the CIDEr evaluation measure when comparing to the previous\nstate-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 19:54:32 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Mogadala", "Aditya", ""], ["Mosbach", "Marius", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2007.06102", "submitter": "SeyedMajid Azimi", "authors": "Seyed Majid Azimi, Corentin Henry, Lars Sommer, Arne Schumann and\n  Eleonora Vig", "title": "SkyScapes -- Fine-Grained Semantic Understanding of Aerial Scenes", "comments": "Accepted in IEEE ICCV19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding the complex urban infrastructure with centimeter-level accuracy\nis essential for many applications from autonomous driving to mapping,\ninfrastructure monitoring, and urban management. Aerial images provide valuable\ninformation over a large area instantaneously; nevertheless, no current dataset\ncaptures the complexity of aerial scenes at the level of granularity required\nby real-world applications. To address this, we introduce SkyScapes, an aerial\nimage dataset with highly-accurate, fine-grained annotations for pixel-level\nsemantic labeling. SkyScapes provides annotations for 31 semantic categories\nranging from large structures, such as buildings, roads and vegetation, to fine\ndetails, such as 12 (sub-)categories of lane markings. We have defined two main\ntasks on this dataset: dense semantic segmentation and multi-class lane-marking\nprediction. We carry out extensive experiments to evaluate state-of-the-art\nsegmentation methods on SkyScapes. Existing methods struggle to deal with the\nwide range of classes, object sizes, scales, and fine details present. We\ntherefore propose a novel multi-task model, which incorporates semantic edge\ndetection and is better tuned for feature extraction from a wide range of\nscales. This model achieves notable improvements over the baselines in region\noutlines and level of detail on both tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:44:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Azimi", "Seyed Majid", ""], ["Henry", "Corentin", ""], ["Sommer", "Lars", ""], ["Schumann", "Arne", ""], ["Vig", "Eleonora", ""]]}, {"id": "2007.06103", "submitter": "Martin Ferianc", "authors": "Martin Ferianc, Hongxiang Fan and Miguel Rodrigues", "title": "VINNAS: Variational Inference-based Neural Network Architecture Search", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural architecture search (NAS) has received intensive\nscientific and industrial interest due to its capability of finding a neural\narchitecture with high accuracy for various artificial intelligence tasks such\nas image classification or object detection. In particular, gradient-based NAS\napproaches have become one of the more popular approaches thanks to their\ncomputational efficiency during the search. However, these methods often\nexperience a mode collapse, where the quality of the found architectures is\npoor due to the algorithm resorting to choosing a single operation type for the\nentire network, or stagnating at a local minima for various datasets or search\nspaces.\n  To address these defects, we present a differentiable variational\ninference-based NAS method for searching sparse convolutional neural networks.\nOur approach finds the optimal neural architecture by dropping out candidate\noperations in an over-parameterised supergraph using variational dropout with\nautomatic relevance determination prior, which makes the algorithm gradually\nremove unnecessary operations and connections without risking mode collapse.\nThe evaluation is conducted through searching two types of convolutional cells\nthat shape the neural network for classifying different image datasets. Our\nmethod finds diverse network cells, while showing state-of-the-art accuracy\nwith up to almost 2 times fewer non-zero parameters.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:47:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:09:14 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 10:58:20 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2020 12:09:06 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 21:26:57 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ferianc", "Martin", ""], ["Fan", "Hongxiang", ""], ["Rodrigues", "Miguel", ""]]}, {"id": "2007.06124", "submitter": "Seyed Majid Azimi", "authors": "Seyed Majid Azimi, Reza Bahmanyar, Corenin Henry and Franz Kurz", "title": "EAGLE: Large-scale Vehicle Detection Dataset in Real-World Scenarios\n  using Aerial Imagery", "comments": "Accepted in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-class vehicle detection from airborne imagery with orientation\nestimation is an important task in the near and remote vision domains with\napplications in traffic monitoring and disaster management. In the last decade,\nwe have witnessed significant progress in object detection in ground imagery,\nbut it is still in its infancy in airborne imagery, mostly due to the scarcity\nof diverse and large-scale datasets. Despite being a useful tool for different\napplications, current airborne datasets only partially reflect the challenges\nof real-world scenarios. To address this issue, we introduce EAGLE (oriEnted\nvehicle detection using Aerial imaGery in real-worLd scEnarios), a large-scale\ndataset for multi-class vehicle detection with object orientation information\nin aerial imagery. It features high-resolution aerial images composed of\ndifferent real-world situations with a wide variety of camera sensor,\nresolution, flight altitude, weather, illumination, haze, shadow, time, city,\ncountry, occlusion, and camera angle. The annotation was done by airborne\nimagery experts with small- and large-vehicle classes. EAGLE contains 215,986\ninstances annotated with oriented bounding boxes defined by four points and\norientation, making it by far the largest dataset to date in this task. It also\nsupports researches on the haze and shadow removal as well as super-resolution\nand in-painting applications. We define three tasks: detection by (1)\nhorizontal bounding boxes, (2) rotated bounding boxes, and (3) oriented\nbounding boxes. We carried out several experiments to evaluate several\nstate-of-the-art methods in object detection on our dataset to form a baseline.\nExperiments show that the EAGLE dataset accurately reflects real-world\nsituations and correspondingly challenging applications.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 23:00:30 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 14:27:21 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 21:45:29 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Azimi", "Seyed Majid", ""], ["Bahmanyar", "Reza", ""], ["Henry", "Corenin", ""], ["Kurz", "Franz", ""]]}, {"id": "2007.06127", "submitter": "Zhizhong Han", "authors": "Zhizhong Han and Chao Chen and Yu-Shen Liu and Matthias Zwicker", "title": "DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D\n  Structure Learning from Silhouette Images", "comments": "Accepted at ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable renderers have been used successfully for unsupervised 3D\nstructure learning from 2D images because they can bridge the gap between 3D\nand 2D. To optimize 3D shape parameters, current renderers rely on pixel-wise\nlosses between rendered images of 3D reconstructions and ground truth images\nfrom corresponding viewpoints. Hence they require interpolation of the\nrecovered 3D structure at each pixel, visibility handling, and optionally\nevaluating a shading model. In contrast, here we propose a Differentiable\nRenderer Without Rendering (DRWR) that omits these steps. DRWR only relies on a\nsimple but effective loss that evaluates how well the projections of\nreconstructed 3D point clouds cover the ground truth object silhouette.\nSpecifically, DRWR employs a smooth silhouette loss to pull the projection of\neach individual 3D point inside the object silhouette, and a structure-aware\nrepulsion loss to push each pair of projections that fall inside the silhouette\nfar away from each other. Although we omit surface interpolation, visibility\nhandling, and shading, our results demonstrate that DRWR achieves\nstate-of-the-art accuracies under widely used benchmarks, outperforming\nprevious methods both qualitatively and quantitatively. In addition, our\ntraining times are significantly lower due to the simplicity of DRWR.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 23:13:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Han", "Zhizhong", ""], ["Chen", "Chao", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2007.06131", "submitter": "Randy Tan", "authors": "Randy Tan, Naimul Khan, and Ling Guan", "title": "Locality Guided Neural Networks for Explainable Artificial Intelligence", "comments": "8 pages, 3 figures, submitted to WCCI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current deep network architectures, deeper layers in networks tend to\ncontain hundreds of independent neurons which makes it hard for humans to\nunderstand how they interact with each other. By organizing the neurons by\ncorrelation, humans can observe how clusters of neighbouring neurons interact\nwith each other. In this paper, we propose a novel algorithm for back\npropagation, called Locality Guided Neural Network(LGNN) for training networks\nthat preserves locality between neighbouring neurons within each layer of a\ndeep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to\nenforce a local topology on each layer of a deep network such that neighbouring\nneurons are highly correlated with each other. This method contributes to the\ndomain of Explainable Artificial Intelligence (XAI), which aims to alleviate\nthe black-box nature of current AI methods and make them understandable by\nhumans. Our method aims to achieve XAI in deep learning without changing the\nstructure of current models nor requiring any post processing. This paper\nfocuses on Convolutional Neural Networks (CNNs), but can theoretically be\napplied to any type of deep learning architecture. In our experiments, we train\nvarious VGG and Wide ResNet (WRN) networks for image classification on\nCIFAR100. In depth analyses presenting both qualitative and quantitative\nresults demonstrate that our method is capable of enforcing a topology on each\nlayer while achieving a small increase in classification accuracy\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 23:45:51 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tan", "Randy", ""], ["Khan", "Naimul", ""], ["Guan", "Ling", ""]]}, {"id": "2007.06141", "submitter": "Wenying Wu", "authors": "Wenying Wu, Pavlos Protopapas, Zheng Yang, Panagiotis Michalatos", "title": "Gender Classification and Bias Mitigation in Facial Images", "comments": "9 pages", "journal-ref": "WebSci (2020) 106-114", "doi": "10.1145/3394231", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender classification algorithms have important applications in many domains\ntoday such as demographic research, law enforcement, as well as human-computer\ninteraction. Recent research showed that algorithms trained on biased benchmark\ndatabases could result in algorithmic bias. However, to date, little research\nhas been carried out on gender classification algorithms' bias towards gender\nminorities subgroups, such as the LGBTQ and the non-binary population, who have\ndistinct characteristics in gender expression. In this paper, we began by\nconducting surveys on existing benchmark databases for facial recognition and\ngender classification tasks. We discovered that the current benchmark databases\nlack representation of gender minority subgroups. We worked on extending the\ncurrent binary gender classifier to include a non-binary gender class. We did\nthat by assembling two new facial image databases: 1) a racially balanced\ninclusive database with a subset of LGBTQ population 2) an inclusive-gender\ndatabase that consists of people with non-binary gender. We worked to increase\nclassification accuracy and mitigate algorithmic biases on our baseline model\ntrained on the augmented benchmark database. Our ensemble model has achieved an\noverall accuracy score of 90.39%, which is a 38.72% increase from the baseline\nbinary gender classifier trained on Adience. While this is an initial attempt\ntowards mitigating bias in gender classification, more work is needed in\nmodeling gender as a continuum by assembling more inclusive databases.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 01:09:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wu", "Wenying", ""], ["Protopapas", "Pavlos", ""], ["Yang", "Zheng", ""], ["Michalatos", "Panagiotis", ""]]}, {"id": "2007.06143", "submitter": "Jinglin Xu", "authors": "Jinglin Xu, Wenbin Li, Jiantao Shen, Xinwang Liu, Peicheng Zhou,\n  Xiangsen Zhang, Xiwen Yao, and Junwei Han", "title": "Embedded Deep Bilinear Interactive Information and Selective Fusion for\n  Multi-view Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a concrete application of multi-view learning, multi-view classification\nimproves the traditional classification methods significantly by integrating\nvarious views optimally. Although most of the previous efforts have been\ndemonstrated the superiority of multi-view learning, it can be further improved\nby comprehensively embedding more powerful cross-view interactive information\nand a more reliable multi-view fusion strategy in intensive studies. To fulfill\nthis goal, we propose a novel multi-view learning framework to make the\nmulti-view classification better aimed at the above-mentioned two aspects. That\nis, we seamlessly embed various intra-view information, cross-view\nmulti-dimension bilinear interactive information, and a new view ensemble\nmechanism into a unified framework to make a decision via the optimization. In\nparticular, we train different deep neural networks to learn various intra-view\nrepresentations, and then dynamically learn multi-dimension bilinear\ninteractive information from different bilinear similarities via the bilinear\nfunction between views. After that, we adaptively fuse the representations of\nmultiple views by flexibly tuning the parameters of the view-weight, which not\nonly avoids the trivial solution of weight but also provides a new way to\nselect a few discriminative views that are beneficial to make a decision for\nthe multi-view classification. Extensive experiments on six publicly available\ndatasets demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 01:13:23 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xu", "Jinglin", ""], ["Li", "Wenbin", ""], ["Shen", "Jiantao", ""], ["Liu", "Xinwang", ""], ["Zhou", "Peicheng", ""], ["Zhang", "Xiangsen", ""], ["Yao", "Xiwen", ""], ["Han", "Junwei", ""]]}, {"id": "2007.06144", "submitter": "Yuanhao Guo", "authors": "Cong Chen and Shouyang Dong and Ye Tian and Kunlin Cao and Li Liu and\n  Yuanhao Guo", "title": "Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection", "comments": "13 papges, 4 figures, preprint for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on Semi-Supervised Object Detection (SSOD). Knowledge\nDistillation (KD) has been widely used for semi-supervised image\nclassification. However, adapting these methods for SSOD has the following\nobstacles. (1) The teacher model serves a dual role as a teacher and a student,\nsuch that the teacher predictions on unlabeled images may be very close to\nthose of student, which limits the upper-bound of the student. (2) The class\nimbalance issue in SSOD hinders an efficient knowledge transfer from teacher to\nstudent. To address these problems, we propose a novel method Temporal\nSelf-Ensembling Teacher (TSE-T) for SSOD. Differently from previous KD based\nmethods, we devise a temporally evolved teacher model. First, our teacher model\nensembles its temporal predictions for unlabeled images under stochastic\nperturbations. Second, our teacher model ensembles its temporal model weights\nwith the student model weights by an exponential moving average (EMA) which\nallows the teacher gradually learn from the student. These self-ensembling\nstrategies increase data and model diversity, thus improving teacher\npredictions on unlabeled images. Finally, we use focal loss to formulate\nconsistency regularization term to handle the data imbalance problem, which is\na more efficient manner to utilize the useful information from unlabeled images\nthan a simple hard-thresholding method which solely preserves confident\npredictions. Evaluated on the widely used VOC and COCO benchmarks, the mAP of\nour method has achieved 80.73% and 40.52% on the VOC2007 test set and the\nCOCO2014 minval5k set respectively, which outperforms a strong fully-supervised\ndetector by 2.37% and 1.49%. Furthermore, our method sets the new\nstate-of-the-art in SSOD on VOC2007 test set which outperforms the baseline\nSSOD method by 1.44%. The source code of this work is publicly available at\nhttp://github.com/syangdong/tse-t.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 01:17:25 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 01:41:42 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 09:26:25 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Chen", "Cong", ""], ["Dong", "Shouyang", ""], ["Tian", "Ye", ""], ["Cao", "Kunlin", ""], ["Liu", "Li", ""], ["Guo", "Yuanhao", ""]]}, {"id": "2007.06146", "submitter": "Jia Wan", "authors": "Jia Wan, Nikil Senthil Kumar, Antoni B. Chan", "title": "Fine-Grained Crowd Counting", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3049938", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current crowd counting algorithms are only concerned about the number of\npeople in an image, which lacks low-level fine-grained information of the\ncrowd. For many practical applications, the total number of people in an image\nis not as useful as the number of people in each sub-category. E.g., knowing\nthe number of people waiting inline or browsing can help retail stores; knowing\nthe number of people standing/sitting can help restaurants/cafeterias; knowing\nthe number of violent/non-violent people can help police in crowd management.\nIn this paper, we propose fine-grained crowd counting, which differentiates a\ncrowd into categories based on the low-level behavior attributes of the\nindividuals (e.g. standing/sitting or violent behavior) and then counts the\nnumber of people in each category. To enable research in this area, we\nconstruct a new dataset of four real-world fine-grained counting tasks:\ntraveling direction on a sidewalk, standing or sitting, waiting in line or not,\nand exhibiting violent behavior or not. Since the appearance features of\ndifferent crowd categories are similar, the challenge of fine-grained crowd\ncounting is to effectively utilize contextual information to distinguish\nbetween categories. We propose a two branch architecture, consisting of a\ndensity map estimation branch and a semantic segmentation branch. We propose\ntwo refinement strategies for improving the predictions of the two branches.\nFirst, to encode contextual information, we propose feature propagation guided\nby the density map prediction, which eliminates the effect of background\nfeatures during propagation. Second, we propose a complementary attention model\nto share information between the two branches. Experiment results confirm the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 01:31:12 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wan", "Jia", ""], ["Kumar", "Nikil Senthil", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2007.06149", "submitter": "Peisen Zhao", "authors": "Peisen Zhao, Lingxi Xie, Ya Zhang, Qi Tian", "title": "Universal-to-Specific Framework for Complex Action Recognition", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based action recognition has recently attracted much attention in the\nfield of computer vision. To solve more complex recognition tasks, it has\nbecome necessary to distinguish different levels of interclass variations.\nInspired by a common flowchart based on the human decision-making process that\nfirst narrows down the probable classes and then applies a \"rethinking\" process\nfor finer-level recognition, we propose an effective universal-to-specific\n(U2S) framework for complex action recognition. The U2S framework is composed\nof three subnetworks: a universal network, a category-specific network, and a\nmask network. The universal network first learns universal feature\nrepresentations. The mask network then generates attention masks for confusing\nclasses through category regularization based on the output of the universal\nnetwork. The mask is further used to guide the category-specific network for\nclass-specific feature representations. The entire framework is optimized in an\nend-to-end manner. Experiments on a variety of benchmark datasets, e.g., the\nSomething-Something, UCF101, and HMDB51 datasets, demonstrate the effectiveness\nof the U2S framework; i.e., U2S can focus on discriminative spatiotemporal\nregions for confusing categories. We further visualize the relationship between\ndifferent classes, showing that U2S indeed improves the discriminability of\nlearned features. Moreover, the proposed U2S model is a general framework and\nmay adopt any base recognition network.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 01:49:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhao", "Peisen", ""], ["Xie", "Lingxi", ""], ["Zhang", "Ya", ""], ["Tian", "Qi", ""]]}, {"id": "2007.06151", "submitter": "Xingang Yan", "authors": "Xingang Yan, Weiwen Jiang, Yiyu Shi, and Cheng Zhuo", "title": "MS-NAS: Multi-Scale Neural Architecture Search for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent breakthroughs of Neural Architecture Search (NAS) have motivated\nvarious applications in medical image segmentation. However, most existing work\neither simply rely on hyper-parameter tuning or stick to a fixed network\nbackbone, thereby limiting the underlying search space to identify more\nefficient architecture. This paper presents a Multi-Scale NAS (MS-NAS)\nframework that is featured with multi-scale search space from network backbone\nto cell operation, and multi-scale fusion capability to fuse features with\ndifferent sizes. To mitigate the computational overhead due to the larger\nsearch space, a partial channel connection scheme and a two-step decoding\nmethod are utilized to reduce computational overhead while maintaining\noptimization quality. Experimental results show that on various datasets for\nsegmentation, MS-NAS outperforms the state-of-the-art methods and achieves\n0.6-5.4% mIOU and 0.4-3.5% DSC improvements, while the computational resource\nconsumption is reduced by 18.0-24.9%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:02:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yan", "Xingang", ""], ["Jiang", "Weiwen", ""], ["Shi", "Yiyu", ""], ["Zhuo", "Cheng", ""]]}, {"id": "2007.06153", "submitter": "Mehdi Mousavi", "authors": "Mehdi Mousavi, Aashis Khanal, Rolando Estrada", "title": "AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning requires data, but acquiring and labeling real-world data is\nchallenging, expensive, and time-consuming. More importantly, it is nearly\nimpossible to alter real data post-acquisition (e.g., change the illumination\nof a room), making it very difficult to measure how specific properties of the\ndata affect performance. In this paper, we present AI Playground (AIP), an\nopen-source, Unreal Engine-based tool for generating and labeling virtual image\ndata. With AIP, it is trivial to capture the same image under different\nconditions (e.g., fidelity, lighting, etc.) and with different ground truths\n(e.g., depth or surface normal values). AIP is easily extendable and can be\nused with or without code. To validate our proposed tool, we generated eight\ndatasets of otherwise identical but varying lighting and fidelity conditions.\nWe then trained deep neural networks to predict (1) depth values, (2) surface\nnormals, or (3) object labels and assessed each network's intra- and\ncross-dataset performance. Among other insights, we verified that sensitivity\nto different settings is problem-dependent. We confirmed the findings of other\nstudies that segmentation models are very sensitive to fidelity, but we also\nfound that they are just as sensitive to lighting. In contrast, depth and\nnormal estimation models seem to be less sensitive to fidelity or lighting and\nmore sensitive to the structure of the image. Finally, we tested our trained\ndepth-estimation networks on two real-world datasets and obtained results\ncomparable to training on real data alone, confirming that our virtual\nenvironments are realistic enough for real-world tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:04:39 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Mousavi", "Mehdi", ""], ["Khanal", "Aashis", ""], ["Estrada", "Rolando", ""]]}, {"id": "2007.06156", "submitter": "Duo Li", "authors": "Duo Li, Qifeng Chen", "title": "Deep Reinforced Attention Learning for Quality-Aware Visual Recognition", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build upon the weakly-supervised generation mechanism of\nintermediate attention maps in any convolutional neural networks and disclose\nthe effectiveness of attention modules more straightforwardly to fully exploit\ntheir potential. Given an existing neural network equipped with arbitrary\nattention modules, we introduce a meta critic network to evaluate the quality\nof attention maps in the main network. Due to the discreteness of our designed\nreward, the proposed learning method is arranged in a reinforcement learning\nsetting, where the attention actors and recurrent critics are alternately\noptimized to provide instant critique and revision for the temporary attention\nrepresentation, hence coined as Deep REinforced Attention Learning (DREAL). It\ncould be applied universally to network architectures with different types of\nattention modules and promotes their expressive ability by maximizing the\nrelative gain of the final recognition performance arising from each individual\nattention module, as demonstrated by extensive experiments on both category and\ninstance recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:44:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Duo", ""], ["Chen", "Qifeng", ""]]}, {"id": "2007.06166", "submitter": "Kausic Gunasekar", "authors": "Kausic Gunasekar, Qiang Qiu and Yezhou Yang", "title": "Low to High Dimensional Modality Hallucination using Aggregated Fields\n  of View", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\n  1983-1990, April 2020", "doi": "10.1109/LRA.2020.2970679", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world robotics systems deal with data from a multitude of modalities,\nespecially for tasks such as navigation and recognition. The performance of\nthose systems can drastically degrade when one or more modalities become\ninaccessible, due to factors such as sensors' malfunctions or adverse\nenvironments. Here, we argue modality hallucination as one effective way to\nensure consistent modality availability and thereby reduce unfavorable\nconsequences. While hallucinating data from a modality with richer information,\ne.g., RGB to depth, has been researched extensively, we investigate the more\nchallenging low-to-high modality hallucination with interesting use cases in\nrobotics and autonomous systems. We present a novel hallucination architecture\nthat aggregates information from multiple fields of view of the local\nneighborhood to recover the lost information from the extant modality. The\nprocess is implemented by capturing a non-linear mapping between the data\nmodalities and the learned mapping is used to aid the extant modality to\nmitigate the risk posed to the system in the adverse scenarios which involve\nmodality loss. We also conduct extensive classification and segmentation\nexperiments on UWRGBD and NYUD datasets and demonstrate that hallucination\nallays the negative effects of the modality loss. Implementation and models:\nhttps://github.com/kausic94/Hallucination\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 03:13:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gunasekar", "Kausic", ""], ["Qiu", "Qiang", ""], ["Yang", "Yezhou", ""]]}, {"id": "2007.06178", "submitter": "Miaoyun Zhao", "authors": "Miaoyun Zhao, Yulai Cong, Shuyang Dai, Lawrence Carin", "title": "Bridging Maximum Likelihood and Adversarial Learning via\n  $\\alpha$-Divergence", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood (ML) and adversarial learning are two popular approaches\nfor training generative models, and from many perspectives these techniques are\ncomplementary. ML learning encourages the capture of all data modes, and it is\ntypically characterized by stable training. However, ML learning tends to\ndistribute probability mass diffusely over the data space, $e.g.$, yielding\nblurry synthetic images. Adversarial learning is well known to synthesize\nhighly realistic natural images, despite practical challenges like mode\ndropping and delicate training. We propose an $\\alpha$-Bridge to unify the\nadvantages of ML and adversarial learning, enabling the smooth transfer from\none to the other via the $\\alpha$-divergence. We reveal that generalizations of\nthe $\\alpha$-Bridge are closely related to approaches developed recently to\nregularize adversarial learning, providing insights into that prior work, and\nfurther understanding of why the $\\alpha$-Bridge performs well in practice.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 04:06:43 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhao", "Miaoyun", ""], ["Cong", "Yulai", ""], ["Dai", "Shuyang", ""], ["Carin", "Lawrence", ""]]}, {"id": "2007.06181", "submitter": "Duo Li", "authors": "Duo Li, Anbang Yao and Qifeng Chen", "title": "Learning to Learn Parameterized Classification Networks for Scalable\n  Input Images", "comments": "Accepted by ECCV 2020. Code and models are available at\n  https://github.com/d-li14/SAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) do not have a predictable recognition\nbehavior with respect to the input resolution change. This prevents the\nfeasibility of deployment on different input image resolutions for a specific\nmodel. To achieve efficient and flexible image classification at runtime, we\nemploy meta learners to generate convolutional weights of main networks for\nvarious input scales and maintain privatized Batch Normalization layers per\nscale. For improved training performance, we further utilize knowledge\ndistillation on the fly over model predictions based on different input\nresolutions. The learned meta network could dynamically parameterize main\nnetworks to act on input images of arbitrary size with consistently better\naccuracy compared to individually trained models. Extensive experiments on the\nImageNet demonstrate that our method achieves an improved accuracy-efficiency\ntrade-off during the adaptive inference process. By switching executable input\nresolutions, our method could satisfy the requirement of fast adaption in\ndifferent resource-constrained environments. Code and models are available at\nhttps://github.com/d-li14/SAN.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 04:27:25 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Duo", ""], ["Yao", "Anbang", ""], ["Chen", "Qifeng", ""]]}, {"id": "2007.06189", "submitter": "Philipp Benz", "authors": "Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In-So Kweon", "title": "Understanding Adversarial Examples from the Mutual Influence of Images\n  and Perturbations", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of works have explored the reason for the existence of\nadversarial examples, but there is no consensus on the explanation. We propose\nto treat the DNN logits as a vector for feature representation, and exploit\nthem to analyze the mutual influence of two independent inputs based on the\nPearson correlation coefficient (PCC). We utilize this vector representation to\nunderstand adversarial examples by disentangling the clean images and\nadversarial perturbations, and analyze their influence on each other. Our\nresults suggest a new perspective towards the relationship between images and\nuniversal perturbations: Universal perturbations contain dominant features, and\nimages behave like noise to them. This feature perspective leads to a new\nmethod for generating targeted universal adversarial perturbations using random\nsource images. We are the first to achieve the challenging task of a targeted\nuniversal attack without utilizing original training data. Our approach using a\nproxy dataset achieves comparable performance to the state-of-the-art baselines\nwhich utilize the original training dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 05:00:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhang", "Chaoning", ""], ["Benz", "Philipp", ""], ["Imtiaz", "Tooba", ""], ["Kweon", "In-So", ""]]}, {"id": "2007.06191", "submitter": "Duo Li", "authors": "Duo Li, Anbang Yao and Qifeng Chen", "title": "PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale\n  Convolutional Layer", "comments": "Accepted by ECCV 2020. Code and models are available at\n  https://github.com/d-li14/PSConv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their strong modeling capacities, Convolutional Neural Networks\n(CNNs) are often scale-sensitive. For enhancing the robustness of CNNs to scale\nvariance, multi-scale feature fusion from different layers or filters attracts\ngreat attention among existing solutions, while the more granular kernel space\nis overlooked. We bridge this regret by exploiting multi-scale features in a\nfiner granularity. The proposed convolution operation, named Poly-Scale\nConvolution (PSConv), mixes up a spectrum of dilation rates and tactfully\nallocate them in the individual convolutional kernels of each filter regarding\na single convolutional layer. Specifically, dilation rates vary cyclically\nalong the axes of input and output channels of the filters, aggregating\nfeatures over a wide range of scales in a neat style. PSConv could be a drop-in\nreplacement of the vanilla convolution in many prevailing CNN backbones,\nallowing better representation learning without introducing additional\nparameters and computational complexities. Comprehensive experiments on the\nImageNet and MS COCO benchmarks validate the superior performance of PSConv.\nCode and models are available at https://github.com/d-li14/PSConv.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 05:14:11 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Duo", ""], ["Yao", "Anbang", ""], ["Chen", "Qifeng", ""]]}, {"id": "2007.06196", "submitter": "Philipp Benz", "authors": "Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In-So Kweon", "title": "Data from Model: Extracting Data from Non-robust and Robust Models", "comments": "Accepted at the CVPR 2020 Workshop on Adversarial Machine Learning in\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essence of deep learning is to exploit data to train a deep neural\nnetwork (DNN) model. This work explores the reverse process of generating data\nfrom a model, attempting to reveal the relationship between the data and the\nmodel. We repeat the process of Data to Model (DtM) and Data from Model (DfM)\nin sequence and explore the loss of feature mapping information by measuring\nthe accuracy drop on the original validation dataset. We perform this\nexperiment for both a non-robust and robust origin model. Our results show that\nthe accuracy drop is limited even after multiple sequences of DtM and DfM,\nespecially for robust models. The success of this cycling transformation can be\nattributed to the shared feature mapping existing in data and model. Using the\nsame data, we observe that different DtM processes result in models having\ndifferent features, especially for different network architecture families,\neven though they achieve comparable performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 05:27:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Benz", "Philipp", ""], ["Zhang", "Chaoning", ""], ["Imtiaz", "Tooba", ""], ["Kweon", "In-So", ""]]}, {"id": "2007.06198", "submitter": "Gouthaman Kv", "authors": "Gouthaman KV and Anurag Mittal", "title": "Reducing Language Biases in Visual Question Answering with\n  Visually-Grounded Question Encoder", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that current VQA models are heavily biased on the\nlanguage priors in the train set to answer the question, irrespective of the\nimage. E.g., overwhelmingly answer \"what sport is\" as \"tennis\" or \"what color\nbanana\" as \"yellow.\" This behavior restricts them from real-world application\nscenarios. In this work, we propose a novel model-agnostic question encoder,\nVisually-Grounded Question Encoder (VGQE), for VQA that reduces this effect.\nVGQE utilizes both visual and language modalities equally while encoding the\nquestion. Hence the question representation itself gets sufficient\nvisual-grounding, and thus reduces the dependency of the model on the language\npriors. We demonstrate the effect of VGQE on three recent VQA models and\nachieve state-of-the-art results on the bias-sensitive split of the VQAv2\ndataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on\nthe standard VQAv2 benchmark, our approach does not drop the accuracy; instead,\nit improves the performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 05:36:36 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 13:09:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["KV", "Gouthaman", ""], ["Mittal", "Anurag", ""]]}, {"id": "2007.06199", "submitter": "Nicholas Phillips", "authors": "Nick A. Phillips, Pranav Rajpurkar, Mark Sabini, Rayan Krishnan,\n  Sharon Zhou, Anuj Pareek, Nguyet Minh Phu, Chris Wang, Mudit Jain, Nguyen\n  Duong Du, Steven QH Truong, Andrew Y. Ng, Matthew P. Lungren", "title": "CheXphoto: 10,000+ Photos and Transformations of Chest X-rays for\n  Benchmarking Deep Learning Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical deployment of deep learning algorithms for chest x-ray\ninterpretation requires a solution that can integrate into the vast spectrum of\nclinical workflows across the world. An appealing approach to scaled deployment\nis to leverage the ubiquity of smartphones by capturing photos of x-rays to\nshare with clinicians using messaging services like WhatsApp. However, the\napplication of chest x-ray algorithms to photos of chest x-rays requires\nreliable classification in the presence of artifacts not typically encountered\nin digital x-rays used to train machine learning models. We introduce\nCheXphoto, a dataset of smartphone photos and synthetic photographic\ntransformations of chest x-rays sampled from the CheXpert dataset. To generate\nCheXphoto we (1) automatically and manually captured photos of digital x-rays\nunder different settings, and (2) generated synthetic transformations of\ndigital x-rays targeted to make them look like photos of digital x-rays and\nx-ray films. We release this dataset as a resource for testing and improving\nthe robustness of deep learning algorithms for automated chest x-ray\ninterpretation on smartphone photos of chest x-rays.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 05:37:00 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:11:00 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Phillips", "Nick A.", ""], ["Rajpurkar", "Pranav", ""], ["Sabini", "Mark", ""], ["Krishnan", "Rayan", ""], ["Zhou", "Sharon", ""], ["Pareek", "Anuj", ""], ["Phu", "Nguyet Minh", ""], ["Wang", "Chris", ""], ["Jain", "Mudit", ""], ["Du", "Nguyen Duong", ""], ["Truong", "Steven QH", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew P.", ""]]}, {"id": "2007.06227", "submitter": "Youwei Pang", "authors": "Youwei Pang, Lihe Zhang, Xiaoqi Zhao, Huchuan Lu", "title": "Hierarchical Dynamic Filtering Network for RGB-D Salient Object\n  Detection", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of RGB-D salient object detection (SOD) is how to better\nintegrate and utilize cross-modal fusion information. In this paper, we explore\nthese issues from a new perspective. We integrate the features of different\nmodalities through densely connected structures and use their mixed features to\ngenerate dynamic filters with receptive fields of different sizes. In the end,\nwe implement a kind of more flexible and efficient multi-scale cross-modal\nfeature processing, i.e. dynamic dilated pyramid module. In order to make the\npredictions have sharper edges and consistent saliency regions, we design a\nhybrid enhanced loss function to further optimize the results. This loss\nfunction is also validated to be effective in the single-modal RGB SOD task. In\nterms of six metrics, the proposed method outperforms the existing twelve\nmethods on eight challenging benchmark datasets. A large number of experiments\nverify the effectiveness of the proposed module and loss function. Our code,\nmodel and results are available at \\url{https://github.com/lartpang/HDFNet}.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:59:55 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 01:56:20 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 09:15:49 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Pang", "Youwei", ""], ["Zhang", "Lihe", ""], ["Zhao", "Xiaoqi", ""], ["Lu", "Huchuan", ""]]}, {"id": "2007.06233", "submitter": "Kaidong Li", "authors": "Wenchi Ma, Kaidong Li, Guanghui Wang", "title": "Location-Aware Box Reasoning for Anchor-Based Single-Shot Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the majority of object detection frameworks, the confidence of instance\nclassification is used as the quality criterion of predicted bounding boxes,\nlike the confidence-based ranking in non-maximum suppression (NMS). However,\nthe quality of bounding boxes, indicating the spatial relations, is not only\ncorrelated with the classification scores. Compared with the region proposal\nnetwork (RPN) based detectors, single-shot object detectors suffer the box\nquality as there is a lack of pre-selection of box proposals. In this paper, we\naim at single-shot object detectors and propose a location-aware anchor-based\nreasoning (LAAR) for the bounding boxes. LAAR takes both the location and\nclassification confidences into consideration for the quality evaluation of\nbounding boxes. We introduce a novel network block to learn the relative\nlocation between the anchors and the ground truths, denoted as a localization\nscore, which acts as a location reference during the inference stage. The\nproposed localization score leads to an independent regression branch and\ncalibrates the bounding box quality by scoring the predicted localization score\nso that the best-qualified bounding boxes can be picked up in NMS. Experiments\non MS COCO and PASCAL VOC benchmarks demonstrate that the proposed\nlocation-aware framework enhances the performances of current anchor-based\nsingle-shot object detection frameworks and yields consistent and robust\ndetection results.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:24:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ma", "Wenchi", ""], ["Li", "Kaidong", ""], ["Wang", "Guanghui", ""]]}, {"id": "2007.06240", "submitter": "Yucan Zhou", "authors": "Yucan Zhou, Yu Wang, Jianfei Cai, Yu Zhou, Qinghua Hu, Weiping Wang", "title": "Expert Training: Task Hardness Aware Meta-Learning for Few-Shot\n  Classification", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are highly effective when a large number of labeled\nsamples are available but fail with few-shot classification tasks. Recently,\nmeta-learning methods have received much attention, which train a meta-learner\non massive additional tasks to gain the knowledge to instruct the few-shot\nclassification. Usually, the training tasks are randomly sampled and performed\nindiscriminately, often making the meta-learner stuck into a bad local optimum.\nSome works in the optimization of deep neural networks have shown that a better\narrangement of training data can make the classifier converge faster and\nperform better. Inspired by this idea, we propose an easy-to-hard expert\nmeta-training strategy to arrange the training tasks properly, where easy tasks\nare preferred in the first phase, then, hard tasks are emphasized in the second\nphase. A task hardness aware module is designed and integrated into the\ntraining procedure to estimate the hardness of a task based on the\ndistinguishability of its categories. In addition, we explore multiple hardness\nmeasurements including the semantic relation, the pairwise Euclidean distance,\nthe Hausdorff distance, and the Hilbert-Schmidt independence criterion.\nExperimental results on the miniImageNet and tieredImageNetSketch datasets show\nthat the meta-learners can obtain better results with our expert training\nstrategy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:49:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhou", "Yucan", ""], ["Wang", "Yu", ""], ["Cai", "Jianfei", ""], ["Zhou", "Yu", ""], ["Hu", "Qinghua", ""], ["Wang", "Weiping", ""]]}, {"id": "2007.06271", "submitter": "Riccardo Del Chiaro", "authors": "Riccardo Del Chiaro, Bart{\\l}omiej Twardowski, Andrew D. Bagdanov,\n  Joost van de Weijer", "title": "RATT: Recurrent Attention to Transient Tasks for Continual Image\n  Captioning", "comments": "9 pages, 4 figures, 8 supplementary pages, 12 supplementary images,\n  to be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research on continual learning has led to a variety of approaches to\nmitigating catastrophic forgetting in feed-forward classification networks.\nUntil now surprisingly little attention has been focused on continual learning\nof recurrent models applied to problems like image captioning. In this paper we\ntake a systematic look at continual learning of LSTM-based models for image\ncaptioning. We propose an attention-based approach that explicitly accommodates\nthe transient nature of vocabularies in continual image captioning tasks --\ni.e. that task vocabularies are not disjoint. We call our method Recurrent\nAttention to Transient Tasks (RATT), and also show how to adapt continual\nlearning approaches based on weight egularization and knowledge distillation to\nrecurrent continual learning problems. We apply our approaches to incremental\nimage captioning problem on two new continual learning benchmarks we define\nusing the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is\nable to sequentially learn five captioning tasks while incurring no forgetting\nof previously learned ones.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:52:37 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 11:20:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Del Chiaro", "Riccardo", ""], ["Twardowski", "Bart\u0142omiej", ""], ["Bagdanov", "Andrew D.", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2007.06272", "submitter": "Alberto Gomez", "authors": "Simona Treivase, Alberto Gomez, Jacqueline Matthew, Emily Skelton,\n  Julia A. Schnabel, Nicolas Toussaint", "title": "Screen Tracking for Clinical Translation of Live Ultrasound Image\n  Analysis Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) imaging is one of the most commonly used non-invasive imaging\ntechniques. However, US image acquisition requires simultaneous guidance of the\ntransducer and interpretation of images, which is a highly challenging task\nthat requires years of training. Despite many recent developments in\nintra-examination US image analysis, the results are not easy to translate to a\nclinical setting. We propose a generic framework to extract the US images and\nsuperimpose the results of an analysis task, without any need for physical\nconnection or alteration to the US system. The proposed method captures the US\nimage by tracking the screen with a camera fixed at the sonographer's view\npoint and reformats the captured image to the right aspect ratio, in 87.66 +-\n3.73ms on average.\n  It is hypothesized that this would enable to input such retrieved image into\nan image processing pipeline to extract information that can help improve the\nexamination. This information could eventually be projected back to the\nsonographer's field of view in real time using, for example, an augmented\nreality (AR) headset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:53:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Treivase", "Simona", ""], ["Gomez", "Alberto", ""], ["Matthew", "Jacqueline", ""], ["Skelton", "Emily", ""], ["Schnabel", "Julia A.", ""], ["Toussaint", "Nicolas", ""]]}, {"id": "2007.06277", "submitter": "Devis Tuia", "authors": "John Vargas, Shivangi Srivastava, Devis Tuia, Alexandre Falcao", "title": "OpenStreetMap: Challenges and Opportunities in Machine Learning and\n  Remote Sensing", "comments": null, "journal-ref": null, "doi": "10.1109/MGRS.2020.2994107", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OpenStreetMap (OSM) is a community-based, freely available, editable map\nservice that was created as an alternative to authoritative ones. Given that it\nis edited mainly by volunteers with different mapping skills, the completeness\nand quality of its annotations are heterogeneous across different geographical\nlocations. Despite that, OSM has been widely used in several applications in\n{Geosciences}, Earth Observation and environmental sciences. In this work, we\npresent a review of recent methods based on machine learning to improve and use\nOSM data. Such methods aim either 1) at improving the coverage and quality of\nOSM layers, typically using GIS and remote sensing technologies, or 2) at using\nthe existing OSM layers to train models based on image data to serve\napplications like navigation or {land use} classification. We believe that OSM\n(as well as other sources of open land maps) can change the way we interpret\nremote sensing data and that the synergy with machine learning can scale\nparticipatory map making and its quality to the level needed to serve global\nand up-to-date land mapping.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:58:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Vargas", "John", ""], ["Srivastava", "Shivangi", ""], ["Tuia", "Devis", ""], ["Falcao", "Alexandre", ""]]}, {"id": "2007.06279", "submitter": "Kang Li", "authors": "Kang Li, Shujun Wang, Lequan Yu, and Pheng-Ann Heng", "title": "Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for\n  Annotation-efficient Cardiac Segmentation", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image annotations are prohibitively time-consuming and expensive to\nobtain. To alleviate annotation scarcity, many approaches have been developed\nto efficiently utilize extra information, e.g.,semi-supervised learning further\nexploring plentiful unlabeled data, domain adaptation including multi-modality\nlearning and unsupervised domain adaptation resorting to the prior knowledge\nfrom additional modality. In this paper, we aim to investigate the feasibility\nof simultaneously leveraging abundant unlabeled data and well-established\ncross-modality data for annotation-efficient medical image segmentation. To\nthis end, we propose a novel semi-supervised domain adaptation approach, namely\nDual-Teacher, where the student model not only learns from labeled target data\n(e.g., CT), but also explores unlabeled target data and labeled source data\n(e.g., MR) by two teacher models. Specifically, the student model learns the\nknowledge of unlabeled target data from intra-domain teacher by encouraging\nprediction consistency, as well as the shape priors embedded in labeled source\ndata from inter-domain teacher via knowledge distillation. Consequently, the\nstudent model can effectively exploit the information from all three data\nresources and comprehensively integrate them to achieve improved performance.\nWe conduct extensive experiments on MM-WHS 2017 dataset and demonstrate that\nour approach is able to concurrently utilize unlabeled data and cross-modality\ndata with superior performance, outperforming semi-supervised learning and\ndomain adaptation methods with a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:00:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Kang", ""], ["Wang", "Shujun", ""], ["Yu", "Lequan", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2007.06288", "submitter": "Zhou Yang", "authors": "Lifang Wu, Zhou Yang, Qi Wang, Meng Jian, Boxuan Zhao, Junchi Yan,\n  Chang Wen Chen", "title": "Fusing Motion Patterns and Key Visual Information for Semantic Event\n  Recognition in Basketball Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many semantic events in team sport activities e.g. basketball often involve\nboth group activities and the outcome (score or not). Motion patterns can be an\neffective means to identify different activities. Global and local motions have\ntheir respective emphasis on different activities, which are difficult to\ncapture from the optical flow due to the mixture of global and local motions.\nHence it calls for a more effective way to separate the global and local\nmotions. When it comes to the specific case for basketball game analysis, the\nsuccessful score for each round can be reliably detected by the appearance\nvariation around the basket. Based on the observations, we propose a scheme to\nfuse global and local motion patterns (MPs) and key visual information (KVI)\nfor semantic event recognition in basketball videos. Firstly, an algorithm is\nproposed to estimate the global motions from the mixed motions based on the\nintrinsic property of camera adjustments. And the local motions could be\nobtained from the mixed and global motions. Secondly, a two-stream 3D CNN\nframework is utilized for group activity recognition over the separated global\nand local motion patterns. Thirdly, the basket is detected and its appearance\nfeatures are extracted through a CNN structure. The features are utilized to\npredict the success or failure. Finally, the group activity recognition and\nsuccess/failure prediction results are integrated using the kronecker product\nfor event recognition. Experiments on NCAA dataset demonstrate that the\nproposed method obtains state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:15:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wu", "Lifang", ""], ["Yang", "Zhou", ""], ["Wang", "Qi", ""], ["Jian", "Meng", ""], ["Zhao", "Boxuan", ""], ["Yan", "Junchi", ""], ["Chen", "Chang Wen", ""]]}, {"id": "2007.06289", "submitter": "Anastasiya V. Dolmatova", "authors": "Anastasiya Dolmatova, Marina Chukalina and Dmitry Nikolaev", "title": "Accelerated FBP for computed tomography image reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtered back projection (FBP) is a commonly used technique in tomographic\nimage reconstruction demonstrating acceptable quality. The classical direct\nimplementations of this algorithm require the execution of $\\Theta(N^3)$\noperations, where $N$ is the linear size of the 2D slice. Recent approaches\nincluding reconstruction via the Fourier slice theorem require $\\Theta(N^2\\log\nN)$ multiplication operations. In this paper, we propose a novel approach that\nreduces the computational complexity of the algorithm to $\\Theta(N^2\\log N)$\naddition operations avoiding Fourier space. For speeding up the convolution,\nramp filter is approximated by a pair of causal and anticausal recursive\nfilters, also known as Infinite Impulse Response filters. The back projection\nis performed with the fast discrete Hough transform. Experimental results on\nsimulated data demonstrate the efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:16:54 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dolmatova", "Anastasiya", ""], ["Chukalina", "Marina", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "2007.06292", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dhaval Salwala, Edward Curry", "title": "Knowledge Graph Driven Approach to Represent Video Streams for\n  Spatiotemporal Event Pattern Matching in Complex Event Processing", "comments": "31 pages, 14 Figures, Publication accepted in International Journal\n  of Graph Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Processing (CEP) is an event processing paradigm to perform\nreal-time analytics over streaming data and match high-level event patterns.\nPresently, CEP is limited to process structured data stream. Video streams are\ncomplicated due to their unstructured data model and limit CEP systems to\nperform matching over them. This work introduces a graph-based structure for\ncontinuous evolving video streams, which enables the CEP system to query\ncomplex video event patterns. We propose the Video Event Knowledge Graph\n(VEKG), a graph driven representation of video data. VEKG models video objects\nas nodes and their relationship interaction as edges over time and space. It\ncreates a semantic knowledge representation of video data derived from the\ndetection of high-level semantic concepts from the video using an ensemble of\ndeep learning models. A CEP-based state optimization - VEKG-Time Aggregated\nGraph (VEKG-TAG) is proposed over VEKG representation for faster event\ndetection. VEKG-TAG is a spatiotemporal graph aggregation method that provides\na summarized view of the VEKG graph over a given time length. We defined a set\nof nine event pattern rules for two domains (Activity Recognition and Traffic\nManagement), which act as a query and applied over VEKG graphs to discover\ncomplex event patterns. To show the efficacy of our approach, we performed\nextensive experiments over 801 video clips across 10 datasets. The proposed\nVEKG approach was compared with other state-of-the-art methods and was able to\ndetect complex event patterns over videos with F-Score ranging from 0.44 to\n0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%\nand 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,\nachieving sub-second median latency of 4-20 milliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:20:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yadav", "Piyush", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2007.06294", "submitter": "Leonard Elia van Dyck", "authors": "Leonard E. van Dyck and Walter R. Gruber", "title": "Seeing eye-to-eye? A comparison of object recognition performance in\n  humans and deep convolutional neural networks under image manipulation", "comments": "19 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a considerable time, deep convolutional neural networks (DCNNs) have\nreached human benchmark performance in object recognition. On that account,\ncomputational neuroscience and the field of machine learning have started to\nattribute numerous similarities and differences to artificial and biological\nvision. This study aims towards a behavioral comparison of visual core object\nrecognition performance between humans and feedforward neural networks in a\nclassification learning paradigm on an ImageNet data set. For this purpose,\nhuman participants (n = 65) competed in an online experiment against different\nfeedforward DCNNs. The designed approach based on a typical learning process of\nseven different monkey categories included a training and validation phase with\nnatural examples, as well as a testing phase with novel, unexperienced shape\nand color manipulations. Analyses of accuracy revealed that humans not only\noutperform DCNNs on all conditions, but also display significantly greater\nrobustness towards shape and most notably color alterations. Furthermore, a\nprecise examination of behavioral patterns highlights these findings by\nrevealing independent classification errors between the groups. The obtained\nresults show that humans contrast strongly with artificial feedforward\narchitectures when it comes to visual core object recognition of manipulated\nimages. In general, these findings are in line with a growing body of\nliterature, that hints towards recurrence as a crucial factor for adequate\ngeneralization abilities.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:26:30 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 11:08:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["van Dyck", "Leonard E.", ""], ["Gruber", "Walter R.", ""]]}, {"id": "2007.06309", "submitter": "Yongfei Liu", "authors": "Yongfei Liu, Xiangyi Zhang, Songyang Zhang, Xuming He", "title": "Part-aware Prototype Network for Few-shot Semantic Segmentation", "comments": "ECCV-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot semantic segmentation aims to learn to segment new object classes\nwith only a few annotated examples, which has a wide range of real-world\napplications. Most existing methods either focus on the restrictive setting of\none-way few-shot segmentation or suffer from incomplete coverage of object\nregions. In this paper, we propose a novel few-shot semantic segmentation\nframework based on the prototype representation. Our key idea is to decompose\nthe holistic class representation into a set of part-aware prototypes, capable\nof capturing diverse and fine-grained object features. In addition, we propose\nto leverage unlabeled data to enrich our part-aware prototypes, resulting in\nbetter modeling of intra-class variations of semantic objects. We develop a\nnovel graph neural network model to generate and enhance the proposed\npart-aware prototypes based on labeled and unlabeled images. Extensive\nexperimental evaluations on two benchmarks show that our method outperforms the\nprior art with a sizable margin.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 11:03:09 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 12:12:46 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Liu", "Yongfei", ""], ["Zhang", "Xiangyi", ""], ["Zhang", "Songyang", ""], ["He", "Xuming", ""]]}, {"id": "2007.06312", "submitter": "Dimitrios Lenis", "authors": "Dimitrios Lenis, David Major, Maria Wimmer, Astrid Berg, Gert Sluiter,\n  and Katja B\\\"uhler", "title": "Domain aware medical image classifier interpretation by counterfactual\n  impact analysis", "comments": "Accepted for publication at International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2020.This version\n  differs from the published conference version only in a funding agencies\n  name, and additional clarifying changes and references in figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of machine learning methods for computer vision tasks has driven\na surge in computer assisted prediction for medicine and biology. Based on a\ndata-driven relationship between input image and pathological classification,\nthese predictors deliver unprecedented accuracy. Yet, the numerous approaches\ntrying to explain the causality of this learned relationship have fallen short:\ntime constraints, coarse, diffuse and at times misleading results, caused by\nthe employment of heuristic techniques like Gaussian noise and blurring, have\nhindered their clinical adoption.\n  In this work, we discuss and overcome these obstacles by introducing a\nneural-network based attribution method, applicable to any trained predictor.\nOur solution identifies salient regions of an input image in a single\nforward-pass by measuring the effect of local image-perturbations on a\npredictor's score. We replace heuristic techniques with a strong neighborhood\nconditioned inpainting approach, avoiding anatomically implausible, hence\nadversarial artifacts. We evaluate on public mammography data and compare\nagainst existing state-of-the-art methods. Furthermore, we exemplify the\napproach's generalizability by demonstrating results on chest X-rays. Our\nsolution shows, both quantitatively and qualitatively, a significant reduction\nof localization ambiguity and clearer conveying results, without sacrificing\ntime efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 11:11:17 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 16:55:12 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Lenis", "Dimitrios", ""], ["Major", "David", ""], ["Wimmer", "Maria", ""], ["Berg", "Astrid", ""], ["Sluiter", "Gert", ""], ["B\u00fchler", "Katja", ""]]}, {"id": "2007.06317", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Heeseung Kwon, Kyoung Mu Lee, Minsu Cho", "title": "IntegralAction: Pose-driven Feature Integration for Robust Human Action\n  Recognition in Videos", "comments": "Published at CVPRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current action recognition methods heavily rely on appearance\ninformation by taking an RGB sequence of entire image regions as input. While\nbeing effective in exploiting contextual information around humans, e.g., human\nappearance and scene category, they are easily fooled by out-of-context action\nvideos where the contexts do not exactly match with target actions. In\ncontrast, pose-based methods, which take a sequence of human skeletons only as\ninput, suffer from inaccurate pose estimation or ambiguity of human pose per\nse. Integrating these two approaches has turned out to be non-trivial; training\na model with both appearance and pose ends up with a strong bias towards\nappearance and does not generalize well to unseen videos. To address this\nproblem, we propose to learn pose-driven feature integration that dynamically\ncombines appearance and pose streams by observing pose features on the fly. The\nmain idea is to let the pose stream decide how much and which appearance\ninformation is used in integration based on whether the given pose information\nis reliable or not. We show that the proposed IntegralAction achieves highly\nrobust performance across in-context and out-of-context action video datasets.\nThe codes are available in https://github.com/mks0601/IntegralAction_RELEASE.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 11:24:48 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 07:29:32 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Kwon", "Heeseung", ""], ["Lee", "Kyoung Mu", ""], ["Cho", "Minsu", ""]]}, {"id": "2007.06334", "submitter": "Miaojing Shi", "authors": "Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, Li Li", "title": "Active Crowd Counting with Limited Supervision", "comments": "ECCV2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn a reliable people counter from crowd images, head center annotations\nare normally required. Annotating head centers is however a laborious and\ntedious process in dense crowds. In this paper, we present an active learning\nframework which enables accurate crowd counting with limited supervision: given\na small labeling budget, instead of randomly selecting images to annotate, we\nfirst introduce an active labeling strategy to annotate the most informative\nimages in the dataset and learn the counting model upon them. The process is\nrepeated such that in every cycle we select the samples that are diverse in\ncrowd density and dissimilar to previous selections. In the last cycle when the\nlabeling budget is met, the large amount of unlabeled data are also utilized: a\ndistribution classifier is introduced to align the labeled data with unlabeled\ndata; furthermore, we propose to mix up the distribution labels and latent\nrepresentations of data in the network to particularly improve the distribution\nalignment in-between training samples. We follow the popular density estimation\npipeline for crowd counting. Extensive experiments are conducted on standard\nbenchmarks i.e. ShanghaiTech, UCF CC 50, MAll, TRANCOS, and DCC. By annotating\nlimited number of images (e.g. 10% of the dataset), our method reaches levels\nof performance not far from the state of the art which utilize full annotations\nof the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:07:25 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 21:28:20 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhao", "Zhen", ""], ["Shi", "Miaojing", ""], ["Zhao", "Xiaoxiao", ""], ["Li", "Li", ""]]}, {"id": "2007.06341", "submitter": "Shunjie Dong", "authors": "Shunjie Dong, Jinlong Zhao, Maojun Zhang, Zhengxue Shi, Jianing Deng,\n  Yiyu Shi, Mei Tian, Cheng Zhuo", "title": "DeU-Net: Deformable U-Net for 3D Cardiac MRI Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of cardiac magnetic resonance imaging (MRI)\nfacilitates efficient and accurate volume measurement in clinical applications.\nHowever, due to anisotropic resolution and ambiguous border (e.g., right\nventricular endocardium), existing methods suffer from the degradation of\naccuracy and robustness in 3D cardiac MRI video segmentation. In this paper, we\npropose a novel Deformable U-Net (DeU-Net) to fully exploit spatio-temporal\ninformation from 3D cardiac MRI video, including a Temporal Deformable\nAggregation Module (TDAM) and a Deformable Global Position Attention (DGPA)\nnetwork. First, the TDAM takes a cardiac MRI video clip as input with temporal\ninformation extracted by an offset prediction network. Then we fuse extracted\ntemporal information via a temporal aggregation deformable convolution to\nproduce fused feature maps. Furthermore, to aggregate meaningful features, we\ndevise the DGPA network by employing deformable attention U-Net, which can\nencode a wider range of multi-dimensional contextual information into global\nand local features. Experimental results show that our DeU-Net achieves the\nstate-of-the-art performance on commonly used evaluation metrics, especially\nfor cardiac marginal information (ASSD and HD).\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:19:03 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dong", "Shunjie", ""], ["Zhao", "Jinlong", ""], ["Zhang", "Maojun", ""], ["Shi", "Zhengxue", ""], ["Deng", "Jianing", ""], ["Shi", "Yiyu", ""], ["Tian", "Mei", ""], ["Zhuo", "Cheng", ""]]}, {"id": "2007.06344", "submitter": "Xingyu Wan", "authors": "Xingyu Wan, Jiakai Cao, Sanping Zhou, Jinjun Wang", "title": "End-to-End Multi-Object Tracking with Global Response Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Multi-Object Tracking (MOT) approaches follow the\nTracking-by-Detection paradigm and the data association framework where objects\nare firstly detected and then associated. Although deep-learning based method\ncan noticeably improve the object detection performance and also provide good\nappearance features for cross-frame association, the framework is not\ncompletely end-to-end, and therefore the computation is huge while the\nperformance is limited. To address the problem, we present a completely\nend-to-end approach that takes image-sequence/video as input and outputs\ndirectly the located and tracked objects of learned types. Specifically, with\nour introduced multi-object representation strategy, a global response map can\nbe accurately generated over frames, from which the trajectory of each tracked\nobject can be easily picked up, just like how a detector inputs an image and\noutputs the bounding boxes of each detected object. The proposed model is fast\nand accurate. Experimental results based on the MOT16 and MOT17 benchmarks show\nthat our proposed on-line tracker achieved state-of-the-art performance on\nseveral tracking metrics.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:30:49 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wan", "Xingyu", ""], ["Cao", "Jiakai", ""], ["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""]]}, {"id": "2007.06346", "submitter": "Aleksandr Ermolov", "authors": "Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe", "title": "Whitening for Self-Supervised Representation Learning", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current self-supervised representation learning (SSL) methods are\nbased on the contrastive loss and the instance-discrimination task, where\naugmented versions of the same image instance (\"positives\") are contrasted with\ninstances extracted from other images (\"negatives\"). For the learning to be\neffective, many negatives should be compared with a positive pair, which is\ncomputationally demanding. In this paper, we propose a different direction and\na new loss function for SSL, which is based on the whitening of the\nlatent-space features. The whitening operation has a \"scattering\" effect on the\nbatch samples, avoiding degenerate solutions where all the sample\nrepresentations collapse to a single point. Our solution does not require\nasymmetric networks and it is conceptually simple. Moreover, since negatives\nare not needed, we can extract multiple positive pairs from the same image\ninstance. The source code of the method and of all the experiments is available\nat: https://github.com/htdt/self-supervised.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:33:25 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 16:24:04 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 14:01:00 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 11:05:58 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 15:10:06 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ermolov", "Aleksandr", ""], ["Siarohin", "Aliaksandr", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "2007.06355", "submitter": "Rui Qian", "authors": "Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, Weiyao Lin", "title": "Multiple Sound Sources Localization from Coarse to Fine", "comments": "to appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to visually localize multiple sound sources in unconstrained videos is a\nformidable problem, especially when lack of the pairwise sound-object\nannotations. To solve this problem, we develop a two-stage audiovisual learning\nframework that disentangles audio and visual representations of different\ncategories from complex scenes, then performs cross-modal feature alignment in\na coarse-to-fine manner. Our model achieves state-of-the-art results on public\ndataset of localization, as well as considerable performance on multi-source\nsound localization in complex scenes. We then employ the localization results\nfor sound separation and obtain comparable performance to existing methods.\nThese outcomes demonstrate our model's ability in effectively aligning sounds\nwith specific visual sources. Code is available at\nhttps://github.com/shvdiwnkozbw/Multi-Source-Sound-Localization\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:59:40 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 13:38:52 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Qian", "Rui", ""], ["Hu", "Di", ""], ["Dinkel", "Heinrich", ""], ["Wu", "Mengyue", ""], ["Xu", "Ning", ""], ["Lin", "Weiyao", ""]]}, {"id": "2007.06356", "submitter": "David Berga", "authors": "David Berga, Marc Masana and Joost Van de Weijer", "title": "Disentanglement of Color and Shape Representations for Continual\n  Learning", "comments": "Accepted at CL-ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We hypothesize that disentangled feature representations suffer less from\ncatastrophic forgetting. As a case study we perform explicit disentanglement of\ncolor and shape, by adjusting the network architecture. We tested\nclassification accuracy and forgetting in a task-incremental setting with\nOxford-102 Flowers dataset. We combine our method with Elastic Weight\nConsolidation, Learning without Forgetting, Synaptic Intelligence and Memory\nAware Synapses, and show that feature disentanglement positively impacts\ncontinual learning performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:05:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Berga", "David", ""], ["Masana", "Marc", ""], ["Van de Weijer", "Joost", ""]]}, {"id": "2007.06364", "submitter": "Tommy Sonne Alstr{\\o}m", "authors": "Bo Li, Tommy Sonne Alstr{\\o}m", "title": "On uncertainty estimation in active learning for image segmentation", "comments": "Presented at ICML 2020 Workshop on Uncertainty & Robustness in Deep\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation is important for interpreting the trustworthiness of\nmachine learning models in many applications. This is especially critical in\nthe data-driven active learning setting where the goal is to achieve a certain\naccuracy with minimum labeling effort. In such settings, the model learns to\nselect the most informative unlabeled samples for annotation based on its\nestimated uncertainty. The highly uncertain predictions are assumed to be more\ninformative for improving model performance. In this paper, we explore\nuncertainty calibration within an active learning framework for medical image\nsegmentation, an area where labels often are scarce. Various uncertainty\nestimation methods and acquisition strategies (regions and full images) are\ninvestigated. We observe that selecting regions to annotate instead of full\nimages leads to more well-calibrated models. Additionally, we experimentally\nshow that annotating regions can cut 50% of pixels that need to be labeled by\nhumans compared to annotating full images.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:20:32 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Bo", ""], ["Alstr\u00f8m", "Tommy Sonne", ""]]}, {"id": "2007.06371", "submitter": "Dong Wei", "authors": "Dong Wei, Shilei Cao, Kai Ma, Yefeng Zheng", "title": "Learning and Exploiting Interclass Visual Correlations for Medical Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network-based medical image classifications often use \"hard\"\nlabels for training, where the probability of the correct category is 1 and\nthose of others are 0. However, these hard targets can drive the networks\nover-confident about their predictions and prone to overfit the training data,\naffecting model generalization and adaption. Studies have shown that label\nsmoothing and softening can improve classification performance. Nevertheless,\nexisting approaches are either non-data-driven or limited in applicability. In\nthis paper, we present the Class-Correlation Learning Network (CCL-Net) to\nlearn interclass visual correlations from given training data, and produce soft\nlabels to help with classification tasks. Instead of letting the network\ndirectly learn the desired correlations, we propose to learn them implicitly\nvia distance metric learning of class-specific embeddings with a lightweight\nplugin CCL block. An intuitive loss based on a geometrical explanation of\ncorrelation is designed for bolstering learning of the interclass correlations.\nWe further present end-to-end training of the proposed CCL block as a plugin\nhead together with the classification backbone while generating soft labels on\nthe fly. Our experimental results on the International Skin Imaging\nCollaboration 2018 dataset demonstrate effective learning of the interclass\ncorrelations from training data, as well as consistent improvements in\nperformance upon several widely used modern network structures with the CCL\nblock.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:31:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wei", "Dong", ""], ["Cao", "Shilei", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.06373", "submitter": "Jinglu Zhang", "authors": "Jinglu Zhang, Yinyu Nie, Yao Lyu, Hailin Li, Jian Chang, Xiaosong\n  Yang, Jian Jun Zhang", "title": "Symmetric Dilated Convolution for Surgical Gesture Recognition", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical gesture recognition is a prerequisite of intra-operative\ncomputer assistance and objective surgical skill assessment. Prior works either\nrequire additional sensors to collect kinematics data or have limitations on\ncapturing temporal information from long and untrimmed surgical videos. To\ntackle these challenges, we propose a novel temporal convolutional architecture\nto automatically detect and segment surgical gestures with corresponding\nboundaries only using RGB videos. We devise our method with a symmetric\ndilation structure bridged by a self-attention module to encode and decode the\nlong-term temporal patterns and establish the frame-to-frame relationship\naccordingly. We validate the effectiveness of our approach on a fundamental\nrobotic suturing task from the JIGSAWS dataset. The experiment results\ndemonstrate the ability of our method on capturing long-term frame\ndependencies, which largely outperform the state-of-the-art methods on the\nframe-wise accuracy up to ~6 points and the F1@50 score ~6 points.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:34:48 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 15:19:55 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhang", "Jinglu", ""], ["Nie", "Yinyu", ""], ["Lyu", "Yao", ""], ["Li", "Hailin", ""], ["Chang", "Jian", ""], ["Yang", "Xiaosong", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "2007.06389", "submitter": "Bradley McDanel", "authors": "H. T. Kung, Bradley McDanel, Sai Qian Zhang", "title": "Term Revealing: Furthering Quantization at Run Time on Quantized DNNs", "comments": "13 pages, 19 figures, 4 tables, To appear in Proceedings of the\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC), 2020 Update: Revised writing/figures and added more\n  references for Section IV Update: Revised Section IV writing/figures and\n  added additional references on signed digit representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique, called Term Revealing (TR), for furthering\nquantization at run time for improved performance of Deep Neural Networks\n(DNNs) already quantized with conventional quantization methods. TR operates on\npower-of-two terms in binary expressions of values. In computing a dot-product\ncomputation, TR dynamically selects a fixed number of largest terms to use from\nthe values of the two vectors in the dot product. By exploiting normal-like\nweight and data distributions typically present in DNNs, TR has a minimal\nimpact on DNN model performance (i.e., accuracy or perplexity). We use TR to\nfacilitate tightly synchronized processor arrays, such as systolic arrays, for\nefficient parallel processing. We show an FPGA implementation that can use a\nsmall number of control bits to switch between conventional quantization and\nTR-enabled quantization with a negligible delay. To enhance TR efficiency\nfurther, we use a signed digit representation (SDR), as opposed to classic\nbinary encoding with only nonnegative power-of-two terms. To perform conversion\nfrom binary to SDR, we develop an efficient encoding method called HESE (Hybrid\nEncoding for Signed Expressions) that can be performed in one pass looking at\nonly two bits at a time. We evaluate TR with HESE encoded values on an MLP for\nMNIST, multiple CNNs for ImageNet, and an LSTM for Wikitext-2, and show\nsignificant reductions in inference computations (between 3-10x) compared to\nconventional quantization for the same level of model performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:03:10 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 19:24:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kung", "H. T.", ""], ["McDanel", "Bradley", ""], ["Zhang", "Sai Qian", ""]]}, {"id": "2007.06392", "submitter": "Mahdi Rezaei", "authors": "Amir Sharifi, Ahmadreza Zibaei, Mahdi Rezaei", "title": "DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with\n  Restricted Computational Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging and non-trivial tasks in robot-based rescue\noperations is the Hazardous Materials or HAZMATs sign detection in the\noperation field, to prevent further unexpected disasters. Each Hazmat sign has\na specific meaning that the rescue robot should detect and interpret it to take\na safe action, accordingly. Accurate Hazmat detection and real-time processing\nare the two most important factors in such robotics applications. Furthermore,\nwe also have to cope with some secondary challenges such as image distortion\nand restricted CPU and computational resources which are embedded in a rescue\nrobot. In this paper, we propose a CNN-Based pipeline called DeepHAZMAT for\ndetecting and segmenting Hazmats in four steps; 1) optimising the number of\ninput images that are fed into the CNN network, 2) using the YOLOv3-tiny\nstructure to collect the required visual information from the hazardous areas,\n3) Hazmat sign segmentation and separation from the background using GrabCut\ntechnique, and 4) post-processing the result with morphological operators and\nconvex hull algorithm. In spite of the utilisation of a very limited memory and\nCPU resources, the experimental results show the proposed method has\nsuccessfully maintained a better performance in terms of detection-speed and\ndetection-accuracy, compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:15:37 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 12:12:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sharifi", "Amir", ""], ["Zibaei", "Ahmadreza", ""], ["Rezaei", "Mahdi", ""]]}, {"id": "2007.06402", "submitter": "Raphael Achddou", "authors": "Rapha\\\"el Achddou, J.Matias di Martino, Guillermo Sapiro", "title": "Nested Learning For Multi-Granular Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard deep neural networks (DNNs) are commonly trained in an end-to-end\nfashion for specific tasks such as object recognition, face identification, or\ncharacter recognition, among many examples. This specificity often leads to\noverconfident models that generalize poorly to samples that are not from the\noriginal training distribution. Moreover, such standard DNNs do not allow to\nleverage information from heterogeneously annotated training data, where for\nexample, labels may be provided with different levels of granularity.\nFurthermore, DNNs do not produce results with simultaneous different levels of\nconfidence for different levels of detail, they are most commonly an all or\nnothing approach. To address these challenges, we introduce the concept of\nnested learning: how to obtain a hierarchical representation of the input such\nthat a coarse label can be extracted first, and sequentially refine this\nrepresentation, if the sample permits, to obtain successively refined\npredictions, all of them with the corresponding confidence. We explicitly\nenforce this behavior by creating a sequence of nested information bottlenecks.\nLooking at the problem of nested learning from an information theory\nperspective, we design a network topology with two important properties. First,\na sequence of low dimensional (nested) feature embeddings are enforced. Then we\nshow how the explicit combination of nested outputs can improve both the\nrobustness and the accuracy of finer predictions. Experimental results on\nCifar-10, Cifar-100, MNIST, Fashion-MNIST, Dbpedia, and Plantvillage\ndemonstrate that nested learning outperforms the same network trained in the\nstandard end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:27:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Achddou", "Rapha\u00ebl", ""], ["di Martino", "J. Matias", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "2007.06404", "submitter": "Minchul Shin", "authors": "Minchul Shin, Yoonjae Cho, Seongwuk Hong", "title": "Fashion-IQ 2020 Challenge 2nd Place Team's Solution", "comments": "4 pages, CVPR 2020 Workshop, Fashion IQ Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is dedicated to team VAA's approach submitted to the Fashion-IQ\nchallenge in CVPR 2020. Given a pair of the image and the text, we present a\nnovel multimodal composition method, RTIC, that can effectively combine the\ntext and the image modalities into a semantic space. We extract the image and\nthe text features that are encoded by the CNNs and the sequential models (e.g.,\nLSTM or GRU), respectively. To emphasize the meaning of the residual of the\nfeature between the target and candidate, the RTIC is composed of N-blocks with\nchannel-wise attention modules. Then, we add the encoded residual to the\nfeature of the candidate image to obtain a synthesized feature. We also\nexplored an ensemble strategy with variants of models and achieved a\nsignificant boost in performance comparing to the best single model. Finally,\nour approach achieved 2nd place in the Fashion-IQ 2020 Challenge with a test\nscore of 48.02 on the leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:28:37 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shin", "Minchul", ""], ["Cho", "Yoonjae", ""], ["Hong", "Seongwuk", ""]]}, {"id": "2007.06426", "submitter": "Xi Li", "authors": "Bin Li, Jian Tian, Zhongfei Zhang, Hailin Feng, and Xi Li", "title": "Multitask Non-Autoregressive Model for Human Motion Prediction", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3038362", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction, which aims at predicting future human skeletons\ngiven the past ones, is a typical sequence-to-sequence problem. Therefore,\nextensive efforts have been continued on exploring different RNN-based\nencoder-decoder architectures. However, by generating target poses conditioned\non the previously generated ones, these models are prone to bringing issues\nsuch as error accumulation problem. In this paper, we argue that such issue is\nmainly caused by adopting autoregressive manner. Hence, a novel\nNon-auToregressive Model (NAT) is proposed with a complete non-autoregressive\ndecoding scheme, as well as a context encoder and a positional encoding module.\nMore specifically, the context encoder embeds the given poses from temporal and\nspatial perspectives. The frame decoder is responsible for predicting each\nfuture pose independently. The positional encoding module injects positional\nsignal into the model to indicate temporal order. Moreover, a multitask\ntraining paradigm is presented for both low-level human skeleton prediction and\nhigh-level human action recognition, resulting in the convincing improvement\nfor the prediction task. Our approach is evaluated on Human3.6M and CMU-Mocap\nbenchmarks and outperforms state-of-the-art autoregressive methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 15:00:19 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Li", "Bin", ""], ["Tian", "Jian", ""], ["Zhang", "Zhongfei", ""], ["Feng", "Hailin", ""], ["Li", "Xi", ""]]}, {"id": "2007.06443", "submitter": "Jiawei Shen", "authors": "Jiawei Shen, Zhuoyan Li, Lei Yu, Gui-Song Xia, Wen Yang", "title": "Implicit Euler ODE Networks for Single-Image Dehazing", "comments": "10pages, 10 figures, \"for the associate project, see\n  https://github.com/Jiawei-Shen?tab=repositories\", submitted to CVPR workshop\n  \"vision for four seasons\",", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have been applied for image dehazing\ntasks, where the residual network (ResNet) is often adopted as the basic\ncomponent to avoid the vanishing gradient problem. Recently, many works\nindicate that the ResNet can be considered as the explicit Euler forward\napproximation of an ordinary differential equation (ODE). In this paper, we\nextend the explicit forward approximation to the implicit backward counterpart,\nwhich can be realized via a recursive neural network, named IM-block. Given\nthat, we propose an efficient end-to-end multi-level implicit network (MI-Net)\nfor the single image dehazing problem. Moreover, multi-level fusing (MLF)\nmechanism and residual channel attention block (RCA-block) are adopted to boost\nperformance of our network. Experiments on several dehazing benchmark datasets\ndemonstrate that our method outperforms existing methods and achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 15:27:33 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Shen", "Jiawei", ""], ["Li", "Zhuoyan", ""], ["Yu", "Lei", ""], ["Xia", "Gui-Song", ""], ["Yang", "Wen", ""]]}, {"id": "2007.06475", "submitter": "Luca Pappalardo", "authors": "Danilo Sorano, Fabio Carrara, Paolo Cintia, Fabrizio Falchi, Luca\n  Pappalardo", "title": "Automatic Pass Annotation from Soccer VideoStreams Based on Object\n  Detection and LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soccer analytics is attracting increasing interest in academia and industry,\nthanks to the availability of data that describe all the spatio-temporal events\nthat occur in each match. These events (e.g., passes, shots, fouls) are\ncollected by human operators manually, constituting a considerable cost for\ndata providers in terms of time and economic resources. In this paper, we\ndescribe PassNet, a method to recognize the most frequent events in soccer,\ni.e., passes, from video streams. Our model combines a set of artificial neural\nnetworks that perform feature extraction from video streams, object detection\nto identify the positions of the ball and the players, and classification of\nframe sequences as passes or not passes. We test PassNet on different\nscenarios, depending on the similarity of conditions to the match used for\ntraining. Our results show good classification results and significant\nimprovement in the accuracy of pass detection with respect to baseline\nclassifiers, even when the match's video conditions of the test and training\nsets are considerably different. PassNet is the first step towards an automated\nevent annotation system that may break the time and the costs for event\nannotation, enabling data collections for minor and non-professional divisions,\nyouth leagues and, in general, competitions whose matches are not currently\nannotated by data providers.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:14:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sorano", "Danilo", ""], ["Carrara", "Fabio", ""], ["Cintia", "Paolo", ""], ["Falchi", "Fabrizio", ""], ["Pappalardo", "Luca", ""]]}, {"id": "2007.06483", "submitter": "Alptekin Temizel", "authors": "Kadir Cenk Alpay, Kadir Berkay Aydemir, Alptekin Temizel", "title": "Accelerating Translational Image Registration for HDR Images on GPU", "comments": "Submitted for Consideration for Publication in High Performance\n  Computing Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Dynamic Range (HDR) images are generated using multiple exposures of a\nscene. When a hand-held camera is used to capture a static scene, these images\nneed to be aligned by globally shifting each image in both dimensions. For a\nfast and robust alignment, the shift amount is commonly calculated using Median\nThreshold Bitmaps (MTB) and creating an image pyramid. In this study, we\noptimize these computations using a parallel processing approach utilizing GPU.\nExperimental evaluation shows that the proposed implementation achieves a\nspeed-up of up to 6.24 times over the baseline multi-threaded CPU\nimplementation on the alignment of one image pair. The source code is available\nat https://github.com/kadircenk/WardMTBCuda\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:34:05 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Alpay", "Kadir Cenk", ""], ["Aydemir", "Kadir Berkay", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2007.06492", "submitter": "Somaiyeh MahmoudZadeh", "authors": "Weixiang Li, Wei Jie, Somaiyeh MahmoudZadeh", "title": "Single Image Dehazing Algorithm Based on Sky Region Segmentation", "comments": null, "journal-ref": "International Conference on Advanced Data Mining and Applications,\n  2019", "doi": "10.1007/978-3-030-35231-8_35", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a hybrid image defogging approach based on region segmentation\nis proposed to address the dark channel priori algorithm's shortcomings in\nde-fogging the sky regions. The preliminary stage of the proposed approach\nfocuses on the segmentation of sky and non-sky regions in a foggy image taking\nthe advantageous of Meanshift and edge detection with embedded confidence. In\nthe second stage, an improved dark channel priori algorithm is employed to\ndefog the non-sky region. Ultimately, the sky area is processed by DehazeNet\nalgorithm, which relies on deep learning Convolutional Neural Networks. The\nsimulation results show that the proposed hybrid approach in this research\naddresses the problem of color distortion associated with sky regions in foggy\nimages. The approach greatly improves the image quality indices including\nentropy information, visibility ratio of the edges, average gradient, and the\nsaturation percentage with a very fast computation time, which is a good\nindication of the excellent performance of this model.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:03:55 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Weixiang", ""], ["Jie", "Wei", ""], ["MahmoudZadeh", "Somaiyeh", ""]]}, {"id": "2007.06504", "submitter": "Pingchuan Ma", "authors": "Pingchuan Ma, Brais Martinez, Stavros Petridis, Maja Pantic", "title": "Towards Practical Lipreading with Distilled and Efficient Models", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipreading has witnessed a lot of progress due to the resurgence of neural\nnetworks. Recent works have placed emphasis on aspects such as improving\nperformance by finding the optimal architecture or improving generalization.\nHowever, there is still a significant gap between the current methodologies and\nthe requirements for an effective deployment of lipreading in practical\nscenarios. In this work, we propose a series of innovations that significantly\nbridge that gap: first, we raise the state-of-the-art performance by a wide\nmargin on LRW and LRW-1000 to 88.5% and 46.6%, respectively using\nself-distillation. Secondly, we propose a series of architectural changes,\nincluding a novel Depthwise Separable Temporal Convolutional Network (DS-TCN)\nhead, that slashes the computational cost to a fraction of the (already quite\nefficient) original model. Thirdly, we show that knowledge distillation is a\nvery effective tool for recovering performance of the lightweight models. This\nresults in a range of models with different accuracy-efficiency trade-offs.\nHowever, our most promising lightweight models are on par with the current\nstate-of-the-art while showing a reduction of 8.2x and 3.9x in terms of\ncomputational cost and number of parameters, respectively, which we hope will\nenable the deployment of lipreading models in practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:56:27 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 15:50:02 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 09:02:09 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ma", "Pingchuan", ""], ["Martinez", "Brais", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2007.06516", "submitter": "Jadie Adams", "authors": "Jadie Adams, Riddhish Bhalodia, Shireen Elhabian", "title": "Uncertain-DeepSSM: From Images to Probabilistic Shape Models", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape modeling (SSM) has recently taken advantage of advances in\ndeep learning to alleviate the need for a time-consuming and expert-driven\nworkflow of anatomy segmentation, shape registration, and the optimization of\npopulation-level shape representations. DeepSSM is an end-to-end deep learning\napproach that extracts statistical shape representation directly from\nunsegmented images with little manual overhead. It performs comparably with\nstate-of-the-art shape modeling methods for estimating morphologies that are\nviable for subsequent downstream tasks. Nonetheless, DeepSSM produces an\noverconfident estimate of shape that cannot be blindly assumed to be accurate.\nHence, conveying what DeepSSM does not know, via quantifying granular estimates\nof uncertainty, is critical for its direct clinical application as an on-demand\ndiagnostic tool to determine how trustworthy the model output is. Here, we\npropose Uncertain-DeepSSM as a unified model that quantifies both,\ndata-dependent aleatoric uncertainty by adapting the network to predict\nintrinsic input variance, and model-dependent epistemic uncertainty via a Monte\nCarlo dropout sampling to approximate a variational distribution over the\nnetwork parameters. Experiments show an accuracy improvement over DeepSSM while\nmaintaining the same benefits of being end-to-end with little pre-processing.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:18:21 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Adams", "Jadie", ""], ["Bhalodia", "Riddhish", ""], ["Elhabian", "Shireen", ""]]}, {"id": "2007.06537", "submitter": "Jay Kumar", "authors": "Rajesh Kumar, Abdullah Aman Khan, Sinmin Zhang, Jay Kumar, Ting Yang,\n  Noorbakhash Amiri Golalirz, Zakria, Ikram Ali, Sidra Shafiq and WenYong Wang", "title": "Blockchain-Federated-Learning and Deep Learning Models for COVID-19\n  detection using CT Imaging", "comments": "arXiv admin note: text overlap with arXiv:2003.10849 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of COVID-19 cases worldwide, an effective way is required\nto diagnose COVID-19 patients. The primary problem in diagnosing COVID-19\npatients is the shortage and reliability of testing kits, due to the quick\nspread of the virus, medical practitioners are facing difficulty identifying\nthe positive cases. The second real-world problem is to share the data among\nthe hospitals globally while keeping in view the privacy concerns of the\norganizations. Building a collaborative model and preserving privacy are major\nconcerns for training a global deep learning model. This paper proposes a\nframework that collects a small amount of data from different sources (various\nhospitals) and trains a global deep learning model using blockchain based\nfederated learning. Blockchain technology authenticates the data and federated\nlearning trains the model globally while preserving the privacy of the\norganization. First, we propose a data normalization technique that deals with\nthe heterogeneity of data as the data is gathered from different hospitals\nhaving different kinds of CT scanners. Secondly, we use Capsule Network-based\nsegmentation and classification to detect COVID-19 patients. Thirdly, we design\na method that can collaboratively train a global model using blockchain\ntechnology with federated learning while preserving privacy. Additionally, we\ncollected real-life COVID-19 patients data, which is, open to the research\ncommunity. The proposed framework can utilize up-to-date data which improves\nthe recognition of computed tomography (CT) images. Finally, our results\ndemonstrate a better performance to detect COVID-19 patients.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:23:14 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 07:52:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kumar", "Rajesh", ""], ["Khan", "Abdullah Aman", ""], ["Zhang", "Sinmin", ""], ["Kumar", "Jay", ""], ["Yang", "Ting", ""], ["Golalirz", "Noorbakhash Amiri", ""], ["Zakria", "", ""], ["Ali", "Ikram", ""], ["Shafiq", "Sidra", ""], ["Wang", "WenYong", ""]]}, {"id": "2007.06542", "submitter": "Xiaobo Wang", "authors": "Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, Tao Mei", "title": "Loss Function Search for Face Recognition", "comments": "Accepted by ICML2020. arXiv admin note: substantial text overlap with\n  arXiv:1912.00833; text overlap with arXiv:1905.07375 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In face recognition, designing margin-based (e.g., angular, additive,\nadditive angular margins) softmax loss functions plays an important role in\nlearning discriminative features. However, these hand-crafted heuristic methods\nare sub-optimal because they require much effort to explore the large design\nspace. Recently, an AutoML for loss function search method AM-LFS has been\nderived, which leverages reinforcement learning to search loss functions during\nthe training process. But its search space is complex and unstable that\nhindering its superiority. In this paper, we first analyze that the key to\nenhance the feature discrimination is actually \\textbf{how to reduce the\nsoftmax probability}. We then design a unified formulation for the current\nmargin-based softmax losses. Accordingly, we define a novel search space and\ndevelop a reward-guided search method to automatically obtain the best\ncandidate. Experimental results on a variety of face recognition benchmarks\nhave demonstrated the effectiveness of our method over the state-of-the-art\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 03:40:10 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wang", "Xiaobo", ""], ["Wang", "Shuo", ""], ["Chi", "Cheng", ""], ["Zhang", "Shifeng", ""], ["Mei", "Tao", ""]]}, {"id": "2007.06544", "submitter": "John Heerfordt", "authors": "John Heerfordt, Kevin K. Whitehead, Jessica A.M. Bastiaansen, Lorenzo\n  Di Sopra, Christopher W. Roy, J\\'er\\^ome Yerly, Bastien Milani, Mark A.\n  Fogel, Matthias Stuber, Davide Piccini", "title": "Free-running SIMilarity-Based Angiography (SIMBA) for simplified\n  anatomical MR imaging of the heart", "comments": "8 figures, 2 tables", "journal-ref": "Magnetic Resonance in Medicine, 24 February 2021", "doi": "10.1002/mrm.28713", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Whole-heart MRA techniques typically target pre-determined motion\nstates and address cardiac and respiratory dynamics independently. We propose a\nnovel fast reconstruction algorithm, applicable to ungated free-running\nsequences, that leverages inherent similarities in the acquired data to avoid\nsuch physiological constraints.\n  Theory and Methods: The proposed SIMilarity-Based Angiography (SIMBA) method\nclusters the continuously acquired k-space data in order to find a\nmotion-consistent subset that can be reconstructed into a motion-suppressed\nwhole-heart MRA. Free-running 3D radial datasets from six ferumoxytol-enhanced\nscans of pediatric cardiac patients and twelve non-contrast scans of healthy\nvolunteers were reconstructed with a non-motion-suppressed regridding of all\nthe acquired data (All Data), our proposed SIMBA method, and a previously\npublished free-running framework (FRF) that uses cardiac and respiratory\nself-gating and compressed sensing. Images were compared for blood-myocardium\ninterface sharpness, contrast ratio, and visibility of coronary artery ostia.\n  Results: Both the fast SIMBA reconstruction (~20s) and the FRF provided\nsignificantly higher blood-myocardium sharpness than All Data (P<0.001). No\nsignificant difference was observed among the former two. Significantly higher\nblood-myocardium contrast ratio was obtained with SIMBA compared to All Data\nand FRF (P<0.01). More coronary ostia could be visualized with both SIMBA and\nFRF than with All Data (All Data: 4/36, SIMBA: 30/36, FRF: 33/36, both P<0.001)\nbut no significant difference was found between the first two.\n  Conclusion: The combination of free-running sequences and the fast SIMBA\nreconstruction, which operates without a priori assumptions related to\nphysiological motion, forms a simple workflow for obtaining whole-heart MRA\nwith sharp anatomical structures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:49:30 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Heerfordt", "John", ""], ["Whitehead", "Kevin K.", ""], ["Bastiaansen", "Jessica A. M.", ""], ["Di Sopra", "Lorenzo", ""], ["Roy", "Christopher W.", ""], ["Yerly", "J\u00e9r\u00f4me", ""], ["Milani", "Bastien", ""], ["Fogel", "Mark A.", ""], ["Stuber", "Matthias", ""], ["Piccini", "Davide", ""]]}, {"id": "2007.06559", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie", "title": "Graph Structure of Neural Networks", "comments": "ICML 2020, with open-source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are often represented as graphs of connections between\nneurons. However, despite their wide use, there is currently little\nunderstanding of the relationship between the graph structure of the neural\nnetwork and its predictive performance. Here we systematically investigate how\ndoes the graph structure of neural networks affect their predictive\nperformance. To this end, we develop a novel graph-based representation of\nneural networks called relational graph, where layers of neural network\ncomputation correspond to rounds of message exchange along the graph structure.\nUsing this representation we show that: (1) a \"sweet spot\" of relational graphs\nleads to neural networks with significantly improved predictive performance;\n(2) neural network's performance is approximately a smooth function of the\nclustering coefficient and average path length of its relational graph; (3) our\nfindings are consistent across many different tasks and datasets; (4) the sweet\nspot can be identified efficiently; (5) top-performing neural networks have\ngraph structure surprisingly similar to those of real biological neural\nnetworks. Our work opens new directions for the design of neural architectures\nand the understanding on neural networks in general.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:59:31 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 17:58:07 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["You", "Jiaxuan", ""], ["Leskovec", "Jure", ""], ["He", "Kaiming", ""], ["Xie", "Saining", ""]]}, {"id": "2007.06565", "submitter": "Zhongling Wang", "authors": "Zhongling Wang, Mahdi S. Hosseini, Adyn Miles, Konstantinos N.\n  Plataniotis, Zhou Wang", "title": "FocusLiteNN: High Efficiency Focus Quality Assessment for Digital\n  Pathology", "comments": "To be published in the 23rd International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-focus microscopy lens in digital pathology is a critical bottleneck in\nhigh-throughput Whole Slide Image (WSI) scanning platforms, for which\npixel-level automated Focus Quality Assessment (FQA) methods are highly\ndesirable to help significantly accelerate the clinical workflows. Existing FQA\nmethods include both knowledge-driven and data-driven approaches. While\ndata-driven approaches such as Convolutional Neural Network (CNN) based methods\nhave shown great promises, they are difficult to use in practice due to their\nhigh computational complexity and lack of transferability. Here, we propose a\nhighly efficient CNN-based model that maintains fast computations similar to\nthe knowledge-driven methods without excessive hardware requirements such as\nGPUs. We create a training dataset using FocusPath which encompasses diverse\ntissue slides across nine different stain colors, where the stain diversity\ngreatly helps the model to learn diverse color spectrum and tissue structures.\nIn our attempt to reduce the CNN complexity, we find with surprise that even\ntrimming down the CNN to the minimal level, it still achieves a highly\ncompetitive performance. We introduce a novel comprehensive evaluation dataset,\nthe largest of its kind, annotated and compiled from TCGA repository for model\nassessment and comparison, for which the proposed method exhibits superior\nprecision-speed trade-off when compared with existing knowledge-driven and\ndata-driven FQA approaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 20:52:01 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 17:21:55 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Wang", "Zhongling", ""], ["Hosseini", "Mahdi S.", ""], ["Miles", "Adyn", ""], ["Plataniotis", "Konstantinos N.", ""], ["Wang", "Zhou", ""]]}, {"id": "2007.06570", "submitter": "Guha Balakrishnan", "authors": "Guha Balakrishnan, Yuanjun Xiong, Wei Xia, Pietro Perona", "title": "Towards causal benchmarking of bias in face analysis algorithms", "comments": "Long-form version of ECCV 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring algorithmic bias is crucial both to assess algorithmic fairness,\nand to guide the improvement of algorithms. Current methods to measure\nalgorithmic bias in computer vision, which are based on observational datasets,\nare inadequate for this task because they conflate algorithmic bias with\ndataset bias.\n  To address this problem we develop an experimental method for measuring\nalgorithmic bias of face analysis algorithms, which manipulates directly the\nattributes of interest, e.g., gender and skin tone, in order to reveal causal\nlinks between attribute variation and performance change. Our proposed method\nis based on generating synthetic ``transects'' of matched sample images that\nare designed to differ along specific attributes while leaving other attributes\nconstant. A crucial aspect of our approach is relying on the perception of\nhuman observers, both to guide manipulations, and to measure algorithmic bias.\n  Besides allowing the measurement of algorithmic bias, synthetic transects\nhave other advantages with respect to observational datasets: they sample\nattributes more evenly allowing for more straightforward bias analysis on\nminority and intersectional groups, they enable prediction of bias in new\nscenarios, they greatly reduce ethical and legal challenges, and they are\neconomical and fast to obtain, helping make bias testing affordable and widely\navailable.\n  We validate our method by comparing it to a study that employs the\ntraditional observational method for analyzing bias in gender classification\nalgorithms. The two methods reach different conclusions. While the\nobservational method reports gender and skin color biases, the experimental\nmethod reveals biases due to gender, hair length, age, and facial hair.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:10:34 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Balakrishnan", "Guha", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Perona", "Pietro", ""]]}, {"id": "2007.06600", "submitter": "Yujun Shen", "authors": "Yujun Shen, Bolei Zhou", "title": "Closed-Form Factorization of Latent Semantics in GANs", "comments": "CVPR 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rich set of interpretable dimensions has been shown to emerge in the latent\nspace of the Generative Adversarial Networks (GANs) trained for synthesizing\nimages. In order to identify such latent dimensions for image editing, previous\nmethods typically annotate a collection of synthesized samples and train linear\nclassifiers in the latent space. However, they require a clear definition of\nthe target attribute as well as the corresponding manual annotations, limiting\ntheir applications in practice. In this work, we examine the internal\nrepresentation learned by GANs to reveal the underlying variation factors in an\nunsupervised manner. In particular, we take a closer look into the generation\nmechanism of GANs and further propose a closed-form factorization algorithm for\nlatent semantic discovery by directly decomposing the pre-trained weights. With\na lightning-fast implementation, our approach is capable of not only finding\nsemantically meaningful dimensions comparably to the state-of-the-art\nsupervised methods, but also resulting in far more versatile concepts across\nmultiple GAN models trained on a wide range of datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:05:36 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 06:37:34 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 04:50:58 GMT"}, {"version": "v4", "created": "Sat, 3 Apr 2021 13:30:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shen", "Yujun", ""], ["Zhou", "Bolei", ""]]}, {"id": "2007.06612", "submitter": "Amirhossein Bayat", "authors": "Amirhossein Bayat, Anjany Sekuboyina, Johannes C. Paetzold, Christian\n  Payer, Darko Stern, Martin Urschler, Jan S. Kirschke, Bjoern H. Menze", "title": "Inferring the 3D Standing Spine Posture from 2D Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment of degenerative spinal disorders requires an understanding of\nthe individual spinal anatomy and curvature in 3D. An upright spinal pose (i.e.\nstanding) under natural weight bearing is crucial for such bio-mechanical\nanalysis. 3D volumetric imaging modalities (e.g. CT and MRI) are performed in\npatients lying down. On the other hand, radiographs are captured in an upright\npose, but result in 2D projections. This work aims to integrate the two realms,\ni.e. it combines the upright spinal curvature from radiographs with the 3D\nvertebral shape from CT imaging for synthesizing an upright 3D model of spine,\nloaded naturally. Specifically, we propose a novel neural network architecture\nworking vertebra-wise, termed \\emph{TransVert}, which takes orthogonal 2D\nradiographs and infers the spine's 3D posture. We validate our architecture on\ndigitally reconstructed radiographs, achieving a 3D reconstruction Dice of\n$95.52\\%$, indicating an almost perfect 2D-to-3D domain translation. Deploying\nour model on clinical radiographs, we successfully synthesise full-3D, upright,\npatient-specific spine models for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:37:00 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 15:15:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bayat", "Amirhossein", ""], ["Sekuboyina", "Anjany", ""], ["Paetzold", "Johannes C.", ""], ["Payer", "Christian", ""], ["Stern", "Darko", ""], ["Urschler", "Martin", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2007.06620", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Tong Che, Yiqun Lu, Chao Yang, Site Li, Jane You", "title": "AUTO3D: Novel view synthesis through unsupervisely learned variational\n  viewpoint and global 3D representation", "comments": "ECCV 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58545-7_4", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets on learning-based novel view synthesis from a single or\nlimited 2D images without the pose supervision. In the viewer-centered\ncoordinates, we construct an end-to-end trainable conditional variational\nframework to disentangle the unsupervisely learned relative-pose/rotation and\nimplicit global 3D representation (shape, texture and the origin of\nviewer-centered coordinates, etc.). The global appearance of the 3D object is\ngiven by several appearance-describing images taken from any number of\nviewpoints. Our spatial correlation module extracts a global 3D representation\nfrom the appearance-describing images in a permutation invariant manner. Our\nsystem can achieve implicitly 3D understanding without explicitly 3D\nreconstruction. With an unsupervisely learned viewer-centered\nrelative-pose/rotation code, the decoder can hallucinate the novel view\ncontinuously by sampling the relative-pose in a prior distribution. In various\napplications, we demonstrate that our model can achieve comparable or even\nbetter results than pose/3D model-supervised learning-based novel view\nsynthesis (NVS) methods with any number of input views.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:51:27 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 22:18:24 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Che", "Tong", ""], ["Lu", "Yiqun", ""], ["Yang", "Chao", ""], ["Li", "Site", ""], ["You", "Jane", ""]]}, {"id": "2007.06624", "submitter": "Pawel Staszewski", "authors": "Pawe{\\l} Staszewski, Maciej Jaworski, Jinde Cao, Leszek Rutkowski", "title": "A new approach to descriptors generation for image retrieval by\n  analyzing activations of deep neural network layers", "comments": "8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of descriptors construction for the\ntask of content-based image retrieval using deep neural networks. The idea of\nneural codes, based on fully connected layers activations, is extended by\nincorporating the information contained in convolutional layers. It is known\nthat the total number of neurons in the convolutional part of the network is\nlarge and the majority of them have little influence on the final\nclassification decision. Therefore, in the paper we propose a novel algorithm\nthat allows us to extract the most significant neuron activations and utilize\nthis information to construct effective descriptors. The descriptors consisting\nof values taken from both the fully connected and convolutional layers\nperfectly represent the whole image content. The images retrieved using these\ndescriptors match semantically very well to the query image, and also they are\nsimilar in other secondary image characteristics, like background, textures or\ncolor distribution. These features of the proposed descriptors are verified\nexperimentally based on the IMAGENET1M dataset using the VGG16 neural network.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:53:10 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Staszewski", "Pawe\u0142", ""], ["Jaworski", "Maciej", ""], ["Cao", "Jinde", ""], ["Rutkowski", "Leszek", ""]]}, {"id": "2007.06630", "submitter": "Javier Gonzalez-Trejo", "authors": "Javier Antonio Gonzalez-Trejo, Diego Alberto Mercado-Ravell", "title": "Dense Crowds Detection and Counting with a Lightweight Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of crowd counting, most of the works have focused on improving\nthe accuracy without regard to the performance leading to algorithms that are\nnot suitable for embedded applications. In this paper, we propose a lightweight\nconvolutional neural network architecture to perform crowd detection and\ncounting using fewer computer resources without a significant loss on count\naccuracy. The architecture was trained using the Bayes loss function to further\nimprove its accuracy and then pruned to further reduce the computational\nresources used. The proposed architecture was tested over the USF-QNRF\nachieving a competitive Mean Average Error of 154.07 and a superior Mean Square\nError of 241.77 while maintaining a competitive number of parameters of 0.067\nMillion. The obtained results suggest that the Bayes loss can be used with\nother architectures to further improve them and also the last convolutional\nlayer provides no significant information and even encourage over-fitting at\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:02:25 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Gonzalez-Trejo", "Javier Antonio", ""], ["Mercado-Ravell", "Diego Alberto", ""]]}, {"id": "2007.06631", "submitter": "Anton Obukhov", "authors": "Anton Obukhov, Maxim Rakhuba, Stamatios Georgoulis, Menelaos Kanakis,\n  Dengxin Dai, Luc Van Gool", "title": "T-Basis: a Compact Representation for Neural Networks", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce T-Basis, a novel concept for a compact representation of a set\nof tensors, each of an arbitrary shape, which is often seen in Neural Networks.\nEach of the tensors in the set is modeled using Tensor Rings, though the\nconcept applies to other Tensor Networks. Owing its name to the T-shape of\nnodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally\nshaped three-dimensional tensors, used to represent Tensor Ring nodes. Such\nrepresentation allows us to parameterize the tensor set with a small number of\nparameters (coefficients of the T-Basis tensors), scaling logarithmically with\neach tensor's size in the set and linearly with the dimensionality of T-Basis.\nWe evaluate the proposed approach on the task of neural network compression and\ndemonstrate that it reaches high compression rates at acceptable performance\ndrops. Finally, we analyze memory and operation requirements of the compressed\nnetworks and conclude that T-Basis networks are equally well suited for\ntraining and inference in resource-constrained environments and usage on the\nedge devices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:03:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 17:34:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Obukhov", "Anton", ""], ["Rakhuba", "Maxim", ""], ["Georgoulis", "Stamatios", ""], ["Kanakis", "Menelaos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.06633", "submitter": "Darrick Lee", "authors": "Darrick Lee, Robert Ghrist", "title": "Path Signatures on Lie Groups", "comments": "64 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path signatures are powerful nonparametric tools for time series analysis,\nshown to form a universal and characteristic feature map for Euclidean valued\ntime series data. We lift the theory of path signatures to the setting of Lie\ngroup valued time series, adapting these tools for time series with underlying\ngeometric constraints. We prove that this generalized path signature is\nuniversal and characteristic. To demonstrate universality, we analyze the human\naction recognition problem in computer vision, using $SO(3)$ representations\nfor the time series, providing comparable performance to other shallow learning\napproaches, while offering an easily interpretable feature set. We also provide\na two-sample hypothesis test for Lie group-valued random walks to illustrate\nits characteristic property. Finally we provide algorithms and a Julia\nimplementation of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:38:49 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 17:29:03 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lee", "Darrick", ""], ["Ghrist", "Robert", ""]]}, {"id": "2007.06634", "submitter": "Xiangmin Han", "authors": "Han Xiangmin, Wang Jun, Zhou Weijun, Chang Cai, Ying Shihui and Shi\n  Jun", "title": "Deep Doubly Supervised Transfer Network for Diagnosis of Breast Cancer\n  with Imbalanced Ultrasound Imaging Modalities", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastography ultrasound (EUS) provides additional bio-mechanical in-formation\nabout lesion for B-mode ultrasound (BUS) in the diagnosis of breast cancers.\nHowever, joint utilization of both BUS and EUS is not popular due to the lack\nof EUS devices in rural hospitals, which arouses a novel modality im-balance\nproblem in computer-aided diagnosis (CAD) for breast cancers. Current transfer\nlearning (TL) pay little attention to this special issue of clinical modality\nimbalance, that is, the source domain (EUS modality) has fewer labeled samples\nthan those in the target domain (BUS modality). Moreover, these TL methods\ncannot fully use the label information to explore the intrinsic relation\nbetween two modalities and then guide the promoted knowledge transfer. To this\nend, we propose a novel doubly supervised TL network (DDSTN) that integrates\nthe Learning Using Privileged Information (LUPI) paradigm and the Maximum Mean\nDiscrepancy (MMD) criterion into a unified deep TL framework. The proposed\nalgorithm can not only make full use of the shared labels to effectively guide\nknowledge transfer by LUPI paradigm, but also perform additional super-vised\ntransfer between unpaired data. We further introduce the MMD criterion to\nenhance the knowledge transfer. The experimental results on the breast\nultra-sound dataset indicate that the proposed DDSTN outperforms all the\ncompared state-of-the-art algorithms for the BUS-based CAD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 07:32:07 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Xiangmin", "Han", ""], ["Jun", "Wang", ""], ["Weijun", "Zhou", ""], ["Cai", "Chang", ""], ["Shihui", "Ying", ""], ["Jun", "Shi", ""]]}, {"id": "2007.06637", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Storing Encoded Episodes as Concepts for Continual Learning", "comments": "Accepted at ICML2020 (Workshop on Lifelong Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two main challenges faced by continual learning approaches are\ncatastrophic forgetting and memory limitations on the storage of data. To cope\nwith these challenges, we propose a novel, cognitively-inspired approach which\ntrains autoencoders with Neural Style Transfer to encode and store images.\nReconstructed images from encoded episodes are replayed when training the\nclassifier model on a new task to avoid catastrophic forgetting. The loss\nfunction for the reconstructed images is weighted to reduce its effect during\nclassifier training to cope with image degradation. When the system runs out of\nmemory the encoded episodes are converted into centroids and covariance\nmatrices, which are used to generate pseudo-images during classifier training,\nkeeping classifier performance stable with less memory. Our approach increases\nclassification accuracy by 13-17% over state-of-the-art methods on benchmark\ndatasets, while requiring 78% less storage space.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 04:15:56 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2007.06643", "submitter": "Kyle Min", "authors": "Kyle Min, Jason J. Corso", "title": "Adversarial Background-Aware Loss for Weakly-supervised Temporal\n  Activity Localization", "comments": "ECCV 2020 camera ready (Supplementary material: on ECVA soon)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporally localizing activities within untrimmed videos has been extensively\nstudied in recent years. Despite recent advances, existing methods for\nweakly-supervised temporal activity localization struggle to recognize when an\nactivity is not occurring. To address this issue, we propose a novel method\nnamed A2CL-PT. Two triplets of the feature space are considered in our\napproach: one triplet is used to learn discriminative features for each\nactivity class, and the other one is used to distinguish the features where no\nactivity occurs (i.e. background features) from activity-related features for\neach video. To further improve the performance, we build our network using two\nparallel branches which operate in an adversarial way: the first branch\nlocalizes the most salient activities of a video and the second one finds other\nsupplementary activities from non-localized parts of the video. Extensive\nexperiments performed on THUMOS14 and ActivityNet datasets demonstrate that our\nproposed method is effective. Specifically, the average mAP of IoU thresholds\nfrom 0.1 to 0.9 on the THUMOS14 dataset is significantly improved from 27.9% to\n30.0%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:33:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Min", "Kyle", ""], ["Corso", "Jason J.", ""]]}, {"id": "2007.06660", "submitter": "Yuli Wu", "authors": "Yuli Wu, Long Chen, Dorit Merhof", "title": "Improving Pixel Embedding Learning through Intermediate Distance\n  Regression Supervision for Instance Segmentation", "comments": "ECCV 2020 Workshop: Computer Vision Problems in Plant Phenotyping\n  (CVPPP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a proposal-free approach, instance segmentation through pixel embedding\nlearning and clustering is gaining more emphasis. Compared with bounding box\nrefinement approaches, such as Mask R-CNN, it has potential advantages in\nhandling complex shapes and dense objects. In this work, we propose a simple,\nyet highly effective, architecture for object-aware embedding learning. A\ndistance regression module is incorporated into our architecture to generate\nseeds for fast clustering. At the same time, we show that the features learned\nby the distance regression module are able to promote the accuracy of learned\nobject-aware embeddings significantly. By simply concatenating features of the\ndistance regression module to the images as inputs of the embedding module, the\nmSBD scores on the CVPPP Leaf Segmentation Challenge can be further improved by\nmore than 8% compared to the identical set-up without concatenation, yielding\nthe best overall result amongst the leaderboard at CodaLab.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:03:30 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wu", "Yuli", ""], ["Chen", "Long", ""], ["Merhof", "Dorit", ""]]}, {"id": "2007.06666", "submitter": "Junyan Wu", "authors": "Junyan Wu, Hao Jiang, Xiaowei Ding, Anudeep Konda, Jin Han, Yang\n  Zhang, Qian Li", "title": "Learning Differential Diagnosis of Skin Conditions with Co-occurrence\n  Supervision using Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin conditions are reported the 4th leading cause of nonfatal disease burden\nworldwide. However, given the colossal spectrum of skin disorders defined\nclinically and shortage in dermatology expertise, diagnosing skin conditions in\na timely and accurate manner remains a challenging task. Using computer vision\ntechnologies, a deep learning system has proven effective assisting clinicians\nin image diagnostics of radiology, ophthalmology and more. In this paper, we\npropose a deep learning system (DLS) that may predict differential diagnosis of\nskin conditions using clinical images. Our DLS formulates the differential\ndiagnostics as a multi-label classification task over 80 conditions when only\nincomplete image labels are available. We tackle the label incompleteness\nproblem by combining a classification network with a Graph Convolutional\nNetwork (GCN) that characterizes label co-occurrence and effectively\nregularizes it towards a sparse representation. Our approach is demonstrated on\n136,462 clinical images and concludes that the classification accuracy greatly\nbenefit from the Co-occurrence supervision. Our DLS achieves 93.6% top-5\naccuracy on 12,378 test images and consistently outperform the baseline\nclassification network.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:13:25 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wu", "Junyan", ""], ["Jiang", "Hao", ""], ["Ding", "Xiaowei", ""], ["Konda", "Anudeep", ""], ["Han", "Jin", ""], ["Zhang", "Yang", ""], ["Li", "Qian", ""]]}, {"id": "2007.06672", "submitter": "Lucas Soares", "authors": "Lucas P. Soares, Helen C. Dias, Carlos H. Grohmann", "title": "Landslide Segmentation with U-Net: Evaluating Different Sampling Methods\n  and Patch Sizes", "comments": "13 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landslide inventory maps are crucial to validate predictive landslide models;\nhowever, since most mapping methods rely on visual interpretation or expert\nknowledge, detailed inventory maps are still lacking. This study used a fully\nconvolutional deep learning model named U-net to automatically segment\nlandslides in the city of Nova Friburgo, located in the mountainous range of\nRio de Janeiro, southeastern Brazil. The objective was to evaluate the impact\nof patch sizes, sampling methods, and datasets on the overall accuracy of the\nmodels. The training data used the optical information from RapidEye satellite,\nand a digital elevation model (DEM) derived from the L-band sensor of the ALOS\nsatellite. The data was sampled using random and regular grid methods and\npatched in three sizes (32x32, 64x64, and 128x128 pixels). The models were\nevaluated on two areas with precision, recall, f1-score, and mean intersect\nover union (mIoU) metrics. The results show that the models trained with 32x32\ntiles tend to have higher recall values due to higher true positive rates;\nhowever, they misclassify more background areas as landslides (false\npositives). Models trained with 128x128 tiles usually achieve higher precision\nvalues because they make less false positive errors. In both test areas, DEM\nand augmentation increased the accuracy of the models. Random sampling helped\nin model generalization. Models trained with 128x128 random tiles from the data\nthat used the RapidEye image, DEM information, and augmentation achieved the\nhighest f1-score, 0.55 in test area one, and 0.58 in test area two. The results\nachieved in this study are comparable to other fully convolutional models found\nin the literature, increasing the knowledge in the area.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:28:46 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Soares", "Lucas P.", ""], ["Dias", "Helen C.", ""], ["Grohmann", "Carlos H.", ""]]}, {"id": "2007.06676", "submitter": "Senthil Yogamani", "authors": "Varun Ravi Kumar, Senthil Yogamani, Markus Bach, Christian Witt,\n  Stefan Milz and Patrick Mader", "title": "UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a\n  Generic Framework for Handling Common Camera Distortion Models", "comments": "Minor fixes added after IROS 2020 Camera ready submission. IROS 2020\n  presentation video - https://www.youtube.com/watch?v=3Br2KSWZRrY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical computer vision, rectification is an integral part of multi-view\ndepth estimation. It typically includes epipolar rectification and lens\ndistortion correction. This process simplifies the depth estimation\nsignificantly, and thus it has been adopted in CNN approaches. However,\nrectification has several side effects, including a reduced field of view\n(FOV), resampling distortion, and sensitivity to calibration errors. The\neffects are particularly pronounced in case of significant distortion (e.g.,\nwide-angle fisheye cameras). In this paper, we propose a generic scale-aware\nself-supervised pipeline for estimating depth, euclidean distance, and visual\nodometry from unrectified monocular videos. We demonstrate a similar level of\nprecision on the unrectified KITTI dataset with barrel distortion comparable to\nthe rectified KITTI dataset. The intuition being that the rectification step\ncan be implicitly absorbed within the CNN model, which learns the distortion\nmodel without increasing complexity. Our approach does not suffer from a\nreduced field of view and avoids computational costs for rectification at\ninference time. To further illustrate the general applicability of the proposed\nframework, we apply it to wide-angle fisheye cameras with 190$^\\circ$\nhorizontal field of view. The training framework UnRectDepthNet takes in the\ncamera distortion model as an argument and adapts projection and unprojection\nfunctions accordingly. The proposed algorithm is evaluated further on the KITTI\nrectified dataset, and we achieve state-of-the-art results that improve upon\nour previous work FisheyeDistanceNet. Qualitative results on a distorted test\nscene video sequence indicate excellent performance\nhttps://youtu.be/K6pbx3bU4Ss.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:35:05 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 14:04:24 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 19:41:35 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""], ["Bach", "Markus", ""], ["Witt", "Christian", ""], ["Milz", "Stefan", ""], ["Mader", "Patrick", ""]]}, {"id": "2007.06682", "submitter": "Robert Ravier", "authors": "Robert J. Ravier, Mohammadreza Soltani, Miguel Sim\\~oes, Denis\n  Garagic, Vahid Tarokh", "title": "GeoStat Representations of Time Series for Fast Classification", "comments": "28 pages, 8 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in time series classification have largely focused on methods\nthat either employ deep learning or utilize other machine learning models for\nfeature extraction. Though successful, their power often comes at the\nrequirement of computational complexity. In this paper, we introduce GeoStat\nrepresentations for time series. GeoStat representations are based off of a\ngeneralization of recent methods for trajectory classification, and summarize\nthe information of a time series in terms of comprehensive statistics of\n(possibly windowed) distributions of easy to compute differential geometric\nquantities, requiring no dynamic time warping. The features used are intuitive\nand require minimal parameter tuning. We perform an exhaustive evaluation of\nGeoStat on a number of real datasets, showing that simple KNN and SVM\nclassifiers trained on these representations exhibit surprising performance\nrelative to modern single model methods requiring significant computational\npower, achieving state of the art results in many cases. In particular, we show\nthat this methodology achieves good performance on a challenging dataset\ninvolving the classification of fishing vessels, where our methods achieve good\nperformance relative to the state of the art despite only having access to\napproximately two percent of the dataset used in training and evaluating this\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:48:03 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 18:44:10 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 22:03:10 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ravier", "Robert J.", ""], ["Soltani", "Mohammadreza", ""], ["Sim\u00f5es", "Miguel", ""], ["Garagic", "Denis", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2007.06705", "submitter": "Paul Henderson", "authors": "Paul Henderson and Christoph H. Lampert", "title": "Unsupervised object-centric video generation and decomposition in 3D", "comments": "Appeared at NeurIPS 2020. Project page: http://pmh47.net/o3v/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural approach to generative modeling of videos is to represent them as a\ncomposition of moving objects. Recent works model a set of 2D sprites over a\nslowly-varying background, but without considering the underlying 3D scene that\ngives rise to them. We instead propose to model a video as the view seen while\nmoving through a scene with multiple 3D objects and a 3D background. Our model\nis trained from monocular videos without any supervision, yet learns to\ngenerate coherent 3D scenes containing several moving objects. We conduct\ndetailed experiments on two datasets, going beyond the visual complexity\nsupported by state-of-the-art generative approaches. We evaluate our method on\ndepth-prediction and 3D object detection -- tasks which cannot be addressed by\nthose earlier works -- and show it out-performs them even on 2D instance\nsegmentation and tracking.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:01:29 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 19:11:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Henderson", "Paul", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2007.06706", "submitter": "Boyang Lyu", "authors": "Boyang Lyu, Thao Pham, Giles Blaney, Zachary Haga, Angelo Sassaroli,\n  Sergio Fantini, Shuchin Aeron", "title": "Domain Adaptation for Robust Workload Level Alignment Between Sessions\n  and Subjects using fNIRS", "comments": null, "journal-ref": null, "doi": "10.1117/1.JBO.26.2.022908", "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significance: We demonstrated the potential of using domain adaptation on\nfunctional Near-Infrared Spectroscopy (fNIRS) data to classify different levels\nof n-back tasks that involve working memory. Aim: Domain shift in fNIRS data is\na challenge in the workload level alignment across different experiment\nsessions and subjects. In order to address this problem, two domain adaptation\napproaches -- Gromov-Wasserstein (G-W) and Fused Gromov-Wasserstein (FG-W) were\nused. Approach: Specifically, we used labeled data from one session or one\nsubject to classify trials in another session (within the same subject) or\nanother subject. We applied G-W for session-by-session alignment and FG-W for\nsubject-by-subject alignment to fNIRS data acquired during different n-back\ntask levels. We compared these approaches with three supervised methods:\nmulti-class Support Vector Machine (SVM), Convolutional Neural Network (CNN),\nand Recurrent Neural Network (RNN). Results: In a sample of six subjects, G-W\nresulted in an alignment accuracy of 68 $\\pm$ 4 % (weighted mean $\\pm$ standard\nerror) for session-by-session alignment, FG-W resulted in an alignment accuracy\nof 55 $\\pm$ 2 % for subject-by-subject alignment. In each of these cases, 25 %\naccuracy represents chance. Alignment accuracy results from both G-W and FG-W\nare significantly greater than those from SVM, CNN and RNN. We also showed that\nremoval of motion artifacts from the fNIRS data plays an important role in\nimproving alignment performance. Conclusions: Domain adaptation has potential\nfor session-by-session and subject-by-subject alignment of mental workload by\nusing fNIRS data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 18:03:50 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 01:52:13 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lyu", "Boyang", ""], ["Pham", "Thao", ""], ["Blaney", "Giles", ""], ["Haga", "Zachary", ""], ["Sassaroli", "Angelo", ""], ["Fantini", "Sergio", ""], ["Aeron", "Shuchin", ""]]}, {"id": "2007.06708", "submitter": "Todd Murphey", "authors": "Taosha Fan, Hanlin Wang, Michael Rubenstein and Todd Murphey", "title": "CPL-SLAM: Efficient and Certifiably Correct Planar Graph-Based SLAM\n  Using the Complex Number Representation", "comments": null, "journal-ref": "IEEE Transactions on Robotics, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of planar graph-based simultaneous\nlocalization and mapping (SLAM) that involves both poses of the autonomous\nagent and positions of observed landmarks. We present CPL-SLAM, an efficient\nand certifiably correct algorithm to solve planar graph-based SLAM using the\ncomplex number representation. We formulate and simplify planar graph-based\nSLAM as the maximum likelihood estimation (MLE) on the product of unit complex\nnumbers, and relax this nonconvex quadratic complex optimization problem to\nconvex complex semidefinite programming (SDP). Furthermore, we simplify the\ncorresponding complex semidefinite programming to Riemannian staircase\noptimization (RSO) on the complex oblique manifold that can be solved with the\nRiemannian trust region (RTR) method. In addition, we prove that the SDP\nrelaxation and RSO simplification are tight as long as the noise magnitude is\nbelow a certain threshold. The efficacy of this work is validated through\napplications of CPL-SLAM and comparisons with existing state-of-the-art methods\non planar graph-based SLAM, which indicates that our proposed algorithm is\ncapable of solving planar graph-based SLAM certifiably, and is more efficient\nin numerical computation and more robust to measurement noise than existing\nstate-of-the-art methods. The C++ code for CPL-SLAM is available at\nhttps://github.com/MurpheyLab/CPL-SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 21:17:50 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fan", "Taosha", ""], ["Wang", "Hanlin", ""], ["Rubenstein", "Michael", ""], ["Murphey", "Todd", ""]]}, {"id": "2007.06709", "submitter": "Subhadip Maji", "authors": "Subhadip Maji and Smarajit Bose", "title": "Deep Image Orientation Angle Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating and rectifying the orientation angle of any image is a pretty\nchallenging task. Initial work used the hand engineering features for this\npurpose, where after the invention of deep learning using convolution-based\nneural network showed significant improvement in this problem. However, this\npaper shows that the combination of CNN and a custom loss function specially\ndesigned for angles lead to a state-of-the-art results. This includes the\nestimation of the orientation angle of any image or document at any degree (0\nto 360 degree),\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 14:24:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Maji", "Subhadip", ""], ["Bose", "Smarajit", ""]]}, {"id": "2007.06710", "submitter": "Amogh Warkhandkar", "authors": "Amogh G. Warkhandkar, Baasit Sharief and Omkar B. Bhambure", "title": "Measuring Performance of Generative Adversarial Networks on Devanagari\n  Script", "comments": "5 pages, 5 figures", "journal-ref": "International Journal of Computer Applications, Volume 176 -\n  No.33, June 2020, Pages 5-9", "doi": "10.5120/ijca2020920393", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The working of neural networks following the adversarial philosophy to create\na generative model is a fascinating field. Multiple papers have already\nexplored the architectural aspect and proposed systems with potentially good\nresults however, very few papers are available which implement it on a\nreal-world example. Traditionally, people use the famous MNIST dataset as a\nHello, World! example for implementing Generative Adversarial Networks (GAN).\nInstead of going the standard route of using handwritten digits, this paper\nuses the Devanagari script which has a more complex structure. As there is no\nconventional way of judging how well the generative models perform, three\nadditional classifiers were built to judge the output of the GAN model. The\nfollowing paper is an explanation of what this implementation has achieved.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:20:51 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Warkhandkar", "Amogh G.", ""], ["Sharief", "Baasit", ""], ["Bhambure", "Omkar B.", ""]]}, {"id": "2007.06711", "submitter": "Derek Prijatelj", "authors": "Derek S. Prijatelj (1), Mel McCurrie (2), Walter J. Scheirer (1) ((1)\n  University of Notre Dame, Notre Dame, USA, (2) Perceptive Automata, Boston,\n  USA)", "title": "A Bayesian Evaluation Framework for Ground Truth-Free Visual Recognition\n  Tasks", "comments": "21 pages. 11 figures. 2 tables. Submitted to NeurIPS2020. Code to be\n  included after publication at\n  https://github.com/prijatelj/bayesian_eval_ground_truth-free", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An interesting development in automatic visual recognition has been the\nemergence of tasks where it is not possible to assign ground truth labels to\nimages, yet still feasible to collect annotations that reflect human judgements\nabout them. Such tasks include subjective visual attribute assignment and the\nlabeling of ambiguous scenes. Machine learning-based predictors for these tasks\nrely on supervised training that models the behavior of the annotators, e.g.,\nwhat would the average person's judgement be for an image? A key open question\nfor this type of work, especially for applications where inconsistency with\nhuman behavior can lead to ethical lapses, is how to evaluate the uncertainty\nof trained predictors. Given that the real answer is unknowable, we are left\nwith often noisy judgements from human annotators to work with. In order to\naccount for the uncertainty that is present, we propose a relative Bayesian\nframework for evaluating predictors trained on such data. The framework\nspecifies how to estimate a predictor's uncertainty due to the human labels by\napproximating a conditional distribution and producing a credible interval for\nthe predictions and their measures of performance. The framework is\nsuccessfully applied to four image classification tasks that use subjective\nhuman judgements: facial beauty assessment using the SCUT-FBP5500 dataset,\nsocial attribute assignment using data from TestMyBrain.org, apparent age\nestimation using data from the ChaLearn series of challenges, and ambiguous\nscene labeling using the LabelMe dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:35:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Prijatelj", "Derek S.", ""], ["McCurrie", "Mel", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "2007.06712", "submitter": "Amirhossein Tavanaei", "authors": "Amirhossein Tavanaei", "title": "Embedded Encoder-Decoder in Convolutional Networks Towards Explainable\n  AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding intermediate layers of a deep learning model and discovering\nthe driving features of stimuli have attracted much interest, recently.\nExplainable artificial intelligence (XAI) provides a new way to open an AI\nblack box and makes a transparent and interpretable decision. This paper\nproposes a new explainable convolutional neural network (XCNN) which represents\nimportant and driving visual features of stimuli in an end-to-end model\narchitecture. This network employs encoder-decoder neural networks in a CNN\narchitecture to represent regions of interest in an image based on its\ncategory. The proposed model is trained without localization labels and\ngenerates a heat-map as part of the network architecture without extra\npost-processing steps. The experimental results on the CIFAR-10, Tiny ImageNet,\nand MNIST datasets showed the success of our algorithm (XCNN) to make CNNs\nexplainable. Based on visual assessment, the proposed model outperforms the\ncurrent algorithms in class-specific feature representation and interpretable\nheatmap generation while providing a simple and flexible network architecture.\nThe initial success of this approach warrants further study to enhance weakly\nsupervised localization and semantic segmentation in explainable frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:49:39 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Tavanaei", "Amirhossein", ""]]}, {"id": "2007.06716", "submitter": "Ali Memariani", "authors": "Ali Memariani and Ioannis A. Kakadiaris", "title": "DETCID: Detection of Elongated Touching Cells with Inhomogeneous\n  Illumination using a Deep Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clostridioides difficile infection (C. diff) is the most common cause of\ndeath due to secondary infection in hospital patients in the United States.\nDetection of C. diff cells in scanning electron microscopy (SEM) images is an\nimportant task to quantify the efficacy of the under-development treatments.\nHowever, detecting C. diff cells in SEM images is a challenging problem due to\nthe presence of inhomogeneous illumination and occlusion. An Illumination\nnormalization pre-processing step destroys the texture and adds noise to the\nimage. Furthermore, cells are often clustered together resulting in touching\ncells and occlusion. In this paper, DETCID, a deep cell detection method using\nadversarial training, specifically robust to inhomogeneous illumination and\nocclusion, is proposed. An adversarial network is developed to provide region\nproposals and pass the proposals to a feature extraction network. Furthermore,\na modified IoU metric is developed to allow the detection of touching cells in\nvarious orientations. The results indicate that DETCID outperforms the\nstate-of-the-art in detection of touching cells in SEM images by at least 20\npercent improvement of mean average precision.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 21:43:27 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Memariani", "Ali", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "2007.06749", "submitter": "Priyanka Chaudhary", "authors": "P. Chaudhary, S. D'Aronco, J.P. Leitao, K. Schindler, J.D. Wegner", "title": "Water level prediction from social media images with a multi-task\n  ranking approach", "comments": "Accepted in ISPRS Journal 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floods are among the most frequent and catastrophic natural disasters and\naffect millions of people worldwide. It is important to create accurate flood\nmaps to plan (offline) and conduct (real-time) flood mitigation and flood\nrescue operations. Arguably, images collected from social media can provide\nuseful information for that task, which would otherwise be unavailable. We\nintroduce a computer vision system that estimates water depth from social media\nimages taken during flooding events, in order to build flood maps in (near)\nreal-time. We propose a multi-task (deep) learning approach, where a model is\ntrained using both a regression and a pairwise ranking loss. Our approach is\nmotivated by the observation that a main bottleneck for image-based flood level\nestimation is training data: it is diffcult and requires a lot of effort to\nannotate uncontrolled images with the correct water depth. We demonstrate how\nto effciently learn a predictor from a small set of annotated water levels and\na larger set of weaker annotations that only indicate in which of two images\nthe water level is higher, and are much easier to obtain. Moreover, we provide\na new dataset, named DeepFlood, with 8145 annotated ground-level images, and\nshow that the proposed multi-task approach can predict the water level from a\nsingle, crowd-sourced image with ~11 cm root mean square error.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 00:51:29 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chaudhary", "P.", ""], ["D'Aronco", "S.", ""], ["Leitao", "J. P.", ""], ["Schindler", "K.", ""], ["Wegner", "J. D.", ""]]}, {"id": "2007.06753", "submitter": "Qing Qu", "authors": "Yuqian Zhang, Qing Qu, and John Wright", "title": "From Symmetry to Geometry: Tractable Nonconvex Problems", "comments": "review paper submitted to SIAM Review, 34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As science and engineering have become increasingly data-driven, the role of\noptimization has expanded to touch almost every stage of the data analysis\npipeline, from the signal and data acquisition to modeling and prediction. The\noptimization problems encountered in practice are often nonconvex. While\nchallenges vary from problem to problem, one common source of nonconvexity is\nnonlinearity in the data or measurement model. Nonlinear models often exhibit\nsymmetries, creating complicated, nonconvex objective landscapes, with multiple\nequivalent solutions. Nevertheless, simple methods (e.g., gradient descent)\noften perform surprisingly well in practice.\n  The goal of this survey is to highlight a class of tractable nonconvex\nproblems, which can be understood through the lens of symmetries. These\nproblems exhibit a characteristic geometric structure: local minimizers are\nsymmetric copies of a single \"ground truth\" solution, while other critical\npoints occur at balanced superpositions of symmetric copies of the ground\ntruth, and exhibit negative curvature in directions that break the symmetry.\nThis structure enables efficient methods to obtain global minimizers. We\ndiscuss examples of this phenomenon arising from a wide range of problems in\nimaging, signal processing, and data analysis. We highlight the key role of\nsymmetry in shaping the objective landscape and discuss the different roles of\nrotational and discrete symmetries. This area is rich with observed phenomena\nand open problems; we close by highlighting directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:19:15 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 03:11:17 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 04:03:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Yuqian", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "2007.06755", "submitter": "Noranart Vesdapunt", "authors": "Noranart Vesdapunt, Mitch Rundle, HsiangTao Wu, Baoyuan Wang", "title": "JNR: Joint-based Neural Rig Representation for Compact 3D Face Modeling", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel approach to learn a 3D face model using a\njoint-based face rig and a neural skinning network. Thanks to the joint-based\nrepresentation, our model enjoys some significant advantages over prior\nblendshape-based models. First, it is very compact such that we are orders of\nmagnitude smaller while still keeping strong modeling capacity. Second, because\neach joint has its semantic meaning, interactive facial geometry editing is\nmade easier and more intuitive. Third, through skinning, our model supports\nadding mouth interior and eyes, as well as accessories (hair, eye glasses,\netc.) in a simpler, more accurate and principled way. We argue that because the\nhuman face is highly structured and topologically consistent, it does not need\nto be learned entirely from data. Instead we can leverage prior knowledge in\nthe form of a human-designed 3D face rig to reduce the data dependency, and\nlearn a compact yet strong face model from only a small dataset (less than one\nhundred 3D scans). To further improve the modeling capacity, we train a\nskinning weight generator through adversarial learning. Experiments on fitting\nhigh-quality 3D scans (both neutral and expressive), noisy depth images, and\nRGB images demonstrate that its modeling capacity is on-par with\nstate-of-the-art face models, such as FLAME and Facewarehouse, even though the\nmodel is 10 to 20 times smaller. This suggests broad value in both graphics and\nvision applications on mobile and edge devices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:21:37 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 01:26:18 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 23:35:13 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vesdapunt", "Noranart", ""], ["Rundle", "Mitch", ""], ["Wu", "HsiangTao", ""], ["Wang", "Baoyuan", ""]]}, {"id": "2007.06759", "submitter": "Noranart Vesdapunt", "authors": "Bindita Chaudhuri, Noranart Vesdapunt, Linda Shapiro, Baoyuan Wang", "title": "Personalized Face Modeling for Improved Face Reconstruction and Motion\n  Retargeting", "comments": "ECCV 2020 (spotlight), webpage:\n  https://homes.cs.washington.edu/~bindita/personalizedfacemodeling.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for image-based 3D face reconstruction and facial motion\nretargeting fit a 3D morphable model (3DMM) to the face, which has limited\nmodeling capacity and fail to generalize well to in-the-wild data. Use of\ndeformation transfer or multilinear tensor as a personalized 3DMM for\nblendshape interpolation does not address the fact that facial expressions\nresult in different local and global skin deformations in different persons.\nMoreover, existing methods learn a single albedo per user which is not enough\nto capture the expression-specific skin reflectance variations. We propose an\nend-to-end framework that jointly learns a personalized face model per user and\nper-frame facial motion parameters from a large corpus of in-the-wild videos of\nuser expressions. Specifically, we learn user-specific expression blendshapes\nand dynamic (expression-specific) albedo maps by predicting personalized\ncorrections on top of a 3DMM prior. We introduce novel constraints to ensure\nthat the corrected blendshapes retain their semantic meanings and the\nreconstructed geometry is disentangled from the albedo. Experimental results\nshow that our personalization accurately captures fine-grained facial dynamics\nin a wide range of conditions and efficiently decouples the learned face model\nfrom facial motion, resulting in more accurate face reconstruction and facial\nmotion retargeting compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:30:14 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 23:08:43 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chaudhuri", "Bindita", ""], ["Vesdapunt", "Noranart", ""], ["Shapiro", "Linda", ""], ["Wang", "Baoyuan", ""]]}, {"id": "2007.06765", "submitter": "Qilong Zhang", "authors": "Lianli Gao and Qilong Zhang and Jingkuan Song and Xianglong Liu and\n  Heng Tao Shen", "title": "Patch-wise Attack for Fooling Deep Neural Network", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By adding human-imperceptible noise to clean images, the resultant\nadversarial examples can fool other unknown models. Features of a pixel\nextracted by deep neural networks (DNNs) are influenced by its surrounding\nregions, and different DNNs generally focus on different discriminative regions\nin recognition. Motivated by this, we propose a patch-wise iterative algorithm\n-- a black-box attack towards mainstream normally trained and defense models,\nwhich differs from the existing attack methods manipulating pixel-wise noise.\nIn this way, without sacrificing the performance of white-box attack, our\nadversarial examples can have strong transferability. Specifically, we\nintroduce an amplification factor to the step size in each iteration, and one\npixel's overall gradient overflowing the $\\epsilon$-constraint is properly\nassigned to its surrounding regions by a project kernel. Our method can be\ngenerally integrated to any gradient-based attack methods. Compared with the\ncurrent state-of-the-art attacks, we significantly improve the success rate by\n9.2\\% for defense models and 3.7\\% for normally trained models on average. Our\ncode is available at\n\\url{https://github.com/qilong-zhang/Patch-wise-iterative-attack}\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:50:22 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 01:40:43 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 05:22:29 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gao", "Lianli", ""], ["Zhang", "Qilong", ""], ["Song", "Jingkuan", ""], ["Liu", "Xianglong", ""], ["Shen", "Heng Tao", ""]]}, {"id": "2007.06769", "submitter": "Minchul Shin", "authors": "Minchul Shin", "title": "Semi-supervised Learning with a Teacher-student Network for Generalized\n  Attribute Prediction", "comments": "14 pages, Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": "1381", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a study on semi-supervised learning to solve the visual\nattribute prediction problem. In many applications of vision algorithms, the\nprecise recognition of visual attributes of objects is important but still\nchallenging. This is because defining a class hierarchy of attributes is\nambiguous, so training data inevitably suffer from class imbalance and label\nsparsity, leading to a lack of effective annotations. An intuitive solution is\nto find a method to effectively learn image representations by utilizing\nunlabeled images. With that in mind, we propose a multi-teacher-single-student\n(MTSS) approach inspired by the multi-task learning and the distillation of\nsemi-supervised learning. Our MTSS learns task-specific domain experts called\nteacher networks using the label embedding technique and learns a unified model\ncalled a student network by forcing a model to mimic the distributions learned\nby domain experts. Our experiments demonstrate that our method not only\nachieves competitive performance on various benchmarks for fashion attribute\nprediction, but also improves robustness and cross-domain adaptability for\nunseen domains.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:06:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Shin", "Minchul", ""]]}, {"id": "2007.06781", "submitter": "Nick Lamm", "authors": "Nick Lamm, Shashank Jaiprakash, Malavika Srikanth, Iddo Drori", "title": "Vehicle Trajectory Prediction by Transfer Learning of Semi-Supervised\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that semi-supervised models for vehicle trajectory\nprediction significantly improve performance over supervised models on\nstate-of-the-art real-world benchmarks. Moving from supervised to\nsemi-supervised models allows scaling-up by using unlabeled data, increasing\nthe number of images in pre-training from Millions to a Billion. We perform\nablation studies comparing transfer learning of semi-supervised and supervised\nmodels while keeping all other factors equal. Within semi-supervised models we\ncompare contrastive learning with teacher-student methods as well as networks\npredicting a small number of trajectories with networks predicting\nprobabilities over a large trajectory set. Our results using both low-level and\nmid-level representations of the driving environment demonstrate the\napplicability of semi-supervised methods for real-world vehicle trajectory\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:42:48 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 01:51:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lamm", "Nick", ""], ["Jaiprakash", "Shashank", ""], ["Srikanth", "Malavika", ""], ["Drori", "Iddo", ""]]}, {"id": "2007.06786", "submitter": "Eugene Lee", "authors": "Eugene Lee, Evan Chen, Chen-Yi Lee", "title": "Meta-rPPG: Remote Heart Rate Estimation Using a Transductive\n  Meta-Learner", "comments": "26 pages, 10 figures, accepted by European Conference on Computer\n  Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Remote heart rate estimation is the measurement of heart rate without any\nphysical contact with the subject and is accomplished using remote\nphotoplethysmography (rPPG) in this work. rPPG signals are usually collected\nusing a video camera with a limitation of being sensitive to multiple\ncontributing factors, e.g. variation in skin tone, lighting condition and\nfacial structure. End-to-end supervised learning approach performs well when\ntraining data is abundant, covering a distribution that doesn't deviate too\nmuch from the distribution of testing data or during deployment. To cope with\nthe unforeseeable distributional changes during deployment, we propose a\ntransductive meta-learner that takes unlabeled samples during testing\n(deployment) for a self-supervised weight adjustment (also known as\ntransductive inference), providing fast adaptation to the distributional\nchanges. Using this approach, we achieve state-of-the-art performance on\nMAHNOB-HCI and UBFC-rPPG.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 03:01:46 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Lee", "Eugene", ""], ["Chen", "Evan", ""], ["Lee", "Chen-Yi", ""]]}, {"id": "2007.06793", "submitter": "Yilun Xu", "authors": "Xinwei Sun, Yilun Xu, Peng Cao, Yuqing Kong, Lingjing Hu, Shanghang\n  Zhang, Yizhou Wang", "title": "TCGM: An Information-Theoretic Framework for Semi-Supervised\n  Multi-Modality Learning", "comments": "ECCV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing data from multiple modalities provides more information to train\nmachine learning systems. However, it is prohibitively expensive and\ntime-consuming to label each modality with a large amount of data, which leads\nto a crucial problem of semi-supervised multi-modal learning. Existing methods\nsuffer from either ineffective fusion across modalities or lack of theoretical\nguarantees under proper assumptions. In this paper, we propose a novel\ninformation-theoretic approach, namely \\textbf{T}otal \\textbf{C}orrelation\n\\textbf{G}ain \\textbf{M}aximization (TCGM), for semi-supervised multi-modal\nlearning, which is endowed with promising properties: (i) it can utilize\neffectively the information across different modalities of unlabeled data\npoints to facilitate training classifiers of each modality (ii) it has\ntheoretical guarantee to identify Bayesian classifiers, i.e., the ground truth\nposteriors of all modalities. Specifically, by maximizing TC-induced loss\n(namely TC gain) over classifiers of all modalities, these classifiers can\ncooperatively discover the equivalent class of ground-truth classifiers; and\nidentify the unique ones by leveraging limited percentage of labeled data. We\napply our method to various tasks and achieve state-of-the-art results,\nincluding news classification, emotion recognition and disease prediction.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 03:32:03 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Sun", "Xinwei", ""], ["Xu", "Yilun", ""], ["Cao", "Peng", ""], ["Kong", "Yuqing", ""], ["Hu", "Lingjing", ""], ["Zhang", "Shanghang", ""], ["Wang", "Yizhou", ""]]}, {"id": "2007.06809", "submitter": "Farzan Shenavarmasouleh", "authors": "Ehsan Asali, Farzan Shenavarmasouleh, Farid Ghareh Mohammadi, Prasanth\n  Sengadu Suresh, and Hamid R. Arabnia", "title": "DeepMSRF: A novel Deep Multimodal Speaker Recognition framework with\n  Feature selection", "comments": "The 24th International Conference on Image Processing, Computer\n  Vision, & Pattern Recognition (IPCV'20: July 27-30, 2020, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For recognizing speakers in video streams, significant research studies have\nbeen made to obtain a rich machine learning model by extracting high-level\nspeaker's features such as facial expression, emotion, and gender. However,\ngenerating such a model is not feasible by using only single modality feature\nextractors that exploit either audio signals or image frames, extracted from\nvideo streams. In this paper, we address this problem from a different\nperspective and propose an unprecedented multimodality data fusion framework\ncalled DeepMSRF, Deep Multimodal Speaker Recognition with Feature selection. We\nexecute DeepMSRF by feeding features of the two modalities, namely speakers'\naudios and face images. DeepMSRF uses a two-stream VGGNET to train on both\nmodalities to reach a comprehensive model capable of accurately recognizing the\nspeaker's identity. We apply DeepMSRF on a subset of VoxCeleb2 dataset with its\nmetadata merged with VGGFace2 dataset. The goal of DeepMSRF is to identify the\ngender of the speaker first, and further to recognize his or her name for any\ngiven video stream. The experimental results illustrate that DeepMSRF\noutperforms single modality speaker recognition methods with at least 3 percent\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 04:28:12 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 05:55:02 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Asali", "Ehsan", ""], ["Shenavarmasouleh", "Farzan", ""], ["Mohammadi", "Farid Ghareh", ""], ["Suresh", "Prasanth Sengadu", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2007.06811", "submitter": "Xiaoqi Zhao", "authors": "Xiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, Lei Zhang", "title": "A Single Stream Network for Robust and Real-time RGB-D Salient Object\n  Detection", "comments": "Accepted in ECCV2020. Code:\n  https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing RGB-D salient object detection (SOD) approaches concentrate on the\ncross-modal fusion between the RGB stream and the depth stream. They do not\ndeeply explore the effect of the depth map itself. In this work, we design a\nsingle stream network to directly use the depth map to guide early fusion and\nmiddle fusion between RGB and depth, which saves the feature encoder of the\ndepth stream and achieves a lightweight and real-time model. We tactfully\nutilize depth information from two perspectives: (1) Overcoming the\nincompatibility problem caused by the great difference between modalities, we\nbuild a single stream encoder to achieve the early fusion, which can take full\nadvantage of ImageNet pre-trained backbone model to extract rich and\ndiscriminative features. (2) We design a novel depth-enhanced dual attention\nmodule (DEDA) to efficiently provide the fore-/back-ground branches with the\nspatially filtered features, which enables the decoder to optimally perform the\nmiddle fusion. Besides, we put forward a pyramidally attended feature\nextraction module (PAFE) to accurately localize the objects of different\nscales. Extensive experiments demonstrate that the proposed model performs\nfavorably against most state-of-the-art methods under different evaluation\nmetrics. Furthermore, this model is 55.5\\% lighter than the current lightest\nmodel and runs at a real-time speed of 32 FPS when processing a $384 \\times\n384$ image.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 04:40:14 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 02:00:22 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhao", "Xiaoqi", ""], ["Zhang", "Lihe", ""], ["Pang", "Youwei", ""], ["Lu", "Huchuan", ""], ["Zhang", "Lei", ""]]}, {"id": "2007.06837", "submitter": "Qian Li", "authors": "Qian Li, Nan Guo, Xiaochun Ye, Duo Wang, Dongrui Fan and Zhimin Tang", "title": "Top-Related Meta-Learning Method for Few-Shot Object Detection", "comments": "meta-learing,few-shot learning, object detection, category-based\n  grouping mechanism", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many meta-learning methods are proposed for few-shot detection. However,\nprevious most methods have two main problems, poor detection APs, and strong\nbias because of imbalance and insufficient datasets. Previous works mainly\nalleviate these issues by additional datasets, multi-relation attention\nmechanisms and sub-modules. However, they require more cost. In this work, for\nmeta-learning, we find that the main challenges focus on related or irrelevant\nsemantic features between categories. Therefore, based on semantic features, we\npropose a Top-C classification loss (i.e., TCL-C) for classification task and a\ncategory-based grouping mechanism for category-based meta-features obtained by\nthe meta-model. The TCL-C exploits the true-label prediction and the most\nlikely C-1 false classification predictions to improve detection performance on\nfew-shot classes. According to similar appearance (i.e., visual appearance,\nshape, and limbs etc.) and environment in which objects often appear, the\ncategory-based grouping mechanism splits categories into disjoint groups to\nmake similar semantic features more compact between categories within a group\nand obtain more significant difference between groups, alleviating the strong\nbias problem and further improving detection APs. The whole training consists\nof the base model and the fine-tuning phases. According to grouping mechanism,\nwe group the meta-features vectors obtained by meta-model, so that the\ndistribution difference between groups is obvious, and the one within each\ngroup is less. Extensive experiments on Pascal VOC dataset demonstrate that\nours which combines the TCL-C with category-based grouping significantly\noutperforms previous state-of-the-art methods for few-shot detection. Compared\nwith previous competitive baseline, ours improves detection APs by almost 4%\nfor few-shot detection.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:52:14 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 06:43:38 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 05:16:57 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 03:57:37 GMT"}, {"version": "v5", "created": "Fri, 20 Nov 2020 03:27:22 GMT"}, {"version": "v6", "created": "Tue, 15 Jun 2021 08:29:50 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Li", "Qian", ""], ["Guo", "Nan", ""], ["Ye", "Xiaochun", ""], ["Wang", "Duo", ""], ["Fan", "Dongrui", ""], ["Tang", "Zhimin", ""]]}, {"id": "2007.06842", "submitter": "Zhe Liu", "authors": "Zhe Liu, Xianzhi Wang, Lina Yao, Jake An, Lei Bai, Ee-Peng Lim", "title": "Face to Purchase: Predicting Consumer Choices with Structured Facial and\n  Behavioral Traits Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting consumers' purchasing behaviors is critical for targeted\nadvertisement and sales promotion in e-commerce. Human faces are an invaluable\nsource of information for gaining insights into consumer personality and\nbehavioral traits. However, consumer's faces are largely unexplored in previous\nresearch, and the existing face-related studies focus on high-level features\nsuch as personality traits while neglecting the business significance of\nlearning from facial data. We propose to predict consumers' purchases based on\ntheir facial features and purchasing histories. We design a semi-supervised\nmodel based on a hierarchical embedding network to extract high-level features\nof consumers and to predict the top-$N$ purchase destinations of a consumer.\nOur experimental results on a real-world dataset demonstrate the positive\neffect of incorporating facial information in predicting consumers' purchasing\nbehaviors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 06:06:41 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Zhe", ""], ["Wang", "Xianzhi", ""], ["Yao", "Lina", ""], ["An", "Jake", ""], ["Bai", "Lei", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2007.06843", "submitter": "Vida Adeli", "authors": "Vida Adeli, Ehsan Adeli, Ian Reid, Juan Carlos Niebles, Hamid\n  Rezatofighi", "title": "Socially and Contextually Aware Human Motion and Pose Forecasting", "comments": "Accepted in RA-L and IROS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smooth and seamless robot navigation while interacting with humans depends on\npredicting human movements. Forecasting such human dynamics often involves\nmodeling human trajectories (global motion) or detailed body joint movements\n(local motion). Prior work typically tackled local and global human movements\nseparately. In this paper, we propose a novel framework to tackle both tasks of\nhuman motion (or trajectory) and body skeleton pose forecasting in a unified\nend-to-end pipeline. To deal with this real-world problem, we consider\nincorporating both scene and social contexts, as critical clues for this\nprediction task, into our proposed framework. To this end, we first couple\nthese two tasks by i) encoding their history using a shared Gated Recurrent\nUnit (GRU) encoder and ii) applying a metric as loss, which measures the source\nof errors in each task jointly as a single distance. Then, we incorporate the\nscene context by encoding a spatio-temporal representation of the video data.\nWe also include social clues by generating a joint feature representation from\nmotion and pose of all individuals from the scene using a social pooling layer.\nFinally, we use a GRU based decoder to forecast both motion and skeleton pose.\nWe demonstrate that our proposed framework achieves a superior performance\ncompared to several baselines on two social datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 06:12:13 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Adeli", "Vida", ""], ["Adeli", "Ehsan", ""], ["Reid", "Ian", ""], ["Niebles", "Juan Carlos", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "2007.06853", "submitter": "Chao Li", "authors": "Chao Li and Xiaohu Guo", "title": "Topology-Change-Aware Volumetric Fusion for Dynamic Scene Reconstruction", "comments": "European Conference on Computer Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology change is a challenging problem for 4D reconstruction of dynamic\nscenes. In the classic volumetric fusion-based framework, a mesh is usually\nextracted from the TSDF volume as the canonical surface representation to help\nestimating deformation field. However, the surface and Embedded Deformation\nGraph (EDG) representations bring conflicts under topology changes since the\nsurface mesh has fixed-connectivity but the deformation field can be\ndiscontinuous. In this paper, the classic framework is re-designed to enable 4D\nreconstruction of dynamic scene under topology changes, by introducing a novel\nstructure of Non-manifold Volumetric Grid to the re-design of both TSDF and\nEDG, which allows connectivity updates by cell splitting and replication.\nExperiments show convincing reconstruction results for dynamic scenes of\ntopology changes, as compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:04:08 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Chao", ""], ["Guo", "Xiaohu", ""]]}, {"id": "2007.06855", "submitter": "Song Bian", "authors": "Song Bian, Xiaowei Xu, Weiwen Jiang, Yiyu Shi, Takashi Sato", "title": "BUNET: Blind Medical Image Segmentation Based on Secure UNET", "comments": "11 pages, 2 figures, in Proceedings of International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strict security requirements placed on medical records by various privacy\nregulations become major obstacles in the age of big data. To ensure efficient\nmachine learning as a service schemes while protecting data confidentiality, in\nthis work, we propose blind UNET (BUNET), a secure protocol that implements\nprivacy-preserving medical image segmentation based on the UNET architecture.\nIn BUNET, we efficiently utilize cryptographic primitives such as homomorphic\nencryption and garbled circuits (GC) to design a complete secure protocol for\nthe UNET neural architecture. In addition, we perform extensive architectural\nsearch in reducing the computational bottleneck of GC-based secure activation\nprotocols with high-dimensional input data. In the experiment, we thoroughly\nexamine the parameter space of our protocol, and show that we can achieve up to\n14x inference time reduction compared to the-state-of-the-art secure inference\ntechnique on a baseline architecture with negligible accuracy degradation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:05:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Bian", "Song", ""], ["Xu", "Xiaowei", ""], ["Jiang", "Weiwen", ""], ["Shi", "Yiyu", ""], ["Sato", "Takashi", ""]]}, {"id": "2007.06866", "submitter": "Yuchi Ishikawa", "authors": "Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, Hirokatsu Kataoka", "title": "Alleviating Over-segmentation Errors by Detecting Action Boundaries", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective framework for the temporal action segmentation task,\nnamely an Action Segment Refinement Framework (ASRF). Our model architecture\nconsists of a long-term feature extractor and two branches: the Action\nSegmentation Branch (ASB) and the Boundary Regression Branch (BRB). The\nlong-term feature extractor provides shared features for the two branches with\na wide temporal receptive field. The ASB classifies video frames with action\nclasses, while the BRB regresses the action boundary probabilities. The action\nboundaries predicted by the BRB refine the output from the ASB, which results\nin a significant performance improvement. Our contributions are three-fold: (i)\nWe propose a framework for temporal action segmentation, the ASRF, which\ndivides temporal action segmentation into frame-wise action classification and\naction boundary regression. Our framework refines frame-level hypotheses of\naction classes using predicted action boundaries. (ii) We propose a loss\nfunction for smoothing the transition of action probabilities, and analyze\ncombinations of various loss functions for temporal action segmentation. (iii)\nOur framework outperforms state-of-the-art methods on three challenging\ndatasets, offering an improvement of up to 13.7% in terms of segmental edit\ndistance and up to 16.1% in terms of segmental F1 score. Our code will be\npublicly available soon.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:20:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ishikawa", "Yuchi", ""], ["Kasai", "Seito", ""], ["Aoki", "Yoshimitsu", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "2007.06877", "submitter": "Jiuniu Wang", "authors": "Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan", "title": "Compare and Reweight: Distinctive Image Captioning Using Similar Images\n  Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": "Accepted at ECCV 2020 (oral)", "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of image captioning models has been developed, achieving\nsignificant improvement based on popular metrics, such as BLEU, CIDEr, and\nSPICE. However, although the generated captions can accurately describe the\nimage, they are generic for similar images and lack distinctiveness, i.e.,\ncannot properly describe the uniqueness of each image. In this paper, we aim to\nimprove the distinctiveness of image captions through training with sets of\nsimilar images. First, we propose a distinctiveness metric -- between-set CIDEr\n(CIDErBtw) to evaluate the distinctiveness of a caption with respect to those\nof similar images. Our metric shows that the human annotations of each image\nare not equivalent based on distinctiveness. Thus we propose several new\ntraining strategies to encourage the distinctiveness of the generated caption\nfor each image, which are based on using CIDErBtw in a weighted loss function\nor as a reinforcement learning reward. Finally, extensive experiments are\nconducted, showing that our proposed approach significantly improves both\ndistinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy\n(e.g., as measured by CIDEr) for a wide variety of image captioning baselines.\nThese results are further confirmed through a user study.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:40:39 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wang", "Jiuniu", ""], ["Xu", "Wenjia", ""], ["Wang", "Qingzhong", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2007.06887", "submitter": "Janghoon Choi", "authors": "Janghoon Choi, Junseok Kwon, Kyoung Mu Lee", "title": "Visual Tracking by TridentAlign and Context Embedding", "comments": "Code available on https://github.com/JanghoonChoi/TACT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Siamese network-based visual tracking methods have enabled\nhigh performance on numerous tracking benchmarks. However, extensive scale\nvariations of the target object and distractor objects with similar categories\nhave consistently posed challenges in visual tracking. To address these\npersisting issues, we propose novel TridentAlign and context embedding modules\nfor Siamese network-based visual tracking methods. The TridentAlign module\nfacilitates adaptability to extensive scale variations and large deformations\nof the target, where it pools the feature representation of the target object\ninto multiple spatial dimensions to form a feature pyramid, which is then\nutilized in the region proposal stage. Meanwhile, context embedding module aims\nto discriminate the target from distractor objects by accounting for the global\ncontext information among objects. The context embedding module extracts and\nembeds the global context information of a given frame into a local feature\nrepresentation such that the information can be utilized in the final\nclassification stage. Experimental results obtained on multiple benchmark\ndatasets show that the performance of the proposed tracker is comparable to\nthat of state-of-the-art trackers, while the proposed tracker runs at real-time\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:00:26 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Choi", "Janghoon", ""], ["Kwon", "Junseok", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2007.06888", "submitter": "Zeyu Hu", "authors": "Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu and Chiew-lan Tai", "title": "JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D\n  Point Clouds", "comments": "Accepted to ECCV 2020, supplementary materials included", "journal-ref": null, "doi": "10.1007/978-3-030-58565-5_14", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation and semantic edge detection can be seen as two dual\nproblems with close relationships in computer vision. Despite the fast\nevolution of learning-based 3D semantic segmentation methods, little attention\nhas been drawn to the learning of 3D semantic edge detectors, even less to a\njoint learning method for the two tasks. In this paper, we tackle the 3D\nsemantic edge detection task for the first time and present a new two-stream\nfully-convolutional network that jointly performs the two tasks. In particular,\nwe design a joint refinement module that explicitly wires region information\nand edge information to improve the performances of both tasks. Further, we\npropose a novel loss function that encourages the network to produce semantic\nsegmentation results with better boundaries. Extensive evaluations on S3DIS and\nScanNet datasets show that our method achieves on par or better performance\nthan the state-of-the-art methods for semantic segmentation and outperforms the\nbaseline methods for semantic edge detection. Code release:\nhttps://github.com/hzykent/JSENet\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:00:35 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Hu", "Zeyu", ""], ["Zhen", "Mingmin", ""], ["Bai", "Xuyang", ""], ["Fu", "Hongbo", ""], ["Tai", "Chiew-lan", ""]]}, {"id": "2007.06889", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li and Hakan Bilen", "title": "Knowledge Distillation for Multi-task Learning", "comments": "We propose a knowledge distillation method for addressing the\n  imbalance problem in multi-task learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) is to learn one single model that performs multiple\ntasks for achieving good performance on all tasks and lower cost on\ncomputation. Learning such a model requires to jointly optimize losses of a set\nof tasks with different difficulty levels, magnitudes, and characteristics\n(e.g. cross-entropy, Euclidean loss), leading to the imbalance problem in\nmulti-task learning. To address the imbalance problem, we propose a knowledge\ndistillation based method in this work. We first learn a task-specific model\nfor each task. We then learn the multi-task model for minimizing task-specific\nloss and for producing the same feature with task-specific models. As the\ntask-specific network encodes different features, we introduce small\ntask-specific adaptors to project multi-task features to the task-specific\nfeatures. In this way, the adaptors align the task-specific feature and the\nmulti-task feature, which enables a balanced parameter sharing across tasks.\nExtensive experimental results demonstrate that our method can optimize a\nmulti-task learning model in a more balanced way and achieve better overall\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:02:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 14:01:27 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Li", "Wei-Hong", ""], ["Bilen", "Hakan", ""]]}, {"id": "2007.06890", "submitter": "Weihong Ma", "authors": "Weihong Ma, Hesuo Zhang, Lianwen Jin, Sihang Wu, Jiapeng Wang, Yongpan\n  Wang", "title": "Joint Layout Analysis, Character Detection and Recognition for\n  Historical Document Digitization", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end trainable framework for restoring\nhistorical documents content that follows the correct reading order. In this\nframework, two branches named character branch and layout branch are added\nbehind the feature extraction network. The character branch localizes\nindividual characters in a document image and recognizes them simultaneously.\nThen we adopt a post-processing method to group them into text lines. The\nlayout branch based on fully convolutional network outputs a binary mask. We\nthen use Hough transform for line detection on the binary mask and combine\ncharacter results with the layout information to restore document content.\nThese two branches can be trained in parallel and are easy to train.\nFurthermore, we propose a re-score mechanism to minimize recognition error.\nExperiment results on the extended Chinese historical document MTHv2 dataset\ndemonstrate the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:02:52 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ma", "Weihong", ""], ["Zhang", "Hesuo", ""], ["Jin", "Lianwen", ""], ["Wu", "Sihang", ""], ["Wang", "Jiapeng", ""], ["Wang", "Yongpan", ""]]}, {"id": "2007.06891", "submitter": "Ren Komatsu", "authors": "Ren Komatsu, Hiromitsu Fujii, Yusuke Tamura, Atsushi Yamashita, Hajime\n  Asama", "title": "360$^\\circ$ Depth Estimation from Multiple Fisheye Images with Origami\n  Crown Representation of Icosahedron", "comments": "8 pages, Accepted to the 2020 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2020). For supplementary video, see\n  https://youtu.be/_vVD-zDMvyM", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9340981", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a method for all-around depth estimation from\nmultiple omnidirectional images for indoor environments. In particular, we\nfocus on plane-sweeping stereo as the method for depth estimation from the\nimages. We propose a new icosahedron-based representation and ConvNets for\nomnidirectional images, which we name \"CrownConv\" because the representation\nresembles a crown made of origami. CrownConv can be applied to both fisheye\nimages and equirectangular images to extract features. Furthermore, we propose\nicosahedron-based spherical sweeping for generating the cost volume on an\nicosahedron from the extracted features. The cost volume is regularized using\nthe three-dimensional CrownConv, and the final depth is obtained by depth\nregression from the cost volume. Our proposed method is robust to camera\nalignments by using the extrinsic camera parameters; therefore, it can achieve\nprecise depth estimation even when the camera alignment differs from that in\nthe training dataset. We evaluate the proposed model on synthetic datasets and\ndemonstrate its effectiveness. As our proposed method is computationally\nefficient, the depth is estimated from four fisheye images in less than a\nsecond using a laptop with a GPU. Therefore, it is suitable for real-world\nrobotics applications. Our source code is available at\nhttps://github.com/matsuren/crownconv360depth.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:02:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Komatsu", "Ren", ""], ["Fujii", "Hiromitsu", ""], ["Tamura", "Yusuke", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""]]}, {"id": "2007.06898", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Anjana Arunkumar, Chris Bryan and Chitta Baral", "title": "Our Evaluation Metric Needs an Update to Encourage Generalization", "comments": "Accepted to ICML UDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that surpass human performance on several popular benchmarks display\nsignificant degradation in performance on exposure to Out of Distribution (OOD)\ndata. Recent research has shown that models overfit to spurious biases and\n`hack' datasets, in lieu of learning generalizable features like humans. In\norder to stop the inflation in model performance -- and thus overestimation in\nAI systems' capabilities -- we propose a simple and novel evaluation metric,\nWOOD Score, that encourages generalization during evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:15:19 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mishra", "Swaroop", ""], ["Arunkumar", "Anjana", ""], ["Bryan", "Chris", ""], ["Baral", "Chitta", ""]]}, {"id": "2007.06918", "submitter": "Aswin Raghavan", "authors": "Aswin Raghavan, Jesse Hostetler, Indranil Sur, Abrar Rahman, Ajay\n  Divakaran", "title": "Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition,\n  and Selective Transfer", "comments": "Accepted at the 4th Lifelong Machine Learning Workshop at the\n  Thirty-seventh International Conference on Machine Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the eigentask framework for lifelong learning. An eigentask is a\npairing of a skill that solves a set of related tasks, paired with a generative\nmodel that can sample from the skill's input space. The framework extends\ngenerative replay approaches, which have mainly been used to avoid catastrophic\nforgetting, to also address other lifelong learning goals such as forward\nknowledge transfer. We propose a wake-sleep cycle of alternating task learning\nand knowledge consolidation for learning in our framework, and instantiate it\nfor lifelong supervised learning and lifelong RL. We achieve improved\nperformance over the state-of-the-art in supervised continual learning, and\nshow evidence of forward knowledge transfer in a lifelong RL application in the\ngame Starcraft2.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:06:13 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Raghavan", "Aswin", ""], ["Hostetler", "Jesse", ""], ["Sur", "Indranil", ""], ["Rahman", "Abrar", ""], ["Divakaran", "Ajay", ""]]}, {"id": "2007.06919", "submitter": "Bohan Zhuang", "authors": "Peng Chen, Jing Liu, Bohan Zhuang, Mingkui Tan, Chunhua Shen", "title": "AQD: Towards Accurate Quantized Object Detection", "comments": "CVPR2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network quantization allows inference to be conducted using low-precision\narithmetic for improved inference efficiency of deep neural networks on edge\ndevices. However, designing aggressively low-bit (e.g., 2-bit) quantization\nschemes on complex tasks, such as object detection, still remains challenging\nin terms of severe performance degradation and unverifiable efficiency on\ncommon hardware. In this paper, we propose an Accurate Quantized object\nDetection solution, termed AQD, to fully get rid of floating-point computation.\nTo this end, we target using fixed-point operations in all kinds of layers,\nincluding the convolutional layers, normalization layers, and skip connections,\nallowing the inference to be executed using integer-only arithmetic. To\ndemonstrate the improved latency-vs-accuracy trade-off, we apply the proposed\nmethods on RetinaNet and FCOS. In particular, experimental results on MS-COCO\ndataset show that our AQD achieves comparable or even better performance\ncompared with the full-precision counterpart under extremely low-bit schemes,\nwhich is of great practical value. Source code and models are available at:\nhttps://github.com/aim-uofa/model-quantization\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:07:29 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 01:44:32 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 05:51:36 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 05:22:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Peng", ""], ["Liu", "Jing", ""], ["Zhuang", "Bohan", ""], ["Tan", "Mingkui", ""], ["Shen", "Chunhua", ""]]}, {"id": "2007.06925", "submitter": "Yang Dongming", "authors": "Dongming Yang and Yuexian Zou", "title": "A Graph-based Interactive Reasoning for Human-Object Interaction\n  Detection", "comments": "Accepted by IJCAI 2020. SOLE copyright holder is IJCAI (international\n  Joint Conferences on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection devotes to learn how humans interact\nwith surrounding objects via inferring triplets of < human, verb, object >.\nHowever, recent HOI detection methods mostly rely on additional annotations\n(e.g., human pose) and neglect powerful interactive reasoning beyond\nconvolutions. In this paper, we present a novel graph-based interactive\nreasoning model called Interactive Graph (abbr. in-Graph) to infer HOIs, in\nwhich interactive semantics implied among visual targets are efficiently\nexploited. The proposed model consists of a project function that maps related\ntargets from convolution space to a graph-based semantic space, a message\npassing process propagating semantics among all nodes and an update function\ntransforming the reasoned nodes back to convolution space. Furthermore, we\nconstruct a new framework to assemble in-Graph models for detecting HOIs,\nnamely in-GraphNet. Beyond inferring HOIs using instance features respectively,\nthe framework dynamically parses pairwise interactive semantics among visual\ntargets by integrating two-level in-Graphs, i.e., scene-wide and instance-wide\nin-Graphs. Our framework is end-to-end trainable and free from costly\nannotations like human pose. Extensive experiments show that our proposed\nframework outperforms existing HOI detection methods on both V-COCO and\nHICO-DET benchmarks and improves the baseline about 9.4% and 15% relatively,\nvalidating its efficacy in detecting HOIs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:29:03 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yang", "Dongming", ""], ["Zou", "Yuexian", ""]]}, {"id": "2007.06929", "submitter": "HongYu Liu", "authors": "Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao Yang", "title": "Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature\n  Equalizations", "comments": "Accepted by ECCV2020(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep encoder-decoder based CNNs have advanced image inpainting methods for\nhole filling. While existing methods recover structures and textures\nstep-by-step in the hole regions, they typically use two encoder-decoders for\nseparate recovery. The CNN features of each encoder are learned to capture\neither missing structures or textures without considering them as a whole. The\ninsufficient utilization of these encoder features limit the performance of\nrecovering both structures and textures. In this paper, we propose a mutual\nencoder-decoder CNN for joint recovery of both. We use CNN features from the\ndeep and shallow layers of the encoder to represent structures and textures of\nan input image, respectively. The deep layer features are sent to a structure\nbranch and the shallow layer features are sent to a texture branch. In each\nbranch, we fill holes in multiple scales of the CNN features. The filled CNN\nfeatures from both branches are concatenated and then equalized. During feature\nequalization, we reweigh channel attentions first and propose a bilateral\npropagation activation function to enable spatial equalization. To this end,\nthe filled CNN features of structure and texture mutually benefit each other to\nrepresent image content at all feature levels. We use the equalized feature to\nsupplement decoder features for output image generation through skip\nconnections. Experiments on the benchmark datasets show the proposed method is\neffective to recover structures and textures and performs favorably against\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:39:50 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Hongyu", ""], ["Jiang", "Bin", ""], ["Song", "Yibing", ""], ["Huang", "Wei", ""], ["Yang", "Chao", ""]]}, {"id": "2007.06932", "submitter": "Mincheol Park", "authors": "Mincheol Park, Woojeong Kim, Suhyun Kim", "title": "REPrune: Filter Pruning via Representative Election", "comments": "Under Review at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though norm-based filter pruning methods are widely accepted, it is\nquestionable whether the \"smaller-norm-less-important\" criterion is optimal in\ndetermining filters to prune. Especially when we can keep only a small fraction\nof the original filters, it is more crucial to choose the filters that can best\nrepresent the whole filters regardless of norm values. Our novel pruning method\nentitled \"REPrune\" addresses this problem by selecting representative filters\nvia clustering. By selecting one filter from a cluster of similar filters and\navoiding selecting adjacent large filters, REPrune can achieve a better\ncompression rate with similar accuracy. Our method also recovers the accuracy\nmore rapidly and requires a smaller shift of filters during fine-tuning.\nEmpirically, REPrune reduces more than 49% FLOPs, with 0.53% accuracy gain on\nResNet-110 for CIFAR-10. Also, REPrune reduces more than 41.8% FLOPs with 1.67%\nTop-1 validation loss on ResNet-18 for ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:41:16 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 10:25:17 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 08:07:33 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Park", "Mincheol", ""], ["Kim", "Woojeong", ""], ["Kim", "Suhyun", ""]]}, {"id": "2007.06936", "submitter": "Marvin Klingner", "authors": "Marvin Klingner, Jan-Aike Term\\\"ohlen, Jonas Mikolajczyk, Tim\n  Fingscheidt", "title": "Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object\n  Problem by Semantic Guidance", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised monocular depth estimation presents a powerful method to\nobtain 3D scene information from single camera images, which is trainable on\narbitrary image sequences without requiring depth labels, e.g., from a LiDAR\nsensor. In this work we present a new self-supervised semantically-guided depth\nestimation (SGDepth) method to deal with moving dynamic-class (DC) objects,\nsuch as moving cars and pedestrians, which violate the static-world assumptions\ntypically made during training of such models. Specifically, we propose (i)\nmutually beneficial cross-domain training of (supervised) semantic segmentation\nand self-supervised depth estimation with task-specific network heads, (ii) a\nsemantic masking scheme providing guidance to prevent moving DC objects from\ncontaminating the photometric loss, and (iii) a detection method for frames\nwith non-moving DC objects, from which the depth of DC objects can be learned.\nWe demonstrate the performance of our method on several benchmarks, in\nparticular on the Eigen split, where we exceed all baselines without test-time\nrefinement.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:47:27 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 11:00:22 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Klingner", "Marvin", ""], ["Term\u00f6hlen", "Jan-Aike", ""], ["Mikolajczyk", "Jonas", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2007.06959", "submitter": "Fatemeh Haghighi", "authors": "Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Zongwei Zhou,\n  Michael B. Gotway, Jianming Liang", "title": "Learning Semantics-enriched Representation via Self-discovery,\n  Self-classification, and Self-restoration", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images are naturally associated with rich semantics about the human\nanatomy, reflected in an abundance of recurring anatomical patterns, offering\nunique potential to foster deep semantic representation learning and yield\nsemantically more powerful models for different medical applications. But how\nexactly such strong yet free semantics embedded in medical images can be\nharnessed for self-supervised learning remains largely unexplored. To this end,\nwe train deep models to learn semantically enriched visual representation by\nself-discovery, self-classification, and self-restoration of the anatomy\nunderneath medical images, resulting in a semantics-enriched, general-purpose,\npre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis\nwith all the publicly-available pre-trained models, by either self-supervision\nor fully supervision, on the six distinct target tasks, covering both\nclassification and segmentation in various medical modalities (i.e.,CT, MRI,\nand X-ray). Our extensive experiments demonstrate that Semantic Genesis\nsignificantly exceeds all of its 3D counterparts as well as the de facto\nImageNet-based transfer learning in 2D. This performance is attributed to our\nnovel self-supervised learning framework, encouraging deep models to learn\ncompelling semantic representation from abundant anatomical patterns resulting\nfrom consistent anatomies embedded in medical images. Code and pre-trained\nSemantic Genesis are available at https://github.com/JLiangLab/SemanticGenesis .\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:36:10 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Haghighi", "Fatemeh", ""], ["Taher", "Mohammad Reza Hosseinzadeh", ""], ["Zhou", "Zongwei", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "2007.06963", "submitter": "Zhiwei Zhang", "authors": "Zhiwei Zhang, Shifeng Chen and Lei Sun", "title": "P-KDGAN: Progressive Knowledge Distillation with GANs for One-class\n  Novelty Detection", "comments": "IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class novelty detection is to identify anomalous instances that do not\nconform to the expected normal instances. In this paper, the Generative\nAdversarial Networks (GANs) based on encoder-decoder-encoder pipeline are used\nfor detection and achieve state-of-the-art performance. However, deep neural\nnetworks are too over-parameterized to deploy on resource-limited devices.\nTherefore, Progressive Knowledge Distillation with GANs (PKDGAN) is proposed to\nlearn compact and fast novelty detection networks. The P-KDGAN is a novel\nattempt to connect two standard GANs by the designed distillation loss for\ntransferring knowledge from the teacher to the student. The progressive\nlearning of knowledge distillation is a two-step approach that continuously\nimproves the performance of the student GAN and achieves better performance\nthan single step methods. In the first step, the student GAN learns the basic\nknowledge totally from the teacher via guiding of the pretrained teacher GAN\nwith fixed weights. In the second step, joint fine-training is adopted for the\nknowledgeable teacher and student GANs to further improve the performance and\nstability. The experimental results on CIFAR-10, MNIST, and FMNIST show that\nour method improves the performance of the student GAN by 2.44%, 1.77%, and\n1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and\n700:1, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:44:57 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 17:25:43 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Zhiwei", ""], ["Chen", "Shifeng", ""], ["Sun", "Lei", ""]]}, {"id": "2007.06965", "submitter": "Zhiding Yu", "authors": "Wuyang Chen, Zhiding Yu, Zhangyang Wang, Anima Anandkumar", "title": "Automated Synthetic-to-Real Generalization", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models trained on synthetic images often face degraded generalization to real\ndata. As a convention, these models are often initialized with ImageNet\npre-trained representation. Yet the role of ImageNet knowledge is seldom\ndiscussed despite common practices that leverage this knowledge to maintain the\ngeneralization ability. An example is the careful hand-tuning of early stopping\nand layer-wise learning rates, which is shown to improve synthetic-to-real\ngeneralization but is also laborious and heuristic. In this work, we explicitly\nencourage the synthetically trained model to maintain similar representations\nwith the ImageNet pre-trained model, and propose a \\textit{learning-to-optimize\n(L2O)} strategy to automate the selection of layer-wise learning rates. We\ndemonstrate that the proposed framework can significantly improve the\nsynthetic-to-real generalization performance without seeing and training on\nreal data, while also benefiting downstream tasks such as domain adaptation.\nCode is available at: https://github.com/NVlabs/ASG.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:57:34 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chen", "Wuyang", ""], ["Yu", "Zhiding", ""], ["Wang", "Zhangyang", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.06995", "submitter": "Aruni RoyChowdhury", "authors": "Aruni RoyChowdhury, Xiang Yu, Kihyuk Sohn, Erik Learned-Miller,\n  Manmohan Chandraker", "title": "Improving Face Recognition by Clustering Unlabeled Faces in the Wild", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep face recognition has benefited significantly from large-scale\nlabeled data, current research is focused on leveraging unlabeled data to\nfurther boost performance, reducing the cost of human annotation. Prior work\nhas mostly been in controlled settings, where the labeled and unlabeled data\nsets have no overlapping identities by construction. This is not realistic in\nlarge-scale face recognition, where one must contend with such overlaps, the\nfrequency of which increases with the volume of data. Ignoring identity overlap\nleads to significant labeling noise, as data from the same identity is split\ninto multiple clusters. To address this, we propose a novel identity separation\nmethod based on extreme value theory. It is formulated as an\nout-of-distribution detection algorithm, and greatly reduces the problems\ncaused by overlapping-identity label noise. Considering cluster assignments as\npseudo-labels, we must also overcome the labeling noise from clustering errors.\nWe propose a modulation of the cosine loss, where the modulation weights\ncorrespond to an estimate of clustering uncertainty. Extensive experiments on\nboth controlled and real settings demonstrate our method's consistent\nimprovements over supervised baselines, e.g., 11.6% improvement on IJB-A\nverification.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 12:26:50 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 17:30:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["RoyChowdhury", "Aruni", ""], ["Yu", "Xiang", ""], ["Sohn", "Kihyuk", ""], ["Learned-Miller", "Erik", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.07012", "submitter": "Issam Hadj Laradji", "authors": "Issam Laradji, Pau Rodriguez, Frederic Branchaud-Charron, Keegan\n  Lensink, Parmida Atighehchian, William Parker, David Vazquez, and Derek\n  Nowrouzezahrai", "title": "A Weakly Supervised Region-Based Active Learning Method for COVID-19\n  Segmentation in CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in the battle against the Coronavirus (COVID-19)\npandemic is to detect and quantify the severity of the disease in a timely\nmanner. Computed tomographies (CT) of the lungs are effective for assessing the\nstate of the infection. Unfortunately, labeling CT scans can take a lot of time\nand effort, with up to 150 minutes per scan. We address this challenge\nintroducing a scalable, fast, and accurate active learning system that\naccelerates the labeling of CT scan images. Conventionally, active learning\nmethods require the labelers to annotate whole images with full supervision,\nbut that can lead to wasted efforts as many of the annotations could be\nredundant. Thus, our system presents the annotator with unlabeled regions that\npromise high information content and low annotation cost. Further, the system\nallows annotators to label regions using point-level supervision, which is much\ncheaper to acquire than per-pixel annotations. Our experiments on open-source\nCOVID-19 datasets show that using an entropy-based method to rank unlabeled\nregions yields to significantly better results than random labeling of these\nregions. Also, we show that labeling small regions of images is more efficient\nthan labeling whole images. Finally, we show that with only 7\\% of the labeling\neffort required to label the whole training set gives us around 90\\% of the\nperformance obtained by training the model on the fully annotated training set.\nCode is available at:\n\\url{https://github.com/IssamLaradji/covid19_active_learning}.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:38:04 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Laradji", "Issam", ""], ["Rodriguez", "Pau", ""], ["Branchaud-Charron", "Frederic", ""], ["Lensink", "Keegan", ""], ["Atighehchian", "Parmida", ""], ["Parker", "William", ""], ["Vazquez", "David", ""], ["Nowrouzezahrai", "Derek", ""]]}, {"id": "2007.07013", "submitter": "Mihai P\\^irvu", "authors": "Mihai Cristian P\\^irvu", "title": "Pose2RGBD. Generating Depth and RGB images from absolute positions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a method at the intersection of Computer Vision and Computer\nGraphics fields, which automatically generates RGBD images using neural\nnetworks, based on previously seen and synchronized video, depth and pose\nsignals. Since the models must be able to reconstruct both texture (RGB) and\nstructure (Depth), it creates an implicit representation of the scene, as\nopposed to explicit ones, such as meshes or point clouds. The process can be\nthought of as neural rendering, where we obtain a function f : Pose -> RGBD,\nwhich we can use to navigate through the generated scene, similarly to graphics\nsimulations. We introduce two new datasets, one based on synthetic data with\nfull ground truth information, while the other one being recorded from a drone\nflight in an university campus, using only video and GPS signals. Finally, we\npropose a fully unsupervised method of generating datasets from videos alone,\nin order to train the Pose2RGBD networks. Code and datasets are available at::\nhttps://gitlab.com/mihaicristianpirvu/pose2rgbd.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:07:06 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["P\u00eervu", "Mihai Cristian", ""]]}, {"id": "2007.07018", "submitter": "Luo Xiong", "authors": "Luo Xiong, Yanjie Liang, Yan Yan, Hanzi Wang", "title": "Correlation filter tracking with adaptive proposal selection for\n  accurate scale estimation", "comments": "6 pages, 14 figures", "journal-ref": null, "doi": "10.1109/ICME.2019.00312", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, some correlation filter based trackers with detection proposals\nhave achieved state-of-the-art tracking results. However, a large number of\nredundant proposals given by the proposal generator may degrade the performance\nand speed of these trackers. In this paper, we propose an adaptive proposal\nselection algorithm which can generate a small number of high-quality proposals\nto handle the problem of scale variations for visual object tracking.\nSpecifically, we firstly utilize the color histograms in the HSV color space to\nrepresent the instances (i.e., the initial target in the first frame and the\npredicted target in the previous frame) and proposals. Then, an adaptive\nstrategy based on the color similarity is formulated to select high-quality\nproposals. We further integrate the proposed adaptive proposal selection\nalgorithm with coarse-to-fine deep features to validate the generalization and\nefficiency of the proposed tracker. Experiments on two benchmark datasets\ndemonstrate that the proposed algorithm performs favorably against several\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:16:52 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Xiong", "Luo", ""], ["Liang", "Yanjie", ""], ["Yan", "Yan", ""], ["Wang", "Hanzi", ""]]}, {"id": "2007.07020", "submitter": "Wenguan Wang", "authors": "Xiankai Lu, Wenguan Wang, Martin Danelljan, Tianfei Zhou, Jianbing\n  Shen and Luc Van Gool", "title": "Video Object Segmentation with Episodic Graph Memory Networks", "comments": "ECCV2020 Spotlight Oral; website:\n  https://github.com/carrierlxk/GraphMemVOS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to make a segmentation model efficiently adapt to a specific video and to\nonline target appearance variations are fundamentally crucial issues in the\nfield of video object segmentation. In this work, a graph memory network is\ndeveloped to address the novel idea of \"learning to update the segmentation\nmodel\". Specifically, we exploit an episodic memory network, organized as a\nfully connected graph, to store frames as nodes and capture cross-frame\ncorrelations by edges. Further, learnable controllers are embedded to ease\nmemory reading and writing, as well as maintain a fixed memory scale. The\nstructured, external memory design enables our model to comprehensively mine\nand quickly store new knowledge, even with limited visual information, and the\ndifferentiable memory controllers slowly learn an abstract method for storing\nuseful representations in the memory and how to later use these representations\nfor prediction, via gradient descent. In addition, the proposed graph memory\nnetwork yields a neat yet principled framework, which can generalize well both\none-shot and zero-shot video object segmentation tasks. Extensive experiments\non four challenging benchmark datasets verify that our graph memory network is\nable to facilitate the adaptation of the segmentation network for case-by-case\nvideo object segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:19:19 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 11:54:37 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 11:01:09 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 09:58:23 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Lu", "Xiankai", ""], ["Wang", "Wenguan", ""], ["Danelljan", "Martin", ""], ["Zhou", "Tianfei", ""], ["Shen", "Jianbing", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.07051", "submitter": "Chongyi Li", "authors": "Chongyi Li and Runmin Cong and Yongri Piao and Qianqian Xu and Chen\n  Change Loy", "title": "RGB-D Salient Object Detection with Cross-Modality Modulation and\n  Selection", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective method to progressively integrate and refine the\ncross-modality complementarities for RGB-D salient object detection (SOD). The\nproposed network mainly solves two challenging issues: 1) how to effectively\nintegrate the complementary information from RGB image and its corresponding\ndepth map, and 2) how to adaptively select more saliency-related features.\nFirst, we propose a cross-modality feature modulation (cmFM) module to enhance\nfeature representations by taking the depth features as prior, which models the\ncomplementary relations of RGB-D data. Second, we propose an adaptive feature\nselection (AFS) module to select saliency-related features and suppress the\ninferior ones. The AFS module exploits multi-modality spatial feature fusion\nwith the self-modality and cross-modality interdependencies of channel features\nare considered. Third, we employ a saliency-guided position-edge attention\n(sg-PEA) module to encourage our network to focus more on saliency-related\nregions. The above modules as a whole, called cmMS block, facilitates the\nrefinement of saliency features in a coarse-to-fine fashion. Coupled with a\nbottom-up inference, the refined saliency features enable accurate and\nedge-preserving SOD. Extensive experiments demonstrate that our network\noutperforms state-of-the-art saliency detectors on six popular RGB-D SOD\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:22:50 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Chongyi", ""], ["Cong", "Runmin", ""], ["Piao", "Yongri", ""], ["Xu", "Qianqian", ""], ["Loy", "Chen Change", ""]]}, {"id": "2007.07053", "submitter": "Qiang Nie", "authors": "Qiang Nie, Ziwei Liu, Yunhui Liu", "title": "Unsupervised Human 3D Pose Representation with Viewpoint and Pose\n  Disentanglement", "comments": "To appear in ECCV 2020. Code and models are available at:\n  https://github.com/NIEQiang001/unsupervised-human-pose.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good 3D human pose representation is important for human pose\nrelated tasks, e.g. human 3D pose estimation and action recognition. Within all\nthese problems, preserving the intrinsic pose information and adapting to view\nvariations are two critical issues. In this work, we propose a novel Siamese\ndenoising autoencoder to learn a 3D pose representation by disentangling the\npose-dependent and view-dependent feature from the human skeleton data, in a\nfully unsupervised manner. These two disentangled features are utilized\ntogether as the representation of the 3D pose. To consider both the kinematic\nand geometric dependencies, a sequential bidirectional recursive network\n(SeBiReNet) is further proposed to model the human skeleton data. Extensive\nexperiments demonstrate that the learned representation 1) preserves the\nintrinsic information of human pose, 2) shows good transferability across\ndatasets and tasks. Notably, our approach achieves state-of-the-art performance\non two inherently different tasks: pose denoising and unsupervised action\nrecognition. Code and models are available at:\n\\url{https://github.com/NIEQiang001/unsupervised-human-pose.git}\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:25:22 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Nie", "Qiang", ""], ["Liu", "Ziwei", ""], ["Liu", "Yunhui", ""]]}, {"id": "2007.07066", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Shijian Lu, Changgong Zhang, Feiying Ma and Xuansong\n  Xie", "title": "Towards Realistic 3D Embedding via View Alignment", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in generative adversarial networks (GANs) have achieved great\nsuccess in automated image composition that generates new images by embedding\ninterested foreground objects into background images automatically. On the\nother hand, most existing works deal with foreground objects in two-dimensional\n(2D) images though foreground objects in three-dimensional (3D) models are more\nflexible with 360-degree view freedom. This paper presents an innovative View\nAlignment GAN (VA-GAN) that composes new images by embedding 3D models into 2D\nbackground images realistically and automatically. VA-GAN consists of a texture\ngenerator and a differential discriminator that are inter-connected and\nend-to-end trainable. The differential discriminator guides to learn geometric\ntransformation from background images so that the composed 3D models can be\naligned with the background images with realistic poses and views. The texture\ngenerator adopts a novel view encoding mechanism for generating accurate object\ntextures for the 3D models under the estimated views. Extensive experiments\nover two synthesis tasks (car synthesis with KITTI and pedestrian synthesis\nwith Cityscapes) show that VA-GAN achieves high-fidelity composition\nqualitatively and quantitatively as compared with state-of-the-art generation\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:45:00 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhan", "Fangneng", ""], ["Lu", "Shijian", ""], ["Zhang", "Changgong", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""]]}, {"id": "2007.07075", "submitter": "Shuvozit Ghose", "authors": "Amandeep Kumar, Shuvozit Ghose, Pinaki Nath Chowdhury, Partha Pratim\n  Roy, Umapada Pal", "title": "UDBNET: Unsupervised Document Binarization Network via Adversarial Game", "comments": "Accepted in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degraded document image binarization is one of the most challenging tasks in\nthe domain of document image analysis. In this paper, we present a novel\napproach towards document image binarization by introducing three-player\nmin-max adversarial game. We train the network in an unsupervised setup by\nassuming that we do not have any paired-training data. In our approach, an\nAdversarial Texture Augmentation Network (ATANet) first superimposes the\ntexture of a degraded reference image over a clean image. Later, the clean\nimage along with its generated degraded version constitute the pseudo\npaired-data which is used to train the Unsupervised Document Binarization\nNetwork (UDBNet). Following this approach, we have enlarged the document\nbinarization datasets as it generates multiple images having same content\nfeature but different textual feature. These generated noisy images are then\nfed into the UDBNet to get back the clean version. The joint discriminator\nwhich is the third-player of our three-player min-max adversarial game tries to\ncouple both the ATANet and UDBNet. The three-player min-max adversarial game\nstops, when the distributions modelled by the ATANet and the UDBNet align to\nthe same joint distribution over time. Thus, the joint discriminator enforces\nthe UDBNet to perform better on real degraded image. The experimental results\nindicate the superior performance of the proposed model over existing\nstate-of-the-art algorithm on widely used DIBCO datasets. The source code of\nthe proposed system is publicly available at\nhttps://github.com/VIROBO-15/UDBNET.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:58:46 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 09:58:28 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kumar", "Amandeep", ""], ["Ghose", "Shuvozit", ""], ["Chowdhury", "Pinaki Nath", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "2007.07077", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Le Thanh Nguyen-Meidine, Atif Belal, Madhu Kiran, Jose Dolz,\n  Louis-Antoine Blais-Morin, Eric Granger", "title": "Unsupervised Multi-Target Domain Adaptation Through Knowledge\n  Distillation", "comments": "Accepted for WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) seeks to alleviate the problem of domain\nshift between the distribution of unlabeled data from the target domain w.r.t.\nlabeled data from the source domain. While the single-target UDA scenario is\nwell studied in the literature, Multi-Target Domain Adaptation (MTDA) remains\nlargely unexplored despite its practical importance, e.g., in multi-camera\nvideo-surveillance applications. The MTDA problem can be addressed by adapting\none specialized model per target domain, although this solution is too costly\nin many real-world applications. Blending multiple targets for MTDA has been\nproposed, yet this solution may lead to a reduction in model specificity and\naccuracy. In this paper, we propose a novel unsupervised MTDA approach to train\na CNN that can generalize well across multiple target domains. Our\nMulti-Teacher MTDA (MT-MTDA) method relies on multi-teacher knowledge\ndistillation (KD) to iteratively distill target domain knowledge from multiple\nteachers to a common student. The KD process is performed in a progressive\nmanner, where the student is trained by each teacher on how to perform UDA for\na specific target, instead of directly learning domain adapted features.\nFinally, instead of combining the knowledge from each teacher, MT-MTDA\nalternates between teachers that distill knowledge, thereby preserving the\nspecificity of each target (teacher) when learning to adapt to the student.\nMT-MTDA is compared against state-of-the-art methods on several challenging UDA\nbenchmarks, and empirical results show that our proposed model can provide a\nconsiderably higher level of accuracy across multiple target domains. Our code\nis available at: https://github.com/LIVIAETS/MT-MTDA\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:59:45 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 16:16:44 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 15:32:16 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 20:07:22 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Nguyen-Meidine", "Le Thanh", ""], ["Belal", "Atif", ""], ["Kiran", "Madhu", ""], ["Dolz", "Jose", ""], ["Blais-Morin", "Louis-Antoine", ""], ["Granger", "Eric", ""]]}, {"id": "2007.07081", "submitter": "Ilia Kravets", "authors": "Ilia Kravets, Tal Heletz, Hayit Greenspan", "title": "Nodule2vec: a 3D Deep Learning System for Pulmonary Nodule Retrieval\n  Using Semantic Representation", "comments": "to appear at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based retrieval supports a radiologist decision making process by\npresenting the doctor the most similar cases from the database containing both\nhistorical diagnosis and further disease development history. We present a deep\nlearning system that transforms a 3D image of a pulmonary nodule from a CT scan\ninto a low-dimensional embedding vector. We demonstrate that such a vector\nrepresentation preserves semantic information about the nodule and offers a\nviable approach for content-based image retrieval (CBIR). We discuss the\ntheoretical limitations of the available datasets and overcome them by applying\ntransfer learning of the state-of-the-art lung nodule detection model. We\nevaluate the system using the LIDC-IDRI dataset of thoracic CT scans. We devise\na similarity score and show that it can be utilized to measure similarity 1)\nbetween annotations of the same nodule by different radiologists and 2) between\nthe query nodule and the top four CBIR results. A comparison between doctors\nand algorithm scores suggests that the benefit provided by the system to the\nradiologist end-user is comparable to obtaining a second radiologist's opinion.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 16:26:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Kravets", "Ilia", ""], ["Heletz", "Tal", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2007.07097", "submitter": "Yupeng Cheng", "authors": "Yupeng Cheng, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Shang-Wei Lin,\n  Weisi Lin, Wei Feng, Yang Liu", "title": "Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved high accuracy on various tasks and\nare even robust to natural noise that widely exists in captured images due to\nlow quality imaging sensors, etc. However, the high performance DNNs also raise\ninevitable security problems, e.g., automatically recognizing a high-profile\nperson's face and switching with a maliciously generated fake one to influence\nthe outcomes of various critical events. This fact posts an important and\npractical problem, i.e., how to generate visually clean images while letting\nthem have the capability of misleading the state-of-the-art DNNs to avoid\npotential security issues. In this paper, we initiate the very first attempt to\naddress this very new problem from the perspective of adversarial attack and\npropose the adversarial denoise attack aiming to simultaneously denoise input\nimages while fooling DNNs. More specifically, our main contributions are\nthree-fold: First, we identify a totally new task that stealthily embeds\nattacks inside image denoising module widely deployed in multimedia devices as\nan image post-processing operation to simultaneously enhance the visual image\nquality and fool DNNs. Second, we formulate this new task as a kernel\nprediction problem for image filtering and propose the adversarial-denoising\nkernel prediction that can produce adversarial-noiseless kernels for effective\ndenoising and adversarial attacking simultaneously. Third, we implement an\nadaptive perceptual region localization to identify semantic-related\nvulnerability regions with which the attack can be more effective while not\ndoing too much harm to the denoising. We validate our method on the NeurIPS'17\nadversarial competition dataset. The comprehensive evaluation and analysis\ndemonstrate that our method not only realizes denoising but also achieves\nhigher success rate and transferability over the state-of-the-art attacks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:18:08 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 13:08:54 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Cheng", "Yupeng", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Xie", "Xiaofei", ""], ["Lin", "Shang-Wei", ""], ["Lin", "Weisi", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""]]}, {"id": "2007.07099", "submitter": "Di Ma", "authors": "Di Ma, Fan Zhang, and David R. Bull", "title": "MFRNet: A New CNN Architecture for Post-Processing and In-loop Filtering", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2020.3043064", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel convolutional neural network (CNN)\narchitecture, MFRNet, for post-processing (PP) and in-loop filtering (ILF) in\nthe context of video compression. This network consists of four Multi-level\nFeature review Residual dense Blocks (MFRBs), which are connected using a\ncascading structure. Each MFRB extracts features from multiple convolutional\nlayers using dense connections and a multi-level residual learning structure.\nIn order to further improve information flow between these blocks, each of them\nalso reuses high dimensional features from the previous MFRB. This network has\nbeen integrated into PP and ILF coding modules for both HEVC (HM 16.20) and VVC\n(VTM 7.0), and fully evaluated under the JVET Common Test Conditions using the\nRandom Access configuration. The experimental results show significant and\nconsistent coding gains over both anchor codecs (HEVC HM and VVC VTM) and also\nover other existing CNN-based PP/ILF approaches based on Bjontegaard Delta\nmeasurements using both PSNR and VMAF for quality assessment. When MFRNet is\nintegrated into HM 16.20, gains up to 16.0% (BD-rate VMAF) are demonstrated for\nILF, and up to 21.0% (BD-rate VMAF) for PP. The respective gains for VTM 7.0\nare up to 5.1% for ILF and up to 7.1% for PP.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:19:32 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 21:59:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2007.07101", "submitter": "Vincent Christlein", "authors": "Simon Jordan, Mathias Seuret, Pavel Kr\\'al, Ladislav Lenc, Ji\\v{r}\\'i\n  Mart\\'inek, Barbara Wiermann, Tobias Schwinger, Andreas Maier, Vincent\n  Christlein", "title": "Re-ranking for Writer Identification and Writer Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic writer identification is a common problem in document analysis.\nState-of-the-art methods typically focus on the feature extraction step with\ntraditional or deep-learning-based techniques. In retrieval problems,\nre-ranking is a commonly used technique to improve the results. Re-ranking\nrefines an initial ranking result by using the knowledge contained in the\nranked result, e. g., by exploiting nearest neighbor relations. To the best of\nour knowledge, re-ranking has not been used for writer\nidentification/retrieval. A possible reason might be that publicly available\nbenchmark datasets contain only few samples per writer which makes a re-ranking\nless promising. We show that a re-ranking step based on k-reciprocal nearest\nneighbor relationships is advantageous for writer identification, even if only\na few samples per writer are available. We use these reciprocal relationships\nin two ways: encode them into new vectors, as originally proposed, or integrate\nthem in terms of query-expansion. We show that both techniques outperform the\nbaseline results in terms of mAP on three writer identification datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:21:17 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Jordan", "Simon", ""], ["Seuret", "Mathias", ""], ["Kr\u00e1l", "Pavel", ""], ["Lenc", "Ladislav", ""], ["Mart\u00ednek", "Ji\u0159\u00ed", ""], ["Wiermann", "Barbara", ""], ["Schwinger", "Tobias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2007.07129", "submitter": "Jannis Walk", "authors": "Alexander Treiss, Jannis Walk, Niklas K\\\"uhl", "title": "An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear\n  Analysis", "comments": "Alexander Treiss and Jannis Walk contributed equally in shared first\n  authorship. To be published at ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks have shown to achieve superior performance on\nimage segmentation tasks. However, convolutional neural networks, operating as\nblack-box systems, generally do not provide a reliable measure about the\nconfidence of their decisions. This leads to various problems in industrial\nsettings, amongst others, inadequate levels of trust from users in the model's\noutputs as well as a non-compliance with current policy guidelines (e.g., EU AI\nStrategy). To address these issues, we use uncertainty measures based on\nMonte-Carlo dropout in the context of a human-in-the-loop system to increase\nthe system's transparency and performance. In particular, we demonstrate the\nbenefits described above on a real-world multi-class image segmentation task of\nwear analysis in the machining industry. Following previous work, we show that\nthe quality of a prediction correlates with the model's uncertainty.\nAdditionally, we demonstrate that a multiple linear regression using the\nmodel's uncertainties as independent variables significantly explains the\nquality of a prediction (\\(R^2=0.718\\)). Within the uncertainty-based\nhuman-in-the-loop system, the multiple regression aims at identifying failed\npredictions on an image-level. The system utilizes a human expert to label\nthese failed predictions manually. A simulation study demonstrates that the\nuncertainty-based human-in-the-loop system increases performance for different\nlevels of human involvement in comparison to a random-based human-in-the-loop\nsystem. To ensure generalizability, we show that the presented approach\nachieves similar results on the publicly available Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:47:37 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 09:46:42 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Treiss", "Alexander", ""], ["Walk", "Jannis", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2007.07171", "submitter": "David Fuentes-Jimenez", "authors": "David Fuentes-Jimenez and Cristina Losada-Gutierrez and David\n  Casillas-Perez and Javier Macias-Guarasa and Roberto Martin-Lopez and Daniel\n  Pizarro and Carlos A.Luna", "title": "Towards Dense People Detection with Deep Learning and Depth images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a DNN-based system that detects multiple people from a\nsingle depth image. Our neural network processes a depth image and outputs a\nlikelihood map in image coordinates, where each detection corresponds to a\nGaussian-shaped local distribution, centered at the person's head. The\nlikelihood map encodes both the number of detected people and their 2D image\npositions, and can be used to recover the 3D position of each person using the\ndepth image and the camera calibration parameters. Our architecture is compact,\nusing separated convolutions to increase performance, and runs in real-time\nwith low budget GPUs. We use simulated data for initially training the network,\nfollowed by fine tuning with a relatively small amount of real data. We show\nthis strategy to be effective, producing networks that generalize to work with\nscenes different from those used during training. We thoroughly compare our\nmethod against the existing state-of-the-art, including both classical and\nDNN-based solutions. Our method outperforms existing methods and can accurately\ndetect people in scenes with significant occlusions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:43:02 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fuentes-Jimenez", "David", ""], ["Losada-Gutierrez", "Cristina", ""], ["Casillas-Perez", "David", ""], ["Macias-Guarasa", "Javier", ""], ["Martin-Lopez", "Roberto", ""], ["Pizarro", "Daniel", ""], ["Luna", "Carlos A.", ""]]}, {"id": "2007.07173", "submitter": "Shanxin Yuan", "authors": "Lin Liu, Jianzhuang Liu, Shanxin Yuan, Gregory Slabaugh, Ales\n  Leonardis, Wengang Zhou, Qi Tian", "title": "Wavelet-Based Dual-Branch Network for Image Demoireing", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When smartphone cameras are used to take photos of digital screens, usually\nmoire patterns result, severely degrading photo quality. In this paper, we\ndesign a wavelet-based dual-branch network (WDNet) with a spatial attention\nmechanism for image demoireing. Existing image restoration methods working in\nthe RGB domain have difficulty in distinguishing moire patterns from true scene\ntexture. Unlike these methods, our network removes moire patterns in the\nwavelet domain to separate the frequencies of moire patterns from the image\ncontent. The network combines dense convolution modules and dilated convolution\nmodules supporting large receptive fields. Extensive experiments demonstrate\nthe effectiveness of our method, and we further show that WDNet generalizes to\nremoving moire artifacts on non-screen images. Although designed for image\ndemoireing, WDNet has been applied to two other low-levelvision tasks,\noutperforming state-of-the-art image deraining and derain-drop methods on the\nRain100h and Raindrop800 data sets, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:44:30 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 06:54:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Lin", ""], ["Liu", "Jianzhuang", ""], ["Yuan", "Shanxin", ""], ["Slabaugh", "Gregory", ""], ["Leonardis", "Ales", ""], ["Zhou", "Wengang", ""], ["Tian", "Qi", ""]]}, {"id": "2007.07175", "submitter": "Abubakar Siddique", "authors": "Abubakar Siddique, Reza Jalil Mozhdehi, and Henry Medeiros", "title": "Deep Heterogeneous Autoencoder for Subspace Clustering of Sequential\n  Data", "comments": "14 pages, 3 figures, submitted to accv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised learning approach using a convolutional and fully\nconnected autoencoder, which we call deep heterogeneous autoencoder, to learn\ndiscriminative features from segmentation masks and detection bounding boxes.\nTo learn the mask shape information and its corresponding location in an input\nimage, we extract coarse masks from a pretrained semantic segmentation network\nas well as their corresponding bounding boxes. We train the autoencoders\njointly using task-dependent uncertainty weights to generate common latent\nfeatures. The feature vector is then fed to the k-means clustering algorithm to\nseparate the data points in the latent space. Finally, we incorporate\nadditional penalties in the form of a constraints graph based on prior\nknowledge of the sequential data to increase clustering robustness. We evaluate\nthe performance of our method using both synthetic and real world multi-object\nvideo datasets to demonstrate the applicability of our proposed model. Our\nresults show that the proposed technique outperforms several state-of-the-art\nmethods on challenging video sequences.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:47:56 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Siddique", "Abubakar", ""], ["Mozhdehi", "Reza Jalil", ""], ["Medeiros", "Henry", ""]]}, {"id": "2007.07177", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp,\n  Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris\n  Hoder, William T. Freeman", "title": "MosAIc: Finding Artistic Connections across Culture with Conditional\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce MosAIc, an interactive web app that allows users to find pairs\nof semantically related artworks that span different cultures, media, and\nmillennia. To create this application, we introduce Conditional Image Retrieval\n(CIR) which combines visual similarity search with user supplied filters or\n\"conditions\". This technique allows one to find pairs of similar images that\nspan distinct subsets of the image corpus. We provide a generic way to adapt\nexisting image retrieval data-structures to this new domain and provide\ntheoretical bounds on our approach's efficiency. To quantify the performance of\nCIR systems, we introduce new datasets for evaluating CIR methods and show that\nCIR performs non-parametric style transfer. Finally, we demonstrate that our\nCIR data-structures can identify \"blind spots\" in Generative Adversarial\nNetworks (GAN) where they fail to properly model the true data distribution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:50:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 18:25:23 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 01:08:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Fu", "Stephanie", ""], ["Lu", "Mindren", ""], ["Bui", "Johnny", ""], ["Bopp", "Darius", ""], ["Chen", "Zhenbang", ""], ["Tran", "Felix", ""], ["Wang", "Margaret", ""], ["Rogers", "Marina", ""], ["Zhang", "Lei", ""], ["Hoder", "Chris", ""], ["Freeman", "William T.", ""]]}, {"id": "2007.07197", "submitter": "Mingkui Tan", "authors": "Yong Guo, Yaofo Chen, Yin Zheng, Peilin Zhao, Jian Chen, Junzhou\n  Huang, Mingkui Tan", "title": "Breaking the Curse of Space Explosion: Towards Efficient NAS with\n  Curriculum Search", "comments": "Accepted by ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has become an important approach to\nautomatically find effective architectures. To cover all possible good\narchitectures, we need to search in an extremely large search space with\nbillions of candidate architectures. More critically, given a large search\nspace, we may face a very challenging issue of space explosion. However, due to\nthe limitation of computational resources, we can only sample a very small\nproportion of the architectures, which provides insufficient information for\nthe training. As a result, existing methods may often produce suboptimal\narchitectures. To alleviate this issue, we propose a curriculum search method\nthat starts from a small search space and gradually incorporates the learned\nknowledge to guide the search in a large space. With the proposed search\nstrategy, our Curriculum Neural Architecture Search (CNAS) method significantly\nimproves the search efficiency and finds better architectures than existing NAS\nmethods. Extensive experiments on CIFAR-10 and ImageNet demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 02:29:06 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 08:56:56 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Guo", "Yong", ""], ["Chen", "Yaofo", ""], ["Zheng", "Yin", ""], ["Zhao", "Peilin", ""], ["Chen", "Jian", ""], ["Huang", "Junzhou", ""], ["Tan", "Mingkui", ""]]}, {"id": "2007.07214", "submitter": "Guojun Wang", "authors": "Guojun Wang, Bin Tian, Yunfeng Ai, Tong Xu, Long Chen and Dongpu Cao", "title": "CenterNet3D:An Anchor free Object Detector for Autonomous Driving", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and fast 3D object detection from point clouds is a key task in\nautonomous driving. Existing one-stage 3D object detection methods can achieve\nreal-time performance, however, they are dominated by anchor-based detectors\nwhich are inefficient and require additional post-processing. In this paper, we\neliminate anchors and model an object as a single point the center point of its\nbounding box. Based on the center point, we propose an anchor-free CenterNet3D\nNetwork that performs 3D object detection without anchors. Our CenterNet3D uses\nkeypoint estimation to find center points and directly regresses 3D bounding\nboxes. However, because inherent sparsity of point clouds, 3D object center\npoints are likely to be in empty space which makes it difficult to estimate\naccurate boundary. To solve this issue, we propose an auxiliary corner\nattention module to enforce the CNN backbone to pay more attention to object\nboundaries which is effective to obtain more accurate bounding boxes. Besides,\nour CenterNet3D is Non-Maximum Suppression free which makes it more efficient\nand simpler. On the KITTI benchmark, our proposed CenterNet3D achieves\ncompetitive performance with other one stage anchor-based methods which show\nthe efficacy of our proposed center point representation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:53:56 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 11:15:54 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 08:09:42 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wang", "Guojun", ""], ["Tian", "Bin", ""], ["Ai", "Yunfeng", ""], ["Xu", "Tong", ""], ["Chen", "Long", ""], ["Cao", "Dongpu", ""]]}, {"id": "2007.07218", "submitter": "Dengxin Dai", "authors": "Simon Hecker, Dengxin Dai, Alexander Liniger, Luc Van Gool", "title": "Learning Accurate and Human-Like Driving using Semantic Maps and\n  Attention", "comments": "IROS 2020 final version. arXiv admin note: text overlap with\n  arXiv:1903.10995", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how end-to-end driving models can be improved to\ndrive more accurately and human-like. To tackle the first issue we exploit\nsemantic and visual maps from HERE Technologies and augment the existing\nDrive360 dataset with such. The maps are used in an attention mechanism that\npromotes segmentation confidence masks, thus focusing the network on semantic\nclasses in the image that are important for the current driving situation.\nHuman-like driving is achieved using adversarial learning, by not only\nminimizing the imitation loss with respect to the human driver but by further\ndefining a discriminator, that forces the driving model to produce action\nsequences that are human-like. Our models are trained and evaluated on the\nDrive360 + HERE dataset, which features 60 hours and 3000 km of real-world\ndriving data. Extensive experiments show that our driving models are more\naccurate and behave more human-like than previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 22:25:27 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hecker", "Simon", ""], ["Dai", "Dengxin", ""], ["Liniger", "Alexander", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.07221", "submitter": "Jishan Shaikh", "authors": "Jishan Shaikh, Adya Sharma, Ankit Chouhan, Avinash Mahawar", "title": "Alpha-Net: Architecture, Models, and Applications", "comments": "13 pages, 8 figures, project paper preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning network training is usually computationally expensive and\nintuitively complex. We present a novel network architecture for custom\ntraining and weight evaluations. We reformulate the layers as ResNet-similar\nblocks with certain inputs and outputs of their own, the blocks (called Alpha\nblocks) on their connection configuration form their own network, combined with\nour novel loss function and normalization function form the complete Alpha-Net\narchitecture. We provided the empirical mathematical formulation of network\nloss function for more understanding of accuracy estimation and further\noptimizations. We implemented Alpha-Net with 4 different layer configurations\nto express the architecture behavior comprehensively. On a custom dataset based\non ImageNet benchmark, we evaluate Alpha-Net v1, v2, v3, and v4 for image\nrecognition to give the accuracy of 78.2%, 79.1%, 79.5%, and 78.3%\nrespectively. The Alpha-Net v3 gives improved accuracy of approx. 3% over the\nlast state-of-the-art network ResNet 50 on ImageNet benchmark. We also present\nan analysis of our dataset with 256, 512, and 1024 layers and different\nversions of the loss function. Input representation is also crucial for\ntraining as initial preprocessing will take only a handful of features to make\ntraining less complex than it needs to be. We also compared network behavior\nwith different layer structures, different loss functions, and different\nnormalization functions for better quantitative modeling of Alpha-Net.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 05:05:01 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Shaikh", "Jishan", ""], ["Sharma", "Adya", ""], ["Chouhan", "Ankit", ""], ["Mahawar", "Avinash", ""]]}, {"id": "2007.07222", "submitter": "Mingkui Tan", "authors": "Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao, Shuaicheng Niu,\n  Junzhou Huang, Mingkui Tan", "title": "Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis", "comments": "IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3006377", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based medical image diagnosis has shown great potential in\nclinical medicine. However, it often suffers two major difficulties in\nreal-world applications: 1) only limited labels are available for model\ntraining, due to expensive annotation costs over medical images; 2) labeled\nimages may contain considerable label noise (e.g., mislabeling labels) due to\ndiagnostic difficulties of diseases. To address these, we seek to exploit rich\nlabeled data from relevant domains to help the learning in the target task via\n{Unsupervised Domain Adaptation} (UDA). Unlike most UDA methods that rely on\nclean labeled data or assume samples are equally transferable, we innovatively\npropose a Collaborative Unsupervised Domain Adaptation algorithm, which\nconducts transferability-aware adaptation and conquers label noise in a\ncollaborative way. We theoretically analyze the generalization performance of\nthe proposed method, and also empirically evaluate it on both medical and\ngeneral images. Promising experimental results demonstrate the superiority and\ngeneralization of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 11:49:17 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhang", "Yifan", ""], ["Wei", "Ying", ""], ["Wu", "Qingyao", ""], ["Zhao", "Peilin", ""], ["Niu", "Shuaicheng", ""], ["Huang", "Junzhou", ""], ["Tan", "Mingkui", ""]]}, {"id": "2007.07227", "submitter": "Istv\\'an S\\'ar\\'andi", "authors": "Istv\\'an S\\'ar\\'andi and Timm Linder and Kai O. Arras and Bastian\n  Leibe", "title": "MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human\n  Pose Estimation", "comments": "See project page at https://vision.rwth-aachen.de/metrabs . Accepted\n  for publication in the IEEE Transactions on Biometrics, Behavior, and\n  Identity Science (TBIOM), Special Issue \"Selected Best Works From Automated\n  Face and Gesture Recognition 2020\". Extended version of FG paper\n  arXiv:2003.02953", "journal-ref": null, "doi": "10.1109/TBIOM.2020.3037257", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap representations have formed the basis of human pose estimation\nsystems for many years, and their extension to 3D has been a fruitful line of\nrecent research. This includes 2.5D volumetric heatmaps, whose X and Y axes\ncorrespond to image space and Z to metric depth around the subject. To obtain\nmetric-scale predictions, 2.5D methods need a separate post-processing step to\nresolve scale ambiguity. Further, they cannot localize body joints outside the\nimage boundaries, leading to incomplete estimates for truncated images. To\naddress these limitations, we propose metric-scale truncation-robust (MeTRo)\nvolumetric heatmaps, whose dimensions are all defined in metric 3D space,\ninstead of being aligned with image space. This reinterpretation of heatmap\ndimensions allows us to directly estimate complete, metric-scale poses without\ntest-time knowledge of distance or relying on anthropometric heuristics, such\nas bone lengths. To further demonstrate the utility our representation, we\npresent a differentiable combination of our 3D metric-scale heatmaps with 2D\nimage-space ones to estimate absolute 3D pose (our MeTRAbs architecture). We\nfind that supervision via absolute pose loss is crucial for accurate\nnon-root-relative localization. Using a ResNet-50 backbone without further\nlearned layers, we obtain state-of-the-art results on Human3.6M, MPI-INF-3DHP\nand MuPoTS-3D. Our code will be made publicly available to facilitate further\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 11:52:09 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 19:32:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["S\u00e1r\u00e1ndi", "Istv\u00e1n", ""], ["Linder", "Timm", ""], ["Arras", "Kai O.", ""], ["Leibe", "Bastian", ""]]}, {"id": "2007.07230", "submitter": "Daniel Elton", "authors": "Yingying Zhu, Youbao Tang, Yuxing Tang, Daniel C. Elton, Sungwon Lee,\n  Perry J. Pickhardt, Ronald M. Summers", "title": "Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture\n  Model", "comments": "Accepted to Medical Image Computing and Computer Assisted\n  Intervention (MICCAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning based segmentation models often generalize poorly\nbetween domains due to insufficient training data. In real-world clinical\napplications, cross-domain image analysis tools are in high demand since\nmedical images from different domains are often needed to achieve a precise\ndiagnosis. An important example in radiology is generalizing from non-contrast\nCT to contrast enhanced CTs. Contrast enhanced CT scans at different phases are\nused to enhance certain pathologies or organs. Many existing cross-domain\nimage-to-image translation models have been shown to improve cross-domain\nsegmentation of large organs. However, such models lack the ability to preserve\nfine structures during the translation process, which is significant for many\nclinical applications, such as segmenting small calcified plaques in the aorta\nand pelvic arteries. In order to preserve fine structures during medical image\ntranslation, we propose a patch-based model using shared latent variables from\na Gaussian mixture model. We compare our image translation framework to several\nstate-of-the-art methods on cross-domain image translation and show our model\ndoes a better job preserving fine structures. The superior performance of our\nmodel is verified by performing two tasks with the translated images -\ndetection and segmentation of aortic plaques and pancreas segmentation. We\nexpect the utility of our framework will extend to other problems beyond\nsegmentation due to the improved quality of the generated images and enhanced\nability to preserve small structures.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:48:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhu", "Yingying", ""], ["Tang", "Youbao", ""], ["Tang", "Yuxing", ""], ["Elton", "Daniel C.", ""], ["Lee", "Sungwon", ""], ["Pickhardt", "Perry J.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2007.07236", "submitter": "Chengzhi Mao", "authors": "Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray, Shuran Song,\n  Junfeng Yang, and Carl Vondrick", "title": "Multitask Learning Strengthens Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep networks achieve strong accuracy on a range of computer vision\nbenchmarks, they remain vulnerable to adversarial attacks, where imperceptible\ninput perturbations fool the network. We present both theoretical and empirical\nanalyses that connect the adversarial robustness of a model to the number of\ntasks that it is trained on. Experiments on two datasets show that attack\ndifficulty increases as the number of target tasks increase. Moreover, our\nresults suggest that when models are trained on multiple tasks at once, they\nbecome more robust to adversarial attacks on individual tasks. While\nadversarial defense remains an open challenge, our results suggest that deep\nnetworks are vulnerable partly because they are trained on too few tasks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:52:45 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 02:03:46 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Mao", "Chengzhi", ""], ["Gupta", "Amogh", ""], ["Nitin", "Vikram", ""], ["Ray", "Baishakhi", ""], ["Song", "Shuran", ""], ["Yang", "Junfeng", ""], ["Vondrick", "Carl", ""]]}, {"id": "2007.07238", "submitter": "Hung-Yu Tseng", "authors": "Hung-Yu Tseng, Matthew Fisher, Jingwan Lu, Yijun Li, Vladimir Kim,\n  Ming-Hsuan Yang", "title": "Modeling Artistic Workflows for Image Generation and Editing", "comments": "ECCV 2020. Code: https://github.com/hytseng0509/ArtEditing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often create art by following an artistic workflow involving multiple\nstages that inform the overall design. If an artist wishes to modify an earlier\ndecision, significant work may be required to propagate this new decision\nforward to the final artwork. Motivated by the above observations, we propose a\ngenerative model that follows a given artistic workflow, enabling both\nmulti-stage image generation as well as multi-stage image editing of an\nexisting piece of art. Furthermore, for the editing scenario, we introduce an\noptimization process along with learning-based regularization to ensure the\nedited image produced by the model closely aligns with the originally provided\nimage. Qualitative and quantitative results on three different artistic\ndatasets demonstrate the effectiveness of the proposed framework on both image\ngeneration and editing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:54:26 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Tseng", "Hung-Yu", ""], ["Fisher", "Matthew", ""], ["Lu", "Jingwan", ""], ["Li", "Yijun", ""], ["Kim", "Vladimir", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2007.07243", "submitter": "Guilin Liu", "authors": "Guilin Liu, Rohan Taori, Ting-Chun Wang, Zhiding Yu, Shiqiu Liu,\n  Fitsum A. Reda, Karan Sapra, Andrew Tao, Bryan Catanzaro", "title": "Transposer: Universal Texture Synthesis Using Feature Maps as Transposed\n  Convolution Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional CNNs for texture synthesis consist of a sequence of\n(de)-convolution and up/down-sampling layers, where each layer operates locally\nand lacks the ability to capture the long-term structural dependency required\nby texture synthesis. Thus, they often simply enlarge the input texture, rather\nthan perform reasonable synthesis. As a compromise, many recent methods\nsacrifice generalizability by training and testing on the same single (or fixed\nset of) texture image(s), resulting in huge re-training time costs for unseen\nimages. In this work, based on the discovery that the assembling/stitching\noperation in traditional texture synthesis is analogous to a transposed\nconvolution operation, we propose a novel way of using transposed convolution\noperation. Specifically, we directly treat the whole encoded feature map of the\ninput texture as transposed convolution filters and the features'\nself-similarity map, which captures the auto-correlation information, as input\nto the transposed convolution. Such a design allows our framework, once\ntrained, to be generalizable to perform synthesis of unseen textures with a\nsingle forward pass in nearly real-time. Our method achieves state-of-the-art\ntexture synthesis quality based on various metrics. While self-similarity helps\npreserve the input textures' regular structural patterns, our framework can\nalso take random noise maps for irregular input textures instead of\nself-similarity maps as transposed convolution inputs. It allows to get more\ndiverse results as well as generate arbitrarily large texture outputs by\ndirectly sampling large noise maps in a single pass as well.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:57:59 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Guilin", ""], ["Taori", "Rohan", ""], ["Wang", "Ting-Chun", ""], ["Yu", "Zhiding", ""], ["Liu", "Shiqiu", ""], ["Reda", "Fitsum A.", ""], ["Sapra", "Karan", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2007.07247", "submitter": "Yunzhong Hou", "authors": "Yunzhong Hou, Liang Zheng, Stephen Gould", "title": "Multiview Detection with Feature Perspective Transformation", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating multiple camera views for detection alleviates the impact of\nocclusions in crowded scenes. In a multiview system, we need to answer two\nimportant questions when dealing with ambiguities that arise from occlusions.\nFirst, how should we aggregate cues from the multiple views? Second, how should\nwe aggregate unreliable 2D and 3D spatial information that has been tainted by\nocclusions? To address these questions, we propose a novel multiview detection\nsystem, MVDet. For multiview aggregation, existing methods combine anchor box\nfeatures from the image plane, which potentially limits performance due to\ninaccurate anchor box shapes and sizes. In contrast, we take an anchor-free\napproach to aggregate multiview information by projecting feature maps onto the\nground plane (bird's eye view). To resolve any remaining spatial ambiguity, we\napply large kernel convolutions on the ground plane feature map and infer\nlocations from detection peaks. Our entire model is end-to-end learnable and\nachieves 88.2% MODA on the standard Wildtrack dataset, outperforming the\nstate-of-the-art by 14.1%. We also provide detailed analysis of MVDet on a\nnewly introduced synthetic dataset, MultiviewX, which allows us to control the\nlevel of occlusion. Code and MultiviewX dataset are available at\nhttps://github.com/hou-yz/MVDet.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:58:30 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 11:15:13 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hou", "Yunzhong", ""], ["Zheng", "Liang", ""], ["Gould", "Stephen", ""]]}, {"id": "2007.07268", "submitter": "Roberto Bigazzi", "authors": "Roberto Bigazzi, Federico Landi, Marcella Cornia, Silvia Cascianelli,\n  Lorenzo Baraldi, Rita Cucchiara", "title": "Explore and Explain: Self-supervised Navigation and Recounting", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied AI has been recently gaining attention as it aims to foster the\ndevelopment of autonomous and intelligent agents. In this paper, we devise a\nnovel embodied setting in which an agent needs to explore a previously unknown\nenvironment while recounting what it sees during the path. In this context, the\nagent needs to navigate the environment driven by an exploration goal, select\nproper moments for description, and output natural language descriptions of\nrelevant objects and scenes. Our model integrates a novel self-supervised\nexploration module with penalty, and a fully-attentive captioning model for\nexplanation. Also, we investigate different policies for selecting proper\nmoments for explanation, driven by information coming from both the environment\nand the navigation. Experiments are conducted on photorealistic environments\nfrom the Matterport3D dataset and investigate the navigation and explanation\ncapabilities of the agent as well as the role of their interactions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 18:00:49 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Bigazzi", "Roberto", ""], ["Landi", "Federico", ""], ["Cornia", "Marcella", ""], ["Cascianelli", "Silvia", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2007.07296", "submitter": "Hanchi Ren", "authors": "Hanchi Ren, Jingjing Deng and Xianghua Xie", "title": "Privacy Preserving Text Recognition with Gradient-Boosting for Federated\n  Learning", "comments": "The paper has been submitted to BMVC2020 on April 30th", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical machine learning approaches require centralized data for model\ntraining, which may not be possible where restrictions on data sharing are in\nplace due to, for instance, privacy protection. The recently proposed Federated\nLearning (FL) frame-work allows learning a shared model collaboratively without\ndata being centralized or data sharing among data owners. However, we show in\nthis paper that the generalization ability of the joint model is poor on\nNon-Independent and Non-Identically Dis-tributed (Non-IID) data, particularly\nwhen the Federated Averaging (FedAvg) strategy is used in this collaborative\nlearning framework thanks to the weight divergence phenomenon. We propose a\nnovel boosting algorithm for FL to address this generalisation issue, as well\nas achieving much faster convergence in gradient based optimization. We\ndemonstrate our Federated Boosting (FedBoost) method on privacy-preserved text\nrecognition, which shows significant improvements in both performance and\nefficiency. The text images are based on publicly available datasets for fair\ncomparison and we intend to make our implementation public to ensure\nreproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 18:47:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Ren", "Hanchi", ""], ["Deng", "Jingjing", ""], ["Xie", "Xianghua", ""]]}, {"id": "2007.07306", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Lorenzo Torresani", "title": "COBE: Contextualized Object Embeddings from Narrated Instructional Video", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objects in the real world undergo dramatic variations in visual\nappearance. For example, a tomato may be red or green, sliced or chopped, fresh\nor fried, liquid or solid. Training a single detector to accurately recognize\ntomatoes in all these different states is challenging. On the other hand,\ncontextual cues (e.g., the presence of a knife, a cutting board, a strainer or\na pan) are often strongly indicative of how the object appears in the scene.\nRecognizing such contextual cues is useful not only to improve the accuracy of\nobject detection or to determine the state of the object, but also to\nunderstand its functional properties and to infer ongoing or upcoming\nhuman-object interactions. A fully-supervised approach to recognizing object\nstates and their contexts in the real-world is unfortunately marred by the\nlong-tailed, open-ended distribution of the data, which would effectively\nrequire massive amounts of annotations to capture the appearance of objects in\nall their different forms. Instead of relying on manually-labeled data for this\ntask, we propose a new framework for learning Contextualized OBject Embeddings\n(COBE) from automatically-transcribed narrations of instructional videos. We\nleverage the semantic and compositional structure of language by training a\nvisual detector to predict a contextualized word embedding of the object and\nits associated narration. This enables the learning of an object representation\nwhere concepts relate according to a semantic language metric. Our experiments\nshow that our detector learns to predict a rich variety of contextual object\ninformation, and that it is highly effective in the settings of few-shot and\nzero-shot learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:04:08 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 21:52:34 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Bertasius", "Gedas", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2007.07307", "submitter": "Matthew Willetts", "authors": "Matthew Willetts, Xenia Miscouridou, Stephen Roberts, Chris Holmes", "title": "Relaxed-Responsibility Hierarchical Discrete VAEs", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successfully training Variational Autoencoders (VAEs) with a hierarchy of\ndiscrete latent variables remains an area of active research.\n  Vector-Quantised VAEs are a powerful approach to discrete VAEs, but naive\nhierarchical extensions can be unstable when training. Leveraging insights from\nclassical methods of inference we introduce \\textit{Relaxed-Responsibility\nVector-Quantisation}, a novel way to parameterise discrete latent variables, a\nrefinement of relaxed Vector-Quantisation that gives better performance and\nmore stable training. This enables a novel approach to hierarchical discrete\nvariational autoencoders with numerous layers of latent variables (here up to\n32) that we train end-to-end. Within hierarchical probabilistic deep generative\nmodels with discrete latent variables trained end-to-end, we achieve\nstate-of-the-art bits-per-dim results for various standard datasets. % Unlike\ndiscrete VAEs with a single layer of latent variables, we can produce samples\nby ancestral sampling: it is not essential to train a second autoregressive\ngenerative model over the learnt latent representations to then sample from and\nthen decode. % Moreover, that latter approach in these deep hierarchical models\nwould require thousands of forward passes to generate a single sample. Further,\nwe observe different layers of our model become associated with different\naspects of the data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:10:05 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:59:59 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Willetts", "Matthew", ""], ["Miscouridou", "Xenia", ""], ["Roberts", "Stephen", ""], ["Holmes", "Chris", ""]]}, {"id": "2007.07350", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "A Generalization of Otsu's Method and Minimum Error Thresholding", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Generalized Histogram Thresholding (GHT), a simple, fast, and\neffective technique for histogram-based image thresholding. GHT works by\nperforming approximate maximum a posteriori estimation of a mixture of\nGaussians with appropriate priors. We demonstrate that GHT subsumes three\nclassic thresholding techniques as special cases: Otsu's method, Minimum Error\nThresholding (MET), and weighted percentile thresholding. GHT thereby enables\nthe continuous interpolation between those three algorithms, which allows\nthresholding accuracy to be improved significantly. GHT also provides a\nclarifying interpretation of the common practice of coarsening a histogram's\nbin width during thresholding. We show that GHT outperforms or matches the\nperformance of all algorithms on a recent challenge for handwritten document\nimage binarization (including deep neural networks trained to produce per-pixel\nbinarizations), and can be implemented in a dozen lines of code or as a trivial\nmodification to Otsu's method or MET.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 20:55:10 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 04:27:21 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 03:30:03 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "2007.07355", "submitter": "Yogesh Rawat", "authors": "Ugur Demir, Yogesh S Rawat, Mubarak Shah", "title": "TinyVIRAT: Low-resolution Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing research in action recognition is mostly focused on high-quality\nvideos where the action is distinctly visible. In real-world surveillance\nenvironments, the actions in videos are captured at a wide range of\nresolutions. Most activities occur at a distance with a small resolution and\nrecognizing such activities is a challenging problem. In this work, we focus on\nrecognizing tiny actions in videos. We introduce a benchmark dataset,\nTinyVIRAT, which contains natural low-resolution activities. The actions in\nTinyVIRAT videos have multiple labels and they are extracted from surveillance\nvideos which makes them realistic and more challenging. We propose a novel\nmethod for recognizing tiny actions in videos which utilizes a progressive\ngenerative approach to improve the quality of low-resolution actions. The\nproposed method also consists of a weakly trained attention mechanism which\nhelps in focusing on the activity regions in the video. We perform extensive\nexperiments to benchmark the proposed TinyVIRAT dataset and observe that the\nproposed method significantly improves the action recognition performance over\nbaselines. We also evaluate the proposed approach on synthetically resized\naction recognition datasets and achieve state-of-the-art results when compared\nwith existing methods. The dataset and code is publicly available at\nhttps://github.com/UgurDemir/Tiny-VIRAT.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:09:18 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Demir", "Ugur", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "2007.07357", "submitter": "Ahmadreza Jeddi", "authors": "Ahmadreza Jeddi", "title": "Tackling the Problem of Limited Data and Annotations in Semantic\n  Segmentation", "comments": "10 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the case of semantic segmentation on a small image dataset\n(simulated by 1000 randomly selected images from PASCAL VOC 2012), where only\nweak supervision signals (scribbles from user interaction) are available is\nstudied. Especially, to tackle the problem of limited data annotations in image\nsegmentation, transferring different pre-trained models and CRF based methods\nare applied to enhance the segmentation performance. To this end, RotNet,\nDeeperCluster, and Semi&Weakly Supervised Learning (SWSL) pre-trained models\nare transferred and finetuned in a DeepLab-v2 baseline, and dense CRF is\napplied both as a post-processing and loss regularization technique. The\nresults of my study show that, on this small dataset, using a pre-trained\nResNet50 SWSL model gives results that are 7.4% better than applying an\nImageNet pre-trained model; moreover, for the case of training on the full\nPASCAL VOC 2012 training data, this pre-training approach increases the mIoU\nresults by almost 4%. On the other hand, dense CRF is shown to be very\neffective as well, enhancing the results both as a loss regularization\ntechnique in weakly supervised training and as a post-processing tool.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:11:11 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Jeddi", "Ahmadreza", ""]]}, {"id": "2007.07375", "submitter": "Maria Brbic", "authors": "Kaidi Cao, Maria Brbic, Jure Leskovec", "title": "Concept Learners for Few-Shot Learning", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing algorithms that are able to generalize to a novel task given only\na few labeled examples represents a fundamental challenge in closing the gap\nbetween machine- and human-level performance. The core of human cognition lies\nin the structured, reusable concepts that help us to rapidly adapt to new tasks\nand provide reasoning behind our decisions. However, existing meta-learning\nmethods learn complex representations across prior labeled tasks without\nimposing any structure on the learned representations. Here we propose COMET, a\nmeta-learning method that improves generalization ability by learning to learn\nalong human-interpretable concept dimensions. Instead of learning a joint\nunstructured metric space, COMET learns mappings of high-level concepts into\nsemi-structured metric spaces, and effectively combines the outputs of\nindependent concept learners. We evaluate our model on few-shot tasks from\ndiverse domains, including fine-grained image classification, document\ncategorization and cell type annotation on a novel dataset from a biological\ndomain developed in our work. COMET significantly outperforms strong\nmeta-learning baselines, achieving 6-15% relative improvement on the most\nchallenging 1-shot learning tasks, while unlike existing methods providing\ninterpretations behind the model's predictions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:04:17 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 03:52:19 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 05:19:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cao", "Kaidi", ""], ["Brbic", "Maria", ""], ["Leskovec", "Jure", ""]]}, {"id": "2007.07396", "submitter": "Fernando Alonso-Fernandez", "authors": "Fredrik Svanstrom, Cristofer Englund, Fernando Alonso-Fernandez", "title": "Real-Time Drone Detection and Tracking With Visible, Thermal and\n  Acoustic Sensors", "comments": null, "journal-ref": "Proc. Intl Conf on Pattern Recognition, ICPR, Milan, Italy, 10-15\n  January 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the process of designing an automatic multi-sensor drone\ndetection system. Besides the common video and audio sensors, the system also\nincludes a thermal infrared camera, which is shown to be a feasible solution to\nthe drone detection task. Even with slightly lower resolution, the performance\nis just as good as a camera in visible range. The detector performance as a\nfunction of the sensor-to-target distance is also investigated. In addition,\nusing sensor fusion, the system is made more robust than the individual\nsensors, helping to reduce false detections. To counteract the lack of public\ndatasets, a novel video dataset containing 650 annotated infrared and visible\nvideos of drones, birds, airplanes and helicopters is also presented\n(https://github.com/DroneDetectionThesis/Drone-detection-dataset). The database\nis complemented with an audio dataset of the classes drones, helicopters and\nbackground noise.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:06:42 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 12:49:35 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Svanstrom", "Fredrik", ""], ["Englund", "Cristofer", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "2007.07400", "submitter": "Vinay Ramasesh", "authors": "Vinay V. Ramasesh, Ethan Dyer, Maithra Raghu", "title": "Anatomy of Catastrophic Forgetting: Hidden Representations and Task\n  Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in developing versatile machine learning systems is\ncatastrophic forgetting: a model trained on tasks in sequence will suffer\nsignificant performance drops on earlier tasks. Despite the ubiquity of\ncatastrophic forgetting, there is limited understanding of the underlying\nprocess and its causes. In this paper, we address this important knowledge gap,\ninvestigating how forgetting affects representations in neural network models.\nThrough representational analysis techniques, we find that deeper layers are\ndisproportionately the source of forgetting. Supporting this, a study of\nmethods to mitigate forgetting illustrates that they act to stabilize deeper\nlayers. These insights enable the development of an analytic argument and\nempirical picture relating the degree of forgetting to representational\nsimilarity between tasks. Consistent with this picture, we observe maximal\nforgetting occurs for task sequences with intermediate similarity. We perform\nempirical studies on the standard split CIFAR-10 setup and also introduce a\nnovel CIFAR-100 based task approximating realistic input distribution shift.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:31:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Ramasesh", "Vinay V.", ""], ["Dyer", "Ethan", ""], ["Raghu", "Maithra", ""]]}, {"id": "2007.07404", "submitter": "Mahmoud Saeedimoghaddam", "authors": "Mahmoud Saeedimoghaddam and T. F. Stepinski", "title": "Automatic extraction of road intersection points from USGS historical\n  map series using deep convolutional neural networks", "comments": "23 pages, 8 figures", "journal-ref": "2020, International Journal of Geographical Information Science,\n  34:5, 947-968", "doi": "10.1080/13658816.2019.1696968", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road intersections data have been used across different geospatial\napplications and analysis. The road network datasets dating from pre-GIS years\nare only available in the form of historical printed maps. Before they can be\nanalyzed by a GIS software, they need to be scanned and transformed into the\nusable vector-based format. Due to the great bulk of scanned historical maps,\nautomated methods of transforming them into digital datasets need to be\nemployed. Frequently, this process is based on computer vision algorithms.\nHowever, low conversion accuracy for low quality and visually complex maps and\nsetting optimal parameters are the two challenges of using those algorithms. In\nthis paper, we employed the standard paradigm of using deep convolutional\nneural network for object detection task named region-based CNN for\nautomatically identifying road intersections in scanned historical USGS maps of\nseveral U.S. cities. We have found that the algorithm showed higher conversion\naccuracy for the double line cartographic representations of the road maps than\nthe single line ones. Also, compared to the majority of traditional computer\nvision algorithms RCNN provides more accurate extraction. Finally, the results\nshow that the amount of errors in the detection outputs is sensitive to\ncomplexity and blurriness of the maps as well as the number of distinct RGB\ncombinations within them.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:51:15 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Saeedimoghaddam", "Mahmoud", ""], ["Stepinski", "T. F.", ""]]}, {"id": "2007.07415", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Wei Zhang, Jinye Peng, Jianping Fan", "title": "Automatic Image Labelling at Pixel Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep networks for semantic image segmentation largely\ndepends on the availability of large-scale training images which are labelled\nat the pixel level. Typically, such pixel-level image labellings are obtained\nmanually by a labour-intensive process. To alleviate the burden of manual image\nlabelling, we propose an interesting learning approach to generate pixel-level\nimage labellings automatically. A Guided Filter Network (GFN) is first\ndeveloped to learn the segmentation knowledge from a source domain, and such\nGFN then transfers such segmentation knowledge to generate coarse object masks\nin the target domain. Such coarse object masks are treated as pseudo labels and\nthey are further integrated to optimize/refine the GFN iteratively in the\ntarget domain. Our experiments on six image sets have demonstrated that our\nproposed approach can generate fine-grained object masks (i.e., pixel-level\nobject labellings), whose quality is very comparable to the manually-labelled\nones. Our proposed approach can also achieve better performance on semantic\nimage segmentation than most existing weakly-supervised approaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 00:34:11 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 03:17:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Xiang", ""], ["Zhang", "Wei", ""], ["Peng", "Jinye", ""], ["Fan", "Jianping", ""]]}, {"id": "2007.07423", "submitter": "Hong-Yu Zhou", "authors": "Hong-Yu Zhou and Shuang Yu and Cheng Bian and Yifan Hu and Kai Ma and\n  Yefeng Zheng", "title": "Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By\n  Comparing Image Representations", "comments": "MICCAI 2020 early accept; Code and pretrained models available at\n  http://github.com/funnyzhou/C2L_MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning era, pretrained models play an important role in medical\nimage analysis, in which ImageNet pretraining has been widely adopted as the\nbest way. However, it is undeniable that there exists an obvious domain gap\nbetween natural images and medical images. To bridge this gap, we propose a new\npretraining method which learns from 700k radiographs given no manual\nannotations. We call our method as Comparing to Learn (C2L) because it learns\nrobust features by comparing different image representations. To verify the\neffectiveness of C2L, we conduct comprehensive ablation studies and evaluate it\non different tasks and datasets. The experimental results on radiographs show\nthat C2L can outperform ImageNet pretraining and previous state-of-the-art\napproaches significantly. Code and models are available.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 01:14:34 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 02:17:03 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 03:00:56 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhou", "Hong-Yu", ""], ["Yu", "Shuang", ""], ["Bian", "Cheng", ""], ["Hu", "Yifan", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.07431", "submitter": "Kuniaki Saito", "authors": "Kuniaki Saito, Kate Saenko, Ming-Yu Liu", "title": "COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content\n  Conditioned Style Encoder", "comments": "The paper will be presented at the EUROPEAN Conference on Computer\n  Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation intends to learn a mapping of an\nimage in a given domain to an analogous image in a different domain, without\nexplicit supervision of the mapping. Few-shot unsupervised image-to-image\ntranslation further attempts to generalize the model to an unseen domain by\nleveraging example images of the unseen domain provided at inference time.\nWhile remarkably successful, existing few-shot image-to-image translation\nmodels find it difficult to preserve the structure of the input image while\nemulating the appearance of the unseen domain, which we refer to as the content\nloss problem. This is particularly severe when the poses of the objects in the\ninput and example images are very different. To address the issue, we propose a\nnew few-shot image translation model, COCO-FUNIT, which computes the style\nembedding of the example images conditioned on the input image and a new module\ncalled the constant style bias. Through extensive experimental validations with\ncomparison to the state-of-the-art, our model shows effectiveness in addressing\nthe content loss problem. For code and pretrained models, please check out\nhttps://nvlabs.github.io/COCO-FUNIT/ .\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:01:14 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 23:45:21 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 02:06:50 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Saito", "Kuniaki", ""], ["Saenko", "Kate", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "2007.07435", "submitter": "Hadi Mohaghegh Dolatabadi", "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie", "title": "AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing\n  Flows", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classifiers are susceptible to well-crafted, imperceptible\nvariations of their inputs, known as adversarial attacks. In this regard, the\nstudy of powerful attack models sheds light on the sources of vulnerability in\nthese classifiers, hopefully leading to more robust ones. In this paper, we\nintroduce AdvFlow: a novel black-box adversarial attack method on image\nclassifiers that exploits the power of normalizing flows to model the density\nof adversarial examples around a given target image. We see that the proposed\nmethod generates adversaries that closely follow the clean data distribution, a\nproperty which makes their detection less likely. Also, our experimental\nresults show competitive performance of the proposed approach with some of the\nexisting attack methods on defended classifiers. The code is available at\nhttps://github.com/hmdolatabadi/AdvFlow.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:13:49 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 00:36:25 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Dolatabadi", "Hadi M.", ""], ["Erfani", "Sarah", ""], ["Leckie", "Christopher", ""]]}, {"id": "2007.07437", "submitter": "Yaran Chen", "authors": "Junwen Chen, Yi Lu, Yaran Chen, Dongbin Zhao, and Zhonghua Pang", "title": "ContourRend: A Segmentation Method for Improving Contours by Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good object segmentation should contain clear contours and complete\nregions. However, mask-based segmentation can not handle contour features well\non a coarse prediction grid, thus causing problems of blurry edges. While\ncontour-based segmentation provides contours directly, but misses contours'\ndetails. In order to obtain fine contours, we propose a segmentation method\nnamed ContourRend which adopts a contour renderer to refine segmentation\ncontours. And we implement our method on a segmentation model based on graph\nconvolutional network (GCN). For the single object segmentation task on\ncityscapes dataset, the GCN-based segmentation con-tour is used to generate a\ncontour of a single object, then our contour renderer focuses on the pixels\naround the contour and predicts the category at high resolution. By rendering\nthe contour result, our method reaches 72.41% mean intersection over union\n(IoU) and surpasses baseline Polygon-GCN by 1.22%.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:16:00 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Chen", "Junwen", ""], ["Lu", "Yi", ""], ["Chen", "Yaran", ""], ["Zhao", "Dongbin", ""], ["Pang", "Zhonghua", ""]]}, {"id": "2007.07452", "submitter": "Ziyue Zhang", "authors": "Ziyue Zhang, Shuai Jiang, Congzhentao Huang, Yang Li and Richard Yi Da\n  Xu", "title": "RGB-IR Cross-modality Person ReID based on Teacher-Student GAN Model", "comments": "8 pages including 1 page reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared (RGB-IR) person re-identification (ReID) is a technology where\nthe system can automatically identify the same person appearing at different\nparts of a video when light is unavailable. The critical challenge of this task\nis the cross-modality gap of features under different modalities. To solve this\nchallenge, we proposed a Teacher-Student GAN model (TS-GAN) to adopt different\ndomains and guide the ReID backbone to learn better ReID information. (1) In\norder to get corresponding RGB-IR image pairs, the RGB-IR Generative\nAdversarial Network (GAN) was used to generate IR images. (2) To kick-start the\ntraining of identities, a ReID Teacher module was trained under IR modality\nperson images, which is then used to guide its Student counterpart in training.\n(3) Likewise, to better adapt different domain features and enhance model ReID\nperformance, three Teacher-Student loss functions were used. Unlike other GAN\nbased models, the proposed model only needs the backbone module at the test\nstage, making it more efficient and resource-saving. To showcase our model's\ncapability, we did extensive experiments on the newly-released SYSU-MM01 RGB-IR\nRe-ID benchmark and achieved superior performance to the state-of-the-art with\n49.8% Rank-1 and 47.4% mAP.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:58:46 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Ziyue", ""], ["Jiang", "Shuai", ""], ["Huang", "Congzhentao", ""], ["Li", "Yang", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "2007.07453", "submitter": "Wanhua Li", "authors": "Wanhua Li, Yueqi Duan, Jiwen Lu, Jianjiang Feng, Jie Zhou", "title": "Graph-Based Social Relation Reasoning", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings are fundamentally sociable -- that we generally organize our\nsocial lives in terms of relations with other people. Understanding social\nrelations from an image has great potential for intelligent systems such as\nsocial chatbots and personal assistants. In this paper, we propose a simpler,\nfaster, and more accurate method named graph relational reasoning network\n(GR2N) for social relation recognition. Different from existing methods which\nprocess all social relations on an image independently, our method considers\nthe paradigm of jointly inferring the relations by constructing a social\nrelation graph. Furthermore, the proposed GR2N constructs several virtual\nrelation graphs to explicitly grasp the strong logical constraints among\ndifferent types of social relations. Experimental results illustrate that our\nmethod generates a reasonable and consistent social relation graph and improves\nthe performance in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:01:11 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 06:32:03 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 07:20:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Wanhua", ""], ["Duan", "Yueqi", ""], ["Lu", "Jiwen", ""], ["Feng", "Jianjiang", ""], ["Zhou", "Jie", ""]]}, {"id": "2007.07456", "submitter": "Joao Florindo", "authors": "Joao Florindo", "title": "Reorganizing local image features with chaotic maps: an application to\n  texture recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of convolutional neural networks in texture\nrecognition, model-based descriptors are still competitive, especially when we\ndo not have access to large amounts of annotated data for training and the\ninterpretation of the model is an important issue. Among the model-based\napproaches, fractal geometry has been one of the most popular, especially in\nbiological applications. Nevertheless, fractals are part of a much broader\nfamily of models, which are the non-linear operators, studied in chaos theory.\nIn this context, we propose here a chaos-based local descriptor for texture\nrecognition. More specifically, we map the image into the three-dimensional\nEuclidean space, iterate a chaotic map over this three-dimensional structure\nand convert it back to the original image. From such chaos-transformed image at\neach iteration we collect local descriptors (here we use local binary patters)\nand those descriptors compose the feature representation of the texture. The\nperformance of our method was verified on the classification of benchmark\ndatabases and in the identification of Brazilian plant species based on the\ntexture of the leaf surface. The achieved results confirmed our expectation of\na competitive performance, even when compared with some learning-based modern\napproaches in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:15:01 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Florindo", "Joao", ""]]}, {"id": "2007.07462", "submitter": "Joao Florindo", "authors": "Joao Florindo, Konradin Metze", "title": "A cellular automata approach to local patterns for texture recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture recognition is one of the most important tasks in computer vision\nand, despite the recent success of learning-based approaches, there is still\nneed for model-based solutions. This is especially the case when the amount of\ndata available for training is not sufficiently large, a common situation in\nseveral applied areas, or when computational resources are limited. In this\ncontext, here we propose a method for texture descriptors that combines the\nrepresentation power of complex objects by cellular automata with the known\neffectiveness of local descriptors in texture analysis. The method formulates a\nnew transition function for the automaton inspired on local binary descriptors.\nIt counterbalances the new state of each cell with the previous state, in this\nway introducing an idea of \"controlled deterministic chaos\". The descriptors\nare obtained from the distribution of cell states. The proposed descriptors are\napplied to the classification of texture images both on benchmark data sets and\na real-world problem, i.e., that of identifying plant species based on the\ntexture of their leaf surfaces. Our proposal outperforms other classical and\nstate-of-the-art approaches, especially in the real-world problem, thus\nrevealing its potential to be applied in numerous practical tasks involving\ntexture recognition at some stage.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:25:51 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Florindo", "Joao", ""], ["Metze", "Konradin", ""]]}, {"id": "2007.07477", "submitter": "Sercan Arik", "authors": "Yu-han Liu and Sercan O. Arik", "title": "Explaining Deep Neural Networks using Unsupervised Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to explain trained deep neural networks (DNNs), by\ndistilling them into surrogate models using unsupervised clustering. Our method\ncan be applied flexibly to any subset of layers of a DNN architecture and can\nincorporate low-level and high-level information. On image datasets given\npre-trained DNNs, we demonstrate the strength of our method in finding similar\ntraining samples, and shedding light on the concepts the DNNs base their\ndecisions on. Via user studies, we show that our model can improve the user\ntrust in model's prediction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 04:49:43 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 00:50:15 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Liu", "Yu-han", ""], ["Arik", "Sercan O.", ""]]}, {"id": "2007.07482", "submitter": "Pradipta Biswas", "authors": "Abhishek Mukhopadhyay, Imon Mukherjee, Pradipta Biswas", "title": "Decoding CNN based Object Classifier Using Visualization", "comments": "Accepted at ACM International conference on Automotive User Interface\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how working of Convolutional Neural Network (CNN) can\nbe explained through visualization in the context of machine perception of\nautonomous vehicles. We visualize what type of features are extracted in\ndifferent convolution layers of CNN that helps to understand how CNN gradually\nincreases spatial information in every layer. Thus, it concentrates on region\nof interests in every transformation. Visualizing heat map of activation helps\nus to understand how CNN classifies and localizes different objects in image.\nThis study also helps us to reason behind low accuracy of a model helps to\nincrease trust on object detection module.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:01:27 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Mukhopadhyay", "Abhishek", ""], ["Mukherjee", "Imon", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2007.07502", "submitter": "Sharath M. Shankaranarayana Mr", "authors": "Sharath M Shankaranarayana and Keerthi Ram and Kaushik Mitra and\n  Mohanasankar Sivaprakasam", "title": "Monocular Retinal Depth Estimation and Joint Optic Disc and Cup\n  Segmentation using Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important parameters for the assessment of glaucoma is optic nerve\nhead (ONH) evaluation, which usually involves depth estimation and subsequent\noptic disc and cup boundary extraction. Depth is usually obtained explicitly\nfrom imaging modalities like optical coherence tomography (OCT) and is very\nchallenging to estimate depth from a single RGB image. To this end, we propose\na novel method using adversarial network to predict depth map from a single\nimage. The proposed depth estimation technique is trained and evaluated using\nindividual retinal images from INSPIRE-stereo dataset. We obtain a very high\naverage correlation coefficient of 0.92 upon five fold cross validation\noutperforming the state of the art. We then use the depth estimation process as\na proxy task for joint optic disc and cup segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:21:46 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Shankaranarayana", "Sharath M", ""], ["Ram", "Keerthi", ""], ["Mitra", "Kaushik", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "2007.07506", "submitter": "Minchul Kim", "authors": "Minchul Kim, Jongchan Park, Seil Na, Chang Min Park, Donggeun Yoo", "title": "Learning Visual Context by Comparison", "comments": "ECCV 2020 spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding diseases from an X-ray image is an important yet highly challenging\ntask. Current methods for solving this task exploit various characteristics of\nthe chest X-ray image, but one of the most important characteristics is still\nmissing: the necessity of comparison between related regions in an image. In\nthis paper, we present Attend-and-Compare Module (ACM) for capturing the\ndifference between an object of interest and its corresponding context. We show\nthat explicit difference modeling can be very helpful in tasks that require\ndirect comparison between locations from afar. This module can be plugged into\nexisting deep learning models. For evaluation, we apply our module to three\nchest X-ray recognition tasks and COCO object detection & segmentation tasks\nand observe consistent improvements across tasks. The code is available at\nhttps://github.com/mk-minchul/attend-and-compare.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:47:06 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Kim", "Minchul", ""], ["Park", "Jongchan", ""], ["Na", "Seil", ""], ["Park", "Chang Min", ""], ["Yoo", "Donggeun", ""]]}, {"id": "2007.07524", "submitter": "Dohyung Kim MR", "authors": "Wonkyung Lee, Junghyup Lee, Dohyung Kim, Bumsub Ham", "title": "Learning with Privileged Information for Efficient Image\n  Super-Resolution", "comments": "ECCV-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have allowed remarkable advances in\nsingle image super-resolution (SISR) over the last decade. Most SR methods\nbased on CNNs have focused on achieving performance gains in terms of quality\nmetrics, such as PSNR and SSIM, over classical approaches. They typically\nrequire a large amount of memory and computational units. FSRCNN, consisting of\nfew numbers of convolutional layers, has shown promising results, while using\nan extremely small number of network parameters. We introduce in this paper a\nnovel distillation framework, consisting of teacher and student networks, that\nallows to boost the performance of FSRCNN drastically. To this end, we propose\nto use ground-truth high-resolution (HR) images as privileged information. The\nencoder in the teacher learns the degradation process, subsampling of HR\nimages, using an imitation loss. The student and the decoder in the teacher,\nhaving the same network architecture as FSRCNN, try to reconstruct HR images.\nIntermediate features in the decoder, affordable for the student to learn, are\ntransferred to the student through feature distillation. Experimental results\non standard benchmarks demonstrate the effectiveness and the generalization\nability of our framework, which significantly boosts the performance of FSRCNN\nas well as other SR methods. Our code and model are available online:\nhttps://cvlab.yonsei.ac.kr/projects/PISR.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:44:18 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lee", "Wonkyung", ""], ["Lee", "Junghyup", ""], ["Kim", "Dohyung", ""], ["Ham", "Bumsub", ""]]}, {"id": "2007.07527", "submitter": "Yifan Wang", "authors": "Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding, Shenghua Gao, Yi Ma", "title": "Learning to Parse Wireframes in Images of Man-Made Environments", "comments": "CVPR 2018", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (2018)\n  626-635", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning-based approach to the task of\nautomatically extracting a \"wireframe\" representation for images of cluttered\nman-made environments. The wireframe (see Fig. 1) contains all salient straight\nlines and their junctions of the scene that encode efficiently and accurately\nlarge-scale geometry and object shapes. To this end, we have built a very large\nnew dataset of over 5,000 images with wireframes thoroughly labelled by humans.\nWe have proposed two convolutional neural networks that are suitable for\nextracting junctions and lines with large spatial support, respectively. The\nnetworks trained on our dataset have achieved significantly better performance\nthan state-of-the-art methods for junction detection and line segment\ndetection, respectively. We have conducted extensive experiments to evaluate\nquantitatively and qualitatively the wireframes obtained by our method, and\nhave convincingly shown that effectively and efficiently parsing wireframes for\nimages of man-made environments is a feasible goal within reach. Such\nwireframes could benefit many important visual tasks such as feature\ncorrespondence, 3D reconstruction, vision-based mapping, localization, and\nnavigation. The data and source code are available at\nhttps://github.com/huangkuns/wireframe.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:54:18 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Huang", "Kun", ""], ["Wang", "Yifan", ""], ["Zhou", "Zihan", ""], ["Ding", "Tianjiao", ""], ["Gao", "Shenghua", ""], ["Ma", "Yi", ""]]}, {"id": "2007.07542", "submitter": "Xiaoyu Yue", "authors": "Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun, and Wayne Zhang", "title": "RobustScanner: Dynamically Enhancing Positional Clues for Robust Text\n  Recognition", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention-based encoder-decoder framework has recently achieved\nimpressive results for scene text recognition, and many variants have emerged\nwith improvements in recognition quality. However, it performs poorly on\ncontextless texts (e.g., random character sequences) which is unacceptable in\nmost of real application scenarios. In this paper, we first deeply investigate\nthe decoding process of the decoder. We empirically find that a representative\ncharacter-level sequence decoder utilizes not only context information but also\npositional information. Contextual information, which the existing approaches\nheavily rely on, causes the problem of attention drift. To suppress such\nside-effect, we propose a novel position enhancement branch, and dynamically\nfuse its outputs with those of the decoder attention module for scene text\nrecognition. Specifically, it contains a position aware module to enable the\nencoder to output feature vectors encoding their own spatial positions, and an\nattention module to estimate glimpses using the positional clue (i.e., the\ncurrent decoding time step) only. The dynamic fusion is conducted for more\nrobust feature via an element-wise gate mechanism. Theoretically, our proposed\nmethod, dubbed \\emph{RobustScanner}, decodes individual characters with dynamic\nratio between context and positional clues, and utilizes more positional ones\nwhen the decoding sequences with scarce context, and thus is robust and\npractical. Empirically, it has achieved new state-of-the-art results on popular\nregular and irregular text recognition benchmarks while without much\nperformance drop on contextless benchmarks, validating its robustness in both\ncontextual and contextless application scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 08:37:40 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 07:16:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yue", "Xiaoyu", ""], ["Kuang", "Zhanghui", ""], ["Lin", "Chenhao", ""], ["Sun", "Hongbin", ""], ["Zhang", "Wayne", ""]]}, {"id": "2007.07547", "submitter": "Joris Voerman", "authors": "Joris Voerman, Aurelie Joseph, Mickael Coustaty, Vincent Poulain d\n  Andecy and Jean-Marc Ogier", "title": "Evaluation of Neural Network Classification Systems on Document Stream", "comments": "15 pages, 3 figures and submitted to DAS conferences 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major drawback of state of the art Neural Networks (NN)-based approaches\nfor document classification purposes is the large number of training samples\nrequired to obtain an efficient classification. The minimum required number is\naround one thousand annotated documents for each class. In many cases it is\nvery difficult, if not impossible, to gather this number of samples in real\nindustrial processes. In this paper, we analyse the efficiency of NN-based\ndocument classification systems in a sub-optimal training case, based on the\nsituation of a company document stream. We evaluated three different\napproaches, one based on image content and two on textual content. The\nevaluation was divided into four parts: a reference case, to assess the\nperformance of the system in the lab; two cases that each simulate a specific\ndifficulty linked to document stream processing; and a realistic case that\ncombined all of these difficulties. The realistic case highlighted the fact\nthat there is a significant drop in the efficiency of NN-Based document\nclassification systems. Although they remain efficient for well represented\nclasses (with an over-fitting of the system for those classes), it is\nimpossible for them to handle appropriately less well represented classes.\nNN-Based document classification systems need to be adapted to resolve these\ntwo problems before they can be considered for use in a company document\nstream.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 08:52:39 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Voerman", "Joris", ""], ["Joseph", "Aurelie", ""], ["Coustaty", "Mickael", ""], ["Andecy", "Vincent Poulain d", ""], ["Ogier", "Jean-Marc", ""]]}, {"id": "2007.07563", "submitter": "Marios Loizou Mr", "authors": "Marios Loizou, Melinos Averkiou, Evangelos Kalogerakis", "title": "Learning Part Boundaries from 3D Point Clouds", "comments": "Appeared in Eurographics Symposium on Geometry Processing 2020", "journal-ref": null, "doi": "10.1111/cgf.14078", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that detects boundaries of parts in 3D shapes represented\nas point clouds. Our method is based on a graph convolutional network\narchitecture that outputs a probability for a point to lie in an area that\nseparates two or more parts in a 3D shape. Our boundary detector is quite\ngeneric: it can be trained to localize boundaries of semantic parts or\ngeometric primitives commonly used in 3D modeling. Our experiments demonstrate\nthat our method can extract more accurate boundaries that are closer to\nground-truth ones compared to alternatives. We also demonstrate an application\nof our network to fine-grained semantic shape segmentation, where we also show\nimprovements in terms of part labeling performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:24:09 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Loizou", "Marios", ""], ["Averkiou", "Melinos", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "2007.07567", "submitter": "Marc Blanchon", "authors": "Marc Blanchon, D\\'esir\\'e Sidib\\'e, Olivier Morel, Ralph Seulin,\n  Daniel Braun and Fabrice Meriaudeau", "title": "P2D: a self-supervised method for depth estimation from polarimetry", "comments": "8 pages, submitted to ICPR2020 second round", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is a recurring subject in the field of computer\nvision. Its ability to describe scenes via a depth map while reducing the\nconstraints related to the formulation of perspective geometry tends to favor\nits use. However, despite the constant improvement of algorithms, most methods\nexploit only colorimetric information. Consequently, robustness to events to\nwhich the modality is not sensitive to, like specularity or transparency, is\nneglected. In response to this phenomenon, we propose using polarimetry as an\ninput for a self-supervised monodepth network. Therefore, we propose exploiting\npolarization cues to encourage accurate reconstruction of scenes. Furthermore,\nwe include a term of polarimetric regularization to state-of-the-art method to\ntake specific advantage of the data. Our method is evaluated both qualitatively\nand quantitatively demonstrating that the contribution of this new information\nas well as an enhanced loss function improves depth estimation results,\nespecially for specular areas.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:32:53 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Blanchon", "Marc", ""], ["Sidib\u00e9", "D\u00e9sir\u00e9", ""], ["Morel", "Olivier", ""], ["Seulin", "Ralph", ""], ["Braun", "Daniel", ""], ["Meriaudeau", "Fabrice", ""]]}, {"id": "2007.07577", "submitter": "Zhongdao Wang", "authors": "Zhongdao Wang, Jingwei Zhang, Liang Zheng, Yixuan Liu, Yifan Sun, Yali\n  Li, Shengjin Wang", "title": "CycAs: Self-supervised Cycle Association for Learning Re-identifiable\n  Descriptions", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a self-supervised learning method for the person\nre-identification (re-ID) problem, where existing unsupervised methods usually\nrely on pseudo labels, such as those from video tracklets or clustering. A\npotential drawback of using pseudo labels is that errors may accumulate and it\nis challenging to estimate the number of pseudo IDs. We introduce a different\nunsupervised method that allows us to learn pedestrian embeddings from raw\nvideos, without resorting to pseudo labels. The goal is to construct a\nself-supervised pretext task that matches the person re-ID objective. Inspired\nby the \\emph{data association} concept in multi-object tracking, we propose the\n\\textbf{Cyc}le \\textbf{As}sociation (\\textbf{CycAs}) task: after performing\ndata association between a pair of video frames forward and then backward, a\npedestrian instance is supposed to be associated to itself. To fulfill this\ngoal, the model must learn a meaningful representation that can well describe\ncorrespondences between instances in frame pairs. We adapt the discrete\nassociation process to a differentiable form, such that end-to-end training\nbecomes feasible. Experiments are conducted in two aspects: We first compare\nour method with existing unsupervised re-ID methods on seven benchmarks and\ndemonstrate CycAs' superiority. Then, to further validate the practical value\nof CycAs in real-world applications, we perform training on self-collected\nvideos and report promising performance on standard test sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:52:35 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wang", "Zhongdao", ""], ["Zhang", "Jingwei", ""], ["Zheng", "Liang", ""], ["Liu", "Yixuan", ""], ["Sun", "Yifan", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "2007.07614", "submitter": "Bo Zhao", "authors": "Baoming Yan, Chen Zhou, Bo Zhao, Kan Guo, Jiang Yang, Xiaobo Li, Ming\n  Zhang, Yizhou Wang", "title": "Augmented Bi-path Network for Few-shot Learning", "comments": null, "journal-ref": "International Conference on Pattern Recognition 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot Learning (FSL) which aims to learn from few labeled training data is\nbecoming a popular research topic, due to the expensive labeling cost in many\nreal-world applications. One kind of successful FSL method learns to compare\nthe testing (query) image and training (support) image by simply concatenating\nthe features of two images and feeding it into the neural network. However,\nwith few labeled data in each class, the neural network has difficulty in\nlearning or comparing the local features of two images. Such simple image-level\ncomparison may cause serious mis-classification. To solve this problem, we\npropose Augmented Bi-path Network (ABNet) for learning to compare both global\nand local features on multi-scales. Specifically, the salient patches are\nextracted and embedded as the local features for every image. Then, the model\nlearns to augment the features for better robustness. Finally, the model learns\nto compare global and local features separately, i.e., in two paths, before\nmerging the similarities. Extensive experiments show that the proposed ABNet\noutperforms the state-of-the-art methods. Both quantitative and visual ablation\nstudies are provided to verify that the proposed modules lead to more precise\ncomparison results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:13:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yan", "Baoming", ""], ["Zhou", "Chen", ""], ["Zhao", "Bo", ""], ["Guo", "Kan", ""], ["Yang", "Jiang", ""], ["Li", "Xiaobo", ""], ["Zhang", "Ming", ""], ["Wang", "Yizhou", ""]]}, {"id": "2007.07617", "submitter": "Ghada Sokar", "authors": "Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "SpaceNet: Make Free Space For Continual Learning", "comments": "Published in Neurocomputing Journal", "journal-ref": "Neurocomputing, 439: 1-11, 2021", "doi": "10.1016/j.neucom.2021.01.078", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continual learning (CL) paradigm aims to enable neural networks to learn\ntasks continually in a sequential fashion. The fundamental challenge in this\nlearning paradigm is catastrophic forgetting previously learned tasks when the\nmodel is optimized for a new task, especially when their data is not\naccessible. Current architectural-based methods aim at alleviating the\ncatastrophic forgetting problem but at the expense of expanding the capacity of\nthe model. Regularization-based methods maintain a fixed model capacity;\nhowever, previous studies showed the huge performance degradation of these\nmethods when the task identity is not available during inference (e.g. class\nincremental learning scenario). In this work, we propose a novel\narchitectural-based method referred as SpaceNet for class incremental learning\nscenario where we utilize the available fixed capacity of the model\nintelligently. SpaceNet trains sparse deep neural networks from scratch in an\nadaptive way that compresses the sparse connections of each task in a compact\nnumber of neurons. The adaptive training of the sparse connections results in\nsparse representations that reduce the interference between the tasks.\nExperimental results show the robustness of our proposed method against\ncatastrophic forgetting old tasks and the efficiency of SpaceNet in utilizing\nthe available capacity of the model, leaving space for more tasks to be\nlearned. In particular, when SpaceNet is tested on the well-known benchmarks\nfor CL: split MNIST, split Fashion-MNIST, and CIFAR-10/100, it outperforms\nregularization-based methods by a big performance gap. Moreover, it achieves\nbetter performance than architectural-based methods without model expansion and\nachieved comparable results with rehearsal-based methods, while offering a huge\nmemory reduction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:21:31 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:09:38 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 08:39:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sokar", "Ghada", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2007.07626", "submitter": "Junwu Weng", "authors": "Junwu Weng and Donghao Luo and Yabiao Wang and Ying Tai and Chengjie\n  Wang and Jilin Li and Feiyue Huang and Xudong Jiang and Junsong Yuan", "title": "Temporal Distinct Representation Learning for Action Recognition", "comments": "16 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the previous success of Two-Dimensional Convolutional Neural\nNetwork (2D CNN) on image recognition, researchers endeavor to leverage it to\ncharacterize videos. However, one limitation of applying 2D CNN to analyze\nvideos is that different frames of a video share the same 2D CNN kernels, which\nmay result in repeated and redundant information utilization, especially in the\nspatial semantics extraction process, hence neglecting the critical variations\namong frames. In this paper, we attempt to tackle this issue through two ways.\n1) Design a sequential channel filtering mechanism, i.e., Progressive\nEnhancement Module (PEM), to excite the discriminative channels of features\nfrom different frames step by step, and thus avoid repeated information\nextraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels\nto concentrate on and capture the variations among frames rather than the image\nregions with similar appearance. Our method is evaluated on benchmark temporal\nreasoning datasets Something-Something V1 and V2, and it achieves visible\nimprovements over the best competitor by 2.4% and 1.3%, respectively. Besides,\nperformance improvements over the 2D-CNN-based state-of-the-arts on the\nlarge-scale dataset Kinetics are also witnessed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:30:40 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Weng", "Junwu", ""], ["Luo", "Donghao", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Jiang", "Xudong", ""], ["Yuan", "Junsong", ""]]}, {"id": "2007.07627", "submitter": "Bailin Deng", "authors": "Juyong Zhang and Yuxin Yao and Bailin Deng", "title": "Fast and Robust Iterative Closest Point", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3054619", "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Iterative Closest Point (ICP) algorithm and its variants are a\nfundamental technique for rigid registration between two point sets, with wide\napplications in different areas from robotics to 3D reconstruction. The main\ndrawbacks for ICP are its slow convergence as well as its sensitivity to\noutliers, missing data, and partial overlaps. Recent work such as Sparse ICP\nachieves robustness via sparsity optimization at the cost of computational\nspeed. In this paper, we propose a new method for robust registration with fast\nconvergence. First, we show that the classical point-to-point ICP can be\ntreated as a majorization-minimization (MM) algorithm, and propose an Anderson\nacceleration approach to speed up its convergence. In addition, we introduce a\nrobust error metric based on the Welsch's function, which is minimized\nefficiently using the MM algorithm with Anderson acceleration. On challenging\ndatasets with noises and partial overlaps, we achieve similar or better\naccuracy than Sparse ICP while being at least an order of magnitude faster.\nFinally, we extend the robust formulation to point-to-plane ICP, and solve the\nresulting problem using a similar Anderson-accelerated MM strategy. Our robust\nICP methods improve the registration accuracy on benchmark datasets while being\ncompetitive in computational time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:32:53 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 14:24:22 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhang", "Juyong", ""], ["Yao", "Yuxin", ""], ["Deng", "Bailin", ""]]}, {"id": "2007.07628", "submitter": "Adri\\'an Csisz\\'arik", "authors": "R\\'obert Szab\\'o, D\\'aniel Katona, M\\'arton Csillag, Adri\\'an\n  Csisz\\'arik, D\\'aniel Varga", "title": "Visualizing Transfer Learning", "comments": "2020 ICML Workshop on Human Interpretability in Machine Learning (WHI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide visualizations of individual neurons of a deep image recognition\nnetwork during the temporal process of transfer learning. These visualizations\nqualitatively demonstrate various novel properties of the transfer learning\nprocess regarding the speed and characteristics of adaptation, neuron reuse,\nspatial scale of the represented image features, and behavior of transfer\nlearning to small data. We publish the large-scale dataset that we have created\nfor the purposes of this analysis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:34:46 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Szab\u00f3", "R\u00f3bert", ""], ["Katona", "D\u00e1niel", ""], ["Csillag", "M\u00e1rton", ""], ["Csisz\u00e1rik", "Adri\u00e1n", ""], ["Varga", "D\u00e1niel", ""]]}, {"id": "2007.07630", "submitter": "Jongseok Lee", "authors": "Kashmira Shinde, Jongseok Lee, Matthias Humt, Aydin Sezgin, Rudolph\n  Triebel", "title": "Learning Multiplicative Interactions with Bayesian Neural Networks for\n  Visual-Inertial Odometry", "comments": "Published at Workshop on AI for Autonomous Driving (AIAD), the 37th\n  International Conference on Machine Learning, Vienna, Austria, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an end-to-end multi-modal learning approach for monocular\nVisual-Inertial Odometry (VIO), which is specifically designed to exploit\nsensor complementarity in the light of sensor degradation scenarios. The\nproposed network makes use of a multi-head self-attention mechanism that learns\nmultiplicative interactions between multiple streams of information. Another\ndesign feature of our approach is the incorporation of the model uncertainty\nusing scalable Laplace Approximation. We evaluate the performance of the\nproposed approach by comparing it against the end-to-end state-of-the-art\nmethods on the KITTI dataset and show that it achieves superior performance.\nImportantly, our work thereby provides an empirical evidence that learning\nmultiplicative interactions can result in a powerful inductive bias for\nincreased robustness to sensor failures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:39:29 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Shinde", "Kashmira", ""], ["Lee", "Jongseok", ""], ["Humt", "Matthias", ""], ["Sezgin", "Aydin", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2007.07645", "submitter": "Yingjun Du", "authors": "Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M.\n  Snoek, Ling Shao", "title": "Learning to Learn with Variational Information Bottleneck for Domain\n  Generalization", "comments": "15 pages, 4 figures, ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization models learn to generalize to previously unseen\ndomains, but suffer from prediction uncertainty and domain shift. In this\npaper, we address both problems. We introduce a probabilistic meta-learning\nmodel for domain generalization, in which classifier parameters shared across\ndomains are modeled as distributions. This enables better handling of\nprediction uncertainty on unseen domains. To deal with domain shift, we learn\ndomain-invariant representations by the proposed principle of meta variational\ninformation bottleneck, we call MetaVIB. MetaVIB is derived from novel\nvariational bounds of mutual information, by leveraging the meta-learning\nsetting of domain generalization. Through episodic training, MetaVIB learns to\ngradually narrow domain gaps to establish domain-invariant representations,\nwhile simultaneously maximizing prediction accuracy. We conduct experiments on\nthree benchmarks for cross-domain visual recognition. Comprehensive ablation\nstudies validate the benefits of MetaVIB for domain generalization. The\ncomparison results demonstrate our method outperforms previous approaches\nconsistently.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 12:05:52 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Du", "Yingjun", ""], ["Xu", "Jun", ""], ["Xiong", "Huan", ""], ["Qiu", "Qiang", ""], ["Zhen", "Xiantong", ""], ["Snoek", "Cees G. M.", ""], ["Shao", "Ling", ""]]}, {"id": "2007.07676", "submitter": "Domen Tabernik", "authors": "Jakob Bo\\v{z}i\\v{c}, Domen Tabernik and Danijel Sko\\v{c}aj", "title": "End-to-end training of a two-stage neural network for defect detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation-based, two-stage neural network has shown excellent results in\nthe surface defect detection, enabling the network to learn from a relatively\nsmall number of samples. In this work, we introduce end-to-end training of the\ntwo-stage network together with several extensions to the training process,\nwhich reduce the amount of training time and improve the results on the surface\ndefect detection tasks. To enable end-to-end training we carefully balance the\ncontributions of both the segmentation and the classification loss throughout\nthe learning. We adjust the gradient flow from the classification into the\nsegmentation network in order to prevent the unstable features from corrupting\nthe learning. As an additional extension to the learning, we propose\nfrequency-of-use sampling scheme of negative samples to address the issue of\nover- and under-sampling of images during the training, while we employ the\ndistance transform algorithm on the region-based segmentation masks as weights\nfor positive pixels, giving greater importance to areas with higher probability\nof presence of defect without requiring a detailed annotation. We demonstrate\nthe performance of the end-to-end training scheme and the proposed extensions\non three defect detection datasets - DAGM, KolektorSDD and Severstal Steel\ndefect dataset - where we show state-of-the-art results. On the DAGM and the\nKolektorSDD we demonstrate 100\\% detection rate, therefore completely solving\nthe datasets. Additional ablation study performed on all three datasets\nquantitatively demonstrates the contribution to the overall result improvements\nfor each of the proposed extensions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:42:26 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Bo\u017ei\u010d", "Jakob", ""], ["Tabernik", "Domen", ""], ["Sko\u010daj", "Danijel", ""]]}, {"id": "2007.07686", "submitter": "Bo Li", "authors": "Bo Li, Evgeniy Martyushev, Gim Hee Lee", "title": "Relative Pose Estimation of Calibrated Cameras with Known\n  $\\mathrm{SE}(3)$ Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathrm{SE}(3)$ invariants of a pose include its rotation angle and\nscrew translation. In this paper, we present a complete comprehensive study of\nthe relative pose estimation problem for a calibrated camera constrained by\nknown $\\mathrm{SE}(3)$ invariant, which involves 5 minimal problems in total.\nThese problems reduces the minimal number of point pairs for relative pose\nestimation and improves the estimation efficiency and robustness. The\n$\\mathrm{SE}(3)$ invariant constraints can come from extra sensor measurements\nor motion assumption. Different from conventional relative pose estimation with\nextra constraints, no extrinsic calibration is required to transform the\nconstraints to the camera frame. This advantage comes from the invariance of\n$\\mathrm{SE}(3)$ invariants cross different coordinate systems on a rigid body\nand makes the solvers more convenient and flexible in practical applications.\n  Besides proposing the concept of relative pose estimation constrained by\n$\\mathrm{SE}(3)$ invariants, we present a comprehensive study of existing\npolynomial formulations for relative pose estimation and discover their\nrelationship. Different formulations are carefully chosen for each proposed\nproblems to achieve best efficiency. Experiments on synthetic and real data\nshows performance improvement compared to conventional relative pose estimation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:55:55 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Li", "Bo", ""], ["Martyushev", "Evgeniy", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2007.07690", "submitter": "Vincent Christlein", "authors": "Vincent Christlein, Nikolaus Weichselbaumer, Saskia Limbach, Mathias\n  Seuret", "title": "Proof of Concept: Automatic Type Recognition", "comments": "InfDH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The type used to print an early modern book can give scholars valuable\ninformation about the time and place of its production as well as its producer.\nRecognizing such type is currently done manually using both the character\nshapes of `M' or `Qu' and the size of the total type to look it up in a large\nreference work. This is a reliable method, but it is also slow and requires\nspecific skills. We investigate the performance of type classification and type\nretrieval using a newly created dataset consisting of easy and difficult types\nused in early printed books. For type classification, we rely on a deep\nConvolutional Neural Network (CNN) originally used for font-group\nclassification while we use a common writer identification method for the\nretrieval case. We show that in both scenarios, easy types can be\nclassified/retrieved with a high accuracy while difficult cases are indeed\ndifficult.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:58:27 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 11:46:11 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Christlein", "Vincent", ""], ["Weichselbaumer", "Nikolaus", ""], ["Limbach", "Saskia", ""], ["Seuret", "Mathias", ""]]}, {"id": "2007.07696", "submitter": "Zehao Yu", "authors": "Zehao Yu, Lei Jin, and Shenghua Gao", "title": "P$^{2}$Net: Patch-match and Plane-regularization for Unsupervised Indoor\n  Depth Estimation", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the unsupervised depth estimation task in indoor\nenvironments. The task is extremely challenging because of the vast areas of\nnon-texture regions in these scenes. These areas could overwhelm the\noptimization process in the commonly used unsupervised depth estimation\nframework proposed for outdoor environments. However, even when those regions\nare masked out, the performance is still unsatisfactory. In this paper, we\nargue that the poor performance suffers from the non-discriminative point-based\nmatching. To this end, we propose P$^2$Net. We first extract points with large\nlocal gradients and adopt patches centered at each point as its representation.\nMultiview consistency loss is then defined over patches. This operation\nsignificantly improves the robustness of the network training. Furthermore,\nbecause those textureless regions in indoor scenes (e.g., wall, floor, roof,\n\\etc) usually correspond to planar regions, we propose to leverage superpixels\nas a plane prior. We enforce the predicted depth to be well fitted by a plane\nwithin each superpixel. Extensive experiments on NYUv2 and ScanNet show that\nour P$^2$Net outperforms existing approaches by a large margin. Code is\navailable at \\url{https://github.com/svip-lab/Indoor-SfMLearner}.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:10:43 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yu", "Zehao", ""], ["Jin", "Lei", ""], ["Gao", "Shenghua", ""]]}, {"id": "2007.07702", "submitter": "Lena Downes", "authors": "Lena M. Downes, Ted J. Steiner, Jonathan P. How", "title": "Lunar Terrain Relative Navigation Using a Convolutional Neural Network\n  for Visual Crater Detection", "comments": "6 pages, 4 figures. This work was accepted by the 2020 American\n  Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrain relative navigation can improve the precision of a spacecraft's\nposition estimate by detecting global features that act as supplementary\nmeasurements to correct for drift in the inertial navigation system. This paper\npresents a system that uses a convolutional neural network (CNN) and image\nprocessing methods to track the location of a simulated spacecraft with an\nextended Kalman filter (EKF). The CNN, called LunaNet, visually detects craters\nin the simulated camera frame and those detections are matched to known lunar\ncraters in the region of the current estimated spacecraft position. These\nmatched craters are treated as features that are tracked using the EKF. LunaNet\nenables more reliable position tracking over a simulated trajectory due to its\ngreater robustness to changes in image brightness and more repeatable crater\ndetections from frame to frame throughout a trajectory. LunaNet combined with\nan EKF produces a decrease of 60% in the average final position estimation\nerror and a decrease of 25% in average final velocity estimation error compared\nto an EKF using an image processing-based crater detection method when tested\non trajectories using images of standard brightness.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:19:27 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Downes", "Lena M.", ""], ["Steiner", "Ted J.", ""], ["How", "Jonathan P.", ""]]}, {"id": "2007.07714", "submitter": "Qingshan Xu", "authors": "Qingshan Xu and Wenbing Tao", "title": "PVSNet: Pixelwise Visibility-Aware Multi-View Stereo Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning-based multi-view stereo methods have achieved promising\nresults. However, they all overlook the visibility difference among different\nviews, which leads to an indiscriminate multi-view similarity definition and\ngreatly limits their performance on datasets with strong viewpoint variations.\nIn this paper, a Pixelwise Visibility-aware multi-view Stereo Network (PVSNet)\nis proposed for robust dense 3D reconstruction. We present a pixelwise\nvisibility network to learn the visibility information for different\nneighboring images before computing the multi-view similarity, and then\nconstruct an adaptive weighted cost volume with the visibility information.\nMoreover, we present an anti-noise training strategy that introduces disturbing\nviews during model training to make the pixelwise visibility network more\ndistinguishable to unrelated views, which is different with the existing\nlearning methods that only use two best neighboring views for training. To the\nbest of our knowledge, PVSNet is the first deep learning framework that is able\nto capture the visibility information of different neighboring views. In this\nway, our method can be generalized well to different types of datasets,\nespecially the ETH3D high-res benchmark with strong viewpoint variations.\nExtensive experiments show that PVSNet achieves the state-of-the-art\nperformance on different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:39:49 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Xu", "Qingshan", ""], ["Tao", "Wenbing", ""]]}, {"id": "2007.07723", "submitter": "Moab Arar", "authors": "Moab Arar, Noa Fish, Dani Daniel, Evgeny Tenetov, Ariel Shamir, Amit\n  Bermano", "title": "Focus-and-Expand: Training Guidance Through Gradual Manipulation of\n  Input Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and intuitive Focus-and-eXpand (\\fax) method to guide the\ntraining process of a neural network towards a specific solution. Optimizing a\nneural network is a highly non-convex problem. Typically, the space of\nsolutions is large, with numerous possible local minima, where reaching a\nspecific minimum depends on many factors. In many cases, however, a solution\nwhich considers specific aspects, or features, of the input is desired. For\nexample, in the presence of bias, a solution that disregards the biased feature\nis a more robust and accurate one. Drawing inspiration from Parameter\nContinuation methods, we propose steering the training process to consider\nspecific features in the input more than others, through gradual shifts in the\ninput domain. \\fax extracts a subset of features from each input data-point,\nand exposes the learner to these features first, Focusing the solution on them.\nThen, by using a blending/mixing parameter $\\alpha$ it gradually eXpands the\nlearning process to include all features of the input. This process encourages\nthe consideration of the desired features more than others. Though not\nrestricted to this field, we quantitatively evaluate the effectiveness of our\napproach on various Computer Vision tasks, and achieve state-of-the-art bias\nremoval, improvements to an established augmentation method, and two examples\nof improvements to image classification tasks. Through these few examples we\ndemonstrate the impact this approach potentially carries for a wide variety of\nproblems, which stand to gain from understanding the solution landscape.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:49:56 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Arar", "Moab", ""], ["Fish", "Noa", ""], ["Daniel", "Dani", ""], ["Tenetov", "Evgeny", ""], ["Shamir", "Ariel", ""], ["Bermano", "Amit", ""]]}, {"id": "2007.07729", "submitter": "Yimian Dai", "authors": "Yimian Dai and Stefan Oehmcke and Fabian Gieseke and Yiquan Wu and\n  Kobus Barnard", "title": "Attention as Activation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions and attention mechanisms are typically treated as having\ndifferent purposes and have evolved differently. However, both concepts can be\nformulated as a non-linear gating function. Inspired by their similarity, we\npropose a novel type of activation units called attentional activation (ATAC)\nunits as a unification of activation functions and attention mechanisms. In\nparticular, we propose a local channel attention module for the simultaneous\nnon-linear activation and element-wise feature refinement, which locally\naggregates point-wise cross-channel feature contexts. By replacing the\nwell-known rectified linear units by such ATAC units in convolutional networks,\nwe can construct fully attentional networks that perform significantly better\nwith a modest number of additional parameters. We conducted detailed ablation\nstudies on the ATAC units using several host networks with varying network\ndepths to empirically verify the effectiveness and efficiency of the units.\nFurthermore, we compared the performance of the ATAC units against existing\nactivation functions as well as other attention mechanisms on the CIFAR-10,\nCIFAR-100, and ImageNet datasets. Our experimental results show that networks\nconstructed with the proposed ATAC units generally yield performance gains over\ntheir competitors given a comparable number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:52:29 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 09:40:56 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dai", "Yimian", ""], ["Oehmcke", "Stefan", ""], ["Gieseke", "Fabian", ""], ["Wu", "Yiquan", ""], ["Barnard", "Kobus", ""]]}, {"id": "2007.07743", "submitter": "Theo Costain", "authors": "Marcelo Gennari do Nascimento, Theo W. Costain, Victor Adrian\n  Prisacariu", "title": "Finding Non-Uniform Quantization Schemes using Multi-Task Gaussian\n  Processes", "comments": "Accepted for publication at ECCV 2020. Code availiable at\n  https://code.active.vision . Updated for typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for neural network quantization that casts the\nneural architecture search problem as one of hyperparameter search to find\nnon-uniform bit distributions throughout the layers of a CNN. We perform the\nsearch assuming a Multi-Task Gaussian Processes prior, which splits the problem\nto multiple tasks, each corresponding to different number of training epochs,\nand explore the space by sampling those configurations that yield maximum\ninformation. We then show that with significantly lower precision in the last\nlayers we achieve a minimal loss of accuracy with appreciable memory savings.\nWe test our findings on the CIFAR10 and ImageNet datasets using the VGG, ResNet\nand GoogLeNet architectures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:16:18 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 09:46:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Nascimento", "Marcelo Gennari do", ""], ["Costain", "Theo W.", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2007.07757", "submitter": "Shivam Chandhok", "authors": "Shivam Chandhok and Vineeth N Balasubramanian", "title": "Two-Level Adversarial Visual-Semantic Coupling for Generalized Zero-shot\n  Learning", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of generative zero-shot methods mainly depends on the quality\nof generated features and how well the model facilitates knowledge transfer\nbetween visual and semantic domains. The quality of generated features is a\ndirect consequence of the ability of the model to capture the several modes of\nthe underlying data distribution. To address these issues, we propose a new\ntwo-level joint maximization idea to augment the generative network with an\ninference network during training which helps our model capture the several\nmodes of the data and generate features that better represent the underlying\ndata distribution. This provides strong cross-modal interaction for effective\ntransfer of knowledge between visual and semantic domains. Furthermore,\nexisting methods train the zero-shot classifier either on generate synthetic\nimage features or latent embeddings produced by leveraging representation\nlearning. In this work, we unify these paradigms into a single model which in\naddition to synthesizing image features, also utilizes the representation\nlearning capabilities of the inference network to provide discriminative\nfeatures for the final zero-shot recognition task. We evaluate our approach on\nfour benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several\nstate-of-the-art methods, and show its performance. We also perform ablation\nstudies to analyze and understand our method more carefully for the Generalized\nZero-shot Learning task.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:34:09 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 11:00:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chandhok", "Shivam", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2007.07761", "submitter": "Siladittya Manna", "authors": "Siladittya Manna, Saumik Bhattacharya, Umapada Pal", "title": "Self-Supervised Representation Learning for Detection of ACL Tear Injury\n  in Knee MR Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning based models for computer vision applications\nrequires large scale human annotated data which are often expensive to\ngenerate. Self-supervised learning, a subset of unsupervised learning, handles\nthis problem by learning meaningful features from unlabeled image or video\ndata. In this paper, we propose a self-supervised learning approach to learn\ntransferable features from MR video clips by enforcing the model to learn\nanatomical features. The pretext task models are designed to predict the\ncorrect ordering of the jumbled image patches that the MR video frames are\ndivided into. To the best of our knowledge, none of the supervised learning\nmodels performing injury classification task from MR video provide any\nexplanation for the decisions made by the models and hence makes our work the\nfirst of its kind on MR video data. Experiments on the pretext task show that\nthis proposed approach enables the model to learn spatial context invariant\nfeatures which help for reliable and explainable performance in downstream\ntasks like classification of Anterior Cruciate Ligament tear injury from knee\nMRI. The efficiency of the novel Convolutional Neural Network proposed in this\npaper is reflected in the experimental results obtained in the downstream task.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:35:47 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 03:06:02 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 12:27:43 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Manna", "Siladittya", ""], ["Bhattacharya", "Saumik", ""], ["Pal", "Umapada", ""]]}, {"id": "2007.07773", "submitter": "Yong Chen", "authors": "Yong Chen, Lu Wang, Jiajia Hu, Mingbin Ye", "title": "Vision-Based Fall Event Detection in Complex Background Using Attention\n  Guided Bi-directional LSTM", "comments": "We have added a lot of experimental data and replaced all the\n  pictures. The previous conclusions have undergone a lot of changes. This\n  paper has almost no practical value", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall event detection, as one of the greatest risks to the elderly, has been a\nhot research issue in the solitary scene in recent years. Nevertheless, there\nare few researches on the fall event detection in complex background. Different\nfrom most conventional background subtraction methods which depend on\nbackground modeling, Mask R-CNN method based on deep learning technique can\nclearly extract the moving object in noise background. We further propose an\nattention guided Bi-directional LSTM model for the final fall event detection.\nTo demonstrate the efficiency, the proposed method is verified in the public\ndataset and self-build dataset. Evaluation of the algorithm performances in\ncomparison with other state-of-the-art methods indicates that the proposed\ndesign is accurate and robust, which means it is suitable for the task of fall\nevent detection in complex situation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:19:21 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 05:46:24 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chen", "Yong", ""], ["Wang", "Lu", ""], ["Hu", "Jiajia", ""], ["Ye", "Mingbin", ""]]}, {"id": "2007.07788", "submitter": "Zhihua Liu", "authors": "Zhihua Liu, Lei Tong, Long Chen, Feixiang Zhou, Zheheng Jiang, Qianni\n  Zhang, Yinhai Wang, Caifeng Shan, Ling Li, Huiyu Zhou", "title": "CANet: Context Aware Network for 3D Brain Glioma Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated segmentation of brain glioma plays an active role in diagnosis\ndecision, progression monitoring and surgery planning. Based on deep neural\nnetworks, previous studies have shown promising technologies for brain glioma\nsegmentation. However, these approaches lack powerful strategies to incorporate\ncontextual information of tumor cells and their surrounding, which has been\nproven as a fundamental cue to deal with local ambiguity. In this work, we\npropose a novel approach named Context-Aware Network (CANet) for brain glioma\nsegmentation. CANet captures high dimensional and discriminative features with\ncontexts from both the convolutional space and feature interaction graphs. We\nfurther propose context guided attentive conditional random fields which can\nselectively aggregate features. We evaluate our method using publicly\naccessible brain glioma segmentation datasets BRATS2017, BRATS2018 and\nBRATS2019. The experimental results show that the proposed algorithm has better\nor competitive performance against several State-of-The-Art approaches under\ndifferent segmentation metrics on the training and validation sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:12:41 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 19:58:26 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 10:03:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Zhihua", ""], ["Tong", "Lei", ""], ["Chen", "Long", ""], ["Zhou", "Feixiang", ""], ["Jiang", "Zheheng", ""], ["Zhang", "Qianni", ""], ["Wang", "Yinhai", ""], ["Shan", "Caifeng", ""], ["Li", "Ling", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2007.07805", "submitter": "Byeongjo Kim", "authors": "Byeongjo Kim, Chanran Kim, Jaehoon Lee, Jein Song, Gyoungsoo Park", "title": "Data-Efficient Deep Learning Method for Image Classification Using Data\n  Augmentation, Focal Cosine Loss, and Ensemble", "comments": "7 pages, 2 figures, technical report of 1st Visual Inductive Priors\n  for Data-Efficient Deep Learning Workshop Challenge in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In general, sufficient data is essential for the better performance and\ngeneralization of deep-learning models. However, lots of limitations(cost,\nresources, etc.) of data collection leads to lack of enough data in most of the\nareas. In addition, various domains of each data sources and licenses also lead\nto difficulties in collection of sufficient data. This situation makes us hard\nto utilize not only the pre-trained model, but also the external knowledge.\nTherefore, it is important to leverage small dataset effectively for achieving\nthe better performance. We applied some techniques in three aspects: data, loss\nfunction, and prediction to enable training from scratch with less data. With\nthese methods, we obtain high accuracy by leveraging ImageNet data which\nconsist of only 50 images per class. Furthermore, our model is ranked 4th in\nVisual Inductive Printers for Data-Effective Computer Vision Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:30:57 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Kim", "Byeongjo", ""], ["Kim", "Chanran", ""], ["Lee", "Jaehoon", ""], ["Song", "Jein", ""], ["Park", "Gyoungsoo", ""]]}, {"id": "2007.07817", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Edward Curry", "title": "VidCEP: Complex Event Processing Framework to Detect Spatiotemporal\n  Patterns in Video Streams", "comments": "10 pages, 19 figures, Paper published in IEEE BigData 2019", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9006018", "report-no": null, "categories": "cs.CV cs.DB cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video data is highly expressive and has traditionally been very difficult for\na machine to interpret. Querying event patterns from video streams is\nchallenging due to its unstructured representation. Middleware systems such as\nComplex Event Processing (CEP) mine patterns from data streams and send\nnotifications to users in a timely fashion. Current CEP systems have inherent\nlimitations to query video streams due to their unstructured data model and\nlack of expressive query language. In this work, we focus on a CEP framework\nwhere users can define high-level expressive queries over videos to detect a\nrange of spatiotemporal event patterns. In this context, we propose: i) VidCEP,\nan in-memory, on the fly, near real-time complex event matching framework for\nvideo streams. The system uses a graph-based event representation for video\nstreams which enables the detection of high-level semantic concepts from video\nusing cascades of Deep Neural Network models, ii) a Video Event Query language\n(VEQL) to express high-level user queries for video streams in CEP, iii) a\ncomplex event matcher to detect spatiotemporal video event patterns by matching\nexpressive user queries over video data. The proposed approach detects\nspatiotemporal video event patterns with an F-score ranging from 0.66 to 0.89.\nVidCEP maintains near real-time performance with an average throughput of 70\nframes per second for 5 parallel videos with sub-second matching latency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:43:37 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yadav", "Piyush", ""], ["Curry", "Edward", ""]]}, {"id": "2007.07843", "submitter": "Yiwei Lu", "authors": "Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy and Yang Wang", "title": "Few-shot Scene-adaptive Anomaly Detection", "comments": "Accepted to ECCV 2020 as a spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of anomaly detection in videos. The goal is to\nidentify unusual behaviours automatically by learning exclusively from normal\nvideos. Most existing approaches are usually data-hungry and have limited\ngeneralization abilities. They usually need to be trained on a large number of\nvideos from a target scene to achieve good results in that scene. In this\npaper, we propose a novel few-shot scene-adaptive anomaly detection problem to\naddress the limitations of previous approaches. Our goal is to learn to detect\nanomalies in a previously unseen scene with only a few frames. A reliable\nsolution for this new problem will have huge potential in real-world\napplications since it is expensive to collect a massive amount of data for each\ntarget scene. We propose a meta-learning based approach for solving this new\nproblem; extensive experimental results demonstrate the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:08:46 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lu", "Yiwei", ""], ["Yu", "Frank", ""], ["Reddy", "Mahesh Kumar Krishna", ""], ["Wang", "Yang", ""]]}, {"id": "2007.07867", "submitter": "Aamir Mustafa", "authors": "Aamir Mustafa and Rafal K. Mantiuk", "title": "Transformation Consistency Regularization- A Semi-Supervised Paradigm\n  for Image-to-Image Translation", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of labeled data has motivated the development of semi-supervised\nlearning methods, which learn from large portions of unlabeled data alongside a\nfew labeled samples. Consistency Regularization between model's predictions\nunder different input perturbations, particularly has shown to provide\nstate-of-the art results in a semi-supervised framework. However, most of these\nmethod have been limited to classification and segmentation applications. We\npropose Transformation Consistency Regularization, which delves into a more\nchallenging setting of image-to-image translation, which remains unexplored by\nsemi-supervised algorithms. The method introduces a diverse set of geometric\ntransformations and enforces the model's predictions for unlabeled data to be\ninvariant to those transformations. We evaluate the efficacy of our algorithm\non three different applications: image colorization, denoising and\nsuper-resolution. Our method is significantly data efficient, requiring only\naround 10 - 20% of labeled samples to achieve similar image reconstructions to\nits fully-supervised counterpart. Furthermore, we show the effectiveness of our\nmethod in video processing applications, where knowledge from a few frames can\nbe leveraged to enhance the quality of the rest of the movie.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:41:35 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Mustafa", "Aamir", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "2007.07869", "submitter": "Paul Micaelli", "authors": "Paul Micaelli and Amos Storkey", "title": "Non-greedy Gradient-based Hyperparameter Optimization Over Long Horizons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based hyperparameter optimization is an attractive way to perform\nmeta-learning across a distribution of tasks, or improve the performance of an\noptimizer on a single task. However, this approach has been unpopular for tasks\nrequiring long horizons (many gradient steps), due to memory scaling and\ngradient degradation issues. A common workaround is to learn hyperparameters\nonline or split the horizon into smaller chunks. However, this introduces\ngreediness which comes with a large performance drop, since the best local\nhyperparameters can make for poor global solutions. In this work, we enable\nnon-greediness over long horizons with a two-fold solution. First, we share\nhyperparameters that are contiguous in time, and show that this drastically\nmitigates gradient degradation issues. Then, we derive a forward-mode\ndifferentiation algorithm for the popular momentum-based SGD optimizer, which\nallows for a memory cost that is constant with horizon size. When put together,\nthese solutions allow us to learn hyperparameters without any prior knowledge.\nCompared to the baseline of hand-tuned off-the-shelf hyperparameters, our\nmethod compares favorably on simple datasets like SVHN. On CIFAR-10 we match\nthe baseline performance, and demonstrate for the first time that learning\nrate, momentum and weight decay schedules can be learned with gradients on a\ndataset of this size. Code is available at\nhttps://github.com/polo5/NonGreedyGradientHPO\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:44:07 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Micaelli", "Paul", ""], ["Storkey", "Amos", ""]]}, {"id": "2007.07875", "submitter": "Xingyang Ni", "authors": "Xingyang Ni, Liang Fang, Heikki Huttunen", "title": "Adaptive L2 Regularization in Person Re-Identification", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an adaptive L2 regularization mechanism in the setting of person\nre-identification. In the literature, it is common practice to utilize\nhand-picked regularization factors which remain constant throughout the\ntraining procedure. Unlike existing approaches, the regularization factors in\nour proposed method are updated adaptively through backpropagation. This is\nachieved by incorporating trainable scalar variables as the regularization\nfactors, which are further fed into a scaled hard sigmoid function. Extensive\nexperiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the\neffectiveness of our framework. Most notably, we obtain state-of-the-art\nperformance on MSMT17, which is the largest dataset for person\nre-identification. Source code is publicly available at\nhttps://github.com/nixingyang/AdaptiveL2Regularization.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:50:34 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 15:26:19 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ni", "Xingyang", ""], ["Fang", "Liang", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2007.07922", "submitter": "Manu Goyal", "authors": "Manu Goyal and Saeed Hassanpour", "title": "A Refined Deep Learning Architecture for Diabetic Foot Ulcers Detection", "comments": "8 Pages and DFUC Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Foot Ulcers (DFU) that affect the lower extremities are a major\ncomplication of diabetes. Each year, more than 1 million diabetic patients\nundergo amputation due to failure to recognize DFU and get the proper treatment\nfrom clinicians. There is an urgent need to use a CAD system for the detection\nof DFU. In this paper, we propose using deep learning methods (EfficientDet\nArchitectures) for the detection of DFU in the DFUC2020 challenge dataset,\nwhich consists of 4,500 DFU images. We further refined the EfficientDet\narchitecture to avoid false negative and false positive predictions. The code\nfor this method is available at\nhttps://github.com/Manugoyal12345/Yet-Another-EfficientDet-Pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:06:53 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Goyal", "Manu", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2007.07923", "submitter": "Kalliopi Basioti", "authors": "Kalliopi Basioti, George V. Moustakides", "title": "Image De-Quantization Using Generative Models as Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quantization is used in several applications aiming in reducing the\nnumber of available colors in an image and therefore its size. De-quantization\nis the task of reversing the quantization effect and recovering the original\nmulti-chromatic level image. Existing techniques achieve de-quantization by\nimposing suitable constraints on the ideal image in order to make the recovery\nproblem feasible since it is otherwise ill-posed. Our goal in this work is to\ndevelop a de-quantization mechanism through a rigorous mathematical analysis\nwhich is based on the classical statistical estimation theory. In this effort\nwe incorporate generative modeling of the ideal image as a suitable prior\ninformation. The resulting technique is simple and capable of de-quantizing\nsuccessfully images that have experienced severe quantization effects.\nInterestingly, our method can recover images even if the quantization process\nis not exactly known and contains unknown parameters.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:09:00 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 21:40:45 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Basioti", "Kalliopi", ""], ["Moustakides", "George V.", ""]]}, {"id": "2007.07924", "submitter": "Abubakar Siddique", "authors": "Abubakar Siddique and Henry Medeiros", "title": "Tracking Passengers and Baggage Items using Multi-camera Systems at\n  Security Checkpoints", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel tracking-by-detection framework to track multiple\nobjects in overhead camera videos for airport checkpoint security scenarios\nwhere targets correspond to passengers and their baggage items. Our approach\nimproves object detection by employing a test-time data augmentation procedure\nthat provides multiple geometrically transformed images as inputs to a\nconvolutional neural network. We cluster the multiple detections generated by\nthe network using the mean-shift algorithm. The multiple hypothesis tracking\nalgorithm then keeps track of the temporal identifiers of the targets based on\nthe cluster centroids. Our method also incorporates a trajectory association\nmechanism to maintain the consistency of the temporal identifiers as passengers\ntravel across camera views. Finally, we also introduce a simple distance-based\nmatching mechanism to associate passengers with their luggage. An evaluation of\ndetection, tracking, and association performances on videos obtained from\nmultiple overhead cameras in a realistic airport checkpoint environment\ndemonstrates the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:09:31 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Siddique", "Abubakar", ""], ["Medeiros", "Henry", ""]]}, {"id": "2007.07925", "submitter": "Jonghwa Yim", "authors": "Jonghwa Yim, Jisung Yoo, Won-joon Do, Beomsu Kim, Jihwan Choe", "title": "Filter Style Transfer between Photos", "comments": "ECCV (Spotlight) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Over the past few years, image-to-image style transfer has risen to the\nfrontiers of neural image processing. While conventional methods were\nsuccessful in various tasks such as color and texture transfer between images,\nnone could effectively work with the custom filter effects that are applied by\nusers through various platforms like Instagram. In this paper, we introduce a\nnew concept of style transfer, Filter Style Transfer (FST). Unlike conventional\nstyle transfer, new technique FST can extract and transfer custom filter style\nfrom a filtered style image to a content image. FST first infers the original\nimage from a filtered reference via image-to-image translation. Then it\nestimates filter parameters from the difference between them. To resolve the\nill-posed nature of reconstructing the original image from the reference, we\nrepresent each pixel color of an image to class mean and deviation. Besides, to\nhandle the intra-class color variation, we propose an uncertainty based\nweighted least square method for restoring an original image. To the best of\nour knowledge, FST is the first style transfer method that can transfer custom\nfilter effects between FHD image under 2ms on a mobile device without any\ntextual context loss.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:09:35 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Yim", "Jonghwa", ""], ["Yoo", "Jisung", ""], ["Do", "Won-joon", ""], ["Kim", "Beomsu", ""], ["Choe", "Jihwan", ""]]}, {"id": "2007.07936", "submitter": "Wilhelm Tranheden", "authors": "Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, Lennart Svensson", "title": "ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised\n  Learning", "comments": "This paper has been accepted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art in semantic segmentation is steadily increasing in\nperformance, resulting in more precise and reliable segmentations in many\ndifferent applications. However, progress is limited by the cost of generating\nlabels for training, which sometimes requires hours of manual labor for a\nsingle image. Because of this, semi-supervised methods have been applied to\nthis task, with varying degrees of success. A key challenge is that common\naugmentations used in semi-supervised classification are less effective for\nsemantic segmentation. We propose a novel data augmentation mechanism called\nClassMix, which generates augmentations by mixing unlabelled samples, by\nleveraging on the network's predictions for respecting object boundaries. We\nevaluate this augmentation technique on two common semi-supervised semantic\nsegmentation benchmarks, showing that it attains state-of-the-art results.\nLastly, we also provide extensive ablation studies comparing different design\ndecisions and training regimes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:21:17 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 11:14:07 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Olsson", "Viktor", ""], ["Tranheden", "Wilhelm", ""], ["Pinto", "Juliano", ""], ["Svensson", "Lennart", ""]]}, {"id": "2007.07943", "submitter": "Vincent Christlein", "authors": "Martin Leipert, Georg Vogeler, Mathias Seuret, Andreas Maier, Vincent\n  Christlein", "title": "The Notary in the Haystack -- Countering Class Imbalance in Document\n  Processing with CNNs", "comments": "Accepted at DAS Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notarial instruments are a category of documents. A notarial instrument can\nbe distinguished from other documents by its notary sign, a prominent symbol in\nthe certificate, which also allows to identify the document's issuer.\nNaturally, notarial instruments are underrepresented in regard to other\ndocuments. This makes a classification difficult because class imbalance in\ntraining data worsens the performance of Convolutional Neural Networks. In this\nwork, we evaluate different countermeasures for this problem. They are applied\nto a binary classification and a segmentation task on a collection of medieval\ndocuments. In classification, notarial instruments are distinguished from other\ndocuments, while the notary sign is separated from the certificate in the\nsegmentation task. We evaluate different techniques, such as data augmentation,\nunder- and oversampling, as well as regularizing with focal loss. The\ncombination of random minority oversampling and data augmentation leads to the\nbest performance. In segmentation, we evaluate three loss-functions and their\ncombinations, where only class-weighted dice loss was able to segment the\nnotary sign sufficiently.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:40:33 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Leipert", "Martin", ""], ["Vogeler", "Georg", ""], ["Seuret", "Mathias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2007.07978", "submitter": "Andreas Holm Nielsen", "authors": "A. H. Nielsen, A. Iosifidis, H. Karstoft", "title": "CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds", "comments": "For the novel dataset, see\n  https://vision.eng.au.dk/cloudcast-dataset/", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing, vol. 14, pp. 3485-3494, 2021", "doi": "10.1109/JSTARS.2021.3062936", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the formation and development of clouds is a central element of\nmodern weather forecasting systems. Incorrect clouds forecasts can lead to\nmajor uncertainty in the overall accuracy of weather forecasts due to their\nintrinsic role in the Earth's climate system. Few studies have tackled this\nchallenging problem from a machine learning point-of-view due to a shortage of\nhigh-resolution datasets with many historical observations globally. In this\npaper, we present a novel satellite-based dataset called ``CloudCast''. It\nconsists of 70,080 images with 10 different cloud types for multiple layers of\nthe atmosphere annotated on a pixel level. The spatial resolution of the\ndataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between\nframes for the period 2017-01-01 to 2018-12-31. All frames are centered and\nprojected over Europe. To supplement the dataset, we conduct an evaluation\nstudy with current state-of-the-art video prediction methods such as\nconvolutional long short-term memory networks, generative adversarial networks,\nand optical flow-based extrapolation methods. As the evaluation of video\nprediction is difficult in practice, we aim for a thorough evaluation in the\nspatial and temporal domain. Our benchmark models show promising results but\nwith ample room for improvement. This is the first publicly available\nglobal-scale dataset with high-resolution cloud types on a high temporal\ngranularity to the authors' best knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:20:55 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 09:08:24 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nielsen", "A. H.", ""], ["Iosifidis", "A.", ""], ["Karstoft", "H.", ""]]}, {"id": "2007.07984", "submitter": "Lingyu Zhu", "authors": "Lingyu Zhu and Esa Rahtu", "title": "Leveraging Category Information for Single-Frame Visual Sound Source\n  Separation", "comments": "6 pages. The code is available at\n  https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual sound source separation aims at identifying sound components from a\ngiven sound mixture with the presence of visual cues. Prior works have\ndemonstrated impressive results, but with the expense of large multi-stage\narchitectures and complex data representations (e.g. optical flow\ntrajectories). In contrast, we study simple yet efficient models for visual\nsound separation using only a single video frame. Furthermore, our models are\nable to exploit the information of the sound source category in the separation\nprocess. To this end, we propose two models where we assume that i) the\ncategory labels are available at the training time, or ii) we know if the\ntraining sample pairs are from the same or different category. The experiments\nwith the MUSIC dataset show that our model obtains comparable or better\nperformance compared to several recent baseline methods. The code is available\nat\nhttps://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:35:29 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 14:30:19 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Lingyu", ""], ["Rahtu", "Esa", ""]]}, {"id": "2007.07986", "submitter": "Yuanyi Zhong", "authors": "Yuanyi Zhong, Jianfeng Wang, Jian Peng, Lei Zhang", "title": "Boosting Weakly Supervised Object Detection with Progressive Knowledge\n  Transfer", "comments": "ECCV 2020. Code: https://github.com/mikuhatsune/wsod_transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective knowledge transfer framework to boost\nthe weakly supervised object detection accuracy with the help of an external\nfully-annotated source dataset, whose categories may not overlap with the\ntarget domain. This setting is of great practical value due to the existence of\nmany off-the-shelf detection datasets. To more effectively utilize the source\ndataset, we propose to iteratively transfer the knowledge from the source\ndomain by a one-class universal detector and learn the target-domain detector.\nThe box-level pseudo ground truths mined by the target-domain detector in each\niteration effectively improve the one-class universal detector. Therefore, the\nknowledge in the source dataset is more thoroughly exploited and leveraged.\nExtensive experiments are conducted with Pascal VOC 2007 as the target\nweakly-annotated dataset and COCO/ImageNet as the source fully-annotated\ndataset. With the proposed solution, we achieved an mAP of $59.7\\%$ detection\nperformance on the VOC test set and an mAP of $60.2\\%$ after retraining a fully\nsupervised Faster RCNN with the mined pseudo ground truths. This is\nsignificantly better than any previously known results in related literature\nand sets a new state-of-the-art of weakly supervised object detection under the\nknowledge transfer setting. Code:\n\\url{https://github.com/mikuhatsune/wsod_transfer}.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:38:25 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhong", "Yuanyi", ""], ["Wang", "Jianfeng", ""], ["Peng", "Jian", ""], ["Zhang", "Lei", ""]]}, {"id": "2007.08003", "submitter": "Mansi Khamkar", "authors": "Gresha Bhatia, Binoy Saha, Mansi Khamkar, Ashish Chandwani, Reshma\n  Khot", "title": "Stutter Diagnosis and Therapy System Based on Deep Learning", "comments": "About stutter classification, severity diagnosis and therapy\n  recommendation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stuttering, also called stammering, is a communication disorder that breaks\nthe continuity of the speech. This program of work is an attempt to develop\nautomatic recognition procedures to assess stuttered dysfluencies and use these\nassessments to filter out speech therapies for an individual. Stuttering may be\nin the form of repetitions, prolongations or abnormal stoppages of sounds and\nsyllables. Our system aims to help stutterers by diagnosing the severity and\ntype of stutter and also by suggesting appropriate therapies for practice by\nlearning the correlation between stutter descriptors and the effectiveness of\nspeech therapies on them. This paper focuses on the implementation of a stutter\ndiagnosis agent using Gated Recurrent CNN on MFCC audio features and therapy\nrecommendation agent using SVM. It also presents the results obtained and\nvarious key findings of the system developed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:24:02 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bhatia", "Gresha", ""], ["Saha", "Binoy", ""], ["Khamkar", "Mansi", ""], ["Chandwani", "Ashish", ""], ["Khot", "Reshma", ""]]}, {"id": "2007.08012", "submitter": "Hyung Jin Chang", "authors": "Kwang In Kim, Christian Richardt, Hyung Jin Chang", "title": "Combining Task Predictors via Enhancing Joint Predictability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictor combination aims to improve a (target) predictor of a learning task\nbased on the (reference) predictors of potentially relevant tasks, without\nhaving access to the internals of individual predictors. We present a new\npredictor combination algorithm that improves the target by i) measuring the\nrelevance of references based on their capabilities in predicting the target,\nand ii) strengthening such estimated relevance. Unlike existing predictor\ncombination approaches that only exploit pairwise relationships between the\ntarget and each reference, and thereby ignore potentially useful dependence\namong references, our algorithm jointly assesses the relevance of all\nreferences by adopting a Bayesian framework. This also offers a rigorous way to\nautomatically select only relevant references. Based on experiments on seven\nreal-world datasets from visual attribute ranking and multi-class\nclassification scenarios, we demonstrate that our algorithm offers a\nsignificant performance gain and broadens the application range of existing\npredictor combination approaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:58:39 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Kim", "Kwang In", ""], ["Richardt", "Christian", ""], ["Chang", "Hyung Jin", ""]]}, {"id": "2007.08019", "submitter": "Albert Gordo", "authors": "Albert Gordo and Filip Radenovic and Tamara Berg", "title": "Attention-Based Query Expansion Learning", "comments": "Accepted for publication at ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query expansion is a technique widely used in image search consisting in\ncombining highly ranked images from an original query into an expanded query\nthat is then reissued, generally leading to increased recall and precision. An\nimportant aspect of query expansion is choosing an appropriate way to combine\nthe images into a new query. Interestingly, despite the undeniable empirical\nsuccess of query expansion, ad-hoc methods with different caveats have\ndominated the landscape, and not a lot of research has been done on learning\nhow to do query expansion. In this paper we propose a more principled framework\nto query expansion, where one trains, in a discriminative manner, a model that\nlearns how images should be aggregated to form the expanded query. Within this\nframework, we propose a model that leverages a self-attention mechanism to\neffectively learn how to transfer information between the different images\nbefore aggregating them. Our approach obtains higher accuracy than existing\napproaches on standard benchmarks. More importantly, our approach is the only\none that consistently shows high accuracy under different regimes, overcoming\ncaveats of existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:15:55 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Gordo", "Albert", ""], ["Radenovic", "Filip", ""], ["Berg", "Tamara", ""]]}, {"id": "2007.08028", "submitter": "Joseph Bae", "authors": "Joseph Bae, Saarthak Kapse, Gagandeep Singh, Rishabh Gattu, Syed Ali,\n  Neal Shah, Colin Marshall, Jonathan Pierce, Tej Phatak, Amit Gupta, Jeremy\n  Green, Nikhil Madan, Prateek Prasanna", "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep\n  Learning on Chest Radiographs: A Multi-Institutional Study", "comments": "Joseph Bae and Saarthak Kapse have contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:48:11 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 18:47:22 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bae", "Joseph", ""], ["Kapse", "Saarthak", ""], ["Singh", "Gagandeep", ""], ["Gattu", "Rishabh", ""], ["Ali", "Syed", ""], ["Shah", "Neal", ""], ["Marshall", "Colin", ""], ["Pierce", "Jonathan", ""], ["Phatak", "Tej", ""], ["Gupta", "Amit", ""], ["Green", "Jeremy", ""], ["Madan", "Nikhil", ""], ["Prasanna", "Prateek", ""]]}, {"id": "2007.08032", "submitter": "Xavier Boix", "authors": "Spandan Madan, Timothy Henry, Jamell Dozier, Helen Ho, Nishchal\n  Bhandari, Tomotake Sasaki, Fr\\'edo Durand, Hanspeter Pfister, Xavier Boix", "title": "When and how do CNNs generalize to out-of-distribution\n  category-viewpoint combinations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object recognition and viewpoint estimation lie at the heart of visual\nunderstanding. Recent works suggest that convolutional neural networks (CNNs)\nfail to generalize to out-of-distribution (OOD) category-viewpoint\ncombinations, ie. combinations not seen during training. In this paper, we\ninvestigate when and how such OOD generalization may be possible by evaluating\nCNNs trained to classify both object category and 3D viewpoint on OOD\ncombinations, and identifying the neural mechanisms that facilitate such OOD\ngeneralization. We show that increasing the number of in-distribution\ncombinations (ie. data diversity) substantially improves generalization to OOD\ncombinations, even with the same amount of training data. We compare learning\ncategory and viewpoint in separate and shared network architectures, and\nobserve starkly different trends on in-distribution and OOD combinations, ie.\nwhile shared networks are helpful in-distribution, separate networks\nsignificantly outperform shared ones at OOD combinations. Finally, we\ndemonstrate that such OOD generalization is facilitated by the neural mechanism\nof specialization, ie. the emergence of two types of neurons -- neurons\nselective to category and invariant to viewpoint, and vice versa.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 23:04:37 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 21:03:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Madan", "Spandan", ""], ["Henry", "Timothy", ""], ["Dozier", "Jamell", ""], ["Ho", "Helen", ""], ["Bhandari", "Nishchal", ""], ["Sasaki", "Tomotake", ""], ["Durand", "Fr\u00e9do", ""], ["Pfister", "Hanspeter", ""], ["Boix", "Xavier", ""]]}, {"id": "2007.08037", "submitter": "Wenguan Wang", "authors": "Hanqing Wang, Wenguan Wang, Tianmin Shu, Wei Liang and Jianbing Shen", "title": "Active Visual Information Gathering for Vision-Language Navigation", "comments": "ECCV2020 (changed with improved perfromance on Pre-Explore and Beam\n  Search settings); website: https://github.com/HanqingWangAI/Active_VLN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-language navigation (VLN) is the task of entailing an agent to carry\nout navigational instructions inside photo-realistic environments. One of the\nkey challenges in VLN is how to conduct a robust navigation by mitigating the\nuncertainty caused by ambiguous instructions and insufficient observation of\nthe environment. Agents trained by current approaches typically suffer from\nthis and would consequently struggle to avoid random and inefficient actions at\nevery step. In contrast, when humans face such a challenge, they can still\nmaintain robust navigation by actively exploring the surroundings to gather\nmore information and thus make more confident navigation decisions. This work\ndraws inspiration from human navigation behavior and endows an agent with an\nactive information gathering ability for a more intelligent vision-language\nnavigation policy. To achieve this, we propose an end-to-end framework for\nlearning an exploration policy that decides i) when and where to explore, ii)\nwhat information is worth gathering during exploration, and iii) how to adjust\nthe navigation decision after the exploration. The experimental results show\npromising exploration strategies emerged from training, which leads to\nsignificant boost in navigation performance. On the R2R challenge leaderboard,\nour agent gets promising results all three VLN settings, i.e., single run,\npre-exploration, and beam search.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 23:54:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:33:31 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 19:48:02 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wang", "Hanqing", ""], ["Wang", "Wenguan", ""], ["Shu", "Tianmin", ""], ["Liang", "Wei", ""], ["Shen", "Jianbing", ""]]}, {"id": "2007.08044", "submitter": "Ryoma Bise", "authors": "Hiroki Tokunaga, Brian Kenji Iwana, Yuki Teramoto, Akihiko Yoshizawa,\n  Ryoma Bise", "title": "Negative Pseudo Labeling using Class Proportion for Semantic\n  Segmentation in Pathology", "comments": "17 pages, 7 figures, Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weakly-supervised cell tracking method that can train a\nconvolutional neural network (CNN) by using only the annotation of \"cell\ndetection\" (i.e., the coordinates of cell positions) without association\ninformation, in which cell positions can be easily obtained by nuclear\nstaining. First, we train a co-detection CNN that detects cells in successive\nframes by using weak-labels. Our key assumption is that the co-detection CNN\nimplicitly learns association in addition to detection. To obtain the\nassociation information, we propose a backward-and-forward propagation method\nthat analyzes the correspondence of cell positions in the detection maps output\nof the co-detection CNN. Experiments demonstrated that the proposed method can\nmatch positions by analyzing the co-detection CNN. Even though the method uses\nonly weak supervision, the performance of our method was almost the same as the\nstate-of-the-art supervised method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:28:07 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Tokunaga", "Hiroki", ""], ["Iwana", "Brian Kenji", ""], ["Teramoto", "Yuki", ""], ["Yoshizawa", "Akihiko", ""], ["Bise", "Ryoma", ""]]}, {"id": "2007.08071", "submitter": "Ziyang Song", "authors": "Ziyang Song, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling and\n  Shenghao Zhang", "title": "Learning End-to-End Action Interaction by Paired-Embedding Data\n  Augmentation", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recognition-based action interaction, robots' responses to human actions\nare often pre-designed according to recognized categories and thus stiff. In\nthis paper, we specify a new Interactive Action Translation (IAT) task which\naims to learn end-to-end action interaction from unlabeled interactive pairs,\nremoving explicit action recognition. To enable learning on small-scale data,\nwe propose a Paired-Embedding (PE) method for effective and reliable data\naugmentation. Specifically, our method first utilizes paired relationships to\ncluster individual actions in an embedding space. Then two actions originally\npaired can be replaced with other actions in their respective neighborhood,\nassembling into new pairs. An Act2Act network based on conditional GAN follows\nto learn from augmented data. Besides, IAT-test and IAT-train scores are\nspecifically proposed for evaluating methods on our task. Experimental results\non two datasets show impressive effects and broad application prospects of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:54:16 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Song", "Ziyang", ""], ["Yuan", "Zejian", ""], ["Zhang", "Chong", ""], ["Chi", "Wanchao", ""], ["Ling", "Yonggen", ""], ["Zhang", "Shenghao", ""]]}, {"id": "2007.08073", "submitter": "Christopher Xie", "authors": "Christopher Xie, Yu Xiang, Arsalan Mousavian, Dieter Fox", "title": "Unseen Object Instance Segmentation for Robotic Environments", "comments": "Extended version of arXiv:1907.13236", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to function in unstructured environments, robots need the ability to\nrecognize unseen objects. We take a step in this direction by tackling the\nproblem of segmenting unseen object instances in tabletop environments.\nHowever, the type of large-scale real-world dataset required for this task\ntypically does not exist for most robotic settings, which motivates the use of\nsynthetic data. Our proposed method, UOIS-Net, separately leverages synthetic\nRGB and synthetic depth for unseen object instance segmentation. UOIS-Net is\ncomprised of two stages: first, it operates only on depth to produce object\ninstance center votes in 2D or 3D and assembles them into rough initial masks.\nSecondly, these initial masks are refined using RGB. Surprisingly, our\nframework is able to learn from synthetic RGB-D data where the RGB is\nnon-photorealistic. To train our method, we introduce a large-scale synthetic\ndataset of random objects on tabletops. We show that our method can produce\nsharp and accurate segmentation masks, outperforming state-of-the-art methods\non unseen object instance segmentation. We also show that our method can\nsegment unseen objects for robot grasping.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:59:13 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Xie", "Christopher", ""], ["Xiang", "Yu", ""], ["Mousavian", "Arsalan", ""], ["Fox", "Dieter", ""]]}, {"id": "2007.08074", "submitter": "Xiaoqi Zhao", "authors": "Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, Lei Zhang", "title": "Suppress and Balance: A Simple Gated Network for Salient Object\n  Detection", "comments": "Accepted in ECCV2020(oral). Code:\n  https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most salient object detection approaches use U-Net or feature pyramid\nnetworks (FPN) as their basic structures. These methods ignore two key problems\nwhen the encoder exchanges information with the decoder: one is the lack of\ninterference control between them, the other is without considering the\ndisparity of the contributions of different encoder blocks. In this work, we\npropose a simple gated network (GateNet) to solve both issues at once. With the\nhelp of multilevel gate units, the valuable context information from the\nencoder can be optimally transmitted to the decoder. We design a novel gated\ndual branch structure to build the cooperation among different levels of\nfeatures and improve the discriminability of the whole network. Through the\ndual branch design, more details of the saliency map can be further restored.\nIn addition, we adopt the atrous spatial pyramid pooling based on the proposed\n\"Fold\" operation (Fold-ASPP) to accurately localize salient objects of various\nscales. Extensive experiments on five challenging datasets demonstrate that the\nproposed model performs favorably against most state-of-the-art methods under\ndifferent evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:00:53 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 04:25:30 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 09:34:12 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhao", "Xiaoqi", ""], ["Pang", "Youwei", ""], ["Zhang", "Lihe", ""], ["Lu", "Huchuan", ""], ["Zhang", "Lei", ""]]}, {"id": "2007.08076", "submitter": "Darshana Priyasad Madduma Kankanamalage Don Mr", "authors": "Darshana Priyasad, Tharindu Fernando, Simon Denman, Sridha Sridharan,\n  Clinton Fookes", "title": "Memory based fusion for multi-modal deep learning", "comments": "Pre-print submitted to Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of multi-modal data for deep machine learning has shown promise when\ncompared to uni-modal approaches with fusion of multi-modal features resulting\nin improved performance in several applications. However, most state-of-the-art\nmethods use naive fusion which processes feature streams independently,\nignoring possible long-term dependencies within the data during fusion. In this\npaper, we present a novel Memory based Attentive Fusion layer, which fuses\nmodes by incorporating both the current features and longterm dependencies in\nthe data, thus allowing the model to understand the relative importance of\nmodes over time. We introduce an explicit memory block within the fusion layer\nwhich stores features containing long-term dependencies of the fused data. The\nfeature inputs from uni-modal encoders are fused through attentive composition\nand transformation followed by naive fusion of the resultant memory derived\nfeatures with layer inputs. Following state-of-the-art methods, we have\nevaluated the performance and the generalizability of the proposed fusion\napproach on two different datasets with different modalities. In our\nexperiments, we replace the naive fusion layer in benchmark networks with our\nproposed layer to enable a fair comparison. Experimental results indicate that\nthe MBAF layer can generalise across different modalities and networks to\nenhance fusion and improve performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:05:54 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 00:48:17 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 05:22:34 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Priyasad", "Darshana", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2007.08090", "submitter": "Christopher Neff", "authors": "Christopher Neff, Aneri Sheth, Steven Furgurson, Hamed Tabkhi", "title": "EfficientHRNet: Efficient Scaling for Lightweight High-Resolution\n  Multi-Person Pose Estimation", "comments": "11 pages (13 with references), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing demand for lightweight multi-person pose estimation\nfor many emerging smart IoT applications. However, the existing algorithms tend\nto have large model sizes and intense computational requirements, making them\nill-suited for real-time applications and deployment on resource-constrained\nhardware. Lightweight and real-time approaches are exceedingly rare and come at\nthe cost of inferior accuracy. In this paper, we present EfficientHRNet, a\nfamily of lightweight multi-person human pose estimators that are able to\nperform in real-time on resource-constrained devices. By unifying recent\nadvances in model scaling with high-resolution feature representations,\nEfficientHRNet creates highly accurate models while reducing computation enough\nto achieve real-time performance. The largest model is able to come within 4.4%\naccuracy of the current state-of-the-art, while having 1/3 the model size and\n1/6 the computation, achieving 23 FPS on Nvidia Jetson Xavier. Compared to the\ntop real-time approach, EfficientHRNet increases accuracy by 22% while\nachieving similar FPS with 1/3 the power. At every level, EfficientHRNet proves\nto be more computationally efficient than other bottom-up 2D human pose\nestimation approaches, while achieving highly competitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 03:27:26 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 17:43:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Neff", "Christopher", ""], ["Sheth", "Aneri", ""], ["Furgurson", "Steven", ""], ["Tabkhi", "Hamed", ""]]}, {"id": "2007.08097", "submitter": "Jungseok Hong", "authors": "Jungseok Hong, Michael Fulton, and Junaed Sattar", "title": "TrashCan: A Semantically-Segmented Dataset towards Visual Detection of\n  Marine Debris", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TrashCan, a large dataset comprised of images of\nunderwater trash collected from a variety of sources, annotated both using\nbounding boxes and segmentation labels, for development of robust detectors of\nmarine debris. The dataset has two versions, TrashCan-Material and\nTrashCan-Instance, corresponding to different object class configurations. The\neventual goal is to develop efficient and accurate trash detection methods\nsuitable for onboard robot deployment. Along with information about the\nconstruction and sourcing of the TrashCan dataset, we present initial results\nof instance segmentation from Mask R-CNN and object detection from Faster\nR-CNN. These do not represent the best possible detection results but provides\nan initial baseline for future work in instance segmentation and object\ndetection on the TrashCan dataset.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:19:06 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hong", "Jungseok", ""], ["Fulton", "Michael", ""], ["Sattar", "Junaed", ""]]}, {"id": "2007.08103", "submitter": "Kang Kim", "authors": "Kang Kim and Hee Seok Lee", "title": "Probabilistic Anchor Assignment with IoU Prediction for Object Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, determining which anchors to assign as positive or\nnegative samples, known as anchor assignment, has been revealed as a core\nprocedure that can significantly affect a model's performance. In this paper we\npropose a novel anchor assignment strategy that adaptively separates anchors\ninto positive and negative samples for a ground truth bounding box according to\nthe model's learning status such that it is able to reason about the separation\nin a probabilistic manner. To do so we first calculate the scores of anchors\nconditioned on the model and fit a probability distribution to these scores.\nThe model is then trained with anchors separated into positive and negative\nsamples according to their probabilities. Moreover, we investigate the gap\nbetween the training and testing objectives and propose to predict the\nIntersection-over-Unions of detected boxes as a measure of localization quality\nto reduce the discrepancy. The combined score of classification and\nlocalization qualities serving as a box selection metric in non-maximum\nsuppression well aligns with the proposed anchor assignment strategy and leads\nsignificant performance improvements. The proposed methods only add a single\nconvolutional layer to RetinaNet baseline and does not require multiple anchors\nper location, so are efficient. Experimental results verify the effectiveness\nof the proposed methods. Especially, our models set new records for\nsingle-stage detectors on MS COCO test-dev dataset with various backbones. Code\nis available at https://github.com/kkhoot/PAA.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:26:57 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 07:44:24 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kim", "Kang", ""], ["Lee", "Hee Seok", ""]]}, {"id": "2007.08113", "submitter": "Xiaodong Cun", "authors": "Xiaodong Cun and Chi-Man Pun", "title": "Defocus Blur Detection via Depth Distillation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defocus Blur Detection(DBD) aims to separate in-focus and out-of-focus\nregions from a single image pixel-wisely. This task has been paid much\nattention since bokeh effects are widely used in digital cameras and smartphone\nphotography. However, identifying obscure homogeneous regions and borderline\ntransitions in partially defocus images is still challenging. To solve these\nproblems, we introduce depth information into DBD for the first time. When the\ncamera parameters are fixed, we argue that the accuracy of DBD is highly\nrelated to scene depth. Hence, we consider the depth information as the\napproximate soft label of DBD and propose a joint learning framework inspired\nby knowledge distillation. In detail, we learn the defocus blur from ground\ntruth and the depth distilled from a well-trained depth estimation network at\nthe same time. Thus, the sharp region will provide a strong prior for depth\nestimation while the blur detection also gains benefits from the distilled\ndepth. Besides, we propose a novel decoder in the fully convolutional\nnetwork(FCN) as our network structure. In each level of the decoder, we design\nthe Selective Reception Field Block(SRFB) for merging multi-scale features\nefficiently and reuse the side outputs as Supervision-guided Attention\nBlock(SAB). Unlike previous methods, the proposed decoder builds reception\nfield pyramids and emphasizes salient regions simply and efficiently.\nExperiments show that our approach outperforms 11 other state-of-the-art\nmethods on two popular datasets. Our method also runs at over 30 fps on a\nsingle GPU, which is 2x faster than previous works. The code is available at:\nhttps://github.com/vinthony/depth-distillation\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:58:09 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Cun", "Xiaodong", ""], ["Pun", "Chi-Man", ""]]}, {"id": "2007.08116", "submitter": "Xibin Song", "authors": "Feixiang Lu, Zongdai Liu, Xibin Song, Dingfu Zhou, Wei Li, Hui Miao,\n  Miao Liao, Liangjun Zhang, Bin Zhou, Ruigang Yang and Dinesh Manocha", "title": "PerMO: Perceiving More at Once from a Single Image for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to detect, segment, and reconstruct complete\ntextured 3D models of vehicles from a single image for autonomous driving. Our\napproach combines the strengths of deep learning and the elegance of\ntraditional techniques from part-based deformable model representation to\nproduce high-quality 3D models in the presence of severe occlusions. We present\na new part-based deformable vehicle model that is used for instance\nsegmentation and automatically generate a dataset that contains dense\ncorrespondences between 2D images and 3D models. We also present a novel\nend-to-end deep neural network to predict dense 2D/3D mapping and highlight its\nbenefits. Based on the dense mapping, we are able to compute precise 6-DoF\nposes and 3D reconstruction results at almost interactive rates on a commodity\nGPU. We have integrated these algorithms with an autonomous driving system. In\npractice, our method outperforms the state-of-the-art methods for all major\nvehicle parsing tasks: 2D instance segmentation by 4.4 points (mAP), 6-DoF pose\nestimation by 9.11 points, and 3D detection by 1.37. Moreover, we have released\nall of the source code, dataset, and the trained model on Github.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 05:02:45 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Lu", "Feixiang", ""], ["Liu", "Zongdai", ""], ["Song", "Xibin", ""], ["Zhou", "Dingfu", ""], ["Li", "Wei", ""], ["Miao", "Hui", ""], ["Liao", "Miao", ""], ["Zhang", "Liangjun", ""], ["Zhou", "Bin", ""], ["Yang", "Ruigang", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2007.08129", "submitter": "Yunxiao Qin", "authors": "Yunxiao Qin, Weiguo Zhang, Zezheng Wang, Chenxu Zhao, Jingping Shi", "title": "Layer-Wise Adaptive Updating for Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2020.3036348", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification (FSIC), which requires a model to recognize new\ncategories via learning from few images of these categories, has attracted lots\nof attention. Recently, meta-learning based methods have been shown as a\npromising direction for FSIC. Commonly, they train a meta-learner\n(meta-learning model) to learn easy fine-tuning weight, and when solving an\nFSIC task, the meta-learner efficiently fine-tunes itself to a task-specific\nmodel by updating itself on few images of the task. In this paper, we propose a\nnovel meta-learning based layer-wise adaptive updating (LWAU) method for FSIC.\nLWAU is inspired by an interesting finding that compared with common deep\nmodels, the meta-learner pays much more attention to update its top layer when\nlearning from few images. According to this finding, we assume that the\nmeta-learner may greatly prefer updating its top layer to updating its bottom\nlayers for better FSIC performance. Therefore, in LWAU, the meta-learner is\ntrained to learn not only the easy fine-tuning model but also its favorite\nlayer-wise adaptive updating rule to improve its learning efficiency. Extensive\nexperiments show that with the layer-wise adaptive updating rule, the proposed\nLWAU: 1) outperforms existing few-shot classification methods with a clear\nmargin; 2) learns from few images more efficiently by at least 5 times than\nexisting meta-learners when solving FSIC.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:02:44 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Qin", "Yunxiao", ""], ["Zhang", "Weiguo", ""], ["Wang", "Zezheng", ""], ["Zhao", "Chenxu", ""], ["Shi", "Jingping", ""]]}, {"id": "2007.08139", "submitter": "Yuk Heo", "authors": "Yuk Heo, Yeong Jun Koh and Chang-Su Kim", "title": "Interactive Video Object Segmentation Using Global and Local Transfer\n  Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interactive video object segmentation algorithm, which takes scribble\nannotations on query objects as input, is proposed in this paper. We develop a\ndeep neural network, which consists of the annotation network (A-Net) and the\ntransfer network (T-Net). First, given user scribbles on a frame, A-Net yields\na segmentation result based on the encoder-decoder architecture. Second, T-Net\ntransfers the segmentation result bidirectionally to the other frames, by\nemploying the global and local transfer modules. The global transfer module\nconveys the segmentation information in an annotated frame to a target frame,\nwhile the local transfer module propagates the segmentation information in a\ntemporally adjacent frame to the target frame. By applying A-Net and T-Net\nalternately, a user can obtain desired segmentation results with minimal\nefforts. We train the entire network in two stages, by emulating user scribbles\nand employing an auxiliary loss. Experimental results demonstrate that the\nproposed interactive video object segmentation algorithm outperforms the\nstate-of-the-art conventional algorithms. Codes and models are available at\nhttps://github.com/yuk6heo/IVOS-ATNet.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:49:07 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Heo", "Yuk", ""], ["Koh", "Yeong Jun", ""], ["Kim", "Chang-Su", ""]]}, {"id": "2007.08142", "submitter": "Marzieh Edraki", "authors": "Marzieh Edraki, Nazmul Karim, Nazanin Rahnavard, Ajmal Mian, Mubarak\n  Shah", "title": "Odyssey: Creation, Analysis and Detection of Trojan Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the success of deep neural network (DNN) models, rise the threats\nto the integrity of these models. A recent threat is the Trojan attack where an\nattacker interferes with the training pipeline by inserting triggers into some\nof the training samples and trains the model to act maliciously only for\nsamples that contain the trigger. Since the knowledge of triggers is privy to\nthe attacker, detection of Trojan networks is challenging. Existing Trojan\ndetectors make strong assumptions about the types of triggers and attacks. We\npropose a detector that is based on the analysis of the intrinsic DNN\nproperties; that are affected due to the Trojaning process. For a comprehensive\nanalysis, we develop Odysseus, the most diverse dataset to date with over 3,000\nclean and Trojan models. Odysseus covers a large spectrum of attacks; generated\nby leveraging the versatility in trigger designs and source to target class\nmappings. Our analysis results show that Trojan attacks affect the classifier\nmargin and shape of decision boundary around the manifold of clean data.\nExploiting these two factors, we propose an efficient Trojan detector that\noperates without any knowledge of the attack and significantly outperforms\nexisting methods. Through a comprehensive set of experiments we demonstrate the\nefficacy of the detector on cross model architectures, unseen Triggers and\nregularized models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:55:00 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 08:09:51 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Edraki", "Marzieh", ""], ["Karim", "Nazmul", ""], ["Rahnavard", "Nazanin", ""], ["Mian", "Ajmal", ""], ["Shah", "Mubarak", ""]]}, {"id": "2007.08146", "submitter": "Molin Zhang", "authors": "Molin Zhang, Junshen Xu, Esra Abaci Turk, P. Ellen Grant, Polina\n  Golland and Elfar Adalsteinsson", "title": "Enhanced detection of fetal pose in 3D MRI by Deep Reinforcement\n  Learning with physical structure priors on anatomy", "comments": "10 pages, 3 figures, MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal MRI is heavily constrained by unpredictable and substantial fetal\nmotion that causes image artifacts and limits the set of viable diagnostic\nimage contrasts. Current mitigation of motion artifacts is predominantly\nperformed by fast, single-shot MRI and retrospective motion correction.\nEstimation of fetal pose in real time during MRI stands to benefit prospective\nmethods to detect and mitigate fetal motion artifacts where inferred fetal\nmotion is combined with online slice prescription with low-latency decision\nmaking. Current developments of deep reinforcement learning (DRL), offer a\nnovel approach for fetal landmarks detection. In this task 15 agents are\ndeployed to detect 15 landmarks simultaneously by DRL. The optimization is\nchallenging, and here we propose an improved DRL that incorporates priors on\nphysical structure of the fetal body. First, we use graph communication layers\nto improve the communication among agents based on a graph where each node\nrepresents a fetal-body landmark. Further, additional reward based on the\ndistance between agents and physical structures such as the fetal limbs is used\nto fully exploit physical structure. Evaluation of this method on a repository\nof 3-mm resolution in vivo data demonstrates a mean accuracy of landmark\nestimation within 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm.\nThe proposed DRL for fetal pose landmark search demonstrates a potential\nclinical utility for online detection of fetal motion that guides real-time\nmitigation of motion artifacts as well as health diagnosis during MRI of the\npregnant mother.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:10:21 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhang", "Molin", ""], ["Xu", "Junshen", ""], ["Turk", "Esra Abaci", ""], ["Grant", "P. Ellen", ""], ["Golland", "Polina", ""], ["Adalsteinsson", "Elfar", ""]]}, {"id": "2007.08154", "submitter": "Yong Man Ro", "authors": "Joanna Hong, Jung Uk Kim, Sangmin Lee, and Yong Man Ro", "title": "Comprehensive Facial Expression Synthesis using Human-Interpretable\n  Language", "comments": "ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in facial expression synthesis have shown promising results\nusing diverse expression representations including facial action units. Facial\naction units for an elaborate facial expression synthesis need to be\nintuitively represented for human comprehension, not a numeric categorization\nof facial action units. To address this issue, we utilize human-friendly\napproach: use of natural language where language helps human grasp conceptual\ncontexts. In this paper, therefore, we propose a new facial expression\nsynthesis model from language-based facial expression description. Our method\ncan synthesize the facial image with detailed expressions. In addition,\neffectively embedding language features on facial features, our method can\ncontrol individual word to handle each part of facial movement. Extensive\nqualitative and quantitative evaluations were conducted to verify the\neffectiveness of the natural language.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:28:25 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hong", "Joanna", ""], ["Kim", "Jung Uk", ""], ["Lee", "Sangmin", ""], ["Ro", "Yong Man", ""]]}, {"id": "2007.08170", "submitter": "Lixuan Che", "authors": "Zhipeng Luo, Lixuan Che", "title": "VIPriors Object Detection Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a brief report to our submission to the VIPriors Object\nDetection Challenge. Object Detection has attracted many researchers' attention\nfor its full application, but it is still a challenging task. In this paper, we\nstudy analysis the characteristics of the data, and an effective data\nenhancement method is proposed. We carefully choose the model which is more\nsuitable for training from scratch. We benefit a lot from using softnms and\nmodel fusion skillfully.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:10:42 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Luo", "Zhipeng", ""], ["Che", "Lixuan", ""]]}, {"id": "2007.08173", "submitter": "Stefano Pellegrini", "authors": "Mykhaylo Andriluka, Stefano Pellegrini, Stefan Popov, Vittorio Ferrari", "title": "Efficient Full Image Interactive Segmentation by Leveraging Within-image\n  Appearance Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to interactive full-image semantic segmentation\nwhich enables quickly collecting training data for new datasets with previously\nunseen semantic classes (A demo is available at https://youtu.be/yUk8D5gEX-o).\nWe leverage a key observation: propagation from labeled to unlabeled pixels\ndoes not necessarily require class-specific knowledge, but can be done purely\nbased on appearance similarity within an image. We build on this observation\nand propose an approach capable of jointly propagating pixel labels from\nmultiple classes without having explicit class-specific appearance models. To\nenable long-range propagation, our approach first globally measures appearance\nsimilarity between labeled and unlabeled pixels across the entire image. Then\nit locally integrates per-pixel measurements which improves the accuracy at\nboundaries and removes noisy label switches in homogeneous regions. We also\ndesign an efficient manual annotation interface that extends the traditional\npolygon drawing tools with a suite of additional convenient features (and add\nautomatic propagation to it). Experiments with human annotators on the COCO\nPanoptic Challenge dataset show that the combination of our better manual\ninterface and our novel automatic propagation mechanism leads to reducing\nannotation time by more than factor of 2x compared to polygon drawing. We also\ntest our method on the ADE-20k and Fashionista datasets without making any\ndataset-specific adaptation nor retraining our model, demonstrating that it can\ngeneralize to new datasets and visual classes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:21:59 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Andriluka", "Mykhaylo", ""], ["Pellegrini", "Stefano", ""], ["Popov", "Stefan", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2007.08180", "submitter": "Zhiguang Zhang", "authors": "Zhipeng Luo, Dawei Xu, Zhiguang Zhang", "title": "Challenge report:VIPriors Action Recognition Challenge", "comments": "ECCV2020,VIPriors Action Recognition Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a brief report to our submission to the VIPriors Action\nRecognition Challenge. Action recognition has attracted many researchers\nattention for its full application, but it is still challenging. In this paper,\nwe study previous methods and propose our method. In our method, we are\nprimarily making improvements on the SlowFast Network and fusing with TSM to\nmake further breakthroughs. Also, we use a fast but effective way to extract\nmotion features from videos by using residual frames as input. Better motion\nfeatures can be extracted using residual frames with SlowFast, and the\nresidual-frame-input path is an excellent supplement for existing\nRGB-frame-input models. And better performance obtained by combining 3D\nconvolution(SlowFast) with 2D convolution(TSM). The above experiments were all\ntrained from scratch on UCF101.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:40:31 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Luo", "Zhipeng", ""], ["Xu", "Dawei", ""], ["Zhang", "Zhiguang", ""]]}, {"id": "2007.08194", "submitter": "Haoyu Liang", "authors": "Haoyu Liang, Zhihao Ouyang, Yuyuan Zeng, Hang Su, Zihao He, Shu-Tao\n  Xia, Jun Zhu, Bo Zhang", "title": "Training Interpretable Convolutional Neural Networks by Differentiating\n  Class-specific Filters", "comments": "European Conference on Computer Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully used in a range\nof tasks. However, CNNs are often viewed as \"black-box\" and lack of\ninterpretability. One main reason is due to the filter-class entanglement -- an\nintricate many-to-many correspondence between filters and classes. Most\nexisting works attempt post-hoc interpretation on a pre-trained model, while\nneglecting to reduce the entanglement underlying the model. In contrast, we\nfocus on alleviating filter-class entanglement during training. Inspired by\ncellular differentiation, we propose a novel strategy to train interpretable\nCNNs by encouraging class-specific filters, among which each filter responds to\nonly one (or few) class. Concretely, we design a learnable sparse\nClass-Specific Gate (CSG) structure to assign each filter with one (or few)\nclass in a flexible way. The gate allows a filter's activation to pass only\nwhen the input samples come from the specific class. Extensive experiments\ndemonstrate the fabulous performance of our method in generating a sparse and\nhighly class-related representation of the input, which leads to stronger\ninterpretability. Moreover, comparing with the standard training strategy, our\nmodel displays benefits in applications like object localization and\nadversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:12:26 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 10:09:12 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 10:40:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liang", "Haoyu", ""], ["Ouyang", "Zhihao", ""], ["Zeng", "Yuyuan", ""], ["Su", "Hang", ""], ["He", "Zihao", ""], ["Xia", "Shu-Tao", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "2007.08199", "submitter": "Hwanjun Song", "authors": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, Jae-Gil Lee", "title": "Learning from Noisy Labels with Deep Neural Networks: A Survey", "comments": "If your paper is missing, contact me: ghkswns91@gmail.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved remarkable success in numerous domains with help\nfrom large amounts of big data. However, the quality of data labels is a\nconcern because of the lack of high-quality labels in many real-world\nscenarios. As noisy labels severely degrade the generalization performance of\ndeep neural networks, learning from noisy labels (robust training) is becoming\nan important task in modern deep learning applications. In this survey, we\nfirst describe the problem of learning with label noise from a supervised\nlearning perspective. Next, we provide a comprehensive review of 57\nstate-of-the-art robust training methods, all of which are categorized into\nfive groups according to their methodological difference, followed by a\nsystematic comparison of six properties used to evaluate their superiority.\nSubsequently, we perform an in-depth analysis of noise rate estimation and\nsummarize the typically used evaluation methodology, including public noisy\ndatasets and evaluation metrics. Finally, we present several promising research\ndirections that can serve as a guideline for future studies. All the contents\nwill be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:23:13 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 14:09:46 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 03:38:50 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 04:43:20 GMT"}, {"version": "v5", "created": "Tue, 8 Jun 2021 11:32:13 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Song", "Hwanjun", ""], ["Kim", "Minseok", ""], ["Park", "Dongmin", ""], ["Shin", "Yooju", ""], ["Lee", "Jae-Gil", ""]]}, {"id": "2007.08211", "submitter": "Yichen Sheng", "authors": "Yichen Sheng, Jianming Zhang, Bedrich Benes", "title": "SSN: Soft Shadow Network for Image Compositing", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an interactive Soft Shadow Network (SSN) to generates\ncontrollable soft shadows for image compositing. SSN takes a 2D object mask as\ninput and thus is agnostic to image types such as painting and vector art. An\nenvironment light map is used to control the shadow's characteristics, such as\nangle and softness. SSN employs an Ambient Occlusion Prediction module to\npredict an intermediate ambient occlusion map, which can be further refined by\nthe user to provides geometric cues to modulate the shadow generation. To train\nour model, we design an efficient pipeline to produce diverse soft shadow\ntraining data using 3D object models. In addition, we propose an inverse shadow\nmap representation to improve model training. We demonstrate that our model\nproduces realistic soft shadows in real-time. Our user studies show that the\ngenerated shadows are often indistinguishable from shadows calculated by a\nphysics-based renderer and users can easily use SSN through an interactive\napplication to generate specific shadow effects in minutes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:36:39 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 11:28:47 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 19:14:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sheng", "Yichen", ""], ["Zhang", "Jianming", ""], ["Benes", "Bedrich", ""]]}, {"id": "2007.08213", "submitter": "Xuesong Niu", "authors": "Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, Shiguang Shan, Guoying\n  Zhao", "title": "Video-based Remote Physiological Measurement via Cross-verified Feature\n  Disentangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote physiological measurements, e.g., remote photoplethysmography (rPPG)\nbased heart rate (HR), heart rate variability (HRV) and respiration frequency\n(RF) measuring, are playing more and more important roles under the application\nscenarios where contact measurement is inconvenient or impossible. Since the\namplitude of the physiological signals is very small, they can be easily\naffected by head movements, lighting conditions, and sensor diversities. To\naddress these challenges, we propose a cross-verified feature disentangling\nstrategy to disentangle the physiological features with non-physiological\nrepresentations, and then use the distilled physiological features for robust\nmulti-task physiological measurements. We first transform the input face videos\ninto a multi-scale spatial-temporal map (MSTmap), which can suppress the\nirrelevant background and noise features while retaining most of the temporal\ncharacteristics of the periodic physiological signals. Then we take pairwise\nMSTmaps as inputs to an autoencoder architecture with two encoders (one for\nphysiological signals and the other for non-physiological information) and use\na cross-verified scheme to obtain physiological features disentangled with the\nnon-physiological features. The disentangled features are finally used for the\njoint prediction of multiple physiological signals like average HR values and\nrPPG signals. Comprehensive experiments on different large-scale public\ndatasets of multiple physiological measurement tasks as well as the\ncross-database testing demonstrate the robustness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:39:17 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Niu", "Xuesong", ""], ["Yu", "Zitong", ""], ["Han", "Hu", ""], ["Li", "Xiaobai", ""], ["Shan", "Shiguang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2007.08214", "submitter": "Peter Jung", "authors": "Martin Reiche and Peter Jung", "title": "DeepInit Phase Retrieval", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how data-driven deep generative models can be utilized to\nsolve challenging phase retrieval problems, in which one wants to reconstruct a\nsignal from only few intensity measurements. Classical iterative algorithms are\nknown to work well if initialized close to the optimum but otherwise suffer\nfrom non-convexity and often get stuck in local minima. We therefore propose\nDeepInit Phase Retrieval, which uses regularized gradient descent under a deep\ngenerative data prior to compute a trained initialization for a fast classical\nalgorithm (e.g. the randomized Kaczmarz method). We empirically show that our\nhybrid approach is able to deliver very high reconstruction results at low\nsampling rates even when there is significant generator model error.\nConceptually, learned initializations may therefore help to overcome the\nnon-convexity of the problem by starting classical descent steps closer to the\nglobal optimum. Also, our idea demonstrates superior runtime performance over\nconventional gradient-based reconstruction methods. We evaluate our method for\ngeneric measurements and show empirically that it is also applicable to\ndiffraction-type measurement models which are found in terahertz single-pixel\nphase retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:39:28 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Reiche", "Martin", ""], ["Jung", "Peter", ""]]}, {"id": "2007.08223", "submitter": "Ali Al-Timemy", "authors": "Ali H. Al-Timemy, Rami N. Khushaba, Zahraa M. Mosa and Javier Escudero", "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19\n  and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings", "comments": "The final constructed dataset named COVID-19 five-class balanced\n  dataset is available from:\n  https://drive.google.com/drive/folders/1toMymyHTy0DR_fyE7hjO3LSBGWtVoPNf?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinicians in the frontline need to assess quickly whether a patient with\nsymptoms indeed has COVID-19 or not. The difficulty of this task is exacerbated\nin low resource settings that may not have access to biotechnology tests.\nFurthermore, Tuberculosis (TB) remains a major health problem in several low-\nand middle-income countries and its common symptoms include fever, cough and\ntiredness, similarly to COVID-19. In order to help in the detection of\nCOVID-19, we propose the extraction of deep features (DF) from chest X-ray\nimages, a technology available in most hospitals, and their subsequent\nclassification using machine learning methods that do not require large\ncomputational resources. We compiled a five-class dataset of X-ray chest images\nincluding a balanced number of COVID-19, viral pneumonia, bacterial pneumonia,\nTB, and healthy cases. We compared the performance of pipelines combining 14\nindividual state-of-the-art pre-trained deep networks for DF extraction with\ntraditional machine learning classifiers. A pipeline consisting of ResNet-50\nfor DF computation and ensemble of subspace discriminant classifier was the\nbest performer in the classification of the five classes, achieving a detection\naccuracy of 91.6+ 2.6% (accuracy + 95% Confidence Interval). Furthermore, the\nsame pipeline achieved accuracies of 98.6+1.4% and 99.9+0.5% in simpler\nthree-class and two-class classification problems focused on distinguishing\nCOVID-19, TB and healthy cases; and COVID-19 and healthy images, respectively.\nThe pipeline was computationally efficient requiring just 0.19 second to\nextract DF per X-ray image and 2 minutes for training a traditional classifier\nwith more than 2000 images on a CPU machine. The results suggest the potential\nbenefits of using our pipeline in the detection of COVID-19, particularly in\nresource-limited settings and it can run with limited computational resources.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:49:49 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Al-Timemy", "Ali H.", ""], ["Khushaba", "Rami N.", ""], ["Mosa", "Zahraa M.", ""], ["Escudero", "Javier", ""]]}, {"id": "2007.08224", "submitter": "Enrico Meloni", "authors": "Enrico Meloni, Luca Pasqualini, Matteo Tiezzi, Marco Gori, Stefano\n  Melacci", "title": "SAILenv: Learning in Virtual Visual Environments Made Simple", "comments": "8 pages, 7 figures, submitted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers in Machine Learning algorithms, Computer Vision\nscientists, engineers and others, showed a growing interest in 3D simulators as\na mean to artificially create experimental settings that are very close to\nthose in the real world. However, most of the existing platforms to interface\nalgorithms with 3D environments are often designed to setup navigation-related\nexperiments, to study physical interactions, or to handle ad-hoc cases that are\nnot thought to be customized, sometimes lacking a strong photorealistic\nappearance and an easy-to-use software interface. In this paper, we present a\nnovel platform, SAILenv, that is specifically designed to be simple and\ncustomizable, and that allows researchers to experiment visual recognition in\nvirtual 3D scenes. A few lines of code are needed to interface every algorithm\nwith the virtual world, and non-3D-graphics experts can easily customize the 3D\nenvironment itself, exploiting a collection of photorealistic objects. Our\nframework yields pixel-level semantic and instance labeling, depth, and, to the\nbest of our knowledge, it is the only one that provides motion-related\ninformation directly inherited from the 3D engine. The client-server\ncommunication operates at a low level, avoiding the overhead of HTTP-based data\nexchanges. We perform experiments using a state-of-the-art object detector\ntrained on real-world images, showing that it is able to recognize the\nphotorealistic 3D objects of our environment. The computational burden of the\noptical flow compares favourably with the estimation performed using modern\nGPU-based convolutional networks or more classic implementations. We believe\nthat the scientific community will benefit from the easiness and high-quality\nof our framework to evaluate newly proposed algorithms in their own customized\nrealistic conditions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:50:23 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 15:42:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Meloni", "Enrico", ""], ["Pasqualini", "Luca", ""], ["Tiezzi", "Matteo", ""], ["Gori", "Marco", ""], ["Melacci", "Stefano", ""]]}, {"id": "2007.08238", "submitter": "Simindokht Jahangard", "authors": "Simindokht Jahangard, Mohammad Hossein Zangooei, Maysam Shahedi", "title": "U-Net Based Architecture for an Improved Multiresolution Segmentation in\n  Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Manual medical image segmentation is an exhausting and\ntime-consuming task along with high inter-observer variability. In this study,\nour objective is to improve the multi-resolution image segmentation performance\nof U-Net architecture. Approach: We have proposed a fully convolutional neural\nnetwork for image segmentation in a multi-resolution framework. We used U-Net\nas the base architecture and modified that to improve its image segmentation\nperformance. In the proposed architecture (mrU-Net), the input image and its\ndown-sampled versions were used as the network inputs. We added more\nconvolution layers to extract features directly from the down-sampled images.\nWe trained and tested the network on four different medical datasets, including\nskin lesion photos, lung computed tomography (CT) images (LUNA dataset), retina\nimages (DRIVE dataset), and prostate magnetic resonance (MR) images (PROMISE12\ndataset). We compared the performance of mrU-Net to U-Net under similar\ntraining and testing conditions. Results: Comparing the results to manual\nsegmentation labels, mrU-Net achieved average Dice similarity coefficients of\n70.6%, 97.9%, 73.6%, and 77.9% for the skin lesion, LUNA, DRIVE, and PROMISE12\nsegmentation, respectively. For the skin lesion, LUNA, and DRIVE datasets,\nmrU-Net outperformed U-Net with significantly higher accuracy and for the\nPROMISE12 dataset, both networks achieved similar accuracy. Furthermore, using\nmrU-Net led to a faster training rate on LUNA and DRIVE datasets when compared\nto U-Net. Conclusions: The striking feature of the proposed architecture is its\nhigher capability in extracting image-derived features compared to U-Net.\nmrU-Net illustrated a faster training rate and slightly more accurate image\nsegmentation compared to U-Net.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:19:01 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 06:17:20 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Jahangard", "Simindokht", ""], ["Zangooei", "Mohammad Hossein", ""], ["Shahedi", "Maysam", ""]]}, {"id": "2007.08247", "submitter": "Yassine Ouali", "authors": "Yassine Ouali, C\\'eline Hudelot, Myriam Tami", "title": "Autoregressive Unsupervised Image Segmentation", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new unsupervised image segmentation approach based\non mutual information maximization between different constructed views of the\ninputs. Taking inspiration from autoregressive generative models that predict\nthe current pixel from past pixels in a raster-scan ordering created with\nmasked convolutions, we propose to use different orderings over the inputs\nusing various forms of masked convolutions to construct different views of the\ndata. For a given input, the model produces a pair of predictions with two\nvalid orderings, and is then trained to maximize the mutual information between\nthe two outputs. These outputs can either be low-dimensional features for\nrepresentation learning or output clusters corresponding to semantic labels for\nclustering. While masked convolutions are used during training, in inference,\nno masking is applied and we fall back to the standard convolution where the\nmodel has access to the full input. The proposed method outperforms current\nstate-of-the-art on unsupervised image segmentation. It is simple and easy to\nimplement, and can be extended to other visual tasks and integrated seamlessly\ninto existing unsupervised learning methods requiring different views of the\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:47:40 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ouali", "Yassine", ""], ["Hudelot", "C\u00e9line", ""], ["Tami", "Myriam", ""]]}, {"id": "2007.08260", "submitter": "Chunhua Shen", "authors": "Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, Chunhua\n  Shen", "title": "Weighing Counts: Sequential Crowd Counting by Reinforcement Learning", "comments": "Accepted to Proc. Eur. Conf. Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We formulate counting as a sequential decision problem and present a novel\ncrowd counting model solvable by deep reinforcement learning. In contrast to\nexisting counting models that directly output count values, we divide one-step\nestimation into a sequence of much easier and more tractable sub-decision\nproblems. Such sequential decision nature corresponds exactly to a physical\nprocess in reality scale weighing. Inspired by scale weighing, we propose a\nnovel 'counting scale' termed LibraNet where the count value is analogized by\nweight. By virtually placing a crowd image on one side of a scale, LibraNet\n(agent) sequentially learns to place appropriate weights on the other side to\nmatch the crowd count. At each step, LibraNet chooses one weight (action) from\nthe weight box (the pre-defined action pool) according to the current crowd\nimage features and weights placed on the scale pan (state). LibraNet is\nrequired to learn to balance the scale according to the feedback of the needle\n(Q values). We show that LibraNet exactly implements scale weighing by\nvisualizing the decision process how LibraNet chooses actions. Extensive\nexperiments demonstrate the effectiveness of our design choices and report\nstate-of-the-art results on a few crowd counting benchmarks. We also\ndemonstrate good cross-dataset generalization of LibraNet. Code and models are\nmade available at: https://git.io/libranet\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 11:16:12 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Liu", "Liang", ""], ["Lu", "Hao", ""], ["Zou", "Hongwei", ""], ["Xiong", "Haipeng", ""], ["Cao", "Zhiguo", ""], ["Shen", "Chunhua", ""]]}, {"id": "2007.08270", "submitter": "Hongje Seong", "authors": "Hongje Seong, Junhyuk Hyun, Euntai Kim", "title": "Kernelized Memory Network for Video Object Segmentation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised video object segmentation (VOS) is a task that involves\npredicting a target object in a video when the ground truth segmentation mask\nof the target object is given in the first frame. Recently, space-time memory\nnetworks (STM) have received significant attention as a promising solution for\nsemi-supervised VOS. However, an important point is overlooked when applying\nSTM to VOS. The solution (STM) is non-local, but the problem (VOS) is\npredominantly local. To solve the mismatch between STM and VOS, we propose a\nkernelized memory network (KMN). Before being trained on real videos, our KMN\nis pre-trained on static images, as in previous works. Unlike in previous\nworks, we use the Hide-and-Seek strategy in pre-training to obtain the best\npossible results in handling occlusions and segment boundary extraction. The\nproposed KMN surpasses the state-of-the-art on standard benchmarks by a\nsignificant margin (+5% on DAVIS 2017 test-dev set). In addition, the runtime\nof KMN is 0.12 seconds per frame on the DAVIS 2016 validation set, and the KMN\nrarely requires extra computation, when compared with STM.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 11:44:12 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Seong", "Hongje", ""], ["Hyun", "Junhyuk", ""], ["Kim", "Euntai", ""]]}, {"id": "2007.08336", "submitter": "Bishan Wang", "authors": "Bishan Wang, Jingwei He, Lei Yu, Gui-Song Xia, Wen Yang", "title": "Event Enhanced High-Quality Image Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With extremely high temporal resolution, event cameras have a large potential\nfor robotics and computer vision. However, their asynchronous imaging mechanism\noften aggravates the measurement sensitivity to noises and brings a physical\nburden to increase the image spatial resolution. To recover high-quality\nintensity images, one should address both denoising and super-resolution\nproblems for event cameras. Since events depict brightness changes, with the\nenhanced degeneration model by the events, the clear and sharp high-resolution\nlatent images can be recovered from the noisy, blurry and low-resolution\nintensity observations. Exploiting the framework of sparse learning, the events\nand the low-resolution intensity observations can be jointly considered. Based\non this, we propose an explainable network, an event-enhanced sparse learning\nnetwork (eSL-Net), to recover the high-quality images from event cameras. After\ntraining with a synthetic dataset, the proposed eSL-Net can largely improve the\nperformance of the state-of-the-art by 7-12 dB. Furthermore, without additional\ntraining process, the proposed eSL-Net can be easily extended to generate\ncontinuous frames with frame-rate as high as the events.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 13:51:15 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wang", "Bishan", ""], ["He", "Jingwei", ""], ["Yu", "Lei", ""], ["Xia", "Gui-Song", ""], ["Yang", "Wen", ""]]}, {"id": "2007.08340", "submitter": "Vinkle Kumar Srivastav", "authors": "Vinkle Srivastav, Afshin Gangi, Nicolas Padoy", "title": "Human Pose Estimation on Privacy-Preserving Low-Resolution Depth Images", "comments": "Published at MICCAI-2019", "journal-ref": "Springer (2019) 583-591", "doi": "10.1007/978-3-030-32254-0_65", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human pose estimation (HPE) is a key building block for developing AI-based\ncontext-aware systems inside the operating room (OR). The 24/7 use of images\ncoming from cameras mounted on the OR ceiling can however raise concerns for\nprivacy, even in the case of depth images captured by RGB-D sensors. Being able\nto solely use low-resolution privacy-preserving images would address these\nconcerns and help scale up the computer-assisted approaches that rely on such\ndata to a larger number of ORs. In this paper, we introduce the problem of HPE\non low-resolution depth images and propose an end-to-end solution that\nintegrates a multi-scale super-resolution network with a 2D human pose\nestimation network. By exploiting intermediate feature-maps generated at\ndifferent super-resolution, our approach achieves body pose results on\nlow-resolution images (of size 64x48) that are on par with those of an approach\ntrained and tested on full resolution images (of size 640x480).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:03:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Srivastav", "Vinkle", ""], ["Gangi", "Afshin", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2007.08349", "submitter": "Pim de Haan", "authors": "Pim de Haan, Taco Cohen, Max Welling", "title": "Natural Graph Networks", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for graph neural networks is that they must process a graph\nin a way that does not depend on how the graph is described. Traditionally this\nhas been taken to mean that a graph network must be equivariant to node\npermutations. Here we show that instead of equivariance, the more general\nconcept of naturality is sufficient for a graph network to be well-defined,\nopening up a larger class of graph networks. We define global and local natural\ngraph networks, the latter of which are as scalable as conventional message\npassing graph neural networks while being more flexible. We give one practical\ninstantiation of a natural network on graphs which uses an equivariant message\nnetwork parameterization, yielding good performance on several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:19:06 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 15:38:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["de Haan", "Pim", ""], ["Cohen", "Taco", ""], ["Welling", "Max", ""]]}, {"id": "2007.08354", "submitter": "Vinkle Kumar Srivastav", "authors": "Vinkle Srivastav, Afshin Gangi, Nicolas Padoy", "title": "Self-supervision on Unlabelled OR Data for Multi-person 2D/3D Human Pose\n  Estimation", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  2D/3D human pose estimation is needed to develop novel intelligent tools for\nthe operating room that can analyze and support the clinical activities. The\nlack of annotated data and the complexity of state-of-the-art pose estimation\napproaches limit, however, the deployment of such techniques inside the OR. In\nthis work, we propose to use knowledge distillation in a teacher/student\nframework to harness the knowledge present in a large-scale non-annotated\ndataset and in an accurate but complex multi-stage teacher network to train a\nlightweight network for joint 2D/3D pose estimation. The teacher network also\nexploits the unlabeled data to generate both hard and soft labels useful in\nimproving the student predictions. The easily deployable network trained using\nthis effective self-supervision strategy performs on par with the teacher\nnetwork on \\emph{MVOR+}, an extension of the public MVOR dataset where all\npersons have been fully annotated, thus providing a viable solution for\nreal-time 2D/3D human pose estimation in the OR.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:28:22 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Srivastav", "Vinkle", ""], ["Gangi", "Afshin", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2007.08364", "submitter": "Tadas Baltrusaitis", "authors": "Tadas Baltrusaitis, Erroll Wood, Virginia Estellers, Charlie Hewitt,\n  Sebastian Dziadzio, Marek Kowalski, Matthew Johnson, Thomas J. Cashman, and\n  Jamie Shotton", "title": "A high fidelity synthetic face framework for computer vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of faces is one of the core applications of computer vision, with\ntasks ranging from landmark alignment, head pose estimation, expression\nrecognition, and face recognition among others. However, building reliable\nmethods requires time-consuming data collection and often even more\ntime-consuming manual annotation, which can be unreliable. In our work we\npropose synthesizing such facial data, including ground truth annotations that\nwould be almost impossible to acquire through manual annotation at the\nconsistency and scale possible through use of synthetic data. We use a\nparametric face model together with hand crafted assets which enable us to\ngenerate training data with unprecedented quality and diversity (varying shape,\ntexture, expression, pose, lighting, and hair).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:40:28 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Baltrusaitis", "Tadas", ""], ["Wood", "Erroll", ""], ["Estellers", "Virginia", ""], ["Hewitt", "Charlie", ""], ["Dziadzio", "Sebastian", ""], ["Kowalski", "Marek", ""], ["Johnson", "Matthew", ""], ["Cashman", "Thomas J.", ""], ["Shotton", "Jamie", ""]]}, {"id": "2007.08373", "submitter": "Mihir Sahasrabudhe", "authors": "Mihir Sahasrabudhe, Stergios Christodoulidis, Roberto Salgado, Stefan\n  Michiels, Sherene Loi, Fabrice Andr\\'e, Nikos Paragios, Maria Vakalopoulou", "title": "Self-Supervised Nuclei Segmentation in Histopathological Images Using\n  Attention", "comments": "10 pages. Code available online at\n  https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmentation and accurate localization of nuclei in histopathological images\nis a very challenging problem, with most existing approaches adopting a\nsupervised strategy. These methods usually rely on manual annotations that\nrequire a lot of time and effort from medical experts. In this study, we\npresent a self-supervised approach for segmentation of nuclei for whole slide\nhistopathology images. Our method works on the assumption that the size and\ntexture of nuclei can determine the magnification at which a patch is\nextracted. We show that the identification of the magnification level for tiles\ncan generate a preliminary self-supervision signal to locate nuclei. We further\nshow that by appropriately constraining our model it is possible to retrieve\nmeaningful segmentation maps as an auxiliary output to the primary\nmagnification identification task. Our experiments show that with standard\npost-processing, our method can outperform other unsupervised nuclei\nsegmentation approaches and report similar performance with supervised ones on\nthe publicly available MoNuSeg dataset. Our code and models are available\nonline to facilitate further research.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:49:20 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Sahasrabudhe", "Mihir", ""], ["Christodoulidis", "Stergios", ""], ["Salgado", "Roberto", ""], ["Michiels", "Stefan", ""], ["Loi", "Sherene", ""], ["Andr\u00e9", "Fabrice", ""], ["Paragios", "Nikos", ""], ["Vakalopoulou", "Maria", ""]]}, {"id": "2007.08378", "submitter": "Vinkle Kumar Srivastav", "authors": "Vinkle Srivastav, Britty Baby, Ramandeep Singh, Prem Kalra, Ashish\n  Suri", "title": "Neuro-Endo-Trainer-Online Assessment System (NET-OAS) for\n  Neuro-Endoscopic Skills Training", "comments": "Published at Federated Conference on Computer Science and Information\n  Systems - FedCSIS 2017", "journal-ref": "IEEE (2017)", "doi": "10.15439/2017F316", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neuro-endoscopy is a challenging minimally invasive neurosurgery that\nrequires surgical skills to be acquired using training methods different from\nthe existing apprenticeship model. There are various training systems developed\nfor imparting fundamental technical skills in laparoscopy where as limited\nsystems for neuro-endoscopy. Neuro-Endo-Trainer was a box-trainer developed for\nendo-nasal transsphenoidal surgical skills training with video based offline\nevaluation system. The objective of the current study was to develop a modified\nversion (Neuro-Endo-Trainer-Online Assessment System (NET-OAS)) by providing a\nstand-alone system with online evaluation and real-time feedback. The\nvalidation study on a group of 15 novice participants shows the improvement in\nthe technical skills for handling the neuro-endoscope and the tool while\nperforming pick and place activity.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:54:09 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Srivastav", "Vinkle", ""], ["Baby", "Britty", ""], ["Singh", "Ramandeep", ""], ["Kalra", "Prem", ""], ["Suri", "Ashish", ""]]}, {"id": "2007.08386", "submitter": "Xinghao Chen", "authors": "Xinghao Chen, Yunhe Wang, Yiman Zhang, Peng Du, Chunjing Xu, Chang Xu", "title": "Multi-Task Pruning for Semantic Segmentation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on channel pruning for semantic segmentation networks.\nThere are a large number of works to compress and accelerate deep neural\nnetworks in the classification task (e.g., ResNet-50 on ImageNet), but they\ncannot be straightforwardly applied to the semantic segmentation network that\ninvolves an implicit multi-task learning problem. To boost the segmentation\nperformance, the backbone of semantic segmentation network is often pre-trained\non a large scale classification dataset (e.g., ImageNet), and then optimized on\nthe desired segmentation dataset. Hence to identify the redundancy in\nsegmentation networks, we present a multi-task channel pruning approach. The\nimportance of each convolution filter w.r.t the channel of an arbitrary layer\nwill be simultaneously determined by the classification and segmentation tasks.\nIn addition, we develop an alternative scheme for optimizing importance scores\nof filters in the entire network. Experimental results on several benchmarks\nillustrate the superiority of the proposed algorithm over the state-of-the-art\npruning methods. Notably, we can obtain an about $2\\times$ FLOPs reduction on\nDeepLabv3 with only an about $1\\%$ mIoU drop on the PASCAL VOC 2012 dataset and\nan about $1.3\\%$ mIoU drop on Cityscapes dataset, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:03:01 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Xinghao", ""], ["Wang", "Yunhe", ""], ["Zhang", "Yiman", ""], ["Du", "Peng", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "2007.08397", "submitter": "Yen-Chi Cheng", "authors": "Yen-Chi Cheng, Hsin-Ying Lee, Min Sun, Ming-Hsuan Yang", "title": "Controllable Image Synthesis via SegVAE", "comments": "ECCV 2020. Project page: https://yccyenchicheng.github.io/SegVAE/\n  Code: https://github.com/yccyenchicheng/SegVAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible user controls are desirable for content creation and image editing.\nA semantic map is commonly used intermediate representation for conditional\nimage generation. Compared to the operation on raw RGB pixels, the semantic map\nenables simpler user modification. In this work, we specifically target at\ngenerating semantic maps given a label-set consisting of desired categories.\nThe proposed framework, SegVAE, synthesizes semantic maps in an iterative\nmanner using conditional variational autoencoder. Quantitative and qualitative\nexperiments demonstrate that the proposed model can generate realistic and\ndiverse semantic maps. We also apply an off-the-shelf image-to-image\ntranslation model to generate realistic RGB images to better understand the\nquality of the synthesized semantic maps. Furthermore, we showcase several\nreal-world image-editing applications including object removal, object\ninsertion, and object replacement.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:18:53 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 04:13:08 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Cheng", "Yen-Chi", ""], ["Lee", "Hsin-Ying", ""], ["Sun", "Min", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2007.08398", "submitter": "Hang Du", "authors": "Hang Du, Hailin Shi, Yuchi Liu, Jun Wang, Zhen Lei, Dan Zeng, Tao Mei", "title": "Semi-Siamese Training for Shallow Face Learning", "comments": "ECCV 2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing public face datasets, such as MS-Celeb-1M and VGGFace2, provide\nabundant information in both breadth (large number of IDs) and depth\n(sufficient number of samples) for training. However, in many real-world\nscenarios of face recognition, the training dataset is limited in depth, i.e.\nonly two face images are available for each ID. $\\textit{We define this\nsituation as Shallow Face Learning, and find it problematic with existing\ntraining methods.}$ Unlike deep face data, the shallow face data lacks\nintra-class diversity. As such, it can lead to collapse of feature dimension\nand consequently the learned network can easily suffer from degeneration and\nover-fitting in the collapsed dimension. In this paper, we aim to address the\nproblem by introducing a novel training method named Semi-Siamese Training\n(SST). A pair of Semi-Siamese networks constitute the forward propagation\nstructure, and the training loss is computed with an updating gallery queue,\nconducting effective optimization on shallow training data. Our method is\ndeveloped without extra-dependency, thus can be flexibly integrated with the\nexisting loss functions and network architectures. Extensive experiments on\nvarious benchmarks of face recognition show the proposed method significantly\nimproves the training, not only in shallow face learning, but also for\nconventional deep face data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:20:04 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Du", "Hang", ""], ["Shi", "Hailin", ""], ["Liu", "Yuchi", ""], ["Wang", "Jun", ""], ["Lei", "Zhen", ""], ["Zeng", "Dan", ""], ["Mei", "Tao", ""]]}, {"id": "2007.08404", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla, Vishal M Patel", "title": "Learning to Restore a Single Face Image Degraded by Atmospheric\n  Turbulence using CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atmospheric turbulence significantly affects imaging systems which use light\nthat has propagated through long atmospheric paths. Images captured under such\ncondition suffer from a combination of geometric deformation and space varying\nblur. We present a deep learning-based solution to the problem of restoring a\nturbulence-degraded face image where prior information regarding the amount of\ngeometric distortion and blur at each location of the face image is first\nestimated using two separate networks. The estimated prior information is then\nused by a network called, Turbulence Distortion Removal Network (TDRN), to\ncorrect geometric distortion and reduce blur in the face image. Furthermore, a\nnovel loss is proposed to train TDRN where first and second order image\ngradients are computed along with their confidence maps to mitigate the effect\nof turbulence degradation. Comprehensive experiments on synthetic and real face\nimages show that this framework is capable of alleviating blur and geometric\ndistortion caused by atmospheric turbulence, and significantly improves the\nvisual quality. In addition, an ablation study is performed to demonstrate the\nimprovements obtained by different modules in the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:25:08 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Yasarla", "Rajeev", ""], ["Patel", "Vishal M", ""]]}, {"id": "2007.08427", "submitter": "Daniele Meli", "authors": "Andrea Roberti, Nicola Piccinelli, Daniele Meli, Riccardo Muradore,\n  Paolo Fiorini", "title": "Improving rigid 3D calibration for robotic surgery", "comments": "Submitted to the special issue of IEEE Transactions on Medical\n  Robotics and Bionics 2020", "journal-ref": null, "doi": "10.1109/TMRB.2020.3033670", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomy is the frontier of research in robotic surgery and its aim is to\nimprove the quality of surgical procedures in the next future. One fundamental\nrequirement for autonomy is advanced perception capability through vision\nsensors. In this paper, we propose a novel calibration technique for a surgical\nscenario with da Vinci robot. Calibration of the camera and the robot is\nnecessary for precise positioning of the tools in order to emulate the high\nperformance surgeons. Our calibration technique is tailored for RGB-D camera.\nDifferent tests performed on relevant use cases for surgery prove that we\nsignificantly improve precision and accuracy with respect to the state of the\nart solutions for similar devices on a surgical-size setup. Moreover, our\ncalibration method can be easily extended to standard surgical endoscope to\nprompt its use in real surgical scenario.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:06:26 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Roberti", "Andrea", ""], ["Piccinelli", "Nicola", ""], ["Meli", "Daniele", ""], ["Muradore", "Riccardo", ""], ["Fiorini", "Paolo", ""]]}, {"id": "2007.08428", "submitter": "Chaitanya Devaguptapu", "authors": "Chaitanya Devaguptapu, Devansh Agarwal, Gaurav Mittal, Vineeth N\n  Balasubramanian", "title": "On Adversarial Robustness: A Neural Architecture Search perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:07:10 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 14:34:28 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Devaguptapu", "Chaitanya", ""], ["Agarwal", "Devansh", ""], ["Mittal", "Gaurav", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2007.08434", "submitter": "Xinqian Gu", "authors": "Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, Xilin Chen", "title": "Appearance-Preserving 3D Convolution for Video-based Person\n  Re-identification", "comments": "Accepted by ECCV2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the imperfect person detection results and posture changes, temporal\nappearance misalignment is unavoidable in video-based person re-identification\n(ReID). In this case, 3D convolution may destroy the appearance representation\nof person video clips, thus it is harmful to ReID. To address this problem, we\npropose AppearancePreserving 3D Convolution (AP3D), which is composed of two\ncomponents: an Appearance-Preserving Module (APM) and a 3D convolution kernel.\nWith APM aligning the adjacent feature maps in pixel level, the following 3D\nconvolution can model temporal information on the premise of maintaining the\nappearance representation quality. It is easy to combine AP3D with existing 3D\nConvNets by simply replacing the original 3D convolution kernels with AP3Ds.\nExtensive experiments demonstrate the effectiveness of AP3D for video-based\nReID and the results on three widely used datasets surpass the\nstate-of-the-arts. Code is available at: https://github.com/guxinqian/AP3D.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:21:34 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 10:57:04 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gu", "Xinqian", ""], ["Chang", "Hong", ""], ["Ma", "Bingpeng", ""], ["Zhang", "Hongkai", ""], ["Chen", "Xilin", ""]]}, {"id": "2007.08442", "submitter": "Hongyang Gao", "authors": "Hongyang Gao, Zhengyang Wang, Shuiwang Ji", "title": "Kronecker Attention Networks", "comments": "9 pages, KDD2020", "journal-ref": null, "doi": "10.1145/3394486.3403065", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention operators have been applied on both 1-D data like texts and\nhigher-order data such as images and videos. Use of attention operators on\nhigh-order data requires flattening of the spatial or spatial-temporal\ndimensions into a vector, which is assumed to follow a multivariate normal\ndistribution. This not only incurs excessive requirements on computational\nresources, but also fails to preserve structures in data. In this work, we\npropose to avoid flattening by assuming the data follow matrix-variate normal\ndistributions. Based on this new view, we develop Kronecker attention operators\n(KAOs) that operate on high-order tensor data directly. More importantly, the\nproposed KAOs lead to dramatic reductions in computational resources.\nExperimental results show that our methods reduce the amount of required\ncomputational resources by a factor of hundreds, with larger factors for\nhigher-dimensional and higher-order data. Results also show that networks with\nKAOs outperform models without attention, while achieving competitive\nperformance as those with original attention operators.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:26:02 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Gao", "Hongyang", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2007.08453", "submitter": "Yigit Alparslan", "authors": "Ken Alparslan, Yigit Alparslan, Matthew Burlick", "title": "Towards Evaluating Driver Fatigue with Robust Deep Learning Models", "comments": "8 pages, 12 figures, fixed typos, converted referencing to BibLatex\n  from plain text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we explore different deep learning based approaches to detect\ndriver fatigue. Drowsy driving results in approximately 72,000 crashes and\n44,000 injuries every year in the US and detecting drowsiness and alerting the\ndriver can save many lives. There have been many approaches to detect fatigue,\nof which eye closedness detection is one. We propose a framework to detect eye\nclosedness in a captured camera frame as a gateway for detecting drowsiness. We\nexplore two different datasets to detect eye closedness. We develop an eye\nmodel by using new Eye-blink dataset and a face model by using the Closed Eyes\nin the Wild (CEW). We also explore different techniques to make the models more\nrobust by adding noise. We achieve 95.84% accuracy on our eye model and 80.01%\naccuracy on our face model. We also see that we can improve our accuracy on the\nface model by 6% via adversarial training and data augmentation. We hope that\nour work will be useful to the field of driver fatigue detection to avoid\npotential vehicle accidents related to drowsy driving.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:44:49 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 21:33:11 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 16:23:44 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 06:35:21 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Alparslan", "Ken", ""], ["Alparslan", "Yigit", ""], ["Burlick", "Matthew", ""]]}, {"id": "2007.08454", "submitter": "Meng Tian", "authors": "Meng Tian, Marcelo H Ang Jr, Gim Hee Lee", "title": "Shape Prior Deformation for Categorical 6D Object Pose and Size\n  Estimation", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning approach to recover the 6D poses and sizes of\nunseen object instances from an RGB-D image. To handle the intra-class shape\nvariation, we propose a deep network to reconstruct the 3D object model by\nexplicitly modeling the deformation from a pre-learned categorical shape prior.\nAdditionally, our network infers the dense correspondences between the depth\nobservation of the object instance and the reconstructed 3D model to jointly\nestimate the 6D object pose and size. We design an autoencoder that trains on a\ncollection of object models and compute the mean latent embedding for each\ncategory to learn the categorical shape priors. Extensive experiments on both\nsynthetic and real-world datasets demonstrate that our approach significantly\noutperforms the state of the art. Our code is available at\nhttps://github.com/mentian/object-deformnet.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:45:05 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Tian", "Meng", ""], ["Ang", "Marcelo H", "Jr"], ["Lee", "Gim Hee", ""]]}, {"id": "2007.08457", "submitter": "Ning Yu", "authors": "Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, Mario Fritz", "title": "Artificial Fingerprinting for Generative Models: Rooting Deepfake\n  Attribution in Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic image generation has reached a new level of quality due to the\nbreakthroughs of generative adversarial networks (GANs). Yet, the dark side of\nsuch deepfakes, the malicious use of generated media, raises concerns about\nvisual misinformation. While existing research work on deepfake detection\ndemonstrates high accuracy, it is subject to advances in generation techniques\nand adversarial iterations on detection countermeasure techniques. Thus, we\nseek a proactive and sustainable solution on deepfake detection, that is\nagnostic to the evolution of generative models, by introducing artificial\nfingerprints into the models.\n  Our approach is simple and effective. We first embed artificial fingerprints\ninto training data, then validate a surprising discovery on the transferability\nof such fingerprints from training data to generative models, which in turn\nappears in the generated deepfakes. Experiments show that our fingerprinting\nsolution (1) holds for a variety of cutting-edge generative models, (2) leads\nto a negligible side effect on generation quality, (3) stays robust against\nimage-level and model-level perturbations, (4) stays hard to be detected by\nadversaries, and (5) converts deepfake detection and attribution into trivial\ntasks and outperforms the recent state-of-the-art baselines. Our solution\ncloses the responsibility loop between publishing pre-trained generative model\ninventions and their possible misuses, which makes it independent of the\ncurrent arms race.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:49:55 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 21:46:54 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 04:17:39 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 00:32:00 GMT"}, {"version": "v5", "created": "Wed, 31 Mar 2021 00:49:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Ning", ""], ["Skripniuk", "Vladislav", ""], ["Abdelnabi", "Sahar", ""], ["Fritz", "Mario", ""]]}, {"id": "2007.08461", "submitter": "Yikai Wang", "authors": "Yikai Wang, Li Zhang, Yuan Yao, Yanwei Fu", "title": "How to trust unlabeled data? Instance Credibility Inference for Few-Shot\n  Learning", "comments": "Journal extension of arXiv:2003.11853 that appears in CVPR 2020 Code\n  and models are released at https://github.com/Yikai-Wang/ICI-FSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based models have excelled in many computer vision tasks and\nappear to surpass humans' performance. However, these models require an\navalanche of expensive human labeled training data and many iterations to train\ntheir large number of parameters. This severely limits their scalability to the\nreal-world long-tail distributed categories, some of which are with a large\nnumber of instances, but with only a few manually annotated. Learning from such\nextremely limited labeled examples is known as Few-shot learning (FSL).\nDifferent to prior arts that leverage meta-learning or data augmentation\nstrategies to alleviate this extremely data-scarce problem, this paper presents\na statistical approach, dubbed Instance Credibility Inference (ICI) to exploit\nthe support of unlabeled instances for few-shot visual recognition. Typically,\nwe repurpose the self-taught learning paradigm to predict pseudo-labels of\nunlabeled instances with an initial classifier trained from the few shot and\nthen select the most confident ones to augment the training set to re-train the\nclassifier. This is achieved by constructing a (Generalized) Linear Model\n(LM/GLM) with incidental parameters to model the mapping from (un-)labeled\nfeatures to their (pseudo-)labels, in which the sparsity of the incidental\nparameters indicates the credibility of the corresponding pseudo-labeled\ninstance. We rank the credibility of pseudo-labeled instances along the\nregularization path of their corresponding incidental parameters, and the most\ntrustworthy pseudo-labeled examples are preserved as the augmented labeled\ninstances. Theoretically, under mild conditions of restricted eigenvalue,\nirrepresentability, and large error, our approach is guaranteed to collect all\nthe correctly-predicted instances from the noisy pseudo-labeled set.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 03:38:09 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 05:08:56 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 08:38:35 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 03:21:15 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Yikai", ""], ["Zhang", "Li", ""], ["Yao", "Yuan", ""], ["Fu", "Yanwei", ""]]}, {"id": "2007.08463", "submitter": "Antonia Breuer", "authors": "Antonia Breuer, Jan-Aike Term\\\"ohlen, Silviu Homoceanu, Tim\n  Fingscheidt", "title": "openDD: A Large-Scale Roundabout Drone Dataset", "comments": "ITSC 2020 Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing and predicting the traffic scene around the ego vehicle has been\none of the key challenges in autonomous driving. Datasets including the\ntrajectories of all road users present in a scene, as well as the underlying\nroad topology are invaluable to analyze the behavior of the different traffic\nparticipants. The interaction between the various traffic participants is\nespecially high in intersection types that are not regulated by traffic lights,\nthe most common one being the roundabout.\n  We introduce the openDD dataset, including 84,774 accurately tracked\ntrajectories and HD map data of seven different roundabouts. The openDD dataset\nis annotated using images taken by a drone in 501 separate flights, totalling\nin over 62 hours of trajectory data. As of today, openDD is by far the largest\npublicly available trajectory dataset recorded from a drone perspective, while\ncomparable datasets span 17 hours at most.\n  The data is available, for both commercial and noncommercial use, at:\nhttp://www.l3pilot.eu/openDD.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:01:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Breuer", "Antonia", ""], ["Term\u00f6hlen", "Jan-Aike", ""], ["Homoceanu", "Silviu", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2007.08473", "submitter": "Julian Bitterwolf", "authors": "Julian Bitterwolf, Alexander Meinke and Matthias Hein", "title": "Certifiably Adversarially Robust Detection of Out-of-Distribution Data", "comments": "Published and presented at NeurIPS 2020. Code available at\n  https://gitlab.com/Bitterwolf/GOOD v3: added missing acknowledgement", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be overconfident when applied to\nout-of-distribution (OOD) inputs which clearly do not belong to any class. This\nis a problem in safety-critical applications since a reliable assessment of the\nuncertainty of a classifier is a key property, allowing the system to trigger\nhuman intervention or to transfer into a safe state. In this paper, we aim for\ncertifiable worst case guarantees for OOD detection by enforcing not only low\nconfidence at the OOD point but also in an $l_\\infty$-ball around it. For this\npurpose, we use interval bound propagation (IBP) to upper bound the maximal\nconfidence in the $l_\\infty$-ball and minimize this upper bound during training\ntime. We show that non-trivial bounds on the confidence for OOD data\ngeneralizing beyond the OOD dataset seen at training time are possible.\nMoreover, in contrast to certified adversarial robustness which typically comes\nwith significant loss in prediction performance, certified guarantees for worst\ncase OOD detection are possible without much loss in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:16:47 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 16:12:57 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 15:55:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bitterwolf", "Julian", ""], ["Meinke", "Alexander", ""], ["Hein", "Matthias", ""]]}, {"id": "2007.08478", "submitter": "Yimin Liu", "authors": "Yimin Liu, Louis J. Durlofsky", "title": "3D CNN-PCA: A Deep-Learning-Based Parameterization for Complex Geomodels", "comments": null, "journal-ref": null, "doi": "10.1016/j.cageo.2020.104676", "report-no": null, "categories": "cs.CV cs.CE cs.LG physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geological parameterization enables the representation of geomodels in terms\nof a relatively small set of variables. Parameterization is therefore very\nuseful in the context of data assimilation and uncertainty quantification. In\nthis study, a deep-learning-based geological parameterization algorithm,\nCNN-PCA, is developed for complex 3D geomodels. CNN-PCA entails the use of\nconvolutional neural networks as a post-processor for the low-dimensional\nprincipal component analysis representation of a geomodel. The 3D treatments\npresented here differ somewhat from those used in the 2D CNN-PCA procedure.\nSpecifically, we introduce a new supervised-learning-based reconstruction loss,\nwhich is used in combination with style loss and hard data loss. The style loss\nuses features extracted from a 3D CNN pretrained for video classification. The\n3D CNN-PCA algorithm is applied for the generation of conditional 3D\nrealizations, defined on $60\\times60\\times40$ grids, for three geological\nscenarios (binary and bimodal channelized systems, and a three-facies\nchannel-levee-mud system). CNN-PCA realizations are shown to exhibit geological\nfeatures that are visually consistent with reference models generated using\nobject-based methods. Statistics of flow responses ($\\text{P}_{10}$,\n$\\text{P}_{50}$, $\\text{P}_{90}$ percentile results) for test sets of 3D\nCNN-PCA models are shown to be in consistent agreement with those from\nreference geomodels. Lastly, CNN-PCA is successfully applied for history\nmatching with ESMDA for the bimodal channelized system.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:25:14 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Liu", "Yimin", ""], ["Durlofsky", "Louis J.", ""]]}, {"id": "2007.08480", "submitter": "Olivia Wiles", "authors": "Olivia Wiles, Sebastien Ehrhardt, Andrew Zisserman", "title": "Co-Attention for Conditioned Image Matching", "comments": "Accepted at CVPR 2021. Project page:\n  https://www.robots.ox.ac.uk/~ow/coam.html. Formerly D2D: Learning to find\n  good correspondences for image matching and manipulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new approach to determine correspondences between image pairs in\nthe wild under large changes in illumination, viewpoint, context, and material.\nWhile other approaches find correspondences between pairs of images by treating\nthe images independently, we instead condition on both images to implicitly\ntake account of the differences between them. To achieve this, we introduce (i)\na spatial attention mechanism (a co-attention module, CoAM) for conditioning\nthe learned features on both images, and (ii) a distinctiveness score used to\nchoose the best matches at test time. CoAM can be added to standard\narchitectures and trained using self-supervision or supervised data, and\nachieves a significant performance improvement under hard conditions, e.g.\nlarge viewpoint changes. We demonstrate that models using CoAM achieve state of\nthe art or competitive results on a wide range of tasks: local matching, camera\nlocalization, 3D reconstruction, and image stylization.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:32:00 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 17:10:13 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wiles", "Olivia", ""], ["Ehrhardt", "Sebastien", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2007.08488", "submitter": "Li Yi", "authors": "Li Yi, Boqing Gong, Thomas Funkhouser", "title": "Complete & Label: A Domain Adaptation Approach to Semantic Segmentation\n  of LiDAR Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an unsupervised domain adaptation problem for the semantic labeling\nof 3D point clouds, with a particular focus on domain discrepancies induced by\ndifferent LiDAR sensors. Based on the observation that sparse 3D point clouds\nare sampled from 3D surfaces, we take a Complete and Label approach to recover\nthe underlying surfaces before passing them to a segmentation network.\nSpecifically, we design a Sparse Voxel Completion Network (SVCN) to complete\nthe 3D surfaces of a sparse point cloud. Unlike semantic labels, to obtain\ntraining pairs for SVCN requires no manual labeling. We also introduce local\nadversarial learning to model the surface prior. The recovered 3D surfaces\nserve as a canonical domain, from which semantic labels can transfer across\ndifferent LiDAR sensors. Experiments and ablation studies with our new\nbenchmark for cross-domain semantic labeling of LiDAR data show that the\nproposed approach provides 8.2-36.6% better performance than previous domain\nadaptation methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:42:05 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 23:12:59 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yi", "Li", ""], ["Gong", "Boqing", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2007.08489", "submitter": "Andrew Ilyas", "authors": "Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander\n  Madry", "title": "Do Adversarially Robust ImageNet Models Transfer Better?", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a widely-used paradigm in deep learning, where models\npre-trained on standard datasets can be efficiently adapted to downstream\ntasks. Typically, better pre-trained models yield better transfer results,\nsuggesting that initial accuracy is a key aspect of transfer learning\nperformance. In this work, we identify another such aspect: we find that\nadversarially robust models, while less accurate, often perform better than\ntheir standard-trained counterparts when used for transfer learning.\nSpecifically, we focus on adversarially robust ImageNet classifiers, and show\nthat they yield improved accuracy on a standard suite of downstream\nclassification tasks. Further analysis uncovers more differences between robust\nand standard models in the context of transfer learning. Our results are\nconsistent with (and in fact, add to) recent hypotheses stating that robustness\nleads to improved feature representations. Our code and models are available at\nhttps://github.com/Microsoft/robust-models-transfer .\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:42:40 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 01:56:10 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Salman", "Hadi", ""], ["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Kapoor", "Ashish", ""], ["Madry", "Aleksander", ""]]}, {"id": "2007.08494", "submitter": "Danfeng Hong", "authors": "Xin Wu and Wei Li and Danfeng Hong and Jiaojiao Tian and Ran Tao and\n  Qian Du", "title": "Vehicle Detection of Multi-source Remote Sensing Data Using Active\n  Fine-tuning Network", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing,167:39-53,2020", "doi": "10.1016/j.isprsjprs.2020.06.016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection in remote sensing images has attracted increasing interest\nin recent years. However, its detection ability is limited due to lack of\nwell-annotated samples, especially in densely crowded scenes. Furthermore,\nsince a list of remotely sensed data sources is available, efficient\nexploitation of useful information from multi-source data for better vehicle\ndetection is challenging. To solve the above issues, a multi-source active\nfine-tuning vehicle detection (Ms-AFt) framework is proposed, which integrates\ntransfer learning, segmentation, and active classification into a unified\nframework for auto-labeling and detection. The proposed Ms-AFt employs a\nfine-tuning network to firstly generate a vehicle training set from an\nunlabeled dataset. To cope with the diversity of vehicle categories, a\nmulti-source based segmentation branch is then designed to construct additional\ncandidate object sets. The separation of high quality vehicles is realized by a\ndesigned attentive classifications network. Finally, all three branches are\ncombined to achieve vehicle detection. Extensive experimental results conducted\non two open ISPRS benchmark datasets, namely the Vaihingen village and Potsdam\ncity datasets, demonstrate the superiority and effectiveness of the proposed\nMs-AFt for vehicle detection. In addition, the generalization ability of Ms-AFt\nin dense remote sensing scenes is further verified on stereo aerial imagery of\na large camping site.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:46:46 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wu", "Xin", ""], ["Li", "Wei", ""], ["Hong", "Danfeng", ""], ["Tian", "Jiaojiao", ""], ["Tao", "Ran", ""], ["Du", "Qian", ""]]}, {"id": "2007.08501", "submitter": "Georgia Gkioxari", "authors": "Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon,\n  Wan-Yen Lo, Justin Johnson, Georgia Gkioxari", "title": "Accelerating 3D Deep Learning with PyTorch3D", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has significantly improved 2D image recognition. Extending into\n3D may advance many new applications including autonomous vehicles, virtual and\naugmented reality, authoring 3D content, and even improving 2D recognition.\nHowever despite growing interest, 3D deep learning remains relatively\nunderexplored. We believe that some of this disparity is due to the engineering\nchallenges involved in 3D deep learning, such as efficiently processing\nheterogeneous data and reframing graphics operations to be differentiable. We\naddress these challenges by introducing PyTorch3D, a library of modular,\nefficient, and differentiable operators for 3D deep learning. It includes a\nfast, modular differentiable renderer for meshes and point clouds, enabling\nanalysis-by-synthesis approaches. Compared with other differentiable renderers,\nPyTorch3D is more modular and efficient, allowing users to more easily extend\nit while also gracefully scaling to large meshes and images. We compare the\nPyTorch3D operators and renderer with other implementations and demonstrate\nsignificant speed and memory improvements. We also use PyTorch3D to improve the\nstate-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D\nimages on ShapeNet. PyTorch3D is open-source and we hope it will help\naccelerate research in 3D deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:53:02 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ravi", "Nikhila", ""], ["Reizenstein", "Jeremy", ""], ["Novotny", "David", ""], ["Gordon", "Taylor", ""], ["Lo", "Wan-Yen", ""], ["Johnson", "Justin", ""], ["Gkioxari", "Georgia", ""]]}, {"id": "2007.08504", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Nilesh Kulkarni, Abhinav Gupta", "title": "Implicit Mesh Reconstruction from Unannotated Image Collections", "comments": "Project page: https://shubhtuls.github.io/imr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to infer the 3D shape, texture, and camera pose for an\nobject from a single RGB image, using only category-level image collections\nwith foreground masks as supervision. We represent the shape as an\nimage-conditioned implicit function that transforms the surface of a sphere to\nthat of the predicted mesh, while additionally predicting the corresponding\ntexture. To derive supervisory signal for learning, we enforce that: a) our\npredictions when rendered should explain the available image evidence, and b)\nthe inferred 3D structure should be geometrically consistent with learned pixel\nto surface mappings. We empirically show that our approach improves over prior\nwork that leverages similar supervision, and in fact performs competitively to\nmethods that use stronger supervision. Finally, as our method enables learning\nwith limited supervision, we qualitatively demonstrate its applicability over a\nset of about 30 object categories.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:55:20 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Kulkarni", "Nilesh", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2007.08505", "submitter": "Chia-Wen Kuo", "authors": "Chia-Wen Kuo and Chih-Yao Ma and Jia-Bin Huang and Zsolt Kira", "title": "FeatMatch: Feature-Based Augmentation for Semi-Supervised Learning", "comments": "Paper accepted in ECCV 2020. Project page:\n  https://sites.google.com/view/chiawen-kuo/home/featmatch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art semi-supervised learning (SSL) methods use a\ncombination of image-based transformations and consistency regularization as\ncore components. Such methods, however, are limited to simple transformations\nsuch as traditional data augmentation or convex combinations of two images. In\nthis paper, we propose a novel learned feature-based refinement and\naugmentation method that produces a varied set of complex transformations.\nImportantly, these transformations also use information from both within-class\nand across-class prototypical representations that we extract through\nclustering. We use features already computed across iterations by storing them\nin a memory bank, obviating the need for significant extra computation. These\ntransformations, combined with traditional image-based augmentation, are then\nused as part of the consistency-based regularization loss. We demonstrate that\nour method is comparable to current state of art for smaller datasets (CIFAR-10\nand SVHN) while being able to scale up to larger datasets such as CIFAR-100 and\nmini-Imagenet where we achieve significant gains over the state of art\n(\\textit{e.g.,} absolute 17.44\\% gain on mini-ImageNet). We further test our\nmethod on DomainNet, demonstrating better robustness to out-of-domain unlabeled\ndata, and perform rigorous ablations and analysis to validate the method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:55:31 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Kuo", "Chia-Wen", ""], ["Ma", "Chih-Yao", ""], ["Huang", "Jia-Bin", ""], ["Kira", "Zsolt", ""]]}, {"id": "2007.08508", "submitter": "Han Hu", "authors": "Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, Han Hu", "title": "RepPoints V2: Verification Meets Regression for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification and regression are two general methodologies for prediction in\nneural networks. Each has its own strengths: verification can be easier to\ninfer accurately, and regression is more efficient and applicable to continuous\ntarget variables. Hence, it is often beneficial to carefully combine them to\ntake advantage of their benefits. In this paper, we take this philosophy to\nimprove state-of-the-art object detection, specifically by RepPoints. Though\nRepPoints provides high performance, we find that its heavy reliance on\nregression for object localization leaves room for improvement. We introduce\nverification tasks into the localization prediction of RepPoints, producing\nRepPoints v2, which provides consistent improvements of about 2.0 mAP over the\noriginal RepPoints on the COCO object detection benchmark using different\nbackbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO\n\\texttt{test-dev} by a single model. Moreover, we show that the proposed\napproach can more generally elevate other object detection frameworks as well\nas applications such as instance segmentation. The code is available at\nhttps://github.com/Scalsol/RepPointsV2.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:57:08 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Yihong", ""], ["Zhang", "Zheng", ""], ["Cao", "Yue", ""], ["Wang", "Liwei", ""], ["Lin", "Stephen", ""], ["Hu", "Han", ""]]}, {"id": "2007.08509", "submitter": "Arun Mallya", "authors": "Arun Mallya, Ting-Chun Wang, Karan Sapra, Ming-Yu Liu", "title": "World-Consistent Video-to-Video Synthesis", "comments": "Published at the European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-to-video synthesis (vid2vid) aims for converting high-level semantic\ninputs to photorealistic videos. While existing vid2vid methods can achieve\nshort-term temporal consistency, they fail to ensure the long-term one. This is\nbecause they lack knowledge of the 3D world being rendered and generate each\nframe only based on the past few frames. To address the limitation, we\nintroduce a novel vid2vid framework that efficiently and effectively utilizes\nall past generated frames during rendering. This is achieved by condensing the\n3D world rendered so far into a physically-grounded estimate of the current\nframe, which we call the guidance image. We further propose a novel neural\nnetwork architecture to take advantage of the information stored in the\nguidance images. Extensive experimental results on several challenging datasets\nverify the effectiveness of our approach in achieving world consistency - the\noutput video is consistent within the entire rendered 3D world.\n  https://nvlabs.github.io/wc-vid2vid/\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:58:13 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Mallya", "Arun", ""], ["Wang", "Ting-Chun", ""], ["Sapra", "Karan", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "2007.08513", "submitter": "Hung-Yu Tseng", "authors": "Hung-Yu Tseng, Hsin-Ying Lee, Lu Jiang, Ming-Hsuan Yang, Weilong Yang", "title": "RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation from scene description is a cornerstone technique for the\ncontrolled generation, which is beneficial to applications such as content\ncreation and image editing. In this work, we aim to synthesize images from\nscene description with retrieved patches as reference. We propose a\ndifferentiable retrieval module. With the differentiable retrieval module, we\ncan (1) make the entire pipeline end-to-end trainable, enabling the learning of\nbetter feature embedding for retrieval; (2) encourage the selection of mutually\ncompatible patches with additional objective functions. We conduct extensive\nquantitative and qualitative experiments to demonstrate that the proposed\nmethod can generate realistic and diverse images, where the retrieved patches\nare reasonable and mutually compatible.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:59:04 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Tseng", "Hung-Yu", ""], ["Lee", "Hsin-Ying", ""], ["Jiang", "Lu", ""], ["Yang", "Ming-Hsuan", ""], ["Yang", "Weilong", ""]]}, {"id": "2007.08517", "submitter": "Armaan Pishori", "authors": "Armaan Pishori, Brittany Rollins, Nicolas van Houten, Nisha Chatwani,\n  Omar Uraimov", "title": "Detecting Deepfake Videos: An Analysis of Three Techniques", "comments": "11 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deepfake generating algorithms that produce manipulated\nmedia have had dangerous implications in privacy, security and mass\ncommunication. Efforts to combat this issue have risen in the form of\ncompetitions and funding for research to detect deepfakes. This paper presents\nthree techniques and algorithms: convolutional LSTM, eye blink detection and\ngrayscale histograms-pursued while participating in the Deepfake Detection\nChallenge. We assessed the current knowledge about deepfake videos, a more\nsevere version of manipulated media, and previous methods used, and found\nrelevance in the grayscale histogram technique over others. We discussed the\nimplications of each method developed and provided further steps to improve the\ngiven findings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:36:23 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Pishori", "Armaan", ""], ["Rollins", "Brittany", ""], ["van Houten", "Nicolas", ""], ["Chatwani", "Nisha", ""], ["Uraimov", "Omar", ""]]}, {"id": "2007.08547", "submitter": "Lele Chen", "authors": "Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, and\n  Chenliang Xu", "title": "Talking-head Generation with Rhythmic Head Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people deliver a speech, they naturally move heads, and this rhythmic\nhead motion conveys prosodic information. However, generating a lip-synced\nvideo while moving head naturally is challenging. While remarkably successful,\nexisting works either generate still talkingface videos or rely on\nlandmark/video frames as sparse/dense mapping guidance to generate head\nmovements, which leads to unrealistic or uncontrollable video synthesis. To\novercome the limitations, we propose a 3D-aware generative network along with a\nhybrid embedding module and a non-linear composition module. Through modeling\nthe head motion and facial expressions1 explicitly, manipulating 3D animation\ncarefully, and embedding reference images dynamically, our approach achieves\ncontrollable, photo-realistic, and temporally coherent talking-head videos with\nnatural head movements. Thoughtful experiments on several standard benchmarks\ndemonstrate that our method achieves significantly better results than the\nstate-of-the-art methods in both quantitative and qualitative comparisons. The\ncode is available on https://github.com/\nlelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:13:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chen", "Lele", ""], ["Cui", "Guofeng", ""], ["Liu", "Celong", ""], ["Li", "Zhong", ""], ["Kou", "Ziyi", ""], ["Xu", "Yi", ""], ["Xu", "Chenliang", ""]]}, {"id": "2007.08551", "submitter": "Sichen Zhao", "authors": "Wei Shao, Sichen Zhao, Zhen Zhang, Shiyu Wang, Mohammad Saiedur\n  Rahaman, Andy Song, Flora Dilys Salim", "title": "FADACS: A Few-shot Adversarial Domain Adaptation Architecture for\n  Context-Aware Parking Availability Sensing", "comments": "9 pages, 3 gifures, 7 tables, Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research on parking availability sensing mainly relies on extensive\ncontextual and historical information. In practice, the availability of such\ninformation is a challenge as it requires continuous collection of sensory\nsignals. In this study, we design an end-to-end transfer learning framework for\nparking availability sensing to predict parking occupancy in areas in which the\nparking data is insufficient to feed into data-hungry models. This framework\novercomes two main challenges: 1) many real-world cases cannot provide enough\ndata for most existing data-driven models, and 2) it is difficult to merge\nsensor data and heterogeneous contextual information due to the differing urban\nfabric and spatial characteristics. Our work adopts a widely-used concept,\nadversarial domain adaptation, to predict the parking occupancy in an area\nwithout abundant sensor data by leveraging data from other areas with similar\nfeatures. In this paper, we utilise more than 35 million parking data records\nfrom sensors placed in two different cities, one a city centre and the other a\ncoastal tourist town. We also utilise heterogeneous spatio-temporal contextual\ninformation from external resources, including weather and points of interest.\nWe quantify the strength of our proposed framework in different cases and\ncompare it to the existing data-driven approaches. The results show that the\nproposed framework is comparable to existing state-of-the-art methods and also\nprovide some valuable insights on parking availability prediction.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:25:26 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 01:05:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shao", "Wei", ""], ["Zhao", "Sichen", ""], ["Zhang", "Zhen", ""], ["Wang", "Shiyu", ""], ["Rahaman", "Mohammad Saiedur", ""], ["Song", "Andy", ""], ["Salim", "Flora Dilys", ""]]}, {"id": "2007.08553", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, Jagadeesan Jayender", "title": "Smooth Deformation Field-based Mismatch Removal in Real-time", "comments": "submitted for peer review since 10/2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the mismatch removal problem, which may serve as the\nsubsequent step of feature matching. Non-rigid deformation makes it difficult\nto remove mismatches because no parametric transformation can be found. To\nsolve this problem, we first propose an algorithm based on the re-weighting and\n1-point RANSAC strategy (R1P-RNSC), which is a parametric method under a\nreasonable assumption that the non-rigid deformation can be approximately\nrepresented by multiple locally rigid transformations. R1P-RNSC is fast but\nsuffers from a drawback that the local smoothing information cannot be taken\ninto account. Then, we propose a non-parametric algorithm based on the\nexpectation maximization algorithm and dual quaternion (EMDQ) representation to\ngenerate the smooth deformation field. The two algorithms compensate for the\ndrawbacks of each other. Specifically, EMDQ needs good initial values provided\nby R1P-RNSC, and R1P-RNSC needs EMDQ for refinement. Experimental results with\nreal-world data demonstrate that the combination of the two algorithms has the\nbest accuracy compared to other state-of-the-art methods, which can handle up\nto 85% of outliers in real-time. The ability to generate dense deformation\nfield from sparse matches with outliers in real-time makes the proposed\nalgorithms have many potential applications, such as non-rigid registration and\nSLAM.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:20:25 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhou", "Haoyin", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "2007.08554", "submitter": "Neofytos Dimitriou", "authors": "Neofytos Dimitriou, Ognjen Arandjelovic", "title": "A New Look at Ghost Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Batch normalization (BatchNorm) is an effective yet poorly understood\ntechnique for neural network optimization. It is often assumed that the\ndegradation in BatchNorm performance to smaller batch sizes stems from it\nhaving to estimate layer statistics using smaller sample sizes. However,\nrecently, Ghost normalization (GhostNorm), a variant of BatchNorm that\nexplicitly uses smaller sample sizes for normalization, has been shown to\nimprove upon BatchNorm in some datasets. Our contributions are: (i) we uncover\na source of regularization that is unique to GhostNorm, and not simply an\nextension from BatchNorm, (ii) three types of GhostNorm implementations are\ndescribed, two of which employ BatchNorm as the underlying normalization\ntechnique, (iii) by visualising the loss landscape of GhostNorm, we observe\nthat GhostNorm consistently decreases the smoothness when compared to\nBatchNorm, (iv) we introduce Sequential Normalization (SeqNorm), and report\nsuperior performance over state-of-the-art methodologies on both CIFAR--10 and\nCIFAR--100 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:23:52 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Dimitriou", "Neofytos", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "2007.08556", "submitter": "Jun Wang", "authors": "Jun Wang, Shiyi Lan, Mingfei Gao, Larry S. Davis", "title": "InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic\n  Information Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time 3D object detection is crucial for autonomous cars. Achieving\npromising performance with high efficiency, voxel-based approaches have\nreceived considerable attention. However, previous methods model the input\nspace with features extracted from equally divided sub-regions without\nconsidering that point cloud is generally non-uniformly distributed over the\nspace. To address this issue, we propose a novel 3D object detection framework\nwith dynamic information modeling. The proposed framework is designed in a\ncoarse-to-fine manner. Coarse predictions are generated in the first stage via\na voxel-based region proposal network. We introduce InfoFocus, which improves\nthe coarse detections by adaptively refining features guided by the information\nof point cloud density. Experiments are conducted on the large-scale nuScenes\n3D detection benchmark. Results show that our framework achieves the\nstate-of-the-art performance with 31 FPS and improves our baseline\nsignificantly by 9.0% mAP on the nuScenes test set.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:27:08 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wang", "Jun", ""], ["Lan", "Shiyi", ""], ["Gao", "Mingfei", ""], ["Davis", "Larry S.", ""]]}, {"id": "2007.08558", "submitter": "Josip Djolonga", "authors": "Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas\n  Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander\n  D'Amour, Dan Moldovan, Sylvain Gelly, Neil Houlsby, Xiaohua Zhai, Mario Lucic", "title": "On Robustness and Transferability of Convolutional Neural Networks", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep convolutional networks (CNNs) are often criticized for not\ngeneralizing under distributional shifts. However, several recent breakthroughs\nin transfer learning suggest that these networks can cope with severe\ndistribution shifts and successfully adapt to new tasks from a few training\nexamples. In this work we study the interplay between out-of-distribution and\ntransfer performance of modern image classification CNNs for the first time and\ninvestigate the impact of the pre-training data size, the model scale, and the\ndata preprocessing pipeline. We find that increasing both the training set and\nmodel sizes significantly improve the distributional shift robustness.\nFurthermore, we show that, perhaps surprisingly, simple changes in the\npreprocessing such as modifying the image resolution can significantly mitigate\nrobustness issues in some cases. Finally, we outline the shortcomings of\nexisting robustness evaluation datasets and introduce a synthetic dataset\nSI-Score we use for a systematic analysis across factors of variation common in\nvisual data such as object size and position.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:39:04 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 16:31:47 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Djolonga", "Josip", ""], ["Yung", "Jessica", ""], ["Tschannen", "Michael", ""], ["Romijnders", "Rob", ""], ["Beyer", "Lucas", ""], ["Kolesnikov", "Alexander", ""], ["Puigcerver", "Joan", ""], ["Minderer", "Matthias", ""], ["D'Amour", "Alexander", ""], ["Moldovan", "Dan", ""], ["Gelly", "Sylvain", ""], ["Houlsby", "Neil", ""], ["Zhai", "Xiaohua", ""], ["Lucic", "Mario", ""]]}, {"id": "2007.08566", "submitter": "Fernando Alonso-Fernandez", "authors": "Fernando Alonso-Fernandez, Javier Barrachina, Kevin Hernandez-Diaz,\n  Josef Bigun", "title": "SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses\n  for Mobile Platforms", "comments": null, "journal-ref": "Published at ICPR 2020/WMWB IAPR TC4 Workshop on Mobile and\n  Wearable Biometrics. Presentation available at https://youtu.be/suJmO8IWp8k", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual applications through mobile platforms are one of the most critical\nand ever-growing fields in AI, where ubiquitous and real-time person\nauthentication has become critical after the breakthrough of all services\nprovided via mobile devices. In this context, face verification technologies\ncan provide reliable and robust user authentication, given the availability of\ncameras in these devices, as well as their widespread use in everyday\napplications. The rapid development of deep Convolutional Neural Networks has\nresulted in many accurate face verification architectures. However, their\ntypical size (hundreds of megabytes) makes them infeasible to be incorporated\nin downloadable mobile applications where the entire file typically may not\nexceed 100 Mb. Accordingly, we address the challenge of developing a\nlightweight face recognition network of just a few megabytes that can operate\nwith sufficient accuracy in comparison to much larger models. The network also\nshould be able to operate under different poses, given the variability\nnaturally observed in uncontrolled environments where mobile devices are\ntypically used. In this paper, we adapt the lightweight SqueezeNet model, of\njust 4.4MB, to effectively provide cross-pose face recognition. After trained\non the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23%\non the difficult frontal vs. profile comparison, and0.54% on profile vs.\nprofile images. Under less extreme variations involving frontal images in any\nof the enrolment/query images pair, EER is pushed down to<0.3%, and the FRR at\nFAR=0.1%to less than 1%. This makes our light model suitable for face\nrecognition where at least acquisition of the enrolment image can be\ncontrolled. At the cost of a slight degradation in performance, we also test an\neven lighter model (of just 2.5MB) where regular convolutions are replaced with\ndepth-wise separable convolutions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:02:38 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:57:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Alonso-Fernandez", "Fernando", ""], ["Barrachina", "Javier", ""], ["Hernandez-Diaz", "Kevin", ""], ["Bigun", "Josef", ""]]}, {"id": "2007.08574", "submitter": "Philip Jackson", "authors": "Philip T. Jackson, Stephen Bonner, Ning Jia, Christopher Holder, Jon\n  Stonehouse, Boguslaw Obara", "title": "Camera Bias in a Fine Grained Classification Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that correlations between the camera used to acquire an image and the\nclass label of that image can be exploited by convolutional neural networks\n(CNN), resulting in a model that \"cheats\" at an image classification task by\nrecognizing which camera took the image and inferring the class label from the\ncamera. We show that models trained on a dataset with camera / label\ncorrelations do not generalize well to images in which those correlations are\nabsent, nor to images from unencountered cameras. Furthermore, we investigate\nwhich visual features they are exploiting for camera recognition. Our\nexperiments present evidence against the importance of global color statistics,\nlens deformation and chromatic aberration, and in favor of high frequency\nfeatures, which may be introduced by image processing algorithms built into the\ncameras.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:18:49 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Jackson", "Philip T.", ""], ["Bonner", "Stephen", ""], ["Jia", "Ning", ""], ["Holder", "Christopher", ""], ["Stonehouse", "Jon", ""], ["Obara", "Boguslaw", ""]]}, {"id": "2007.08576", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, Jagadeesan Jayender", "title": "Real-time Surface Deformation Recovery from Stereo Videos", "comments": "In International Conference on Medical Image Computing and\n  Computer-Assisted Intervention (MICCAI) (pp. 339-347). Springer, Cham", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue deformation during the surgery may significantly decrease the accuracy\nof surgical navigation systems. In this paper, we propose an approach to\nestimate the deformation of tissue surface from stereo videos in real-time,\nwhich is capable of handling occlusion, smooth surface and fast deformation. We\nfirst use a stereo matching method to extract depth information from stereo\nvideo frames and generate the tissue template, and then estimate the\ndeformation of the obtained template by minimizing ICP, ORB feature matching\nand as-rigid-as-possible (ARAP) costs. The main novelties are twofold: (1) Due\nto non-rigid deformation, feature matching outliers are difficult to be removed\nby traditional RANSAC methods; therefore we propose a novel 1-point RANSAC and\nreweighting method to preselect matching inliers, which handles smooth surfaces\nand fast deformations. (2) We propose a novel ARAP cost function based on dense\nconnections between the control points to achieve better smoothing performance\nwith limited number of iterations. Algorithms are designed and implemented for\nGPU parallel computing. Experiments on ex- and in vivo data showed that this\napproach works at an update rate of 15Hz with an accuracy of less than 2.5 mm\non a NVIDIA Titan X GPU.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:24:47 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhou", "Haoyin", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "2007.08577", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, Tao Zhang, Jagadeesan Jayender", "title": "Re-weighting and 1-Point RANSAC-Based PnP Solution to Handle Outliers", "comments": "https://github.com/haoyinzhou/PnP_Toolbox", "journal-ref": "IEEE transactions on pattern analysis and machine intelligence 41,\n  no. 12 (2018): 3022-3033", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to handle outliers is essential for performing the\nperspective-n-point (PnP) approach in practical applications, but conventional\nRANSAC+P3P or P4P methods have high time complexities. We propose a fast PnP\nsolution named R1PPnP to handle outliers by utilizing a soft re-weighting\nmechanism and the 1-point RANSAC scheme. We first present a PnP algorithm,\nwhich serves as the core of R1PPnP, for solving the PnP problem in outlier-free\nsituations. The core algorithm is an optimal process minimizing an objective\nfunction conducted with a random control point. Then, to reduce the impact of\noutliers, we propose a reprojection error-based re-weighting method and\nintegrate it into the core algorithm. Finally, we employ the 1-point RANSAC\nscheme to try different control points. Experiments with synthetic and\nreal-world data demonstrate that R1PPnP is faster than RANSAC+P3P or P4P\nmethods especially when the percentage of outliers is large, and is accurate.\nBesides, comparisons with outlier-free synthetic data show that R1PPnP is among\nthe most accurate and fast PnP solutions, which usually serve as the final\nrefinement step of RANSAC+P3P or P4P. Compared with REPPnP, which is the\nstate-of-the-art PnP algorithm with an explicit outliers-handling mechanism,\nR1PPnP is slower but does not suffer from the percentage of outliers limitation\nas REPPnP.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:28:17 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhou", "Haoyin", ""], ["Zhang", "Tao", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "2007.08592", "submitter": "Xiong Zhou", "authors": "Xiong Zhou and Saurabh Prasad", "title": "Advances in Deep Learning for Hyperspectral Image Analysis--Addressing\n  Challenges Arising in Practical Imaging Scenarios", "comments": "Published as a chapter in Hyperspectral Image Analysis. Advances in\n  Computer Vision and Pattern Recognition", "journal-ref": null, "doi": "10.1007/978-3-030-38617-7_5", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proven to be very effective for computer vision\ntasks, such as image classification, object detection, and semantic\nsegmentation -- these are primarily applied to color imagery and video. In\nrecent years, there has been an emergence of deep learning algorithms being\napplied to hyperspectral and multispectral imagery for remote sensing and\nbiomedicine tasks. These multi-channel images come with their own unique set of\nchallenges that must be addressed for effective image analysis. Challenges\ninclude limited ground truth (annotation is expensive and extensive labeling is\noften not feasible), and high dimensional nature of the data (each pixel is\nrepresented by hundreds of spectral bands), despite being presented by a large\namount of unlabeled data and the potential to leverage multiple sensors/sources\nthat observe the same scene. In this chapter, we will review recent advances in\nthe community that leverage deep learning for robust hyperspectral image\nanalysis despite these unique challenges -- specifically, we will review\nunsupervised, semi-supervised and active learning approaches to image analysis,\nas well as transfer learning approaches for multi-source (e.g. multi-sensor, or\nmulti-temporal) image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:51:02 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhou", "Xiong", ""], ["Prasad", "Saurabh", ""]]}, {"id": "2007.08614", "submitter": "Stanley Chan", "authors": "Yiheng Chi, Abhiram Gnanasambandam, Vladlen Koltun, Stanley H. Chan", "title": "Dynamic Low-light Imaging with Quanta Image Sensors", "comments": "Published in the 16th European Conference on Computer Vision (ECCV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging in low light is difficult because the number of photons arriving at\nthe sensor is low. Imaging dynamic scenes in low-light environments is even\nmore difficult because as the scene moves, pixels in adjacent frames need to be\naligned before they can be denoised. Conventional CMOS image sensors (CIS) are\nat a particular disadvantage in dynamic low-light settings because the exposure\ncannot be too short lest the read noise overwhelms the signal. We propose a\nsolution using Quanta Image Sensors (QIS) and present a new image\nreconstruction algorithm. QIS are single-photon image sensors with photon\ncounting capabilities. Studies over the past decade have confirmed the\neffectiveness of QIS for low-light imaging but reconstruction algorithms for\ndynamic scenes in low light remain an open problem. We fill the gap by\nproposing a student-teacher training protocol that transfers knowledge from a\nmotion teacher and a denoising teacher to a student network. We show that\ndynamic scenes can be reconstructed from a burst of frames at a photon level of\n1 photon per pixel per frame. Experimental results confirm the advantages of\nthe proposed method compared to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:29:52 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chi", "Yiheng", ""], ["Gnanasambandam", "Abhiram", ""], ["Koltun", "Vladlen", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2007.08617", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Adriana Kovashka", "title": "Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of multimodal data (e.g. social media posts) has inspired\ninterest in cross-modal retrieval methods. Popular approaches rely on a variety\nof metric learning losses, which prescribe what the proximity of image and text\nshould be, in the learned space. However, most prior methods have focused on\nthe case where image and text convey redundant information; in contrast,\nreal-world image-text pairs convey complementary information with little\noverlap. Further, images in news articles and media portray topics in a\nvisually diverse fashion; thus, we need to take special care to ensure a\nmeaningful image representation. We propose novel within-modality losses which\nencourage semantic coherency in both the text and image subspaces, which does\nnot necessarily align with visual coherency. Our method ensures that not only\nare paired images and texts close, but the expected image-image and text-text\nrelationships are also observed. Our approach improves the results of\ncross-modal retrieval on four datasets compared to five baselines.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:32:54 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Thomas", "Christopher", ""], ["Kovashka", "Adriana", ""]]}, {"id": "2007.08628", "submitter": "Yang Feng", "authors": "Yang Feng, Yubao Liu, Jiebo Luo", "title": "Universal Model for Multi-Domain Medical Image Retrieval", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.03701", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Image Retrieval (MIR) helps doctors quickly find similar patients'\ndata, which can considerably aid the diagnosis process. MIR is becoming\nincreasingly helpful due to the wide use of digital imaging modalities and the\ngrowth of the medical image repositories. However, the popularity of various\ndigital imaging modalities in hospitals also poses several challenges to MIR.\nUsually, one image retrieval model is only trained to handle images from one\nmodality or one source. When there are needs to retrieve medical images from\nseveral sources or domains, multiple retrieval models need to be maintained,\nwhich is cost ineffective. In this paper, we study an important but unexplored\ntask: how to train one MIR model that is applicable to medical images from\nmultiple domains? Simply fusing the training data from multiple domains cannot\nsolve this problem because some domains become over-fit sooner when trained\ntogether using existing methods. Therefore, we propose to distill the knowledge\nin multiple specialist MIR models into a single multi-domain MIR model via\nuniversal embedding to solve this problem. Using skin disease, x-ray, and\nretina image datasets, we validate that our proposed universal model can\neffectively accomplish multi-domain MIR.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:22:04 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Feng", "Yang", ""], ["Liu", "Yubao", ""], ["Luo", "Jiebo", ""]]}, {"id": "2007.08631", "submitter": "Shusen Liu", "authors": "Shusen Liu, Bhavya Kailkhura, Jize Zhang, Anna M. Hiszpanski, Emily\n  Robertson, Donald Loveland, T. Yong-Jin Han", "title": "Explainable Deep Learning for Uncovering Actionable Scientific Insights\n  for Materials Discovery and Design", "comments": null, "journal-ref": null, "doi": null, "report-no": "LLNL-JRNL-811201", "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific community has been increasingly interested in harnessing the\npower of deep learning to solve various domain challenges. However, despite the\neffectiveness in building predictive models, fundamental challenges exist in\nextracting actionable knowledge from deep neural networks due to their opaque\nnature. In this work, we propose techniques for exploring the behavior of deep\nlearning models by injecting domain-specific actionable attributes as tunable\n\"knobs\" in the analysis pipeline. By incorporating the domain knowledge in a\ngenerative modeling framework, we are not only able to better understand the\nbehavior of these black-box models, but also provide scientists with actionable\ninsights that can potentially lead to fundamental discoveries.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:53:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Shusen", ""], ["Kailkhura", "Bhavya", ""], ["Zhang", "Jize", ""], ["Hiszpanski", "Anna M.", ""], ["Robertson", "Emily", ""], ["Loveland", "Donald", ""], ["Han", "T. Yong-Jin", ""]]}, {"id": "2007.08637", "submitter": "Ankit Rajpal", "authors": "Sheetal Rajpal, Manoj Agarwal, Ankit Rajpal, Navin Lakhyani, Naveen\n  Kumar", "title": "COV-ELM classifier: An Extreme Learning Machine based identification of\n  COVID-19 using Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. COVID-19 is an infectious disease caused by a newly discovered\ncoronavirus also termed Severe acute respiratory syndrome coronavirus 2\n(SARS-CoV-2). As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the reverse\ntranscription-polymerase chain reaction (RT-PCR) test which is considered to be\na gold standard in the diagnosis of COVID-19 suffers from a high false-negative\nrate. Therefore, the research community is exploring alternative diagnostic\nmechanisms. Chest X-ray (CXR) image analysis has emerged as a feasible and\neffective diagnostic technique towards this objective. In this work, we propose\nthe COVID-19 classification problem as a three-class classification problem\nnamely COVID-19, normal, and pneumonia. We propose a three-stage framework,\nnamed COV-ELM based on extreme learning machine (ELM). Our dataset comprises\nCXR images in a frontal view, namely Poster anterior (PA) and Erect\nanteroposterior (AP). Stage one deals with preprocessing and transformation,\nstage 2 deals with the challenge of extracting relevant features which are\npassed as input to the ELM at the third stage, resulting in the identification\nof COVID-19. The choice of ELM in this work has been motivated by its\nsignificantly shorter training time as compared to conventional gradient-based\nlearning algorithms. As bigger and diverse datasets become available, it can be\nquickly retrained as compared to its gradient-based competitor models. We use\n10-fold cross-validation to evaluate the results of applying COV-ELM. The\nCOV-ELM achieved a macro average F1-score of 0.95 and the overall sensitivity\nof ${0.94 \\pm 0.02}$ at a 95% confidence interval. When compared to\nstate-of-the-art machine learning algorithms, the COV-ELM is found to\noutperform its competitors.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:03:22 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 09:33:37 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 16:00:52 GMT"}, {"version": "v4", "created": "Fri, 12 Mar 2021 16:26:10 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Rajpal", "Sheetal", ""], ["Agarwal", "Manoj", ""], ["Rajpal", "Ankit", ""], ["Lakhyani", "Navin", ""], ["Kumar", "Naveen", ""]]}, {"id": "2007.08646", "submitter": "Haomiao Ni", "authors": "Haomiao Ni, Yuan Xue, Qian Zhang, Xiaolei Huang", "title": "SiamParseNet: Joint Body Parsing and Label Propagation in Infant\n  Movement Videos", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General movement assessment (GMA) of infant movement videos (IMVs) is an\neffective method for the early detection of cerebral palsy (CP) in infants.\nAutomated body parsing is a crucial step towards computer-aided GMA, in which\ninfant body parts are segmented and tracked over time for movement analysis.\nHowever, acquiring fully annotated data for video-based body parsing is\nparticularly expensive due to the large number of frames in IMVs. In this\npaper, we propose a semi-supervised body parsing model, termed SiamParseNet\n(SPN), to jointly learn single frame body parsing and label propagation between\nframes in a semi-supervised fashion. The Siamese-structured SPN consists of a\nshared feature encoder, followed by two separate branches: one for intra-frame\nbody parts segmentation, and one for inter-frame label propagation. The two\nbranches are trained jointly, taking pairs of frames from the same videos as\ntheir input. An adaptive training process is proposed that alternates training\nmodes between using input pairs of only labeled frames and using inputs of both\nlabeled and unlabeled frames. During testing, we employ a multi-source\ninference mechanism, where the final result for a test frame is either obtained\nvia the segmentation branch or via propagation from a nearby key frame. We\nconduct extensive experiments on a partially-labeled IMV dataset where SPN\noutperforms all prior arts, demonstrating the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:14:25 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ni", "Haomiao", ""], ["Xue", "Yuan", ""], ["Zhang", "Qian", ""], ["Huang", "Xiaolei", ""]]}, {"id": "2007.08661", "submitter": "Dizhong Zhu", "authors": "Dizhong Zhu, William A P Smith", "title": "Least squares surface reconstruction on arbitrary domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost universally in computer vision, when surface derivatives are required,\nthey are computed using only first order accurate finite difference\napproximations. We propose a new method for computing numerical derivatives\nbased on 2D Savitzky-Golay filters and K-nearest neighbour kernels. The\nresulting derivative matrices can be used for least squares surface\nreconstruction over arbitrary (even disconnected) domains in the presence of\nlarge noise and allowing for higher order polynomial local surface\napproximations. They are useful for a range of tasks including\nnormal-from-depth (i.e. surface differentiation), height-from-normals (i.e.\nsurface integration) and shape-from-x. We show how to write both orthographic\nor perspective height-from-normals as a linear least squares problem using the\nsame formulation and avoiding a nonlinear change of variables in the\nperspective case. We demonstrate improved performance relative to\nstate-of-the-art across these tasks on both synthetic and real data and make\navailable an open source implementation of our method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:33:39 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 14:27:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Dizhong", ""], ["Smith", "William A P", ""]]}, {"id": "2007.08667", "submitter": "Ji Hyun Nam", "authors": "Ji Hyun Nam, Andreas Velten", "title": "Super-Resolution Remote Imaging using Time Encoded Remote Apertures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging of scenes using light or other wave phenomena is subject to the\ndiffraction limit. The spatial profile of a wave propagating between a scene\nand the imaging system is distorted by diffraction resulting in a loss of\nresolution that is proportional with traveled distance. We show here that it is\npossible to reconstruct sparse scenes from the temporal profile of the\nwave-front using only one spatial pixel or a spatial average. The temporal\nprofile of the wave is not affected by diffraction yielding an imaging method\nthat can in theory achieve wavelength scale resolution independent of distance\nfrom the scene.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:55:09 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nam", "Ji Hyun", ""], ["Velten", "Andreas", ""]]}, {"id": "2007.08674", "submitter": "Seung Yeon Shin", "authors": "Seung Yeon Shin, Sungwon Lee, Daniel C. Elton, James L. Gulley, Ronald\n  M. Summers", "title": "Deep Small Bowel Segmentation with Cylindrical Topological Constraints", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59719-1_21", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for small bowel segmentation where a cylindrical\ntopological constraint based on persistent homology is applied. To address the\ntouching issue which could break the applied constraint, we propose to augment\na network with an additional branch to predict an inner cylinder of the small\nbowel. Since the inner cylinder is free of the touching issue, a cylindrical\nshape constraint applied on this augmented branch guides the network to\ngenerate a topologically correct segmentation. For strict evaluation, we\nachieved an abdominal computed tomography dataset with dense segmentation\nground-truths. The proposed method showed clear improvements in terms of four\ndifferent metrics compared to the baseline method, and also showed the\nstatistical significance from a paired t-test.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 22:28:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Shin", "Seung Yeon", ""], ["Lee", "Sungwon", ""], ["Elton", "Daniel C.", ""], ["Gulley", "James L.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2007.08696", "submitter": "Xianping Li", "authors": "Karrar Abbas and Xianping Li", "title": "Anisotropic Mesh Adaptation for Image Segmentation Based on Mumford-Shah\n  Functional", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the resolution of digital images increase significantly, the processing of\nimages becomes more challenging in terms of accuracy and efficiency. In this\npaper, we consider image segmentation by solving a partial differentiation\nequation (PDE) model based on the Mumford-Shah functional. We develop a new\nalgorithm by combining anisotropic mesh adaptation for image representation and\nfinite element method for solving the PDE model. Comparing to traditional\nalgorithms solved by finite difference method, our algorithm provides faster\nand better results without the need to resizing the images to lower quality. We\nalso extend the algorithm to segment images with multiple regions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 00:00:31 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Abbas", "Karrar", ""], ["Li", "Xianping", ""]]}, {"id": "2007.08702", "submitter": "Wilhelm Tranheden", "authors": "Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, Lennart Svensson", "title": "DACS: Domain Adaptation via Cross-domain Mixed Sampling", "comments": "This paper has been accepted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation models based on convolutional neural networks have\nrecently displayed remarkable performance for a multitude of applications.\nHowever, these models typically do not generalize well when applied on new\ndomains, especially when going from synthetic to real data. In this paper we\naddress the problem of unsupervised domain adaptation (UDA), which attempts to\ntrain on labelled data from one domain (source domain), and simultaneously\nlearn from unlabelled data in the domain of interest (target domain). Existing\nmethods have seen success by training on pseudo-labels for these unlabelled\nimages. Multiple techniques have been proposed to mitigate low-quality\npseudo-labels arising from the domain shift, with varying degrees of success.\nWe propose DACS: Domain Adaptation via Cross-domain mixed Sampling, which mixes\nimages from the two domains along with the corresponding labels and\npseudo-labels. These mixed samples are then trained on, in addition to the\nlabelled data itself. We demonstrate the effectiveness of our solution by\nachieving state-of-the-art results for GTA5 to Cityscapes, a common\nsynthetic-to-real semantic segmentation benchmark for UDA.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 00:43:11 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 11:13:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Tranheden", "Wilhelm", ""], ["Olsson", "Viktor", ""], ["Pinto", "Juliano", ""], ["Svensson", "Lennart", ""]]}, {"id": "2007.08711", "submitter": "Yu Liang", "authors": "Yu Liang, Arin Chaudhuri, and Haoyu Wang", "title": "Visualizing the Finer Cluster Structure of Large-Scale and\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction and visualization of high-dimensional data have become\nvery important research topics because of the rapid growth of large databases\nin data science. In this paper, we propose using a generalized sigmoid function\nto model the distance similarity in both high- and low-dimensional spaces. In\nparticular, the parameter b is introduced to the generalized sigmoid function\nin low-dimensional space, so that we can adjust the heaviness of the function\ntail by changing the value of b. Using both simulated and real-world data sets,\nwe show that our proposed method can generate visualization results comparable\nto those of uniform manifold approximation and projection (UMAP), which is a\nnewly developed manifold learning technique with fast running speed, better\nglobal structure, and scalability to massive data sets. In addition, according\nto the purpose of the study and the data structure, we can decrease or increase\nthe value of b to either reveal the finer cluster structure of the data or\nmaintain the neighborhood continuity of the embedding for better visualization.\nFinally, we use domain knowledge to demonstrate that the finer subclusters\nrevealed with small values of b are meaningful.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:36:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liang", "Yu", ""], ["Chaudhuri", "Arin", ""], ["Wang", "Haoyu", ""]]}, {"id": "2007.08714", "submitter": "Yun-Yun Tsai", "authors": "Yun-Yun Tsai and Pin-Yu Chen and Tsung-Yi Ho", "title": "Transfer Learning without Knowing: Reprogramming Black-box Machine\n  Learning Models with Scarce Data and Limited Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current transfer learning methods are mainly based on finetuning a pretrained\nmodel with target-domain data. Motivated by the techniques from adversarial\nmachine learning (ML) that are capable of manipulating the model prediction via\ndata perturbations, in this paper we propose a novel approach, black-box\nadversarial reprogramming (BAR), that repurposes a well-trained black-box ML\nmodel (e.g., a prediction API or a proprietary software) for solving different\nML tasks, especially in the scenario with scarce data and constrained\nresources. The rationale lies in exploiting high-performance but unknown ML\nmodels to gain learning capability for transfer learning. Using zeroth order\noptimization and multi-label mapping techniques, BAR can reprogram a black-box\nML model solely based on its input-output responses without knowing the model\narchitecture or changing any parameter. More importantly, in the limited\nmedical data setting, on autism spectrum disorder classification, diabetic\nretinopathy detection, and melanoma detection tasks, BAR outperforms\nstate-of-the-art methods and yields comparable performance to the vanilla\nadversarial reprogramming method requiring complete knowledge of the target ML\nmodel. BAR also outperforms baseline transfer learning approaches by a\nsignificant margin, demonstrating cost-effective means and new insights for\ntransfer learning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:52:34 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 12:12:30 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Tsai", "Yun-Yun", ""], ["Chen", "Pin-Yu", ""], ["Ho", "Tsung-Yi", ""]]}, {"id": "2007.08716", "submitter": "Haizhong Zheng", "authors": "Haizhong Zheng, Ziqi Zhang, Honglak Lee, Atul Prakash", "title": "Understanding and Diagnosing Vulnerability under Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are known to be vulnerable to adversarial\nattacks. Currently, there is no clear insight into how slight perturbations\ncause such a large difference in classification results and how we can design a\nmore robust model architecture. In this work, we propose a novel\ninterpretability method, InterpretGAN, to generate explanations for features\nused for classification in latent variables. Interpreting the classification\nprocess of adversarial examples exposes how adversarial perturbations influence\nfeatures layer by layer as well as which features are modified by\nperturbations. Moreover, we design the first diagnostic method to quantify the\nvulnerability contributed by each layer, which can be used to identify\nvulnerable parts of model architectures. The diagnostic results show that the\nlayers introducing more information loss tend to be more vulnerable than other\nlayers. Based on the findings, our evaluation results on MNIST and CIFAR10\ndatasets suggest that average pooling layers, with lower information loss, are\nmore robust than max pooling layers for the network architectures studied in\nthis paper.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:56:28 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zheng", "Haizhong", ""], ["Zhang", "Ziqi", ""], ["Lee", "Honglak", ""], ["Prakash", "Atul", ""]]}, {"id": "2007.08722", "submitter": "Zhiguang Zhang", "authors": "Zhipeng Luo, Ge Li, Zhiguang Zhang", "title": "A Technical Report for VIPriors Image Classification Challenge", "comments": "ECCV2020,VIPriors Image Classification Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has always been a hot and challenging task. This paper\nis a brief report to our submission to the VIPriors Image Classification\nChallenge. In this challenge, the difficulty is how to train the model from\nscratch without any pretrained weight. In our method, several strong backbones\nand multiple loss functions are used to learn more representative features. To\nimprove the models' generalization and robustness, efficient image augmentation\nstrategies are utilized, like autoaugment and cutmix. Finally, ensemble\nlearning is used to increase the performance of the models. The final Top-1\naccuracy of our team DeepBlueAI is 0.7015, ranking second in the leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 02:30:09 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Luo", "Zhipeng", ""], ["Li", "Ge", ""], ["Zhang", "Zhiguang", ""]]}, {"id": "2007.08723", "submitter": "Joshua Peterson", "authors": "Pulkit Singh, Joshua C. Peterson, Ruairidh M. Battleday, Thomas L.\n  Griffiths", "title": "End-to-end Deep Prototype and Exemplar Models for Predicting Human\n  Behavior", "comments": "7 pages, 4 figures, 2 tables. Accepted as a paper to the 42nd Annual\n  Meeting of the Cognitive Science Society (CogSci 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional models of category learning in psychology focus on representation\nat the category level as opposed to the stimulus level, even though the two are\nlikely to interact. The stimulus representations employed in such models are\neither hand-designed by the experimenter, inferred circuitously from human\njudgments, or borrowed from pretrained deep neural networks that are themselves\ncompeting models of category learning. In this work, we extend classic\nprototype and exemplar models to learn both stimulus and category\nrepresentations jointly from raw input. This new class of models can be\nparameterized by deep neural networks (DNN) and trained end-to-end. Following\ntheir namesakes, we refer to them as Deep Prototype Models, Deep Exemplar\nModels, and Deep Gaussian Mixture Models. Compared to typical DNNs, we find\nthat their cognitively inspired counterparts both provide better intrinsic fit\nto human behavior and improve ground-truth classification.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 02:32:17 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Singh", "Pulkit", ""], ["Peterson", "Joshua C.", ""], ["Battleday", "Ruairidh M.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "2007.08728", "submitter": "Dong-Jin Kim", "authors": "Dong-Jin Kim, Xiao Sun, Jinsoo Choi, Stephen Lin, In So Kweon", "title": "Detecting Human-Object Interactions with Action Co-occurrence Priors", "comments": "ECCV 2020. Source code :\n  https://github.com/Dong-JinKim/ActionCooccurrencePriors/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in human-object interaction (HOI) detection task is that\nnumerous HOI classes have only a small number of labeled examples, resulting in\ntraining sets with a long-tailed distribution. The lack of positive labels can\nlead to low classification accuracy for these classes. Towards addressing this\nissue, we observe that there exist natural correlations and anti-correlations\namong human-object interactions. In this paper, we model the correlations as\naction co-occurrence matrices and present techniques to learn these priors and\nleverage them for more effective training, especially in rare classes. The\nutility of our approach is demonstrated experimentally, where the performance\nof our approach exceeds the state-of-the-art methods on both of the two leading\nHOI detection benchmark datasets, HICO-Det and V-COCO.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 02:47:45 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 05:42:32 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kim", "Dong-Jin", ""], ["Sun", "Xiao", ""], ["Choi", "Jinsoo", ""], ["Lin", "Stephen", ""], ["Kweon", "In So", ""]]}, {"id": "2007.08735", "submitter": "Chenghao Liu", "authors": "Chenghao Liu and Zhihao Wang and Doyen Sahoo and Yuan Fang and Kun\n  Zhang and Steven C.H. Hoi", "title": "Adaptive Task Sampling for Meta-Learning", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning methods have been extensively studied and applied in computer\nvision, especially for few-shot classification tasks. The key idea of\nmeta-learning for few-shot classification is to mimic the few-shot situations\nfaced at test time by randomly sampling classes in meta-training data to\nconstruct few-shot tasks for episodic training. While a rich line of work\nfocuses solely on how to extract meta-knowledge across tasks, we exploit the\ncomplementary problem on how to generate informative tasks. We argue that the\nrandomly sampled tasks could be sub-optimal and uninformative (e.g., the task\nof classifying \"dog\" from \"laptop\" is often trivial) to the meta-learner. In\nthis paper, we propose an adaptive task sampling method to improve the\ngeneralization performance. Unlike instance based sampling, task based sampling\nis much more challenging due to the implicit definition of the task in each\nepisode. Therefore, we accordingly propose a greedy class-pair based sampling\nmethod, which selects difficult tasks according to class-pair potentials. We\nevaluate our adaptive task sampling method on two few-shot classification\nbenchmarks, and it achieves consistent improvements across different feature\nbackbones, meta-learning algorithms and datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 03:15:53 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Chenghao", ""], ["Wang", "Zhihao", ""], ["Sahoo", "Doyen", ""], ["Fang", "Yuan", ""], ["Zhang", "Kun", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2007.08739", "submitter": "David Minnen", "authors": "David Minnen and Saurabh Singh", "title": "Channel-wise Autoregressive Entropy Models for Learned Image Compression", "comments": "Published at the IEEE International Conference on Image Processing\n  (ICIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In learning-based approaches to image compression, codecs are developed by\noptimizing a computational model to minimize a rate-distortion objective.\nCurrently, the most effective learned image codecs take the form of an\nentropy-constrained autoencoder with an entropy model that uses both forward\nand backward adaptation. Forward adaptation makes use of side information and\ncan be efficiently integrated into a deep neural network. In contrast, backward\nadaptation typically makes predictions based on the causal context of each\nsymbol, which requires serial processing that prevents efficient GPU / TPU\nutilization. We introduce two enhancements, channel-conditioning and latent\nresidual prediction, that lead to network architectures with better\nrate-distortion performance than existing context-adaptive models while\nminimizing serial processing. Empirically, we see an average rate savings of\n6.7% on the Kodak image set and 11.4% on the Tecnick image set compared to a\ncontext-adaptive baseline model. At low bit rates, where the improvements are\nmost effective, our model saves up to 18% over the baseline and outperforms\nhand-engineered codecs like BPG by up to 25%.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 03:33:53 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Minnen", "David", ""], ["Singh", "Saurabh", ""]]}, {"id": "2007.08740", "submitter": "Xinwei Sun", "authors": "Xinwei Sun, Wenjing Han, Lingjing Hu, Yuan Yao, Yizhou Wang", "title": "Leveraging both Lesion Features and Procedural Bias in Neuroimaging: An\n  Dual-Task Split dynamics of inverse scale space", "comments": "Thanks to Xinwei's girlfriend Yue Cao, for her love and support", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction and selection of lesion features are two important tasks in\nvoxel-based neuroimage analysis. Existing multivariate learning models take two\ntasks equivalently and optimize simultaneously. However, in addition to lesion\nfeatures, we observe that there is another type of feature, which is commonly\nintroduced during the procedure of preprocessing steps, which can improve the\nprediction result. We call such a type of feature as procedural bias.\nTherefore, in this paper, we propose that the features/voxels in neuroimage\ndata are consist of three orthogonal parts: lesion features, procedural bias,\nand null features. To stably select lesion features and leverage procedural\nbias into prediction, we propose an iterative algorithm (termed GSplit LBI) as\na discretization of differential inclusion of inverse scale space, which is the\ncombination of Variable Splitting scheme and Linearized Bregman Iteration\n(LBI). Specifically, with a variable the splitting term, two estimators are\nintroduced and split apart, i.e. one is for feature selection (the sparse\nestimator) and the other is for prediction (the dense estimator). Implemented\nwith Linearized Bregman Iteration (LBI), the solution path of both estimators\ncan be returned with different sparsity levels on the sparse estimator for the\nselection of lesion features. Besides, the dense the estimator can additionally\nleverage procedural bias to further improve prediction results. To test the\nefficacy of our method, we conduct experiments on the simulated study and\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. The validity and\nthe benefit of our model can be shown by the improvement of prediction results\nand the interpretability of visualized procedural bias and lesion features.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 03:41:48 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Sun", "Xinwei", ""], ["Han", "Wenjing", ""], ["Hu", "Lingjing", ""], ["Yao", "Yuan", ""], ["Wang", "Yizhou", ""]]}, {"id": "2007.08745", "submitter": "Yiming Li", "authors": "Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia", "title": "Backdoor Learning: A Survey", "comments": "12 pages. A curated list of backdoor learning resources in this paper\n  is presented in the Github Repo\n  (https://github.com/THUYimingLi/backdoor-learning-resources). We will try our\n  best to continuously maintain the repo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attack intends to embed hidden backdoor into deep neural networks\n(DNNs), such that the attacked model performs well on benign samples, whereas\nits prediction will be maliciously changed if the hidden backdoor is activated\nby the attacker-defined trigger. This threat could happen when the training\nprocess is not fully controlled, such as training on third-party datasets or\nadopting third-party models, which poses a new and realistic threat. Although\nbackdoor learning is an emerging and rapidly growing research area, its\nsystematic review, however, remains blank. In this paper, we present the first\ncomprehensive survey of this realm. We summarize and categorize existing\nbackdoor attacks and defenses based on their characteristics, and provide a\nunified framework for analyzing poisoning-based backdoor attacks. Besides, we\nalso analyze the relation between backdoor attacks and relevant fields ($i.e.,$\nadversarial attacks and data poisoning), and summarize widely adopted benchmark\ndatasets. Finally, we briefly outline certain future research directions\nrelying upon reviewed works. A curated list of backdoor-related resources is\nalso available at\n\\url{https://github.com/THUYimingLi/backdoor-learning-resources}.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 04:09:20 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 06:27:07 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 02:14:14 GMT"}, {"version": "v4", "created": "Sun, 14 Feb 2021 04:46:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Li", "Yiming", ""], ["Wu", "Baoyuan", ""], ["Jiang", "Yong", ""], ["Li", "Zhifeng", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2007.08751", "submitter": "Noa Garcia", "authors": "Noa Garcia and Yuta Nakashima", "title": "Knowledge-Based Video Question Answering with Unsupervised Scene\n  Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand movies, humans constantly reason over the dialogues and actions\nshown in specific scenes and relate them to the overall storyline already seen.\nInspired by this behaviour, we design ROLL, a model for knowledge-based video\nstory question answering that leverages three crucial aspects of movie\nunderstanding: dialog comprehension, scene reasoning, and storyline recalling.\nIn ROLL, each of these tasks is in charge of extracting rich and diverse\ninformation by 1) processing scene dialogues, 2) generating unsupervised video\nscene descriptions, and 3) obtaining external knowledge in a weakly supervised\nfashion. To answer a given question correctly, the information generated by\neach inspired-cognitive task is encoded via Transformers and fused through a\nmodality weighting mechanism, which balances the information from the different\nsources. Exhaustive evaluation demonstrates the effectiveness of our approach,\nwhich yields a new state-of-the-art on two challenging video question answering\ndatasets: KnowIT VQA and TVQA+.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 04:26:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Garcia", "Noa", ""], ["Nakashima", "Yuta", ""]]}, {"id": "2007.08752", "submitter": "Jingjie Zhu", "authors": "Jingjie Zhu, Karthik Sundaresan, Jason Rupe", "title": "Proactive Network Maintenance using Fast, Accurate Anomaly Localization\n  and Classification on 1-D Data Series", "comments": "This paper has been accepted by ICPHM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive network maintenance (PNM) is the concept of using data from a\nnetwork to identify and locate network faults, many or all of which could\nworsen to become service failures. The separation between the network fault and\nthe service failure affords early detection of problems in the network to allow\nPNM to take place. Consequently, PNM is a form of prognostics and health\nmanagement (PHM).\n  The problem of localizing and classifying anomalies on 1-dimensional data\nseries has been under research for years. We introduce a new algorithm that\nleverages Deep Convolutional Neural Networks to efficiently and accurately\ndetect anomalies and events on data series, and it reaches 97.82% mean average\nprecision (mAP) in our evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 04:27:20 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhu", "Jingjie", ""], ["Sundaresan", "Karthik", ""], ["Rupe", "Jason", ""]]}, {"id": "2007.08760", "submitter": "Wenbin Wang", "authors": "Wenbin Wang, Ruiping Wang, Shiguang Shan, Xilin Chen", "title": "Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph aims to faithfully reveal humans' perception of image content.\nWhen humans analyze a scene, they usually prefer to describe image gist first,\nnamely major objects and key relations in a scene graph. This humans' inherent\nperceptive habit implies that there exists a hierarchical structure about\nhumans' preference during the scene parsing procedure. Therefore, we argue that\na desirable scene graph should be also hierarchically constructed, and\nintroduce a new scheme for modeling scene graph. Concretely, a scene is\nrepresented by a human-mimetic Hierarchical Entity Tree (HET) consisting of a\nseries of image regions. To generate a scene graph based on HET, we parse HET\nwith a Hybrid Long Short-Term Memory (Hybrid-LSTM) which specifically encodes\nhierarchy and siblings context to capture the structured information embedded\nin HET. To further prioritize key relations in the scene graph, we devise a\nRelation Ranking Module (RRM) to dynamically adjust their rankings by learning\nto capture humans' subjective perceptive habits from objective entity saliency\nand size. Experiments indicate that our method not only achieves\nstate-of-the-art performances for scene graph generation, but also is expert in\nmining image-specific relations which play a great role in serving downstream\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 05:12:13 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wang", "Wenbin", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2007.08763", "submitter": "Fang Aiqing", "authors": "Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Shihao Cao, Yanning Zhang", "title": "AE-Net: Autonomous Evolution Image Fusion Method Inspired by Human\n  Cognitive Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to solve the robustness and generality problems of the image fusion\ntask,inspired by the human brain cognitive mechanism, we propose a robust and\ngeneral image fusion method with autonomous evolution ability, and is therefore\ndenoted with AE-Net. Through the collaborative optimization of multiple image\nfusion methods to simulate the cognitive process of human brain, unsupervised\nlearning image fusion task can be transformed into semi-supervised image fusion\ntask or supervised image fusion task, thus promoting the evolutionary ability\nof network model weight. Firstly, the relationship between human brain\ncognitive mechanism and image fusion task is analyzed and a physical model is\nestablished to simulate human brain cognitive mechanism. Secondly, we analyze\nexisting image fusion methods and image fusion loss functions, select the image\nfusion method with complementary features to construct the algorithm module,\nestablish the multi-loss joint evaluation function to obtain the optimal\nsolution of algorithm module. The optimal solution of each image is used to\nguide the weight training of network model. Our image fusion method can\neffectively unify the cross-modal image fusion task and the same modal image\nfusion task, and effectively overcome the difference of data distribution\nbetween different datasets. Finally, extensive numerical results verify the\neffectiveness and superiority of our method on a variety of image fusion\ndatasets, including multi-focus dataset, infrared and visi-ble dataset, medical\nimage dataset and multi-exposure dataset. Comprehensive experiments demonstrate\nthe superiority of our image fusion method in robustness and generality. In\naddition, experimental results also demonstate the effectiveness of human brain\ncognitive mechanism to improve the robustness and generality of image fusion.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 05:19:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Fang", "Aiqing", ""], ["Zhao", "Xinbo", ""], ["Yang", "Jiaqi", ""], ["Cao", "Shihao", ""], ["Zhang", "Yanning", ""]]}, {"id": "2007.08767", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Jing Yao and Xin Wu and Jocelyn Chanussot and Xiao\n  Xiang Zhu", "title": "Spatial-Spectral Manifold Embedding of Hyperspectral Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, hyperspectral imaging, also known as imaging spectroscopy,\nhas been paid an increasing interest in geoscience and remote sensing\ncommunity. Hyperspectral imagery is characterized by very rich spectral\ninformation, which enables us to recognize the materials of interest lying on\nthe surface of the Earth more easier. We have to admit, however, that high\nspectral dimension inevitably brings some drawbacks, such as expensive data\nstorage and transmission, information redundancy, etc. Therefore, to reduce the\nspectral dimensionality effectively and learn more discriminative spectral\nlow-dimensional embedding, in this paper we propose a novel hyperspectral\nembedding approach by simultaneously considering spatial and spectral\ninformation, called spatial-spectral manifold embedding (SSME). Beyond the\npixel-wise spectral embedding approaches, SSME models the spatial and spectral\ninformation jointly in a patch-based fashion. SSME not only learns the spectral\nembedding by using the adjacency matrix obtained by similarity measurement\nbetween spectral signatures, but also models the spatial neighbours of a target\npixel in hyperspectral scene by sharing the same weights (or edges) in the\nprocess of learning embedding. Classification is explored as a potential\nstrategy to quantitatively evaluate the performance of learned embedding\nrepresentations. Classification is explored as a potential application for\nquantitatively evaluating the performance of these hyperspectral embedding\nalgorithms. Extensive experiments conducted on the widely-used hyperspectral\ndatasets demonstrate the superiority and effectiveness of the proposed SSME as\ncompared to several state-of-the-art embedding methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 05:40:27 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Hong", "Danfeng", ""], ["Yao", "Jing", ""], ["Wu", "Xin", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2007.08779", "submitter": "Yan Zhang", "authors": "Yan Zhang, Binyu He, Li Sun", "title": "Progressive Multi-stage Feature Mix for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image features from a small local region often give strong evidence in person\nre-identification task. However, CNN suffers from paying too much attention on\nthe most salient local areas, thus ignoring other discriminative clues, e.g.,\nhair, shoes or logos on clothes. %BDB proposes to randomly drop one block in a\nbatch to enlarge the high response areas. Although BDB has achieved remarkable\nresults, there still room for improvement. In this work, we propose a\nProgressive Multi-stage feature Mix network (PMM), which enables the model to\nfind out the more precise and diverse features in a progressive manner.\nSpecifically, 1. to enforce the model to look for different clues in the image,\nwe adopt a multi-stage classifier and expect that the model is able to focus on\na complementary region in each stage. 2. we propose an Attentive feature\nHard-Mix (A-Hard-Mix) to replace the salient feature blocks by the negative\nexample in the current batch, whose label is different from the current sample.\n3. extensive experiments have been carried out on reID datasets such as the\nMarket-1501, DukeMTMC-reID and CUHK03, showing that the proposed method can\nboost the re-identification performance significantly.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 06:59:39 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 06:39:04 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Yan", ""], ["He", "Binyu", ""], ["Sun", "Li", ""]]}, {"id": "2007.08781", "submitter": "Viktor Seib", "authors": "Viktor Seib, Benjamin Lange and Stefan Wirtz", "title": "Mixing Real and Synthetic Data to Enhance Neural Network Training -- A\n  Review of Current Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have gained tremendous importance in many computer\nvision tasks. However, their power comes at the cost of large amounts of\nannotated data required for supervised training. In this work we review and\ncompare different techniques available in the literature to improve training\nresults without acquiring additional annotated real-world data. This goal is\nmostly achieved by applying annotation-preserving transformations to existing\ndata or by synthetically creating more data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:12:31 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Seib", "Viktor", ""], ["Lange", "Benjamin", ""], ["Wirtz", "Stefan", ""]]}, {"id": "2007.08783", "submitter": "Kadambari K", "authors": "K.V. Kadambari, Vishnu Vardhan Nimmalapudi", "title": "Deep Learning Based Traffic Surveillance System For Missing and\n  Suspicious Car Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle theft is arguably one of the fastest-growing types of crime in India.\nIn some of the urban areas, vehicle theft cases are believed to be around 100\neach day. Identification of stolen vehicles in such precarious scenarios is not\npossible using traditional methods like manual checking and radio frequency\nidentification(RFID) based technologies. This paper presents a deep learning\nbased automatic traffic surveillance system for the detection of\nstolen/suspicious cars from the closed circuit television(CCTV) camera footage.\nIt mainly comprises of four parts: Select-Detector, Image Quality Enhancer,\nImage Transformer, and Smart Recognizer. The Select-Detector is used for\nextracting the frames containing vehicles and to detect the license plates much\nefficiently with minimum time complexity. The quality of the license plates is\nthen enhanced using Image Quality Enhancer which uses pix2pix generative\nadversarial network(GAN) for enhancing the license plates that are affected by\ntemporal changes like low light, shadow, etc. Image Transformer is used to\ntackle the problem of inefficient recognition of license plates which are not\nhorizontal(which are at an angle) by transforming the license plate to\ndifferent levels of rotation and cropping. Smart Recognizer recognizes the\nlicense plate number using Tesseract optical character recognition(OCR) and\ncorrects the wrongly recognized characters using Error-Detector. The\neffectiveness of the proposed approach is tested on the government's CCTV\ncamera footage, which resulted in identifying the stolen/suspicious cars with\nan accuracy of 87%.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:18:12 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Kadambari", "K. V.", ""], ["Nimmalapudi", "Vishnu Vardhan", ""]]}, {"id": "2007.08785", "submitter": "Yan Zhang", "authors": "Yan Zhang, Zhilin Zheng, Binyu He, Li Sun", "title": "Learning Posterior and Prior for Uncertainty Modeling in Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data uncertainty in practical person reID is ubiquitous, hence it requires\nnot only learning the discriminative features, but also modeling the\nuncertainty based on the input. This paper proposes to learn the sample\nposterior and the class prior distribution in the latent space, so that not\nonly representative features but also the uncertainty can be built by the\nmodel. The prior reflects the distribution of all data in the same class, and\nit is the trainable model parameters. While the posterior is the probability\ndensity of a single sample, so it is actually the feature defined on the input.\nWe assume that both of them are in Gaussian form. To simultaneously model them,\nwe put forward a distribution loss, which measures the KL divergence from the\nposterior to the priors in the manner of supervised learning. In addition, we\nassume that the posterior variance, which is essentially the uncertainty, is\nsupposed to have the second-order characteristic. Therefore, a $\\Sigma-$net is\nproposed to compute it by the high order representation from its input.\nExtensive experiments have been carried out on Market1501, DukeMTMC, MARS and\nnoisy dataset as well.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:20:39 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhang", "Yan", ""], ["Zheng", "Zhilin", ""], ["He", "Binyu", ""], ["Sun", "Li", ""]]}, {"id": "2007.08786", "submitter": "Subin Jeon", "authors": "Subin Jeon, Seonghyeon Nam, Seoung Wug Oh, Seon Joo Kim", "title": "Cross-Identity Motion Transfer for Arbitrary Objects through\n  Pose-Attentive Video Reassembling", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an attention-based networks for transferring motions between\narbitrary objects. Given a source image(s) and a driving video, our networks\nanimate the subject in the source images according to the motion in the driving\nvideo. In our attention mechanism, dense similarities between the learned\nkeypoints in the source and the driving images are computed in order to\nretrieve the appearance information from the source images. Taking a different\napproach from the well-studied warping based models, our attention-based model\nhas several advantages. By reassembling non-locally searched pieces from the\nsource contents, our approach can produce more realistic outputs. Furthermore,\nour system can make use of multiple observations of the source appearance (e.g.\nfront and sides of faces) to make the results more accurate. To reduce the\ntraining-testing discrepancy of the self-supervised learning, a novel\ncross-identity training scheme is additionally introduced. With the training\nscheme, our networks is trained to transfer motions between different subjects,\nas in the real testing scenario. Experimental results validate that our method\nproduces visually pleasing results in various object domains, showing better\nperformances compared to previous works.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:21:12 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Jeon", "Subin", ""], ["Nam", "Seonghyeon", ""], ["Oh", "Seoung Wug", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2007.08789", "submitter": "Vahid Yaghoubi", "authors": "Vahid Yaghoubi, Liangliang Cheng, Wim Van Paepegem, Mathias Kersemans", "title": "An ensemble classifier for vibration-based quality monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vibration-based quality monitoring of manufactured components often employs\npattern recognition methods. Albeit developing several classification methods,\nthey usually provide high accuracy for specific types of datasets, but not for\ngeneral cases. In this paper, this issue has been addressed by developing a\nnovel ensemble classifier based on the Dempster-Shafer theory of evidence. To\ndeal with conflicting evidences, three remedies are proposed prior to\ncombination: (i) selection of proper classifiers by evaluating the relevancy\nbetween the predicted and target outputs, (ii) devising an optimization method\nto minimize the distance between the predicted and target outputs, (iii)\nutilizing five different weighting factors, including a new one, to enhance the\nfusion performance. The effectiveness of the proposed framework is validated by\nits application to 15 UCI and KEEL machine learning datasets. It is then\napplied to two vibration-based datasets to detect defected samples: one\nsynthetic dataset generated from the finite element model of a dogbone\ncylinder, and one real experimental dataset generated by collecting broadband\nvibrational response of polycrystalline Nickel alloy first-stage turbine\nblades. The investigation is made through statistical analysis in presence of\ndifferent levels of noise-to-signal ratio. Comparing the results with those of\nfour state-of-the-art fusion techniques reveals the good performance of the\nproposed ensemble method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:23:56 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:01:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yaghoubi", "Vahid", ""], ["Cheng", "Liangliang", ""], ["Van Paepegem", "Wim", ""], ["Kersemans", "Mathias", ""]]}, {"id": "2007.08790", "submitter": "Jiamei Sun", "authors": "Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing Zhao,\n  Ngai-Man Cheung, Alexander Binder", "title": "Explanation-Guided Training for Cross-Domain Few-Shot Classification", "comments": null, "journal-ref": "Proceedings of the 25th International Conference on Pattern\n  Recognition 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain few-shot classification task (CD-FSC) combines few-shot\nclassification with the requirement to generalize across domains represented by\ndatasets. This setup faces challenges originating from the limited labeled data\nin each class and, additionally, from the domain shift between training and\ntest sets. In this paper, we introduce a novel training approach for existing\nFSC models. It leverages on the explanation scores, obtained from existing\nexplanation methods when applied to the predictions of FSC models, computed for\nintermediate feature maps of the models. Firstly, we tailor the layer-wise\nrelevance propagation (LRP) method to explain the predictions of FSC models.\nSecondly, we develop a model-agnostic explanation-guided training strategy that\ndynamically finds and emphasizes the features which are important for the\npredictions. Our contribution does not target a novel explanation method but\nlies in a novel application of explanations for the training phase. We show\nthat explanation-guided training effectively improves the model generalization.\nWe observe improved accuracy for three different FSC models: RelationNet, cross\nattention network, and a graph neural network-based formulation, on five\nfew-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. The\nsource code is available at https://github.com/SunJiamei/few-shot-lrp-guided\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:28:08 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 09:53:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sun", "Jiamei", ""], ["Lapuschkin", "Sebastian", ""], ["Samek", "Wojciech", ""], ["Zhao", "Yunqing", ""], ["Cheung", "Ngai-Man", ""], ["Binder", "Alexander", ""]]}, {"id": "2007.08801", "submitter": "Hang Wang", "authors": "Hang Wang, Minghao Xu, Bingbing Ni, Wenjun Zhang", "title": "Learning to Combine: Knowledge Aggregation for Multi-Source Domain\n  Adaptation", "comments": "Accepted by ECCV 2020. Code is available at\n  \\url{https://github.com/ChrisAllenMing/LtC-MSDA}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledges learned from multiple source domains to target domain\nis a more practical and challenging task than conventional single-source domain\nadaptation. Furthermore, the increase of modalities brings more difficulty in\naligning feature distributions among multiple domains. To mitigate these\nproblems, we propose a Learning to Combine for Multi-Source Domain Adaptation\n(LtC-MSDA) framework via exploring interactions among domains. In the nutshell,\na knowledge graph is constructed on the prototypes of various domains to\nrealize the information propagation among semantically adjacent\nrepresentations. On such basis, a graph model is learned to predict query\nsamples under the guidance of correlated prototypes. In addition, we design a\nRelation Alignment Loss (RAL) to facilitate the consistency of categories'\nrelational interdependency and the compactness of features, which boosts\nfeatures' intra-class invariance and inter-class separability. Comprehensive\nresults on public benchmark datasets demonstrate that our approach outperforms\nexisting methods with a remarkable margin. Our code is available at\n\\url{https://github.com/ChrisAllenMing/LtC-MSDA}\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:52:44 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 07:07:24 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 15:12:38 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wang", "Hang", ""], ["Xu", "Minghao", ""], ["Ni", "Bingbing", ""], ["Zhang", "Wenjun", ""]]}, {"id": "2007.08802", "submitter": "Lei Yang", "authors": "Lei Yang, Qingqiu Huang, Huaiyi Huang, Linning Xu, Dahua Lin", "title": "Learn to Propagate Reliably on Noisy Affinity Graphs", "comments": "14 pages, 7 figures, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that exploiting unlabeled data through label\npropagation can substantially reduce the labeling cost, which has been a\ncritical issue in developing visual recognition models. Yet, how to propagate\nlabels reliably, especially on a dataset with unknown outliers, remains an open\nquestion. Conventional methods such as linear diffusion lack the capability of\nhandling complex graph structures and may perform poorly when the seeds are\nsparse. Latest methods based on graph neural networks would face difficulties\non performance drop as they scale out to noisy graphs. To overcome these\ndifficulties, we propose a new framework that allows labels to be propagated\nreliably on large-scale real-world data. This framework incorporates (1) a\nlocal graph neural network to predict accurately on varying local structures\nwhile maintaining high scalability, and (2) a confidence-based path scheduler\nthat identifies outliers and moves forward the propagation frontier in a\nprudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our\nconfidence guided framework can significantly improve the overall accuracies of\nthe propagated labels, especially when the graph is very noisy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:55:59 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yang", "Lei", ""], ["Huang", "Qingqiu", ""], ["Huang", "Huaiyi", ""], ["Xu", "Linning", ""], ["Lin", "Dahua", ""]]}, {"id": "2007.08809", "submitter": "Jungin Park", "authors": "Jungin Park, Jiyoung Lee, Ig-Jae Kim, and Kwanghoon Sohn", "title": "SumGraph: Video Summarization via Recursive Graph Modeling", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of video summarization is to select keyframes that are visually\ndiverse and can represent a whole story of an input video. State-of-the-art\napproaches for video summarization have mostly regarded the task as a\nframe-wise keyframe selection problem by aggregating all frames with equal\nweight. However, to find informative parts of the video, it is necessary to\nconsider how all the frames of the video are related to each other. To this\nend, we cast video summarization as a graph modeling problem. We propose\nrecursive graph modeling networks for video summarization, termed SumGraph, to\nrepresent a relation graph, where frames are regarded as nodes and nodes are\nconnected by semantic relationships among frames. Our networks accomplish this\nthrough a recursive approach to refine an initially estimated graph to\ncorrectly classify each node as a keyframe by reasoning the graph\nrepresentation via graph convolutional networks. To leverage SumGraph in a more\npractical environment, we also present a way to adapt our graph modeling in an\nunsupervised fashion. With SumGraph, we achieved state-of-the-art performance\non several benchmarks for video summarization in both supervised and\nunsupervised manners.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:11:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Park", "Jungin", ""], ["Lee", "Jiyoung", ""], ["Kim", "Ig-Jae", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2007.08814", "submitter": "Junbin Xiao", "authors": "Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, Tat-Seng Chua", "title": "Visual Relation Grounding in Videos", "comments": "ECCV2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore a novel task named visual Relation Grounding in\nVideos (vRGV). The task aims at spatio-temporally localizing the given\nrelations in the form of subject-predicate-object in the videos, so as to\nprovide supportive visual facts for other high-level video-language tasks\n(e.g., video-language grounding and video question answering). The challenges\nin this task include but not limited to: (1) both the subject and object are\nrequired to be spatio-temporally localized to ground a query relation; (2) the\ntemporal dynamic nature of visual relations in videos is difficult to capture;\nand (3) the grounding should be achieved without any direct supervision in\nspace and time. To ground the relations, we tackle the challenges by\ncollaboratively optimizing two sequences of regions over a constructed\nhierarchical spatio-temporal region graph through relation attending and\nreconstruction, in which we further propose a message passing mechanism by\nspatial attention shifting between visual entities. Experimental results\ndemonstrate that our model can not only outperform baseline approaches\nsignificantly, but also produces visually meaningful facts to support visual\ngrounding. (Code is available at https://github.com/doc-doc/vRGV).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:20:39 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 07:20:32 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Xiao", "Junbin", ""], ["Shang", "Xindi", ""], ["Yang", "Xun", ""], ["Tang", "Sheng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2007.08826", "submitter": "Yuexiang Li", "authors": "Xing Tao, Yuexiang Li, Wenhui Zhou, Kai Ma, Yefeng Zheng", "title": "Revisiting Rubik's Cube: Self-supervised Learning with Volume-wise\n  Transformation for 3D Medical Image Segmentation", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning highly relies on the quantity of annotated data. However, the\nannotations for 3D volumetric medical data require experienced physicians to\nspend hours or even days for investigation. Self-supervised learning is a\npotential solution to get rid of the strong requirement of training data by\ndeeply exploiting raw data information. In this paper, we propose a novel\nself-supervised learning framework for volumetric medical images. Specifically,\nwe propose a context restoration task, i.e., Rubik's cube++, to pre-train 3D\nneural networks. Different from the existing context-restoration-based\napproaches, we adopt a volume-wise transformation for context permutation,\nwhich encourages network to better exploit the inherent 3D anatomical\ninformation of organs. Compared to the strategy of training from scratch,\nfine-tuning from the Rubik's cube++ pre-trained weight can achieve better\nperformance in various tasks such as pancreas segmentation and brain tissue\nsegmentation. The experimental results show that our self-supervised learning\nmethod can significantly improve the accuracy of 3D deep learning networks on\nvolumetric medical datasets without the use of extra data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:53:53 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Tao", "Xing", ""], ["Li", "Yuexiang", ""], ["Zhou", "Wenhui", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.08830", "submitter": "Jinyu Zhao", "authors": "Jinyu Zhao, Yusuke Monno, Masatoshi Okutomi", "title": "Polarimetric Multi-View Inverse Rendering", "comments": "Paper accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A polarization camera has great potential for 3D reconstruction since the\nangle of polarization (AoP) of reflected light is related to an object's\nsurface normal. In this paper, we propose a novel 3D reconstruction method\ncalled Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR) that\neffectively exploits geometric, photometric, and polarimetric cues extracted\nfrom input multi-view color polarization images. We first estimate camera poses\nand an initial 3D model by geometric reconstruction with a standard\nstructure-from-motion and multi-view stereo pipeline. We then refine the\ninitial model by optimizing photometric and polarimetric rendering errors using\nmulti-view RGB and AoP images, where we propose a novel polarimetric rendering\ncost function that enables us to effectively constrain each estimated surface\nvertex's normal while considering four possible ambiguous azimuth angles\nrevealed from the AoP measurement. Experimental results using both synthetic\nand real data demonstrate that our Polarimetric MVIR can reconstruct a detailed\n3D shape without assuming a specific polarized reflection depending on the\nmaterial.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:00:20 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhao", "Jinyu", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2007.08847", "submitter": "Debajit Sarma", "authors": "Debajit Sarma, V. Kavyasree and M.K. Bhuyan", "title": "Two-stream Fusion Model for Dynamic Hand Gesture Recognition using\n  3D-CNN and 2D-CNN Optical Flow guided Motion Template", "comments": "7 pages, 6 figures, 2 tables. Keywords: Action and gesture\n  recognition, Two-stream fusion model, Optical flow guided motion template\n  (OFMT), 2D and 3D-CNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The use of hand gestures can be a useful tool for many applications in the\nhuman-computer interaction community. In a broad range of areas hand gesture\ntechniques can be applied specifically in sign language recognition, robotic\nsurgery, etc. In the process of hand gesture recognition, proper detection, and\ntracking of the moving hand become challenging due to the varied shape and size\nof the hand. Here the objective is to track the movement of the hand\nirrespective of the shape, size, and color of the hand. And, for this, a motion\ntemplate guided by optical flow (OFMT) is proposed. OFMT is a compact\nrepresentation of the motion information of a gesture encoded into a single\nimage. In the experimentation, different datasets using bare hand with an open\npalm, and folded palm wearing green-glove are used, and in both cases, we could\ngenerate the OFMT images with equal precision. Recently, deep network-based\ntechniques have shown impressive improvements as compared to conventional\nhand-crafted feature-based techniques. Moreover, in the literature, it is seen\nthat the use of different streams with informative input data helps to increase\nthe performance in the recognition accuracy. This work basically proposes a\ntwo-stream fusion model for hand gesture recognition and a compact yet\nefficient motion template based on optical flow. Specifically, the two-stream\nnetwork consists of two layers: a 3D convolutional neural network (C3D) that\ntakes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D\nhas shown its efficiency in capturing spatio-temporal information of a video.\nWhereas OFMT helps to eliminate irrelevant gestures providing additional motion\ninformation. Though each stream can work independently, they are combined with\na fusion scheme to boost the recognition results. We have shown the efficiency\nof the proposed two-stream network on two databases.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:20:20 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Sarma", "Debajit", ""], ["Kavyasree", "V.", ""], ["Bhuyan", "M. K.", ""]]}, {"id": "2007.08849", "submitter": "Yz Gu", "authors": "Yinzheng Gu, Yihan Pan, Shizhe Chen", "title": "2nd Place Solution to ECCV 2020 VIPriors Object Detection Challenge", "comments": "Technical report for the ECCV 2020 VIPriors Object Detection\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we descibe our approach to the ECCV 2020 VIPriors Object\nDetection Challenge which took place from March to July in 2020. We show that\nby using state-of-the-art data augmentation strategies, model designs, and\npost-processing ensemble methods, it is possible to overcome the difficulty of\ndata shortage and obtain competitive results. Notably, our overall detection\nsystem achieves 36.6$\\%$ AP on the COCO 2017 validation set using only 10K\ntraining images without any pre-training or transfer learning weights ranking\nus 2nd place in the challenge.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:21:29 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Gu", "Yinzheng", ""], ["Pan", "Yihan", ""], ["Chen", "Shizhe", ""]]}, {"id": "2007.08854", "submitter": "Sibo Zhang", "authors": "Miao Liao, Feixiang Lu, Dingfu Zhou, Sibo Zhang, Wei Li, Ruigang Yang", "title": "DVI: Depth Guided Video Inpainting for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To get clear street-view and photo-realistic simulation in autonomous\ndriving, we present an automatic video inpainting algorithm that can remove\ntraffic agents from videos and synthesize missing regions with the guidance of\ndepth/point cloud. By building a dense 3D map from stitched point clouds,\nframes within a video are geometrically correlated via this common 3D map. In\norder to fill a target inpainting area in a frame, it is straightforward to\ntransform pixels from other frames into the current one with correct occlusion.\nFurthermore, we are able to fuse multiple videos through 3D point cloud\nregistration, making it possible to inpaint a target video with multiple source\nvideos. The motivation is to solve the long-time occlusion problem where an\noccluded area has never been visible in the entire video. To our knowledge, we\nare the first to fuse multiple videos for video inpainting. To verify the\neffectiveness of our approach, we build a large inpainting dataset in the real\nurban road environment with synchronized images and Lidar data including many\nchallenge scenes, e.g., long time occlusion. The experimental results show that\nthe proposed approach outperforms the state-of-the-art approaches for all the\ncriteria, especially the RMSE (Root Mean Squared Error) has been reduced by\nabout 13%.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:29:53 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Liao", "Miao", ""], ["Lu", "Feixiang", ""], ["Zhou", "Dingfu", ""], ["Zhang", "Sibo", ""], ["Li", "Wei", ""], ["Yang", "Ruigang", ""]]}, {"id": "2007.08856", "submitter": "Zhe Liu", "authors": "Tengteng Huang, Zhe Liu, Xiwu Chen and Xiang Bai", "title": "EPNet: Enhancing Point Features with Image Semantics for 3D Object\n  Detection", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at addressing two critical issues in the 3D detection\ntask, including the exploitation of multiple sensors~(namely LiDAR point cloud\nand camera image), as well as the inconsistency between the localization and\nclassification confidence. To this end, we propose a novel fusion module to\nenhance the point features with semantic image features in a point-wise manner\nwithout any image annotations. Besides, a consistency enforcing loss is\nemployed to explicitly encourage the consistency of both the localization and\nclassification confidence. We design an end-to-end learnable framework named\nEPNet to integrate these two components. Extensive experiments on the KITTI and\nSUN-RGBD datasets demonstrate the superiority of EPNet over the\nstate-of-the-art methods. Codes and models are available at:\n\\url{https://github.com/happinesslz/EPNet}.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:33:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Huang", "Tengteng", ""], ["Liu", "Zhe", ""], ["Chen", "Xiwu", ""], ["Bai", "Xiang", ""]]}, {"id": "2007.08872", "submitter": "Othman Sbai", "authors": "Othman Sbai, Camille Couprie and Mathieu Aubry", "title": "Impact of base dataset design on few-shot image classification", "comments": "23 pages, 11 figures, to appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality and generality of deep image features is crucially determined by\nthe data they have been trained on, but little is known about this often\noverlooked effect. In this paper, we systematically study the effect of\nvariations in the training data by evaluating deep features trained on\ndifferent image sets in a few-shot classification setting. The experimental\nprotocol we define allows to explore key practical questions. What is the\ninfluence of the similarity between base and test classes? Given a fixed\nannotation budget, what is the optimal trade-off between the number of images\nper class and the number of classes? Given a fixed dataset, can features be\nimproved by splitting or combining different classes? Should simple or diverse\nclasses be annotated? In a wide range of experiments, we provide clear answers\nto these questions on the miniImageNet, ImageNet and CUB-200 benchmarks. We\nalso show how the base dataset design can improve performance in few-shot\nclassification more drastically than replacing a simple baseline by an advanced\nstate of the art algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:58:50 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Sbai", "Othman", ""], ["Couprie", "Camille", ""], ["Aubry", "Mathieu", ""]]}, {"id": "2007.08883", "submitter": "Haoran Wang", "authors": "Haoran Wang, Ying Zhang, Zhong Ji, Yanwei Pang, Lin Ma", "title": "Consensus-Aware Visual-Semantic Embedding for Image-Text Matching", "comments": "Accepted by ECCV 2020, Code is publicly available at:\n  https://github.com/BruceW91/CVSE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching plays a central role in bridging vision and language.\nMost existing approaches only rely on the image-text instance pair to learn\ntheir representations, thereby exploiting their matching relationships and\nmaking the corresponding alignments. Such approaches only exploit the\nsuperficial associations contained in the instance pairwise data, with no\nconsideration of any external commonsense knowledge, which may hinder their\ncapabilities to reason the higher-level relationships between image and text.\nIn this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE)\nmodel to incorporate the consensus information, namely the commonsense\nknowledge shared between both modalities, into image-text matching.\nSpecifically, the consensus information is exploited by computing the\nstatistical co-occurrence correlations between the semantic concepts from the\nimage captioning corpus and deploying the constructed concept correlation graph\nto yield the consensus-aware concept (CAC) representations. Afterwards, CVSE\nlearns the associations and alignments between image and text based on the\nexploited consensus as well as the instance-level representations for both\nmodalities. Extensive experiments conducted on two public datasets verify that\nthe exploited consensus makes significant contributions to constructing more\nmeaningful visual-semantic embeddings, with the superior performances over the\nstate-of-the-art approaches on the bidirectional image and text retrieval task.\nOur code of this paper is available at: https://github.com/BruceW91/CVSE.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 10:22:57 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 12:35:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Haoran", ""], ["Zhang", "Ying", ""], ["Ji", "Zhong", ""], ["Pang", "Yanwei", ""], ["Ma", "Lin", ""]]}, {"id": "2007.08897", "submitter": "Dong Wei", "authors": "Hang Li, Dong Wei, Shilei Cao, Kai Ma, Liansheng Wang, and Yefeng\n  Zheng", "title": "Superpixel-Guided Label Softening for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of objects of interest is one of the central tasks in medical\nimage analysis, which is indispensable for quantitative analysis. When\ndeveloping machine-learning based methods for automated segmentation, manual\nannotations are usually used as the ground truth toward which the models learn\nto mimic. While the bulky parts of the segmentation targets are relatively easy\nto label, the peripheral areas are often difficult to handle due to ambiguous\nboundaries and the partial volume effect, etc., and are likely to be labeled\nwith uncertainty. This uncertainty in labeling may, in turn, result in\nunsatisfactory performance of the trained models. In this paper, we propose\nsuperpixel-based label softening to tackle the above issue. Generated by\nunsupervised over-segmentation, each superpixel is expected to represent a\nlocally homogeneous area. If a superpixel intersects with the annotation\nboundary, we consider a high probability of uncertain labeling within this\narea. Driven by this intuition, we soften labels in this area based on signed\ndistances to the annotation boundary and assign probability values within [0,\n1] to them, in comparison with the original \"hard\", binary labels of either 0\nor 1. The softened labels are then used to train the segmentation models\ntogether with the hard labels. Experimental results on a brain MRI dataset and\nan optical coherence tomography dataset demonstrate that this conceptually\nsimple and implementation-wise easy method achieves overall superior\nsegmentation performances to baseline and comparison methods for both 3D and 2D\nmedical images.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 10:55:59 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Hang", ""], ["Wei", "Dong", ""], ["Cao", "Shilei", ""], ["Ma", "Kai", ""], ["Wang", "Liansheng", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.08907", "submitter": "Andreas Kamilaris", "authors": "Sarah Kentsch, Savvas Karatsiolis, Andreas Kamilaris, Luca Tomhave and\n  Maximo Larry Lopez Caceres", "title": "Identification of Tree Species in Japanese Forests based on Aerial\n  Photography and Deep Learning", "comments": "Proc. of EnviroInfo 2020, Nicosia, Cyprus, September 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural forests are complex ecosystems whose tree species distribution and\ntheir ecosystem functions are still not well understood. Sustainable management\nof these forests is of high importance because of their significant role in\nclimate regulation, biodiversity, soil erosion and disaster prevention among\nmany other ecosystem services they provide. In Japan particularly, natural\nforests are mainly located in steep mountains, hence the use of aerial imagery\nin combination with computer vision are important modern tools that can be\napplied to forest research. Thus, this study constitutes a preliminary research\nin this field, aiming at classifying tree species in Japanese mixed forests\nusing UAV images and deep learning in two different mixed forest types: a black\npine (Pinus thunbergii)-black locust (Robinia pseudoacacia) and a larch (Larix\nkaempferi)-oak (Quercus mongolica) mixed forest. Our results indicate that it\nis possible to identify black locust trees with 62.6 % True Positives (TP) and\n98.1% True Negatives (TN), while lower precision was reached for larch trees\n(37.4% TP and 97.7% TN).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:26:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Kentsch", "Sarah", ""], ["Karatsiolis", "Savvas", ""], ["Kamilaris", "Andreas", ""], ["Tomhave", "Luca", ""], ["Caceres", "Maximo Larry Lopez", ""]]}, {"id": "2007.08919", "submitter": "Chih-Chung Hsu", "authors": "Chih-Chung Hsu and Hsin-Ti Ma", "title": "Edge-Preserving Guided Semantic Segmentation for VIPriors Challenge", "comments": "Technical report for VIPChallenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is one of the most attractive research fields in\ncomputer vision. In the VIPriors challenge, only very limited numbers of\ntraining samples are allowed, leading to that the current state-of-the-art and\ndeep learning-based semantic segmentation techniques are hard to train well. To\novercome this shortcoming, therefore, we propose edge-preserving guidance to\nobtain the extra prior information, to avoid the overfitting under small-scale\ntraining dataset. First, a two-channeled convolutional layer is concatenated to\nthe last layer of the conventional semantic segmentation network. Then, an edge\nmap is calculated from the ground truth by Sobel operation and followed by\nconcatenating a hard-thresholding operation to indicate whether the pixel is\nthe edge or not. Then, the two-dimensional cross-entropy loss is adopted to\ncalculate the loss between the predicted edge map and its ground truth, termed\nas an edge-preserving loss. In this way, the continuity of boundaries between\ndifferent instances can be forced by the proposed edge-preserving loss.\nExperiments demonstrate that the proposed method can achieve excellent\nperformance under small-scale training set, compared to state-of-the-art\nsemantic segmentation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:49:10 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Ma", "Hsin-Ti", ""]]}, {"id": "2007.08920", "submitter": "Mandy Lu", "authors": "Mandy Lu, Kathleen Poston, Adolf Pfefferbaum, Edith V. Sullivan, Li\n  Fei-Fei, Kilian M. Pohl, Juan Carlos Niebles and Ehsan Adeli", "title": "Vision-based Estimation of MDS-UPDRS Gait Scores for Assessing\n  Parkinson's Disease Motor Severity", "comments": "Accepted as a conference paper at MICCAI (Medical Image Computing and\n  Computer Assisted Intervention), Lima, Peru, October 2020. 11 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a progressive neurological disorder primarily\naffecting motor function resulting in tremor at rest, rigidity, bradykinesia,\nand postural instability. The physical severity of PD impairments can be\nquantified through the Movement Disorder Society Unified Parkinson's Disease\nRating Scale (MDS-UPDRS), a widely used clinical rating scale. Accurate and\nquantitative assessment of disease progression is critical to developing a\ntreatment that slows or stops further advancement of the disease. Prior work\nhas mainly focused on dopamine transport neuroimaging for diagnosis or costly\nand intrusive wearables evaluating motor impairments. For the first time, we\npropose a computer vision-based model that observes non-intrusive video\nrecordings of individuals, extracts their 3D body skeletons, tracks them\nthrough time, and classifies the movements according to the MDS-UPDRS gait\nscores. Experimental results show that our proposed method performs\nsignificantly better than chance and competing methods with an F1-score of 0.83\nand a balanced accuracy of 81%. This is the first benchmark for classifying PD\npatients based on MDS-UPDRS gait severity and could be an objective biomarker\nfor disease severity. Our work demonstrates how computer-assisted technologies\ncan be used to non-intrusively monitor patients and their motor impairments.\nThe code is available at\nhttps://github.com/mlu355/PD-Motor-Severity-Estimation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:49:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lu", "Mandy", ""], ["Poston", "Kathleen", ""], ["Pfefferbaum", "Adolf", ""], ["Sullivan", "Edith V.", ""], ["Fei-Fei", "Li", ""], ["Pohl", "Kilian M.", ""], ["Niebles", "Juan Carlos", ""], ["Adeli", "Ehsan", ""]]}, {"id": "2007.08921", "submitter": "Tianheng Cheng", "authors": "Tianheng Cheng and Xinggang Wang and Lichao Huang and Wenyu Liu", "title": "Boundary-preserving Mask R-CNN", "comments": "17 pages, 8 figures. Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous efforts have been made to improve mask localization accuracy in\ninstance segmentation. Modern instance segmentation methods relying on fully\nconvolutional networks perform pixel-wise classification, which ignores object\nboundaries and shapes, leading coarse and indistinct mask prediction results\nand imprecise localization. To remedy these problems, we propose a conceptually\nsimple yet effective Boundary-preserving Mask R-CNN (BMask R-CNN) to leverage\nobject boundary information to improve mask localization accuracy. BMask R-CNN\ncontains a boundary-preserving mask head in which object boundary and mask are\nmutually learned via feature fusion blocks. As a result, the predicted masks\nare better aligned with object boundaries. Without bells and whistles, BMask\nR-CNN outperforms Mask R-CNN by a considerable margin on the COCO dataset; in\nthe Cityscapes dataset, there are more accurate boundary groundtruths\navailable, so that BMask R-CNN obtains remarkable improvements over Mask R-CNN.\nBesides, it is not surprising to observe that BMask R-CNN obtains more obvious\nimprovement when the evaluation criterion requires better localization (e.g.,\nAP$_{75}$) as shown in Fig.1. Code and models are available at\n\\url{https://github.com/hustvl/BMaskR-CNN}.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:54:02 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Cheng", "Tianheng", ""], ["Wang", "Xinggang", ""], ["Huang", "Lichao", ""], ["Liu", "Wenyu", ""]]}, {"id": "2007.08922", "submitter": "Serkan S\\\"ul\\\"un", "authors": "Serkan Sulun, A. Murat Tekalp", "title": "Can Learned Frame-Prediction Compete with Block-Motion Compensation for\n  Video Coding?", "comments": "Accepted for publication in Springer Journal of Signal, Image and\n  Video Processing", "journal-ref": null, "doi": "10.1007/s11760-020-01751-y", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given recent advances in learned video prediction, we investigate whether a\nsimple video codec using a pre-trained deep model for next frame prediction\nbased on previously encoded/decoded frames without sending any motion side\ninformation can compete with standard video codecs based on block-motion\ncompensation. Frame differences given learned frame predictions are encoded by\na standard still-image (intra) codec. Experimental results show that the\nrate-distortion performance of the simple codec with symmetric complexity is on\naverage better than that of x264 codec on 10 MPEG test videos, but does not yet\nreach the level of x265 codec. This result demonstrates the power of learned\nframe prediction (LFP), since unlike motion compensation, LFP does not use\ninformation from the current picture. The implications of training with L1, L2,\nor combined L2 and adversarial loss on prediction performance and compression\nefficiency are analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:54:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sulun", "Serkan", ""], ["Tekalp", "A. Murat", ""]]}, {"id": "2007.08939", "submitter": "Alexander Grabner", "authors": "Alexander Grabner, Yaming Wang, Peizhao Zhang, Peihong Guo, Tong Xiao,\n  Peter Vajda, Peter M. Roth, Vincent Lepetit", "title": "Geometric Correspondence Fields: Learned Differentiable Rendering for 3D\n  Pose Refinement in the Wild", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel 3D pose refinement approach based on differentiable\nrendering for objects of arbitrary categories in the wild. In contrast to\nprevious methods, we make two main contributions: First, instead of comparing\nreal-world images and synthetic renderings in the RGB or mask space, we compare\nthem in a feature space optimized for 3D pose refinement. Second, we introduce\na novel differentiable renderer that learns to approximate the rasterization\nbackward pass from data instead of relying on a hand-crafted algorithm. For\nthis purpose, we predict deep cross-domain correspondences between RGB images\nand 3D model renderings in the form of what we call geometric correspondence\nfields. These correspondence fields serve as pixel-level gradients which are\nanalytically propagated backward through the rendering pipeline to perform a\ngradient-based optimization directly on the 3D pose. In this way, we precisely\nalign 3D models to objects in RGB images which results in significantly\nimproved 3D pose estimates. We evaluate our approach on the challenging Pix3D\ndataset and achieve up to 55% relative improvement compared to state-of-the-art\nrefinement methods in multiple metrics.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:34:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Grabner", "Alexander", ""], ["Wang", "Yaming", ""], ["Zhang", "Peizhao", ""], ["Guo", "Peihong", ""], ["Xiao", "Tong", ""], ["Vajda", "Peter", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2007.08943", "submitter": "Jiahao Lin", "authors": "Jiahao Lin, Gim Hee Lee", "title": "HDNet: Human Depth Estimation for Multi-Person Camera-Space Localization", "comments": "16 pages, 5 figures. Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current works on multi-person 3D pose estimation mainly focus on the\nestimation of the 3D joint locations relative to the root joint and ignore the\nabsolute locations of each pose. In this paper, we propose the Human Depth\nEstimation Network (HDNet), an end-to-end framework for absolute root joint\nlocalization in the camera coordinate space. Our HDNet first estimates the 2D\nhuman pose with heatmaps of the joints. These estimated heatmaps serve as\nattention masks for pooling features from image regions corresponding to the\ntarget person. A skeleton-based Graph Neural Network (GNN) is utilized to\npropagate features among joints. We formulate the target depth regression as a\nbin index estimation problem, which can be transformed with a soft-argmax\noperation from the classification output of our HDNet. We evaluate our HDNet on\nthe root joint localization and root-relative 3D pose estimation tasks with two\nbenchmark datasets, i.e., Human3.6M and MuPoTS-3D. The experimental results\nshow that we outperform the previous state-of-the-art consistently under\nmultiple evaluation metrics. Our source code is available at:\nhttps://github.com/jiahaoLjh/HumanDepth.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:44:23 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lin", "Jiahao", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2007.08969", "submitter": "Petrissa Zell", "authors": "Petrissa Zell, Bodo Rosenhahn, Bastian Wandt", "title": "Weakly-supervised Learning of Human Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a weakly-supervised learning framework for dynamics\nestimation from human motion. Although there are many solutions to capture pure\nhuman motion readily available, their data is not sufficient to analyze quality\nand efficiency of movements. Instead, the forces and moments driving human\nmotion (the dynamics) need to be considered. Since recording dynamics is a\nlaborious task that requires expensive sensors and complex, time-consuming\noptimization, dynamics data sets are small compared to human motion data sets\nand are rarely made public. The proposed approach takes advantage of easily\nobtainable motion data which enables weakly-supervised learning on small\ndynamics sets and weakly-supervised domain transfer. Our method includes novel\nneural network (NN) layers for forward and inverse dynamics during end-to-end\ntraining. On this basis, a cyclic loss between pure motion data can be\nminimized, i.e. no ground truth forces and moments are required during\ntraining. The proposed method achieves state-of-the-art results in terms of\nground reaction force, ground reaction moment and joint torque regression and\nis able to maintain good performance on substantially reduced sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:32:57 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 11:10:00 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zell", "Petrissa", ""], ["Rosenhahn", "Bodo", ""], ["Wandt", "Bastian", ""]]}, {"id": "2007.08971", "submitter": "Rongliang Wu", "authors": "Rongliang Wu, Shijian Lu", "title": "LEED: Label-Free Expression Editing via Disentanglement", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on facial expression editing have obtained very promising\nprogress. On the other hand, existing methods face the constraint of requiring\na large amount of expression labels which are often expensive and\ntime-consuming to collect. This paper presents an innovative label-free\nexpression editing via disentanglement (LEED) framework that is capable of\nediting the expression of both frontal and profile facial images without\nrequiring any expression label. The idea is to disentangle the identity and\nexpression of a facial image in the expression manifold, where the neutral face\ncaptures the identity attribute and the displacement between the neutral image\nand the expressive image captures the expression attribute. Two novel losses\nare designed for optimal expression disentanglement and consistent synthesis,\nincluding a mutual expression information loss that aims to extract pure\nexpression-related features and a siamese loss that aims to enhance the\nexpression similarity between the synthesized image and the reference image.\nExtensive experiments over two public facial expression datasets show that LEED\nachieves superior facial expression editing qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:36:15 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wu", "Rongliang", ""], ["Lu", "Shijian", ""]]}, {"id": "2007.08973", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Kyriacos Nikiforou, Oriol Vinyals, Andre Saraiva,\n  Rishabh Kabra, Loic Matthey, Chris Burgess, Malcolm Reynolds, Richard\n  Tanburn, Marta Garnelo, Murray Shanahan", "title": "AlignNet: Unsupervised Entity Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed deep learning models are able to learn to segment scenes\ninto component objects without supervision. This opens many new and exciting\navenues of research, allowing agents to take objects (or entities) as inputs,\nrather that pixels. Unfortunately, while these models provide excellent\nsegmentation of a single frame, they do not keep track of how objects segmented\nat one time-step correspond (or align) to those at a later time-step. The\nalignment (or correspondence) problem has impeded progress towards using object\nrepresentations in downstream tasks. In this paper we take steps towards\nsolving the alignment problem, presenting the AlignNet, an unsupervised\nalignment module.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:38:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 13:12:07 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Creswell", "Antonia", ""], ["Nikiforou", "Kyriacos", ""], ["Vinyals", "Oriol", ""], ["Saraiva", "Andre", ""], ["Kabra", "Rishabh", ""], ["Matthey", "Loic", ""], ["Burgess", "Chris", ""], ["Reynolds", "Malcolm", ""], ["Tanburn", "Richard", ""], ["Garnelo", "Marta", ""], ["Shanahan", "Murray", ""]]}, {"id": "2007.08979", "submitter": "Taeyoung Son", "authors": "Taeyoung Son, Juwon Kang, Namyup Kim, Sunghyun Cho and Suha Kwak", "title": "URIE: Universal Image Enhancement for Visual Recognition in the Wild", "comments": "Accepted as a conference paper at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great advances in visual recognition, it has been witnessed that\nrecognition models trained on clean images of common datasets are not robust\nagainst distorted images in the real world. To tackle this issue, we present a\nUniversal and Recognition-friendly Image Enhancement network, dubbed URIE,\nwhich is attached in front of existing recognition models and enhances\ndistorted input to improve their performance without retraining them. URIE is\nuniversal in that it aims to handle various factors of image degradation and to\nbe incorporated with any arbitrary recognition models. Also, it is\nrecognition-friendly since it is optimized to improve the robustness of\nfollowing recognition models, instead of perceptual quality of output image.\nOur experiments demonstrate that URIE can handle various and latent image\ndistortions and improve the performance of existing models for five diverse\nrecognition tasks when input images are degraded.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:45:56 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 06:39:24 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 12:21:01 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Son", "Taeyoung", ""], ["Kang", "Juwon", ""], ["Kim", "Namyup", ""], ["Cho", "Sunghyun", ""], ["Kwak", "Suha", ""]]}, {"id": "2007.08988", "submitter": "Remi Pautrat", "authors": "R\\'emi Pautrat, Viktor Larsson, Martin R. Oswald and Marc Pollefeys", "title": "Online Invariance Selection for Local Feature Descriptors", "comments": "27 pages, Accepted at ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be invariant, or not to be invariant: that is the question formulated in\nthis work about local descriptors. A limitation of current feature descriptors\nis the trade-off between generalization and discriminative power: more\ninvariance means less informative descriptors. We propose to overcome this\nlimitation with a disentanglement of invariance in local descriptors and with\nan online selection of the most appropriate invariance given the context. Our\nframework consists in a joint learning of multiple local descriptors with\ndifferent levels of invariance and of meta descriptors encoding the regional\nvariations of an image. The similarity of these meta descriptors across images\nis used to select the right invariance when matching the local descriptors. Our\napproach, named Local Invariance Selection at Runtime for Descriptors (LISRD),\nenables descriptors to adapt to adverse changes in images, while remaining\ndiscriminative when invariance is not required. We demonstrate that our method\ncan boost the performance of current descriptors and outperforms\nstate-of-the-art descriptors in several matching tasks, when evaluated on\nchallenging datasets with day-night illumination as well as viewpoint changes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:08:22 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 11:53:54 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 15:16:23 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Pautrat", "R\u00e9mi", ""], ["Larsson", "Viktor", ""], ["Oswald", "Martin R.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2007.09033", "submitter": "Guoxi Huang", "authors": "Guoxi Huang and Adrian G. Bors", "title": "Region-based Non-local Operation for Video Classification", "comments": null, "journal-ref": "ICPR2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) model long-range dependencies by deeply\nstacking convolution operations with small window sizes, which makes the\noptimizations difficult. This paper presents region-based non-local (RNL)\noperations as a family of self-attention mechanisms, which can directly capture\nlong-range dependencies without using a deep stack of local operations. Given\nan intermediate feature map, our method recalibrates the feature at a position\nby aggregating the information from the neighboring regions of all positions.\nBy combining a channel attention module with the proposed RNL, we design an\nattention chain, which can be integrated into the off-the-shelf CNNs for\nend-to-end training. We evaluate our method on two video classification\nbenchmarks. The experimental results of our method outperform other attention\nmechanisms, and we achieve state-of-the-art performance on the\nSomething-Something V1 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:57:05 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 22:13:13 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 22:57:35 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2020 21:39:57 GMT"}, {"version": "v5", "created": "Tue, 2 Feb 2021 00:21:37 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Huang", "Guoxi", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2007.09049", "submitter": "Ganchao Tan", "authors": "Ganchao Tan, Daqing Liu, Meng Wang, Zheng-Jun Zha", "title": "Learning to Discretely Compose Reasoning Module Networks for Video\n  Captioning", "comments": "Accepted at IJCAI 2020 Main Track. Sole copyright holder is IJCAI.\n  Code is available at https://github.com/tgc1997/RMN", "journal-ref": "IJCAI 2020, Pages 745-752", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural language descriptions for videos, i.e., video captioning,\nessentially requires step-by-step reasoning along the generation process. For\nexample, to generate the sentence \"a man is shooting a basketball\", we need to\nfirst locate and describe the subject \"man\", next reason out the man is\n\"shooting\", then describe the object \"basketball\" of shooting. However,\nexisting visual reasoning methods designed for visual question answering are\nnot appropriate to video captioning, for it requires more complex visual\nreasoning on videos over both space and time, and dynamic module composition\nalong the generation process. In this paper, we propose a novel visual\nreasoning approach for video captioning, named Reasoning Module Networks (RMN),\nto equip the existing encoder-decoder framework with the above reasoning\ncapacity. Specifically, our RMN employs 1) three sophisticated spatio-temporal\nreasoning modules, and 2) a dynamic and discrete module selector trained by a\nlinguistic loss with a Gumbel approximation. Extensive experiments on MSVD and\nMSR-VTT datasets demonstrate the proposed RMN outperforms the state-of-the-art\nmethods while providing an explicit and explainable generation process. Our\ncode is available at https://github.com/tgc1997/RMN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:27:37 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Tan", "Ganchao", ""], ["Liu", "Daqing", ""], ["Wang", "Meng", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2007.09062", "submitter": "Youwei Pang", "authors": "Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu", "title": "Multi-scale Interactive Network for Salient Object Detection", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning based salient object detection methods achieve great progress.\nHowever, the variable scale and unknown category of salient objects are great\nchallenges all the time. These are closely related to the utilization of\nmulti-level and multi-scale features. In this paper, we propose the aggregate\ninteraction modules to integrate the features from adjacent levels, in which\nless noise is introduced because of only using small up-/down-sampling rates.\nTo obtain more efficient multi-scale features from the integrated features, the\nself-interaction modules are embedded in each decoder unit. Besides, the class\nimbalance issue caused by the scale variation weakens the effect of the binary\ncross entropy loss and results in the spatial inconsistency of the predictions.\nTherefore, we exploit the consistency-enhanced loss to highlight the\nfore-/back-ground difference and preserve the intra-class consistency.\nExperimental results on five benchmark datasets demonstrate that the proposed\nmethod without any post-processing performs favorably against 23\nstate-of-the-art approaches. The source code will be publicly available at\nhttps://github.com/lartpang/MINet.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:41:37 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Pang", "Youwei", ""], ["Zhao", "Xiaoqi", ""], ["Zhang", "Lihe", ""], ["Lu", "Huchuan", ""]]}, {"id": "2007.09070", "submitter": "Hao Liu", "authors": "Hao Liu, Pieter Abbeel", "title": "Hybrid Discriminative-Generative Training via Contrastive Learning", "comments": "Code: https://github.com/lhao499/HDGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning and supervised learning have both seen significant\nprogress and success. However, thus far they have largely been treated as two\nseparate objectives, brought together only by having a shared neural network.\nIn this paper we show that through the perspective of hybrid\ndiscriminative-generative training of energy-based models we can make a direct\nconnection between contrastive learning and supervised learning. Beyond\npresenting this unified view, we show our specific choice of approximation of\nthe energy-based loss outperforms the existing practice in terms of\nclassification accuracy of WideResNet on CIFAR-10 and CIFAR-100. It also leads\nto improved performance on robustness, out-of-distribution detection, and\ncalibration.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:50:34 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 07:34:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Hao", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2007.09073", "submitter": "Umberto Michieli", "authors": "Umberto Michieli, Edoardo Borsato, Luca Rossi, Pietro Zanuttigh", "title": "GMNet: Graph Matching Network for Large Scale Part Semantic Segmentation\n  in the Wild", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic segmentation of parts of objects in the wild is a challenging\ntask in which multiple instances of objects and multiple parts within those\nobjects must be detected in the scene. This problem remains nowadays very\nmarginally explored, despite its fundamental importance towards detailed object\nunderstanding. In this work, we propose a novel framework combining higher\nobject-level context conditioning and part-level spatial relationships to\naddress the task. To tackle object-level ambiguity, a class-conditioning module\nis introduced to retain class-level semantics when learning parts-level\nsemantics. In this way, mid-level features carry also this information prior to\nthe decoding stage. To tackle part-level ambiguity and localization we propose\na novel adjacency graph-based module that aims at matching the relative spatial\nrelationships between ground truth and predicted parts. The experimental\nevaluation on the Pascal-Part dataset shows that we achieve state-of-the-art\nresults on this task.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:53:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Michieli", "Umberto", ""], ["Borsato", "Edoardo", ""], ["Rossi", "Luca", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2007.09077", "submitter": "Siyu Huang", "authors": "Siyu Huang, Haoyi Xiong, Zhi-Qi Cheng, Qingzhong Wang, Xingran Zhou,\n  Bihan Wen, Jun Huan, Dejing Dou", "title": "Generating Person Images with Appearance-aware Pose Stylizer", "comments": "Appearing at IJCAI 2020. The code is available at\n  https://github.com/siyuhuang/PoseStylizer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generation of high-quality person images is challenging, due to the\nsophisticated entanglements among image factors, e.g., appearance, pose,\nforeground, background, local details, global structures, etc. In this paper,\nwe present a novel end-to-end framework to generate realistic person images\nbased on given person poses and appearances. The core of our framework is a\nnovel generator called Appearance-aware Pose Stylizer (APS) which generates\nhuman images by coupling the target pose with the conditioned person appearance\nprogressively. The framework is highly flexible and controllable by effectively\ndecoupling various complex person image factors in the encoding phase, followed\nby re-coupling them in the decoding phase. In addition, we present a new\nnormalization method named adaptive patch normalization, which enables\nregion-specific normalization and shows a good performance when adopted in\nperson image generation model. Experiments on two benchmark datasets show that\nour method is capable of generating visually appealing and realistic-looking\nresults using arbitrary image and pose inputs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:58:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Huang", "Siyu", ""], ["Xiong", "Haoyi", ""], ["Cheng", "Zhi-Qi", ""], ["Wang", "Qingzhong", ""], ["Zhou", "Xingran", ""], ["Wen", "Bihan", ""], ["Huan", "Jun", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.09081", "submitter": "Hongge Chen", "authors": "Hongge Chen, Si Si, Yang Li, Ciprian Chelba, Sanjiv Kumar, Duane\n  Boning, Cho-Jui Hsieh", "title": "Multi-Stage Influence Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage training and knowledge transfer, from a large-scale pretraining\ntask to various finetuning tasks, have revolutionized natural language\nprocessing and computer vision resulting in state-of-the-art performance\nimprovements. In this paper, we develop a multi-stage influence function score\nto track predictions from a finetuned model all the way back to the pretraining\ndata. With this score, we can identify the pretraining examples in the\npretraining task that contribute most to a prediction in the finetuning task.\nThe proposed multi-stage influence function generalizes the original influence\nfunction for a single model in (Koh & Liang, 2017), thereby enabling influence\ncomputation through both pretrained and finetuned models. We study two\ndifferent scenarios with the pretrained embeddings fixed or updated in the\nfinetuning tasks. We test our proposed method in various experiments to show\nits effectiveness and potential applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:03:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chen", "Hongge", ""], ["Si", "Si", ""], ["Li", "Yang", ""], ["Chelba", "Ciprian", ""], ["Kumar", "Sanjiv", ""], ["Boning", "Duane", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2007.09084", "submitter": "Subeesh Vasu", "authors": "Subeesh Vasu, Mateusz Kozinski, Leonardo Citraro, and Pascal Fua", "title": "TopoAL: An Adversarial Learning Approach for Topology-Aware Road\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art approaches to road extraction from aerial images rely\non a CNN trained to label road pixels as foreground and remainder of the image\nas background. The CNN is usually trained by minimizing pixel-wise losses,\nwhich is less than ideal to produce binary masks that preserve the road\nnetwork's global connectivity. To address this issue, we introduce an\nAdversarial Learning (AL) strategy tailored for our purposes. A naive one would\ntreat the segmentation network as a generator and would feed its output along\nwith ground-truth segmentations to a discriminator. It would then train the\ngenerator and discriminator jointly. We will show that this is not enough\nbecause it does not capture the fact that most errors are local and need to be\ntreated as such. Instead, we use a more sophisticated discriminator that\nreturns a label pyramid describing what portions of the road network are\ncorrect at several different scales. This discriminator and the structured\nlabels it returns are what gives our approach its edge and we will show that it\noutperforms state-of-the-art ones on the challenging RoadTracer dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:06:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Vasu", "Subeesh", ""], ["Kozinski", "Mateusz", ""], ["Citraro", "Leonardo", ""], ["Fua", "Pascal", ""]]}, {"id": "2007.09107", "submitter": "Emanuele Colleoni", "authors": "Emanuele Colleoni, Philip Edwards, Danail Stoyanov", "title": "Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic tool segmentation in surgical videos is important for surgical scene\nunderstanding and computer-assisted interventions as well as for the\ndevelopment of robotic automation. The problem is challenging because different\nillumination conditions, bleeding, smoke and occlusions can reduce algorithm\nrobustness. At present labelled data for training deep learning models is still\nlacking for semantic surgical instrument segmentation and in this paper we show\nthat it may be possible to use robot kinematic data coupled with laparoscopic\nimages to alleviate the labelling problem. We propose a new deep learning based\nmodel for parallel processing of both laparoscopic and simulation images for\nrobust segmentation of surgical tools. Due to the lack of laparoscopic frames\nannotated with both segmentation ground truth and kinematic information a new\ncustom dataset was generated using the da Vinci Research Kit (dVRK) and is made\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:33:33 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 08:27:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Colleoni", "Emanuele", ""], ["Edwards", "Philip", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2007.09115", "submitter": "Artem Moskalev", "authors": "Ivan Sosnovik, Artem Moskalev, Arnold Smeulders", "title": "Scale Equivariance Improves Siamese Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese trackers turn tracking into similarity estimation between a template\nand the candidate regions in the frame. Mathematically, one of the key\ningredients of success of the similarity function is translation equivariance.\nNon-translation-equivariant architectures induce a positional bias during\ntraining, so the location of the target will be hard to recover from the\nfeature space. In real life scenarios, objects undergoe various transformations\nother than translation, such as rotation or scaling. Unless the model has an\ninternal mechanism to handle them, the similarity may degrade. In this paper,\nwe focus on scaling and we aim to equip the Siamese network with additional\nbuilt-in scale equivariance to capture the natural variations of the target a\npriori. We develop the theory for scale-equivariant Siamese trackers, and\nprovide a simple recipe for how to make a wide range of existing trackers\nscale-equivariant. We present SE-SiamFC, a scale-equivariant variant of SiamFC\nbuilt according to the recipe. We conduct experiments on OTB and VOT benchmarks\nand on the synthetically generated T-MNIST and S-MNIST datasets. We demonstrate\nthat a built-in additional scale equivariance is useful for visual object\ntracking.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:55:51 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 12:29:44 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sosnovik", "Ivan", ""], ["Moskalev", "Artem", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2007.09162", "submitter": "Yandong Li", "authors": "Yandong Li, Di Huang, Danfeng Qin, Liqiang Wang, Boqing Gong", "title": "Improving Object Detection with Selective Self-supervised Self-training", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to leverage Web images to augment human-curated object detection\ndatasets. Our approach is two-pronged. On the one hand, we retrieve Web images\nby image-to-image search, which incurs less domain shift from the curated data\nthan other search methods. The Web images are diverse, supplying a wide variety\nof object poses, appearances, their interactions with the context, etc. On the\nother hand, we propose a novel learning method motivated by two parallel lines\nof work that explore unlabeled data for image classification: self-training and\nself-supervised learning. They fail to improve object detectors in their\nvanilla forms due to the domain gap between the Web images and curated\ndatasets. To tackle this challenge, we propose a selective net to rectify the\nsupervision signals in Web images. It not only identifies positive bounding\nboxes but also creates a safe zone for mining hard negative boxes. We report\nstate-of-the-art results on detecting backpacks and chairs from everyday\nscenes, along with other challenging object classes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:05:01 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 19:34:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Yandong", ""], ["Huang", "Di", ""], ["Qin", "Danfeng", ""], ["Wang", "Liqiang", ""], ["Gong", "Boqing", ""]]}, {"id": "2007.09163", "submitter": "C.-H. Huck Yang", "authors": "Hao-Hsiang Yang, Chao-Han Huck Yang, Yu-Chiang Frank Wang", "title": "Wavelet Channel Attention Module with a Fusion Network for Single Image\n  Deraining", "comments": "Accepted to IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image deraining is a crucial problem because rain severely degenerates\nthe visibility of images and affects the performance of computer vision tasks\nlike outdoor surveillance systems and intelligent vehicles. In this paper, we\npropose the new convolutional neural network (CNN) called the wavelet channel\nattention module with a fusion network. Wavelet transform and the inverse\nwavelet transform are substituted for down-sampling and up-sampling so feature\nmaps from the wavelet transform and convolutions contain different frequencies\nand scales. Furthermore, feature maps are integrated by channel attention. Our\nproposed network learns confidence maps of four sub-band images derived from\nthe wavelet transform of the original images. Finally, the clear image can be\nwell restored via the wavelet reconstruction and fusion of the low-frequency\npart and high-frequency parts. Several experimental results on synthetic and\nreal images present that the proposed algorithm outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:06:13 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yang", "Hao-Hsiang", ""], ["Yang", "Chao-Han Huck", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2007.09170", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Dai Hasegawa, Naoshi Kaneko, Gustav Eje Henter,\n  Hedvig Kjellstr\\\"om", "title": "Moving fast and slow: Analysis of representations and post-processing in\n  speech-driven automatic gesture generation", "comments": "Extension of our IVA'19 paper. Accepted at the International Journal\n  of Human-Computer Interaction. See more at\n  https://svito-zar.github.io/audio2gestures/. arXiv admin note: substantial\n  text overlap with arXiv:1903.03369", "journal-ref": "Int. J. Hum. Comput.Interact.(2021)", "doi": "10.1080/10447318.2021.1883883", "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for speech-driven gesture production,\napplicable to virtual agents to enhance human-computer interaction.\nSpecifically, we extend recent deep-learning-based, data-driven methods for\nspeech-driven gesture generation by incorporating representation learning. Our\nmodel takes speech as input and produces gestures as output, in the form of a\nsequence of 3D coordinates. We provide an analysis of different representations\nfor the input (speech) and the output (motion) of the network by both objective\nand subjective evaluations. We also analyse the importance of smoothing of the\nproduced motion. Our results indicated that the proposed method improved on our\nbaseline in terms of objective measures. For example, it better captured the\nmotion dynamics and better matched the motion-speed distribution. Moreover, we\nperformed user studies on two different datasets. The studies confirmed that\nour proposed method is perceived as more natural than the baseline, although\nthe difference in the studies was eliminated by appropriate post-processing:\nhip-centering and smoothing. We conclude that it is important to take both\nmotion representation and post-processing into account when designing an\nautomatic gesture-production method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:32:00 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 17:30:30 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 12:49:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kucherenko", "Taras", ""], ["Hasegawa", "Dai", ""], ["Kaneko", "Naoshi", ""], ["Henter", "Gustav Eje", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2007.09178", "submitter": "Jordan Ubbens", "authors": "Jordan Ubbens, Tewodros Ayalew, Steve Shirtliffe, Anique Josuttes,\n  Curtis Pozniak, Ian Stavness", "title": "AutoCount: Unsupervised Segmentation and Counting of Organs in Field\n  Images", "comments": "Computer Vision Problems in Plant Phenotyping (CVPPP) in conjunction\n  with ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting plant organs such as heads or tassels from outdoor imagery is a\npopular benchmark computer vision task in plant phenotyping, which has been\npreviously investigated in the literature using state-of-the-art supervised\ndeep learning techniques. However, the annotation of organs in field images is\ntime-consuming and prone to errors. In this paper, we propose a fully\nunsupervised technique for counting dense objects such as plant organs. We use\na convolutional network-based unsupervised segmentation method followed by two\npost-hoc optimization steps. The proposed technique is shown to provide\ncompetitive counting performance on a range of organ counting tasks in sorghum\n(S. bicolor) and wheat (T. aestivum) with no dataset-dependent tuning or\nmodifications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:27:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ubbens", "Jordan", ""], ["Ayalew", "Tewodros", ""], ["Shirtliffe", "Steve", ""], ["Josuttes", "Anique", ""], ["Pozniak", "Curtis", ""], ["Stavness", "Ian", ""]]}, {"id": "2007.09180", "submitter": "Qin Wang", "authors": "Yuan Tian, Qin Wang, Zhiwu Huang, Wen Li, Dengxin Dai, Minghao Yang,\n  Jun Wang, Olga Fink", "title": "Off-Policy Reinforcement Learning for Efficient and Effective GAN\n  Architecture Search", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new reinforcement learning (RL) based neural\narchitecture search (NAS) methodology for effective and efficient generative\nadversarial network (GAN) architecture search. The key idea is to formulate the\nGAN architecture search problem as a Markov decision process (MDP) for smoother\narchitecture sampling, which enables a more effective RL-based search algorithm\nby targeting the potential global optimal architecture. To improve efficiency,\nwe exploit an off-policy GAN architecture search algorithm that makes efficient\nuse of the samples generated by previous policies. Evaluation on two standard\nbenchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed\nmethod is able to discover highly competitive architectures for generally\nbetter image generation results with a considerably reduced computational\nburden: 7 GPU hours. Our code is available at\nhttps://github.com/Yuantian013/E2GAN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:29:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tian", "Yuan", ""], ["Wang", "Qin", ""], ["Huang", "Zhiwu", ""], ["Li", "Wen", ""], ["Dai", "Dengxin", ""], ["Yang", "Minghao", ""], ["Wang", "Jun", ""], ["Fink", "Olga", ""]]}, {"id": "2007.09183", "submitter": "Xiaokang Chen", "authors": "Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian,\n  Hongsheng Li, Gang Zeng", "title": "Bi-directional Cross-Modality Feature Propagation with\n  Separation-and-Aggregation Gate for RGB-D Semantic Segmentation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information has proven to be a useful cue in the semantic segmentation\nof RGB-D images for providing a geometric counterpart to the RGB\nrepresentation. Most existing works simply assume that depth measurements are\naccurate and well-aligned with the RGB pixels and models the problem as a\ncross-modal feature fusion to obtain better feature representations to achieve\nmore accurate segmentation. This, however, may not lead to satisfactory results\nas actual depth data are generally noisy, which might worsen the accuracy as\nthe networks go deeper.\n  In this paper, we propose a unified and efficient Cross-modality Guided\nEncoder to not only effectively recalibrate RGB feature responses, but also to\ndistill accurate depth information via multiple stages and aggregate the two\nrecalibrated representations alternatively. The key of the proposed\narchitecture is a novel Separation-and-Aggregation Gating operation that\njointly filters and recalibrates both representations before cross-modality\naggregation. Meanwhile, a Bi-direction Multi-step Propagation strategy is\nintroduced, on the one hand, to help to propagate and fuse information between\nthe two modalities, and on the other hand, to preserve their specificity along\nthe long-term propagation process. Besides, our proposed encoder can be easily\ninjected into the previous encoder-decoder structures to boost their\nperformance on RGB-D semantic segmentation. Our model outperforms\nstate-of-the-arts consistently on both in-door and out-door challenging\ndatasets. Code of this work is available at https://charlescxk.github.io/\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:35:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Xiaokang", ""], ["Lin", "Kwan-Yee", ""], ["Wang", "Jingbo", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["Li", "Hongsheng", ""], ["Zeng", "Gang", ""]]}, {"id": "2007.09191", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli,\n  Stewart Lee Zuckerbrod", "title": "Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal\n  Fundus Images using Generative Adversarial Networks", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluorescein Angiography (FA) is a technique that employs the designated\ncamera for Fundus photography incorporating excitation and barrier filters. FA\nalso requires fluorescein dye that is injected intravenously, which might cause\nadverse effects ranging from nausea, vomiting to even fatal anaphylaxis.\nCurrently, no other fast and non-invasive technique exists that can generate FA\nwithout coupling with Fundus photography. To eradicate the need for an invasive\nFA extraction procedure, we introduce an Attention-based Generative network\nthat can synthesize Fluorescein Angiography from Fundus images. The proposed\ngan incorporates multiple attention based skip connections in generators and\ncomprises novel residual blocks for both generators and discriminators. It\nutilizes reconstruction, feature-matching, and perceptual loss along with\nadversarial training to produces realistic Angiograms that is hard for experts\nto distinguish from real ones. Our experiments confirm that the proposed\narchitecture surpasses recent state-of-the-art generative networks for\nfundus-to-angio translation task.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:58:44 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Hossain", "Khondker Fariha", ""], ["Tavakkoli", "Alireza", ""], ["Zuckerbrod", "Stewart Lee", ""]]}, {"id": "2007.09198", "submitter": "Sibo Zhang", "authors": "Miao Liao, Sibo Zhang, Peng Wang, Hao Zhu, Xinxin Zuo, and Ruigang\n  Yang", "title": "Speech2Video Synthesis with 3D Skeleton Regularization and Expressive\n  Body Poses", "comments": "Accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to convert given speech audio to a\nphoto-realistic speaking video of a specific person, where the output video has\nsynchronized, realistic, and expressive rich body dynamics. We achieve this by\nfirst generating 3D skeleton movements from the audio sequence using a\nrecurrent neural network (RNN), and then synthesizing the output video via a\nconditional generative adversarial network (GAN). To make the skeleton movement\nrealistic and expressive, we embed the knowledge of an articulated 3D human\nskeleton and a learned dictionary of personal speech iconic gestures into the\ngeneration process in both learning and testing pipelines. The former prevents\nthe generation of unreasonable body distortion, while the later helps our model\nquickly learn meaningful body movement through a few recorded videos. To\nproduce photo-realistic and high-resolution video with motion details, we\npropose to insert part attention mechanisms in the conditional GAN, where each\ndetailed part, e.g. head and hand, is automatically zoomed in to have their own\ndiscriminators. To validate our approach, we collect a dataset with 20\nhigh-quality videos from 1 male and 1 female model reading various documents\nunder different topics. Compared with previous SoTA pipelines handling similar\ntasks, our approach achieves better results by a user study.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:30:14 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 07:44:41 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 00:41:31 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2020 06:32:33 GMT"}, {"version": "v5", "created": "Thu, 8 Oct 2020 23:19:03 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Liao", "Miao", ""], ["Zhang", "Sibo", ""], ["Wang", "Peng", ""], ["Zhu", "Hao", ""], ["Zuo", "Xinxin", ""], ["Yang", "Ruigang", ""]]}, {"id": "2007.09200", "submitter": "Yujia Huang", "authors": "Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y.\n  Tsao, Anima Anandkumar", "title": "Neural Networks with Recurrent Generative Feedback", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to input perturbations such as additive noise\nand adversarial attacks. In contrast, human perception is much more robust to\nsuch perturbations. The Bayesian brain hypothesis states that human brains use\nan internal generative model to update the posterior beliefs of the sensory\ninput. This mechanism can be interpreted as a form of self-consistency between\nthe maximum a posteriori (MAP) estimation of an internal generative model and\nthe external environment. Inspired by such hypothesis, we enforce\nself-consistency in neural networks by incorporating generative recurrent\nfeedback. We instantiate this design on convolutional neural networks (CNNs).\nThe proposed framework, termed Convolutional Neural Networks with Feedback\n(CNN-F), introduces a generative feedback with latent variables to existing CNN\narchitectures, where consistent predictions are made through alternating MAP\ninference under a Bayesian framework. In the experiments, CNN-F shows\nconsiderably improved adversarial robustness over conventional feedforward CNNs\non standard benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:32:48 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 08:29:39 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Huang", "Yujia", ""], ["Gornet", "James", ""], ["Dai", "Sihui", ""], ["Yu", "Zhiding", ""], ["Nguyen", "Tan", ""], ["Tsao", "Doris Y.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.09209", "submitter": "Yifan Wang", "authors": "Yifan Wang, Brian Curless, Steve Seitz", "title": "People as Scene Probes", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By analyzing the motion of people and other objects in a scene, we\ndemonstrate how to infer depth, occlusion, lighting, and shadow information\nfrom video taken from a single camera viewpoint. This information is then used\nto composite new objects into the same scene with a high degree of automation\nand realism. In particular, when a user places a new object (2D cut-out) in the\nimage, it is automatically rescaled, relit, occluded properly, and casts\nrealistic shadows in the correct direction relative to the sun, and which\nconform properly to scene geometry. We demonstrate results (best viewed in\nsupplementary video) on a range of scenes and compare to alternative methods\nfor depth estimation and shadow compositing.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:50:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Yifan", ""], ["Curless", "Brian", ""], ["Seitz", "Steve", ""]]}, {"id": "2007.09217", "submitter": "Rui Wang", "authors": "Juan Du, Rui Wang, Daniel Cremers", "title": "DH3D: Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DoF\n  Relocalization", "comments": "ECCV 2020, sportlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For relocalization in large-scale point clouds, we propose the first approach\nthat unifies global place recognition and local 6DoF pose refinement. To this\nend, we design a Siamese network that jointly learns 3D local feature detection\nand description directly from raw 3D points. It integrates FlexConv and\nSqueeze-and-Excitation (SE) to assure that the learned local descriptor\ncaptures multi-level geometric information and channel-wise relations. For\ndetecting 3D keypoints we predict the discriminativeness of the local\ndescriptors in an unsupervised manner. We generate the global descriptor by\ndirectly aggregating the learned local descriptors with an effective attention\nmechanism. In this way, local and global 3D descriptors are inferred in one\nsingle forward pass. Experiments on various benchmarks demonstrate that our\nmethod achieves competitive results for both global point cloud retrieval and\nlocal point cloud registration in comparison to state-of-the-art approaches. To\nvalidate the generalizability and robustness of our 3D keypoints, we\ndemonstrate that our method also performs favorably without fine-tuning on the\nregistration of point clouds that were generated by a visual SLAM system. Code\nand related materials are available at\nhttps://vision.in.tum.de/research/vslam/dh3d.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 20:21:22 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Du", "Juan", ""], ["Wang", "Rui", ""], ["Cremers", "Daniel", ""]]}, {"id": "2007.09221", "submitter": "Hui Qu", "authors": "Hui Qu, Yikai Zhang, Qi Chang, Zhennan Yan, Chao Chen, Dimitris\n  Metaxas", "title": "Learn distributed GAN with Temporary Discriminators", "comments": "Accepted by ECCV2020. Code: https://github.com/huiqu18/TDGAN-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for training distributed GAN with\nsequential temporary discriminators. Our proposed method tackles the challenge\nof training GAN in the federated learning manner: How to update the generator\nwith a flow of temporary discriminators? We apply our proposed method to learn\na self-adaptive generator with a series of local discriminators from multiple\ndata centers. We show our design of loss function indeed learns the correct\ndistribution with provable guarantees. The empirical experiments show that our\napproach is capable of generating synthetic data which is practical for\nreal-world applications such as training a segmentation model.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 20:45:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Qu", "Hui", ""], ["Zhang", "Yikai", ""], ["Chang", "Qi", ""], ["Yan", "Zhennan", ""], ["Chen", "Chao", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2007.09222", "submitter": "Haoran Wang", "authors": "Haoran Wang, Tong Shen, Wei Zhang, Lingyu Duan, Tao Mei", "title": "Classes Matter: A Fine-grained Adversarial Approach to Cross-domain\n  Semantic Segmentation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great progress in supervised semantic segmentation,a large\nperformance drop is usually observed when deploying the model in the wild.\nDomain adaptation methods tackle the issue by aligning the source domain and\nthe target domain. However, most existing methods attempt to perform the\nalignment from a holistic view, ignoring the underlying class-level data\nstructure in the target domain. To fully exploit the supervision in the source\ndomain, we propose a fine-grained adversarial learning strategy for class-level\nfeature alignment while preserving the internal structure of semantics across\ndomains. We adopt a fine-grained domain discriminator that not only plays as a\ndomain distinguisher, but also differentiates domains at class level. The\ntraditional binary domain labels are also generalized to domain encodings as\nthe supervision signal to guide the fine-grained feature alignment. An analysis\nwith Class Center Distance (CCD) validates that our fine-grained adversarial\nstrategy achieves better class-level alignment compared to other\nstate-of-the-art methods. Our method is easy to implement and its effectiveness\nis evaluated on three classical domain adaptation tasks, i.e., GTA5 to\nCityscapes, SYNTHIA to Cityscapes and Cityscapes to Cross-City. Large\nperformance gains show that our method outperforms other global feature\nalignment based and class-wise alignment based counterparts. The code is\npublicly available at https://github.com/JDAI-CV/FADA.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 20:50:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Haoran", ""], ["Shen", "Tong", ""], ["Zhang", "Wei", ""], ["Duan", "Lingyu", ""], ["Mei", "Tao", ""]]}, {"id": "2007.09250", "submitter": "Grigorios Chrysos", "authors": "Grigorios G Chrysos, Jean Kossaifi, Zhiding Yu, Anima Anandkumar", "title": "Unsupervised Controllable Generation with Self-Training", "comments": "Accepted in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent generative adversarial networks (GANs) are able to generate impressive\nphoto-realistic images. However, controllable generation with GANs remains a\nchallenging research problem. Achieving controllable generation requires\nsemantically interpretable and disentangled factors of variation. It is\nchallenging to achieve this goal using simple fixed distributions such as\nGaussian distribution. Instead, we propose an unsupervised framework to learn a\ndistribution of latent codes that control the generator through self-training.\nSelf-training provides an iterative feedback in the GAN training, from the\ndiscriminator to the generator, and progressively improves the proposal of the\nlatent codes as training proceeds. The latent codes are sampled from a latent\nvariable model that is learned in the feature space of the discriminator. We\nconsider a normalized independent component analysis model and learn its\nparameters through tensor factorization of the higher-order moments. Our\nframework exhibits better disentanglement compared to other variants such as\nthe variational autoencoder, and is able to discover semantically meaningful\nlatent codes without any supervision. We demonstrate empirically on both cars\nand faces datasets that each group of elements in the learned code controls a\nmode of variation with a semantic meaning, e.g. pose or background change. We\nalso demonstrate with quantitative metrics that our method generates better\nresults compared to other approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:50:35 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 06:59:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chrysos", "Grigorios G", ""], ["Kossaifi", "Jean", ""], ["Yu", "Zhiding", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2007.09257", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Yichen Li, Kate Saenko", "title": "Domain2Vec: Domain Embedding for Unsupervised Domain Adaptation", "comments": "ECCV 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional unsupervised domain adaptation (UDA) studies the knowledge\ntransfer between a limited number of domains. This neglects the more practical\nscenario where data are distributed in numerous different domains in the real\nworld. The domain similarity between those domains is critical for domain\nadaptation performance. To describe and learn relations between different\ndomains, we propose a novel Domain2Vec model to provide vectorial\nrepresentations of visual domains based on joint learning of feature\ndisentanglement and Gram matrix. To evaluate the effectiveness of our\nDomain2Vec model, we create two large-scale cross-domain benchmarks. The first\none is TinyDA, which contains 54 domains and about one million MNIST-style\nimages. The second benchmark is DomainBank, which is collected from 56 existing\nvision datasets. We demonstrate that our embedding is capable of predicting\ndomain similarities that match our intuition about visual relations between\ndifferent domains. Extensive experiments are conducted to demonstrate the power\nof our new datasets in benchmarking state-of-the-art multi-source domain\nadaptation methods, as well as the advantage of our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:05:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Peng", "Xingchao", ""], ["Li", "Yichen", ""], ["Saenko", "Kate", ""]]}, {"id": "2007.09264", "submitter": "Tien Do", "authors": "Tien Do, Khiem Vuong, Stergios I. Roumeliotis, and Hyun Soo Park", "title": "Surface Normal Estimation of Tilted Images via Spatial Rectifier", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a spatial rectifier to estimate surface normals of\ntilted images. Tilted images are of particular interest as more visual data are\ncaptured by arbitrarily oriented sensors such as body-/robot-mounted cameras.\nExisting approaches exhibit bounded performance on predicting surface normals\nbecause they were trained using gravity-aligned images. Our two main hypotheses\nare: (1) visual scene layout is indicative of the gravity direction; and (2)\nnot all surfaces are equally represented by a learned estimator due to the\nstructured distribution of the training data, thus, there exists a\ntransformation for each tilted image that is more responsive to the learned\nestimator than others. We design a spatial rectifier that is learned to\ntransform the surface normal distribution of a tilted image to the rectified\none that matches the gravity-aligned training data distribution. Along with the\nspatial rectifier, we propose a novel truncated angular loss that offers a\nstronger gradient at smaller angular errors and robustness to outliers. The\nresulting estimator outperforms the state-of-the-art methods including data\naugmentation baselines not only on ScanNet and NYUv2 but also on a new dataset\ncalled Tilt-RGBD that includes considerable roll and pitch camera motion.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:22:34 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Do", "Tien", ""], ["Vuong", "Khiem", ""], ["Roumeliotis", "Stergios I.", ""], ["Park", "Hyun Soo", ""]]}, {"id": "2007.09267", "submitter": "Minghua Liu", "authors": "Minghua Liu, Xiaoshuai Zhang, Hao Su", "title": "Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance", "comments": "ECCV 2020, code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in reconstructing the mesh representation of object\nsurfaces from point clouds. Surface reconstruction is a prerequisite for\ndownstream applications such as rendering, collision avoidance for planning,\nanimation, etc. However, the task is challenging if the input point cloud has a\nlow resolution, which is common in real-world scenarios (e.g., from LiDAR or\nKinect sensors). Existing learning-based mesh generative methods mostly predict\nthe surface by first building a shape embedding that is at the whole object\nlevel, a design that causes issues in generating fine-grained details and\ngeneralizing to unseen categories. Instead, we propose to leverage the input\npoint cloud as much as possible, by only adding connectivity information to\nexisting points. Particularly, we predict which triplets of points should form\nfaces. Our key innovation is a surrogate of local connectivity, calculated by\ncomparing the intrinsic/extrinsic metrics. We learn to predict this surrogate\nusing a deep point cloud network and then feed it to an efficient\npost-processing module for high-quality mesh generation. We demonstrate that\nour method can not only preserve details, handle ambiguous structures, but also\npossess strong generalizability to unseen categories by experiments on\nsynthetic and real data. The code is available at\nhttps://github.com/Colin97/Point2Mesh.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:36:00 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:49:28 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Liu", "Minghua", ""], ["Zhang", "Xiaoshuai", ""], ["Su", "Hao", ""]]}, {"id": "2007.09268", "submitter": "Shihao Zou", "authors": "Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chi Xu, Minglun Gong,\n  Li Cheng", "title": "3D Human Shape Reconstruction from a Polarization Image", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of estimating 3D body shape of clothed humans\nfrom single polarized 2D images, i.e. polarization images. Polarization images\nare known to be able to capture polarized reflected lights that preserve rich\ngeometric cues of an object, which has motivated its recent applications in\nreconstructing surface normal of the objects of interest. Inspired by the\nrecent advances in human shape estimation from single color images, in this\npaper, we attempt at estimating human body shapes by leveraging the geometric\ncues from single polarization images. A dedicated two-stage deep learning\napproach, SfP, is proposed: given a polarization image, stage one aims at\ninferring the fined-detailed body surface normal; stage two gears to\nreconstruct the 3D body shape of clothing details. Empirical evaluations on a\nsynthetic dataset (SURREAL) as well as a real-world dataset (PHSPD) demonstrate\nthe qualitative and quantitative performance of our approach in estimating\nhuman poses and shapes. This indicates polarization camera is a promising\nalternative to the more conventional color or depth imaging for human shape\nestimation. Further, normal maps inferred from polarization imaging play a\nsignificant role in accurately recovering the body shapes of clothed people.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:36:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zou", "Shihao", ""], ["Zuo", "Xinxin", ""], ["Qian", "Yiming", ""], ["Wang", "Sen", ""], ["Xu", "Chi", ""], ["Gong", "Minglun", ""], ["Cheng", "Li", ""]]}, {"id": "2007.09271", "submitter": "Zhiqiang Tang", "authors": "Zhiqiang Tang, Yunhe Gao, Leonid Karlinsky, Prasanna Sattigeri,\n  Rogerio Feris, Dimitris Metaxas", "title": "OnlineAugment: Online Data Augmentation with Less Domain Knowledge", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is one of the most important tools in training modern deep\nneural networks. Recently, great advances have been made in searching for\noptimal augmentation policies in the image classification domain. However, two\nkey points related to data augmentation remain uncovered by the current\nmethods. First is that most if not all modern augmentation search methods are\noffline and learning policies are isolated from their usage. The learned\npolicies are mostly constant throughout the training process and are not\nadapted to the current training model state. Second, the policies rely on\nclass-preserving image processing functions. Hence applying current offline\nmethods to new tasks may require domain knowledge to specify such kind of\noperations. In this work, we offer an orthogonal online data augmentation\nscheme together with three new augmentation networks, co-trained with the\ntarget learning task. It is both more efficient, in the sense that it does not\nrequire expensive offline training when entering a new domain, and more\nadaptive as it adapts to the learner state. Our augmentation networks require\nless domain knowledge and are easily applicable to new tasks. Extensive\nexperiments demonstrate that the proposed scheme alone performs on par with the\nstate-of-the-art offline data augmentation methods, as well as improving upon\nthe state-of-the-art in combination with those methods. Code is available at\nhttps://github.com/zhiqiangdon/online-augment .\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:01:17 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 19:54:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Tang", "Zhiqiang", ""], ["Gao", "Yunhe", ""], ["Karlinsky", "Leonid", ""], ["Sattigeri", "Prasanna", ""], ["Feris", "Rogerio", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2007.09273", "submitter": "Yaojie Liu", "authors": "Yaojie Liu, Joel Stehouwer, Xiaoming Liu", "title": "On Disentangling Spoof Trace for Generic Face Anti-Spoofing", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior studies show that the key to face anti-spoofing lies in the subtle\nimage pattern, termed \"spoof trace\", e.g., color distortion, 3D mask edge,\nMoire pattern, and many others. Designing a generic anti-spoofing model to\nestimate those spoof traces can improve not only the generalization of the\nspoof detection, but also the interpretability of the model's decision. Yet,\nthis is a challenging task due to the diversity of spoof types and the lack of\nground truth in spoof traces. This work designs a novel adversarial learning\nframework to disentangle the spoof traces from input faces as a hierarchical\ncombination of patterns at multiple scales. With the disentangled spoof traces,\nwe unveil the live counterpart of the original spoof face, and further\nsynthesize realistic new spoof faces after a proper geometric correction. Our\nmethod demonstrates superior spoof detection performance on both seen and\nunseen spoof scenarios while providing visually convincing estimation of spoof\ntraces. Code is available at https://github.com/yaojieliu/ECCV20-STDN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:14:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Yaojie", ""], ["Stehouwer", "Joel", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2007.09278", "submitter": "Hao Tang", "authors": "Hao Tang, Song Bai, Li Zhang, Philip H.S. Torr, Nicu Sebe", "title": "XingGAN for Person Image Generation", "comments": "Accepted to ECCV 2020, camera ready (16 pages) + supplementary (6\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Generative Adversarial Network (XingGAN or CrossingGAN)\nfor person image generation tasks, i.e., translating the pose of a given person\nto a desired one. The proposed Xing generator consists of two generation\nbranches that model the person's appearance and shape information,\nrespectively. Moreover, we propose two novel blocks to effectively transfer and\nupdate the person's shape and appearance embeddings in a crossing way to\nmutually improve each other, which has not been considered by any other\nexisting GAN-based image generation work. Extensive experiments on two\nchallenging datasets, i.e., Market-1501 and DeepFashion, demonstrate that the\nproposed XingGAN advances the state-of-the-art performance both in terms of\nobjective quantitative scores and subjective visual realness. The source code\nand trained models are available at https://github.com/Ha0Tang/XingGAN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:40:22 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tang", "Hao", ""], ["Bai", "Song", ""], ["Zhang", "Li", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "2007.09294", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Sarath Chandar", "title": "Slot Contrastive Networks: A Contrastive Approach for Representing\n  Objects", "comments": "Presented at ICML 2020 Workshop: Object-Oriented Learning (OOL):\n  Perception, Representation, and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised extraction of objects from low-level visual data is an important\ngoal for further progress in machine learning. Existing approaches for\nrepresenting objects without labels use structured generative models with\nstatic images. These methods focus a large amount of their capacity on\nreconstructing unimportant background pixels, missing low contrast or small\nobjects. Conversely, we present a new method that avoids losses in pixel space\nand over-reliance on the limited signal a static image provides. Our approach\ntakes advantage of objects' motion by learning a discriminative,\ntime-contrastive loss in the space of slot representations, attempting to force\neach slot to not only capture entities that move, but capture distinct objects\nfrom the other slots. Moreover, we introduce a new quantitative evaluation\nmetric to measure how \"diverse\" a set of slot vectors are, and use it to\nevaluate our model on 20 Atari games.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:01:39 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Racah", "Evan", ""], ["Chandar", "Sarath", ""]]}, {"id": "2007.09307", "submitter": "Wenzheng Tao", "authors": "Wenzheng Tao, Riddhish Bhalodia, Erin Anstadt, Ladislav Kavan, Ross T.\n  Whitaker, Jesse A. Goldstein", "title": "Unsupervised Shape Normality Metric for Severity Quantification", "comments": "Add acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes an unsupervised method to objectively quantify the\nabnormality of general anatomical shapes. The severity of an anatomical\ndeformity often serves as a determinant in the clinical management of patients.\nHowever, experiential bias and distinctive random residuals among specialist\nindividuals bring variability in diagnosis and patient management decisions,\nirrespective of the objective deformity degree. Therefore, supervised methods\nare prone to be misled given insufficient labeling of pathological samples that\ninevitably preserve human bias and inconsistency. Furthermore, subjects\ndemonstrating a specific pathology are naturally rare relative to the normal\npopulation. To avoid relying on sufficient pathological samples by fully\nutilizing the power of normal samples, we propose the shape normality metric\n(SNM), which requires learning only from normal samples and zero knowledge\nabout the pathology. We represent shapes by landmarks automatically inferred\nfrom the data and model the normal group by a multivariate Gaussian\ndistribution. Extensive experiments on different anatomical datasets, including\nskulls, femurs, scapulae, and humeri, demonstrate that SNM can provide an\neffective normality measurement, which can significantly detect and indicate\npathology. Therefore, SNM offers promising value in a variety of clinical\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 01:53:45 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 15:53:20 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tao", "Wenzheng", ""], ["Bhalodia", "Riddhish", ""], ["Anstadt", "Erin", ""], ["Kavan", "Ladislav", ""], ["Whitaker", "Ross T.", ""], ["Goldstein", "Jesse A.", ""]]}, {"id": "2007.09314", "submitter": "Mang Ye", "authors": "Mang Ye, Jianbing Shen, David J. Crandall, Ling Shao, Jiebo Luo", "title": "Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person\n  Re-Identification", "comments": "Accepted by ECCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-infrared person re-identification (VI-ReID) is a challenging\ncross-modality pedestrian retrieval problem. Due to the large intra-class\nvariations and cross-modality discrepancy with large amount of sample noise, it\nis difficult to learn discriminative part features. Existing VI-ReID methods\ninstead tend to learn global representations, which have limited\ndiscriminability and weak robustness to noisy images. In this paper, we propose\na novel dynamic dual-attentive aggregation (DDAG) learning method by mining\nboth intra-modality part-level and cross-modality graph-level contextual cues\nfor VI-ReID. We propose an intra-modality weighted-part attention module to\nextract discriminative part-aggregated features, by imposing the domain\nknowledge on the part relationship mining. To enhance robustness against noisy\nsamples, we introduce cross-modality graph structured attention to reinforce\nthe representation with the contextual relations across the two modalities. We\nalso develop a parameter-free dynamic dual aggregation learning strategy to\nadaptively integrate the two components in a progressive joint training manner.\nExtensive experiments demonstrate that DDAG outperforms the state-of-the-art\nmethods under various settings.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 03:08:13 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ye", "Mang", ""], ["Shen", "Jianbing", ""], ["Crandall", "David J.", ""], ["Shao", "Ling", ""], ["Luo", "Jiebo", ""]]}, {"id": "2007.09316", "submitter": "Shujun Wang", "authors": "Shujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, and Pheng-Ann Heng", "title": "Learning from Extrinsic and Intrinsic Supervisions for Domain\n  Generalization", "comments": "Accepted at ECCV 2020. Code is available at\n  https://github.com/EmmaW8/EISNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization capability of neural networks across domains is crucial\nfor real-world applications. We argue that a generalized object recognition\nsystem should well understand the relationships among different images and also\nthe images themselves at the same time. To this end, we present a new domain\ngeneralization framework that learns how to generalize across domains\nsimultaneously from extrinsic relationship supervision and intrinsic\nself-supervision for images from multi-source domains. To be specific, we\nformulate our framework with feature embedding using a multi-task learning\nparadigm. Besides conducting the common supervised recognition task, we\nseamlessly integrate a momentum metric learning task and a self-supervised\nauxiliary task to collectively utilize the extrinsic supervision and intrinsic\nsupervision. Also, we develop an effective momentum metric learning scheme with\nK-hard negative mining to boost the network to capture image relationship for\ndomain generalization. We demonstrate the effectiveness of our approach on two\nstandard object recognition benchmarks VLCS and PACS, and show that our methods\nachieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 03:12:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Shujun", ""], ["Yu", "Lequan", ""], ["Li", "Caizi", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2007.09319", "submitter": "Tak-Wai Hui", "authors": "Tak-Wai Hui, Chen Change Loy", "title": "LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate\n  Optical Flow Estimation", "comments": "Accepted to ECCV 2020. Trained models and code package are available\n  at https://github.com/twhui/LiteFlowNet3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning approaches have achieved great success in addressing the\nproblem of optical flow estimation. The keys to success lie in the use of cost\nvolume and coarse-to-fine flow inference. However, the matching problem becomes\nill-posed when partially occluded or homogeneous regions exist in images. This\ncauses a cost volume to contain outliers and affects the flow decoding from it.\nBesides, the coarse-to-fine flow inference demands an accurate flow\ninitialization. Ambiguous correspondence yields erroneous flow fields and\naffects the flow inferences in subsequent levels. In this paper, we introduce\nLiteFlowNet3, a deep network consisting of two specialized modules, to address\nthe above challenges. (1) We ameliorate the issue of outliers in the cost\nvolume by amending each cost vector through an adaptive modulation prior to the\nflow decoding. (2) We further improve the flow accuracy by exploring local flow\nconsistency. To this end, each inaccurate optical flow is replaced with an\naccurate one from a nearby position through a novel warping of the flow field.\nLiteFlowNet3 not only achieves promising results on public benchmarks but also\nhas a small model size and a fast runtime.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 03:30:39 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hui", "Tak-Wai", ""], ["Loy", "Chen Change", ""]]}, {"id": "2007.09336", "submitter": "Hang Xu", "authors": "Wenshuo Ma, Tingzhong Tian, Hang Xu, Yimin Huang, Zhenguo Li", "title": "AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian\n  Sub-sampling", "comments": "Accepted by ECCV 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art object detection systems follow an anchor-based\ndiagram. Anchor boxes are densely proposed over the images and the network is\ntrained to predict the boxes position offset as well as the classification\nconfidence. Existing systems pre-define anchor box shapes and sizes and ad-hoc\nheuristic adjustments are used to define the anchor configurations. However,\nthis might be sub-optimal or even wrong when a new dataset or a new model is\nadopted. In this paper, we study the problem of automatically optimizing anchor\nboxes for object detection. We first demonstrate that the number of anchors,\nanchor scales and ratios are crucial factors for a reliable object detection\nsystem. By carefully analyzing the existing bounding box patterns on the\nfeature hierarchy, we design a flexible and tight hyper-parameter space for\nanchor configurations. Then we propose a novel hyper-parameter optimization\nmethod named AABO to determine more appropriate anchor boxes for a certain\ndataset, in which Bayesian Optimization and subsampling method are combined to\nachieve precise and efficient anchor configuration optimization. Experiments\ndemonstrate the effectiveness of our proposed method on different detectors and\ndatasets, e.g. achieving around 2.4% mAP improvement on COCO, 1.6% on ADE and\n1.5% on VG, and the optimal anchors can bring 1.4% to 2.4% mAP improvement on\nSOTA detectors by only optimizing anchor configurations, e.g. boosting Mask\nRCNN from 40.3% to 42.3%, and HTC detector from 46.8% to 48.2%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 05:44:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ma", "Wenshuo", ""], ["Tian", "Tingzhong", ""], ["Xu", "Hang", ""], ["Huang", "Yimin", ""], ["Li", "Zhenguo", ""]]}, {"id": "2007.09337", "submitter": "Shuang Yu", "authors": "Wenao Ma, Shuang Yu, Kai Ma, Jiexiang Wang, Xinghao Ding and Yefeng\n  Zheng", "title": "Multi-Task Neural Networks with Spatial Activation for Retinal Vessel\n  Segmentation and Artery/Vein Classification", "comments": null, "journal-ref": "MICCAI 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Retinal artery/vein (A/V) classification plays a critical role in the\nclinical biomarker study of how various systemic and cardiovascular diseases\naffect the retinal vessels. Conventional methods of automated A/V\nclassification are generally complicated and heavily depend on the accurate\nvessel segmentation. In this paper, we propose a multi-task deep neural network\nwith spatial activation mechanism that is able to segment full retinal vessel,\nartery and vein simultaneously, without the pre-requirement of vessel\nsegmentation. The input module of the network integrates the domain knowledge\nof widely used retinal preprocessing and vessel enhancement techniques. We\nspecially customize the output block of the network with a spatial activation\nmechanism, which takes advantage of a relatively easier task of vessel\nsegmentation and exploits it to boost the performance of A/V classification. In\naddition, deep supervision is introduced to the network to assist the low level\nlayers to extract more semantic information. The proposed network achieves\npixel-wise accuracy of 95.70% for vessel segmentation, and A/V classification\naccuracy of 94.50%, which is the state-of-the-art performance for both tasks on\nthe AV-DRIVE dataset. Furthermore, we have also tested the model performance on\nINSPIRE-AVR dataset, which achieves a skeletal A/V classification accuracy of\n91.6%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 05:46:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ma", "Wenao", ""], ["Yu", "Shuang", ""], ["Ma", "Kai", ""], ["Wang", "Jiexiang", ""], ["Ding", "Xinghao", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.09344", "submitter": "Kelei He", "authors": "Wen Ji, Kelei He, Jing Huo, Zheng Gu, Yang Gao", "title": "Unsupervised Domain Attention Adaptation Network for Caricature\n  Attribute Recognition", "comments": "This paper has been accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Caricature attributes provide distinctive facial features to help research in\nPsychology and Neuroscience. However, unlike the facial photo attribute\ndatasets that have a quantity of annotated images, the annotations of\ncaricature attributes are rare. To facility the research in attribute learning\nof caricatures, we propose a caricature attribute dataset, namely WebCariA.\nMoreover, to utilize models that trained by face attributes, we propose a novel\nunsupervised domain adaptation framework for cross-modality (i.e., photos to\ncaricatures) attribute recognition, with an integrated inter- and intra-domain\nconsistency learning scheme. Specifically, the inter-domain consistency\nlearning scheme consisting an image-to-image translator to first fill the\ndomain gap between photos and caricatures by generating intermediate image\nsamples, and a label consistency learning module to align their semantic\ninformation. The intra-domain consistency learning scheme integrates the common\nfeature consistency learning module with a novel attribute-aware\nattention-consistency learning module for a more efficient alignment. We did an\nextensive ablation study to show the effectiveness of the proposed method. And\nthe proposed method also outperforms the state-of-the-art methods by a margin.\nThe implementation of the proposed method is available at\nhttps://github.com/KeleiHe/DAAN.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 06:38:45 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ji", "Wen", ""], ["He", "Kelei", ""], ["Huo", "Jing", ""], ["Gu", "Zheng", ""], ["Gao", "Yang", ""]]}, {"id": "2007.09355", "submitter": "Guojun Yin", "authors": "Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen and Jing Shao", "title": "Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware\n  Clues", "comments": "21 pages, 9 figures, accepted as a POSTER at ECCV2020, UPDATE the\n  appendix of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As realistic facial manipulation technologies have achieved remarkable\nprogress, social concerns about potential malicious abuse of these technologies\nbring out an emerging research topic of face forgery detection. However, it is\nextremely challenging since recent advances are able to forge faces beyond the\nperception ability of human eyes, especially in compressed images and videos.\nWe find that mining forgery patterns with the awareness of frequency could be a\ncure, as frequency provides a complementary viewpoint where either subtle\nforgery artifacts or compression errors could be well described. To introduce\nfrequency into the face forgery detection, we propose a novel Frequency in Face\nForgery Network (F3-Net), taking advantages of two different but complementary\nfrequency-aware clues, 1) frequency-aware decomposed image components, and 2)\nlocal frequency statistics, to deeply mine the forgery patterns via our\ntwo-stream collaborative learning framework. We apply DCT as the applied\nfrequency-domain transformation. Through comprehensive studies, we show that\nthe proposed F3-Net significantly outperforms competing state-of-the-art\nmethods on all compression qualities in the challenging FaceForensics++\ndataset, especially wins a big lead upon low-quality media.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 07:39:08 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 04:04:29 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Qian", "Yuyang", ""], ["Yin", "Guojun", ""], ["Sheng", "Lu", ""], ["Chen", "Zixuan", ""], ["Shao", "Jing", ""]]}, {"id": "2007.09357", "submitter": "Ruibing Hou", "authors": "Ruibing Hou and Hong Chang and Bingpeng Ma and Shiguang Shan and Xilin\n  Chen", "title": "Temporal Complementary Learning for Video Person Re-Identification", "comments": "17 pages, 6 figures, accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Temporal Complementary Learning Network that extracts\ncomplementary features of consecutive video frames for video person\nre-identification. Firstly, we introduce a Temporal Saliency Erasing (TSE)\nmodule including a saliency erasing operation and a series of ordered learners.\nSpecifically, for a specific frame of a video, the saliency erasing operation\ndrives the specific learner to mine new and complementary parts by erasing the\nparts activated by previous frames. Such that the diverse visual features can\nbe discovered for consecutive frames and finally form an integral\ncharacteristic of the target identity. Furthermore, a Temporal Saliency\nBoosting (TSB) module is designed to propagate the salient information among\nvideo frames to enhance the salient feature. It is complementary to TSE by\neffectively alleviating the information loss caused by the erasing operation of\nTSE. Extensive experiments show our method performs favorably against\nstate-of-the-arts. The source code is available at\nhttps://github.com/blue-blue272/VideoReID-TCLNet.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 07:59:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hou", "Ruibing", ""], ["Chang", "Hong", ""], ["Ma", "Bingpeng", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2007.09365", "submitter": "Yajie Xing", "authors": "Yajie Xing, Jingbo Wang, Gang Zeng", "title": "Malleable 2.5D Convolution: Learning Receptive Fields along the\n  Depth-axis for RGB-D Scene Parsing", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth data provide geometric information that can bring progress in RGB-D\nscene parsing tasks. Several recent works propose RGB-D convolution operators\nthat construct receptive fields along the depth-axis to handle 3D neighborhood\nrelations between pixels. However, these methods pre-define depth receptive\nfields by hyperparameters, making them rely on parameter selection. In this\npaper, we propose a novel operator called malleable 2.5D convolution to learn\nthe receptive field along the depth-axis. A malleable 2.5D convolution has one\nor more 2D convolution kernels. Our method assigns each pixel to one of the\nkernels or none of them according to their relative depth differences, and the\nassigning process is formulated as a differentiable form so that it can be\nlearnt by gradient descent. The proposed operator runs on standard 2D feature\nmaps and can be seamlessly incorporated into pre-trained CNNs. We conduct\nextensive experiments on two challenging RGB-D semantic segmentation dataset\nNYUDv2 and Cityscapes to validate the effectiveness and the generalization\nability of our method.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 08:26:11 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Xing", "Yajie", ""], ["Wang", "Jingbo", ""], ["Zeng", "Gang", ""]]}, {"id": "2007.09375", "submitter": "Taekyung Kim", "authors": "Taekyung Kim and Changick Kim", "title": "Attract, Perturb, and Explore: Learning a Feature Alignment Network for\n  Semi-supervised Domain Adaptation", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although unsupervised domain adaptation methods have been widely adopted\nacross several computer vision tasks, it is more desirable if we can exploit a\nfew labeled data from new domains encountered in a real application. The novel\nsetting of the semi-supervised domain adaptation (SSDA) problem shares the\nchallenges with the domain adaptation problem and the semi-supervised learning\nproblem. However, a recent study shows that conventional domain adaptation and\nsemi-supervised learning methods often result in less effective or negative\ntransfer in the SSDA problem. In order to interpret the observation and address\nthe SSDA problem, in this paper, we raise the intra-domain discrepancy issue\nwithin the target domain, which has never been discussed so far. Then, we\ndemonstrate that addressing the intra-domain discrepancy leads to the ultimate\ngoal of the SSDA problem. We propose an SSDA framework that aims to align\nfeatures via alleviation of the intra-domain discrepancy. Our framework mainly\nconsists of three schemes, i.e., attraction, perturbation, and exploration.\nFirst, the attraction scheme globally minimizes the intra-domain discrepancy\nwithin the target domain. Second, we demonstrate the incompatibility of the\nconventional adversarial perturbation methods with SSDA. Then, we present a\ndomain adaptive adversarial perturbation scheme, which perturbs the given\ntarget samples in a way that reduces the intra-domain discrepancy. Finally, the\nexploration scheme locally aligns features in a class-wise manner complementary\nto the attraction scheme by selectively aligning unlabeled target features\ncomplementary to the perturbation scheme. We conduct extensive experiments on\ndomain adaptation benchmark datasets such as DomainNet, Office-Home, and\nOffice. Our method achieves state-of-the-art performances on all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:26:25 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "2007.09383", "submitter": "Han Li", "authors": "Han Li, Hu Han, and S. Kevin Zhou", "title": "Bounding Maps for Universal Lesion Detection", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Lesion Detection (ULD) in computed tomography plays an essential\nrole in computer-aided diagnosis systems. Many detection approaches achieve\nexcellent results for ULD using possible bounding boxes (or anchors) as\nproposals. However, empirical evidence shows that using anchor-based proposals\nleads to a high false-positive (FP) rate. In this paper, we propose a\nbox-to-map method to represent a bounding box with three soft continuous maps\nwith bounds in x-, y- and xy- directions. The bounding maps (BMs) are used in\ntwo-stage anchor-based ULD frameworks to reduce the FP rate. In the 1 st stage\nof the region proposal network, we replace the sharp binary ground-truth label\nof anchors with the corresponding xy-direction BM hence the positive anchors\nare now graded. In the 2 nd stage, we add a branch that takes our continuous\nBMs in x- and y- directions for extra supervision of detailed locations. Our\nmethod, when embedded into three state-of-the-art two-stage anchor-based\ndetection methods, brings a free detection accuracy improvement (e.g., a 1.68%\nto 3.85% boost of sensitivity at 4 FPs) without extra inference time.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:47:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Han", ""], ["Han", "Hu", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2007.09384", "submitter": "Jiaxi Wu", "authors": "Jiaxi Wu, Songtao Liu, Di Huang, Yunhong Wang", "title": "Multi-Scale Positive Sample Refinement for Few-Shot Object Detection", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection (FSOD) helps detectors adapt to unseen classes with\nfew training instances, and is useful when manual annotation is time-consuming\nor data acquisition is limited. Unlike previous attempts that exploit few-shot\nclassification techniques to facilitate FSOD, this work highlights the\nnecessity of handling the problem of scale variations, which is challenging due\nto the unique sample distribution. To this end, we propose a Multi-scale\nPositive Sample Refinement (MPSR) approach to enrich object scales in FSOD. It\ngenerates multi-scale positive samples as object pyramids and refines the\nprediction at various scales. We demonstrate its advantage by integrating it as\nan auxiliary branch to the popular architecture of Faster R-CNN with FPN,\ndelivering a strong FSOD solution. Several experiments are conducted on PASCAL\nVOC and MS COCO, and the proposed approach achieves state of the art results\nand significantly outperforms other counterparts, which shows its\neffectiveness. Code is available at https://github.com/jiaxi-wu/MPSR.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:48:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wu", "Jiaxi", ""], ["Liu", "Songtao", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""]]}, {"id": "2007.09389", "submitter": "Xiao Sun", "authors": "Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang Xu, Stephen Lin", "title": "SRNet: Improving Generalization in 3D Human Pose Estimation with a\n  Split-and-Recombine Approach", "comments": "European Conference on Computer Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human poses that are rare or unseen in a training set are challenging for a\nnetwork to predict. Similar to the long-tailed distribution problem in visual\nrecognition, the small number of examples for such poses limits the ability of\nnetworks to model them. Interestingly, local pose distributions suffer less\nfrom the long-tail problem, i.e., local joint configurations within a rare pose\nmay appear within other poses in the training set, making them less rare. We\npropose to take advantage of this fact for better generalization to rare and\nunseen poses. To be specific, our method splits the body into local regions and\nprocesses them in separate network branches, utilizing the property that a\njoint position depends mainly on the joints within its local body region.\nGlobal coherence is maintained by recombining the global context from the rest\nof the body into each branch as a low-dimensional vector. With the reduced\ndimensionality of less relevant body areas, the training set distribution\nwithin network branches more closely reflects the statistics of local poses\ninstead of global body poses, without sacrificing information important for\njoint inference. The proposed split-and-recombine approach, called SRNet, can\nbe easily adapted to both single-image and temporal models, and it leads to\nappreciable improvements in the prediction of rare and unseen poses.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 10:02:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zeng", "Ailing", ""], ["Sun", "Xiao", ""], ["Huang", "Fuyang", ""], ["Liu", "Minhao", ""], ["Xu", "Qiang", ""], ["Lin", "Stephen", ""]]}, {"id": "2007.09397", "submitter": "Aditya Arun", "authors": "Aditya Arun, C.V. Jawahar, M. Pawan Kumar", "title": "Weakly Supervised Instance Segmentation by Learning Annotation\n  Consistent Instances", "comments": "To appear at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for weakly supervised instance segmentations depend on two\ncomponents: (i) a pseudo label generation model that provides instances which\nare consistent with a given annotation; and (ii) an instance segmentation\nmodel, which is trained in a supervised manner using the pseudo labels as\nground-truth. Unlike previous approaches, we explicitly model the uncertainty\nin the pseudo label generation process using a conditional distribution. The\nsamples drawn from our conditional distribution provide accurate pseudo labels\ndue to the use of semantic class aware unary terms, boundary aware pairwise\nsmoothness terms, and annotation aware higher order terms. Furthermore, we\nrepresent the instance segmentation model as an annotation agnostic prediction\ndistribution. In contrast to previous methods, our representation allows us to\ndefine a joint probabilistic learning objective that minimizes the\ndissimilarity between the two distributions. Our approach achieves state of the\nart results on the PASCAL VOC 2012 data set, outperforming the best baseline by\n4.2% mAP@0.5 and 4.8% mAP@0.75.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 10:32:11 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Arun", "Aditya", ""], ["Jawahar", "C. V.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "2007.09431", "submitter": "Dongyun Lin", "authors": "Dongyun Lin, Yiqun Li, Shudong Xie, Tin Lay Nwe, Sheng Dong", "title": "DDR-ID: Dual Deep Reconstruction Networks Based Image Decomposition for\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One pivot challenge for image anomaly (AD) detection is to learn\ndiscriminative information only from normal class training images. Most image\nreconstruction based AD methods rely on the discriminative capability of\nreconstruction error. This is heuristic as image reconstruction is unsupervised\nwithout incorporating normal-class-specific information. In this paper, we\npropose an AD method called dual deep reconstruction networks based image\ndecomposition (DDR-ID). The networks are trained by jointly optimizing for\nthree losses: the one-class loss, the latent space constrain loss and the\nreconstruction loss. After training, DDR-ID can decompose an unseen image into\nits normal class and the residual components, respectively. Two anomaly scores\nare calculated to quantify the anomalous degree of the image in either normal\nclass latent space or reconstruction image space. Thereby, anomaly detection\ncan be performed via thresholding the anomaly score. The experiments\ndemonstrate that DDR-ID outperforms multiple related benchmarking methods in\nimage anomaly detection using MNIST, CIFAR-10 and Endosome datasets and\nadversarial attack detection using GTSRB dataset.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 13:54:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lin", "Dongyun", ""], ["Li", "Yiqun", ""], ["Xie", "Shudong", ""], ["Nwe", "Tin Lay", ""], ["Dong", "Sheng", ""]]}, {"id": "2007.09433", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Sabine S\\\"usstrunk, Mathieu Salzmann", "title": "Volumetric Transformer Networks", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to encode spatial invariance within deep convolutional\nneural networks (CNNs) apply the same warping field to all the feature\nchannels. This does not account for the fact that the individual feature\nchannels can represent different semantic parts, which can undergo different\nspatial transformations w.r.t. a canonical configuration. To overcome this\nlimitation, we introduce a learnable module, the volumetric transformer network\n(VTN), that predicts channel-wise warping fields so as to reconfigure\nintermediate CNN features spatially and channel-wisely. We design our VTN as an\nencoder-decoder network, with modules dedicated to letting the information flow\nacross the feature channels, to account for the dependencies between the\nsemantic parts. We further propose a loss function defined between the warped\nfeatures of pairs of instances, which improves the localization ability of VTN.\nOur experiments show that VTN consistently boosts the features' representation\npower and consequently the networks' accuracy on fine-grained image recognition\nand instance-level image retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 14:00:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Seungryong", ""], ["S\u00fcsstrunk", "Sabine", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2007.09438", "submitter": "Dongyun Lin", "authors": "Dongyun Lin, Yanpeng Cao, Wenbing Zhu, and Yiqun Li", "title": "Few-Shot Defect Segmentation Leveraging Abundant Normal Training Samples\n  Through Normal Background Regularization and Crop-and-Paste Operation", "comments": "Will be appeared in ICME2021 Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial product quality assessment, it is essential to determine\nwhether a product is defect-free and further analyze the severity of anomality.\nTo this end, accurate defect segmentation on images of products provides an\nimportant functionality. In industrial inspection tasks, it is common to\ncapture abundant defect-free image samples but very limited anomalous ones.\nTherefore, it is critical to develop automatic and accurate defect segmentation\nsystems using only a small number of annotated anomalous training images. This\npaper tackles the challenging few-shot defect segmentation task with sufficient\nnormal (defect-free) training images but very few anomalous ones. We present\ntwo effective regularization techniques via incorporating abundant defect-free\nimages into the training of a UNet-like encoder-decoder defect segmentation\nnetwork. We first propose a Normal Background Regularization (NBR) loss which\nis jointly minimized with the segmentation loss, enhancing the encoder network\nto produce distinctive representations for normal regions. Secondly, we\ncrop/paste defective regions to the randomly selected normal images for data\naugmentation and propose a weighted binary cross-entropy loss to enhance the\ntraining by emphasizing more realistic crop-and-pasted augmented images based\non feature-level similarity comparison. Both techniques are implemented on an\nencoder-decoder segmentation network backboned by ResNet-34 for few-shot defect\nsegmentation. Extensive experiments are conducted on the recently released\nMVTec Anomaly Detection dataset with high-resolution industrial images. Under\nboth 1-shot and 5-shot defect segmentation settings, the proposed method\nsignificantly outperforms several benchmarking methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 14:15:42 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 07:14:39 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lin", "Dongyun", ""], ["Cao", "Yanpeng", ""], ["Zhu", "Wenbing", ""], ["Li", "Yiqun", ""]]}, {"id": "2007.09451", "submitter": "Dong Zhang", "authors": "Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua and\n  Qianru Sun", "title": "Feature Pyramid Transformer", "comments": "Published at the European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature interactions across space and scales underpin modern visual\nrecognition systems because they introduce beneficial visual contexts.\nConventionally, spatial contexts are passively hidden in the CNN's increasing\nreceptive fields or actively encoded by non-local convolution. Yet, the\nnon-local spatial interactions are not across scales, and thus they fail to\ncapture the non-local contexts of objects (or parts) residing in different\nscales. To this end, we propose a fully active feature interaction across both\nspace and scales, called Feature Pyramid Transformer (FPT). It transforms any\nfeature pyramid into another feature pyramid of the same size but with richer\ncontexts, by using three specially designed transformers in self-level,\ntop-down, and bottom-up interaction fashion. FPT serves as a generic visual\nbackbone with fair computational overhead. We conduct extensive experiments in\nboth instance-level (i.e., object detection and instance segmentation) and\npixel-level segmentation tasks, using various backbones and head networks, and\nobserve consistent improvement over all the baselines and the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:16:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Dong", ""], ["Zhang", "Hanwang", ""], ["Tang", "Jinhui", ""], ["Wang", "Meng", ""], ["Hua", "Xiansheng", ""], ["Sun", "Qianru", ""]]}, {"id": "2007.09453", "submitter": "Md Tahmid Hossain", "authors": "Md Tahmid Hossain, Shyh Wei Teng, Ferdous Sohel, Guojun Lu", "title": "Robust Image Classification Using A Low-Pass Activation Function and DCT\n  Augmentation", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2021.3089598", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Network's (CNN's) performance disparity on clean and\ncorrupted datasets has recently come under scrutiny. In this work, we analyse\ncommon corruptions in the frequency domain, i.e., High Frequency corruptions\n(HFc, e.g., noise) and Low Frequency corruptions (LFc, e.g., blur). Although a\nsimple solution to HFc is low-pass filtering, ReLU -- a widely used Activation\nFunction (AF), does not have any filtering mechanism. In this work, we instill\nlow-pass filtering into the AF (LP-ReLU) to improve robustness against HFc. To\ndeal with LFc, we complement LP-ReLU with Discrete Cosine Transform based\naugmentation. LP-ReLU, coupled with DCT augmentation, enables a deep network to\ntackle the entire spectrum of corruption. We use CIFAR-10-C and Tiny ImageNet-C\nfor evaluation and demonstrate improvements of 5% and 7.3% in accuracy\nrespectively, compared to the State-Of-The-Art (SOTA). We further evaluate our\nmethod's stability on a variety of perturbations in CIFAR-10-P and Tiny\nImageNet-P, achieving new SOTA in these experiments as well. To further\nstrengthen our understanding regarding CNN's lack of robustness, a decision\nspace visualisation process is proposed and presented in this work.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:24:13 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:01:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hossain", "Md Tahmid", ""], ["Teng", "Shyh Wei", ""], ["Sohel", "Ferdous", ""], ["Lu", "Guojun", ""]]}, {"id": "2007.09454", "submitter": "Xiaobin Hu", "authors": "Xiaobin Hu, Wenqi Ren, John LaMaster, Xiaochun Cao, Xiaoming Li,\n  Zechao Li, Bjoern Menze, and Wei Liu", "title": "Face Super-Resolution Guided by 3D Facial Priors", "comments": "Accepted as a spotlight paper, European Conference on Computer Vision\n  2020 (ECCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art face super-resolution methods employ deep convolutional\nneural networks to learn a mapping between low- and high- resolution facial\npatterns by exploring local appearance knowledge. However, most of these\nmethods do not well exploit facial structures and identity information, and\nstruggle to deal with facial images that exhibit large pose variations. In this\npaper, we propose a novel face super-resolution method that explicitly\nincorporates 3D facial priors which grasp the sharp facial structures. Our work\nis the first to explore 3D morphable knowledge based on the fusion of\nparametric descriptions of face attributes (e.g., identity, facial expression,\ntexture, illumination, and face pose). Furthermore, the priors can easily be\nincorporated into any network and are extremely efficient in improving the\nperformance and accelerating the convergence speed. Firstly, a 3D face\nrendering branch is set up to obtain 3D priors of salient facial structures and\nidentity knowledge. Secondly, the Spatial Attention Module is used to better\nexploit this hierarchical information (i.e., intensity similarity, 3D facial\nstructure, and identity content) for the super-resolution problem. Extensive\nexperiments demonstrate that the proposed 3D priors achieve superior face\nsuper-resolution results over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:26:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hu", "Xiaobin", ""], ["Ren", "Wenqi", ""], ["LaMaster", "John", ""], ["Cao", "Xiaochun", ""], ["Li", "Xiaoming", ""], ["Li", "Zechao", ""], ["Menze", "Bjoern", ""], ["Liu", "Wei", ""]]}, {"id": "2007.09455", "submitter": "Tianchen Wang", "authors": "Tianchen Wang, Xiaowei Xu, Jinjun Xiong, Qianjun Jia, Haiyun Yuan,\n  Meiping Huang, Jian Zhuang, Yiyu Shi", "title": "ICA-UNet: ICA Inspired Statistical UNet for Real-time 3D Cardiac Cine\n  MRI Segmentation", "comments": "MICCAI2020, 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time cine magnetic resonance imaging (MRI) plays an increasingly\nimportant role in various cardiac interventions. In order to enable fast and\naccurate visual assistance, the temporal frames need to be segmented\non-the-fly. However, state-of-the-art MRI segmentation methods are used either\noffline because of their high computation complexity, or in real-time but with\nsignificant accuracy loss and latency increase (causing visually noticeable\nlag). As such, they can hardly be adopted to assist visual guidance. In this\nwork, inspired by a new interpretation of Independent Component Analysis (ICA)\nfor learning, we propose a novel ICA-UNet for real-time 3D cardiac cine MRI\nsegmentation. Experiments using the MICCAI ACDC 2017 dataset show that,\ncompared with the state-of-the-arts, ICA-UNet not only achieves higher Dice\nscores, but also meets the real-time requirements for both throughput and\nlatency (up to 12.6X reduction), enabling real-time guidance for cardiac\ninterventions without visual lag.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:29:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Tianchen", ""], ["Xu", "Xiaowei", ""], ["Xiong", "Jinjun", ""], ["Jia", "Qianjun", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""], ["Shi", "Yiyu", ""]]}, {"id": "2007.09464", "submitter": "Sowmya Kamath", "authors": "Sowmya Kamath S and Karthik K", "title": "A Bag of Visual Words Model for Medical Image Retrieval", "comments": "In the proceedings of the 7th International Engineering Symposium\n  (IES 2018), Kumamoto University, Kumamoto, Japan, Mar 7-9, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Image Retrieval is a challenging field in Visual information\nretrieval, due to the multi-dimensional and multi-modal context of the\nunderlying content. Traditional models often fail to take the intrinsic\ncharacteristics of data into consideration, and have thus achieved limited\naccuracy when applied to medical images. The Bag of Visual Words (BoVW) is a\ntechnique that can be used to effectively represent intrinsic image features in\nvector space, so that applications like image classification and similar-image\nsearch can be optimized. In this paper, we present a MedIR approach based on\nthe BoVW model for content-based medical image retrieval. As medical images as\nmulti-dimensional, they exhibit underlying cluster and manifold information\nwhich enhances semantic relevance and allows for label uniformity. Hence, the\nBoVW features extracted for each image are used to train a supervised machine\nlearning classifier based on positive and negative training images, for\nextending content based image retrieval. During experimental validation, the\nproposed model performed very well, achieving a Mean Average Precision of\n88.89% during top-3 image retrieval experiments.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:21:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["S", "Sowmya Kamath", ""], ["K", "Karthik", ""]]}, {"id": "2007.09465", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang, Yu Chi Hu, Neelam Tyagi, Andreas Rimner, Nancy Lee, Joseph\n  O. Deasy, Sean Berry, Harini Veeraraghavan", "title": "PSIGAN: Joint probabilistic segmentation and image distribution matching\n  for unpaired cross-modality adaptation based MRI segmentation", "comments": "This paper has been accepted by IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging, 2020", "doi": "10.1109/TMI.2020.3011626", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a new joint probabilistic segmentation and image distribution\nmatching generative adversarial network (PSIGAN) for unsupervised domain\nadaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI)\nimages. Our UDA approach models the co-dependency between images and their\nsegmentation as a joint probability distribution using a new structure\ndiscriminator. The structure discriminator computes structure of interest\nfocused adversarial loss by combining the generated pseudo MRI with\nprobabilistic segmentations produced by a simultaneously trained segmentation\nsub-network. The segmentation sub-network is trained using the pseudo MRI\nproduced by the generator sub-network. This leads to a cyclical optimization of\nboth the generator and segmentation sub-networks that are jointly trained as\npart of an end-to-end network. Extensive experiments and comparisons against\nmultiple state-of-the-art methods were done on four different MRI sequences\ntotalling 257 scans for generating multi-organ and tumor segmentation. The\nexperiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20\nT2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right\nkidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for\nparotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our\nmethod achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the\nabdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for\nlung tumors.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:23:02 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 16:01:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jiang", "Jue", ""], ["Hu", "Yu Chi", ""], ["Tyagi", "Neelam", ""], ["Rimner", "Andreas", ""], ["Lee", "Nancy", ""], ["Deasy", "Joseph O.", ""], ["Berry", "Sean", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "2007.09469", "submitter": "Alberto Santamaria-Pang", "authors": "Aritra Chowdhury, James R. Kubricht, Anup Sood, Peter Tu, Alberto\n  Santamaria-Pang", "title": "ESCELL: Emergent Symbolic Cellular Language", "comments": "IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging\n  (ISBI), Iowa City, IA, USA, 2020, pp. 1604-1607", "doi": "10.1109/ISBI45749.2020.9098343", "report-no": null, "categories": "cs.AI cs.CV cs.LG q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ESCELL, a method for developing an emergent symbolic language of\ncommunication between multiple agents reasoning about cells. We show how agents\nare able to cooperate and communicate successfully in the form of symbols\nsimilar to human language to accomplish a task in the form of a referential\ngame (Lewis' signaling game). In one form of the game, a sender and a receiver\nobserve a set of cells from 5 different cell phenotypes. The sender is told one\ncell is a target and is allowed to send one symbol to the receiver from a fixed\narbitrary vocabulary size. The receiver relies on the information in the symbol\nto identify the target cell. We train the sender and receiver networks to\ndevelop an innate emergent language between themselves to accomplish this task.\nWe observe that the networks are able to successfully identify cells from 5\ndifferent phenotypes with an accuracy of 93.2%. We also introduce a new form of\nthe signaling game where the sender is shown one image instead of all the\nimages that the receiver sees. The networks successfully develop an emergent\nlanguage to get an identification accuracy of 77.8%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:34:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chowdhury", "Aritra", ""], ["Kubricht", "James R.", ""], ["Sood", "Anup", ""], ["Tu", "Peter", ""], ["Santamaria-Pang", "Alberto", ""]]}, {"id": "2007.09470", "submitter": "Rui Yan", "authors": "Rui Yan, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian", "title": "Social Adaptive Module for Weakly-supervised Group Activity Recognition", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new task named weakly-supervised group activity\nrecognition (GAR) which differs from conventional GAR tasks in that only\nvideo-level labels are available, yet the important persons within each frame\nare not provided even in the training data. This eases us to collect and\nannotate a large-scale NBA dataset and thus raise new challenges to GAR. To\nmine useful information from weak supervision, we present a key insight that\nkey instances are likely to be related to each other, and thus design a social\nadaptive module (SAM) to reason about key persons and frames from noisy data.\nExperiments show significant improvement on the NBA dataset as well as the\npopular volleyball dataset. In particular, our model trained on video-level\nannotation achieves comparable accuracy to prior algorithms which required\nstrong labels.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:40:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yan", "Rui", ""], ["Xie", "Lingxi", ""], ["Tang", "Jinhui", ""], ["Shu", "Xiangbo", ""], ["Tian", "Qi", ""]]}, {"id": "2007.09471", "submitter": "Alberto Santamaria-Pang", "authors": "Alberto Santamaria-Pang, Anup Sood, Dan Meyer, Aritra Chowdhury, Fiona\n  Ginty", "title": "Automated Phenotyping via Cell Auto Training (CAT) on the Cell DIVE\n  Platform", "comments": "2019 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)", "journal-ref": "2019 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM), San Diego, CA, USA, 2019, pp. 2750-2756", "doi": "10.1109/BIBM47256.2019.8983271", "report-no": null, "categories": "eess.IV cs.CV q-bio.CB q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for automatic cell classification in tissue samples using\nan automated training set from multiplexed immunofluorescence images. The\nmethod utilizes multiple markers stained in situ on a single tissue section on\na robust hyperplex immunofluorescence platform (Cell DIVE, GE Healthcare) that\nprovides multi-channel images allowing analysis at single cell/sub-cellular\nlevels. The cell classification method consists of two steps: first, an\nautomated training set from every image is generated using marker-to-cell\nstaining information. This mimics how a pathologist would select samples from a\nvery large cohort at the image level. In the second step, a probability model\nis inferred from the automated training set. The probabilistic model captures\nstaining patterns in mutually exclusive cell types and builds a single\nprobability model for the data cohort. We have evaluated the proposed approach\nto classify: i) immune cells in cancer and ii) brain cells in neurological\ndegenerative diseased tissue with average accuracies above 95%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:45:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Santamaria-Pang", "Alberto", ""], ["Sood", "Anup", ""], ["Meyer", "Dan", ""], ["Chowdhury", "Aritra", ""], ["Ginty", "Fiona", ""]]}, {"id": "2007.09478", "submitter": "Nabil Ettehadi", "authors": "Hangwei Zhuang and Nabil Ettehadi", "title": "Classification of Diabetic Retinopathy via Fundus Photography:\n  Utilization of Deep Learning Approaches to Speed up Disease Detection", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two distinct solutions to the problem of Diabetic\nRetinopathy (DR) classification. In the first approach, we introduce a shallow\nneural network architecture. This model performs well on classification of the\nmost frequent classes while fails at classifying the less frequent ones. In the\nsecond approach, we use transfer learning to re-train the last modified layer\nof a very deep neural network to improve the generalization ability of the\nmodel to the less frequent classes. Our results demonstrate superior abilities\nof transfer learning in DR classification of less frequent classes compared to\nthe shallow neural network.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 17:11:20 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhuang", "Hangwei", ""], ["Ettehadi", "Nabil", ""]]}, {"id": "2007.09479", "submitter": "Zhihua Liu", "authors": "Zhihua Liu, Long Chen, Lei Tong, Feixiang Zhou, Zheheng Jiang, Qianni\n  Zhang, Caifeng Shan, Yinhai Wang, Xiangrong Zhang, Ling Li, Huiyu Zhou", "title": "Deep Learning Based Brain Tumor Segmentation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain tumor segmentation is a challenging problem in medical image analysis.\nThe goal of brain tumor segmentation is to generate accurate delineation of\nbrain tumor regions with correctly located masks. In recent years, deep\nlearning methods have shown very promising performance in solving various\ncomputer vision problems, such as image classification, object detection and\nsemantic segmentation. A number of deep learning based methods have been\napplied to brain tumor segmentation and achieved impressive system performance.\nConsidering state-of-the-art technologies and their performance, the purpose of\nthis paper is to provide a comprehensive survey of recently developed deep\nlearning based brain tumor segmentation techniques. The established works\nincluded in this survey extensively cover technical aspects such as the\nstrengths and weaknesses of different approaches, pre- and post-processing\nframeworks, datasets and evaluation metrics. Finally, we conclude this survey\nby discussing the potential development in future research work.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 17:14:50 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 20:00:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Liu", "Zhihua", ""], ["Chen", "Long", ""], ["Tong", "Lei", ""], ["Zhou", "Feixiang", ""], ["Jiang", "Zheheng", ""], ["Zhang", "Qianni", ""], ["Shan", "Caifeng", ""], ["Wang", "Yinhai", ""], ["Zhang", "Xiangrong", ""], ["Li", "Ling", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2007.09482", "submitter": "Minghui Liao", "authors": "Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, Xiang Bai", "title": "Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text\n  Spotting", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent end-to-end trainable methods for scene text spotting, integrating\ndetection and recognition, showed much progress. However, most of the current\narbitrary-shape scene text spotters use region proposal networks (RPN) to\nproduce proposals. RPN relies heavily on manually designed anchors and its\nproposals are represented with axis-aligned rectangles. The former presents\ndifficulties in handling text instances of extreme aspect ratios or irregular\nshapes, and the latter often includes multiple neighboring instances into a\nsingle proposal, in cases of densely oriented text. To tackle these problems,\nwe propose Mask TextSpotter v3, an end-to-end trainable scene text spotter that\nadopts a Segmentation Proposal Network (SPN) instead of an RPN. Our SPN is\nanchor-free and gives accurate representations of arbitrary-shape proposals. It\nis therefore superior to RPN in detecting text instances of extreme aspect\nratios or irregular shapes. Furthermore, the accurate proposals produced by SPN\nallow masked RoI features to be used for decoupling neighboring text instances.\nAs a result, our Mask TextSpotter v3 can handle text instances of extreme\naspect ratios or irregular shapes, and its recognition accuracy won't be\naffected by nearby text or background noise. Specifically, we outperform\nstate-of-the-art methods by 21.9 percent on the Rotated ICDAR 2013 dataset\n(rotation robustness), 5.9 percent on the Total-Text dataset (shape\nrobustness), and achieve state-of-the-art performance on the MSRA-TD500 dataset\n(aspect ratio robustness). Code is available at:\nhttps://github.com/MhLiao/MaskTextSpotterV3\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 17:25:50 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liao", "Minghui", ""], ["Pang", "Guan", ""], ["Huang", "Jing", ""], ["Hassner", "Tal", ""], ["Bai", "Xiang", ""]]}, {"id": "2007.09493", "submitter": "Yancong Lin", "authors": "Yancong Lin, Silvia L. Pintea, and Jan C. van Gemert", "title": "Deep Hough-Transform Line Priors", "comments": "ECCV 2020, code online:\n  https://github.com/yanconglin/Deep-Hough-Transform-Line-Priors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical work on line segment detection is knowledge-based; it uses\ncarefully designed geometric priors using either image gradients, pixel\ngroupings, or Hough transform variants. Instead, current deep learning methods\ndo away with all prior knowledge and replace priors by training deep networks\non large manually annotated datasets. Here, we reduce the dependency on labeled\ndata by building on the classic knowledge-based priors while using deep\nnetworks to learn features. We add line priors through a trainable Hough\ntransform block into a deep network. Hough transform provides the prior\nknowledge about global line parameterizations, while the convolutional layers\ncan learn the local gradient-like line features. On the Wireframe\n(ShanghaiTech) and York Urban datasets we show that adding prior knowledge\nimproves data efficiency as line priors no longer need to be learned from data.\nKeywords: Hough transform; global line prior, line segment detection.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 18:12:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lin", "Yancong", ""], ["Pintea", "Silvia L.", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2007.09502", "submitter": "Ali Varamesh", "authors": "Ali Varamesh, Tinne Tuytelaars", "title": "MIX'EM: Unsupervised Image Classification using a Mixture of Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MIX'EM, a novel solution for unsupervised image classification.\nMIX'EM generates representations that by themselves are sufficient to drive a\ngeneral-purpose clustering algorithm to deliver high-quality classification.\nThis is accomplished by building a mixture of embeddings module into a\ncontrastive visual representation learning framework in order to disentangle\nrepresentations at the category level. It first generates a set of embedding\nand mixing coefficients from a given visual representation, and then combines\nthem into a single embedding. We introduce three techniques to successfully\ntrain MIX'EM and avoid degenerate solutions; (i) diversify mixture components\nby maximizing entropy, (ii) minimize instance conditioned component entropy to\nenforce a clustered embedding space, and (iii) use an associative embedding\nloss to enforce semantic separability. By applying (i) and (ii), semantic\ncategories emerge through the mixture coefficients, making it possible to apply\n(iii). Subsequently, we run K-means on the representations to acquire semantic\nclassification. We conduct extensive experiments and analyses on STL10,\nCIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification\naccuracy of 78\\%, 82\\%, and 44\\%, respectively. To achieve robust and high\naccuracy, it is essential to use the mixture components to initialize K-means.\nFinally, we report competitive baselines (70\\% on STL10) obtained by applying\nK-means to the \"normalized\" representations learned using the contrastive loss.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 19:24:22 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 23:01:29 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Varamesh", "Ali", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2007.09507", "submitter": "Gukyeong Kwon", "authors": "Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib", "title": "Backpropagated Gradient Representations for Anomaly Detection", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations that clearly distinguish between normal and abnormal\ndata is key to the success of anomaly detection. Most of existing anomaly\ndetection algorithms use activation representations from forward propagation\nwhile not exploiting gradients from backpropagation to characterize data.\nGradients capture model updates required to represent data. Anomalies require\nmore drastic model updates to fully represent them compared to normal data.\nHence, we propose the utilization of backpropagated gradients as\nrepresentations to characterize model behavior on anomalies and, consequently,\ndetect such anomalies. We show that the proposed method using gradient-based\nrepresentations achieves state-of-the-art anomaly detection performance in\nbenchmark image recognition datasets. Also, we highlight the computational\nefficiency and the simplicity of the proposed method in comparison with other\nstate-of-the-art methods relying on adversarial networks or autoregressive\nmodels, which require at least 27 times more model parameters than the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 19:39:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kwon", "Gukyeong", ""], ["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2007.09509", "submitter": "Weihong Ren", "authors": "Weihong Ren, Xinchao Wang, Jiandong Tian, Yandong Tang and Antoni B.\n  Chan", "title": "Tracking-by-Counting: Using Network Flows on Crowd Density Maps for\n  Tracking Multiple Targets", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TIP.2020.3044219", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art multi-object tracking~(MOT) methods follow the\ntracking-by-detection paradigm, where object trajectories are obtained by\nassociating per-frame outputs of object detectors. In crowded scenes, however,\ndetectors often fail to obtain accurate detections due to heavy occlusions and\nhigh crowd density. In this paper, we propose a new MOT paradigm,\ntracking-by-counting, tailored for crowded scenes. Using crowd density maps, we\njointly model detection, counting, and tracking of multiple targets as a\nnetwork flow program, which simultaneously finds the global optimal detections\nand trajectories of multiple targets over the whole video. This is in contrast\nto prior MOT methods that either ignore the crowd density and thus are prone to\nerrors in crowded scenes, or rely on a suboptimal two-step process using\nheuristic density-aware point-tracks for matching targets.Our approach yields\npromising results on public benchmarks of various domains including people\ntracking, cell tracking, and fish tracking.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 19:51:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ren", "Weihong", ""], ["Wang", "Xinchao", ""], ["Tian", "Jiandong", ""], ["Tang", "Yandong", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2007.09510", "submitter": "Mozhdeh Rouhsedaghat", "authors": "Mozhdeh Rouhsedaghat, Yifan Wang, Xiou Ge, Shuowen Hu, Suya You, C.-C.\n  Jay Kuo", "title": "FaceHop: A Light-Weight Low-Resolution Face Gender Classification Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A light-weight low-resolution face gender classification method, called\nFaceHop, is proposed in this research. We have witnessed rapid progress in face\ngender classification accuracy due to the adoption of deep learning (DL)\ntechnology. Yet, DL-based systems are not suitable for resource-constrained\nenvironments with limited networking and computing. FaceHop offers an\ninterpretable non-parametric machine learning solution. It has desired\ncharacteristics such as a small model size, a small training data amount, low\ntraining complexity, and low-resolution input images. FaceHop is developed with\nthe successive subspace learning (SSL) principle and built upon the foundation\nof PixelHop++. The effectiveness of the FaceHop method is demonstrated by\nexperiments. For gray-scale face images of resolution $32 \\times 32$ in the LFW\nand the CMU Multi-PIE datasets, FaceHop achieves correct gender classification\nrates of 94.63% and 95.12% with model sizes of 16.9K and 17.6K parameters,\nrespectively. It outperforms LeNet-5 in classification accuracy while LeNet-5\nhas a model size of 75.8K parameters.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 19:59:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 02:58:46 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 02:16:59 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Rouhsedaghat", "Mozhdeh", ""], ["Wang", "Yifan", ""], ["Ge", "Xiou", ""], ["Hu", "Shuowen", ""], ["You", "Suya", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2007.09529", "submitter": "Rui Zhu", "authors": "Rui Zhu, Xingyi Yang, Yannick Hold-Geoffroy, Federico Perazzi,\n  Jonathan Eisenmann, Kalyan Sunkavalli, Manmohan Chandraker", "title": "Single View Metrology in the Wild", "comments": "ECCV 2020, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3D reconstruction methods may only recover scene properties up to a\nglobal scale ambiguity. We present a novel approach to single view metrology\nthat can recover the absolute scale of a scene represented by 3D heights of\nobjects or camera height above the ground as well as camera parameters of\norientation and field of view, using just a monocular image acquired in\nunconstrained condition. Our method relies on data-driven priors learned by a\ndeep network specifically designed to imbibe weakly supervised constraints from\nthe interplay of the unknown camera with 3D entities such as object heights,\nthrough estimation of bounding box projections. We leverage categorical priors\nfor objects such as humans or cars that commonly occur in natural images, as\nreferences for scale estimation. We demonstrate state-of-the-art qualitative\nand quantitative results on several datasets as well as applications including\nvirtual object insertion. Furthermore, the perceptual quality of our outputs is\nvalidated by a user study.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 22:31:33 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 22:49:13 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 05:40:10 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhu", "Rui", ""], ["Yang", "Xingyi", ""], ["Hold-Geoffroy", "Yannick", ""], ["Perazzi", "Federico", ""], ["Eisenmann", "Jonathan", ""], ["Sunkavalli", "Kalyan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.09539", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Gaussian kernel smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image acquisition and segmentation are likely to introduce noise. Further\nimage processing such as image registration and parameterization can introduce\nadditional noise. It is thus imperative to reduce noise measurements and boost\nsignal. In order to increase the signal-to-noise ratio (SNR) and smoothness of\ndata required for the subsequent random field theory based statistical\ninference, some type of smoothing is necessary. Among many image smoothing\nmethods, Gaussian kernel smoothing has emerged as a de facto smoothing\ntechnique among brain imaging researchers due to its simplicity in numerical\nimplementation. Gaussian kernel smoothing also increases statistical\nsensitivity and statistical power as well as Gausianness. Gaussian kernel\nsmoothing can be viewed as weighted averaging of voxel values. Then from the\ncentral limit theorem, the weighted average should be more Gaussian.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 00:19:07 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:37:44 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2007.09545", "submitter": "Samarth Brahmbhatt", "authors": "Samarth Brahmbhatt, Chengcheng Tang, Christopher D. Twigg, Charles C.\n  Kemp, James Hays", "title": "ContactPose: A Dataset of Grasps with Object Contact and Hand Pose", "comments": "The European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping is natural for humans. However, it involves complex hand\nconfigurations and soft tissue deformation that can result in complicated\nregions of contact between the hand and the object. Understanding and modeling\nthis contact can potentially improve hand models, AR/VR experiences, and\nrobotic grasping. Yet, we currently lack datasets of hand-object contact paired\nwith other data modalities, which is crucial for developing and evaluating\ncontact modeling techniques. We introduce ContactPose, the first dataset of\nhand-object contact paired with hand pose, object pose, and RGB-D images.\nContactPose has 2306 unique grasps of 25 household objects grasped with 2\nfunctional intents by 50 participants, and more than 2.9 M RGB-D grasp images.\nAnalysis of ContactPose data reveals interesting relationships between hand\npose and contact. We use this data to rigorously evaluate various data\nrepresentations, heuristics from the literature, and learning methods for\ncontact modeling. Data, code, and trained models are available at\nhttps://contactpose.cc.gatech.edu.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:01:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Tang", "Chengcheng", ""], ["Twigg", "Christopher D.", ""], ["Kemp", "Charles C.", ""], ["Hays", "James", ""]]}, {"id": "2007.09547", "submitter": "Songtao He", "authors": "Songtao He, Favyen Bastani, Satvat Jagwani, Mohammad Alizadeh, Hari\n  Balakrishnan, Sanjay Chawla, Mohamed M. Elshrif, Samuel Madden, Amin Sadeghi", "title": "Sat2Graph: Road Graph Extraction through Graph-Tensor Encoding", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring road graphs from satellite imagery is a challenging computer vision\ntask. Prior solutions fall into two categories: (1) pixel-wise\nsegmentation-based approaches, which predict whether each pixel is on a road,\nand (2) graph-based approaches, which predict the road graph iteratively. We\nfind that these two approaches have complementary strengths while suffering\nfrom their own inherent limitations.\n  In this paper, we propose a new method, Sat2Graph, which combines the\nadvantages of the two prior categories into a unified framework. The key idea\nin Sat2Graph is a novel encoding scheme, graph-tensor encoding (GTE), which\nencodes the road graph into a tensor representation. GTE makes it possible to\ntrain a simple, non-recurrent, supervised model to predict a rich set of\nfeatures that capture the graph structure directly from an image. We evaluate\nSat2Graph using two large datasets. We find that Sat2Graph surpasses prior\nmethods on two widely used metrics, TOPO and APLS. Furthermore, whereas prior\nwork only infers planar road graphs, our approach is capable of inferring\nstacked roads (e.g., overpasses), and does so robustly.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:04:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["He", "Songtao", ""], ["Bastani", "Favyen", ""], ["Jagwani", "Satvat", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hari", ""], ["Chawla", "Sanjay", ""], ["Elshrif", "Mohamed M.", ""], ["Madden", "Samuel", ""], ["Sadeghi", "Amin", ""]]}, {"id": "2007.09548", "submitter": "Garrick Brazil", "authors": "Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, Bernt Schiele", "title": "Kinematic 3D Object Detection in Monocular Video", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving the physical world in 3D is fundamental for self-driving\napplications. Although temporal motion is an invaluable resource to human\nvision for detection, tracking, and depth perception, such features have not\nbeen thoroughly utilized in modern 3D object detectors. In this work, we\npropose a novel method for monocular video-based 3D object detection which\ncarefully leverages kinematic motion to improve precision of 3D localization.\nSpecifically, we first propose a novel decomposition of object orientation as\nwell as a self-balancing 3D confidence. We show that both components are\ncritical to enable our kinematic model to work effectively. Collectively, using\nonly a single model, we efficiently leverage 3D kinematics from monocular\nvideos to improve the overall localization precision in 3D object detection\nwhile also producing useful by-products of scene dynamics (ego-motion and\nper-object velocity). We achieve state-of-the-art performance on monocular 3D\nobject detection and the Bird's Eye View tasks within the KITTI self-driving\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:15:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Brazil", "Garrick", ""], ["Pons-Moll", "Gerard", ""], ["Liu", "Xiaoming", ""], ["Schiele", "Bernt", ""]]}, {"id": "2007.09549", "submitter": "Maunil Vyas", "authors": "Maunil R Vyas, Hemanth Venkateswara, Sethuraman Panchanathan", "title": "Leveraging Seen and Unseen Semantic Relationships for Generative\n  Zero-Shot Learning", "comments": "19 Pages, To be appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot learning (ZSL) addresses the unseen class recognition problem by\nleveraging semantic information to transfer knowledge from seen classes to\nunseen classes. Generative models synthesize the unseen visual features and\nconvert ZSL into a classical supervised learning problem. These generative\nmodels are trained using the seen classes and are expected to implicitly\ntransfer the knowledge from seen to unseen classes. However, their performance\nis stymied by overfitting, which leads to substandard performance on\nGeneralized Zero-Shot learning (GZSL). To address this concern, we propose the\nnovel LsrGAN, a generative model that Leverages the Semantic Relationship\nbetween seen and unseen categories and explicitly performs knowledge transfer\nby incorporating a novel Semantic Regularized Loss (SR-Loss). The SR-loss\nguides the LsrGAN to generate visual features that mirror the semantic\nrelationships between seen and unseen classes. Experiments on seven benchmark\ndatasets, including the challenging Wikipedia text-based CUB and NABirds\nsplits, and Attribute-based AWA, CUB, and SUN, demonstrates the superiority of\nthe LsrGAN compared to previous state-of-the-art approaches under both ZSL and\nGZSL. Code is available at https: // github. com/ Maunil/ LsrGAN\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:25:53 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vyas", "Maunil R", ""], ["Venkateswara", "Hemanth", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "2007.09550", "submitter": "Yifan Peng", "authors": "Yifan Peng, Tiarnan D. Keenan, Qingyu Chen, Elvira Agr\\'on, Alexis\n  Allot, Wai T. Wong, Emily Y. Chew, Zhiyong Lu", "title": "Predicting risk of late age-related macular degeneration using deep\n  learning", "comments": "Accepted by npj Digital Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By 2040, age-related macular degeneration (AMD) will affect approximately 288\nmillion people worldwide. Identifying individuals at high risk of progression\nto late AMD, the sight-threatening stage, is critical for clinical actions,\nincluding medical interventions and timely monitoring. Although deep learning\nhas shown promise in diagnosing/screening AMD using color fundus photographs,\nit remains difficult to predict individuals' risks of late AMD accurately. For\nboth tasks, these initial deep learning attempts have remained largely\nunvalidated in independent cohorts. Here, we demonstrate how deep learning and\nsurvival analysis can predict the probability of progression to late AMD using\n3,298 participants (over 80,000 images) from the Age-Related Eye Disease\nStudies AREDS and AREDS2, the largest longitudinal clinical trials in AMD. When\nvalidated against an independent test dataset of 601 participants, our model\nachieved high prognostic accuracy (five-year C-statistic 86.4 (95% confidence\ninterval 86.2-86.6)) that substantially exceeded that of retinal specialists\nusing two existing clinical standards (81.3 (81.1-81.5) and 82.0 (81.8-82.3),\nrespectively). Interestingly, our approach offers additional strengths over the\nexisting clinical standards in AMD prognosis (e.g., risk ascertainment above\n50%) and is likely to be highly generalizable, given the breadth of training\ndata from 82 US retinal specialty clinics. Indeed, during external validation\nthrough training on AREDS and testing on AREDS2 as an independent cohort, our\nmodel retained substantially higher prognostic accuracy than existing clinical\nstandards. These results highlight the potential of deep learning systems to\nenhance clinical decision-making in AMD patients.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:32:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Peng", "Yifan", ""], ["Keenan", "Tiarnan D.", ""], ["Chen", "Qingyu", ""], ["Agr\u00f3n", "Elvira", ""], ["Allot", "Alexis", ""], ["Wong", "Wai T.", ""], ["Chew", "Emily Y.", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2007.09551", "submitter": "Soham Dan", "authors": "Soham Dan, Hangfeng He, Dan Roth", "title": "Understanding Spatial Relations through Multiple Modalities", "comments": null, "journal-ref": "LREC 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing spatial relations and reasoning about them is essential in\nmultiple applications including navigation, direction giving and human-computer\ninteraction in general. Spatial relations between objects can either be\nexplicit -- expressed as spatial prepositions, or implicit -- expressed by\nspatial verbs such as moving, walking, shifting, etc. Both these, but implicit\nrelations in particular, require significant common sense understanding. In\nthis paper, we introduce the task of inferring implicit and explicit spatial\nrelations between two entities in an image. We design a model that uses both\ntextual and visual information to predict the spatial relations, making use of\nboth positional and size information of objects and image embeddings. We\ncontrast our spatial model with powerful language models and show how our\nmodeling complements the power of these, improving prediction accuracy and\ncoverage and facilitates dealing with unseen subjects, objects and relations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:35:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dan", "Soham", ""], ["He", "Hangfeng", ""], ["Roth", "Dan", ""]]}, {"id": "2007.09552", "submitter": "Yuqing Liu", "authors": "Yuqing Liu and Xinfeng Zhang and Shanshe Wang and Siwei Ma and Wen Gao", "title": "Progressive Multi-Scale Residual Network for Single Image\n  Super-Resolution", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale convolutional neural networks (CNNs) achieve significant success\nin single image super-resolution (SISR), which considers the comprehensive\ninformation from different receptive fields. However, recent multi-scale\nnetworks usually aim to build the hierarchical exploration with different sizes\nof filters, which lead to high computation complexity costs, and seldom focus\non the inherent correlations among different scales. This paper converts the\nmulti-scale exploration into a sequential manner, and proposes a progressive\nmulti-scale residual network (PMRN) for SISR problem. Specifically, we devise a\nprogressive multi-scale residual block (PMRB) to substitute the larger filters\nwith small filter combinations, and gradually explore the hierarchical\ninformation. Furthermore, channel- and pixel-wise attention mechanism (CPA) is\ndesigned for finding the inherent correlations among image features with\nweighting and bias factors, which concentrates more on high-frequency\ninformation. Experimental results show that the proposed PMRN recovers\nstructural textures more effectively with superior PSNR/SSIM results than other\nsmall networks. The extension model PMRN$^+$ with self-ensemble achieves\ncompetitive or better results than large networks with much fewer parameters\nand lower computation complexity.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:35:53 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 05:55:45 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 16:57:04 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Liu", "Yuqing", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2007.09554", "submitter": "Qi Wu", "authors": "Yanyuan Qiao, Chaorui Deng, Qi Wu", "title": "Referring Expression Comprehension: A Survey of Methods and Datasets", "comments": "Accepted to IEEE TMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension (REC) aims to localize a target object in\nan image described by a referring expression phrased in natural language.\nDifferent from the object detection task that queried object labels have been\npre-defined, the REC problem only can observe the queries during the test. It\nthus more challenging than a conventional computer vision problem. This task\nhas attracted a lot of attention from both computer vision and natural language\nprocessing community, and several lines of work have been proposed, from\nCNN-RNN model, modular network to complex graph-based model. In this survey, we\nfirst examine the state of the art by comparing modern approaches to the\nproblem. We classify methods by their mechanism to encode the visual and\ntextual modalities. In particular, we examine the common approach of joint\nembedding images and expressions to a common feature space. We also discuss\nmodular architectures and graph-based models that interface with structured\ngraph representation. In the second part of this survey, we review the datasets\navailable for training and evaluating REC systems. We then group results\naccording to the datasets, backbone models, settings so that they can be fairly\ncompared. Finally, we discuss promising future directions for the field, in\nparticular the compositional referring expression comprehension that requires\nlonger reasoning chain to address.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 01:45:02 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 04:56:24 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Qiao", "Yanyuan", ""], ["Deng", "Chaorui", ""], ["Wu", "Qi", ""]]}, {"id": "2007.09558", "submitter": "Yikai Wang", "authors": "Yikai Wang, Fuchun Sun, Duo Li, Anbang Yao", "title": "Resolution Switchable Networks for Runtime Efficient Image Recognition", "comments": "ECCV 2020. Code and models: https://github.com/yikaiw/RS-Nets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method to train a single convolutional neural network\nwhich is capable of switching image resolutions at inference. Thus the running\nspeed can be selected to meet various computational resource limits. Networks\ntrained with the proposed method are named Resolution Switchable Networks\n(RS-Nets). The basic training framework shares network parameters for handling\nimages which differ in resolution, yet keeps separate batch normalization\nlayers. Though it is parameter-efficient in design, it leads to inconsistent\naccuracy variations at different resolutions, for which we provide a detailed\nanalysis from the aspect of the train-test recognition discrepancy. A\nmulti-resolution ensemble distillation is further designed, where a teacher is\nlearnt on the fly as a weighted ensemble over resolutions. Thanks to the\nensemble and knowledge distillation, RS-Nets enjoy accuracy improvements at a\nwide range of resolutions compared with individually trained models. Extensive\nexperiments on the ImageNet dataset are provided, and we additionally consider\nquantization problems. Code and models are available at\nhttps://github.com/yikaiw/RS-Nets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 02:12:59 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 19:16:23 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 07:18:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Yikai", ""], ["Sun", "Fuchun", ""], ["Li", "Duo", ""], ["Yao", "Anbang", ""]]}, {"id": "2007.09580", "submitter": "Chaorui Deng", "authors": "Chaorui Deng, Ning Ding, Mingkui Tan, Qi Wu", "title": "Length-Controllable Image Captioning", "comments": "To be appeared in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed remarkable progress in the image captioning\ntask; however, most existing methods cannot control their captions,\n\\emph{e.g.}, choosing to describe the image either roughly or in detail. In\nthis paper, we propose to use a simple length level embedding to endow them\nwith this ability. Moreover, due to their autoregressive nature, the\ncomputational complexity of existing models increases linearly as the length of\nthe generated captions grows. Thus, we further devise a non-autoregressive\nimage captioning approach that can generate captions in a length-irrelevant\ncomplexity. We verify the merit of the proposed length level embedding on three\nmodels: two state-of-the-art (SOTA) autoregressive models with different types\nof decoder, as well as our proposed non-autoregressive model, to show its\ngeneralization ability. In the experiments, our length-controllable image\ncaptioning models not only achieve SOTA performance on the challenging MS COCO\ndataset but also generate length-controllable and diverse image captions.\nSpecifically, our non-autoregressive model outperforms the autoregressive\nbaselines in terms of controllability and diversity, and also significantly\nimproves the decoding efficiency for long captions. Our code and models are\nreleased at \\textcolor{magenta}{\\texttt{https://github.com/bearcatt/LaBERT}}.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 03:40:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Deng", "Chaorui", ""], ["Ding", "Ning", ""], ["Tan", "Mingkui", ""], ["Wu", "Qi", ""]]}, {"id": "2007.09584", "submitter": "Cong Yang", "authors": "Zhiming Chen and Kean Chen and Weiyao Lin and John See and Hui Yu and\n  Yan Ke and Cong Yang", "title": "PIoU Loss: Towards Accurate Oriented Object Detection in Complex\n  Environments", "comments": null, "journal-ref": "European Conference on Computer Vision, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection using an oriented bounding box (OBB) can better target\nrotated objects by reducing the overlap with background areas. Existing OBB\napproaches are mostly built on horizontal bounding box detectors by introducing\nan additional angle dimension optimized by a distance loss. However, as the\ndistance loss only minimizes the angle error of the OBB and that it loosely\ncorrelates to the IoU, it is insensitive to objects with high aspect ratios.\nTherefore, a novel loss, Pixels-IoU (PIoU) Loss, is formulated to exploit both\nthe angle and IoU for accurate OBB regression. The PIoU loss is derived from\nIoU metric with a pixel-wise form, which is simple and suitable for both\nhorizontal and oriented bounding box. To demonstrate its effectiveness, we\nevaluate the PIoU loss on both anchor-based and anchor-free frameworks. The\nexperimental results show that PIoU loss can dramatically improve the\nperformance of OBB detectors, particularly on objects with high aspect ratios\nand complex backgrounds. Besides, previous evaluation datasets did not include\nscenarios where the objects have high aspect ratios, hence a new dataset,\nRetail50K, is introduced to encourage the community to adapt OBB detectors for\nmore complex environments.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 03:51:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Zhiming", ""], ["Chen", "Kean", ""], ["Lin", "Weiyao", ""], ["See", "John", ""], ["Yu", "Hui", ""], ["Ke", "Yan", ""], ["Yang", "Cong", ""]]}, {"id": "2007.09590", "submitter": "Weiting Huang", "authors": "Weiting Huang and Pengfei Ren and Jingyu Wang and Qi Qi and Haifeng\n  Sun", "title": "AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation", "comments": "Accepted by AAAI-2020", "journal-ref": "published 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adaptive weighting regression (AWR) method to\nleverage the advantages of both detection-based and regression-based methods.\nHand joint coordinates are estimated as discrete integration of all pixels in\ndense representation, guided by adaptive weight maps. This learnable\naggregation process introduces both dense and joint supervision that allows\nend-to-end training and brings adaptability to weight maps, making the network\nmore accurate and robust. Comprehensive exploration experiments are conducted\nto validate the effectiveness and generality of AWR under various experimental\nsettings, especially its usefulness for different types of dense representation\nand input modality. Our method outperforms other state-of-the-art methods on\nfour publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 04:57:13 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Huang", "Weiting", ""], ["Ren", "Pengfei", ""], ["Wang", "Jingyu", ""], ["Qi", "Qi", ""], ["Sun", "Haifeng", ""]]}, {"id": "2007.09592", "submitter": "Ruixue Tang", "authors": "Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, Xiaokang Yang", "title": "Semantic Equivalent Adversarial Data Augmentation for Visual Question\n  Answering", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has achieved great success thanks to the fast\ndevelopment of deep neural networks (DNN). On the other hand, the data\naugmentation, as one of the major tricks for DNN, has been widely used in many\ncomputer vision tasks. However, there are few works studying the data\naugmentation problem for VQA and none of the existing image based augmentation\nschemes (such as rotation and flipping) can be directly applied to VQA due to\nits semantic structure -- an $\\langle image, question, answer\\rangle$ triplet\nneeds to be maintained correctly. For example, a direction related\nQuestion-Answer (QA) pair may not be true if the associated image is rotated or\nflipped. In this paper, instead of directly manipulating images and questions,\nwe use generated adversarial examples for both images and questions as the\naugmented data. The augmented examples do not change the visual properties\npresented in the image as well as the \\textbf{semantic} meaning of the\nquestion, the correctness of the $\\langle image, question, answer\\rangle$ is\nthus still maintained. We then use adversarial learning to train a classic VQA\nmodel (BUTD) with our augmented data. We find that we not only improve the\noverall performance on VQAv2, but also can withstand adversarial attack\neffectively, compared to the baseline model. The source code is available at\nhttps://github.com/zaynmi/seada-vqa.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 05:01:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tang", "Ruixue", ""], ["Ma", "Chao", ""], ["Zhang", "Wei Emma", ""], ["Wu", "Qi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2007.09594", "submitter": "Lei Yang", "authors": "Lei Yang, Wenxi Liu, Zhiming Cui, Nenglun Chen, Wenping Wang", "title": "Mapping in a cycle: Sinkhorn regularized unsupervised learning for point\n  cloud shapes", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised learning framework with the pretext task of\nfinding dense correspondences between point cloud shapes from the same category\nbased on the cycle-consistency formulation. In order to learn discriminative\npointwise features from point cloud data, we incorporate in the formulation a\nregularization term based on Sinkhorn normalization to enhance the learned\npointwise mappings to be as bijective as possible. Besides, a random rigid\ntransform of the source shape is introduced to form a triplet cycle to improve\nthe model's robustness against perturbations. Comprehensive experiments\ndemonstrate that the learned pointwise features through our framework benefits\nvarious point cloud analysis tasks, e.g. partial shape registration and\nkeypoint transfer. We also show that the learned pointwise features can be\nleveraged by supervised methods to improve the part segmentation performance\nwith either the full training dataset or just a small portion of it.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 05:21:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yang", "Lei", ""], ["Liu", "Wenxi", ""], ["Cui", "Zhiming", ""], ["Chen", "Nenglun", ""], ["Wang", "Wenping", ""]]}, {"id": "2007.09598", "submitter": "Mrigank Rochan", "authors": "Mrigank Rochan, Mahesh Kumar Krishna Reddy, Linwei Ye, Yang Wang", "title": "Adaptive Video Highlight Detection by Learning from User History", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is an increasing interest in highlight detection research\nwhere the goal is to create a short duration video from a longer video by\nextracting its interesting moments. However, most existing methods ignore the\nfact that the definition of video highlight is highly subjective. Different\nusers may have different preferences of highlight for the same input video. In\nthis paper, we propose a simple yet effective framework that learns to adapt\nhighlight detection to a user by exploiting the user's history in the form of\nhighlights that the user has previously created. Our framework consists of two\nsub-networks: a fully temporal convolutional highlight detection network $H$\nthat predicts highlight for an input video and a history encoder network $M$\nfor user history. We introduce a newly designed temporal-adaptive instance\nnormalization (T-AIN) layer to $H$ where the two sub-networks interact with\neach other. T-AIN has affine parameters that are predicted from $M$ based on\nthe user history and is responsible for the user-adaptive signal to $H$.\nExtensive experiments on a large-scale dataset show that our framework can make\nmore accurate and user-specific highlight predictions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 05:52:20 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Rochan", "Mrigank", ""], ["Reddy", "Mahesh Kumar Krishna", ""], ["Ye", "Linwei", ""], ["Wang", "Yang", ""]]}, {"id": "2007.09600", "submitter": "Rakshit Kothari", "authors": "Rakshit S. Kothari, Aayush K. Chaudhary, Reynold J. Bailey, Jeff B.\n  Pelz, Gabriel J. Diaz", "title": "EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2021.3067765", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ellipse fitting, an essential component in pupil or iris tracking based video\noculography, is performed on previously segmented eye parts generated using\nvarious computer vision techniques. Several factors, such as occlusions due to\neyelid shape, camera position or eyelashes, frequently break ellipse fitting\nalgorithms that rely on well-defined pupil or iris edge segments. In this work,\nwe propose training a convolutional neural network to directly segment entire\nelliptical structures and demonstrate that such a framework is robust to\nocclusions and offers superior pupil and iris tracking performance (at least\n10$\\%$ and 24$\\%$ increase in pupil and iris center detection rate respectively\nwithin a two-pixel error margin) compared to using standard eye parts\nsegmentation for multiple publicly available synthetic segmentation datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 06:13:01 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kothari", "Rakshit S.", ""], ["Chaudhary", "Aayush K.", ""], ["Bailey", "Reynold J.", ""], ["Pelz", "Jeff B.", ""], ["Diaz", "Gabriel J.", ""]]}, {"id": "2007.09609", "submitter": "Yu-Tong Cao", "authors": "Yu-Tong Cao, Jingya Wang, Dacheng Tao", "title": "Symbiotic Adversarial Learning for Attribute-based Person Search", "comments": "17 pages, 5 figures. Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute-based person search is in significant demand for applications where\nno detected query images are available, such as identifying a criminal from\nwitness. However, the task itself is quite challenging because there is a huge\nmodality gap between images and physical descriptions of attributes. Often,\nthere may also be a large number of unseen categories (attribute combinations).\nThe current state-of-the-art methods either focus on learning better\ncross-modal embeddings by mining only seen data, or they explicitly use\ngenerative adversarial networks (GANs) to synthesize unseen features. The\nformer tends to produce poor embeddings due to insufficient data, while the\nlatter does not preserve intra-class compactness during generation. In this\npaper, we present a symbiotic adversarial learning framework, called SAL.Two\nGANs sit at the base of the framework in a symbiotic learning scheme: one\nsynthesizes features of unseen classes/categories, while the other optimizes\nthe embedding and performs the cross-modal alignment on the common embedding\nspace .Specifically, two different types of generative adversarial networks\nlearn collaboratively throughout the training process and the interactions\nbetween the two mutually benefit each other. Extensive evaluations show SAL's\nsuperiority over nine state-of-the-art methods with two challenging pedestrian\nbenchmarks, PETA and Market-1501. The code is publicly available at:\nhttps://github.com/ycao5602/SAL .\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 07:24:45 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 12:24:34 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Cao", "Yu-Tong", ""], ["Wang", "Jingya", ""], ["Tao", "Dacheng", ""]]}, {"id": "2007.09610", "submitter": "Hsien-Tzu Cheng", "authors": "Hsien-Tzu Cheng, Chun-Fu Yeh, Po-Chen Kuo, Andy Wei, Keng-Chi Liu,\n  Mong-Chi Ko, Kuan-Hua Chao, Yu-Ching Peng, and Tyng-Luh Liu", "title": "Self-similarity Student for Partial Label Histopathology Image\n  Segmentation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineation of cancerous regions in gigapixel whole slide images (WSIs) is a\ncrucial diagnostic procedure in digital pathology. This process is\ntime-consuming because of the large search space in the gigapixel WSIs, causing\nchances of omission and misinterpretation at indistinct tumor lesions. To\ntackle this, the development of an automated cancerous region segmentation\nmethod is imperative. We frame this issue as a modeling problem with partial\nlabel WSIs, where some cancerous regions may be misclassified as benign and\nvice versa, producing patches with noisy labels. To learn from these patches,\nwe propose Self-similarity Student, combining teacher-student model paradigm\nwith similarity learning. Specifically, for each patch, we first sample its\nsimilar and dissimilar patches according to spatial distance. A teacher-student\nmodel is then introduced, featuring the exponential moving average on both\nstudent model weights and teacher predictions ensemble. While our student model\ntakes patches, teacher model takes all their corresponding similar and\ndissimilar patches for learning robust representation against noisy label\npatches. Following this similarity learning, our similarity ensemble merges\nsimilar patches' ensembled predictions as the pseudo-label of a given patch to\ncounteract its noisy label. On the CAMELYON16 dataset, our method substantially\noutperforms state-of-the-art noise-aware learning methods by 5$\\%$ and the\nsupervised-trained baseline by 10$\\%$ in various degrees of noise. Moreover,\nour method is superior to the baseline on our TVGH TURP dataset with 2$\\%$\nimprovement, demonstrating the generalizability to more clinical histopathology\nsegmentation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 07:34:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cheng", "Hsien-Tzu", ""], ["Yeh", "Chun-Fu", ""], ["Kuo", "Po-Chen", ""], ["Wei", "Andy", ""], ["Liu", "Keng-Chi", ""], ["Ko", "Mong-Chi", ""], ["Chao", "Kuan-Hua", ""], ["Peng", "Yu-Ching", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "2007.09629", "submitter": "Youngmin Baek", "authors": "Youngmin Baek, Seung Shin, Jeonghun Baek, Sungrae Park, Junyeop Lee,\n  Daehyun Nam, Hwalsuk Lee", "title": "Character Region Attention For Text Spotting", "comments": "17 pages, 9 figures, Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scene text spotter is composed of text detection and recognition modules.\nMany studies have been conducted to unify these modules into an end-to-end\ntrainable model to achieve better performance. A typical architecture places\ndetection and recognition modules into separate branches, and a RoI pooling is\ncommonly used to let the branches share a visual feature. However, there still\nexists a chance of establishing a more complimentary connection between the\nmodules when adopting recognizer that uses attention-based decoder and detector\nthat represents spatial information of the character regions. This is possible\nsince the two modules share a common sub-task which is to find the location of\nthe character regions. Based on the insight, we construct a tightly coupled\nsingle pipeline model. This architecture is formed by utilizing detection\noutputs in the recognizer and propagating the recognition loss through the\ndetection stage. The use of character score map helps the recognizer attend\nbetter to the character center points, and the recognition loss propagation to\nthe detector module enhances the localization of the character regions. Also, a\nstrengthened sharing stage allows feature rectification and boundary\nlocalization of arbitrary-shaped text regions. Extensive experiments\ndemonstrate state-of-the-art performance in publicly available straight and\ncurved benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 09:12:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Baek", "Youngmin", ""], ["Shin", "Seung", ""], ["Baek", "Jeonghun", ""], ["Park", "Sungrae", ""], ["Lee", "Junyeop", ""], ["Nam", "Daehyun", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "2007.09637", "submitter": "Kazuya Ueki", "authors": "Kazuya Ueki, Tomoka Kojima", "title": "Survey on Deep Learning-based Kuzushiji Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the overwhelming accuracy of the deep learning method demonstrated\nat the 2012 image classification competition, deep learning has been\nsuccessfully applied to a variety of other tasks. The high-precision detection\nand recognition of Kuzushiji, a Japanese cursive script used for transcribing\nhistorical documents, has been made possible through the use of deep learning.\nIn recent years, competitions on Kuzushiji recognition have been held, and many\nresearchers have proposed various recognition methods. This study examines\nrecent research trends, current problems, and future prospects in Kuzushiji\nrecognition using deep learning.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 09:46:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ueki", "Kazuya", ""], ["Kojima", "Tomoka", ""]]}, {"id": "2007.09654", "submitter": "Tong Wu", "authors": "Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, Dahua Lin", "title": "Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed\n  Datasets", "comments": "To appear in ECCV 2020 as a spotlight presentation. Code and models\n  are available at: https://github.com/wutong16/DistributionBalancedLoss", "journal-ref": "Proceedings Of The European Conference On Computer Vision (ECCV),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new loss function called Distribution-Balanced Loss for the\nmulti-label recognition problems that exhibit long-tailed class distributions.\nCompared to conventional single-label classification problem, multi-label\nrecognition problems are often more challenging due to two significant issues,\nnamely the co-occurrence of labels and the dominance of negative labels (when\ntreated as multiple binary classification problems). The Distribution-Balanced\nLoss tackles these issues through two key modifications to the standard binary\ncross-entropy loss: 1) a new way to re-balance the weights that takes into\naccount the impact caused by label co-occurrence, and 2) a negative tolerant\nregularization to mitigate the over-suppression of negative labels. Experiments\non both Pascal VOC and COCO show that the models trained with this new loss\nfunction achieve significant performance gains over existing methods. Code and\nmodels are available at: https://github.com/wutong16/DistributionBalancedLoss .\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 11:50:10 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 09:41:18 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 14:49:02 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wu", "Tong", ""], ["Huang", "Qingqiu", ""], ["Liu", "Ziwei", ""], ["Wang", "Yu", ""], ["Lin", "Dahua", ""]]}, {"id": "2007.09669", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang and Harini Veeraraghavan", "title": "Unified cross-modality feature disentangler for unsupervised\n  multi-domain MRI abdomen organs segmentation", "comments": "This paper has been accepted by MICCAI2020", "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our contribution is a unified cross-modality feature disentagling approach\nfor multi-domain image translation and multiple organ segmentation. Using CT as\nthe labeled source domain, our approach learns to segment multi-modal\n(T1-weighted and T2-weighted) MRI having no labeled data. Our approach uses a\nvariational auto-encoder (VAE) to disentangle the image content from style. The\nVAE constrains the style feature encoding to match a universal prior (Gaussian)\nthat is assumed to span the styles of all the source and target modalities. The\nextracted image style is converted into a latent style scaling code, which\nmodulates the generator to produce multi-modality images according to the\ntarget domain code from the image content features. Finally, we introduce a\njoint distribution matching discriminator that combines the translated images\nwith task-relevant segmentation probability maps to further constrain and\nregularize image-to-image (I2I) translations. We performed extensive\ncomparisons to multiple state-of-the-art I2I translation and segmentation\nmethods. Our approach resulted in the lowest average multi-domain image\nreconstruction error of 1.34$\\pm$0.04. Our approach produced an average Dice\nsimilarity coefficient (DSC) of 0.85 for T1w and 0.90 for T2w MRI for\nmulti-organ segmentation, which was highly comparable to a fully supervised MRI\nmulti-organ segmentation network (DSC of 0.86 for T1w and 0.90 for T2w MRI).\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 13:33:41 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jiang", "Jue", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "2007.09676", "submitter": "Wenxi Li", "authors": "Wenxi Li, Zhuoqun Cao, Qian Wang, Songjian Chen and Rui Feng", "title": "Learning Error-Driven Curriculum for Crowd Counting", "comments": "This paper is accepted by ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density regression has been widely employed in crowd counting. However, the\nfrequency imbalance of pixel values in the density map is still an obstacle to\nimprove the performance. In this paper, we propose a novel learning strategy\nfor learning error-driven curriculum, which uses an additional network to\nsupervise the training of the main network. A tutoring network called TutorNet\nis proposed to repetitively indicate the critical errors of the main network.\nTutorNet generates pixel-level weights to formulate the curriculum for the main\nnetwork during training, so that the main network will assign a higher weight\nto those hard examples than easy examples. Furthermore, we scale the density\nmap by a factor to enlarge the distance among inter-examples, which is well\nknown to improve the performance. Extensive experiments on two challenging\nbenchmark datasets show that our method has achieved state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 14:18:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Wenxi", ""], ["Cao", "Zhuoqun", ""], ["Wang", "Qian", ""], ["Chen", "Songjian", ""], ["Feng", "Rui", ""]]}, {"id": "2007.09690", "submitter": "Hanzhe Hu", "authors": "Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, Junjie Yan", "title": "Class-wise Dynamic Graph Convolution for Semantic Segmentation", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have made great progress in semantic segmentation by exploiting\ncontextual information in a local or global manner with dilated convolutions,\npyramid pooling or self-attention mechanism. In order to avoid potential\nmisleading contextual information aggregation in previous works, we propose a\nclass-wise dynamic graph convolution (CDGC) module to adaptively propagate\ninformation. The graph reasoning is performed among pixels in the same class.\nBased on the proposed CDGC module, we further introduce the Class-wise Dynamic\nGraph Convolution Network(CDGCNet), which consists of two main parts including\nthe CDGC module and a basic segmentation network, forming a coarse-to-fine\nparadigm. Specifically, the CDGC module takes the coarse segmentation result as\nclass mask to extract node features for graph construction and performs dynamic\ngraph convolutions on the constructed graph to learn the feature aggregation\nand weight allocation. Then the refined feature and the original feature are\nfused to get the final prediction. We conduct extensive experiments on three\npopular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012\nand COCO Stuff, and achieve state-of-the-art performance on all three\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 15:26:50 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hu", "Hanzhe", ""], ["Ji", "Deyi", ""], ["Gan", "Weihao", ""], ["Bai", "Shuai", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "2007.09695", "submitter": "Yi Zhong", "authors": "Yi Zhong", "title": "Using Deep Convolutional Neural Networks to Diagnose COVID-19 From Chest\n  X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 epidemic has become a major safety and health threat worldwide.\nImaging diagnosis is one of the most effective ways to screen COVID-19. This\nproject utilizes several open-source or public datasets to present an\nopen-source dataset of COVID-19 CXRs, named COVID-19-CXR-Dataset, and\nintroduces a deep convolutional neural network model. The model validates on\n740 test images and achieves 87.3% accuracy, 89.67 % precision, and 84.46%\nrecall, and correctly classifies 98 out of 100 COVID-19 x-ray images in test\nset with more than 81% prediction probability under the condition of 95%\nconfidence interval. This project may serve as a reference for other\nresearchers aiming to advance the development of deep learning applications in\nmedical imaging.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 15:47:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhong", "Yi", ""]]}, {"id": "2007.09699", "submitter": "Milan Pultar", "authors": "Milan Pultar", "title": "Improving the HardNet Descriptor", "comments": "The thesis was supervised by Dmytro Mishkin. Many pieces of advice\n  came from Ji\\v{r}\\'i Matas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the thesis we consider the problem of local feature descriptor learning\nfor wide baseline stereo focusing on the HardNet descriptor, which is close to\nstate-of-the-art. AMOS Patches dataset is introduced, which improves robustness\nto illumination and appearance changes. It is based on registered images from\nselected cameras from the AMOS dataset. We provide recommendations on the patch\ndataset creation process and evaluate HardNet trained on data of different\nmodalities. We also introduce a dataset combination and reduction methods, that\nallow comparable performance on a significantly smaller dataset.\n  HardNet8, consistently outperforming the original HardNet, benefits from the\narchitectural choices made: connectivity pattern, final pooling, receptive\nfield, CNN building blocks found by manual or automatic search algorithms --\nDARTS. We show impact of overlooked hyperparameters such as batch size and\nlength of training on the descriptor quality. PCA dimensionality reduction\nfurther boosts performance and also reduces memory footprint.\n  Finally, the insights gained lead to two HardNet8 descriptors: one performing\nwell on a variety of benchmarks -- HPatches, AMOS Patches and IMW Phototourism,\nthe other is optimized for IMW Phototourism.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 16:05:09 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 23:02:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Pultar", "Milan", ""]]}, {"id": "2007.09727", "submitter": "Jinming Duan", "authors": "Weizeng Lu, Xi Jia, Weicheng Xie, Linlin Shen, Yicong Zhou, Jinming\n  Duan", "title": "Geometry Constrained Weakly Supervised Object Localization", "comments": "This paper (ID 5424) is accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometry constrained network, termed GC-Net, for weakly\nsupervised object localization (WSOL). GC-Net consists of three modules: a\ndetector, a generator and a classifier. The detector predicts the object\nlocation defined by a set of coefficients describing a geometric shape (i.e.\nellipse or rectangle), which is geometrically constrained by the mask produced\nby the generator. The classifier takes the resulting masked images as input and\nperforms two complementary classification tasks for the object and background.\nTo make the mask more compact and more complete, we propose a novel multi-task\nloss function that takes into account area of the geometric shape, the\ncategorical cross-entropy and the negative entropy. In contrast to previous\napproaches, GC-Net is trained end-to-end and predict object location without\nany post-processing (e.g. thresholding) that may require additional tuning.\nExtensive experiments on the CUB-200-2011 and ILSVRC2012 datasets show that\nGC-Net outperforms state-of-the-art methods by a large margin. Our source code\nis available at https://github.com/lwzeng/GC-Net.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 17:33:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lu", "Weizeng", ""], ["Jia", "Xi", ""], ["Xie", "Weicheng", ""], ["Shen", "Linlin", ""], ["Zhou", "Yicong", ""], ["Duan", "Jinming", ""]]}, {"id": "2007.09746", "submitter": "Senthil Yogamani", "authors": "Gabriel L. Oliveira, Senthil Yogamani, Wolfram Burgard and Thomas Brox", "title": "Beyond Single Stage Encoder-Decoder Networks: Deep Decoders for Semantic\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single encoder-decoder methodologies for semantic segmentation are reaching\ntheir peak in terms of segmentation quality and efficiency per number of\nlayers. To address these limitations, we propose a new architecture based on a\ndecoder which uses a set of shallow networks for capturing more information\ncontent. The new decoder has a new topology of skip connections, namely\nbackward and stacked residual connections. In order to further improve the\narchitecture we introduce a weight function which aims to re-balance classes to\nincrease the attention of the networks to under-represented objects. We carried\nout an extensive set of experiments that yielded state-of-the-art results for\nthe CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the\neffectiveness of our decoder, we conducted a set of experiments studying the\nimpact of our decoder to state-of-the-art segmentation techniques.\nAdditionally, we present a set of experiments augmenting semantic segmentation\nwith optical flow information, showing that motion clues can boost pure image\nbased semantic segmentation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:44:34 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Oliveira", "Gabriel L.", ""], ["Yogamani", "Senthil", ""], ["Burgard", "Wolfram", ""], ["Brox", "Thomas", ""]]}, {"id": "2007.09748", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Xitong Yang, Abhinav Shrivastava, and Larry Davis", "title": "A Generic Visualization Approach for Convolutional Neural Networks", "comments": "ECCV'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval networks are essential for searching and indexing. Compared to\nclassification networks, attention visualization for retrieval networks is\nhardly studied. We formulate attention visualization as a constrained\noptimization problem. We leverage the unit L2-Norm constraint as an attention\nfilter (L2-CAF) to localize attention in both classification and retrieval\nnetworks. Unlike recent literature, our approach requires neither architectural\nchanges nor fine-tuning. Thus, a pre-trained network's performance is never\nundermined\n  L2-CAF is quantitatively evaluated using weakly supervised object\nlocalization. State-of-the-art results are achieved on classification networks.\nFor retrieval networks, significant improvement margins are achieved over a\nGrad-CAM baseline. Qualitative evaluation demonstrates how the L2-CAF\nvisualizes attention per frame for a recurrent retrieval network. Further\nablation studies highlight the computational cost of our approach and compare\nL2-CAF with other feasible alternatives. Code available at\nhttps://bit.ly/3iDBLFv\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:46:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Taha", "Ahmed", ""], ["Yang", "Xitong", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry", ""]]}, {"id": "2007.09758", "submitter": "Alireza Parchami", "authors": "Alireza Parchami, Mojtaba Mahdavi", "title": "Full Quaternion Representation of Color images: A Case Study on\n  QSVD-based Color Image Compression", "comments": "15 pages, 16 figures, 1 table, submitted to Signal Processing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, channels of a color image have been processed individually,\nor the image has been converted to grayscale one with respect to color image\nprocessing. Pure quaternion representation of color images solves this issue as\nit allows images to be processed in a holistic space. Nevertheless, it brings\nadditional costs due to the extra fourth dimension. In this paper, we propose\nan approach for representing color images with full quaternion numbers that\nenables us to process color images holistically without additional cost in\ntime, space and computation. With taking auto- and cross-correlation of color\nchannels into account, an autoencoder neural network is used to generate a\nglobal model for transforming a color image into a full quaternion matrix. To\nevaluate the model, we use UCID dataset, and the results indicate that the\nmodel has an acceptable performance on color images. Moreover, we propose a\ncompression method based on the generated model and QSVD as a case study. The\nmethod is compared with the same compression method using pure quaternion\nrepresentation and is assessed with UCID dataset. The results demonstrate that\nthe compression method using the proposed full quaternion representation fares\nbetter than the other in terms of time, quality, and size of compressed files.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 19:13:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Parchami", "Alireza", ""], ["Mahdavi", "Mojtaba", ""]]}, {"id": "2007.09763", "submitter": "Shasha Li", "authors": "Shasha Li, Shitong Zhu, Sudipta Paul, Amit Roy-Chowdhury, Chengyu\n  Song, Srikanth Krishnamurthy, Ananthram Swami, Kevin S Chan", "title": "Connecting the Dots: Detecting Adversarial Perturbations Using Context\n  Inconsistency", "comments": "The paper is accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent surge in research on adversarial perturbations that\ndefeat Deep Neural Networks (DNNs) in machine vision; most of these\nperturbation-based attacks target object classifiers. Inspired by the\nobservation that humans are able to recognize objects that appear out of place\nin a scene or along with other unlikely objects, we augment the DNN with a\nsystem that learns context consistency rules during training and checks for the\nviolations of the same during testing. Our approach builds a set of\nauto-encoders, one for each object class, appropriately trained so as to output\na discrepancy between the input and output if an added adversarial perturbation\nviolates context consistency rules. Experiments on PASCAL VOC and MS COCO show\nthat our method effectively detects various adversarial attacks and achieves\nhigh ROC-AUC (over 0.95 in most cases); this corresponds to over 20%\nimprovement over a state-of-the-art context-agnostic method.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 19:46:45 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 17:02:41 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Li", "Shasha", ""], ["Zhu", "Shitong", ""], ["Paul", "Sudipta", ""], ["Roy-Chowdhury", "Amit", ""], ["Song", "Chengyu", ""], ["Krishnamurthy", "Srikanth", ""], ["Swami", "Ananthram", ""], ["Chan", "Kevin S", ""]]}, {"id": "2007.09766", "submitter": "Ricardo Sanchez-Matilla", "authors": "Ricardo Sanchez-Matilla, Chau Yi Li, Ali Shahin Shamsabadi, Riccardo\n  Mazzon, Andrea Cavallaro", "title": "Exploiting vulnerabilities of deep neural networks for privacy\n  protection", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 2020", "doi": "10.1109/TMM.2020.2987694", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial perturbations can be added to images to protect their content\nfrom unwanted inferences. These perturbations may, however, be ineffective\nagainst classifiers that were not {seen} during the generation of the\nperturbation, or against defenses {based on re-quantization, median filtering\nor JPEG compression. To address these limitations, we present an adversarial\nattack {that is} specifically designed to protect visual content against {\nunseen} classifiers and known defenses. We craft perturbations using an\niterative process that is based on the Fast Gradient Signed Method and {that}\nrandomly selects a classifier and a defense, at each iteration}. This\nrandomization prevents an undesirable overfitting to a specific classifier or\ndefense. We validate the proposed attack in both targeted and untargeted\nsettings on the private classes of the Places365-Standard dataset. Using\nResNet18, ResNet50, AlexNet and DenseNet161 {as classifiers}, the performance\nof the proposed attack exceeds that of eleven state-of-the-art attacks. The\nimplementation is available at https://github.com/smartcameras/RP-FGSM/.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 20:03:42 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Sanchez-Matilla", "Ricardo", ""], ["Li", "Chau Yi", ""], ["Shamsabadi", "Ali Shahin", ""], ["Mazzon", "Riccardo", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2007.09777", "submitter": "Wen Zhang", "authors": "Wen Zhang, Liang Zhan, Paul Thompson, Yalin Wang", "title": "Deep Representation Learning For Multimodal Brain Networks", "comments": "11 pages, 3 figures, MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying network science approaches to investigate the functions and anatomy\nof the human brain is prevalent in modern medical imaging analysis. Due to the\ncomplex network topology, for an individual brain, mining a discriminative\nnetwork representation from the multimodal brain networks is non-trivial. The\nrecent success of deep learning techniques on graph-structured data suggests a\nnew way to model the non-linear cross-modality relationship. However, current\ndeep brain network methods either ignore the intrinsic graph topology or\nrequire a network basis shared within a group. To address these challenges, we\npropose a novel end-to-end deep graph representation learning (Deep Multimodal\nBrain Networks - DMBN) to fuse multimodal brain networks. Specifically, we\ndecipher the cross-modality relationship through a graph encoding and decoding\nprocess. The higher-order network mappings from brain structural networks to\nfunctional networks are learned in the node domain. The learned network\nrepresentation is a set of node features that are informative to induce brain\nsaliency maps in a supervised manner. We test our framework in both synthetic\nand real image data. The experimental results show the superiority of the\nproposed method over some other state-of-the-art deep brain network models.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 20:32:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Wen", ""], ["Zhan", "Liang", ""], ["Thompson", "Paul", ""], ["Wang", "Yalin", ""]]}, {"id": "2007.09785", "submitter": "Vasu Singla", "authors": "Rohun Tripathi, Vasu Singla, Mahyar Najibi, Bharat Singh, Abhishek\n  Sharma and Larry Davis", "title": "ASAP-NMS: Accelerating Non-Maximum Suppression Using Spatially Aware\n  Priors", "comments": "Under Review at CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely adopted sequential variant of Non Maximum Suppression (or\nGreedy-NMS) is a crucial module for object-detection pipelines. Unfortunately,\nfor the region proposal stage of two/multi-stage detectors, NMS is turning out\nto be a latency bottleneck due to its sequential nature. In this article, we\ncarefully profile Greedy-NMS iterations to find that a major chunk of\ncomputation is wasted in comparing proposals that are already far-away and have\na small chance of suppressing each other. We address this issue by comparing\nonly those proposals that are generated from nearby anchors. The\ntranslation-invariant property of the anchor lattice affords generation of a\nlookup table, which provides an efficient access to nearby proposals, during\nNMS. This leads to an Accelerated NMS algorithm which leverages Spatially Aware\nPriors, or ASAP-NMS, and improves the latency of the NMS step from 13.6ms to\n1.2 ms on a CPU without sacrificing the accuracy of a state-of-the-art\ntwo-stage detector on COCO and VOC datasets. Importantly, ASAP-NMS is agnostic\nto image resolution and can be used as a simple drop-in module during\ninference. Using ASAP-NMS at run-time only, we obtain an mAP of 44.2\\%@25Hz on\nthe COCO dataset with a V100 GPU.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 21:15:48 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 15:18:57 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Tripathi", "Rohun", ""], ["Singla", "Vasu", ""], ["Najibi", "Mahyar", ""], ["Singh", "Bharat", ""], ["Sharma", "Abhishek", ""], ["Davis", "Larry", ""]]}, {"id": "2007.09790", "submitter": "Ariel Ruiz-Garcia", "authors": "Ariel Ruiz-Garcia, Vasile Palade, Mark Elshaw, Mariette Awad", "title": "Generative Adversarial Stacked Autoencoders for Facial Pose\n  Normalization and Emotion Recognition", "comments": "Accepted at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we propose a novel Generative Adversarial Stacked Autoencoder\nthat learns to map facial expressions, with up to plus or minus 60 degrees, to\nan illumination invariant facial representation of 0 degrees. We accomplish\nthis by using a novel convolutional layer that exploits both local and global\nspatial information, and a convolutional layer with a reduced number of\nparameters that exploits facial symmetry. Furthermore, we introduce a\ngenerative adversarial gradual greedy layer-wise learning algorithm designed to\ntrain Adversarial Autoencoders in an efficient and incremental manner. We\ndemonstrate the efficiency of our method and report state-of-the-art\nperformance on several facial emotion recognition corpora, including one\ncollected in the wild.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 21:47:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ruiz-Garcia", "Ariel", ""], ["Palade", "Vasile", ""], ["Elshaw", "Mark", ""], ["Awad", "Mariette", ""]]}, {"id": "2007.09791", "submitter": "Youbao Tang", "authors": "Youbao Tang, Yuxing Tang, Yingying Zhu, Jing Xiao and Ronald M.\n  Summers", "title": "E$^2$Net: An Edge Enhanced Network for Accurate Liver and Tumor\n  Segmentation on CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing an effective liver and liver tumor segmentation model from CT\nscans is very important for the success of liver cancer diagnosis, surgical\nplanning and cancer treatment. In this work, we propose a two-stage framework\nfor 2D liver and tumor segmentation. The first stage is a coarse liver\nsegmentation network, while the second stage is an edge enhanced network\n(E$^2$Net) for more accurate liver and tumor segmentation. E$^2$Net explicitly\nmodels complementary objects (liver and tumor) and their edge information\nwithin the network to preserve the organ and lesion boundaries. We introduce an\nedge prediction module in E$^2$Net and design an edge distance map between\nliver and tumor boundaries, which is used as an extra supervision signal to\ntrain the edge enhanced network. We also propose a deep cross feature fusion\nmodule to refine multi-scale features from both objects and their edges.\nE$^2$Net is more easily and efficiently trained with a small labeled dataset,\nand it can be trained/tested on the original 2D CT slices (resolve resampling\nerror issue in 3D models). The proposed framework has shown superior\nperformance on both liver and liver tumor segmentation compared to several\nstate-of-the-art 2D, 3D and 2D/3D hybrid frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 21:50:22 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tang", "Youbao", ""], ["Tang", "Yuxing", ""], ["Zhu", "Yingying", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2007.09805", "submitter": "Rolandos Alexandros Potamias", "authors": "Rolandos Alexandros Potamias, Jiali Zheng, Stylianos Ploumpis, Giorgos\n  Bouritsas, Evangelos Ververas, Stefanos Zafeiriou", "title": "Learning to Generate Customized Dynamic 3D Facial Expressions", "comments": "accepted at European Conference on Computer Vision 2020 (ECCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have significantly pushed the\nstate-of-the-art in photorealistic video animation given a single image. In\nthis paper, we extrapolate those advances to the 3D domain, by studying 3D\nimage-to-video translation with a particular focus on 4D facial expressions.\nAlthough 3D facial generative models have been widely explored during the past\nyears, 4D animation remains relatively unexplored. To this end, in this study\nwe employ a deep mesh encoder-decoder like architecture to synthesize realistic\nhigh resolution facial expressions by using a single neutral frame along with\nan expression identification. In addition, processing 3D meshes remains a\nnon-trivial task compared to data that live on grid-like structures, such as\nimages. Given the recent progress in mesh processing with graph convolutions,\nwe make use of a recently introduced learnable operator which acts directly on\nthe mesh structure by taking advantage of local vertex orderings. In order to\ngeneralize to 4D facial expressions across subjects, we trained our model using\na high resolution dataset with 4D scans of six facial expressions from 180\nsubjects. Experimental results demonstrate that our approach preserves the\nsubject's identity information even for unseen subjects and generates high\nquality expressions. To the best of our knowledge, this is the first study\ntackling the problem of 4D facial expression synthesis.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 22:38:43 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 16:18:15 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Potamias", "Rolandos Alexandros", ""], ["Zheng", "Jiali", ""], ["Ploumpis", "Stylianos", ""], ["Bouritsas", "Giorgos", ""], ["Ververas", "Evangelos", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2007.09818", "submitter": "Hassan Dbouk", "authors": "Hassan Dbouk, Hetul Sanghvi, Mahesh Mehendale, Naresh Shanbhag", "title": "DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural\n  Networks", "comments": "Published as a conference paper in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved state-of-the art performance on various\ncomputer vision tasks. However, their deployment on resource-constrained\ndevices has been hindered due to their high computational and storage\ncomplexity. While various complexity reduction techniques, such as lightweight\nnetwork architecture design and parameter quantization, have been successful in\nreducing the cost of implementing these networks, these methods have often been\nconsidered orthogonal. In reality, existing quantization techniques fail to\nreplicate their success on lightweight architectures such as MobileNet. To this\nend, we present a novel fully differentiable non-uniform quantizer that can be\nseamlessly mapped onto efficient ternary-based dot product engines. We conduct\ncomprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words\ndatasets. The proposed quantizer (DBQ) successfully tackles the daunting task\nof aggressively quantizing lightweight networks such as MobileNetV1,\nMobileNetV2, and ShuffleNetV2. DBQ achieves state-of-the art results with\nminimal training overhead and provides the best (pareto-optimal)\naccuracy-complexity trade-off.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:50:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dbouk", "Hassan", ""], ["Sanghvi", "Hetul", ""], ["Mehendale", "Mahesh", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "2007.09824", "submitter": "Nibaran Das", "authors": "Hmrishav Bandyopadhyay, Tanmoy Dasgupta, Nibaran Das, Mita Nasipuri", "title": "A Gated and Bifurcated Stacked U-Net Module for Document Image Dewarping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing images of documents is one of the easiest and most used methods of\nrecording them. These images however, being captured with the help of handheld\ndevices, often lead to undesirable distortions that are hard to remove. We\npropose a supervised Gated and Bifurcated Stacked U-Net module to predict a\ndewarping grid and create a distortion free image from the input. While the\nnetwork is trained on synthetically warped document images, results are\ncalculated on the basis of real world images. The novelty in our methods exists\nnot only in a bifurcation of the U-Net to help eliminate the intermingling of\nthe grid coordinates, but also in the use of a gated network which adds\nboundary and other minute line level details to the model. The end-to-end\npipeline proposed by us achieves state-of-the-art performance on the DocUNet\ndataset after being trained on just 8 percent of the data used in previous\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 01:22:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bandyopadhyay", "Hmrishav", ""], ["Dasgupta", "Tanmoy", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2007.09833", "submitter": "Fa-Ting Hong", "authors": "Fa-Ting Hong, Xuanteng Huang, Wei-Hong Li, and Wei-Shi Zheng", "title": "MINI-Net: Multiple Instance Ranking Network for Video Highlight\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the weakly supervised video highlight detection problem for\nlearning to detect segments that are more attractive in training videos given\ntheir video event label but without expensive supervision of manually\nannotating highlight segments. While manually averting localizing highlight\nsegments, weakly supervised modeling is challenging, as a video in our daily\nlife could contain highlight segments with multiple event types, e.g., skiing\nand surfing. In this work, we propose casting weakly supervised video highlight\ndetection modeling for a given specific event as a multiple instance ranking\nnetwork (MINI-Net) learning. We consider each video as a bag of segments, and\ntherefore, the proposed MINI-Net learns to enforce a higher highlight score for\na positive bag that contains highlight segments of a specific event than those\nfor negative bags that are irrelevant. In particular, we form a max-max ranking\nloss to acquire a reliable relative comparison between the most likely positive\nsegment instance and the hardest negative segment instance. With this max-max\nranking loss, our MINI-Net effectively leverages all segment information to\nacquire a more distinct video feature representation for localizing the\nhighlight segments of a specific event in a video. The extensive experimental\nresults on three challenging public benchmarks clearly validate the efficacy of\nour multiple instance ranking approach for solving the problem.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 01:56:32 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 05:42:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Hong", "Fa-Ting", ""], ["Huang", "Xuanteng", ""], ["Li", "Wei-Hong", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2007.09835", "submitter": "Mengshu Sun", "authors": "Wei Niu, Mengshu Sun, Zhengang Li, Jou-An Chen, Jiexiong Guan, Xipeng\n  Shen, Yanzhi Wang, Sijia Liu, Xue Lin, Bin Ren", "title": "RT3D: Achieving Real-Time Execution of 3D Convolutional Neural Networks\n  on Mobile Devices", "comments": "To appear in Proceedings of the 35th AAAI Conference on Artificial\n  Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices are becoming an important carrier for deep learning tasks, as\nthey are being equipped with powerful, high-end mobile CPUs and GPUs. However,\nit is still a challenging task to execute 3D Convolutional Neural Networks\n(CNNs) targeting for real-time performance, besides high inference accuracy.\nThe reason is more complex model structure and higher model dimensionality\noverwhelm the available computation/storage resources on mobile devices. A\nnatural way may be turning to deep learning weight pruning techniques. However,\nthe direct generalization of existing 2D CNN weight pruning methods to 3D CNNs\nis not ideal for fully exploiting mobile parallelism while achieving high\ninference accuracy.\n  This paper proposes RT3D, a model compression and mobile acceleration\nframework for 3D CNNs, seamlessly integrating neural network weight pruning and\ncompiler code generation techniques. We propose and investigate two structured\nsparsity schemes i.e., the vanilla structured sparsity and kernel group\nstructured (KGS) sparsity that are mobile acceleration friendly. The vanilla\nsparsity removes whole kernel groups, while KGS sparsity is a more fine-grained\nstructured sparsity that enjoys higher flexibility while exploiting full\non-device parallelism. We propose a reweighted regularization pruning algorithm\nto achieve the proposed sparsity schemes. The inference time speedup due to\nsparsity is approaching the pruning rate of the whole model FLOPs (floating\npoint operations). RT3D demonstrates up to 29.1$\\times$ speedup in end-to-end\ninference time comparing with current mobile frameworks supporting 3D CNNs,\nwith moderate 1%-1.5% accuracy loss. The end-to-end inference time for 16 video\nframes could be within 150 ms, when executing representative C3D and R(2+1)D\nmodels on a cellphone. For the first time, real-time execution of 3D CNNs is\nachieved on off-the-shelf mobiles.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:05:32 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 18:03:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Niu", "Wei", ""], ["Sun", "Mengshu", ""], ["Li", "Zhengang", ""], ["Chen", "Jou-An", ""], ["Guan", "Jiexiong", ""], ["Shen", "Xipeng", ""], ["Wang", "Yanzhi", ""], ["Liu", "Sijia", ""], ["Lin", "Xue", ""], ["Ren", "Bin", ""]]}, {"id": "2007.09836", "submitter": "Wentao Bao", "authors": "Wentao Bao and Qi Yu and Yu Kong", "title": "Object-Aware Centroid Voting for Monocular 3D Object Detection", "comments": "IROS 2020 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection aims to detect objects in a 3D physical world\nfrom a single camera. However, recent approaches either rely on expensive LiDAR\ndevices, or resort to dense pixel-wise depth estimation that causes prohibitive\ncomputational cost. In this paper, we propose an end-to-end trainable monocular\n3D object detector without learning the dense depth. Specifically, the grid\ncoordinates of a 2D box are first projected back to 3D space with the pinhole\nmodel as 3D centroids proposals. Then, a novel object-aware voting approach is\nintroduced, which considers both the region-wise appearance attention and the\ngeometric projection distribution, to vote the 3D centroid proposals for 3D\nobject localization. With the late fusion and the predicted 3D orientation and\ndimension, the 3D bounding boxes of objects can be detected from a single RGB\nimage. The method is straightforward yet significantly superior to other\nmonocular-based methods. Extensive experimental results on the challenging\nKITTI benchmark validate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:11:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bao", "Wentao", ""], ["Yu", "Qi", ""], ["Kong", "Yu", ""]]}, {"id": "2007.09841", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Erik Wijmans, Xinlei Chen, Trevor Darrell, Dhruv\n  Batra, Devi Parikh, Amanpreet Singh", "title": "Seeing the Un-Scene: Learning Amodal Semantic Maps for Room Navigation", "comments": "Published at the European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning-based approach for room navigation using semantic\nmaps. Our proposed architecture learns to predict top-down belief maps of\nregions that lie beyond the agent's field of view while modeling architectural\nand stylistic regularities in houses. First, we train a model to generate\namodal semantic top-down maps indicating beliefs of location, size, and shape\nof rooms by learning the underlying architectural patterns in houses. Next, we\nuse these maps to predict a point that lies in the target room and train a\npolicy to navigate to the point. We empirically demonstrate that by predicting\nsemantic maps, the model learns common correlations found in houses and\ngeneralizes to novel environments. We also demonstrate that reducing the task\nof room navigation to point navigation improves the performance further.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:19:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Wijmans", "Erik", ""], ["Chen", "Xinlei", ""], ["Darrell", "Trevor", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Singh", "Amanpreet", ""]]}, {"id": "2007.09854", "submitter": "Yuexiang Li", "authors": "Yuexiang Li, Jiawei Chen, Xinpeng Xie, Kai Ma, Yefeng Zheng", "title": "Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical\n  Image Segmentation", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Witnessing the success of deep learning neural networks in natural image\nprocessing, an increasing number of studies have been proposed to develop\ndeep-learning-based frameworks for medical image segmentation. However, since\nthe pixel-wise annotation of medical images is laborious and expensive, the\namount of annotated data is usually deficient to well-train a neural network.\nIn this paper, we propose a semi-supervised approach to train neural networks\nwith limited labeled data and a large quantity of unlabeled images for medical\nimage segmentation. A novel pseudo-label (namely self-loop uncertainty),\ngenerated by recurrently optimizing the neural network with a self-supervised\ntask, is adopted as the ground-truth for the unlabeled images to augment the\ntraining set and boost the segmentation accuracy. The proposed self-loop\nuncertainty can be seen as an approximation of the uncertainty estimation\nyielded by ensembling multiple models with a significant reduction of inference\ntime. Experimental results on two publicly available datasets demonstrate the\neffectiveness of our semi-supervied approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:52:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Yuexiang", ""], ["Chen", "Jiawei", ""], ["Xie", "Xinpeng", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.09858", "submitter": "Hao Ding", "authors": "Hao Ding, Songsong Wu, Hao Tang, Fei Wu, Guangwei Gao and Xiao-Yuan\n  Jing", "title": "Cross-View Image Synthesis with Deformable Convolution and Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to generate natural scenes has always been a daunting task in\ncomputer vision. This is even more laborious when generating images with very\ndifferent views. When the views are very different, the view fields have little\noverlap or objects are occluded, leading the task very challenging. In this\npaper, we propose to use Generative Adversarial Networks(GANs) based on a\ndeformable convolution and attention mechanism to solve the problem of\ncross-view image synthesis (see Fig.1). It is difficult to understand and\ntransform scenes appearance and semantic information from another view, thus we\nuse deformed convolution in the U-net network to improve the network's ability\nto extract features of objects at different scales. Moreover, to better learn\nthe correspondence between images from different views, we apply an attention\nmechanism to refine the intermediate feature map thus generating more realistic\nimages. A large number of experiments on different size images on the Dayton\ndataset[1] show that our model can produce better results than state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:08:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ding", "Hao", ""], ["Wu", "Songsong", ""], ["Tang", "Hao", ""], ["Wu", "Fei", ""], ["Gao", "Guangwei", ""], ["Jing", "Xiao-Yuan", ""]]}, {"id": "2007.09859", "submitter": "Matheesha Fernando Miss", "authors": "Matheesha Fernando and Janaka Wijayanayake", "title": "Novel Approach to Use HU Moments with Image Processing Techniques for\n  Real Time Sign Language Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language is the fundamental communication method among people who suffer\nfrom speech and hearing defects. The rest of the world doesn't have a clear\nidea of sign language. \"Sign Language Communicator\" (SLC) is designed to solve\nthe language barrier between the sign language users and the rest of the world.\nThe main objective of this research is to provide a low cost affordable method\nof sign language interpretation. This system will also be very useful to the\nsign language learners as they can practice the sign language. During the\nresearch available human computer interaction techniques in posture recognition\nwas tested and evaluated. A series of image processing techniques with\nHu-moment classification was identified as the best approach. To improve the\naccuracy of the system, a new approach height to width ratio filtration was\nimplemented along with Hu-moments. System is able to recognize selected Sign\nLanguage signs with the accuracy of 84% without a controlled background with\nsmall light adjustments\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:10:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fernando", "Matheesha", ""], ["Wijayanayake", "Janaka", ""]]}, {"id": "2007.09860", "submitter": "Hwann-Tzong Chen", "authors": "Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, Tyng-Luh\n  Liu", "title": "Learning Gaussian Instance Segmentation in Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for instance segmentation of 3D point\nclouds. The proposed method is called Gaussian Instance Center Network (GICN),\nwhich can approximate the distributions of instance centers scattered in the\nwhole scene as Gaussian center heatmaps. Based on the predicted heatmaps, a\nsmall number of center candidates can be easily selected for the subsequent\npredictions with efficiency, including i) predicting the instance size of each\ncenter to decide a range for extracting features, ii) generating bounding boxes\nfor centers, and iii) producing the final instance masks. GICN is a\nsingle-stage, anchor-free, and end-to-end architecture that is easy to train\nand efficient to perform inference. Benefited from the center-dictated\nmechanism with adaptive instance size selection, our method achieves\nstate-of-the-art performance in the task of 3D instance segmentation on ScanNet\nand S3DIS datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:11:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Shih-Hung", ""], ["Yu", "Shang-Yi", ""], ["Wu", "Shao-Chi", ""], ["Chen", "Hwann-Tzong", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "2007.09861", "submitter": "Limin Wang", "authors": "Jianchao Wu, Zhanghui Kuang, Limin Wang, Wayne Zhang, Gangshan Wu", "title": "Context-Aware RCNN: A Baseline for Action Detection in Videos", "comments": "ECCV 2020 Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action detection approaches usually conduct actor-centric action\nrecognition over RoI-pooled features following the standard pipeline of\nFaster-RCNN. In this work, we first empirically find the recognition accuracy\nis highly correlated with the bounding box size of an actor, and thus higher\nresolution of actors contributes to better performance. However, video models\nrequire dense sampling in time to achieve accurate recognition. To fit in GPU\nmemory, the frames to backbone network must be kept low-resolution, resulting\nin a coarse feature map in RoI-Pooling layer. Thus, we revisit RCNN for\nactor-centric action recognition via cropping and resizing image patches around\nactors before feature extraction with I3D deep network. Moreover, we found that\nexpanding actor bounding boxes slightly and fusing the context features can\nfurther boost the performance. Consequently, we develop a surpringly effective\nbaseline (Context-Aware RCNN) and it achieves new state-of-the-art results on\ntwo challenging action detection benchmarks of AVA and JHMDB. Our observations\nchallenge the conventional wisdom of RoI-Pooling based pipeline and encourage\nresearchers rethink the importance of resolution in actor-centric action\nrecognition. Our approach can serve as a strong baseline for video action\ndetection and is expected to inspire new ideas for this filed. The code is\navailable at \\url{https://github.com/MCG-NJU/CRCNN-Action}.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:11:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wu", "Jianchao", ""], ["Kuang", "Zhanghui", ""], ["Wang", "Limin", ""], ["Zhang", "Wayne", ""], ["Wu", "Gangshan", ""]]}, {"id": "2007.09867", "submitter": "Boren Li", "authors": "Boren Li, Po-Yu Zhuang, Jian Gu, Mingyang Li, Ping Tan", "title": "Interpretable Foreground Object Search As Knowledge Distillation", "comments": "This paper will appear at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a knowledge distillation method for foreground object\nsearch (FoS). Given a background and a rectangle specifying the foreground\nlocation and scale, FoS retrieves compatible foregrounds in a certain category\nfor later image composition. Foregrounds within the same category can be\ngrouped into a small number of patterns. Instances within each pattern are\ncompatible with any query input interchangeably. These instances are referred\nto as interchangeable foregrounds. We first present a pipeline to build\npattern-level FoS dataset containing labels of interchangeable foregrounds. We\nthen establish a benchmark dataset for further training and testing following\nthe pipeline. As for the proposed method, we first train a foreground encoder\nto learn representations of interchangeable foregrounds. We then train a query\nencoder to learn query-foreground compatibility following a knowledge\ndistillation framework. It aims to transfer knowledge from interchangeable\nforegrounds to supervise representation learning of compatibility. The query\nfeature representation is projected to the same latent space as interchangeable\nforegrounds, enabling very efficient and interpretable instance-level search.\nFurthermore, pattern-level search is feasible to retrieve more controllable,\nreasonable and diverse foregrounds. The proposed method outperforms the\nprevious state-of-the-art by 10.42% in absolute difference and 24.06% in\nrelative improvement evaluated by mean average precision (mAP). Extensive\nexperimental results also demonstrate its efficacy from various aspects. The\nbenchmark dataset and code will be release shortly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:37:15 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 03:33:16 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Li", "Boren", ""], ["Zhuang", "Po-Yu", ""], ["Gu", "Jian", ""], ["Li", "Mingyang", ""], ["Tan", "Ping", ""]]}, {"id": "2007.09877", "submitter": "MingFei Wang", "authors": "Yuan Zhou, Mingfei Wang, Ruolin Wang, Shuwei Huo", "title": "Graph Neural Network for Video-Query based Video Moment Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on Video Query based Video Moment Retrieval (VQ-VMR)\ntask, which uses a query video clip as input to retrieve a semantic relative\nvideo clip in another untrimmed long video. we find that in VQ-VMR datasets,\nthere exists a phenomenon showing that there does not exist consistent\nrelationship between feature similarity by frame and feature similarity by\nvideo, which affects the feature fusion among frames. However, existing VQ-VMR\nmethods do not fully consider it. Taking this phenomenon into account, in this\narticle, we treat video features as a graph by concatenating the query video\nfeature and proposal video feature along time dimension, where each timestep is\ntreated as a node, each row of the feature matrix is treated as feature of each\nnode. Then, with the power of graph neural networks, we propose a Multi-Graph\nFeature Fusion Module to fuse the relation feature of this graph. After\nevaluating our method on ActivityNet v1.2 dataset and Thumos14 dataset, we find\nthat our proposed method outperforms the state of art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:01:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhou", "Yuan", ""], ["Wang", "Mingfei", ""], ["Wang", "Ruolin", ""], ["Huo", "Shuwei", ""]]}, {"id": "2007.09883", "submitter": "Haisheng Su", "authors": "Haisheng Su, Jinyuan Feng, Hao Shao, Zhenyu Jiang, Manyuan Zhang, Wei\n  Wu, Yu Liu, Hongsheng Li, Junjie Yan", "title": "Complementary Boundary Generator with Scale-Invariant Relation Modeling\n  for Temporal Action Localization: Submission to ActivityNet Challenge 2020", "comments": "Submitted to CVPR workshop of ActivityNet Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents an overview of our solution used in the\nsubmission to ActivityNet Challenge 2020 Task 1 (\\textbf{temporal action\nlocalization/detection}). Temporal action localization requires to not only\nprecisely locate the temporal boundaries of action instances, but also\naccurately classify the untrimmed videos into specific categories. In this\npaper, we decouple the temporal action localization task into two stages (i.e.\nproposal generation and classification) and enrich the proposal diversity\nthrough exhaustively exploring the influences of multiple components from\ndifferent but complementary perspectives. Specifically, in order to generate\nhigh-quality proposals, we consider several factors including the video feature\nencoder, the proposal generator, the proposal-proposal relations, the scale\nimbalance, and ensemble strategy. Finally, in order to obtain accurate\ndetections, we need to further train an optimal video classifier to recognize\nthe generated proposals. Our proposed scheme achieves the state-of-the-art\nperformance on the temporal action localization task with \\textbf{42.26}\naverage mAP on the challenge testing set.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:35:40 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 01:51:02 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Su", "Haisheng", ""], ["Feng", "Jinyuan", ""], ["Shao", "Hao", ""], ["Jiang", "Zhenyu", ""], ["Zhang", "Manyuan", ""], ["Wu", "Wei", ""], ["Liu", "Yu", ""], ["Li", "Hongsheng", ""], ["Yan", "Junjie", ""]]}, {"id": "2007.09886", "submitter": "Cheng Ouyang", "authors": "Cheng Ouyang, Carlo Biffi, Chen Chen, Turkay Kart, Huaqi Qiu, Daniel\n  Rueckert", "title": "Self-Supervision with Superpixels: Training Few-shot Medical Image\n  Segmentation without Annotation", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot semantic segmentation (FSS) has great potential for medical imaging\napplications. Most of the existing FSS techniques require abundant annotated\nsemantic classes for training. However, these methods may not be applicable for\nmedical images due to the lack of annotations. To address this problem we make\nseveral contributions: (1) A novel self-supervised FSS framework for medical\nimages in order to eliminate the requirement for annotations during training.\nAdditionally, superpixel-based pseudo-labels are generated to provide\nsupervision; (2) An adaptive local prototype pooling module plugged into\nprototypical networks, to solve the common challenging foreground-background\nimbalance problem in medical image segmentation; (3) We demonstrate the general\napplicability of the proposed approach for medical images using three different\ntasks: abdominal organ segmentation for CT and MRI, as well as cardiac\nsegmentation for MRI. Our results show that, for medical image segmentation,\nthe proposed method outperforms conventional FSS methods which require manual\nannotations for training.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:46:33 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 21:36:05 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ouyang", "Cheng", ""], ["Biffi", "Carlo", ""], ["Chen", "Chen", ""], ["Kart", "Turkay", ""], ["Qiu", "Huaqi", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2007.09892", "submitter": "Sai Bi", "authors": "Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\\v{s} Ha\\v{s}an, Yannick\n  Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi", "title": "Deep Reflectance Volumes: Relightable Reconstructions from Multi-View\n  Photometric Images", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning approach to reconstruct scene appearance from\nunstructured images captured under collocated point lighting. At the heart of\nDeep Reflectance Volumes is a novel volumetric scene representation consisting\nof opacity, surface normal and reflectance voxel grids. We present a novel\nphysically-based differentiable volume ray marching framework to render these\nscene volumes under arbitrary viewpoint and lighting. This allows us to\noptimize the scene volumes to minimize the error between their rendered images\nand the captured images. Our method is able to reconstruct real scenes with\nchallenging non-Lambertian reflectance and complex geometry with occlusions and\nshadowing. Moreover, it accurately generalizes to novel viewpoints and\nlighting, including non-collocated lighting, rendering photorealistic images\nthat are significantly better than state-of-the-art mesh-based methods. We also\nshow that our learned reflectance volumes are editable, allowing for modifying\nthe materials of the captured scenes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:38:11 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Sunkavalli", "Kalyan", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Hold-Geoffroy", "Yannick", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2007.09898", "submitter": "Wu Tz-Ying", "authors": "Tz-Ying Wu, Pedro Morgado, Pei Wang, Chih-Hui Ho, and Nuno Vasconcelos", "title": "Solving Long-tailed Recognition with Deep Realistic Taxonomic Classifier", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-tail recognition tackles the natural non-uniformly distributed data in\nreal-world scenarios. While modern classifiers perform well on populated\nclasses, its performance degrades significantly on tail classes. Humans,\nhowever, are less affected by this since, when confronted with uncertain\nexamples, they simply opt to provide coarser predictions. Motivated by this, a\ndeep realistic taxonomic classifier (Deep-RTC) is proposed as a new solution to\nthe long-tail problem, combining realism with hierarchical predictions. The\nmodel has the option to reject classifying samples at different levels of the\ntaxonomy, once it cannot guarantee the desired performance. Deep-RTC is\nimplemented with a stochastic tree sampling during training to simulate all\npossible classification conditions at finer or coarser levels and a rejection\nmechanism at inference time. Experiments on the long-tailed version of four\ndatasets, CIFAR100, AWA2, Imagenet, and iNaturalist, demonstrate that the\nproposed approach preserves more information on all classes with different\npopularity levels. Deep-RTC also outperforms the state-of-the-art methods in\nlongtailed recognition, hierarchical classification, and learning with\nrejection literature using the proposed correctly predicted bits (CPB) metric.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:57:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wu", "Tz-Ying", ""], ["Morgado", "Pedro", ""], ["Wang", "Pei", ""], ["Ho", "Chih-Hui", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2007.09902", "submitter": "Hang Zhou", "authors": "Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, Ziwei Liu", "title": "Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating\n  Source Separation", "comments": "To appear in Proceedings of the European Conference on Computer\n  Vision (ECCV), 2020. Code, models, and video results are available on our\n  webpage: https://hangz-nju-cuhk.github.io/projects/Sep-Stereo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stereophonic audio is an indispensable ingredient to enhance human auditory\nexperience. Recent research has explored the usage of visual information as\nguidance to generate binaural or ambisonic audio from mono ones with stereo\nsupervision. However, this fully supervised paradigm suffers from an inherent\ndrawback: the recording of stereophonic audio usually requires delicate devices\nthat are expensive for wide accessibility. To overcome this challenge, we\npropose to leverage the vastly available mono data to facilitate the generation\nof stereophonic audio. Our key observation is that the task of visually\nindicated audio separation also maps independent audios to their corresponding\nvisual positions, which shares a similar objective with stereophonic audio\ngeneration. We integrate both stereo generation and source separation into a\nunified framework, Sep-Stereo, by considering source separation as a particular\ntype of audio spatialization. Specifically, a novel associative pyramid network\narchitecture is carefully designed for audio-visual feature fusion. Extensive\nexperiments demonstrate that our framework can improve the stereophonic audio\ngeneration results while performing accurate sound separation with a shared\nbackbone.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 06:20:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhou", "Hang", ""], ["Xu", "Xudong", ""], ["Lin", "Dahua", ""], ["Wang", "Xiaogang", ""], ["Liu", "Ziwei", ""]]}, {"id": "2007.09916", "submitter": "Nupur Thakur", "authors": "Nupur Thakur, Yuzhen Ding, Baoxin Li", "title": "Evaluating a Simple Retraining Strategy as a Defense Against Adversarial\n  Attacks", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep neural networks (DNNs) have shown superiority over other\ntechniques in major fields like computer vision, natural language processing,\nrobotics, recently, it has been proven that they are vulnerable to adversarial\nattacks. The addition of a simple, small and almost invisible perturbation to\nthe original input image can be used to fool DNNs into making wrong decisions.\nWith more attack algorithms being designed, a need for defending the neural\nnetworks from such attacks arises. Retraining the network with adversarial\nimages is one of the simplest techniques. In this paper, we evaluate the\neffectiveness of such a retraining strategy in defending against adversarial\nattacks. We also show how simple algorithms like KNN can be used to determine\nthe labels of the adversarial images needed for retraining. We present the\nresults on two standard datasets namely, CIFAR-10 and TinyImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 07:49:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Thakur", "Nupur", ""], ["Ding", "Yuzhen", ""], ["Li", "Baoxin", ""]]}, {"id": "2007.09919", "submitter": "Shuai Jia", "authors": "Shuai Jia, Chao Ma, Yibing Song, and Xiaokang Yang", "title": "Robust Tracking against Adversarial Attacks", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep convolutional neural networks (CNNs) are vulnerable to adversarial\nattacks, considerably few efforts have been paid to construct robust deep\ntracking algorithms against adversarial attacks. Current studies on adversarial\nattack and defense mainly reside in a single image. In this work, we first\nattempt to generate adversarial examples on top of video sequences to improve\nthe tracking robustness against adversarial attacks. To this end, we take\ntemporal motion into consideration when generating lightweight perturbations\nover the estimated tracking results frame-by-frame. On one hand, we add the\ntemporal perturbations into the original video sequences as adversarial\nexamples to greatly degrade the tracking performance. On the other hand, we\nsequentially estimate the perturbations from input sequences and learn to\neliminate their effect for performance restoration. We apply the proposed\nadversarial attack and defense approaches to state-of-the-art deep tracking\nalgorithms. Extensive evaluations on the benchmark datasets demonstrate that\nour defense method not only eliminates the large performance drops caused by\nadversarial attacks, but also achieves additional performance gains when deep\ntrackers are not under adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:05:55 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:03:25 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Jia", "Shuai", ""], ["Ma", "Chao", ""], ["Song", "Yibing", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2007.09923", "submitter": "Kenan Ak", "authors": "Kenan E. Ak, Ning Xu, Zhe Lin, Yilin Wang", "title": "Incorporating Reinforced Adversarial Learning in Autoregressive Image\n  Generation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive models recently achieved comparable results versus\nstate-of-the-art Generative Adversarial Networks (GANs) with the help of Vector\nQuantized Variational AutoEncoders (VQ-VAE). However, autoregressive models\nhave several limitations such as exposure bias and their training objective\ndoes not guarantee visual fidelity. To address these limitations, we propose to\nuse Reinforced Adversarial Learning (RAL) based on policy gradient optimization\nfor autoregressive models. By applying RAL, we enable a similar process for\ntraining and testing to address the exposure bias issue. In addition, visual\nfidelity has been further optimized with adversarial loss inspired by their\nstrong counterparts: GANs. Due to the slow sampling speed of autoregressive\nmodels, we propose to use partial generation for faster training. RAL also\nempowers the collaboration between different modules of the VQ-VAE framework.\nTo our best knowledge, the proposed method is first to enable adversarial\nlearning in autoregressive models for image generation. Experiments on\nsynthetic and real-world datasets show improvements over the MLE trained\nmodels. The proposed method improves both negative log-likelihood (NLL) and\nFr\\'echet Inception Distance (FID), which indicates improvements in terms of\nvisual quality and diversity. The proposed method achieves state-of-the-art\nresults on Celeba for 64 $\\times$ 64 image resolution, showing promise for\nlarge scale image generation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:10:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ak", "Kenan E.", ""], ["Xu", "Ning", ""], ["Lin", "Zhe", ""], ["Wang", "Yilin", ""]]}, {"id": "2007.09933", "submitter": "Heeseung Kwon", "authors": "Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho", "title": "MotionSqueeze: Neural Motion Feature Learning for Video Understanding", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion plays a crucial role in understanding videos and most state-of-the-art\nneural models for video classification incorporate motion information typically\nusing optical flows extracted by a separate off-the-shelf method. As the\nframe-by-frame optical flows require heavy computation, incorporating motion\ninformation has remained a major computational bottleneck for video\nunderstanding. In this work, we replace external and heavy computation of\noptical flows with internal and light-weight learning of motion features. We\npropose a trainable neural module, dubbed MotionSqueeze, for effective motion\nfeature extraction. Inserted in the middle of any neural network, it learns to\nestablish correspondences across frames and convert them into motion features,\nwhich are readily fed to the next downstream layer for better prediction. We\ndemonstrate that the proposed method provides a significant gain on four\nstandard benchmarks for action recognition with only a small amount of\nadditional cost, outperforming the state of the art on\nSomething-Something-V1&V2 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:30:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kwon", "Heeseung", ""], ["Kim", "Manjin", ""], ["Kwak", "Suha", ""], ["Cho", "Minsu", ""]]}, {"id": "2007.09943", "submitter": "Sucheng Ren", "authors": "Sucheng Ren and Chu Han and Xin Yang and Guoqiang Han and Shengfeng He", "title": "TENet: Triple Excitation Network for Video Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet effective approach, named Triple\nExcitation Network, to reinforce the training of video salient object detection\n(VSOD) from three aspects, spatial, temporal, and online excitations. These\nexcitation mechanisms are designed following the spirit of curriculum learning\nand aim to reduce learning ambiguities at the beginning of training by\nselectively exciting feature activations using ground truth. Then we gradually\nreduce the weight of ground truth excitations by a curriculum rate and replace\nit by a curriculum complementary map for better and faster convergence. In\nparticular, the spatial excitation strengthens feature activations for clear\nobject boundaries, while the temporal excitation imposes motions to emphasize\nspatio-temporal salient regions. Spatial and temporal excitations can combat\nthe saliency shifting problem and conflict between spatial and temporal\nfeatures of VSOD. Furthermore, our semi-curriculum learning design enables the\nfirst online refinement strategy for VSOD, which allows exciting and boosting\nsaliency responses during testing without re-training. The proposed triple\nexcitations can easily plug in different VSOD methods. Extensive experiments\nshow the effectiveness of all three excitation methods and the proposed method\noutperforms state-of-the-art image and video salient object detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:45:41 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 12:59:31 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ren", "Sucheng", ""], ["Han", "Chu", ""], ["Yang", "Xin", ""], ["Han", "Guoqiang", ""], ["He", "Shengfeng", ""]]}, {"id": "2007.09945", "submitter": "Jun Kwan", "authors": "Jun Kwan, Chinkye Tan and Akansel Cosgun", "title": "Gesture Recognition for Initiating Human-to-Robot Handovers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-to-Robot handovers are useful for many Human-Robot Interaction\nscenarios. It is important to recognize when a human intends to initiate\nhandovers, so that the robot does not try to take objects from humans when a\nhandover is not intended. We pose the handover gesture recognition as a binary\nclassification problem in a single RGB image. Three separate neural network\nmodules for detecting the object, human body key points and head orientation,\nare implemented to extract relevant features from the RGB images, and then the\nfeature vectors are passed into a deep neural net to perform binary\nclassification. Our results show that the handover gestures are correctly\nidentified with an accuracy of over 90%. The abstraction of the features makes\nour approach modular and generalizable to different objects and human body\ntypes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:49:34 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 07:51:16 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kwan", "Jun", ""], ["Tan", "Chinkye", ""], ["Cosgun", "Akansel", ""]]}, {"id": "2007.09952", "submitter": "Roy Jennings", "authors": "Hai Victor Habi, Roy H. Jennings, Arnon Netzer", "title": "HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in network quantization produced state-of-the-art results using\nmixed precision quantization. An imperative requirement for many efficient edge\ndevice hardware implementations is that their quantizers are uniform and with\npower-of-two thresholds. In this work, we introduce the Hardware Friendly Mixed\nPrecision Quantization Block (HMQ) in order to meet this requirement. The HMQ\nis a mixed precision quantization block that repurposes the Gumbel-Softmax\nestimator into a smooth estimator of a pair of quantization parameters, namely,\nbit-width and threshold. HMQs use this to search over a finite space of\nquantization schemes. Empirically, we apply HMQs to quantize classification\nmodels trained on CIFAR10 and ImageNet. For ImageNet, we quantize four\ndifferent architectures and show that, in spite of the added restrictions to\nour quantization scheme, we achieve competitive and, in some cases,\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:02:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Habi", "Hai Victor", ""], ["Jennings", "Roy H.", ""], ["Netzer", "Arnon", ""]]}, {"id": "2007.09963", "submitter": "Petar Jokic", "authors": "Petar Jokic, Stephane Emery, Luca Benini", "title": "Improving Memory Utilization in Convolutional Neural Network\n  Accelerators", "comments": null, "journal-ref": null, "doi": "10.1109/LES.2020.3009924", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the accuracy of convolutional neural networks has achieved vast\nimprovements by introducing larger and deeper network architectures, also the\nmemory footprint for storing their parameters and activations has increased.\nThis trend especially challenges power- and resource-limited accelerator\ndesigns, which are often restricted to store all network data in on-chip memory\nto avoid interfacing energy-hungry external memories. Maximizing the network\nsize that fits on a given accelerator thus requires to maximize its memory\nutilization. While the traditionally used ping-pong buffering technique is\nmapping subsequent activation layers to disjunctive memory regions, we propose\na mapping method that allows these regions to overlap and thus utilize the\nmemory more efficiently. This work presents the mathematical model to compute\nthe maximum activations memory overlap and thus the lower bound of on-chip\nmemory needed to perform layer-by-layer processing of convolutional neural\nnetworks on memory-limited accelerators. Our experiments with various\nreal-world object detector networks show that the proposed mapping technique\ncan decrease the activations memory by up to 32.9%, reducing the overall memory\nfor the entire network by up to 23.9% compared to traditional ping-pong\nbuffering. For higher resolution de-noising networks, we achieve activation\nmemory savings of 48.8%. Additionally, we implement a face detector network on\nan FPGA-based camera to validate these memory savings on a complete end-to-end\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:34:36 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 15:45:49 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Jokic", "Petar", ""], ["Emery", "Stephane", ""], ["Benini", "Luca", ""]]}, {"id": "2007.09968", "submitter": "Lijun Gong", "authors": "Shaoteng Liu, Lijun Gong, Kai Ma, Yefeng Zheng", "title": "GREEN: a Graph REsidual rE-ranking Network for Grading Diabetic\n  Retinopathy", "comments": "MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic grading of diabetic retinopathy (DR) facilitates medical\ndiagnosis for both patients and physicians. Existing researches formulate DR\ngrading as an image classification problem. As the stages/categories of DR\ncorrelate with each other, the relationship between different classes cannot be\nexplicitly described via a one-hot label because it is empirically estimated by\ndifferent physicians with different outcomes. This class correlation limits\nexisting networks to achieve effective classification. In this paper, we\npropose a Graph REsidual rE-ranking Network (GREEN) to introduce a class\ndependency prior into the original image classification network. The class\ndependency prior is represented by a graph convolutional network with an\nadjacency matrix. This prior augments image classification pipeline by\nre-ranking classification results in a residual aggregation manner. Experiments\non the standard benchmarks have shown that GREEN performs favorably against\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:41:18 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 09:02:15 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Liu", "Shaoteng", ""], ["Gong", "Lijun", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.09971", "submitter": "Riccardo Barbano", "authors": "Riccardo Barbano, Chen Zhang, Simon Arridge, Bangti Jin", "title": "Quantifying Model Uncertainty in Inverse Problems via Bayesian Deep\n  Gradient Descent", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in reconstruction methods for inverse problems leverage\npowerful data-driven models, e.g., deep neural networks. These techniques have\ndemonstrated state-of-the-art performances for several imaging tasks, but they\noften do not provide uncertainty on the obtained reconstruction. In this work,\nwe develop a scalable, data-driven, knowledge-aided computational framework to\nquantify the model uncertainty via Bayesian neural networks. The approach\nbuilds on, and extends deep gradient descent, a recently developed greedy\niterative training scheme, and recasts it within a probabilistic framework.\nScalability is achieved by being hybrid in the architecture: only the last\nlayer of each block is Bayesian, while the others remain deterministic, and by\nbeing greedy in training. The framework is showcased on one representative\nmedical imaging modality, viz. computed tomography with either sparse view or\nlimited view data, and exhibits competitive performance with respect to\nstate-of-the-art benchmarks, e.g., total variation, deep gradient descent and\nlearned primal-dual.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:43:31 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 10:58:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Barbano", "Riccardo", ""], ["Zhang", "Chen", ""], ["Arridge", "Simon", ""], ["Jin", "Bangti", ""]]}, {"id": "2007.09979", "submitter": "Lijun Gong", "authors": "Lijun Gong, Kai Ma, Yefeng Zheng", "title": "Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image\n  Classifications", "comments": "MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis benefits Computer Aided Diagnosis (CADx). A\nfundamental analyzing approach is the classification of medical images, which\nserves for skin lesion diagnosis, diabetic retinopathy grading, and cancer\nclassification on histological images. When learning these discriminative\nclassifiers, we observe that the convolutional neural networks (CNNs) are\nvulnerable to distractor interference. This is due to the similar sample\nappearances from different categories (i.e., small inter-class distance).\nExisting attempts select distractors from input images by empirically\nestimating their potential effects to the classifier. The essences of how these\ndistractors affect CNN classification are not known. In this paper, we explore\ndistractors from the CNN feature space via proposing a neuron intrinsic\nlearning method. We formulate a novel distractor-aware loss that encourages\nlarge distance between the original image and its distractor in the feature\nspace. The novel loss is combined with the original classification loss to\nupdate network parameters by back-propagation. Neuron intrinsic learning first\nexplores distractors crucial to the deep classifier and then uses them to\nrobustify CNN inherently. Extensive experiments on medical image benchmark\ndatasets indicate that the proposed method performs favorably against the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:59:04 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 08:58:04 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gong", "Lijun", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.09990", "submitter": "Wonjik Kim", "authors": "Wonjik Kim, Asako Kanezaki, and Masayuki Tanaka", "title": "Unsupervised Learning of Image Segmentation Based on Differentiable\n  Feature Clustering", "comments": "IEEE Transactions on Image Processing, Accepted in July, 2020", "journal-ref": null, "doi": "10.1109/TIP.2020.3011269", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of convolutional neural networks (CNNs) for unsupervised image\nsegmentation was investigated in this study. In the proposed approach, label\nprediction and network parameter learning are alternately iterated to meet the\nfollowing criteria: (a) pixels of similar features should be assigned the same\nlabel, (b) spatially continuous pixels should be assigned the same label, and\n(c) the number of unique labels should be large. Although these criteria are\nincompatible, the proposed approach minimizes the combination of similarity\nloss and spatial continuity loss to find a plausible solution of label\nassignment that balances the aforementioned criteria well. The contributions of\nthis study are four-fold. First, we propose a novel end-to-end network of\nunsupervised image segmentation that consists of normalization and an argmax\nfunction for differentiable clustering. Second, we introduce a spatial\ncontinuity loss function that mitigates the limitations of fixed segment\nboundaries possessed by previous work. Third, we present an extension of the\nproposed method for segmentation with scribbles as user input, which showed\nbetter accuracy than existing methods while maintaining efficiency. Finally, we\nintroduce another extension of the proposed method: unseen image segmentation\nby using networks pre-trained with a few reference images without re-training\nthe networks. The effectiveness of the proposed approach was examined on\nseveral benchmark datasets of image segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:28:36 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kim", "Wonjik", ""], ["Kanezaki", "Asako", ""], ["Tanaka", "Masayuki", ""]]}, {"id": "2007.10000", "submitter": "Kristijan Bartol", "authors": "Kristijan Bartol and David Bojani\\'c and Tomislav Pribani\\'c and\n  Tomislav Petkovi\\'c and Yago Diez Donoso and Joaquim Salvi Mas", "title": "On the Comparison of Classic and Deep Keypoint Detector and Descriptor\n  Methods", "comments": null, "journal-ref": "Proceedings of the 2019 11th International Symposium on Image and\n  Signal Processing and Analysis (ISPA), Page(s): 64-69", "doi": "10.1109/ISPA.2019.8868792", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to give a performance comparison between several\nclassic hand-crafted and deep key-point detector and descriptor methods. In\nparticular, we consider the following classical algorithms: SIFT, SURF, ORB,\nFAST, BRISK, MSER, HARRIS, KAZE, AKAZE, AGAST, GFTT, FREAK, BRIEF and RootSIFT,\nwhere a subset of all combinations is paired into detector-descriptor\npipelines. Additionally, we analyze the performance of two recent and\nperspective deep detector-descriptor models, LF-Net and SuperPoint. Our\nbenchmark relies on the HPSequences dataset that provides real and diverse\nimages under various geometric and illumination changes. We analyze the\nperformance on three evaluation tasks: keypoint verification, image matching\nand keypoint retrieval. The results show that certain classic and deep\napproaches are still comparable, with some classic detector-descriptor\ncombinations overperforming pretrained deep models. In terms of the execution\ntimes of tested implementations, SuperPoint model is the fastest, followed by\nORB. The source code is published on\n\\url{https://github.com/kristijanbartol/keypoint-algorithms-benchmark}.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 11:01:01 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:26:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bartol", "Kristijan", ""], ["Bojani\u0107", "David", ""], ["Pribani\u0107", "Tomislav", ""], ["Petkovi\u0107", "Tomislav", ""], ["Donoso", "Yago Diez", ""], ["Mas", "Joaquim Salvi", ""]]}, {"id": "2007.10004", "submitter": "Junjie Zhao", "authors": "Junjie Zhao, Donghuan Lu, Kai Ma, Yu Zhang, Yefeng Zheng", "title": "Deep Image Clustering with Category-Style Representation", "comments": "Accepted at ECCV 2020. Project address:\n  https://github.com/sKamiJ/DCCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering which adopts deep neural networks to obtain optimal\nrepresentations for clustering has been widely studied recently. In this paper,\nwe propose a novel deep image clustering framework to learn a category-style\nlatent representation in which the category information is disentangled from\nimage style and can be directly used as the cluster assignment. To achieve this\ngoal, mutual information maximization is applied to embed relevant information\nin the latent representation. Moreover, augmentation-invariant loss is employed\nto disentangle the representation into category part and style part. Last but\nnot least, a prior distribution is imposed on the latent representation to\nensure the elements of the category vector can be used as the probabilities\nover clusters. Comprehensive experiments demonstrate that the proposed approach\noutperforms state-of-the-art methods significantly on five public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 11:20:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhao", "Junjie", ""], ["Lu", "Donghuan", ""], ["Ma", "Kai", ""], ["Zhang", "Yu", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.10007", "submitter": "Munan Ning", "authors": "Munan Ning, Cheng Bian, Donghuan Lu, Hong-Yu Zhou, Shuang Yu,\n  Chenglang Yuan, Yang Guo, Yaohua Wang, Kai Ma, Yefeng Zheng", "title": "A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary angle closure glaucoma (PACG) is the leading cause of irreversible\nblindness among Asian people. Early detection of PACG is essential, so as to\nprovide timely treatment and minimize the vision loss. In the clinical\npractice, PACG is diagnosed by analyzing the angle between the cornea and iris\nwith anterior segment optical coherence tomography (AS-OCT). The rapid\ndevelopment of deep learning technologies provides the feasibility of building\na computer-aided system for the fast and accurate segmentation of cornea and\niris tissues. However, the application of deep learning methods in the medical\nimaging field is still restricted by the lack of enough fully-annotated\nsamples. In this paper, we propose a novel framework to segment the target\ntissues accurately for the AS-OCT images, by using the combination of\nweakly-annotated images (majority) and fully-annotated images (minority). The\nproposed framework consists of two models which provide reliable guidance for\neach other. In addition, uncertainty guided strategies are adopted to increase\nthe accuracy and stability of the guidance. Detailed experiments on the\npublicly available AGE dataset demonstrate that the proposed framework\noutperforms the state-of-the-art semi-/weakly-supervised methods and has a\ncomparable performance as the fully-supervised method. Therefore, the proposed\nmethod is demonstrated to be effective in exploiting information contained in\nthe weakly-annotated images and has the capability to substantively relieve the\nannotation workload.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 11:26:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ning", "Munan", ""], ["Bian", "Cheng", ""], ["Lu", "Donghuan", ""], ["Zhou", "Hong-Yu", ""], ["Yu", "Shuang", ""], ["Yuan", "Chenglang", ""], ["Guo", "Yang", ""], ["Wang", "Yaohua", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.10026", "submitter": "Haibao Yu", "authors": "Haibao Yu, Qi Han, Jianbo Li, Jianping Shi, Guangliang Cheng, Bin Fan", "title": "Search What You Want: Barrier Panelty NAS for Mixed Precision\n  Quantization", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent hardwares can support mixed precision CNN models inference that\nassign different bitwidths for different layers. Learning to find an optimal\nmixed precision model that can preserve accuracy and satisfy the specific\nconstraints on model size and computation is extremely challenge due to the\ndifficult in training a mixed precision model and the huge space of all\npossible bit quantizations. In this paper, we propose a novel soft Barrier\nPenalty based NAS (BP-NAS) for mixed precision quantization, which ensures all\nthe searched models are inside the valid domain defined by the complexity\nconstraint, thus could return an optimal model under the given constraint by\nconducting search only one time. The proposed soft Barrier Penalty is\ndifferentiable and can impose very large losses to those models outside the\nvalid domain while almost no punishment for models inside the valid domain,\nthus constraining the search only in the feasible domain. In addition, a\ndifferentiable Prob-1 regularizer is proposed to ensure learning with NAS is\nreasonable. A distribution reshaping training strategy is also used to make\ntraining more stable. BP-NAS sets new state of the arts on both classification\n(Cifar-10, ImageNet) and detection (COCO), surpassing all the efficient mixed\nprecision methods designed manually and automatically. Particularly, BP-NAS\nachieves higher mAP (up to 2.7\\% mAP improvement) together with lower bit\ncomputation cost compared with the existing best mixed precision model on COCO\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:00:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yu", "Haibao", ""], ["Han", "Qi", ""], ["Li", "Jianbo", ""], ["Shi", "Jianping", ""], ["Cheng", "Guangliang", ""], ["Fan", "Bin", ""]]}, {"id": "2007.10032", "submitter": "Daniel Barath", "authors": "Daniel Barath, Michal Polic, Wolfgang F\\\"orstner, Torsten Sattler,\n  Tomas Pajdla, Zuzana Kukelova", "title": "Making Affine Correspondences Work in Camera Geometry Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local features e.g. SIFT and its affine and learned variants provide\nregion-to-region rather than point-to-point correspondences. This has recently\nbeen exploited to create new minimal solvers for classical problems such as\nhomography, essential and fundamental matrix estimation. The main advantage of\nsuch solvers is that their sample size is smaller, e.g., only two instead of\nfour matches are required to estimate a homography. Works proposing such\nsolvers often claim a significant improvement in run-time thanks to fewer\nRANSAC iterations. We show that this argument is not valid in practice if the\nsolvers are used naively. To overcome this, we propose guidelines for effective\nuse of region-to-region matches in the course of a full model estimation\npipeline. We propose a method for refining the local feature geometries by\nsymmetric intensity-based matching, combine uncertainty propagation inside\nRANSAC with preemptive model verification, show a general scheme for computing\nuncertainty of minimal solvers results, and adapt the sample cheirality check\nfor homography estimation. Our experiments show that affine solvers can achieve\naccuracy comparable to point-based solvers at faster run-times when following\nour guidelines. We make code available at\nhttps://github.com/danini/affine-correspondences-for-camera-geometry.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:07:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Barath", "Daniel", ""], ["Polic", "Michal", ""], ["F\u00f6rstner", "Wolfgang", ""], ["Sattler", "Torsten", ""], ["Pajdla", "Tomas", ""], ["Kukelova", "Zuzana", ""]]}, {"id": "2007.10035", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi,\n  Zhouchen Lin, Shaohua Tan, Yunhai Tong", "title": "Improving Semantic Segmentation via Decoupled Body and Edge Supervision", "comments": "accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing semantic segmentation approaches either aim to improve the object's\ninner consistency by modeling the global context, or refine objects detail\nalong their boundaries by multi-scale feature fusion. In this paper, a new\nparadigm for semantic segmentation is proposed. Our insight is that appealing\nperformance of semantic segmentation requires \\textit{explicitly} modeling the\nobject \\textit{body} and \\textit{edge}, which correspond to the high and low\nfrequency of the image. To do so, we first warp the image feature by learning a\nflow field to make the object part more consistent. The resulting body feature\nand the residual edge feature are further optimized under decoupled supervision\nby explicitly sampling different parts (body or edge) pixels. We show that the\nproposed framework with various baselines or backbone networks leads to better\nobject inner consistency and object boundaries. Extensive experiments on four\nmajor road scene semantic segmentation benchmarks including\n\\textit{Cityscapes}, \\textit{CamVid}, \\textit{KIITI} and \\textit{BDD} show that\nour proposed approach establishes new state of the art while retaining high\nefficiency in inference. In particular, we achieve 83.7 mIoU \\% on Cityscape\nwith only fine-annotated data. Code and models are made available to foster any\nfurther research (\\url{https://github.com/lxtGH/DecoupleSegNets}).\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:11:22 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 03:41:10 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Li", "Xiangtai", ""], ["Li", "Xia", ""], ["Zhang", "Li", ""], ["Cheng", "Guangliang", ""], ["Shi", "Jianping", ""], ["Lin", "Zhouchen", ""], ["Tan", "Shaohua", ""], ["Tong", "Yunhai", ""]]}, {"id": "2007.10042", "submitter": "Jinsun Park", "authors": "Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, In So Kweon", "title": "Non-Local Spatial Propagation Network for Depth Completion", "comments": "To appear in ECCV 2020. Project page:\n  https://github.com/zzangjinsun/NLSPN_ECCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a robust and efficient end-to-end non-local spatial\npropagation network for depth completion. The proposed network takes RGB and\nsparse depth images as inputs and estimates non-local neighbors and their\naffinities of each pixel, as well as an initial depth map with pixel-wise\nconfidences. The initial depth prediction is then iteratively refined by its\nconfidence and non-local spatial propagation procedure based on the predicted\nnon-local neighbors and corresponding affinities. Unlike previous algorithms\nthat utilize fixed-local neighbors, the proposed algorithm effectively avoids\nirrelevant local neighbors and concentrates on relevant non-local neighbors\nduring propagation. In addition, we introduce a learnable affinity\nnormalization to better learn the affinity combinations compared to\nconventional methods. The proposed algorithm is inherently robust to the\nmixed-depth problem on depth boundaries, which is one of the major issues for\nexisting depth estimation/completion algorithms. Experimental results on indoor\nand outdoor datasets demonstrate that the proposed algorithm is superior to\nconventional algorithms in terms of depth completion accuracy and robustness to\nthe mixed-depth problem. Our implementation is publicly available on the\nproject page.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:26:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Park", "Jinsun", ""], ["Joo", "Kyungdon", ""], ["Hu", "Zhe", ""], ["Liu", "Chi-Kuei", ""], ["Kweon", "In So", ""]]}, {"id": "2007.10052", "submitter": "Dmitrii Lachinov", "authors": "Dmitry Lachinov, Alexandra Getmanskaya and Vadim Turlapov", "title": "Cephalometric Landmark Regression with Convolutional Neural Networks on\n  3D Computed Tomography Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of automatic three-dimensional\ncephalometric analysis. Cephalometric analysis performed on lateral radiographs\ndoesn't fully exploit the structure of 3D objects due to projection onto the\nlateral plane. With the development of three-dimensional imaging techniques\nsuch as CT, several analysis methods have been proposed that extend to the 3D\ncase. The analysis based on these methods is invariant to rotations and\ntranslations and can describe difficult skull deformation, where 2D\ncephalometry has no use. In this paper, we provide a wide overview of existing\napproaches for cephalometric landmark regression. Moreover, we perform a series\nof experiments with state of the art 3D convolutional neural network (CNN)\nbased methods for keypoint regression: direct regression with CNN, heatmap\nregression and Softargmax regression. For the first time, we extensively\nevaluate the described methods and demonstrate their effectiveness in the\nestimation of Frankfort Horizontal and cephalometric points locations for\npatients with severe skull deformations. We demonstrate that Heatmap and\nSoftargmax regression models provide sufficient regression error for medical\napplications (less than 4 mm). Moreover, the Softargmax model achieves 1.15o\ninclination error for the Frankfort horizontal. For the fair comparison with\nthe prior art, we also report results projected on the lateral plane.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:45:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lachinov", "Dmitry", ""], ["Getmanskaya", "Alexandra", ""], ["Turlapov", "Vadim", ""]]}, {"id": "2007.10066", "submitter": "Ga\\\"el \\'Ecorchard", "authors": "Ga\\\"el \\'Ecorchard and Karel Ko\\v{s}nar and Libor P\\v{r}eu\\v{c}il", "title": "Wearable camera-based human absolute localization in large warehouses", "comments": "Conference paper presented at Twelfth International Conference on\n  Machine Vision, 2019", "journal-ref": "Twelfth International Conference on Machine Vision, 2019", "doi": "10.1117/12.2559424", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a robotised warehouse, as in any place where robots move autonomously, a\nmajor issue is the localization or detection of human operators during their\nintervention in the work area of the robots. This paper introduces a wearable\nhuman localization system for large warehouses, which utilize preinstalled\ninfrastructure used for localization of automated guided vehicles (AGVs). A\nmonocular down-looking camera is detecting ground nodes, identifying them and\ncomputing the absolute position of the human to allow safe cooperation and\ncoexistence of humans and AGVs in the same workspace. A virtual safety area\naround the human operator is set up and any AGV in this area is immediately\nstopped. In order to avoid triggering an emergency stop because of the short\ndistance between robots and human operators, the trajectories of the robots\nhave to be modified so that they do not interfere with the human. The purpose\nof this paper is to demonstrate an absolute visual localization method working\nin the challenging environment of an automated warehouse with low intensity of\nlight, massively changing environment and using solely monocular camera placed\non the human body.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:57:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["\u00c9corchard", "Ga\u00ebl", ""], ["Ko\u0161nar", "Karel", ""], ["P\u0159eu\u010dil", "Libor", ""]]}, {"id": "2007.10075", "submitter": "Tian Xu", "authors": "Tian Xu, Jennifer White, Sinan Kalkan, Hatice Gunes", "title": "Investigating Bias and Fairness in Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of expressions of emotions and affect from facial images is a\nwell-studied research problem in the fields of affective computing and computer\nvision with a large number of datasets available containing facial images and\ncorresponding expression labels. However, virtually none of these datasets have\nbeen acquired with consideration of fair distribution across the human\npopulation. Therefore, in this work, we undertake a systematic investigation of\nbias and fairness in facial expression recognition by comparing three different\napproaches, namely a baseline, an attribute-aware and a disentangled approach,\non two well-known datasets, RAF-DB and CelebA. Our results indicate that: (i)\ndata augmentation improves the accuracy of the baseline model, but this alone\nis unable to mitigate the bias effect; (ii) both the attribute-aware and the\ndisentangled approaches fortified with data augmentation perform better than\nthe baseline approach in terms of accuracy and fairness; (iii) the disentangled\napproach is the best for mitigating demographic bias; and (iv) the bias\nmitigation strategies are more suitable in the existence of uneven attribute\ndistribution or imbalanced number of subgroup data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:12:53 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 16:59:35 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 15:29:22 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Xu", "Tian", ""], ["White", "Jennifer", ""], ["Kalkan", "Sinan", ""], ["Gunes", "Hatice", ""]]}, {"id": "2007.10082", "submitter": "Daniel Barath", "authors": "Ivan Eichhardt, Daniel Barath", "title": "Relative Pose from Deep Learned Depth and a Single Affine Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for combining deep-learned non-metric monocular\ndepth with affine correspondences (ACs) to estimate the relative pose of two\ncalibrated cameras from a single correspondence. Considering the depth\ninformation and affine features, two new constraints on the camera pose are\nderived. The proposed solver is usable within 1-point RANSAC approaches. Thus,\nthe processing time of the robust estimation is linear in the number of\ncorrespondences and, therefore, orders of magnitude faster than by using\ntraditional approaches. The proposed 1AC+D solver is tested both on synthetic\ndata and on 110395 publicly available real image pairs where we used an\noff-the-shelf monocular depth network to provide up-to-scale depth per pixel.\nThe proposed 1AC+D leads to similar accuracy as traditional approaches while\nbeing significantly faster. When solving large-scale problems, e.g., pose-graph\ninitialization for Structure-from-Motion (SfM) pipelines, the overhead of\nobtaining ACs and monocular depth is negligible compared to the speed-up gained\nin the pairwise geometric verification, i.e., relative pose estimation. This is\ndemonstrated on scenes from the 1DSfM dataset using a state-of-the-art global\nSfM algorithm. Source code: https://github.com/eivan/one-ac-pose\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:24:28 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Eichhardt", "Ivan", ""], ["Barath", "Daniel", ""]]}, {"id": "2007.10092", "submitter": "Zhe Li", "authors": "Zhe Li, Lianwen Jin, Songxuan Lai, Yecheng Zhu", "title": "Improving Attention-Based Handwritten Mathematical Expression\n  Recognition with Scale Augmentation and Drop Attention", "comments": "Accepted to appear in ICFHR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten mathematical expression recognition (HMER) is an important\nresearch direction in handwriting recognition. The performance of HMER suffers\nfrom the two-dimensional structure of mathematical expressions (MEs). To\naddress this issue, in this paper, we propose a high-performance HMER model\nwith scale augmentation and drop attention. Specifically, tackling ME with\nunstable scale in both horizontal and vertical directions, scale augmentation\nimproves the performance of the model on MEs of various scales. An\nattention-based encoder-decoder network is used for extracting features and\ngenerating predictions. In addition, drop attention is proposed to further\nimprove performance when the attention distribution of the decoder is not\nprecise. Compared with previous methods, our method achieves state-of-the-art\nperformance on two public datasets of CROHME 2014 and CROHME 2016.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:35:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Zhe", ""], ["Jin", "Lianwen", ""], ["Lai", "Songxuan", ""], ["Zhu", "Yecheng", ""]]}, {"id": "2007.10100", "submitter": "Snehal Bhayani", "authors": "Snehal Bhayani, Zuzana Kukelova and Janne Heikkil\\\"a", "title": "Computing stable resultant-based minimal solvers by hiding a variable", "comments": "arXiv admin note: text overlap with arXiv:1912.10268", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications require robust and efficient estimation of\ncamera geometry. The robust estimation is usually based on solving camera\ngeometry problems from a minimal number of input data measurements, i.e.,\nsolving minimal problems, in a RANSAC-style framework. Minimal problems often\nresult in complex systems of polynomial equations. The existing\nstate-of-the-art methods for solving such systems are either based on Gr\\\"obner\nbases and the action matrix method, which have been extensively studied and\noptimized in the recent years or recently proposed approach based on a sparse\nresultant computation using an extra variable.\n  In this paper, we study an interesting alternative sparse resultant-based\nmethod for solving sparse systems of polynomial equations by hiding one\nvariable. This approach results in a larger eigenvalue problem than the action\nmatrix and extra variable sparse resultant-based methods; however, it does not\nneed to compute an inverse or elimination of large matrices that may be\nnumerically unstable. The proposed approach includes several improvements to\nthe standard sparse resultant algorithms, which significantly improves the\nefficiency and stability of the hidden variable resultant-based solvers as we\ndemonstrate on several interesting computer vision problems. We show that for\nthe studied problems, our sparse resultant based approach leads to more stable\nsolvers than the state-of-the-art Gr\\\"obner bases-based solvers as well as\nexisting sparse resultant-based solvers, especially in close to critical\nconfigurations. Our new method can be fully automated and incorporated into\nexisting tools for the automatic generation of efficient minimal solvers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:40:10 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bhayani", "Snehal", ""], ["Kukelova", "Zuzana", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "2007.10106", "submitter": "Ghouthi Boukli Hacene", "authors": "Guillaume Coiffier, Ghouthi Boukli Hacene, Vincent Gripon", "title": "ThriftyNets : Convolutional Neural Networks with Tiny Parameter Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical deep convolutional architectures present an increasing number of\nfeature maps as we go deeper in the network, whereas spatial resolution of\ninputs is decreased through downsampling operations. This means that most of\nthe parameters lay in the final layers, while a large portion of the\ncomputations are performed by a small fraction of the total parameters in the\nfirst layers. In an effort to use every parameter of a network at its maximum,\nwe propose a new convolutional neural network architecture, called ThriftyNet.\nIn ThriftyNet, only one convolutional layer is defined and used recursively,\nleading to a maximal parameter factorization. In complement, normalization,\nnon-linearities, downsamplings and shortcut ensure sufficient expressivity of\nthe model. ThriftyNet achieves competitive performance on a tiny parameters\nbudget, exceeding 91% accuracy on CIFAR-10 with less than 40K parameters in\ntotal, and 74.3% on CIFAR-100 with less than 600K parameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:50:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Coiffier", "Guillaume", ""], ["Hacene", "Ghouthi Boukli", ""], ["Gripon", "Vincent", ""]]}, {"id": "2007.10114", "submitter": "Firas Laakom", "authors": "Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Jarno Nikkanen\n  and Moncef Gabbouj", "title": "Monte Carlo Dropout Ensembles for Robust Illumination Estimation", "comments": "7 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational color constancy is a preprocessing step used in many camera\nsystems. The main aim is to discount the effect of the illumination on the\ncolors in the scene and restore the original colors of the objects. Recently,\nseveral deep learning-based approaches have been proposed to solve this problem\nand they often led to state-of-the-art performance in terms of average errors.\nHowever, for extreme samples, these methods fail and lead to high errors. In\nthis paper, we address this limitation by proposing to aggregate different deep\nlearning methods according to their output uncertainty. We estimate the\nrelative uncertainty of each approach using Monte Carlo dropout and the final\nillumination estimate is obtained as the sum of the different model estimates\nweighted by the log-inverse of their corresponding uncertainties. The proposed\nframework leads to state-of-the-art performance on INTEL-TAU dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:56:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Laakom", "Firas", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Nikkanen", "Jarno", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2007.10143", "submitter": "Rumen Dangovski", "authors": "Evan Vogelbaum and Rumen Dangovski and Li Jing and Marin\n  Solja\\v{c}i\\'c", "title": "Contextualizing Enhances Gradient Based Meta Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta learning methods have found success when applied to few shot\nclassification problems, in which they quickly adapt to a small number of\nlabeled examples. Prototypical representations, each representing a particular\nclass, have been of particular importance in this setting, as they provide a\ncompact form to convey information learned from the labeled examples. However,\nthese prototypes are just one method of representing this information, and they\nare narrow in their scope and ability to classify unseen examples. We propose\nthe implementation of contextualizers, which are generalizable prototypes that\nadapt to given examples and play a larger role in classification for\ngradient-based models. We demonstrate how to equip meta learning methods with\ncontextualizers and show that their use can significantly boost performance on\na range of few shot learning datasets. We also present figures of merit\ndemonstrating the potential benefits of contextualizers, along with analysis of\nhow models make use of them. Our approach is particularly apt for low-data\nenvironments where it is difficult to update parameters without overfitting.\nOur implementation and instructions to reproduce the experiments are available\nat https://github.com/naveace/proto-context.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 04:01:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vogelbaum", "Evan", ""], ["Dangovski", "Rumen", ""], ["Jing", "Li", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "2007.10148", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang", "title": "Tracking the Untrackable", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although short-term fully occlusion happens rare in visual object tracking,\nmost trackers will fail under these circumstances. However, humans can still\ncatch up the target by anticipating the trajectory of the target even the\ntarget is invisible. Recent psychology also has shown that humans build the\nmental image of the future. Inspired by that, we present a HAllucinating\nFeatures to Track (HAFT) model that enables to forecast the visual feature\nembedding of future frames. The anticipated future frames focus on the movement\nof the target while hallucinating the occluded part of the target. Jointly\ntracking on the hallucinated features and the real features improves the\nrobustness of the tracker even when the target is highly occluded. Through\nextensive experimental evaluations, we achieve promising results on multiple\ndatasets: OTB100, VOT2018, LaSOT, TrackingNet, and UAV123.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:17:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Fangyi", ""]]}, {"id": "2007.10170", "submitter": "Roman Klokov", "authors": "Roman Klokov, Edmond Boyer, Jakob Verbeek", "title": "Discrete Point Flow Networks for Efficient Point Cloud Generation", "comments": "In ECCV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have proven effective at modeling 3D shapes and their\nstatistical variations. In this paper we investigate their application to point\nclouds, a 3D shape representation widely used in computer vision for which,\nhowever, only few generative models have yet been proposed. We introduce a\nlatent variable model that builds on normalizing flows with affine coupling\nlayers to generate 3D point clouds of an arbitrary size given a latent shape\nrepresentation. To evaluate its benefits for shape modeling we apply this model\nfor generation, autoencoding, and single-view shape reconstruction tasks. We\nimprove over recent GAN-based models in terms of most metrics that assess\ngeneration and autoencoding. Compared to recent work based on continuous flows,\nour model offers a significant speedup in both training and inference times for\nsimilar or better performance. For single-view shape reconstruction we also\nobtain results on par with state-of-the-art voxel, point cloud, and mesh-based\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:48:00 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Klokov", "Roman", ""], ["Boyer", "Edmond", ""], ["Verbeek", "Jakob", ""]]}, {"id": "2007.10172", "submitter": "Hang Du", "authors": "Dan Zeng, Hailin Shi, Hang Du, Jun Wang, Zhen Lei, and Tao Mei", "title": "NPCFace: Negative-Positive Collaborative Training for Large-scale Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training scheme of deep face recognition has greatly evolved in the past\nyears, yet it encounters new challenges in the large-scale data situation where\nmassive and diverse hard cases occur. Especially in the range of low false\naccept rate (FAR), there are various hard cases in both positives (intra-class)\nand negatives (inter-class). In this paper, we study how to make better use of\nthese hard samples for improving the training. The literature approaches this\nby margin-based formulation in either positive logit or negative logits.\nHowever, the correlation between hard positive and hard negative is overlooked,\nand so is the relation between the margins in positive and negative logits. We\nfind such correlation is significant, especially in the large-scale dataset,\nand one can take advantage from it to boost the training via relating the\npositive and negative margins for each training sample. To this end, we propose\nan explicit collaboration between positive and negative margins sample-wisely.\nGiven a batch of hard samples, a novel Negative-Positive Collaboration loss,\nnamed NPCFace, is formulated, which emphasizes the training on both negative\nand positive hard cases via the collaborative-margin mechanism in the softmax\nlogits, and also brings better interpretation of negative-positive hardness\ncorrelation. Besides, the emphasis is implemented with an improved formulation\nto achieve stable convergence and flexible parameter setting. We validate the\neffectiveness of our approach on various benchmarks of large-scale face\nrecognition, and obtain advantageous results especially in the low FAR range.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:52:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 07:22:16 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 11:37:18 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zeng", "Dan", ""], ["Shi", "Hailin", ""], ["Du", "Hang", ""], ["Wang", "Jun", ""], ["Lei", "Zhen", ""], ["Mei", "Tao", ""]]}, {"id": "2007.10175", "submitter": "Jordan J. Bird", "authors": "Jordan J. Bird, Diego R. Faria, Cristiano Premebida, Anik\\'o Ek\\'art,\n  George Vogiatzis", "title": "Look and Listen: A Multi-modality Late Fusion Approach to Scene\n  Classification for Autonomous Machines", "comments": "6 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novelty of this study consists in a multi-modality approach to scene\nclassification, where image and audio complement each other in a process of\ndeep late fusion. The approach is demonstrated on a difficult classification\nproblem, consisting of two synchronised and balanced datasets of 16,000 data\nobjects, encompassing 4.4 hours of video of 8 environments with varying degrees\nof similarity. We first extract video frames and accompanying audio at one\nsecond intervals. The image and the audio datasets are first classified\nindependently, using a fine-tuned VGG16 and an evolutionary optimised deep\nneural network, with accuracies of 89.27% and 93.72%, respectively. This is\nfollowed by late fusion of the two neural networks to enable a higher order\nfunction, leading to accuracy of 96.81% in this multi-modality classifier with\nsynchronised video frames and audio clips. The tertiary neural network\nimplemented for late fusion outperforms classical state-of-the-art classifiers\nby around 3% when the two primary networks are considered as feature\ngenerators. We show that situations where a single-modality may be confused by\nanomalous data points are now corrected through an emerging higher order\nintegration. Prominent examples include a water feature in a city misclassified\nas a river by the audio classifier alone and a densely crowded street\nmisclassified as a forest by the image classifier alone. Both are examples\nwhich are correctly classified by our multi-modality approach.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 16:47:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bird", "Jordan J.", ""], ["Faria", "Diego R.", ""], ["Premebida", "Cristiano", ""], ["Ek\u00e1rt", "Anik\u00f3", ""], ["Vogiatzis", "George", ""]]}, {"id": "2007.10202", "submitter": "Wei Mao", "authors": "Wei Mao, Jiaming Zhang, Kailun Yang, Rainer Stiefelhagen", "title": "Can we cover navigational perception needs of the visually impaired by\n  panoptic segmentation?", "comments": "14 pages, 6 Figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigational perception for visually impaired people has been substantially\npromoted by both classic and deep learning based segmentation methods. In\nclassic visual recognition methods, the segmentation models are mostly\nobject-dependent, which means a specific algorithm has to be devised for the\nobject of interest. In contrast, deep learning based models such as instance\nsegmentation and semantic segmentation allow to individually recognize part of\nthe entire scene, namely things or stuff, for blind individuals. However, both\nof them can not provide a holistic understanding of the surroundings for the\nvisually impaired. Panoptic segmentation is a newly proposed visual model with\nthe aim of unifying semantic segmentation and instance segmentation. Motivated\nby that, we propose to utilize panoptic segmentation as an approach to\nnavigating visually impaired people by offering both things and stuff awareness\nin the proximity of the visually impaired. We demonstrate that panoptic\nsegmentation is able to equip the visually impaired with a holistic real-world\nscene perception through a wearable assistive system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:35:58 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mao", "Wei", ""], ["Zhang", "Jiaming", ""], ["Yang", "Kailun", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2007.10221", "submitter": "Fei Ye", "authors": "Fei Ye and Adrian G. Bors", "title": "Learning latent representations across multiple data domains using\n  Lifelong VAEGAN", "comments": "Accepted as a conference paper at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of catastrophic forgetting occurs in deep learning models trained\non multiple databases in a sequential manner. Recently, generative replay\nmechanisms (GRM), have been proposed to reproduce previously learned knowledge\naiming to reduce the forgetting. However, such approaches lack an appropriate\ninference model and therefore can not provide latent representations of data.\nIn this paper, we propose a novel lifelong learning approach, namely the\nLifelong VAEGAN (L-VAEGAN), which not only induces a powerful generative replay\nnetwork but also learns meaningful latent representations, benefiting\nrepresentation learning. L-VAEGAN can allow to automatically embed the\ninformation associated with different domains into several clusters in the\nlatent space, while also capturing semantically meaningful shared latent\nvariables, across different data domains. The proposed model supports many\ndownstream tasks that traditional generative replay methods can not, including\ninterpolation and inference across different data domains.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:08:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ye", "Fei", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2007.10232", "submitter": "Zahra Sadeghi", "authors": "Zahra Sadeghi", "title": "The Effect of Top-Down Attention in Occluded Object Recognition", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is concerned with the top-down visual processing benefit in the\ntask of occluded object recognition. To this end, a psychophysical experiment\nis designed and carried out which aimed at investigating the effect of\nconsistency of contextual information on the recognition of objects which are\npartially occluded. The results demonstrate the facilitative impact of\nconsistent contextual clues on the task of object recognition in presence of\nocclusion.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:09:41 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sadeghi", "Zahra", ""]]}, {"id": "2007.10233", "submitter": "Divya Shanmugam", "authors": "Roshni Sahoo, Divya Shanmugam, John Guttag", "title": "Unsupervised Domain Adaptation in the Absence of Source Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current unsupervised domain adaptation methods can address many types of\ndistribution shift, but they assume data from the source domain is freely\navailable. As the use of pre-trained models becomes more prevalent, it is\nreasonable to assume that source data is unavailable. We propose an\nunsupervised method for adapting a source classifier to a target domain that\nvaries from the source domain along natural axes, such as brightness and\ncontrast. Our method only requires access to unlabeled target instances and the\nsource classifier. We validate our method in scenarios where the distribution\nshift involves brightness, contrast, and rotation and show that it outperforms\nfine-tuning baselines in scenarios with limited labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:22:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sahoo", "Roshni", ""], ["Shanmugam", "Divya", ""], ["Guttag", "John", ""]]}, {"id": "2007.10243", "submitter": "Matteo Fabbri Ing.", "authors": "Matteo Fabbri, Fabio Lanzi, Riccardo Gasparini, Simone Calderara,\n  Lorenzo Baraldi, Rita Cucchiara", "title": "Inter-Homines: Distance-Based Risk Estimation for Human Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we report our proposal for modeling the risk of possible\ncontagiousity in a given area monitored by RGB cameras where people freely move\nand interact. Our system, called Inter-Homines, evaluates in real-time the\ncontagion risk in a monitored area by analyzing video streams: it is able to\nlocate people in 3D space, calculate interpersonal distances and predict risk\nlevels by building dynamic maps of the monitored area. Inter-Homines works both\nindoor and outdoor, in public and private crowded areas. The software is\napplicable to already installed cameras or low-cost cameras on industrial PCs,\nequipped with an additional embedded edge-AI system for temporary measurements.\nFrom the AI-side, we exploit a robust pipeline for real-time people detection\nand localization in the ground plane by homographic transformation based on\nstate-of-the-art computer vision algorithms; it is a combination of a people\ndetector and a pose estimator. From the risk modeling side, we propose a\nparametric model for a spatio-temporal dynamic risk estimation, that, validated\nby epidemiologists, could be useful for safety monitoring the acceptance of\nsocial distancing prevention measures by predicting the risk level of the\nscene.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:32:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fabbri", "Matteo", ""], ["Lanzi", "Fabio", ""], ["Gasparini", "Riccardo", ""], ["Calderara", "Simone", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2007.10247", "submitter": "Yanhong Zeng", "authors": "Yanhong Zeng, Jianlong Fu, Hongyang Chao", "title": "Learning Joint Spatial-Temporal Transformations for Video Inpainting", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality video inpainting that completes missing regions in video frames\nis a promising yet challenging task. State-of-the-art approaches adopt\nattention models to complete a frame by searching missing contents from\nreference frames, and further complete whole videos frame by frame. However,\nthese approaches can suffer from inconsistent attention results along spatial\nand temporal dimensions, which often leads to blurriness and temporal artifacts\nin videos. In this paper, we propose to learn a joint Spatial-Temporal\nTransformer Network (STTN) for video inpainting. Specifically, we\nsimultaneously fill missing regions in all input frames by self-attention, and\npropose to optimize STTN by a spatial-temporal adversarial loss. To show the\nsuperiority of the proposed model, we conduct both quantitative and qualitative\nevaluations by using standard stationary masks and more realistic moving object\nmasks. Demo videos are available at https://github.com/researchmm/STTN.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:35:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zeng", "Yanhong", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""]]}, {"id": "2007.10252", "submitter": "Xingjian Li", "authors": "Xingjian Li, Haoyi Xiong, Haozhe An, Chengzhong Xu, Dejing Dou", "title": "XMixup: Efficient Transfer Learning with Auxiliary Samples by\n  Cross-domain Mixup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from large source datasets is an effective way to\nfine-tune the deep neural networks of the target task with a small sample size.\nA great number of algorithms have been proposed to facilitate deep transfer\nlearning, and these techniques could be generally categorized into two groups -\nRegularized Learning of the target task using models that have been pre-trained\nfrom source datasets, and Multitask Learning with both source and target\ndatasets to train a shared backbone neural network. In this work, we aim to\nimprove the multitask paradigm for deep transfer learning via Cross-domain\nMixup (XMixup). While the existing multitask learning algorithms need to run\nbackpropagation over both the source and target datasets and usually consume a\nhigher gradient complexity, XMixup transfers the knowledge from source to\ntarget tasks more efficiently: for every class of the target task, XMixup\nselects the auxiliary samples from the source dataset and augments training\nsamples via the simple mixup strategy. We evaluate XMixup over six real world\ntransfer learning datasets. Experiment results show that XMixup improves the\naccuracy by 1.9% on average. Compared with other state-of-the-art transfer\nlearning approaches, XMixup costs much less training time while still obtains\nhigher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:42:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Xingjian", ""], ["Xiong", "Haoyi", ""], ["An", "Haozhe", ""], ["Xu", "Chengzhong", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.10283", "submitter": "Thomas Truong", "authors": "Thomas Truong and Svetlana Yanushkevich", "title": "Relatable Clothing: Detecting Visual Relationships between People and\n  Clothing", "comments": "7 pages, 7 figures, accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting visual relationships between people and clothing in an image has\nbeen a relatively unexplored problem in the field of computer vision and\nbiometrics. The lack readily available public dataset for ``worn'' and\n``unworn'' classification has slowed the development of solutions for this\nproblem. We present the release of the Relatable Clothing Dataset which\ncontains 35287 person-clothing pairs and segmentation masks for the development\nof ``worn'' and ``unworn'' classification models. Additionally, we propose a\nnovel soft attention unit for performing ``worn'' and ``unworn'' classification\nusing deep neural networks. The proposed soft attention models have an accuracy\nof upward $98.55\\% \\pm 0.35\\%$ on the Relatable Clothing Dataset and\ndemonstrate high generalizable, allowing us to classify unseen articles of\nclothing such as high visibility vests as ``worn'' or ``unworn''.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:10:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Truong", "Thomas", ""], ["Yanushkevich", "Svetlana", ""]]}, {"id": "2007.10294", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed and Matthew Fisher and Noam Aigerman and Vladimir G.\n  Kim", "title": "Coupling Explicit and Implicit Surface Representations for Generative 3D\n  Modeling", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural architecture for representing 3D surfaces, which\nharnesses two complementary shape representations: (i) an explicit\nrepresentation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an\nimplicit-function representation, i.e., a scalar function over the 3D volume,\nwith its levels denoting surfaces. We make these two representations\nsynergistic by introducing novel consistency losses that ensure that the\nsurface created from the atlas aligns with the level-set of the implicit\nfunction. Our hybrid architecture outputs results which are superior to the\noutput of the two equivalent single-representation networks, yielding smoother\nexplicit surfaces with more accurate normals, and a more accurate implicit\noccupancy function. Additionally, our surface reconstruction step can directly\nleverage the explicit atlas-based representation. This process is\ncomputationally efficient, and can be directly used by differentiable\nrasterizers, enabling training our hybrid representation with image-based\nlosses.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:24:51 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 02:10:58 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poursaeed", "Omid", ""], ["Fisher", "Matthew", ""], ["Aigerman", "Noam", ""], ["Kim", "Vladimir G.", ""]]}, {"id": "2007.10298", "submitter": "Darshan Gera", "authors": "Darshan Gera and S Balasubramanian", "title": "Landmark Guidance Independent Spatio-channel Attention and Complementary\n  Context Information based Facial Expression Recognition", "comments": "A couple of reference citations corrected, few details added and code\n  link provided", "journal-ref": "Pattern Recognition Letters 145 (2021)", "doi": "10.1016/j.patrec.2021.01.029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend to recognize facial expressions in the real-world scenario is\nto deploy attention based convolutional neural networks (CNNs) locally to\nsignify the importance of facial regions and, combine it with global facial\nfeatures and/or other complementary context information for performance gain.\nHowever, in the presence of occlusions and pose variations, different channels\nrespond differently, and further that the response intensity of a channel\ndiffer across spatial locations. Also, modern facial expression\nrecognition(FER) architectures rely on external sources like landmark detectors\nfor defining attention. Failure of landmark detector will have a cascading\neffect on FER. Additionally, there is no emphasis laid on the relevance of\nfeatures that are input to compute complementary context information.\nLeveraging on the aforementioned observations, an end-to-end architecture for\nFER is proposed in this work that obtains both local and global attention per\nchannel per spatial location through a novel spatio-channel attention net\n(SCAN), without seeking any information from the landmark detectors. SCAN is\ncomplemented by a complementary context information (CCI) branch. Further,\nusing efficient channel attention (ECA), the relevance of features input to CCI\nis also attended to. The representation learnt by the proposed architecture is\nrobust to occlusions and pose variations. Robustness and superior performance\nof the proposed model is demonstrated on both in-lab and in-the-wild datasets\n(AffectNet, FERPlus, RAF-DB, FED-RO, SFEW, CK+, Oulu-CASIA and JAFFE) along\nwith a couple of constructed face mask datasets resembling masked faces in\nCOVID-19 scenario. Codes are publicly available at\nhttps://github.com/1980x/SCAN-CCI-FER\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:33:32 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 14:50:25 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Gera", "Darshan", ""], ["Balasubramanian", "S", ""]]}, {"id": "2007.10300", "submitter": "Or Litany", "authors": "Shubham Tulsiani, Or Litany, Charles R. Qi, He Wang, Leonidas J.\n  Guibas", "title": "Object-Centric Multi-View Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for aggregating a sparse set of views of an object in\norder to compute a semi-implicit 3D representation in the form of a volumetric\nfeature grid. Key to our approach is an object-centric canonical 3D coordinate\nsystem into which views can be lifted, without explicit camera pose estimation,\nand then combined -- in a manner that can accommodate a variable number of\nviews and is view order independent. We show that computing a symmetry-aware\nmapping from pixels to the canonical coordinate system allows us to better\npropagate information to unseen regions, as well as to robustly overcome pose\nambiguities during inference. Our aggregate representation enables us to\nperform 3D inference tasks like volumetric reconstruction and novel view\nsynthesis, and we use these tasks to demonstrate the benefits of our\naggregation approach as compared to implicit or camera-centric alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:38:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 05:17:19 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Litany", "Or", ""], ["Qi", "Charles R.", ""], ["Wang", "He", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2007.10315", "submitter": "Yang Zou", "authors": "Yang Zou, Xiaodong Yang, Zhiding Yu, B.V.K. Vijaya Kumar, Jan Kautz", "title": "Joint Disentangling and Adaptation for Cross-Domain Person\n  Re-Identification", "comments": "ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a significant progress has been witnessed in supervised person\nre-identification (re-id), it remains challenging to generalize re-id models to\nnew domains due to the huge domain gaps. Recently, there has been a growing\ninterest in using unsupervised domain adaptation to address this scalability\nissue. Existing methods typically conduct adaptation on the representation\nspace that contains both id-related and id-unrelated factors, thus inevitably\nundermining the adaptation efficacy of id-related features. In this paper, we\nseek to improve adaptation by purifying the representation space to be adapted.\nTo this end, we propose a joint learning framework that disentangles\nid-related/unrelated features and enforces adaptation to work on the id-related\nfeature space exclusively. Our model involves a disentangling module that\nencodes cross-domain images into a shared appearance space and two separate\nstructure spaces, and an adaptation module that performs adversarial alignment\nand self-training on the shared appearance space. The two modules are\nco-designed to be mutually beneficial. Extensive experiments demonstrate that\nthe proposed joint learning framework outperforms the state-of-the-art methods\nby clear margins.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:57:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zou", "Yang", ""], ["Yang", "Xiaodong", ""], ["Yu", "Zhiding", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Kautz", "Jan", ""]]}, {"id": "2007.10319", "submitter": "Ji Lin", "authors": "Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, Song Han", "title": "MCUNet: Tiny Deep Learning on IoT Devices", "comments": "NeurIPS 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning on tiny IoT devices based on microcontroller units (MCU) is\nappealing but challenging: the memory of microcontrollers is 2-3 orders of\nmagnitude smaller even than mobile phones. We propose MCUNet, a framework that\njointly designs the efficient neural architecture (TinyNAS) and the lightweight\ninference engine (TinyEngine), enabling ImageNet-scale inference on\nmicrocontrollers. TinyNAS adopts a two-stage neural architecture search\napproach that first optimizes the search space to fit the resource constraints,\nthen specializes the network architecture in the optimized search space.\nTinyNAS can automatically handle diverse constraints (i.e.device, latency,\nenergy, memory) under low search costs.TinyNAS is co-designed with TinyEngine,\na memory-efficient inference library to expand the search space and fit a\nlarger model. TinyEngine adapts the memory scheduling according to the overall\nnetwork topology rather than layer-wise optimization, reducing the memory usage\nby 4.8x, and accelerating the inference by 1.7-3.3x compared to TF-Lite Micro\nand CMSIS-NN. MCUNet is the first to achieves >70% ImageNet top1 accuracy on an\noff-the-shelf commercial microcontroller, using 3.5x less SRAM and 5.7x less\nFlash compared to quantized MobileNetV2 and ResNet-18. On visual&audio wake\nwords tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4x faster\nthan MobileNetV2 and ProxylessNAS-based solutions with 3.7-4.1x smaller peak\nSRAM. Our study suggests that the era of always-on tiny machine learning on IoT\ndevices has arrived. Code and models can be found here: https://tinyml.mit.edu.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:59:01 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 17:29:28 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Lin", "Ji", ""], ["Chen", "Wei-Ming", ""], ["Lin", "Yujun", ""], ["Cohn", "John", ""], ["Gan", "Chuang", ""], ["Han", "Song", ""]]}, {"id": "2007.10321", "submitter": "Xitong Yang", "authors": "Xitong Yang, Xiaodong Yang, Sifei Liu, Deqing Sun, Larry Davis, Jan\n  Kautz", "title": "Hierarchical Contrastive Motion Learning for Video Action Recognition", "comments": "Updated version with analysis on motion features at different levels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One central question for video action recognition is how to model motion. In\nthis paper, we present hierarchical contrastive motion learning, a new\nself-supervised learning framework to extract effective motion representations\nfrom raw video frames. Our approach progressively learns a hierarchy of motion\nfeatures that correspond to different abstraction levels in a network. This\nhierarchical design bridges the semantic gap between low-level motion cues and\nhigh-level recognition tasks, and promotes the fusion of appearance and motion\ninformation at multiple levels. At each level, an explicit motion\nself-supervision is provided via contrastive learning to enforce the motion\nfeatures at the current level to predict the future ones at the previous level.\nThus, the motion features at higher levels are trained to gradually capture\nsemantic dynamics and evolve more discriminative for action recognition. Our\nmotion learning module is lightweight and flexible to be embedded into various\nbackbone networks. Extensive experiments on four benchmarks show that the\nproposed approach consistently achieves superior results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:59:22 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 15:47:45 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Yang", "Xitong", ""], ["Yang", "Xiaodong", ""], ["Liu", "Sifei", ""], ["Sun", "Deqing", ""], ["Davis", "Larry", ""], ["Kautz", "Jan", ""]]}, {"id": "2007.10323", "submitter": "Yue Wang", "authors": "Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline\n  Pantofaru, Thomas Funkhouser, Justin Solomon", "title": "Pillar-based Object Detection for Autonomous Driving", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and flexible object detection framework optimized for\nautonomous driving. Building on the observation that point clouds in this\napplication are extremely sparse, we propose a practical pillar-based approach\nto fix the imbalance issue caused by anchors. In particular, our algorithm\nincorporates a cylindrical projection into multi-view feature learning,\npredicts bounding box parameters per pillar rather than per point or per\nanchor, and includes an aligned pillar-to-point projection module to improve\nthe final prediction. Our anchor-free approach avoids hyperparameter search\nassociated with past methods, simplifying 3D object detection while\nsignificantly improving upon state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:59:28 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 21:13:04 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Yue", ""], ["Fathi", "Alireza", ""], ["Kundu", "Abhijit", ""], ["Ross", "David", ""], ["Pantofaru", "Caroline", ""], ["Funkhouser", "Thomas", ""], ["Solomon", "Justin", ""]]}, {"id": "2007.10361", "submitter": "Mikiya Shibuya", "authors": "Mikiya Shibuya, Shinya Sumikura, and Ken Sakurada", "title": "Privacy Preserving Visual SLAM", "comments": "ECCV2020, Project: https://xdspacelab.github.io/lcvslam/ , Video:\n  https://youtu.be/gEtUqnHx83w", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a privacy-preserving Visual SLAM framework for estimating\ncamera poses and performing bundle adjustment with mixed line and point clouds\nin real time. Previous studies have proposed localization methods to estimate a\ncamera pose using a line-cloud map for a single image or a reconstructed point\ncloud. These methods offer a scene privacy protection against the inversion\nattacks by converting a point cloud to a line cloud, which reconstruct the\nscene images from the point cloud. However, they are not directly applicable to\na video sequence because they do not address computational efficiency. This is\na critical issue to solve for estimating camera poses and performing bundle\nadjustment with mixed line and point clouds in real time. Moreover, there has\nbeen no study on a method to optimize a line-cloud map of a server with a point\ncloud reconstructed from a client video because any observation points on the\nimage coordinates are not available to prevent the inversion attacks, namely\nthe reversibility of the 3D lines. The experimental results with synthetic and\nreal data show that our Visual SLAM framework achieves the intended\nprivacy-preserving formation and real-time performance using a line-cloud map.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:00:06 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 07:34:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Shibuya", "Mikiya", ""], ["Sumikura", "Shinya", ""], ["Sakurada", "Ken", ""]]}, {"id": "2007.10379", "submitter": "Yujun Shen", "authors": "Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou", "title": "Generative Hierarchical Features from Synthesizing Images", "comments": "CVPR 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently advanced image synthesis\nby learning the underlying distribution of the observed data. However, how the\nfeatures learned from solving the task of image generation are applicable to\nother vision tasks remains seldom explored. In this work, we show that learning\nto synthesize images can bring remarkable hierarchical visual features that are\ngeneralizable across a wide range of applications. Specifically, we consider\nthe pre-trained StyleGAN generator as a learned loss function and utilize its\nlayer-wise representation to train a novel hierarchical encoder. The visual\nfeature produced by our encoder, termed as Generative Hierarchical Feature\n(GH-Feat), has strong transferability to both generative and discriminative\ntasks, including image editing, image harmonization, image classification, face\nverification, landmark detection, and layout prediction. Extensive qualitative\nand quantitative experimental results demonstrate the appealing performance of\nGH-Feat.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:04:14 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:21:08 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xu", "Yinghao", ""], ["Shen", "Yujun", ""], ["Zhu", "Jiapeng", ""], ["Yang", "Ceyuan", ""], ["Zhou", "Bolei", ""]]}, {"id": "2007.10396", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu and Kalyanmoy Deb and Erik Goodman and Wolfgang Banzhaf and\n  Vishnu Naresh Boddeti", "title": "NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural\n  Architecture Search", "comments": "Accepted for oral presentation at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient NAS algorithm for generating\ntask-specific models that are competitive under multiple competing objectives.\nIt comprises of two surrogates, one at the architecture level to improve sample\nefficiency and one at the weights level, through a supernet, to improve\ngradient descent training efficiency. On standard benchmark datasets (C10,\nC100, ImageNet), the resulting models, dubbed NSGANetV2, either match or\noutperform models from existing approaches with the search being orders of\nmagnitude more sample efficient. Furthermore, we demonstrate the effectiveness\nand versatility of the proposed method on six diverse non-standard datasets,\ne.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases,\nNSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that\nNAS can be a viable alternative to conventional transfer learning approaches in\nhandling diverse scenarios such as small-scale or fine-grained datasets. Code\nis available at https://github.com/mikelzc1990/nsganetv2\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:30:11 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Lu", "Zhichao", ""], ["Deb", "Kalyanmoy", ""], ["Goodman", "Erik", ""], ["Banzhaf", "Wolfgang", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "2007.10408", "submitter": "Zhengyang Shen", "authors": "Zhengyang Shen, Lingshen He, Zhouchen Lin, Jinwen Ma", "title": "PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions", "comments": "Accepted by ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that incorporating equivariance into neural network\narchitectures is very helpful, and there have been some works investigating the\nequivariance of networks under group actions. However, as digital images and\nfeature maps are on the discrete meshgrid, corresponding\nequivariance-preserving transformation groups are very limited. In this work,\nwe deal with this issue from the connection between convolutions and partial\ndifferential operators (PDOs). In theory, assuming inputs to be smooth, we\ntransform PDOs and propose a system which is equivariant to a much more general\ncontinuous group, the $n$-dimension Euclidean group. In implementation, we\ndiscretize the system using the numerical schemes of PDOs, deriving\napproximately equivariant convolutions (PDO-eConvs). Theoretically, the\napproximation error of PDO-eConvs is of the quadratic order. It is the first\ntime that the error analysis is provided when the equivariance is approximate.\nExtensive experiments on rotated MNIST and natural image classification show\nthat PDO-eConvs perform competitively yet use parameters much more efficiently.\nParticularly, compared with Wide ResNets, our methods result in better results\nusing only 12.6% parameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:57:26 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 14:19:55 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shen", "Zhengyang", ""], ["He", "Lingshen", ""], ["Lin", "Zhouchen", ""], ["Ma", "Jinwen", ""]]}, {"id": "2007.10416", "submitter": "Hanqing Chao", "authors": "Hanqing Chao, Xi Fang, Jiajin Zhang, Fatemeh Homayounieh, Chiara D.\n  Arru, Subba R. Digumarthy, Rosa Babaei, Hadi K. Mobin, Iman Mohseni, Luca\n  Saba, Alessandro Carriero, Zeno Falaschi, Alessio Pasche, Ge Wang, Mannudeep\n  K. Kalra, Pingkun Yan", "title": "Integrative Analysis for COVID-19 Patient Outcome Prediction", "comments": "This paper has been accepted by Medical Image Analysis. The source\n  code of this work is available at\n  https://github.com/DIAL-RPI/COVID19-ICUPrediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While image analysis of chest computed tomography (CT) for COVID-19 diagnosis\nhas been intensively studied, little work has been performed for image-based\npatient outcome prediction. Management of high-risk patients with early\nintervention is a key to lower the fatality rate of COVID-19 pneumonia, as a\nmajority of patients recover naturally. Therefore, an accurate prediction of\ndisease progression with baseline imaging at the time of the initial\npresentation can help in patient management. In lieu of only size and volume\ninformation of pulmonary abnormalities and features through deep learning based\nimage segmentation, here we combine radiomics of lung opacities and non-imaging\nfeatures from demographic data, vital signs, and laboratory findings to predict\nneed for intensive care unit (ICU) admission. To our knowledge, this is the\nfirst study that uses holistic information of a patient including both imaging\nand non-imaging data for outcome prediction. The proposed methods were\nthoroughly evaluated on datasets separately collected from three hospitals, one\nin the United States, one in Iran, and another in Italy, with a total 295\npatients with reverse transcription polymerase chain reaction (RT-PCR) assay\npositive COVID-19 pneumonia. Our experimental results demonstrate that adding\nnon-imaging features can significantly improve the performance of prediction to\nachieve AUC up to 0.884 and sensitivity as high as 96.1%, which can be valuable\nto provide clinical decision support in managing COVID-19 patients. Our methods\nmay also be applied to other lung diseases including but not limited to\ncommunity acquired pneumonia. The source code of our work is available at\nhttps://github.com/DIAL-RPI/COVID19-ICUPrediction.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 19:08:50 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 19:44:07 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Chao", "Hanqing", ""], ["Fang", "Xi", ""], ["Zhang", "Jiajin", ""], ["Homayounieh", "Fatemeh", ""], ["Arru", "Chiara D.", ""], ["Digumarthy", "Subba R.", ""], ["Babaei", "Rosa", ""], ["Mobin", "Hadi K.", ""], ["Mohseni", "Iman", ""], ["Saba", "Luca", ""], ["Carriero", "Alessandro", ""], ["Falaschi", "Zeno", ""], ["Pasche", "Alessio", ""], ["Wang", "Ge", ""], ["Kalra", "Mannudeep K.", ""], ["Yan", "Pingkun", ""]]}, {"id": "2007.10453", "submitter": "Philipp Erler", "authors": "Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Michael Wimmer,\n  Niloy J. Mitra", "title": "Points2Surf: Learning Implicit Surfaces from Point Cloud Patches", "comments": "To be published at ECCV 2020 Repository:\n  https://github.com/ErlerPhilipp/points2surf", "journal-ref": "Computer Vision -- ECCV 2020, 108--124", "doi": "10.1007/978-3-030-58558-7_7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key step in any scanning-based asset creation workflow is to convert\nunordered point clouds to a surface. Classical methods (e.g., Poisson\nreconstruction) start to degrade in the presence of noisy and partial scans.\nHence, deep learning based methods have recently been proposed to produce\ncomplete surfaces, even from partial scans. However, such data-driven methods\nstruggle to generalize to new shapes with large geometric and topological\nvariations. We present Points2Surf, a novel patch-based learning framework that\nproduces accurate surfaces directly from raw scans without normals. Learning a\nprior over a combination of detailed local patches and coarse global\ninformation improves generalization performance and reconstruction accuracy.\nOur extensive comparison on both synthetic and real data demonstrates a clear\nadvantage of our method over state-of-the-art alternatives on previously unseen\nclasses (on average, Points2Surf brings down reconstruction error by 30\\% over\nSPR and by 270\\%+ over deep learning based SotA methods) at the cost of longer\ncomputation times and a slight increase in small-scale topological noise in\nsome cases. Our source code, pre-trained model, and dataset are available on:\nhttps://github.com/ErlerPhilipp/points2surf\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:25:39 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Erler", "Philipp", ""], ["Guerrero", "Paul", ""], ["Ohrhallinger", "Stefan", ""], ["Wimmer", "Michael", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2007.10466", "submitter": "Lakshmanan Nataraj", "authors": "Michael Goebel, Lakshmanan Nataraj, Tejaswi Nanjundaswamy, Tajuddin\n  Manhar Mohammed, Shivkumar Chandrasekaran and B.S. Manjunath", "title": "Detection, Attribution and Localization of GAN Generated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have led to the\ncreation of realistic-looking digital images that pose a major challenge to\ntheir detection by humans or computers. GANs are used in a wide range of tasks,\nfrom modifying small attributes of an image (StarGAN [14]), transferring\nattributes between image pairs (CycleGAN [91]), as well as generating entirely\nnew images (ProGAN [36], StyleGAN [37], SPADE/GauGAN [64]). In this paper, we\npropose a novel approach to detect, attribute and localize GAN generated images\nthat combines image features with deep learning methods. For every image,\nco-occurrence matrices are computed on neighborhood pixels of RGB channels in\ndifferent directions (horizontal, vertical and diagonal). A deep learning\nnetwork is then trained on these features to detect, attribute and localize\nthese GAN generated/manipulated images. A large scale evaluation of our\napproach on 5 GAN datasets comprising over 2.76 million images (ProGAN,\nStarGAN, CycleGAN, StyleGAN and SPADE/GauGAN) shows promising results in\ndetecting GAN generated images.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:49:34 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Goebel", "Michael", ""], ["Nataraj", "Lakshmanan", ""], ["Nanjundaswamy", "Tejaswi", ""], ["Mohammed", "Tajuddin Manhar", ""], ["Chandrasekaran", "Shivkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2007.10467", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang and Shuiwang Ji", "title": "Second-Order Pooling for Graph Neural Networks", "comments": "12 pages, 2 figures,\n  https://www.computer.org/csdl/journal/tp/5555/01/09104936/1kj0O2A1yBa", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.2999032", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks have achieved great success in learning node\nrepresentations for graph tasks such as node classification and link\nprediction. Graph representation learning requires graph pooling to obtain\ngraph representations from node representations. It is challenging to develop\ngraph pooling methods due to the variable sizes and isomorphic structures of\ngraphs. In this work, we propose to use second-order pooling as graph pooling,\nwhich naturally solves the above challenges. In addition, compared to existing\ngraph pooling methods, second-order pooling is able to use information from all\nnodes and collect second-order statistics, making it more powerful. We show\nthat direct use of second-order pooling with graph neural networks leads to\npractical problems. To overcome these problems, we propose two novel global\ngraph pooling methods based on second-order pooling; namely, bilinear mapping\nand attentional second-order pooling. In addition, we extend attentional\nsecond-order pooling to hierarchical graph pooling for more flexible use in\nGNNs. We perform thorough experiments on graph classification tasks to\ndemonstrate the effectiveness and superiority of our proposed methods.\nExperimental results show that our methods improve the performance\nsignificantly and consistently.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:52:36 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2007.10469", "submitter": "Luis Pineda", "authors": "Luis Pineda, Sumana Basu, Adriana Romero, Roberto Calandra, Michal\n  Drozdzal", "title": "Active MR k-space Sampling with Reinforcement Learning", "comments": "Presented at the 23rd International Conference on Medical Image\n  Computing and Computer Assisted Intervention, MICCAI 2020", "journal-ref": "LNCS vol. 12262 (2020) 23-33", "doi": "10.1007/978-3-030-59713-9_3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have recently shown great promise in accelerating\nmagnetic resonance image (MRI) acquisition. The majority of existing work have\nfocused on designing better reconstruction models given a pre-determined\nacquisition trajectory, ignoring the question of trajectory optimization. In\nthis paper, we focus on learning acquisition trajectories given a fixed image\nreconstruction model. We formulate the problem as a sequential decision process\nand propose the use of reinforcement learning to solve it. Experiments on a\nlarge scale public MRI dataset of knees show that our proposed models\nsignificantly outperform the state-of-the-art in active MRI acquisition, over a\nlarge range of acceleration factors.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:54:57 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 21:54:14 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Pineda", "Luis", ""], ["Basu", "Sumana", ""], ["Romero", "Adriana", ""], ["Calandra", "Roberto", ""], ["Drozdzal", "Michal", ""]]}, {"id": "2007.10479", "submitter": "Xinggang Wang", "authors": "Jiwei Xu and Xinggang Wang and Bin Feng and Wenyu Liu", "title": "Deep multi-metric learning for text-independent speaker verification", "comments": null, "journal-ref": "Neurocomputing, Volume 410, 14 October 2020, Pages 394-400", "doi": "10.1016/j.neucom.2020.06.045", "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-independent speaker verification is an important artificial intelligence\nproblem that has a wide spectrum of applications, such as criminal\ninvestigation, payment certification, and interest-based customer services. The\npurpose of text-independent speaker verification is to determine whether two\ngiven uncontrolled utterances originate from the same speaker or not.\nExtracting speech features for each speaker using deep neural networks is a\npromising direction to explore and a straightforward solution is to train the\ndiscriminative feature extraction network by using a metric learning loss\nfunction. However, a single loss function often has certain limitations. Thus,\nwe use deep multi-metric learning to address the problem and introduce three\ndifferent losses for this problem, i.e., triplet loss, n-pair loss and angular\nloss. The three loss functions work in a cooperative way to train a feature\nextraction network equipped with Residual connections and\nsqueeze-and-excitation attention. We conduct experiments on the large-scale\n\\texttt{VoxCeleb2} dataset, which contains over a million utterances from over\n$6,000$ speakers, and the proposed deep neural network obtains an equal error\nrate of $3.48\\%$, which is a very competitive result. Codes for both training\nand testing and pretrained models are available at\n\\url{https://github.com/GreatJiweix/DmmlTiSV}, which is the first publicly\navailable code repository for large-scale text-independent speaker verification\nwith performance on par with the state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:19:44 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Xu", "Jiwei", ""], ["Wang", "Xinggang", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2007.10485", "submitter": "Nupur Thakur", "authors": "Yuzhen Ding, Nupur Thakur, Baoxin Li", "title": "AdvFoolGen: Creating Persistent Troubles for Deep Classifiers", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches have shown that deep neural networks are vulnerable to malicious\nattacks, where adversarial images are created to trick a network into\nmisclassification even if the images may give rise to totally different labels\nby human eyes. To make deep networks more robust to such attacks, many defense\nmechanisms have been proposed in the literature, some of which are quite\neffective for guarding against typical attacks. In this paper, we present a new\nblack-box attack termed AdvFoolGen, which can generate attacking images from\nthe same feature space as that of the natural images, so as to keep baffling\nthe network even though state-of-the-art defense mechanisms have been applied.\nWe systematically evaluate our model by comparing with well-established attack\nalgorithms. Through experiments, we demonstrate the effectiveness and\nrobustness of our attack in the face of state-of-the-art defense techniques and\nunveil the potential reasons for its effectiveness through principled analysis.\nAs such, AdvFoolGen contributes to understanding the vulnerability of deep\nnetworks from a new perspective and may, in turn, help in developing and\nevaluating new defense mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:27:41 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ding", "Yuzhen", ""], ["Thakur", "Nupur", ""], ["Li", "Baoxin", ""]]}, {"id": "2007.10495", "submitter": "Andr\\'as Horv\\'ath", "authors": "Andr\\'as Horv\\'ath", "title": "Sorted Pooling in Convolutional Networks for One-shot Learning", "comments": "Old paper submitted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present generalized versions of the commonly used maximum pooling\noperation: $k$th maximum and sorted pooling operations which selects the $k$th\nlargest response in each pooling region, selecting locally consistent features\nof the input images. This method is able to increase the generalization power\nof a network and can be used to decrease training time and error rate of\nnetworks and it can significantly improve accuracy in case of training\nscenarios where the amount of available data is limited, like one-shot learning\nscenarios\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:45:37 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Horv\u00e1th", "Andr\u00e1s", ""]]}, {"id": "2007.10500", "submitter": "Min Soo Kim", "authors": "Min Soo Kim, Alberto A. Del Barrio, HyunJin Kim, Nader Bagherzadeh", "title": "The Effects of Approximate Multiplication on Convolutional Neural\n  Networks", "comments": "12 pages, 11 figures, 4 tables, accepted for publication in the IEEE\n  Transactions on Emerging Topics in Computing", "journal-ref": null, "doi": "10.1109/TETC.2021.3050989", "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the effects of approximate multiplication when performing\ninferences on deep convolutional neural networks (CNNs). The approximate\nmultiplication can reduce the cost of the underlying circuits so that CNN\ninferences can be performed more efficiently in hardware accelerators. The\nstudy identifies the critical factors in the convolution, fully-connected, and\nbatch normalization layers that allow more accurate CNN predictions despite the\nerrors from approximate multiplication. The same factors also provide an\narithmetic explanation of why bfloat16 multiplication performs well on CNNs.\nThe experiments are performed with recognized network architectures to show\nthat the approximate multipliers can produce predictions that are nearly as\naccurate as the FP32 references, without additional training. For example, the\nResNet and Inception-v4 models with Mitch-$w$6 multiplication produces Top-5\nerrors that are within 0.2% compared to the FP32 references. A brief cost\ncomparison of Mitch-$w$6 against bfloat16 is presented, where a MAC operation\nsaves up to 80% of energy compared to the bfloat16 arithmetic. The most\nfar-reaching contribution of this paper is the analytical justification that\nmultiplications can be approximated while additions need to be exact in CNN MAC\noperations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:52:41 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 17:06:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kim", "Min Soo", ""], ["Del Barrio", "Alberto A.", ""], ["Kim", "HyunJin", ""], ["Bagherzadeh", "Nader", ""]]}, {"id": "2007.10521", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Hieu Pham, Ye Han, Andy Kuhl, Wade Kent, and Lizhi Wang", "title": "DeepCorn: A Semi-Supervised Deep Learning Method for High-Throughput\n  Image-Based Corn Kernel Counting and Yield Estimation", "comments": "27 pages, 7 figures", "journal-ref": "Knowledge-Based Systems (2021): 106874", "doi": "10.1016/j.knosys.2021.106874", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The success of modern farming and plant breeding relies on accurate and\nefficient collection of data. For a commercial organization that manages large\namounts of crops, collecting accurate and consistent data is a bottleneck. Due\nto limited time and labor, accurately phenotyping crops to record color, head\ncount, height, weight, etc. is severely limited. However, this information,\ncombined with other genetic and environmental factors, is vital for developing\nnew superior crop species that help feed the world's growing population. Recent\nadvances in machine learning, in particular deep learning, have shown promise\nin mitigating this bottleneck. In this paper, we propose a novel deep learning\nmethod for counting on-ear corn kernels in-field to aid in the gathering of\nreal-time data and, ultimately, to improve decision making to maximize yield.\nWe name this approach DeepCorn, and show that this framework is robust under\nvarious conditions. DeepCorn estimates the density of corn kernels in an image\nof corn ears and predicts the number of kernels based on the estimated density\nmap. DeepCorn uses a truncated VGG-16 as a backbone for feature extraction and\nmerges feature maps from multiple scales of the network to make it robust\nagainst image scale variations. We also adopt a semi-supervised learning\napproach to further improve the performance of our proposed method. Our\nproposed method achieves the MAE and RMSE of 41.36 and 60.27 in the corn kernel\ncounting task, respectively. Our experimental results demonstrate the\nsuperiority and effectiveness of our proposed method compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 23:00:39 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 01:37:18 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Khaki", "Saeed", ""], ["Pham", "Hieu", ""], ["Han", "Ye", ""], ["Kuhl", "Andy", ""], ["Kent", "Wade", ""], ["Wang", "Lizhi", ""]]}, {"id": "2007.10538", "submitter": "Yulin Wang", "authors": "Yulin Wang, Gao Huang, Shiji Song, Xuran Pan, Yitong Xia, Cheng Wu", "title": "Regularizing Deep Networks with Semantic Data Augmentation", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). Journal version of arXiv:1909.12220 (NeurIPS 2019).\n  Code is available at\n  https://github.com/blackfeather-wang/ISDA-for-Deep-Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is widely known as a simple yet surprisingly effective\ntechnique for regularizing deep networks. Conventional data augmentation\nschemes, e.g., flipping, translation or rotation, are low-level,\ndata-independent and class-agnostic operations, leading to limited diversity\nfor augmented samples. To this end, we propose a novel semantic data\naugmentation algorithm to complement traditional approaches. The proposed\nmethod is inspired by the intriguing property that deep networks are effective\nin learning linearized features, i.e., certain directions in the deep feature\nspace correspond to meaningful semantic transformations, e.g., changing the\nbackground or view angle of an object. Based on this observation, translating\ntraining samples along many such directions in the feature space can\neffectively augment the dataset for more diversity. To implement this idea, we\nfirst introduce a sampling based method to obtain semantically meaningful\ndirections efficiently. Then, an upper bound of the expected cross-entropy (CE)\nloss on the augmented training set is derived by assuming the number of\naugmented samples goes to infinity, yielding a highly efficient algorithm. In\nfact, we show that the proposed implicit semantic data augmentation (ISDA)\nalgorithm amounts to minimizing a novel robust CE loss, which adds minimal\nextra computational cost to a normal training procedure. In addition to\nsupervised learning, ISDA can be applied to semi-supervised learning tasks\nunder the consistency regularization framework, where ISDA amounts to\nminimizing the upper bound of the expected KL-divergence between the augmented\nfeatures and the original features. Although being simple, ISDA consistently\nimproves the generalization performance of popular deep models (e.g., ResNets\nand DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,\nImageNet, and Cityscapes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 00:32:44 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 00:31:31 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 07:20:42 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 07:30:54 GMT"}, {"version": "v5", "created": "Fri, 4 Jun 2021 09:52:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Yulin", ""], ["Huang", "Gao", ""], ["Song", "Shiji", ""], ["Pan", "Xuran", ""], ["Xia", "Yitong", ""], ["Wu", "Cheng", ""]]}, {"id": "2007.10558", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Dingzeyu Li, and Chenliang Xu", "title": "Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video\n  Parsing", "comments": "ECCV 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new problem, named audio-visual video parsing,\nwhich aims to parse a video into temporal event segments and label them as\neither audible, visible, or both. Such a problem is essential for a complete\nunderstanding of the scene depicted inside a video. To facilitate exploration,\nwe collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual\nvideo parsing in a weakly-supervised manner. This task can be naturally\nformulated as a Multimodal Multiple Instance Learning (MMIL) problem.\nConcretely, we propose a novel hybrid attention network to explore unimodal and\ncross-modal temporal contexts simultaneously. We develop an attentive MMIL\npooling method to adaptively explore useful audio and visual content from\ndifferent temporal extent and modalities. Furthermore, we discover and mitigate\nmodality bias and noisy label issues with an individual-guided learning\nmechanism and label smoothing technique, respectively. Experimental results\nshow that the challenging audio-visual video parsing can be achieved even with\nonly video-level weak labels. Our proposed framework can effectively leverage\nunimodal and cross-modal temporal contexts and alleviate modality bias and\nnoisy labels problems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:53:31 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tian", "Yapeng", ""], ["Li", "Dingzeyu", ""], ["Xu", "Chenliang", ""]]}, {"id": "2007.10570", "submitter": "Jiaqi Yang", "authors": "Jiaqi Yang and Jiahao Chen and Zhiqiang Huang and Siwen Quan and\n  Yanning Zhang and Zhiguo Cao", "title": "3D Correspondence Grouping with Compatibility Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective method for 3D correspondence grouping. The\nobjective is to accurately classify initial correspondences obtained by\nmatching local geometric descriptors into inliers and outliers. Although the\nspatial distribution of correspondences is irregular, inliers are expected to\nbe geometrically compatible with each other. Based on such observation, we\npropose a novel representation for 3D correspondences, dubbed compatibility\nfeature (CF), to describe the consistencies within inliers and inconsistencies\nwithin outliers. CF consists of top-ranked compatibility scores of a candidate\nto other correspondences, which purely relies on robust and rotation-invariant\ngeometric constraints. We then formulate the grouping problem as a\nclassification problem for CF features, which is accomplished via a simple\nmultilayer perceptron (MLP) network. Comparisons with nine state-of-the-art\nmethods on four benchmarks demonstrate that: 1) CF is distinctive, robust, and\nrotation-invariant; 2) our CF-based method achieves the best overall\nperformance and holds good generalization ability.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 02:39:48 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Yang", "Jiaqi", ""], ["Chen", "Jiahao", ""], ["Huang", "Zhiqiang", ""], ["Quan", "Siwen", ""], ["Zhang", "Yanning", ""], ["Cao", "Zhiguo", ""]]}, {"id": "2007.10573", "submitter": "Fan Zhou", "authors": "Fan Zhou, Zhuqing Jiang, Changjian Shui, Boyu Wang and Brahim\n  Chaib-draa", "title": "Domain Generalization with Optimal Transport and Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing knowledge to unseen domains, where data and labels are\nunavailable, is crucial for machine learning models. We tackle the domain\ngeneralization problem to learn from multiple source domains and generalize to\na target domain with unknown statistics. The crucial idea is to extract the\nunderlying invariant features across all the domains. Previous domain\ngeneralization approaches mainly focused on learning invariant features and\nstacking the learned features from each source domain to generalize to a new\ntarget domain while ignoring the label information, which will lead to\nindistinguishable features with an ambiguous classification boundary. For this,\none possible solution is to constrain the label-similarity when extracting the\ninvariant features and to take advantage of the label similarities for\nclass-specific cohesion and separation of features across domains. Therefore we\nadopt optimal transport with Wasserstein distance, which could constrain the\nclass label similarity, for adversarial training and also further deploy a\nmetric learning objective to leverage the label information for achieving\ndistinguishable classification boundary. Empirical results show that our\nproposed method could outperform most of the baselines. Furthermore, ablation\nstudies also demonstrate the effectiveness of each component of our method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 02:56:05 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhou", "Fan", ""], ["Jiang", "Zhuqing", ""], ["Shui", "Changjian", ""], ["Wang", "Boyu", ""], ["Chaib-draa", "Brahim", ""]]}, {"id": "2007.10587", "submitter": "Juhong Min", "authors": "Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho", "title": "Learning to Compose Hypercolumns for Visual Correspondence", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature representation plays a crucial role in visual correspondence, and\nrecent methods for image matching resort to deeply stacked convolutional\nlayers. These models, however, are both monolithic and static in the sense that\nthey typically use a specific level of features, e.g., the output of the last\nlayer, and adhere to it regardless of the images to match. In this work, we\nintroduce a novel approach to visual correspondence that dynamically composes\neffective features by leveraging relevant layers conditioned on the images to\nmatch. Inspired by both multi-layer feature composition in object detection and\nadaptive inference architectures in classification, the proposed method, dubbed\nDynamic Hyperpixel Flow, learns to compose hypercolumn features on the fly by\nselecting a small number of relevant layers from a deep convolutional neural\nnetwork. We demonstrate the effectiveness on the task of semantic\ncorrespondence, i.e., establishing correspondences between images depicting\ndifferent instances of the same object or scene category. Experiments on\nstandard benchmarks show that the proposed method greatly improves matching\nperformance over the state of the art in an adaptive and efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:03:22 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Min", "Juhong", ""], ["Lee", "Jongmin", ""], ["Ponce", "Jean", ""], ["Cho", "Minsu", ""]]}, {"id": "2007.10588", "submitter": "Jinpyo Kim", "authors": "Jinpyo Kim, Wooekun Jung, Hyungmo Kim, Jaejin Lee", "title": "CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical\n  Convolution Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are empirically known to be\ninvariant to moderate translation but not to rotation in image classification.\nThis paper proposes a deep CNN model, called CyCNN, which exploits polar\nmapping of input images to convert rotation to translation. To deal with the\ncylindrical property of the polar coordinates, we replace convolution layers in\nconventional CNNs to cylindrical convolutional (CyConv) layers. A CyConv layer\nexploits the cylindrically sliding windows (CSW) mechanism that vertically\nextends the input-image receptive fields of boundary units in a convolutional\nlayer. We evaluate CyCNN and conventional CNN models for classification tasks\non rotated MNIST, CIFAR-10, and SVHN datasets. We show that if there is no data\naugmentation during training, CyCNN significantly improves classification\naccuracies when compared to conventional CNN models. Our implementation of\nCyCNN is publicly available on https://github.com/mcrl/CyCNN.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:05:35 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kim", "Jinpyo", ""], ["Jung", "Wooekun", ""], ["Kim", "Hyungmo", ""], ["Lee", "Jaejin", ""]]}, {"id": "2007.10591", "submitter": "Jiahong Wu", "authors": "Jiahong Wu, Jianfei Lu, Xinxin Kang, Yiming Zhang, Yinhang Tang,\n  Jianfei Song, Ze Huang, Shenglan Ben, Jiashui Huang, Faen Zhang", "title": "AinnoSeg: Panoramic Segmentation with High Perfomance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoramic segmentation is a scene where image segmentation tasks is more\ndifficult. With the development of CNN networks, panoramic segmentation tasks\nhave been sufficiently developed.However, the current panoramic segmentation\nalgorithms are more concerned with context semantics, but the details of image\nare not processed enough. Moreover, they cannot solve the problems which\ncontains the accuracy of occluded object segmentation,little object\nsegmentation,boundary pixel in object segmentation etc. Aiming to address these\nissues, this paper presents some useful tricks. (a) By changing the basic\nsegmentation model, the model can take into account the large objects and the\nboundary pixel classification of image details. (b) Modify the loss function so\nthat it can take into account the boundary pixels of multiple objects in the\nimage. (c) Use a semi-supervised approach to regain control of the training\nprocess. (d) Using multi-scale training and reasoning. All these operations\nnamed AinnoSeg, AinnoSeg can achieve state-of-art performance on the well-known\ndataset ADE20K.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:16:46 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Wu", "Jiahong", ""], ["Lu", "Jianfei", ""], ["Kang", "Xinxin", ""], ["Zhang", "Yiming", ""], ["Tang", "Yinhang", ""], ["Song", "Jianfei", ""], ["Huang", "Ze", ""], ["Ben", "Shenglan", ""], ["Huang", "Jiashui", ""], ["Zhang", "Faen", ""]]}, {"id": "2007.10593", "submitter": "Nannan Li", "authors": "Nannan Li and Zhenzhong Chen", "title": "Towards Visual Distortion in Black-Box Attacks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3092822", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing adversarial examples in a black-box threat model injures the\noriginal images by introducing visual distortion. In this paper, we propose a\nnovel black-box attack approach that can directly minimize the induced\ndistortion by learning the noise distribution of the adversarial example,\nassuming only loss-oracle access to the black-box network. The quantified\nvisual distortion, which measures the perceptual distance between the\nadversarial example and the original image, is introduced in our loss whilst\nthe gradient of the corresponding non-differentiable loss function is\napproximated by sampling noise from the learned noise distribution. We validate\nthe effectiveness of our attack on ImageNet. Our attack results in much lower\ndistortion when compared to the state-of-the-art black-box attacks and achieves\n$100\\%$ success rate on InceptionV3, ResNet50 and VGG16bn. The code is\navailable at https://github.com/Alina-1997/visual-distortion-in-attack.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:42:43 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 07:35:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Nannan", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2007.10595", "submitter": "Takashi Isobe", "authors": "Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh,\n  Chunjing Xu, Ya-Li Li, Shengjin Wang, Qi Tian", "title": "Video Super-resolution with Temporal Group Attention", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution, which aims at producing a high-resolution video from\nits corresponding low-resolution version, has recently drawn increasing\nattention. In this work, we propose a novel method that can effectively\nincorporate temporal information in a hierarchical way. The input sequence is\ndivided into several groups, with each one corresponding to a kind of frame\nrate. These groups provide complementary information to recover missing details\nin the reference frame, which is further integrated with an attention module\nand a deep intra-group fusion module. In addition, a fast spatial alignment is\nproposed to handle videos with large motion. Extensive results demonstrate the\ncapability of the proposed model in handling videos with various motion. It\nachieves favorable performance against state-of-the-art methods on several\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:54:30 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Isobe", "Takashi", ""], ["Li", "Songjiang", ""], ["Jia", "Xu", ""], ["Yuan", "Shanxin", ""], ["Slabaugh", "Gregory", ""], ["Xu", "Chunjing", ""], ["Li", "Ya-Li", ""], ["Wang", "Shengjin", ""], ["Tian", "Qi", ""]]}, {"id": "2007.10599", "submitter": "Xiang Long", "authors": "Jian Wang, Xiang Long, Yuan Gao, Errui Ding, Shilei Wen", "title": "Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, most of the state-of-the-art human pose estimation methods are\nbased on heatmap regression. The final coordinates of keypoints are obtained by\ndecoding heatmap directly. In this paper, we aim to find a better approach to\nget more accurate localization results. We mainly put forward two suggestions\nfor improvement: 1) different features and methods should be applied for rough\nand accurate localization, 2) relationship between keypoints should be\nconsidered. Specifically, we propose a two-stage graph-based and model-agnostic\nframework, called Graph-PCNN, with a localization subnet and a graph pose\nrefinement module added onto the original heatmap regression network. In the\nfirst stage, heatmap regression network is applied to obtain a rough\nlocalization result, and a set of proposal keypoints, called guided points, are\nsampled. In the second stage, for each guided point, different visual feature\nis extracted by the localization subnet. The relationship between guided points\nis explored by the graph pose refinement module to get more accurate\nlocalization results. Experiments show that Graph-PCNN can be used in various\nbackbones to boost the performance by a large margin. Without bells and\nwhistles, our best model can achieve a new state-of-the-art 76.8% AP on COCO\ntest-dev split.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:59:15 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Jian", ""], ["Long", "Xiang", ""], ["Gao", "Yuan", ""], ["Ding", "Errui", ""], ["Wen", "Shilei", ""]]}, {"id": "2007.10603", "submitter": "Chang Shu", "authors": "Chang Shu, Kun Yu, Zhixiang Duan, and Kuiyuan Yang", "title": "Feature-metric Loss for Self-supervised Learning of Depth and Egomotion", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Photometric loss is widely used for self-supervised depth and egomotion\nestimation. However, the loss landscapes induced by photometric differences are\noften problematic for optimization, caused by plateau landscapes for pixels in\ntextureless regions or multiple local minima for less discriminative pixels. In\nthis work, feature-metric loss is proposed and defined on feature\nrepresentation, where the feature representation is also learned in a\nself-supervised manner and regularized by both first-order and second-order\nderivatives to constrain the loss landscapes to form proper convergence basins.\nComprehensive experiments and detailed analysis via visualization demonstrate\nthe effectiveness of the proposed feature-metric loss. In particular, our\nmethod improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured\nby $\\delta_1$ for depth estimation, and significantly outperforms previous\nmethod for visual odometry.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 05:19:07 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Shu", "Chang", ""], ["Yu", "Kun", ""], ["Duan", "Zhixiang", ""], ["Yang", "Kuiyuan", ""]]}, {"id": "2007.10618", "submitter": "Mingyu Yin", "authors": "Mingyu Yin, Li Sun, Qingli Li", "title": "Novel View Synthesis on Unpaired Data by Conditional Deformable\n  Variational Auto-Encoder", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis often needs the paired data from both the source and\ntarget views. This paper proposes a view translation model under cVAE-GAN\nframework without requiring the paired data. We design a conditional deformable\nmodule (CDM) which uses the view condition vectors as the filters to convolve\nthe feature maps of the main branch in VAE. It generates several pairs of\ndisplacement maps to deform the features, like the 2D optical flows. The\nresults are fed into the deformed feature based normalization module (DFNM),\nwhich scales and offsets the main branch feature, given its deformed one as the\ninput from the side branch. Taking the advantage of the CDM and DFNM, the\nencoder outputs a view-irrelevant posterior, while the decoder takes the code\ndrawn from it to synthesize the reconstructed and the viewtranslated images. To\nfurther ensure the disentanglement between the views and other factors, we add\nadversarial training on the code. The results and ablation studies on MultiPIE\nand 3D chair datasets validate the effectiveness of the framework in cVAE and\nthe designed module.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:44:01 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Yin", "Mingyu", ""], ["Sun", "Li", ""], ["Li", "Qingli", ""]]}, {"id": "2007.10626", "submitter": "Xiongjun Zhang", "authors": "Xiongjun Zhang and Michael K. Ng", "title": "Sparse Nonnegative Tensor Factorization and Completion with Noisy\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the sparse nonnegative tensor factorization and\ncompletion problem from partial and noisy observations for third-order tensors.\nBecause of sparsity and nonnegativity, the underling tensor is decomposed into\nthe tensor-tensor product of one sparse nonnegative tensor and one nonnegative\ntensor. We propose to minimize the sum of the maximum likelihood estimate for\nthe observations with nonnegativity constraints and the tensor $\\ell_0$ norm\nfor the sparse factor. We show that the error bounds of the estimator of the\nproposed model can be established under general noise observations. The\ndetailed error bounds under specific noise distributions including additive\nGaussian noise, additive Laplace noise, and Poisson observations can be\nderived. Moreover, the minimax lower bounds are shown to be matched with the\nestablished upper bounds up to a logarithmic factor of the sizes of the\nunderlying tensor. These theoretical results for tensors are better than those\nobtained for matrices, and this illustrates the advantage of the use of\nnonnegative sparse tensor models for completion and denoising. Numerical\nexperiments are provided to validate the superiority of the proposed\ntensor-based method compared with the matrix-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:17:52 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 04:07:11 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Xiongjun", ""], ["Ng", "Michael K.", ""]]}, {"id": "2007.10629", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Taotao Li, Youyong Kong, Guanyu Yang, Lotfi Senhadji,\n  Huazhong Shu", "title": "SLNSpeech: solving extended speech separation problem by the help of\n  sign language", "comments": "33 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A speech separation task can be roughly divided into audio-only separation\nand audio-visual separation. In order to make speech separation technology\napplied in the real scenario of the disabled, this paper presents an extended\nspeech separation problem which refers in particular to sign language assisted\nspeech separation. However, most existing datasets for speech separation are\naudios and videos which contain audio and/or visual modalities. To address the\nextended speech separation problem, we introduce a large-scale dataset named\nSign Language News Speech (SLNSpeech) dataset in which three modalities of\naudio, visual, and sign language are coexisted. Then, we design a general deep\nlearning network for the self-supervised learning of three modalities,\nparticularly, using sign language embeddings together with audio or\naudio-visual information for better solving the speech separation task.\nSpecifically, we use 3D residual convolutional network to extract sign language\nfeatures and use pretrained VGGNet model to exact visual features. After that,\nan improved U-Net with skip connections in feature extraction stage is applied\nfor learning the embeddings among the mixed spectrogram transformed from source\naudios, the sign language features and visual features. Experiments results\nshow that, besides visual modality, sign language modality can also be used\nalone to supervise speech separation task. Moreover, we also show the\neffectiveness of sign language assisted speech separation when the visual\nmodality is disturbed. Source code will be released in\nhttp://cheertt.top/homepage/\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:22:33 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wu", "Jiasong", ""], ["Li", "Taotao", ""], ["Kong", "Youyong", ""], ["Yang", "Guanyu", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "2007.10631", "submitter": "Xueping Wang", "authors": "Xueping Wang, Sujoy Paul, Dripta S. Raychaudhuri, Min Liu, Yaonan Wang\n  and Amit K. Roy-Chowdhury, Fellow, IEEE", "title": "Learning Person Re-identification Models from Videos with Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most person re-identification methods, being supervised techniques, suffer\nfrom the burden of massive annotation requirement. Unsupervised methods\novercome this need for labeled data, but perform poorly compared to the\nsupervised alternatives. In order to cope with this issue, we introduce the\nproblem of learning person re-identification models from videos with weak\nsupervision. The weak nature of the supervision arises from the requirement of\nvideo-level labels, i.e. person identities who appear in the video, in contrast\nto the more precise framelevel annotations. Towards this goal, we propose a\nmultiple instance attention learning framework for person re-identification\nusing such video-level labels. Specifically, we first cast the video person\nre-identification task into a multiple instance learning setting, in which\nperson images in a video are collected into a bag. The relations between videos\nwith similar labels can be utilized to identify persons, on top of that, we\nintroduce a co-person attention mechanism which mines the similarity\ncorrelations between videos with person identities in common. The attention\nweights are obtained based on all person images instead of person tracklets in\na video, making our learned model less affected by noisy annotations. Extensive\nexperiments demonstrate the superiority of the proposed method over the related\nmethods on two weakly labeled person re-identification datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:23:32 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Xueping", ""], ["Paul", "Sujoy", ""], ["Raychaudhuri", "Dripta S.", ""], ["Liu", "Min", ""], ["Wang", "Yaonan", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Fellow", "", ""], ["IEEE", "", ""]]}, {"id": "2007.10639", "submitter": "Valentin Gabeur", "authors": "Valentin Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid", "title": "Multi-modal Transformer for Video Retrieval", "comments": "ECCV 2020 (spotlight paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of retrieving video content relevant to natural language queries\nplays a critical role in effectively handling internet-scale datasets. Most of\nthe existing methods for this caption-to-video retrieval problem do not fully\nexploit cross-modal cues present in video. Furthermore, they aggregate\nper-frame visual features with limited or no temporal information. In this\npaper, we present a multi-modal transformer to jointly encode the different\nmodalities in video, which allows each of them to attend to the others. The\ntransformer architecture is also leveraged to encode and model the temporal\ninformation. On the natural language side, we investigate the best practices to\njointly optimize the language embedding together with the multi-modal\ntransformer. This novel framework allows us to establish state-of-the-art\nresults for video retrieval on three datasets. More details are available at\nhttp://thoth.inrialpes.fr/research/MMT.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:38:46 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gabeur", "Valentin", ""], ["Sun", "Chen", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2007.10662", "submitter": "Tianshui Chen", "authors": "Jie Wu, Tianshui Chen, Hefeng Wu, Zhi Yang, Guangchun Luo, Liang Lin", "title": "Fine-Grained Image Captioning with Global-Local Discriminative Objective", "comments": "Accepted by TMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in recent years in image captioning, an\nactive topic in the fields of vision and language. However, existing methods\ntend to yield overly general captions and consist of some of the most frequent\nwords/phrases, resulting in inaccurate and indistinguishable descriptions (see\nFigure 1). This is primarily due to (i) the conservative characteristic of\ntraditional training objectives that drives the model to generate correct but\nhardly discriminative captions for similar images and (ii) the uneven word\ndistribution of the ground-truth captions, which encourages generating highly\nfrequent words/phrases while suppressing the less frequent but more concrete\nones. In this work, we propose a novel global-local discriminative objective\nthat is formulated on top of a reference model to facilitate generating\nfine-grained descriptive captions. Specifically, from a global perspective, we\ndesign a novel global discriminative constraint that pulls the generated\nsentence to better discern the corresponding image from all others in the\nentire dataset. From the local perspective, a local discriminative constraint\nis proposed to increase attention such that it emphasizes the less frequent but\nmore concrete words/phrases, thus facilitating the generation of captions that\nbetter describe the visual details of the given images. We evaluate the\nproposed method on the widely used MS-COCO dataset, where it outperforms the\nbaseline methods by a sizable margin and achieves competitive performance over\nexisting leading approaches. We also conduct self-retrieval experiments to\ndemonstrate the discriminability of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:46:02 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wu", "Jie", ""], ["Chen", "Tianshui", ""], ["Wu", "Hefeng", ""], ["Yang", "Zhi", ""], ["Luo", "Guangchun", ""], ["Lin", "Liang", ""]]}, {"id": "2007.10665", "submitter": "Zhenzhou Wang", "authors": "ZiHao Wang and ZhenZhou Wang", "title": "Fully Automated Segmentation of the Left Ventricle in Magnetic Resonance\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and robust segmentation of the left ventricle (LV) in magnetic\nresonance images (MRI) has remained challenging for many decades. With the\ngreat success of deep learning in object detection and classification, the\nresearch focus of LV segmentation has changed to convolutional neural network\n(CNN) in recent years. However, LV segmentation is a pixel-level classification\nproblem and its categories are intractable compared to object detection and\nclassification. Although lots of CNN based methods have been proposed for LV\nsegmentation, no robust and reproducible results are achieved yet. In this\npaper, we try to reproduce the CNN based LV segmentation methods with their\ndisclosed codes and trained CNN models. Not surprisingly, the reproduced\nresults are significantly worse than their claimed accuracies. We also proposed\na fully automated LV segmentation method based on slope difference distribution\n(SDD) threshold selection to compare with the reproduced CNN methods. The\nproposed method achieved 95.44% DICE score on the test set of automated cardiac\ndiagnosis challenge (ACDC) while the two compared CNN methods achieved 90.28%\nand 87.13% DICE scores. Our achieved accuracy is also higher than the best\naccuracy reported in the published literatures. The MATLAB codes of our\nproposed method are freely available on line.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:50:21 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "ZiHao", ""], ["Wang", "ZhenZhou", ""]]}, {"id": "2007.10689", "submitter": "Kang Liao", "authors": "Kang Liao, Chunyu Lin, Yao Zhao", "title": "A Deep Ordinal Distortion Estimation Approach for Distortion\n  Rectification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distortion is widely existed in the images captured by popular wide-angle\ncameras and fisheye cameras. Despite the long history of distortion\nrectification, accurately estimating the distortion parameters from a single\ndistorted image is still challenging. The main reason is these parameters are\nimplicit to image features, influencing the networks to fully learn the\ndistortion information. In this work, we propose a novel distortion\nrectification approach that can obtain more accurate parameters with higher\nefficiency. Our key insight is that distortion rectification can be cast as a\nproblem of learning an ordinal distortion from a single distorted image. To\nsolve this problem, we design a local-global associated estimation network that\nlearns the ordinal distortion to approximate the realistic distortion\ndistribution. In contrast to the implicit distortion parameters, the proposed\nordinal distortion have more explicit relationship with image features, and\nthus significantly boosts the distortion perception of neural networks.\nConsidering the redundancy of distortion information, our approach only uses a\npart of distorted image for the ordinal distortion estimation, showing\npromising applications in the efficient distortion rectification. To our\nknowledge, we first unify the heterogeneous distortion parameters into a\nlearning-friendly intermediate representation through ordinal distortion,\nbridging the gap between image feature and distortion rectification. The\nexperimental results demonstrate that our approach outperforms the\nstate-of-the-art methods by a significant margin, with approximately 23%\nimprovement on the quantitative evaluation while displaying the best\nperformance on visual appearance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:03:42 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Liao", "Kang", ""], ["Lin", "Chunyu", ""], ["Zhao", "Yao", ""]]}, {"id": "2007.10700", "submitter": "Ji Zhao", "authors": "Banglei Guan, Ji Zhao, Daniel Barath, Friedrich Fraundorfer", "title": "Relative Pose Estimation for Multi-Camera Systems from Affine\n  Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose four novel solvers for estimating the relative pose of a\nmulti-camera system from affine correspondences (ACs). A new constraint is\nderived interpreting the relationship of ACs and the generalized camera model.\nUsing the constraint, it is shown that a minimum of two ACs are enough for\nrecovering the 6DOF relative pose, i.e., 3D rotation and translation, of the\nsystem. Considering planar camera motion, we propose a minimal solution using a\nsingle AC and a solver with two ACs to overcome the degenerate case. Also, we\npropose a minimal solution using two ACs with known gravity vector, e.g., from\nan IMU. Since the proposed methods require significantly fewer correspondences\nthan state-of-the-art algorithms, they can be efficiently used within RANSAC\nfor outlier removal and initial motion estimation. The solvers are tested both\non synthetic data and on real-world scenes from the KITTI benchmark. It is\nshown that the accuracy of the estimated poses is superior to the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:34:45 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Guan", "Banglei", ""], ["Zhao", "Ji", ""], ["Barath", "Daniel", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2007.10701", "submitter": "Man M. Ho", "authors": "Man M. Ho, Jinjia Zhou", "title": "Deep Preset: Blending and Retouching Photos with Color Style Transfer", "comments": "Revised and Accepted to WACV'2021. Our work is available at\n  https://minhmanho.github.io/deep_preset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-users, without knowledge in photography, desire to beautify their photos\nto have a similar color style as a well-retouched reference. However, the\ndefinition of style in recent image style transfer works is inappropriate. They\nusually synthesize undesirable results due to transferring exact colors to the\nwrong destination. It becomes even worse in sensitive cases such as portraits.\nIn this work, we concentrate on learning low-level image transformation,\nespecially color-shifting methods, rather than mixing contextual features, then\npresent a novel scheme to train color style transfer with ground-truth.\nFurthermore, we propose a color style transfer named Deep Preset. It is\ndesigned to 1) generalize the features representing the color transformation\nfrom content with natural colors to retouched reference, then blend it into the\ncontextual features of content, 2) predict hyper-parameters (settings or\npreset) of the applied low-level color transformation methods, 3) stylize\ncontent to have a similar color style as reference. We script Lightroom, a\npowerful tool in editing photos, to generate 600,000 training samples using\n1,200 images from the Flick2K dataset and 500 user-generated presets with 69\nsettings. Experimental results show that our Deep Preset outperforms the\nprevious works in color style transfer quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:41:03 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 10:53:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ho", "Man M.", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2007.10703", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Chen Sun, Arsha Nagrani, Cordelia Schmid", "title": "Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed\n  Videos", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in video classification, progress in\nspatio-temporal action recognition has lagged behind. A major contributing\nfactor has been the prohibitive cost of annotating videos frame-by-frame. In\nthis paper, we present a spatio-temporal action recognition model that is\ntrained with only video-level labels, which are significantly easier to\nannotate. Our method leverages per-frame person detectors which have been\ntrained on large image datasets within a Multiple Instance Learning framework.\nWe show how we can apply our method in cases where the standard Multiple\nInstance Learning assumption, that each bag contains at least one instance with\nthe specified label, is invalid using a novel probabilistic variant of MIL\nwhere we estimate the uncertainty of each prediction. Furthermore, we report\nthe first weakly-supervised results on the AVA dataset and state-of-the-art\nresults among weakly-supervised methods on UCF101-24.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:45:05 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Arnab", "Anurag", ""], ["Sun", "Chen", ""], ["Nagrani", "Arsha", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2007.10714", "submitter": "Tianwen Zhang", "authors": "Tianwen Zhang, Xiaoling Zhang, Jun Shi, Shunjun Wei, Jianguo Wang,\n  Jianwei Li, Hao Su, and Yue Zhou", "title": "Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection\n  in SAR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge imbalance of different scenes' sample numbers seriously reduces\nSynthetic Aperture Radar (SAR) ship detection accuracy. Thus, to solve this\nproblem, this letter proposes a Balance Scene Learning Mechanism (BSLM) for\noffshore and inshore ship detection in SAR images.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:12:05 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 12:55:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhang", "Tianwen", ""], ["Zhang", "Xiaoling", ""], ["Shi", "Jun", ""], ["Wei", "Shunjun", ""], ["Wang", "Jianguo", ""], ["Li", "Jianwei", ""], ["Su", "Hao", ""], ["Zhou", "Yue", ""]]}, {"id": "2007.10729", "submitter": "Md Sahidullah", "authors": "Susanta Sarangi, Md Sahidullah, Goutam Saha", "title": "Optimization of data-driven filterbank for automatic speaker\n  verification", "comments": "Published in Digital Signal Processing journal (Elsevier)", "journal-ref": null, "doi": "10.1016/j.dsp.2020.102795", "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most of the speech processing applications use triangular filters spaced in\nmel-scale for feature extraction. In this paper, we propose a new data-driven\nfilter design method which optimizes filter parameters from a given speech\ndata. First, we introduce a frame-selection based approach for developing\nspeech-signal-based frequency warping scale. Then, we propose a new method for\ncomputing the filter frequency responses by using principal component analysis\n(PCA). The main advantage of the proposed method over the recently introduced\ndeep learning based methods is that it requires very limited amount of\nunlabeled speech-data. We demonstrate that the proposed filterbank has more\nspeaker discriminative power than commonly used mel filterbank as well as\nexisting data-driven filterbank. We conduct automatic speaker verification\n(ASV) experiments with different corpora using various classifier back-ends. We\nshow that the acoustic features created with proposed filterbank are better\nthan existing mel-frequency cepstral coefficients (MFCCs) and\nspeech-signal-based frequency cepstral coefficients (SFCCs) in most cases. In\nthe experiments with VoxCeleb1 and popular i-vector back-end, we observe 9.75%\nrelative improvement in equal error rate (EER) over MFCCs. Similarly, the\nrelative improvement is 4.43% with recently introduced x-vector system. We\nobtain further improvement using fusion of the proposed method with standard\nMFCC-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:42:20 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Sarangi", "Susanta", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "2007.10730", "submitter": "Simon Jenni", "authors": "Simon Jenni, Givi Meishvili, Paolo Favaro", "title": "Video Representation Learning by Recognizing Temporal Transformations", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel self-supervised learning approach to learn\nrepresentations of videos that are responsive to changes in the motion\ndynamics. Our representations can be learned from data without human annotation\nand provide a substantial boost to the training of neural networks on small\nlabeled data sets for tasks such as action recognition, which require to\naccurately distinguish the motion of objects. We promote an accurate learning\nof motion without human annotation by training a neural network to discriminate\na video sequence from its temporally transformed versions. To learn to\ndistinguish non-trivial motions, the design of the transformations is based on\ntwo principles: 1) To define clusters of motions based on time warps of\ndifferent magnitude; 2) To ensure that the discrimination is feasible only by\nobserving and analyzing as many image frames as possible. Thus, we introduce\nthe following transformations: forward-backward playback, random frame\nskipping, and uniform frame skipping. Our experiments show that networks\ntrained with the proposed method yield representations with improved transfer\nperformance for action recognition on UCF101 and HMDB51.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:43:01 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Jenni", "Simon", ""], ["Meishvili", "Givi", ""], ["Favaro", "Paolo", ""]]}, {"id": "2007.10732", "submitter": "Chuyu Zhang", "authors": "Shuailin Li, Chuyu Zhang, and Xuming He", "title": "Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images", "comments": "Appear in MICCAI2020", "journal-ref": "MICCAI 2020. Lecture Notes in Computer Science, vol 12261.\n  Springer, Cham", "doi": "10.1007/978-3-030-59710-8_54", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning has attracted much attention in medical image\nsegmentation due to challenges in acquiring pixel-wise image annotations, which\nis a crucial step for building high-performance deep learning methods. Most\nexisting semi-supervised segmentation approaches either tend to neglect\ngeometric constraint in object segments, leading to incomplete object coverage,\nor impose strong shape prior that requires extra alignment. In this work, we\npropose a novel shapeaware semi-supervised segmentation strategy to leverage\nabundant unlabeled data and to enforce a geometric shape constraint on the\nsegmentation output. To achieve this, we develop a multi-task deep network that\njointly predicts semantic segmentation and signed distance map(SDM) of object\nsurfaces. During training, we introduce an adversarial loss between the\npredicted SDMs of labeled and unlabeled data so that our network is able to\ncapture shape-aware features more effectively. Experiments on the Atrial\nSegmentation Challenge dataset show that our method outperforms current\nstate-of-the-art approaches with improved shape estimation, which validates its\nefficacy. Code is available at https://github.com/kleinzcy/SASSnet.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:44:52 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Shuailin", ""], ["Zhang", "Chuyu", ""], ["He", "Xuming", ""]]}, {"id": "2007.10737", "submitter": "Tal Hakim", "authors": "Tal Hakim", "title": "A Comprehensive Review of Skeleton-based Movement Assessment Methods", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.26189.46569/1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raising availability of 3D cameras and dramatic improvement of computer\nvision algorithms in the recent decade, accelerated the research of automatic\nmovement assessment solutions. Such solutions can be implemented at home, using\naffordable equipment and dedicated software. In this paper, we divide the\nmovement assessment task into secondary tasks and explain why they are needed\nand how they can be addressed. We review the recent solutions for automatic\nmovement assessment from skeleton videos, comparing them by their objectives,\nfeatures, movement domains and algorithmic approaches. In addition, we discuss\nthe status of the research on this topic in a high level.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:58:29 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:21:47 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 23:29:30 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Hakim", "Tal", ""]]}, {"id": "2007.10740", "submitter": "Jiawei Ren", "authors": "Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi,\n  Hongsheng Li", "title": "Balanced Meta-Softmax for Long-Tailed Visual Recognition", "comments": "NeurIPS 2020 camera-ready; Code available at\n  https://github.com/jiawei-ren/BalancedMetaSoftmax", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep classifiers have achieved great success in visual recognition. However,\nreal-world data is long-tailed by nature, leading to the mismatch between\ntraining and testing distributions. In this paper, we show that the Softmax\nfunction, though used in most classification tasks, gives a biased gradient\nestimation under the long-tailed setup. This paper presents Balanced Softmax,\nan elegant unbiased extension of Softmax, to accommodate the label distribution\nshift between training and testing. Theoretically, we derive the generalization\nbound for multiclass Softmax regression and show our loss minimizes the bound.\nIn addition, we introduce Balanced Meta-Softmax, applying a complementary Meta\nSampler to estimate the optimal class sample rate and further improve\nlong-tailed learning. In our experiments, we demonstrate that Balanced\nMeta-Softmax outperforms state-of-the-art long-tailed classification solutions\non both visual recognition and instance segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:05:00 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 04:00:11 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 05:27:41 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ren", "Jiawei", ""], ["Yu", "Cunjun", ""], ["Sheng", "Shunan", ""], ["Ma", "Xiao", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Li", "Hongsheng", ""]]}, {"id": "2007.10753", "submitter": "Sergio P. Perez", "authors": "Jos\\'e A. Carrillo, Serafim Kalliadasis, Fuyue Liang and Sergio P.\n  Perez", "title": "Enhancement of damaged-image prediction through Cahn-Hilliard Image\n  Inpainting", "comments": "An interactive jupyter notebook with the code of this work is\n  available at https://github.com/sergiopperez/Image_Inpainting. The MNIST\n  dataset employed in this work can be downloaded from\n  http://yann.lecun.com/exdb/mnist/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the benefit of including an image inpainting filter before passing\ndamaged images into a classification neural network. For this we employ a\nmodified Cahn-Hilliard equation as an image inpainting filter, which is solved\nvia a finite volume scheme with reduced computational cost and adequate\nproperties for energy stability and boundedness. The benchmark dataset employed\nhere is MNIST, which consists of binary images of handwritten digits and is a\nstandard dataset to validate image-processing methodologies. We train a neural\nnetwork based of dense layers with the training set of MNIST, and subsequently\nwe contaminate the test set with damage of different types and intensities. We\nthen compare the prediction accuracy of the neural network with and without\napplying the Cahn-Hilliard filter to the damaged images test. Our results\nquantify the significant improvement of damaged-image prediction due to\napplying the Cahn-Hilliard filter, which for specific damages can increase up\nto 50% and is in general advantageous for low to moderate damage.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:29:43 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 17:26:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Carrillo", "Jos\u00e9 A.", ""], ["Kalliadasis", "Serafim", ""], ["Liang", "Fuyue", ""], ["Perez", "Sergio P.", ""]]}, {"id": "2007.10760", "submitter": "Yansong Gao Dr", "authors": "Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin\n  Fu, Surya Nepal, and Hyoungshick Kim", "title": "Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive\n  Review", "comments": "29 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides the community with a timely comprehensive review of\nbackdoor attacks and countermeasures on deep learning. According to the\nattacker's capability and affected stage of the machine learning pipeline, the\nattack surfaces are recognized to be wide and then formalized into six\ncategorizations: code poisoning, outsourcing, pretrained, data collection,\ncollaborative learning and post-deployment. Accordingly, attacks under each\ncategorization are combed. The countermeasures are categorized into four\ngeneral classes: blind backdoor removal, offline backdoor inspection, online\nbackdoor inspection, and post backdoor removal. Accordingly, we review\ncountermeasures, and compare and analyze their advantages and disadvantages. We\nhave also reviewed the flip side of backdoor attacks, which are explored for i)\nprotecting intellectual property of deep learning models, ii) acting as a\nhoneypot to catch adversarial example attacks, and iii) verifying data deletion\nrequested by the data contributor.Overall, the research on defense is far\nbehind the attack, and there is no single defense that can prevent all types of\nbackdoor attacks. In some cases, an attacker can intelligently bypass existing\ndefenses with an adaptive attack. Drawing the insights from the systematic\nreview, we also present key areas for future research on the backdoor, such as\nempirical security evaluations from physical trigger attacks, and in\nparticular, more efficient and practical countermeasures are solicited.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:49:12 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 09:57:20 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 08:38:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gao", "Yansong", ""], ["Doan", "Bao Gia", ""], ["Zhang", "Zhi", ""], ["Ma", "Siqi", ""], ["Zhang", "Jiliang", ""], ["Fu", "Anmin", ""], ["Nepal", "Surya", ""], ["Kim", "Hyoungshick", ""]]}, {"id": "2007.10778", "submitter": "Cheng Gu", "authors": "Xian Zhong, Cheng Gu, Wenxin Huang, Lin Li, Shuqin Chen and Chia-Wen\n  Lin", "title": "Complementing Representation Deficiency in Few-shot Image\n  Classification: A Meta-Learning Approach", "comments": "25th International Conference on Pattern Recognition (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a challenging problem that has attracted more and more\nattention recently since abundant training samples are difficult to obtain in\npractical applications. Meta-learning has been proposed to address this issue,\nwhich focuses on quickly adapting a predictor as a base-learner to new tasks,\ngiven limited labeled samples. However, a critical challenge for meta-learning\nis the representation deficiency since it is hard to discover common\ninformation from a small number of training samples or even one, as is the\nrepresentation of key features from such little information. As a result, a\nmeta-learner cannot be trained well in a high-dimensional parameter space to\ngeneralize to new tasks. Existing methods mostly resort to extracting less\nexpressive features so as to avoid the representation deficiency. Aiming at\nlearning better representations, we propose a meta-learning approach with\ncomplemented representations network (MCRNet) for few-shot image\nclassification. In particular, we embed a latent space, where latent codes are\nreconstructed with extra representation information to complement the\nrepresentation deficiency. Furthermore, the latent space is established with\nvariational inference, collaborating well with different base-learners, and can\nbe extended to other models. Finally, our end-to-end framework achieves the\nstate-of-the-art performance in image classification on three standard few-shot\nlearning datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:25:54 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhong", "Xian", ""], ["Gu", "Cheng", ""], ["Huang", "Wenxin", ""], ["Li", "Lin", ""], ["Chen", "Shuqin", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2007.10787", "submitter": "Yanning Zhou", "authors": "Yanning Zhou, Hao Chen, Huangjing Lin, Pheng-Ann Heng", "title": "Deep Semi-supervised Knowledge Distillation for Overlapping Cervical\n  Cell Instance Segmentation", "comments": "to appear at MICCAI2020, supplementary material attached", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods show promising results for overlapping cervical cell\ninstance segmentation. However, in order to train a model with good\ngeneralization ability, voluminous pixel-level annotations are demanded which\nis quite expensive and time-consuming for acquisition. In this paper, we\npropose to leverage both labeled and unlabeled data for instance segmentation\nwith improved accuracy by knowledge distillation. We propose a novel\nMask-guided Mean Teacher framework with Perturbation-sensitive Sample Mining\n(MMT-PSM), which consists of a teacher and a student network during training.\nTwo networks are encouraged to be consistent both in feature and semantic level\nunder small perturbations. The teacher's self-ensemble predictions from\n$K$-time augmented samples are used to construct the reliable pseudo-labels for\noptimizing the student. We design a novel strategy to estimate the sensitivity\nto perturbations for each proposal and select informative samples from massive\ncases to facilitate fast and effective semantic distillation. In addition, to\neliminate the unavoidable noise from the background region, we propose to use\nthe predicted segmentation mask as guidance to enforce the feature distillation\nin the foreground region. Experiments show that the proposed method improves\nthe performance significantly compared with the supervised method learned from\nlabeled data only, and outperforms state-of-the-art semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:27:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zhou", "Yanning", ""], ["Chen", "Hao", ""], ["Lin", "Huangjing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2007.10791", "submitter": "Jindong Wang", "authors": "Chaohui Yu, Jindong Wang, Chang Liu, Tao Qin, Renjun Xu, Wenjie Feng,\n  Yiqiang Chen, Tie-Yan Liu", "title": "Learning to Match Distributions for Domain Adaptation", "comments": "Preprint. 20 Pages. Code available at\n  https://github.com/jindongwang/transferlearning/tree/master/code/deep/Learning-to-Match", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the training and test data are from different distributions, domain\nadaptation is needed to reduce dataset bias to improve the model's\ngeneralization ability. Since it is difficult to directly match the\ncross-domain joint distributions, existing methods tend to reduce the marginal\nor conditional distribution divergence using predefined distances such as MMD\nand adversarial-based discrepancies. However, it remains challenging to\ndetermine which method is suitable for a given application since they are built\nwith certain priors or bias. Thus they may fail to uncover the underlying\nrelationship between transferable features and joint distributions. This paper\nproposes Learning to Match (L2M) to automatically learn the cross-domain\ndistribution matching without relying on hand-crafted priors on the matching\nloss. Instead, L2M reduces the inductive bias by using a meta-network to learn\nthe distribution matching loss in a data-driven way. L2M is a general framework\nthat unifies task-independent and human-designed matching features. We design a\nnovel optimization algorithm for this challenging objective with\nself-supervised label propagation. Experiments on public datasets substantiate\nthe superiority of L2M over SOTA methods. Moreover, we apply L2M to transfer\nfrom pneumonia to COVID-19 chest X-ray images with remarkable performance. L2M\ncan also be extended in other distribution matching applications where we show\nin a trial experiment that L2M generates more realistic and sharper MNIST\nsamples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 03:26:13 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 11:05:56 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 01:44:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yu", "Chaohui", ""], ["Wang", "Jindong", ""], ["Liu", "Chang", ""], ["Qin", "Tao", ""], ["Xu", "Renjun", ""], ["Feng", "Wenjie", ""], ["Chen", "Yiqiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2007.10795", "submitter": "Aydogan Ozcan", "authors": "Zoltan Gorocs, David Baum, Fang Song, Kevin DeHaan, Hatice Ceylan\n  Koydemir, Yunzhe Qiu, Zilin Cai, Thamira Skandakumar, Spencer Peterman, Miu\n  Tamamitsu, and Aydogan Ozcan", "title": "Label-free detection of Giardia lamblia cysts using a deep\n  learning-enabled portable imaging flow cytometer", "comments": "17 Pages, 5 Figures, 1 Table", "journal-ref": "Lab on a Chip (2020)", "doi": "10.1039/D0LC00708K", "report-no": null, "categories": "eess.IV cs.CV physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a field-portable and cost-effective imaging flow cytometer that\nuses deep learning to accurately detect Giardia lamblia cysts in water samples\nat a volumetric throughput of 100 mL/h. This flow cytometer uses lensfree color\nholographic imaging to capture and reconstruct phase and intensity images of\nmicroscopic objects in a continuously flowing sample, and automatically\nidentifies Giardia Lamblia cysts in real-time without the use of any labels or\nfluorophores. The imaging flow cytometer is housed in an environmentally-sealed\nenclosure with dimensions of 19 cm x 19 cm x 16 cm and weighs 1.6 kg. We\ndemonstrate that this portable imaging flow cytometer coupled to a laptop\ncomputer can detect and quantify, in real-time, low levels of Giardia\ncontamination (e.g., <10 cysts per 50 mL) in both freshwater and seawater\nsamples. The field-portable and label-free nature of this method has the\npotential to allow rapid and automated screening of drinking water supplies in\nresource limited settings in order to detect waterborne parasites and monitor\nthe integrity of the filters used for water treatment.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 08:40:18 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gorocs", "Zoltan", ""], ["Baum", "David", ""], ["Song", "Fang", ""], ["DeHaan", "Kevin", ""], ["Koydemir", "Hatice Ceylan", ""], ["Qiu", "Yunzhe", ""], ["Cai", "Zilin", ""], ["Skandakumar", "Thamira", ""], ["Peterman", "Spencer", ""], ["Tamamitsu", "Miu", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2007.10800", "submitter": "Ankur Mallick", "authors": "Ankur Mallick, Chaitanya Dwivedi, Bhavya Kailkhura, Gauri Joshi, T.\n  Yong-Jin Han", "title": "Probabilistic Neighbourhood Component Analysis: Sample Efficient\n  Uncertainty Estimation in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in\nvarious applications, they often fall short in accurately estimating their\npredictive uncertainty and, in turn, fail to recognize when these predictions\nmay be wrong. Several uncertainty-aware models, such as Bayesian Neural Network\n(BNNs) and Deep Ensembles have been proposed in the literature for quantifying\npredictive uncertainty. However, research in this area has been largely\nconfined to the big data regime. In this work, we show that the uncertainty\nestimation capability of state-of-the-art BNNs and Deep Ensemble models\ndegrades significantly when the amount of training data is small. To address\nthe issue of accurate uncertainty estimation in the small-data regime, we\npropose a probabilistic generalization of the popular sample-efficient\nnon-parametric kNN approach. Our approach enables deep kNN classifier to\naccurately quantify underlying uncertainties in its prediction. We demonstrate\nthe usefulness of the proposed approach by achieving superior uncertainty\nquantification as compared to state-of-the-art on a real-world application of\nCOVID-19 diagnosis from chest X-Rays. Our code is available at\nhttps://github.com/ankurmallick/sample-efficient-uq\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 21:36:31 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Mallick", "Ankur", ""], ["Dwivedi", "Chaitanya", ""], ["Kailkhura", "Bhavya", ""], ["Joshi", "Gauri", ""], ["Han", "T. Yong-Jin", ""]]}, {"id": "2007.10812", "submitter": "Sayeed Shafayet Chowdhury", "authors": "Sayeed Shafayet Chowdhury, Kaji Mejbaul Islam and Rouhan Noor", "title": "Anomaly Detection in Unsupervised Surveillance Setting Using Ensemble of\n  Multimodal Data with Adversarial Defense", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.03733", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous aerial surveillance using drone feed is an interesting and\nchallenging research domain. To ensure safety from intruders and potential\nobjects posing threats to the zone being protected, it is crucial to be able to\ndistinguish between normal and abnormal states in real-time. Additionally, we\nalso need to consider any device malfunction. However, the inherent uncertainty\nembedded within the type and level of abnormality makes supervised techniques\nless suitable since the adversary may present a unique anomaly for intrusion.\nAs a result, an unsupervised method for anomaly detection is preferable taking\nthe unpredictable nature of attacks into account. Again in our case, the\nautonomous drone provides heterogeneous data streams consisting of images and\nother analog or digital sensor data, all of which can play a role in anomaly\ndetection if they are ensembled synergistically. To that end, an ensemble\ndetection mechanism is proposed here which estimates the degree of abnormality\nof analyzing the real-time image and IMU (Inertial Measurement Unit) sensor\ndata in an unsupervised manner. First, we have implemented a Convolutional\nNeural Network (CNN) regression block, named AngleNet to estimate the angle\nbetween a reference image and current test image, which provides us with a\nmeasure of the anomaly of the device. Moreover, the IMU data are used in\nautoencoders to predict abnormality. Finally, the results from these two\npipelines are ensembled to estimate the final degree of abnormality.\nFurthermore, we have applied adversarial attack to test the robustness and\nsecurity of the proposed approach and integrated defense mechanism. The\nproposed method performs satisfactorily on the IEEE SP Cup-2020 dataset with an\naccuracy of 97.8%. Additionally, we have also tested this approach on an\nin-house dataset to validate its robustness.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 20:03:02 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chowdhury", "Sayeed Shafayet", ""], ["Islam", "Kaji Mejbaul", ""], ["Noor", "Rouhan", ""]]}, {"id": "2007.10817", "submitter": "Lin Geng Foo", "authors": "Lin Geng Foo, Jiamei Sun, Alexander Binder", "title": "Split and Expand: An inference-time improvement for Weakly Supervised\n  Cell Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting cell nuclei instances from Hematoxylin\nand Eosin (H&E) stains with dot annotations only. While most recent works focus\non improving the segmentation quality, this is usually insufficient for\ninstance segmentation of cell instances clustered together or with a small\nsize. In this work, we propose a simple two-step post-processing procedure,\nSplit and Expand, that directly improves the conversion of segmentation maps to\ninstances. In the splitting step, we generate fine-grained cell instances from\nthe segmentation map with the guidance of cell-center predictions. For the\nexpansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation\nresults to add small cells that are not captured in the segmentation map.\nAlthough we additionally train an output head to predict cell-centers, the\npost-processing procedure itself is not explicitly trained and is executed at\ninference-time only. A feature re-weighting loss based on LRP is proposed to\nimprove our method even further. We test our procedure on the MoNuSeg and TNBC\ndatasets and show quantitatively and qualitatively that our proposed method\nimproves object-level metrics substantially.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:05:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 11:32:32 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Foo", "Lin Geng", ""], ["Sun", "Jiamei", ""], ["Binder", "Alexander", ""]]}, {"id": "2007.10822", "submitter": "Vishal Keswani", "authors": "Vishal Keswani, Sakshi Singh, Suryansh Agarwal, Ashutosh Modi", "title": "IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of\n  Internet Memes", "comments": "7 pages, 2 figures, 3 tables. Accepted at Proceedings of the 14th\n  International Workshop on Semantic Evaluation (SemEval-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is abundant in visual and textual information presented together\nor in isolation. Memes are the most popular form, belonging to the former\nclass. In this paper, we present our approaches for the Memotion Analysis\nproblem as posed in SemEval-2020 Task 8. The goal of this task is to classify\nmemes based on their emotional content and sentiment. We leverage techniques\nfrom Natural Language Processing (NLP) and Computer Vision (CV) towards the\nsentiment classification of internet memes (Subtask A). We consider Bimodal\n(text and image) as well as Unimodal (text-only) techniques in our study\nranging from the Na\\\"ive Bayes classifier to Transformer-based approaches. Our\nresults show that a text-only approach, a simple Feed Forward Neural Network\n(FFNN) with Word2vec embeddings as input, performs superior to all the others.\nWe stand first in the Sentiment analysis task with a relative improvement of\n63% over the baseline macro-F1 score. Our work is relevant to any task\nconcerned with the combination of different modalities.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:06:26 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Keswani", "Vishal", ""], ["Singh", "Sakshi", ""], ["Agarwal", "Suryansh", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2007.10835", "submitter": "Hu Wang", "authors": "Hu Wang, Qi Wu, Chunhua Shen", "title": "Soft Expert Reward Learning for Vision-and-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to find a specified\nspot in an unseen environment by following natural language instructions.\nDominant methods based on supervised learning clone expert's behaviours and\nthus perform better on seen environments, while showing restricted performance\non unseen ones. Reinforcement Learning (RL) based models show better\ngeneralisation ability but have issues as well, requiring large amount of\nmanual reward engineering is one of which. In this paper, we introduce a Soft\nExpert Reward Learning (SERL) model to overcome the reward engineering\ndesigning and generalisation problems of the VLN task. Our proposed method\nconsists of two complementary components: Soft Expert Distillation (SED) module\nencourages agents to behave like an expert as much as possible, but in a soft\nfashion; Self Perceiving (SP) module targets at pushing the agent towards the\nfinal destination as fast as possible. Empirically, we evaluate our model on\nthe VLN seen, unseen and test splits and the model outperforms the\nstate-of-the-art methods on most of the evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:17:36 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Hu", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""]]}, {"id": "2007.10839", "submitter": "Tsuyoshi Sekiyama", "authors": "Tsuyoshi Thomas Sekiyama", "title": "Statistical Downscaling of Temperature Distributions from the Synoptic\n  Scale to the Mesoscale Using Deep Convolutional Neural Networks", "comments": "15 pages, 4 figures, 4 tables.\n  https://scholar.google.com/citations?user=K3vMmpMAAAAJ&hl=en", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, particularly convolutional neural networks for image\nrecognition, has been recently used in meteorology. One of the promising\napplications is developing a statistical surrogate model that converts the\noutput images of low-resolution dynamic models to high-resolution images. Our\nstudy exhibits a preliminary experiment that evaluates the performance of a\nmodel that downscales synoptic temperature fields to mesoscale temperature\nfields every 6 hours. The deep learning model was trained with operational\n22-km gridded global analysis surface winds and temperatures as the input,\noperational 5-km gridded regional analysis surface temperatures as the desired\noutput, and a target domain covering central Japan. The results confirm that\nour deep convolutional neural network (DCNN) is capable of estimating the\nlocations of coastlines and mountain ridges in great detail, which are not\nretained in the inputs, and providing high-resolution surface temperature\ndistributions. For instance, while the average root-mean-square error (RMSE) is\n2.7 K between the global and regional analyses at altitudes greater than 1000\nm, the RMSE is reduced to 1.0 K, and the correlation coefficient is improved\nfrom 0.6 to 0.9 by the surrogate model. Although this study evaluates a\nsurrogate model only for surface temperature, it probably can be improved by\naugmenting the downscaling variables and vertical profiles. Surrogate models of\nDCNNs require only a small amount of computational power once their training is\nfinished. Therefore, if the surrogate models are implemented at short time\nintervals, they will provide high-resolution weather forecast guidance or\nenvironment emergency alerts at low cost.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 06:24:08 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Sekiyama", "Tsuyoshi Thomas", ""]]}, {"id": "2007.10854", "submitter": "Jianing Li", "authors": "Jianing Li, Shiliang Zhang", "title": "Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptive person Re-IDentification (ReID) is challenging\nbecause of the large domain gap between source and target domains, as well as\nthe lackage of labeled data on the target domain. This paper tackles this\nchallenge through jointly enforcing visual and temporal consistency in the\ncombination of a local one-hot classification and a global multi-class\nclassification. The local one-hot classification assigns images in a training\nbatch with different person IDs, then adopts a Self-Adaptive Classification\n(SAC) model to classify them. The global multi-class classification is achieved\nby predicting labels on the entire unlabeled training set with the Memory-based\nTemporal-guided Cluster (MTC). MTC predicts multi-class labels by considering\nboth visual similarity and temporal consistency to ensure the quality of label\nprediction. The two classification models are combined in a unified framework,\nwhich effectively leverages the unlabeled data for discriminative feature\nlearning. Experimental results on three large-scale ReID datasets demonstrate\nthe superiority of proposed method in both unsupervised and unsupervised domain\nadaptive ReID tasks. For example, under unsupervised setting, our method\noutperforms recent unsupervised domain adaptive methods, which leverage more\nlabels for training.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:31:27 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Li", "Jianing", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2007.10859", "submitter": "Hu Wang", "authors": "Congbo Ma, Hu Wang, Steven C.H. Hoi", "title": "Multi-label Thoracic Disease Image Classification with Cross-Attention\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated disease classification of radiology images has been emerging as a\npromising technique to support clinical diagnosis and treatment planning.\nUnlike generic image classification tasks, a real-world radiology image\nclassification task is significantly more challenging as it is far more\nexpensive to collect the training data where the labeled data is in nature\nmulti-label; and more seriously samples from easy classes often dominate;\ntraining data is highly class-imbalanced problem exists in practice as well. To\novercome these challenges, in this paper, we propose a novel scheme of\nCross-Attention Networks (CAN) for automated thoracic disease classification\nfrom chest x-ray images, which can effectively excavate more meaningful\nrepresentation from data to boost the performance through cross-attention by\nonly image-level annotations. We also design a new loss function that beyond\ncross-entropy loss to help cross-attention process and is able to overcome the\nimbalance between classes and easy-dominated samples within each class. The\nproposed method achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:37:00 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ma", "Congbo", ""], ["Wang", "Hu", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2007.10867", "submitter": "Erhan Gundogdu", "authors": "Erhan Gundogdu, Victor Constantin, Shaifali Parashar, Amrollah\n  Seifoddini, Minh Dang, Mathieu Salzmann, and Pascal Fua", "title": "GarNet++: Improving Fast and Accurate Static3D Cloth Draping by\n  Curvature Loss", "comments": "Accepted to be published in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), July 2020. arXiv admin note: text overlap with\n  arXiv:1811.10983", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of static 3D cloth draping on virtual\nhuman bodies. We introduce a two-stream deep network model that produces a\nvisually plausible draping of a template cloth on virtual 3D bodies by\nextracting features from both the body and garment shapes. Our network learns\nto mimic a Physics-Based Simulation (PBS) method while requiring two orders of\nmagnitude less computation time. To train the network, we introduce loss terms\ninspired by PBS to produce plausible results and make the model\ncollision-aware. To increase the details of the draped garment, we introduce\ntwo loss functions that penalize the difference between the curvature of the\npredicted cloth and PBS. Particularly, we study the impact of mean curvature\nnormal and a novel detail-preserving loss both qualitatively and\nquantitatively. Our new curvature loss computes the local covariance matrices\nof the 3D points, and compares the Rayleigh quotients of the prediction and\nPBS. This leads to more details while performing favorably or comparably\nagainst the loss that considers mean curvature normal vectors in the 3D\ntriangulated meshes. We validate our framework on four garment types for\nvarious body shapes and poses. Finally, we achieve superior performance against\na recently proposed data-driven method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:40:15 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gundogdu", "Erhan", ""], ["Constantin", "Victor", ""], ["Parashar", "Shaifali", ""], ["Seifoddini", "Amrollah", ""], ["Dang", "Minh", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2007.10872", "submitter": "Hongwei Yi", "authors": "Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang,\n  Yisong Chen, Guoping Wang, Yu-Wing Tai", "title": "Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency\n  Checking", "comments": "Accepted by ECCV2020 as Spotlight", "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and effective dense hybrid recurrent\nmulti-view stereo net with dynamic consistency checking, namely\n$D^{2}$HC-RMVSNet, for accurate dense point cloud reconstruction. Our novel\nhybrid recurrent multi-view stereo net consists of two core modules: 1) a light\nDRENet (Dense Reception Expanded) module to extract dense feature maps of\noriginal size with multi-scale context information, 2) a HU-LSTM (Hybrid\nU-LSTM) to regularize 3D matching volume into predicted depth map, which\nefficiently aggregates different scale information by coupling LSTM and U-Net\narchitecture. To further improve the accuracy and completeness of reconstructed\npoint clouds, we leverage a dynamic consistency checking strategy instead of\nprefixed parameters and strategies widely adopted in existing methods for dense\npoint cloud reconstruction. In doing so, we dynamically aggregate geometric\nconsistency matching error among all the views. Our method ranks\n\\textbf{$1^{st}$} on the complex outdoor \\textsl{Tanks and Temples} benchmark\nover all the methods. Extensive experiments on the in-door DTU dataset show our\nmethod exhibits competitive performance to the state-of-the-art method while\ndramatically reduces memory consumption, which costs only $19.4\\%$ of R-MVSNet\nmemory consumption. The codebase is available at\n\\hyperlink{https://github.com/yhw-yhw/D2HC-RMVSNet}{https://github.com/yhw-yhw/D2HC-RMVSNet}.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:59:59 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Yan", "Jianfeng", ""], ["Wei", "Zizhuang", ""], ["Yi", "Hongwei", ""], ["Ding", "Mingyu", ""], ["Zhang", "Runze", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2007.10891", "submitter": "Razieh Kaviani Baghbaderani", "authors": "Razieh Kaviani Baghbaderani, Ying Qu, Hairong Qi, Craig Stutts", "title": "Representative-Discriminative Learning for Open-set Land Cover\n  Classification of Satellite Imagery", "comments": "20 pages, 10 figures, European Conference on Computer Vision (ECCV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Land cover classification of satellite imagery is an important step toward\nanalyzing the Earth's surface. Existing models assume a closed-set setting\nwhere both the training and testing classes belong to the same label set.\nHowever, due to the unique characteristics of satellite imagery with an\nextremely vast area of versatile cover materials, the training data are bound\nto be non-representative. In this paper, we study the problem of open-set land\ncover classification that identifies the samples belonging to unknown classes\nduring testing, while maintaining performance on known classes. Although\ninherently a classification problem, both representative and discriminative\naspects of data need to be exploited in order to better distinguish unknown\nclasses from known. We propose a representative-discriminative open-set\nrecognition (RDOSR) framework, which 1) projects data from the raw image space\nto the embedding feature space that facilitates differentiating similar\nclasses, and further 2) enhances both the representative and discriminative\ncapacity through transformation to a so-called abundance space. Experiments on\nmultiple satellite benchmarks demonstrate the effectiveness of the proposed\nmethod. We also show the generality of the proposed approach by achieving\npromising results on open-set classification tasks using RGB images.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:28:56 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Baghbaderani", "Razieh Kaviani", ""], ["Qu", "Ying", ""], ["Qi", "Hairong", ""], ["Stutts", "Craig", ""]]}, {"id": "2007.10930", "submitter": "Yash Sharma", "authors": "David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland\n  Brendel, Matthias Bethge, Dylan Paiton", "title": "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse\n  Coding", "comments": "ICLR 2021. Code is available at\n  https://github.com/bethgelab/slow_disentanglement. The first three authors,\n  as well as the last two authors, contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an unsupervised learning model that achieves nonlinear\ndisentanglement of underlying factors of variation in naturalistic videos.\nPrevious work suggests that representations can be disentangled if all but a\nfew factors in the environment stay constant at any point in time. As a result,\nalgorithms proposed for this problem have only been tested on carefully\nconstructed datasets with this exact property, leaving it unclear whether they\nwill transfer to natural scenes. Here we provide evidence that objects in\nsegmented natural movies undergo transitions that are typically small in\nmagnitude with occasional large jumps, which is characteristic of a temporally\nsparse distribution. We leverage this finding and present SlowVAE, a model for\nunsupervised representation learning that uses a sparse prior on temporally\nadjacent observations to disentangle generative factors without any assumptions\non the number of changing factors. We provide a proof of identifiability and\nshow that the model reliably learns disentangled representations on several\nestablished benchmark datasets, often surpassing the current state-of-the-art.\nWe additionally demonstrate transferability towards video datasets with natural\ndynamics, Natural Sprites and KITTI Masks, which we contribute as benchmarks\nfor guiding disentanglement research towards more natural data domains.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:46:05 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:20:05 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Klindt", "David", ""], ["Schott", "Lukas", ""], ["Sharma", "Yash", ""], ["Ustyuzhaninov", "Ivan", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""], ["Paiton", "Dylan", ""]]}, {"id": "2007.10937", "submitter": "Yu Xiong", "authors": "Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin", "title": "MovieNet: A Holistic Dataset for Movie Understanding", "comments": "Accepted by ECCV2020 as spotlight presentation. Project page:\n  http://movienet.site", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen remarkable advances in visual understanding. However,\nhow to understand a story-based long video with artistic styles, e.g. movie,\nremains challenging. In this paper, we introduce MovieNet -- a holistic dataset\nfor movie understanding. MovieNet contains 1,100 movies with a large amount of\nmulti-modal data, e.g. trailers, photos, plot descriptions, etc. Besides,\ndifferent aspects of manual annotations are provided in MovieNet, including\n1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K\naligned description sentences, 65K tags of place and action, and 92K tags of\ncinematic style. To the best of our knowledge, MovieNet is the largest dataset\nwith richest annotations for comprehensive movie understanding. Based on\nMovieNet, we set up several benchmarks for movie understanding from different\nangles. Extensive experiments are executed on these benchmarks to show the\nimmeasurable value of MovieNet and the gap of current approaches towards\ncomprehensive movie understanding. We believe that such a holistic dataset\nwould promote the researches on story-based long video understanding and\nbeyond. MovieNet will be published in compliance with regulations at\nhttps://movienet.github.io.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:54:33 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Huang", "Qingqiu", ""], ["Xiong", "Yu", ""], ["Rao", "Anyi", ""], ["Wang", "Jiaze", ""], ["Lin", "Dahua", ""]]}, {"id": "2007.10947", "submitter": "Chenxi Yuan", "authors": "Chenxi Yuan, Mohsen Moghaddam", "title": "Garment Design with Generative Adversarial Networks", "comments": "AdvML 2020, KDD workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The designers' tendency to adhere to a specific mental set and heavy\nemotional investment in their initial ideas often hinder their ability to\ninnovate during the design thinking and ideation process. In the fashion\nindustry, in particular, the growing diversity of customers' needs, the intense\nglobal competition, and the shrinking time-to-market (a.k.a., \"fast fashion\")\nfurther exacerbate this challenge for designers. Recent advances in deep\ngenerative models have created new possibilities to overcome the cognitive\nobstacles of designers through automated generation and/or editing of design\nconcepts. This paper explores the capabilities of generative adversarial\nnetworks (GAN) for automated attribute-level editing of design concepts.\nSpecifically, attribute GAN (AttGAN)---a generative model proven successful for\nattribute editing of human faces---is utilized for automated editing of the\nvisual attributes of garments and tested on a large fashion dataset. The\nexperiments support the hypothesized potentials of GAN for attribute-level\nediting of design concepts, and underscore several key limitations and research\nquestions to be addressed in future work.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:03:33 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 00:59:37 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yuan", "Chenxi", ""], ["Moghaddam", "Mohsen", ""]]}, {"id": "2007.10961", "submitter": "Sungheon Park", "authors": "Sungheon Park, Minsik Lee, Nojun Kwak", "title": "Procrustean Regression Networks: Learning 3D Structure of Non-Rigid\n  Objects from 2D Annotations", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for training neural networks which is capable of\nlearning 3D information of non-rigid objects when only 2D annotations are\navailable as ground truths. Recently, there have been some approaches that\nincorporate the problem setting of non-rigid structure-from-motion (NRSfM) into\ndeep learning to learn 3D structure reconstruction. The most important\ndifficulty of NRSfM is to estimate both the rotation and deformation at the\nsame time, and previous works handle this by regressing both of them. In this\npaper, we resolve this difficulty by proposing a loss function wherein the\nsuitable rotation is automatically determined. Trained with the cost function\nconsisting of the reprojection error and the low-rank term of aligned shapes,\nthe network learns the 3D structures of such objects as human skeletons and\nfaces during the training, whereas the testing is done in a single-frame basis.\nThe proposed method can handle inputs with missing entries and experimental\nresults validate that the proposed framework shows superior reconstruction\nperformance to the state-of-the-art method on the Human 3.6M, 300-VW, and\nSURREAL datasets, even though the underlying network structure is very simple.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:29:20 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Park", "Sungheon", ""], ["Lee", "Minsik", ""], ["Kwak", "Nojun", ""]]}, {"id": "2007.10963", "submitter": "Jinxiu Liang", "authors": "Jinxiu Liang, Jingwen Wang, Yuhui Quan, Tianyi Chen, Jiaying Liu,\n  Haibin Ling and Yong Xu", "title": "Recurrent Exposure Generation for Low-Light Face Detection", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection from low-light images is challenging due to limited photos and\ninevitable noise, which, to make the task even harder, are often spatially\nunevenly distributed. A natural solution is to borrow the idea from\nmulti-exposure, which captures multiple shots to obtain well-exposed images\nunder challenging conditions. High-quality implementation/approximation of\nmulti-exposure from a single image is however nontrivial. Fortunately, as shown\nin this paper, neither is such high-quality necessary since our task is face\ndetection rather than image enhancement. Specifically, we propose a novel\nRecurrent Exposure Generation (REG) module and couple it seamlessly with a\nMulti-Exposure Detection (MED) module, and thus significantly improve face\ndetection performance by effectively inhibiting non-uniform illumination and\nnoise issues. REG produces progressively and efficiently intermediate images\ncorresponding to various exposure settings, and such pseudo-exposures are then\nfused by MED to detect faces across different lighting conditions. The proposed\nmethod, named REGDet, is the first `detection-with-enhancement' framework for\nlow-light face detection. It not only encourages rich interaction and feature\nfusion across different illumination levels, but also enables effective\nend-to-end learning of the REG component to be better tailored for face\ndetection. Moreover, as clearly shown in our experiments, REG can be flexibly\ncoupled with different face detectors without extra low/normal-light image\npairs for training. We tested REGDet on the DARK FACE low-light face benchmark\nwith thorough ablation study, where REGDet outperforms previous\nstate-of-the-arts by a significant margin, with only negligible extra\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:30:51 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Liang", "Jinxiu", ""], ["Wang", "Jingwen", ""], ["Quan", "Yuhui", ""], ["Chen", "Tianyi", ""], ["Liu", "Jiaying", ""], ["Ling", "Haibin", ""], ["Xu", "Yong", ""]]}, {"id": "2007.10973", "submitter": "Kunal Gupta", "authors": "Kunal Gupta and Manmohan Chandraker", "title": "Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows", "comments": "Project Page:\n  https://kunalmgupta.github.io/projects/NeuralMeshflow.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meshes are important representations of physical 3D entities in the virtual\nworld. Applications like rendering, simulations and 3D printing require meshes\nto be manifold so that they can interact with the world like the real objects\nthey represent. Prior methods generate meshes with great geometric accuracy but\npoor manifoldness. In this work, we propose Neural Mesh Flow (NMF) to generate\ntwo-manifold meshes for genus-0 shapes. Specifically, NMF is a shape\nauto-encoder consisting of several Neural Ordinary Differential Equation\n(NODE)[1] blocks that learn accurate mesh geometry by progressively deforming a\nspherical mesh. Training NMF is simpler compared to state-of-the-art methods\nsince it does not require any explicit mesh-based regularization. Our\nexperiments demonstrate that NMF facilitates several applications such as\nsingle-view mesh reconstruction, global shape parameterization, texture\nmapping, shape deformation and correspondence. Importantly, we demonstrate that\nmanifold meshes generated using NMF are better-suited for physically-based\nrendering and simulation. Code and data are released.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:45:41 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:00:19 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gupta", "Kunal", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.10982", "submitter": "Shubham Goel", "authors": "Shubham Goel, Angjoo Kanazawa, Jitendra Malik", "title": "Shape and Viewpoint without Keypoints", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning framework that learns to recover the 3D shape, pose and\ntexture from a single image, trained on an image collection without any ground\ntruth 3D shape, multi-view, camera viewpoints or keypoint supervision. We\napproach this highly under-constrained problem in a \"analysis by synthesis\"\nframework where the goal is to predict the likely shape, texture and camera\nviewpoint that could produce the image with various learned category-specific\npriors. Our particular contribution in this paper is a representation of the\ndistribution over cameras, which we call \"camera-multiplex\". Instead of picking\na point estimate, we maintain a set of camera hypotheses that are optimized\nduring training to best explain the image given the current shape and texture.\nWe call our approach Unsupervised Category-Specific Mesh Reconstruction\n(U-CMR), and present qualitative and quantitative results on CUB, Pascal 3D and\nnew web-scraped datasets. We obtain state-of-the-art camera prediction results\nand show that we can learn to predict diverse shapes and textures across\nobjects using an image collection without any keypoint annotations or 3D ground\ntruth. Project page: https://shubham-goel.github.io/ucmr\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:58:28 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Goel", "Shubham", ""], ["Kanazawa", "Angjoo", ""], ["Malik", "Jitendra", ""]]}, {"id": "2007.10983", "submitter": "Yuliang Zou", "authors": "Yuliang Zou, Pan Ji, Quoc-Huy Tran, Jia-Bin Huang, Manmohan Chandraker", "title": "Learning Monocular Visual Odometry via Self-Supervised Long-Term\n  Modeling", "comments": "ECCV 2020. Project page: https://yuliang.vision/LTMVO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular visual odometry (VO) suffers severely from error accumulation\nduring frame-to-frame pose estimation. In this paper, we present a\nself-supervised learning method for VO with special consideration for\nconsistency over longer sequences. To this end, we model the long-term\ndependency in pose prediction using a pose network that features a two-layer\nconvolutional LSTM module. We train the networks with purely self-supervised\nlosses, including a cycle consistency loss that mimics the loop closure module\nin geometric VO. Inspired by prior geometric systems, we allow the networks to\nsee beyond a small temporal window during training, through a novel a loss that\nincorporates temporally distant (e.g., O(100)) frames. Given GPU memory\nconstraints, we propose a stage-wise training mechanism, where the first stage\noperates in a local time window and the second stage refines the poses with a\n\"global\" loss given the first stage features. We demonstrate competitive\nresults on several standard VO datasets, including KITTI and TUM RGB-D.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:59:01 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zou", "Yuliang", ""], ["Ji", "Pan", ""], ["Tran", "Quoc-Huy", ""], ["Huang", "Jia-Bin", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.10984", "submitter": "Chuang Gan", "authors": "Chuang Gan, Deng Huang, Peihao Chen, Joshua B. Tenenbaum, Antonio\n  Torralba", "title": "Foley Music: Learning to Generate Music from Videos", "comments": "ECCV 2020. Project page: http://foley-music.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Foley Music, a system that can synthesize\nplausible music for a silent video clip about people playing musical\ninstruments. We first identify two key intermediate representations for a\nsuccessful video to music generator: body keypoints from videos and MIDI events\nfrom audio recordings. We then formulate music generation from videos as a\nmotion-to-MIDI translation problem. We present a Graph$-$Transformer framework\nthat can accurately predict MIDI event sequences in accordance with the body\nmovements. The MIDI event can then be converted to realistic music using an\noff-the-shelf music synthesizer tool. We demonstrate the effectiveness of our\nmodels on videos containing a variety of music performances. Experimental\nresults show that our model outperforms several existing systems in generating\nmusic that is pleasant to listen to. More importantly, the MIDI representations\nare fully interpretable and transparent, thus enabling us to perform music\nediting flexibly. We encourage the readers to watch the demo video with audio\nturned on to experience the results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:59:06 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gan", "Chuang", ""], ["Huang", "Deng", ""], ["Chen", "Peihao", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""]]}, {"id": "2007.10985", "submitter": "Saining Xie", "authors": "Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas J. Guibas,\n  Or Litany", "title": "PointContrast: Unsupervised Pre-training for 3D Point Cloud\n  Understanding", "comments": "ECCV 2020 (Spotlight); code available at\n  https://github.com/facebookresearch/PointContrast", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguably one of the top success stories of deep learning is transfer\nlearning. The finding that pre-training a network on a rich source set (eg.,\nImageNet) can help boost performance once fine-tuned on a usually much smaller\ntarget set, has been instrumental to many applications in language and vision.\nYet, very little is known about its usefulness in 3D point cloud understanding.\nWe see this as an opportunity considering the effort required for annotating\ndata in 3D. In this work, we aim at facilitating research on 3D representation\nlearning. Different from previous works, we focus on high-level scene\nunderstanding tasks. To this end, we select a suite of diverse datasets and\ntasks to measure the effect of unsupervised pre-training on a large source set\nof 3D scenes. Our findings are extremely encouraging: using a unified triplet\nof architecture, source dataset, and contrastive loss for pre-training, we\nachieve improvement over recent best results in segmentation and detection\nacross 6 different benchmarks for indoor and outdoor, real and synthetic\ndatasets -- demonstrating that the learned representation can generalize across\ndomains. Furthermore, the improvement was similar to supervised pre-training,\nsuggesting that future efforts should favor scaling data collection over more\ndetailed annotation. We hope these findings will encourage more research on\nunsupervised pretext task design for 3D deep learning.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:59:22 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 03:56:46 GMT"}, {"version": "v3", "created": "Sat, 21 Nov 2020 00:42:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Xie", "Saining", ""], ["Gu", "Jiatao", ""], ["Guo", "Demi", ""], ["Qi", "Charles R.", ""], ["Guibas", "Leonidas J.", ""], ["Litany", "Or", ""]]}, {"id": "2007.10986", "submitter": "He Chen", "authors": "He Chen, Pengfei Guo, Pengfei Li, Gim Hee Lee, Gregory Chirikjian", "title": "Multi-person 3D Pose Estimation in Crowded Scenes Based on Multi-View\n  Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epipolar constraints are at the core of feature matching and depth estimation\nin current multi-person multi-camera 3D human pose estimation methods. Despite\nthe satisfactory performance of this formulation in sparser crowd scenes, its\neffectiveness is frequently challenged under denser crowd circumstances mainly\ndue to two sources of ambiguity. The first is the mismatch of human joints\nresulting from the simple cues provided by the Euclidean distances between\njoints and epipolar lines. The second is the lack of robustness from the naive\nformulation of the problem as a least squares minimization. In this paper, we\ndepart from the multi-person 3D pose estimation formulation, and instead\nreformulate it as crowd pose estimation. Our method consists of two key\ncomponents: a graph model for fast cross-view matching, and a maximum a\nposteriori (MAP) estimator for the reconstruction of the 3D human poses. We\ndemonstrate the effectiveness and superiority of our proposed method on four\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:59:36 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chen", "He", ""], ["Guo", "Pengfei", ""], ["Li", "Pengfei", ""], ["Lee", "Gim Hee", ""], ["Chirikjian", "Gregory", ""]]}, {"id": "2007.11018", "submitter": "Heming Du", "authors": "Heming Du, Xin Yu, Liang Zheng", "title": "Learning Object Relation Graph and Tentative Policy for Visual\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target-driven visual navigation aims at navigating an agent towards a given\ntarget based on the observation of the agent. In this task, it is critical to\nlearn informative visual representation and robust navigation policy. Aiming to\nimprove these two components, this paper proposes three complementary\ntechniques, object relation graph (ORG), trial-driven imitation learning (IL),\nand a memory-augmented tentative policy network (TPN). ORG improves visual\nrepresentation learning by integrating object relationships, including category\ncloseness and spatial correlations, e.g., a TV usually co-occurs with a remote\nspatially. Both Trial-driven IL and TPN underlie robust navigation policy,\ninstructing the agent to escape from deadlock states, such as looping or being\nstuck. Specifically, trial-driven IL is a type of supervision used in policy\nnetwork training, while TPN, mimicking the IL supervision in unseen\nenvironment, is applied in testing. Experiment in the artificial environment\nAI2-Thor validates that each of the techniques is effective. When combined, the\ntechniques bring significantly improvement over baseline methods in navigation\neffectiveness and efficiency in unseen environments. We report 22.8% and 23.5%\nincrease in success rate and Success weighted by Path Length (SPL),\nrespectively. The code is available at\nhttps://github.com/xiaobaishu0097/ECCV-VN.git.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:03:05 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Du", "Heming", ""], ["Yu", "Xin", ""], ["Zheng", "Liang", ""]]}, {"id": "2007.11040", "submitter": "Xinyu Li", "authors": "Xinyu Li, Bing Shuai, Joseph Tighe", "title": "Directional Temporal Modeling for Action Recognition", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current activity recognition models use 3D convolutional neural networks\n(e.g. I3D, I3D-NL) to generate local spatial-temporal features. However, such\nfeatures do not encode clip-level ordered temporal information. In this paper,\nwe introduce a channel independent directional convolution (CIDC) operation,\nwhich learns to model the temporal evolution among local features. By applying\nmultiple CIDC units we construct a light-weight network that models the\nclip-level temporal evolution across multiple spatial scales. Our CIDC network\ncan be attached to any activity recognition backbone network. We evaluate our\nmethod on four popular activity recognition datasets and consistently improve\nupon state-of-the-art techniques. We further visualize the activation map of\nour CIDC network and show that it is able to focus on more meaningful, action\nrelated parts of the frame.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:49:57 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Li", "Xinyu", ""], ["Shuai", "Bing", ""], ["Tighe", "Joseph", ""]]}, {"id": "2007.11047", "submitter": "Slimane Larabi", "authors": "Slimane Larabi and Neil M. Robertson", "title": "An Image Analogies Approach for Multi-Scale Contour Detection", "comments": "Not published paper", "journal-ref": null, "doi": null, "report-no": "RIIMA_report_01_2020", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with contour detection based on the recent image\nanalogy principle which has been successfully used for super-resolution,\ntexture and curves synthesis and interactive editing. Hand-drawn outlines are\ninitially as benchmarks. Given such a reference image, we present a new method\nbased on this expertise to locate contours of a query image in the same way\nthat it is done for the reference (i.e by analogy).\n  Applying a image analogies for contour detection using hand drawn images as\nleaning images cannot gives good result for any query image. The contour\ndetection may be improved if we increase the number of learning images such\nthat there will be exist similarity between query image and some reference\nimages. In addition of the hardness of contours drawing task, this will\nincrease considerably the time computation.\n  We investigated in this work, how can we avoid this constraint in order to\nguaranty that all contour pixels will be located for any query image. Fourteen\nderived stereo patches, derived from a mathematical study, are the knowledge\nused in order to locate contours at different scales independently of the light\nconditions.\n  Comprehensive experiments are conducted on different data sets (BSD 500,\nHorses of Weizmann). The obtained results show superior performance via\nprecision and recall vs. hand-drawn contours at multiple resolutions to the\nreported state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:14:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Larabi", "Slimane", ""], ["Robertson", "Neil M.", ""]]}, {"id": "2007.11052", "submitter": "Mona Minakshi", "authors": "Mona Minakshi, Pratool Bharti, Tanvir Bhuiyan, Sherzod Kariev, Sriram\n  Chellappan", "title": "A Framework based on Deep Neural Networks to Extract Anatomy of\n  Mosquitoes from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a framework based on Mask Region-based Convolutional Neural Network\n(Mask R-CNN) to automatically detect and separately extract anatomical\ncomponents of mosquitoes - thorax, wings, abdomen and legs from images. Our\ntraining dataset consisted of 1500 smartphone images of nine mosquito species\ntrapped in Florida. In the proposed technique, the first step is to detect\nanatomical components within a mosquito image. Then, we localize and classify\nthe extracted anatomical components, while simultaneously adding a branch in\nthe neural network architecture to segment pixels containing only the\nanatomical components. Evaluation results are favorable. To evaluate\ngenerality, we test our architecture trained only with mosquito images on\nbumblebee images. We again reveal favorable results, particularly in extracting\nwings. Our techniques in this paper have practical applications in public\nhealth, taxonomy and citizen-science efforts.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:27:46 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 14:00:13 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Minakshi", "Mona", ""], ["Bharti", "Pratool", ""], ["Bhuiyan", "Tanvir", ""], ["Kariev", "Sherzod", ""], ["Chellappan", "Sriram", ""]]}, {"id": "2007.11056", "submitter": "Yuchen Ma", "authors": "Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, Jian Sun", "title": "BorderDet: Border Feature for Dense Object Detection", "comments": "Accepted by ECCV 2020 as Oral. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense object detectors rely on the sliding-window paradigm that predicts the\nobject over a regular grid of image. Meanwhile, the feature maps on the point\nof the grid are adopted to generate the bounding box predictions. The point\nfeature is convenient to use but may lack the explicit border information for\naccurate localization. In this paper, We propose a simple and efficient\noperator called Border-Align to extract \"border features\" from the extreme\npoint of the border to enhance the point feature. Based on the BorderAlign, we\ndesign a novel detection architecture called BorderDet, which explicitly\nexploits the border information for stronger classification and more accurate\nlocalization. With ResNet-50 backbone, our method improves single-stage\ndetector FCOS by 2.8 AP gains (38.6 v.s. 41.4). With the ResNeXt-101-DCN\nbackbone, our BorderDet obtains 50.3 AP, outperforming the existing\nstate-of-the-art approaches. The code is available at\n(https://github.com/Megvii-BaseDetection/BorderDet).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:30:36 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 13:09:14 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 04:53:42 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Qiu", "Han", ""], ["Ma", "Yuchen", ""], ["Li", "Zeming", ""], ["Liu", "Songtao", ""], ["Sun", "Jian", ""]]}, {"id": "2007.11064", "submitter": "Dripta S. Raychaudhuri", "authors": "Dripta S. Raychaudhuri and Amit K. Roy-Chowdhury", "title": "Exploiting Temporal Coherence for Self-Supervised One-shot Video\n  Re-identification", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While supervised techniques in re-identification are extremely effective, the\nneed for large amounts of annotations makes them impractical for large camera\nnetworks. One-shot re-identification, which uses a singular labeled tracklet\nfor each identity along with a pool of unlabeled tracklets, is a potential\ncandidate towards reducing this labeling effort. Current one-shot\nre-identification methods function by modeling the inter-relationships amongst\nthe labeled and the unlabeled data, but fail to fully exploit such\nrelationships that exist within the pool of unlabeled data itself. In this\npaper, we propose a new framework named Temporal Consistency Progressive\nLearning, which uses temporal coherence as a novel self-supervised auxiliary\ntask in the one-shot learning paradigm to capture such relationships amongst\nthe unlabeled tracklets. Optimizing two new losses, which enforce consistency\non a local and global scale, our framework can learn learn richer and more\ndiscriminative representations. Extensive experiments on two challenging video\nre-identification datasets - MARS and DukeMTMC-VideoReID - demonstrate that our\nproposed method is able to estimate the true labels of the unlabeled data more\naccurately by up to $8\\%$, and obtain significantly better re-identification\nperformance compared to the existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:49:06 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Raychaudhuri", "Dripta S.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2007.11067", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Mengyu Jia, Md Tauhidul Islam, Lequan Yu, Lei Xing", "title": "Self-supervised Feature Learning via Exploiting Multi-modal Data for\n  Retinal Disease Diagnosis", "comments": "IEEE Transactions on Medical Imaging, code is at\n  https://github.com/xmengli999/self_supervised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic diagnosis of various retinal diseases from fundus images is\nimportant to support clinical decision-making. However, developing such\nautomatic solutions is challenging due to the requirement of a large amount of\nhuman-annotated data. Recently, unsupervised/self-supervised feature learning\ntechniques receive a lot of attention, as they do not need massive annotations.\nMost of the current self-supervised methods are analyzed with single imaging\nmodality and there is no method currently utilize multi-modal images for better\nresults. Considering that the diagnostics of various vitreoretinal diseases can\ngreatly benefit from another imaging modality, e.g., FFA, this paper presents a\nnovel self-supervised feature learning method by effectively exploiting\nmulti-modal data for retinal disease diagnosis. To achieve this, we first\nsynthesize the corresponding FFA modality and then formulate a patient\nfeature-based softmax embedding objective. Our objective learns both\nmodality-invariant features and patient-similarity features. Through this\nmechanism, the neural network captures the semantically shared information\nacross different modalities and the apparent visual similarity between\npatients. We evaluate our method on two public benchmark datasets for retinal\ndisease diagnosis. The experimental results demonstrate that our method clearly\noutperforms other self-supervised feature learning methods and is comparable to\nthe supervised baseline.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:49:45 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Li", "Xiaomeng", ""], ["Jia", "Mengyu", ""], ["Islam", "Md Tauhidul", ""], ["Yu", "Lequan", ""], ["Xing", "Lei", ""]]}, {"id": "2007.11087", "submitter": "Youbao Tang", "authors": "Youbao Tang, Ke Yan, Jing Xiao and Ranold M. Summers", "title": "One Click Lesion RECIST Measurement and Segmentation on CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials, one of the radiologists' routine work is to measure tumor\nsizes on medical images using the RECIST criteria (Response Evaluation Criteria\nIn Solid Tumors). However, manual measurement is tedious and subject to\ninter-observer variability. We propose a unified framework named SEENet for\nsemi-automatic lesion \\textit{SE}gmentation and RECIST \\textit{E}stimation on a\nvariety of lesions over the entire human body. The user is only required to\nprovide simple guidance by clicking once near the lesion. SEENet consists of\ntwo main parts. The first one extracts the lesion of interest with the\none-click guidance, roughly segments the lesion, and estimates its RECIST\nmeasurement. Based on the results of the first network, the second one refines\nthe lesion segmentation and RECIST estimation. SEENet achieves state-of-the-art\nperformance in lesion segmentation and RECIST estimation on the large-scale\npublic DeepLesion dataset. It offers a practical tool for radiologists to\ngenerate reliable lesion measurements (i.e. segmentation mask and RECIST) with\nminimal human effort and greatly reduced time.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 20:53:43 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tang", "Youbao", ""], ["Yan", "Ke", ""], ["Xiao", "Jing", ""], ["Summers", "Ranold M.", ""]]}, {"id": "2007.11089", "submitter": "Jos\\'e Cano", "authors": "Martina Lofqvist, Jos\\'e Cano", "title": "Accelerating Deep Learning Applications in Space", "comments": "Published as a workshop paper at SmallSat 2020 - The 34th Annual\n  Small Satellite Conference. 19 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computing at the edge offers intriguing possibilities for the development of\nautonomy and artificial intelligence. The advancements in autonomous\ntechnologies and the resurgence of computer vision have led to a rise in demand\nfor fast and reliable deep learning applications. In recent years, the industry\nhas introduced devices with impressive processing power to perform various\nobject detection tasks. However, with real-time detection, devices are\nconstrained in memory, computational capacity, and power, which may compromise\nthe overall performance. This could be solved either by optimizing the object\ndetector or modifying the images. In this paper, we investigate the performance\nof CNN-based object detectors on constrained devices when applying different\nimage compression techniques. We examine the capabilities of a NVIDIA Jetson\nNano; a low-power, high-performance computer, with an integrated GPU, small\nenough to fit on-board a CubeSat. We take a closer look at the Single Shot\nMultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN)\nthat are pre-trained on DOTA - a Large Scale Dataset for Object Detection in\nAerial Images. The performance is measured in terms of inference time, memory\nconsumption, and accuracy. By applying image compression techniques, we are\nable to optimize performance. The two techniques applied, lossless compression\nand image scaling, improves speed and memory consumption with no or little\nchange in accuracy. The image scaling technique achieves a 100% runnable\ndataset and we suggest combining both techniques in order to optimize the\nspeed/memory/accuracy trade-off.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:06:30 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lofqvist", "Martina", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2007.11110", "submitter": "Benjamin Biggs", "authors": "Benjamin Biggs, Oliver Boyne, James Charles, Andrew Fitzgibbon and\n  Roberto Cipolla", "title": "Who Left the Dogs Out? 3D Animal Reconstruction with Expectation\n  Maximization in the Loop", "comments": "Accepted at ECCV 2020", "journal-ref": "16th European Conference Glasgow UK August 23 to 28 2020\n  Proceedings Part XI", "doi": "10.1007/978-3-030-58621-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an automatic, end-to-end method for recovering the 3D pose and\nshape of dogs from monocular internet images. The large variation in shape\nbetween dog breeds, significant occlusion and low quality of internet images\nmakes this a challenging problem. We learn a richer prior over shapes than\nprevious work, which helps regularize parameter estimation. We demonstrate\nresults on the Stanford Dog dataset, an 'in the wild' dataset of 20,580 dog\nimages for which we have collected 2D joint and silhouette annotations to split\nfor training and evaluation. In order to capture the large shape variety of\ndogs, we show that the natural variation in the 2D dataset is enough to learn a\ndetailed 3D prior through expectation maximization (EM). As a by-product of\ntraining, we generate a new parameterized model (including limb scaling) SMBLD\nwhich we release alongside our new annotation dataset StanfordExtra to the\nresearch community.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:52:56 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 13:47:24 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Biggs", "Benjamin", ""], ["Boyne", "Oliver", ""], ["Charles", "James", ""], ["Fitzgibbon", "Andrew", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2007.11118", "submitter": "Tarun Srivastava", "authors": "Ollie Matthews, Koki Ryu, Tarun Srivastava", "title": "Creating a Large-scale Synthetic Dataset for Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating and labelling datasets of videos for use in training Human Activity\nRecognition models is an arduous task. In this paper, we approach this by using\n3D rendering tools to generate a synthetic dataset of videos, and show that a\nclassifier trained on these videos can generalise to real videos. We use five\ndifferent augmentation techniques to generate the videos, leading to a wide\nvariety of accurately labelled unique videos. We fine tune a pre-trained I3D\nmodel on our videos, and find that the model is able to achieve a high accuracy\nof 73% on the HMDB51 dataset over three classes. We also find that augmenting\nthe HMDB training set with our dataset provides a 2% improvement in the\nperformance of the classifier. Finally, we discuss possible extensions to the\ndataset, including virtual try on and modeling motion of the people.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:20:21 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Matthews", "Ollie", ""], ["Ryu", "Koki", ""], ["Srivastava", "Tarun", ""]]}, {"id": "2007.11142", "submitter": "Gilles Puy", "authors": "Gilles Puy and Alexandre Boulch and Renaud Marlet", "title": "FLOT: Scene Flow on Point Clouds Guided by Optimal Transport", "comments": "Accepted at ECCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a method called FLOT that estimates scene flow on point\nclouds. We start the design of FLOT by noticing that scene flow estimation on\npoint clouds reduces to estimating a permutation matrix in a perfect world.\nInspired by recent works on graph matching, we build a method to find these\ncorrespondences by borrowing tools from optimal transport. Then, we relax the\ntransport constraints to take into account real-world imperfections. The\ntransport cost between two points is given by the pairwise similarity between\ndeep features extracted by a neural network trained under full supervision\nusing synthetic datasets. Our main finding is that FLOT can perform as well as\nthe best existing methods on synthetic and real-world datasets while requiring\nmuch less parameters and without using multiscale analysis. Our second finding\nis that, on the training datasets considered, most of the performance can be\nexplained by the learned transport cost. This yields a simpler method,\nFLOT$_0$, which is obtained using a particular choice of optimal transport\nparameters and performs nearly as well as FLOT.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 00:15:30 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Puy", "Gilles", ""], ["Boulch", "Alexandre", ""], ["Marlet", "Renaud", ""]]}, {"id": "2007.11149", "submitter": "Sk Miraj Ahmed", "authors": "Sk Miraj Ahmed, Aske R Lejb{\\o}lle, Rameswar Panda, Amit K.\n  Roy-Chowdhury", "title": "Camera On-boarding for Person Re-identification using Hypothesis\n  Transfer Learning", "comments": "Accepted to CVPR 2020", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (2020) 12144-12153", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing approaches for person re-identification consider a\nstatic setting where the number of cameras in the network is fixed. An\ninteresting direction, which has received little attention, is to explore the\ndynamic nature of a camera network, where one tries to adapt the existing\nre-identification models after on-boarding new cameras, with little additional\neffort. There have been a few recent methods proposed in person\nre-identification that attempt to address this problem by assuming the labeled\ndata in the existing network is still available while adding new cameras. This\nis a strong assumption since there may exist some privacy issues for which one\nmay not have access to those data. Rather, based on the fact that it is easy to\nstore the learned re-identifications models, which mitigates any data privacy\nconcern, we develop an efficient model adaptation approach using hypothesis\ntransfer learning that aims to transfer the knowledge using only source models\nand limited labeled data, but without using any source camera data from the\nexisting network. Our approach minimizes the effect of negative transfer by\nfinding an optimal weighted combination of multiple source models for\ntransferring the knowledge. Extensive experiments on four challenging benchmark\ndatasets with a variable number of cameras well demonstrate the efficacy of our\nproposed approach over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 00:43:29 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 21:48:31 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Ahmed", "Sk Miraj", ""], ["Lejb\u00f8lle", "Aske R", ""], ["Panda", "Rameswar", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2007.11154", "submitter": "Kamalesh Palanisamy", "authors": "Kamalesh Palanisamy, Dipika Singhania, Angela Yao", "title": "Rethinking CNN Models for Audio Classification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that ImageNet-Pretrained standard deep CNN models can\nbe used as strong baseline networks for audio classification. Even though there\nis a significant difference between audio Spectrogram and standard ImageNet\nimage samples, transfer learning assumptions still hold firmly. To understand\nwhat enables the ImageNet pretrained models to learn useful audio\nrepresentations, we systematically study how much of pretrained weights is\nuseful for learning spectrograms. We show (1) that for a given standard model\nusing pretrained weights is better than using randomly initialized weights (2)\nqualitative results of what the CNNs learn from the spectrograms by visualizing\nthe gradients. Besides, we show that even though we use the pretrained model\nweights for initialization, there is variance in performance in various output\nruns of the same model. This variance in performance is due to the random\ninitialization of linear classification layer and random mini-batch orderings\nin multiple runs. This brings significant diversity to build stronger ensemble\nmodels with an overall improvement in accuracy. An ensemble of ImageNet\npretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset\nand 87.42% validation accuracy on the UrbanSound8K dataset which is the current\nstate-of-the-art on both of these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 01:31:44 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 19:09:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Palanisamy", "Kamalesh", ""], ["Singhania", "Dipika", ""], ["Yao", "Angela", ""]]}, {"id": "2007.11180", "submitter": "Jiawei Chen", "authors": "Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma and Yefeng\n  Zheng", "title": "MI^2GAN: Generative Adversarial Network for Medical Image Domain\n  Adaptation using Mutual Information Constraint", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain shift between medical images from multicentres is still an open\nquestion for the community, which degrades the generalization performance of\ndeep learning models. Generative adversarial network (GAN), which synthesize\nplausible images, is one of the potential solutions to address the problem.\nHowever, the existing GAN-based approaches are prone to fail at preserving\nimage-objects in image-to-image (I2I) translation, which reduces their\npracticality on domain adaptation tasks. In this paper, we propose a novel GAN\n(namely MI$^2$GAN) to maintain image-contents during cross-domain I2I\ntranslation. Particularly, we disentangle the content features from domain\ninformation for both the source and translated images, and then maximize the\nmutual information between the disentangled content features to preserve the\nimage-objects. The proposed MI$^2$GAN is evaluated on two tasks---polyp\nsegmentation using colonoscopic images and the segmentation of optic disc and\ncup in fundus images. The experimental results demonstrate that the proposed\nMI$^2$GAN can not only generate elegant translated images, but also\nsignificantly improve the generalization performance of widely used deep\nlearning networks (e.g., U-Net).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 03:19:54 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 07:57:03 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Xie", "Xinpeng", ""], ["Chen", "Jiawei", ""], ["Li", "Yuexiang", ""], ["Shen", "Linlin", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.11186", "submitter": "Chen Jiawei", "authors": "Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma and Yefeng\n  Zheng", "title": "Instance-aware Self-supervised Learning for Nuclei Segmentation", "comments": "MICCAI 2020; The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide existence and large morphological variances of nuclei,\naccurate nuclei instance segmentation is still one of the most challenging\ntasks in computational pathology. The annotating of nuclei instances, requiring\nexperienced pathologists to manually draw the contours, is extremely laborious\nand expensive, which often results in the deficiency of annotated data. The\ndeep learning based segmentation approaches, which highly rely on the quantity\nof training data, are difficult to fully demonstrate their capacity in this\narea. In this paper, we propose a novel self-supervised learning framework to\ndeeply exploit the capacity of widely-used convolutional neural networks (CNNs)\non the nuclei instance segmentation task. The proposed approach involves two\nsub-tasks (i.e., scale-wise triplet learning and count ranking), which enable\nneural networks to implicitly leverage the prior-knowledge of nuclei size and\nquantity, and accordingly mine the instance-aware feature representations from\nthe raw data. Experimental results on the publicly available MoNuSeg dataset\nshow that the proposed self-supervised learning approach can remarkably boost\nthe segmentation accuracy of nuclei instance---a new state-of-the-art average\nAggregated Jaccard Index (AJI) of 70.63%, is achieved by our self-supervised\nResUNet-101. To our best knowledge, this is the first work focusing on the\nself-supervised learning for instance segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 03:37:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xie", "Xinpeng", ""], ["Chen", "Jiawei", ""], ["Li", "Yuexiang", ""], ["Shen", "Linlin", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.11222", "submitter": "Orkhan Baghirli", "authors": "Orkhan Baghirli, Imran Ibrahimli, and Tarlan Mammadzada", "title": "Greenhouse Segmentation on High-Resolution Optical Satellite Imagery\n  using Deep Learning Techniques", "comments": "12 pages, 14 Figures, 3 Tables, uses arxiv.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greenhouse segmentation has pivotal importance for climate-smart agricultural\nland-use planning. Deep learning-based approaches provide state-of-the-art\nperformance in natural image segmentation. However, semantic segmentation on\nhigh-resolution optical satellite imagery is a challenging task because of the\ncomplex environment. In this paper, a sound methodology is proposed for\npixel-wise classification on images acquired by the Azersky (SPOT-7) optical\nsatellite. In particular, customized variations of U-Net-like architectures are\nemployed to identify greenhouses. Two models are proposed which uniquely\nincorporate dilated convolutions and skip connections, and the results are\ncompared to that of the baseline U-Net model. The dataset used consists of\npan-sharpened orthorectified Azersky images (red, green, blue,and near infrared\nchannels) with 1.5-meter resolution and annotation masks, collected from 15\nregions in Azerbaijan where the greenhouses are densely congested. The images\ncover the cumulative area of 1008 $km^2$ and annotation masks contain 47559\npolygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used for\nperformance evaluation. It is observed that the use of the deconvolutional\nlayers alone throughout the expansive path does not yield satisfactory results;\ntherefore, they are either replaced or coupled with bilinear interpolation. All\nmodels benefit from the hard example mining (HEM) strategy. It is also reported\nthat the best accuracy of $93.29\\%$ ($F_1\\,score$) is recorded when the\nweighted binary cross-entropy loss is coupled with the dice loss. Experimental\nresults showed that both of the proposed models outperformed the baseline U-Net\narchitecture such that the best model proposed scored $4.48\\%$ higher in\ncomparison to the baseline architecture.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 06:12:57 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Baghirli", "Orkhan", ""], ["Ibrahimli", "Imran", ""], ["Mammadzada", "Tarlan", ""]]}, {"id": "2007.11240", "submitter": "Gusi Te", "authors": "Gusi Te, Yinglu Liu, Wei Hu, Hailin Shi, and Tao Mei", "title": "Edge-aware Graph Representation Learning and Reasoning for Face Parsing", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face parsing infers a pixel-wise label to each facial component, which has\ndrawn much attention recently. Previous methods have shown their efficiency in\nface parsing, which however overlook the correlation among different face\nregions. The correlation is a critical clue about the facial appearance, pose,\nexpression etc., and should be taken into account for face parsing. To this\nend, we propose to model and reason the region-wise relations by learning graph\nrepresentations, and leverage the edge information between regions for\noptimized abstraction. Specifically, we encode a facial image onto a global\ngraph representation where a collection of pixels (\"regions\") with similar\nfeatures are projected to each vertex. Our model learns and reasons over\nrelations between the regions by propagating information across vertices on the\ngraph. Furthermore, we incorporate the edge information to aggregate the\npixel-wise features onto vertices, which emphasizes on the features around\nedges for fine segmentation along edges. The finally learned graph\nrepresentation is projected back to pixel grids for parsing. Experiments\ndemonstrate that our model outperforms state-of-the-art methods on the widely\nused Helen dataset, and also exhibits the superior performance on the\nlarge-scale CelebAMask-HQ and LaPa dataset. The code is available at\nhttps://github.com/tegusi/EAGRNet.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 07:46:34 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Te", "Gusi", ""], ["Liu", "Yinglu", ""], ["Hu", "Wei", ""], ["Shi", "Hailin", ""], ["Mei", "Tao", ""]]}, {"id": "2007.11245", "submitter": "Qingchao Zhang", "authors": "Yunmei Chen, Hongcheng Liu, Xiaojing Ye, Qingchao Zhang", "title": "Learnable Descent Algorithm for Nonsmooth Nonconvex Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general learning based framework for solving nonsmooth and\nnonconvex image reconstruction problems. We model the regularization function\nas the composition of the $l_{2,1}$ norm and a smooth but nonconvex feature\nmapping parametrized as a deep convolutional neural network. We develop a\nprovably convergent descent-type algorithm to solve the nonsmooth nonconvex\nminimization problem by leveraging the Nesterov's smoothing technique and the\nidea of residual learning, and learn the network parameters such that the\noutputs of the algorithm match the references in training data. Our method is\nversatile as one can employ various modern network structures into the\nregularization, and the resulting network inherits the guaranteed convergence\nof the algorithm. We also show that the proposed network is parameter-efficient\nand its performance compares favorably to the state-of-the-art methods in a\nvariety of image reconstruction problems in practice.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 07:59:07 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 21:07:03 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 03:47:44 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chen", "Yunmei", ""], ["Liu", "Hongcheng", ""], ["Ye", "Xiaojing", ""], ["Zhang", "Qingchao", ""]]}, {"id": "2007.11246", "submitter": "Mehdi Teimouri", "authors": "Mehdi Teimouri, Zahra Seyedghorban, Fatemeh Amirjani", "title": "Fragments-Expert: A Graphical User Interface MATLAB Toolbox for\n  Classification of File Fragments", "comments": "47 Pages, 34 Figures, and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of file fragments of various file formats is an essential\ntask in various applications such as firewalls, intrusion detection systems,\nanti-viruses, web content filtering, and digital forensics. However, the\ncommunity lacks a suitable software tool that can integrate major methods for\nfeature extraction from file fragments and classification among various file\nformats. In this paper, we present Fragments-Expert that is a graphical user\ninterface MATLAB toolbox for the classification of file fragments. It provides\nusers with 22 categories of features extracted from file fragments. These\nfeatures can be employed by 7 categories of machine learning algorithms for the\ntask of classification among various file formats.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:03:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Teimouri", "Mehdi", ""], ["Seyedghorban", "Zahra", ""], ["Amirjani", "Fatemeh", ""]]}, {"id": "2007.11255", "submitter": "Markus Horn", "authors": "Markus Horn, Nico Engel, Vasileios Belagiannis, Michael Buchholz and\n  Klaus Dietmayer", "title": "DeepCLR: Correspondence-Less Architecture for Deep End-to-End Point\n  Cloud Registration", "comments": "7 pages, 5 figures, 4 tables", "journal-ref": "2020 IEEE 23rd International Conference on Intelligent\n  Transportation Systems (ITSC)", "doi": "10.1109/ITSC45102.2020.9294279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of point cloud registration using deep neural\nnetworks. We propose an approach to predict the alignment between two point\nclouds with overlapping data content, but displaced origins. Such point clouds\noriginate, for example, from consecutive measurements of a LiDAR mounted on a\nmoving platform. The main difficulty in deep registration of raw point clouds\nis the fusion of template and source point cloud. Our proposed architecture\napplies flow embedding to tackle this problem, which generates features that\ndescribe the motion of each template point. These features are then used to\npredict the alignment in an end-to-end fashion without extracting explicit\npoint correspondences between both input clouds. We rely on the KITTI odometry\nand ModelNet40 datasets for evaluating our method on various point\ndistributions. Our approach achieves state-of-the-art accuracy and the lowest\nrun-time of the compared methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:20:57 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 10:04:51 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Horn", "Markus", ""], ["Engel", "Nico", ""], ["Belagiannis", "Vasileios", ""], ["Buchholz", "Michael", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2007.11256", "submitter": "Tian Chen", "authors": "Tian Chen, Shijie An, Yuan Zhang, Chongyang Ma, Huayan Wang, Xiaoyan\n  Guo, and Wen Zheng", "title": "Improving Monocular Depth Estimation by Leveraging Structural Awareness\n  and Complementary Datasets", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Monocular depth estimation plays a crucial role in 3D recognition and\nunderstanding. One key limitation of existing approaches lies in their lack of\nstructural information exploitation, which leads to inaccurate spatial layout,\ndiscontinuous surface, and ambiguous boundaries. In this paper, we tackle this\nproblem in three aspects. First, to exploit the spatial relationship of visual\nfeatures, we propose a structure-aware neural network with spatial attention\nblocks. These blocks guide the network attention to global structures or local\ndetails across different feature layers. Second, we introduce a global focal\nrelative loss for uniform point pairs to enhance spatial constraint in the\nprediction, and explicitly increase the penalty on errors in depth-wise\ndiscontinuous regions, which helps preserve the sharpness of estimation\nresults. Finally, based on analysis of failure cases for prior methods, we\ncollect a new Hard Case (HC) Depth dataset of challenging scenes, such as\nspecial lighting conditions, dynamic objects, and tilted camera angles. The new\ndataset is leveraged by an informed learning curriculum that mixes training\nexamples incrementally to handle diverse data distributions. Experimental\nresults show that our method outperforms state-of-the-art approaches by a large\nmargin in terms of both prediction accuracy on NYUDv2 dataset and\ngeneralization performance on unseen datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:21:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Chen", "Tian", ""], ["An", "Shijie", ""], ["Zhang", "Yuan", ""], ["Ma", "Chongyang", ""], ["Wang", "Huayan", ""], ["Guo", "Xiaoyan", ""], ["Zheng", "Wen", ""]]}, {"id": "2007.11257", "submitter": "Ao Luo", "authors": "Ao Luo, Ning Xie, Zhijia Tao, Feng Jiang", "title": "Deep-VFX: Deep Action Recognition Driven VFX for Short Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion is a key function to communicate information. In the\napplication, short-form mobile video is so popular all over the world such as\nTik Tok. The users would like to add more VFX so as to pursue creativity and\npersonlity. Many special effects are added on the short video platform. These\ngives the users more possibility to show off these personality. The common and\ntraditional way is to create the template of VFX. However, in order to\nsynthesis the perfect, the users have to tedious attempt to grasp the timing\nand rhythm of new templates. It is not easy-to-use especially for the mobile\napp. This paper aims to change the VFX synthesis by motion driven instead of\nthe traditional template matching. We propose the AI method to improve this VFX\nsynthesis. In detail, in order to add the special effect on the human body. The\nskeleton extraction is essential in this system. We also propose a novel form\nof LSTM to find out the user's intention by action recognition. The experiment\nshows that our system enables to generate VFX for short video more easier and\nefficient.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:27:52 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Luo", "Ao", ""], ["Xie", "Ning", ""], ["Tao", "Zhijia", ""], ["Jiang", "Feng", ""]]}, {"id": "2007.11259", "submitter": "Matteo Terzi", "authors": "Matteo Terzi, Alessandro Achille, Marco Maggipinto, Gian Antonio Susto", "title": "Adversarial Training Reduces Information and Improves Transferability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results show that features of adversarially trained networks for\nclassification, in addition to being robust, enable desirable properties such\nas invertibility. The latter property may seem counter-intuitive as it is\nwidely accepted by the community that classification models should only capture\nthe minimal information (features) required for the task. Motivated by this\ndiscrepancy, we investigate the dual relationship between Adversarial Training\nand Information Theory. We show that the Adversarial Training can improve\nlinear transferability to new tasks, from which arises a new trade-off between\ntransferability of representations and accuracy on the source task. We validate\nour results employing robust networks trained on CIFAR-10, CIFAR-100 and\nImageNet on several datasets. Moreover, we show that Adversarial Training\nreduces Fisher information of representations about the input and of the\nweights about the task, and we provide a theoretical argument which explains\nthe invertibility of deterministic networks without violating the principle of\nminimality. Finally, we leverage our theoretical insights to remarkably improve\nthe quality of reconstructed images through inversion.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:30:16 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 09:42:50 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 15:04:47 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 22:52:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Terzi", "Matteo", ""], ["Achille", "Alessandro", ""], ["Maggipinto", "Marco", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2007.11301", "submitter": "Alexandre Carlier", "authors": "Alexandre Carlier, Martin Danelljan, Alexandre Alahi, Radu Timofte", "title": "DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable Vector Graphics (SVG) are ubiquitous in modern 2D interfaces due to\ntheir ability to scale to different resolutions. However, despite the success\nof deep learning-based models applied to rasterized images, the problem of\nvector graphics representation learning and generation remains largely\nunexplored. In this work, we propose a novel hierarchical generative network,\ncalled DeepSVG, for complex SVG icons generation and interpolation. Our\narchitecture effectively disentangles high-level shapes from the low-level\ncommands that encode the shape itself. The network directly predicts a set of\nshapes in a non-autoregressive fashion. We introduce the task of complex SVG\nicons generation by releasing a new large-scale dataset along with an\nopen-source library for SVG manipulation. We demonstrate that our network\nlearns to accurately reconstruct diverse vector graphics, and can serve as a\npowerful animation tool by performing interpolations and other latent space\noperations. Our code is available at https://github.com/alexandre01/deepsvg.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 09:36:31 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:41:45 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 14:31:42 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Carlier", "Alexandre", ""], ["Danelljan", "Martin", ""], ["Alahi", "Alexandre", ""], ["Timofte", "Radu", ""]]}, {"id": "2007.11318", "submitter": "Kenneth Lai", "authors": "K. Lai, S. Samoil, and S.N.Yanushkevich", "title": "Multi-Spectral Facial Biometrics in Access Control", "comments": null, "journal-ref": "2014 IEEE Symposium on Computational Intelligence in Biometrics\n  and Identity Management (CIBIM), Orlando, FL, 2014, pp. 102-109", "doi": "10.1109/CIBIM.2014.7015450", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study demonstrates how facial biometrics, acquired using multi-spectral\nsensors, such as RGB, depth, and infrared, assist the data accumulation in the\nprocess of authorizing users of automated and semi-automated access systems.\nThis data serves the purposes of person authentication, as well as facial\ntemperature estimation. We utilize depth data taken using an inexpensive RGB-D\nsensor to find the head pose of a subject. This allows the selection of video\nframes containing a frontal-view head pose for face recognition and face\ntemperature reading. Usage of the frontal-view frames improves the efficiency\nof face recognition while the corresponding synchronized IR video frames allow\nfor more efficient temperature estimation for facial regions of interest. In\naddition, this study reports emerging applications of biometrics in biomedical\nand health care solutions. Including surveys of recent pilot projects,\ninvolving new sensors of biometric data and new applications of human\nphysiological and behavioral biometrics. It also shows the new and promising\nhorizons of using biometrics in natural and contactless control interfaces for\nsurgical control, rehabilitation and accessibility.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:16:05 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lai", "K.", ""], ["Samoil", "S.", ""], ["Yanushkevich", "S. N.", ""]]}, {"id": "2007.11319", "submitter": "Mobarakol Islam", "authors": "Mobarakol Islam, Daniel A. Atputharuban, Ravikiran Ramesh, Hongliang\n  Ren", "title": "Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary\n  Supervised Deep Adversarial Learning", "comments": "Published in IEEE RAL", "journal-ref": null, "doi": "10.1109/LRA.2019.2900854", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted surgery is an emerging technology which has undergone rapid\ngrowth with the development of robotics and imaging systems. Innovations in\nvision, haptics and accurate movements of robot arms have enabled surgeons to\nperform precise minimally invasive surgeries. Real-time semantic segmentation\nof the robotic instruments and tissues is a crucial step in robot-assisted\nsurgery. Accurate and efficient segmentation of the surgical scene not only\naids in the identification and tracking of instruments but also provided\ncontextual information about the different tissues and instruments being\noperated with. For this purpose, we have developed a light-weight cascaded\nconvolutional neural network (CNN) to segment the surgical instruments from\nhigh-resolution videos obtained from a commercial robotic system. We propose a\nmulti-resolution feature fusion module (MFF) to fuse the feature maps of\ndifferent dimensions and channels from the auxiliary and main branch. We also\nintroduce a novel way of combining auxiliary loss and adversarial loss to\nregularize the segmentation model. Auxiliary loss helps the model to learn\nlow-resolution features, and adversarial loss improves the segmentation\nprediction by learning higher order structural information. The model also\nconsists of a light-weight spatial pyramid pooling (SPP) unit to aggregate rich\ncontextual information in the intermediate stage. We show that our model\nsurpasses existing algorithms for pixel-wise segmentation of surgical\ninstruments in both prediction accuracy and segmentation time of\nhigh-resolution videos.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:16:07 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:07:50 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Islam", "Mobarakol", ""], ["Atputharuban", "Daniel A.", ""], ["Ramesh", "Ravikiran", ""], ["Ren", "Hongliang", ""]]}, {"id": "2007.11323", "submitter": "Kenneth Lai", "authors": "Kenneth Lai, Svetlana N. Yanushkevich, and Vlad Shmerko", "title": "Risk Assessment in the Face-based Watchlist Screening in e-Border", "comments": null, "journal-ref": "2017 IEEE International Joint Conference on Biometrics (IJCB),\n  Denver, CO, 2017, pp. 16-21", "doi": "10.1109/BTAS.2017.8272677", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns with facial-based watchlist technology as a component of\nautomated border control machines deployed in e-borders. The key task of the\nwatchlist technology is to mitigate effects of mis-identification and\nimpersonation. To address this problem, we developed a novel cost-based model\nof traveler risk assessment and proved its efficiency via intensive experiments\nusing large-scale facial databases. The results of this study are applicable to\nany biometric modality to be used in watchlist technology.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:20:22 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lai", "Kenneth", ""], ["Yanushkevich", "Svetlana N.", ""], ["Shmerko", "Vlad", ""]]}, {"id": "2007.11328", "submitter": "Kenneth Lai", "authors": "K. Lai and S.N. Yanushkevich", "title": "Watchlist Risk Assessment using Multiparametric Cost and Relative\n  Entropy", "comments": null, "journal-ref": "2017 IEEE Symposium Series on Computational Intelligence (SSCI),\n  Honolulu, HI, 2017, pp. 1-7", "doi": "10.1109/SSCI.2017.8285219", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the facial biometric-enabled watchlist technology in\nwhich risk detectors are mandatory mechanisms for early detection of threats,\nas well as for avoiding offense to innocent travelers. We propose a\nmultiparametric cost assessment and relative entropy measures as risk\ndetectors. We experimentally demonstrate the effects of mis-identification and\nimpersonation under various watchlist screening scenarios and constraints. The\nkey contributions of this paper are the novel techniques for design and\nanalysis of the biometric-enabled watchlist and the supporting infrastructure,\nas well as measuring the impersonation impact on e-border performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:27:53 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lai", "K.", ""], ["Yanushkevich", "S. N.", ""]]}, {"id": "2007.11330", "submitter": "Qing Yu", "authors": "Qing Yu, Daiki Ikami, Go Irie, Kiyoharu Aizawa", "title": "Multi-Task Curriculum Framework for Open-Set Semi-Supervised Learning", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) has been proposed to leverage unlabeled data\nfor training powerful models when only limited labeled data is available. While\nexisting SSL methods assume that samples in the labeled and unlabeled data\nshare the classes of their samples, we address a more complex novel scenario\nnamed open-set SSL, where out-of-distribution (OOD) samples are contained in\nunlabeled data. Instead of training an OOD detector and SSL separately, we\npropose a multi-task curriculum learning framework. First, to detect the OOD\nsamples in unlabeled data, we estimate the probability of the sample belonging\nto OOD. We use a joint optimization framework, which updates the network\nparameters and the OOD score alternately. Simultaneously, to achieve high\nperformance on the classification of in-distribution (ID) data, we select ID\nsamples in unlabeled data having small OOD scores, and use these data with\nlabeled data for training the deep neural networks to classify ID samples in a\nsemi-supervised manner. We conduct several experiments, and our method achieves\nstate-of-the-art results by successfully eliminating the effect of OOD samples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:33:55 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Yu", "Qing", ""], ["Ikami", "Daiki", ""], ["Irie", "Go", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2007.11341", "submitter": "Keyang Zhou", "authors": "Keyang Zhou, Bharat Lal Bhatnagar, Gerard Pons-Moll", "title": "Unsupervised Shape and Pose Disentanglement for 3D Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric models of humans, faces, hands and animals have been widely used\nfor a range of tasks such as image-based reconstruction, shape correspondence\nestimation, and animation. Their key strength is the ability to factor surface\nvariations into shape and pose dependent components. Learning such models\nrequires lots of expert knowledge and hand-defined object-specific constraints,\nmaking the learning approach unscalable to novel objects. In this paper, we\npresent a simple yet effective approach to learn disentangled shape and pose\nrepresentations in an unsupervised setting. We use a combination of\nself-consistency and cross-consistency constraints to learn pose and shape\nspace from registered meshes. We additionally incorporate as-rigid-as-possible\ndeformation(ARAP) into the training loop to avoid degenerate solutions. We\ndemonstrate the usefulness of learned representations through a number of tasks\nincluding pose transfer and shape retrieval. The experiments on datasets of 3D\nhumans, faces, hands and animals demonstrate the generality of our approach.\nCode is made available at\nhttps://virtualhumans.mpi-inf.mpg.de/unsup_shape_pose/.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:00:27 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhou", "Keyang", ""], ["Bhatnagar", "Bharat Lal", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2007.11344", "submitter": "Patrick Hemmer", "authors": "Patrick Hemmer, Niklas K\\\"uhl and Jakob Sch\\\"offer", "title": "DEAL: Deep Evidential Active Learning for Image Classification", "comments": "Extended version of the paper \"DEAL: Deep Evidential Active Learning\n  for Image Classification\" accepted for publication at ICMLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proven to be state-of-the-art\nmodels for supervised computer vision tasks, such as image classification.\nHowever, large labeled data sets are generally needed for the training and\nvalidation of such models. In many domains, unlabeled data is available but\nlabeling is expensive, for instance when specific expert knowledge is required.\nActive Learning (AL) is one approach to mitigate the problem of limited labeled\ndata. Through selecting the most informative and representative data instances\nfor labeling, AL can contribute to more efficient learning of the model. Recent\nAL methods for CNNs propose different solutions for the selection of instances\nto be labeled. However, they do not perform consistently well and are often\ncomputationally expensive. In this paper, we propose a novel AL algorithm that\nefficiently learns from unlabeled data by capturing high prediction\nuncertainty. By replacing the softmax standard output of a CNN with the\nparameters of a Dirichlet density, the model learns to identify data instances\nthat contribute efficiently to improving model performance during training. We\ndemonstrate in several experiments with publicly available data that our method\nconsistently outperforms other state-of-the-art AL approaches. It can be easily\nimplemented and does not require extensive computational resources for\ntraining. Additionally, we are able to show the benefits of the approach on a\nreal-world medical use case in the field of automated detection of visual\nsignals for pneumonia on chest radiographs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:14:23 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 07:35:51 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hemmer", "Patrick", ""], ["K\u00fchl", "Niklas", ""], ["Sch\u00f6ffer", "Jakob", ""]]}, {"id": "2007.11349", "submitter": "Feng Cheng", "authors": "Feng Cheng, Cheng Chen, Yukang Wang, Heshui Shi, Yukun Cao, Dandan Tu,\n  Changzheng Zhang, Yongchao Xu", "title": "Learning Directional Feature Maps for Cardiac MRI Segmentation", "comments": "Accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac MRI segmentation plays a crucial role in clinical diagnosis for\nevaluating personalized cardiac performance parameters. Due to the indistinct\nboundaries and heterogeneous intensity distributions in the cardiac MRI, most\nexisting methods still suffer from two aspects of challenges: inter-class\nindistinction and intra-class inconsistency. To tackle these two problems, we\npropose a novel method to exploit the directional feature maps, which can\nsimultaneously strengthen the differences between classes and the similarities\nwithin classes. Specifically, we perform cardiac segmentation and learn a\ndirection field pointing away from the nearest cardiac tissue boundary to each\npixel via a direction field (DF) module. Based on the learned direction field,\nwe then propose a feature rectification and fusion (FRF) module to improve the\noriginal segmentation features, and obtain the final segmentation. The proposed\nmodules are simple yet effective and can be flexibly added to any existing\nsegmentation network without excessively increasing time and space complexity.\nWe evaluate the proposed method on the 2017 MICCAI Automated Cardiac Diagnosis\nChallenge (ACDC) dataset and a large-scale self-collected dataset, showing good\nsegmentation performance and robust generalization ability of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:31:04 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cheng", "Feng", ""], ["Chen", "Cheng", ""], ["Wang", "Yukang", ""], ["Shi", "Heshui", ""], ["Cao", "Yukun", ""], ["Tu", "Dandan", ""], ["Zhang", "Changzheng", ""], ["Xu", "Yongchao", ""]]}, {"id": "2007.11355", "submitter": "Shuang Yu", "authors": "Junde Wu, Shuang Yu, Wenting Chen, Kai Ma, Rao Fu, Hanruo Liu,\n  Xiaoguang Di and Yefeng Zheng", "title": "Leveraging Undiagnosed Data for Glaucoma Classification with\n  Teacher-Student Learning", "comments": null, "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, deep learning has been adopted to the glaucoma classification task\nwith performance comparable to that of human experts. However, a well trained\ndeep learning model demands a large quantity of properly labeled data, which is\nrelatively expensive since the accurate labeling of glaucoma requires years of\nspecialist training. In order to alleviate this problem, we propose a glaucoma\nclassification framework which takes advantage of not only the properly labeled\nimages, but also undiagnosed images without glaucoma labels. To be more\nspecific, the proposed framework is adapted from the teacher-student-learning\nparadigm. The teacher model encodes the wrapped information of undiagnosed\nimages to a latent feature space, meanwhile the student model learns from the\nteacher through knowledge transfer to improve the glaucoma classification. For\nthe model training procedure, we propose a novel training strategy that\nsimulates the real-world teaching practice named as 'Learning To Teach with\nKnowledge Transfer (L2T-KT)', and establish a 'Quiz Pool' as the teacher's\noptimization target. Experiments show that the proposed framework is able to\nutilize the undiagnosed data effectively to improve the glaucoma prediction\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:05:26 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Wu", "Junde", ""], ["Yu", "Shuang", ""], ["Chen", "Wenting", ""], ["Ma", "Kai", ""], ["Fu", "Rao", ""], ["Liu", "Hanruo", ""], ["Di", "Xiaoguang", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.11361", "submitter": "Gregor Koporec", "authors": "Gregor Koporec and Janez Per\\v{s}", "title": "Human-Centered Unsupervised Segmentation Fusion", "comments": "Accepted to the IROS2019 Workshop: Benchmark and Dataset for\n  Probabilistic Prediction of Interactive Human Behavior, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is generally an ill-posed problem since it results in multiple\nsolutions and is, therefore, hard to define ground truth data to evaluate\nalgorithms. The problem can be naively surpassed by using only one annotator\nper image, but such acquisition doesn't represent the cognitive perception of\nan image by the majority of people. Nowadays, it is not difficult to obtain\nmultiple segmentations with crowdsourcing, so the only problem that stays is\nhow to get one ground truth segmentation per image. There already exist\nnumerous algorithmic solutions, but most methods are supervised or don't\nconsider confidence per human segmentation. In this paper, we introduce a new\nsegmentation fusion model that is based on K-Modes clustering. Results obtained\nfrom publicly available datasets with human ground truth segmentations clearly\nshow that our model outperforms the state-of-the-art on human segmentations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:18:31 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Koporec", "Gregor", ""], ["Per\u0161", "Janez", ""]]}, {"id": "2007.11365", "submitter": "Sudhakar Kumawat", "authors": "Sudhakar Kumawat, Manisha Verma, Yuta Nakashima, and Shanmuganathan\n  Raman", "title": "Depthwise Spatio-Temporal STFT Convolutional Neural Networks for Human\n  Action Recognition", "comments": "Extended version of our CVPR 2019 work", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3076522", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional 3D convolutional neural networks (CNNs) are computationally\nexpensive, memory intensive, prone to overfitting, and most importantly, there\nis a need to improve their feature learning capabilities. To address these\nissues, we propose spatio-temporal short term Fourier transform (STFT) blocks,\na new class of convolutional blocks that can serve as an alternative to the 3D\nconvolutional layer and its variants in 3D CNNs. An STFT block consists of\nnon-trainable convolution layers that capture spatially and/or temporally local\nFourier information using a STFT kernel at multiple low frequency points,\nfollowed by a set of trainable linear weights for learning channel\ncorrelations. The STFT blocks significantly reduce the space-time complexity in\n3D CNNs. In general, they use 3.5 to 4.5 times less parameters and 1.5 to 1.8\ntimes less computational costs when compared to the state-of-the-art methods.\nFurthermore, their feature learning capabilities are significantly better than\nthe conventional 3D convolutional layer and its variants. Our extensive\nevaluation on seven action recognition datasets, including Something-something\nv1 and v2, Jester, Diving-48, Kinetics-400, UCF 101, and HMDB 51, demonstrate\nthat STFT blocks based 3D CNNs achieve on par or even better performance\ncompared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:26:04 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kumawat", "Sudhakar", ""], ["Verma", "Manisha", ""], ["Nakashima", "Yuta", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2007.11392", "submitter": "Sudhir Sornapudi", "authors": "Sudhir Sornapudi, R. Joe Stanley, William V. Stoecker, Rodney Long,\n  Zhiyun Xue, Rosemary Zuna, Shelliane R. Frazier, Sameer Antani", "title": "Feature based Sequential Classifier with Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical cancer is one of the deadliest cancers affecting women globally.\nCervical intraepithelial neoplasia (CIN) assessment using histopathological\nexamination of cervical biopsy slides is subject to interobserver variability.\nAutomated processing of digitized histopathology slides has the potential for\nmore accurate classification for CIN grades from normal to increasing grades of\npre-malignancy: CIN1, CIN2 and CIN3. Cervix disease is generally understood to\nprogress from the bottom (basement membrane) to the top of the epithelium. To\nmodel this relationship of disease severity to spatial distribution of\nabnormalities, we propose a network pipeline, DeepCIN, to analyze\nhigh-resolution epithelium images (manually extracted from whole-slide images)\nhierarchically by focusing on localized vertical regions and fusing this local\ninformation for determining Normal/CIN classification. The pipeline contains\ntwo classifier networks: 1) a cross-sectional, vertical segment-level sequence\ngenerator (two-stage encoder model) is trained using weak supervision to\ngenerate feature sequences from the vertical segments to preserve the\nbottom-to-top feature relationships in the epithelium image data; 2) an\nattention-based fusion network image-level classifier predicting the final CIN\ngrade by merging vertical segment sequences. The model produces the CIN\nclassification results and also determines the vertical segment contributions\nto CIN grade prediction. Experiments show that DeepCIN achieves\npathologist-level CIN classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:54:30 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Sornapudi", "Sudhir", ""], ["Stanley", "R. Joe", ""], ["Stoecker", "William V.", ""], ["Long", "Rodney", ""], ["Xue", "Zhiyun", ""], ["Zuna", "Rosemary", ""], ["Frazier", "Shelliane R.", ""], ["Antani", "Sameer", ""]]}, {"id": "2007.11404", "submitter": "Andres Ussa Caycedo", "authors": "Andres Ussa, Chockalingam Senthil Rajen, Deepak Singla, Jyotibdha\n  Acharya, Gideon Fu Chuanrong, Arindam Basu and Bharath Ramesh", "title": "A Hybrid Neuromorphic Object Tracking and Classification Framework for\n  Real-time Systems", "comments": "11 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1910.09806", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning inference that needs to largely take place on the 'edge' is a\nhighly computational and memory intensive workload, making it intractable for\nlow-power, embedded platforms such as mobile nodes and remote security\napplications. To address this challenge, this paper proposes a real-time,\nhybrid neuromorphic framework for object tracking and classification using\nevent-based cameras that possess properties such as low-power consumption (5-14\nmW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches\nof using event-by-event processing, this work uses a mixed frame and event\napproach to get energy savings with high performance. Using a frame-based\nregion proposal method based on the density of foreground events, a\nhardware-friendly object tracking scheme is implemented using the apparent\nobject velocity while tackling occlusion scenarios. The object track input is\nconverted back to spikes for TrueNorth classification via the energy-efficient\ndeep network (EEDN) pipeline. Using originally collected datasets, we train the\nTrueNorth model on the hardware track outputs, instead of using ground truth\nobject locations as commonly done, and demonstrate the ability of our system to\nhandle practical surveillance scenarios. As an optional paradigm, to exploit\nthe low latency and asynchronous nature of neuromorphic vision sensors (NVS),\nwe also propose a continuous-time tracker with C++ implementation where each\nevent is processed individually. Thereby, we extensively compare the proposed\nmethodologies to state-of-the-art event-based and frame-based methods for\nobject tracking and classification, and demonstrate the use case of our\nneuromorphic approach for real-time and embedded applications without\nsacrificing performance. Finally, we also showcase the efficacy of the proposed\nsystem to a standard RGB camera setup when evaluated over several hours of\ntraffic recordings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:11:27 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Ussa", "Andres", ""], ["Rajen", "Chockalingam Senthil", ""], ["Singla", "Deepak", ""], ["Acharya", "Jyotibdha", ""], ["Chuanrong", "Gideon Fu", ""], ["Basu", "Arindam", ""], ["Ramesh", "Bharath", ""]]}, {"id": "2007.11430", "submitter": "Xin Li", "authors": "Xin Li, Xin Jin, Jianxin Lin, Tao Yu, Sen Liu, Yaojun Wu, Wei Zhou,\n  and Zhibo Chen", "title": "Learning Disentangled Feature Representation for Hybrid-distorted Image\n  Restoration", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid-distorted image restoration (HD-IR) is dedicated to restore real\ndistorted image that is degraded by multiple distortions. Existing HD-IR\napproaches usually ignore the inherent interference among hybrid distortions\nwhich compromises the restoration performance. To decompose such interference,\nwe introduce the concept of Disentangled Feature Learning to achieve the\nfeature-level divide-and-conquer of hybrid distortions. Specifically, we\npropose the feature disentanglement module (FDM) to distribute feature\nrepresentations of different distortions into different channels by revising\ngain-control-based normalization. We also propose a feature aggregation module\n(FAM) with channel-wise attention to adaptively filter out the distortion\nrepresentations and aggregate useful content information from different\nchannels for the construction of raw image. The effectiveness of the proposed\nscheme is verified by visualizing the correlation matrix of features and\nchannel responses of different distortions. Extensive experimental results also\nprove superior performance of our approach compared with the latest HD-IR\nschemes.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:43:40 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Li", "Xin", ""], ["Jin", "Xin", ""], ["Lin", "Jianxin", ""], ["Yu", "Tao", ""], ["Liu", "Sen", ""], ["Wu", "Yaojun", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2007.11431", "submitter": "Taihong Xiao", "authors": "Taihong Xiao, Jinwei Yuan, Deqing Sun, Qifei Wang, Xin-Yu Zhang, Kehan\n  Xu, Ming-Hsuan Yang", "title": "Learnable Cost Volume Using the Cayley Representation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cost volume is an essential component of recent deep models for optical flow\nestimation and is usually constructed by calculating the inner product between\ntwo feature vectors. However, the standard inner product in the commonly-used\ncost volume may limit the representation capacity of flow models because it\nneglects the correlation among different channel dimensions and weighs each\ndimension equally. To address this issue, we propose a learnable cost volume\n(LCV) using an elliptical inner product, which generalizes the standard inner\nproduct by a positive definite kernel matrix. To guarantee its positive\ndefiniteness, we perform spectral decomposition on the kernel matrix and\nre-parameterize it via the Cayley representation. The proposed LCV is a\nlightweight module and can be easily plugged into existing models to replace\nthe vanilla cost volume. Experimental results show that the LCV module not only\nimproves the accuracy of state-of-the-art models on standard benchmarks, but\nalso promotes their robustness against illumination change, noises, and\nadversarial perturbations of the input signals.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:59:36 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xiao", "Taihong", ""], ["Yuan", "Jinwei", ""], ["Sun", "Deqing", ""], ["Wang", "Qifei", ""], ["Zhang", "Xin-Yu", ""], ["Xu", "Kehan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2007.11432", "submitter": "Bharat Lal Bhatnagar", "authors": "Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt,\n  Gerard Pons-Moll", "title": "Combining Implicit Function Learning and Parametric Models for 3D Human\n  Reconstruction", "comments": "Accepted at ECCV'20 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit functions represented as deep learning approximations are powerful\nfor reconstructing 3D surfaces. However, they can only produce static surfaces\nthat are not controllable, which provides limited ability to modify the\nresulting model by editing its pose or shape parameters. Nevertheless, such\nfeatures are essential in building flexible models for both computer graphics\nand computer vision. In this work, we present methodology that combines\ndetail-rich implicit functions and parametric representations in order to\nreconstruct 3D models of people that remain controllable and accurate even in\nthe presence of clothing. Given sparse 3D point clouds sampled on the surface\nof a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict\nthe outer 3D surface of the dressed person, the and inner body surface, and the\nsemantic correspondences to a parametric body model. We subsequently use\ncorrespondences to fit the body model to our inner surface and then non-rigidly\ndeform it (under a parametric body + displacement model) to the outer surface\nin order to capture garment, face and hair detail. In quantitative and\nqualitative experiments with both full body data and hand scans we show that\nthe proposed methodology generalizes, and is effective even given incomplete\npoint clouds collected from single-view depth images. Our models and code can\nbe downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:46:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bhatnagar", "Bharat Lal", ""], ["Sminchisescu", "Cristian", ""], ["Theobalt", "Christian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2007.11450", "submitter": "Mohammed Abdelsamea", "authors": "Asmaa Abbas, Mohammed M. Abdelsamea, and Mohamed Gaber", "title": "4S-DT: Self Supervised Super Sample Decomposition for Transfer learning\n  with application to COVID-19 detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Due to the high availability of large-scale annotated image datasets,\nknowledge transfer from pre-trained models showed outstanding performance in\nmedical image classification. However, building a robust image classification\nmodel for datasets with data irregularity or imbalanced classes can be a very\nchallenging task, especially in the medical imaging domain. In this paper, we\npropose a novel deep convolutional neural network, we called Self Supervised\nSuper Sample Decomposition for Transfer learning (4S-DT) model. 4S-DT\nencourages a coarse-to-fine transfer learning from large-scale image\nrecognition tasks to a specific chest X-ray image classification task using a\ngeneric self-supervised sample decomposition approach. Our main contribution is\na novel self-supervised learning mechanism guided by a super sample\ndecomposition of unlabelled chest X-ray images. 4S-DT helps in improving the\nrobustness of knowledge transformation via a downstream learning strategy with\na class-decomposition layer to simplify the local structure of the data. 4S-DT\ncan deal with any irregularities in the image dataset by investigating its\nclass boundaries using a downstream class-decomposition mechanism. We used\n50,000 unlabelled chest X-ray images to achieve our coarse-to-fine transfer\nlearning with an application to COVID-19 detection, as an exemplar. 4S-DT has\nachieved a high accuracy of 99.8% (95% CI: 99.44%, 99.98%) in the detection of\nCOVID-19 cases on a large dataset and an accuracy of 97.54% (95%$ CI: 96.22%,\n98.91%) on an extended test set enriched by augmented images of a small\ndataset, out of which all real COVID-19 cases were detected, which was the\nhighest accuracy obtained when compared to other methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 16:09:31 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 17:29:33 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Abbas", "Asmaa", ""], ["Abdelsamea", "Mohammed M.", ""], ["Gaber", "Mohamed", ""]]}, {"id": "2007.11457", "submitter": "Anjith George", "authors": "Anjith George and Sebastien Marcel", "title": "Learning One Class Representations for Face Presentation Attack\n  Detection using Multi-channel Convolutional Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has evolved as a widely used biometric modality. However,\nits vulnerability against presentation attacks poses a significant security\nthreat. Though presentation attack detection (PAD) methods try to address this\nissue, they often fail in generalizing to unseen attacks. In this work, we\npropose a new framework for PAD using a one-class classifier, where the\nrepresentation used is learned with a Multi-Channel Convolutional Neural\nNetwork (MCCNN). A novel loss function is introduced, which forces the network\nto learn a compact embedding for bonafide class while being far from the\nrepresentation of attacks. A one-class Gaussian Mixture Model is used on top of\nthese embeddings for the PAD task. The proposed framework introduces a novel\napproach to learn a robust PAD system from bonafide and available (known)\nattack classes. This is particularly important as collecting bonafide data and\nsimpler attacks are much easier than collecting a wide variety of expensive\nattacks. The proposed system is evaluated on the publicly available WMCA\nmulti-channel face PAD database, which contains a wide variety of 2D and 3D\nattacks. Further, we have performed experiments with MLFP and SiW-M datasets\nusing RGB channels only. Superior performance in unseen attack protocols shows\nthe effectiveness of the proposed approach. Software, data, and protocols to\nreproduce the results are made available publicly.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:19:33 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["George", "Anjith", ""], ["Marcel", "Sebastien", ""]]}, {"id": "2007.11460", "submitter": "Yuan Tian", "authors": "Yuan Tian, Guangtao Zhai, Zhiyong Gao", "title": "Perceptron Synthesis Network: Rethinking the Action Scale Variances in\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action recognition has been partially addressed by the CNNs stacking of\nfixed-size 3D kernels. However, these methods may under-perform for only\ncapturing rigid spatial-temporal patterns in single-scale spaces, while\nneglecting the scale variances across different action primitives. To overcome\nthis limitation, we propose to learn the optimal-scale kernels from the data.\nMore specifically, an \\textit{action perceptron synthesizer} is proposed to\ngenerate the kernels from a bag of fixed-size kernels that are interacted by\ndense routing paths. To guarantee the interaction richness and the information\ncapacity of the paths, we design the novel \\textit{optimized feature fusion\nlayer}. This layer establishes a principled universal paradigm that suffices to\ncover most of the current feature fusion techniques (e.g., channel shuffling,\nand channel dropout) for the first time. By inserting the \\textit{synthesizer},\nour method can easily adapt the traditional 2D CNNs to the video understanding\ntasks such as action recognition with marginal additional computation cost. The\nproposed method is thoroughly evaluated over several challenging datasets\n(i.e., Somehting-to-Somthing, Kinetics and Diving48) that highly require\ntemporal reasoning or appearance discriminating, achieving new state-of-the-art\nresults. Particularly, our low-resolution model outperforms the recent strong\nbaseline methods, i.e., TSM and GST, with less than 30\\% of their computation\ncost.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:22:29 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 08:15:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tian", "Yuan", ""], ["Zhai", "Guangtao", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2007.11462", "submitter": "Wenqing Zhang", "authors": "Wenqing Zhang, Yang Qiu, Song Bai, Rui Zhang, Xiaolin Wei, Xiang Bai", "title": "FedOCR: Communication-Efficient Federated Learning for Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While scene text recognition techniques have been widely used in commercial\napplications, data privacy has rarely been taken into account by this research\ncommunity. Most existing algorithms have assumed a set of shared or centralized\ntraining data. However, in practice, data may be distributed on different local\ndevices that can not be centralized to share due to the privacy restrictions.\nIn this paper, we study how to make use of decentralized datasets for training\na robust scene text recognizer while keeping them stay on local devices. To the\nbest of our knowledge, we propose the first framework leveraging federated\nlearning for scene text recognition, which is trained with decentralized\ndatasets collaboratively. Hence we name it FedOCR. To make FedCOR fairly\nsuitable to be deployed on end devices, we make two improvements including\nusing lightweight models and hashing techniques. We argue that both are crucial\nfor FedOCR in terms of the communication efficiency of federated learning. The\nsimulations on decentralized datasets show that the proposed FedOCR achieves\ncompetitive results to the models that are trained with centralized data, with\nfewer communication costs and higher-level privacy-preserving.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:30:50 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhang", "Wenqing", ""], ["Qiu", "Yang", ""], ["Bai", "Song", ""], ["Zhang", "Rui", ""], ["Wei", "Xiaolin", ""], ["Bai", "Xiang", ""]]}, {"id": "2007.11465", "submitter": "Alexander Fuchs", "authors": "Alexander Fuchs, Franz Pernkopf", "title": "Wasserstein Routed Capsule Networks", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks offer interesting properties and provide an alternative to\ntoday's deep neural network architectures. However, recent approaches have\nfailed to consistently achieve competitive results across different image\ndatasets. We propose a new parameter efficient capsule architecture, that is\nable to tackle complex tasks by using neural networks trained with an\napproximate Wasserstein objective to dynamically select capsules throughout the\nentire architecture. This approach focuses on implementing a robust routing\nscheme, which can deliver improved results using little overhead. We perform\nseveral ablation studies verifying the proposed concepts and show that our\nnetwork is able to substantially outperform other capsule approaches by over\n1.2 % on CIFAR-10, using fewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:38:05 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Fuchs", "Alexander", ""], ["Pernkopf", "Franz", ""]]}, {"id": "2007.11469", "submitter": "Anjith George", "authors": "Guillaume Heusch and Anjith George and David Geissbuhler and Zohreh\n  Mostaani and Sebastien Marcel", "title": "Deep Models and Shortwave Infrared Information to Detect Face\n  Presentation Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of face presentation attack detection using\ndifferent image modalities. In particular, the usage of short wave infrared\n(SWIR) imaging is considered. Face presentation attack detection is performed\nusing recent models based on Convolutional Neural Networks using only carefully\nselected SWIR image differences as input. Conducted experiments show superior\nperformance over similar models acting on either color images or on a\ncombination of different modalities (visible, NIR, thermal and depth), as well\nas on a SVM-based classifier acting on SWIR image differences. Experiments have\nbeen carried on a new public and freely available database, containing a wide\nvariety of attacks. Video sequences have been recorded thanks to several\nsensors resulting in 14 different streams in the visible, NIR, SWIR and thermal\nspectra, as well as depth data. The best proposed approach is able to almost\nperfectly detect all impersonation attacks while ensuring low bonafide\nclassification errors. On the other hand, obtained results show that\nobfuscation attacks are more difficult to detect. We hope that the proposed\ndatabase will foster research on this challenging problem. Finally, all the\ncode and instructions to reproduce presented experiments is made available to\nthe research community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:41:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Heusch", "Guillaume", ""], ["George", "Anjith", ""], ["Geissbuhler", "David", ""], ["Mostaani", "Zohreh", ""], ["Marcel", "Sebastien", ""]]}, {"id": "2007.11498", "submitter": "Carl Doersch", "authors": "Carl Doersch, Ankush Gupta, Andrew Zisserman", "title": "CrossTransformers: spatially-aware few-shot transfer", "comments": "Published at NeurIPS 2020. Code/checkpoints:\n  https://github.com/google-research/meta-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given new tasks with very little data$-$such as new classes in a\nclassification problem or a domain shift in the input$-$performance of modern\nvision systems degrades remarkably quickly. In this work, we illustrate how the\nneural network representations which underpin modern vision systems are subject\nto supervision collapse, whereby they lose any information that is not\nnecessary for performing the training task, including information that may be\nnecessary for transfer to new tasks or domains. We then propose two methods to\nmitigate this problem. First, we employ self-supervised learning to encourage\ngeneral-purpose features that transfer better. Second, we propose a novel\nTransformer based neural network architecture called CrossTransformers, which\ncan take a small number of labeled images and an unlabeled query, find coarse\nspatial correspondence between the query and the labeled images, and then infer\nclass membership by computing distances between spatially-corresponding\nfeatures. The result is a classifier that is more robust to task and domain\nshift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a\nrecent dataset for evaluating transfer from ImageNet to many other vision\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:37:08 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 16:20:28 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 18:43:57 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2020 17:40:46 GMT"}, {"version": "v5", "created": "Wed, 17 Feb 2021 18:05:48 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Doersch", "Carl", ""], ["Gupta", "Ankush", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2007.11514", "submitter": "Manish Sahu", "authors": "Manish Sahu, Ronja Str\\\"omsd\\\"orfer, Anirban Mukhopadhyay, and Stefan\n  Zachow", "title": "Endo-Sim2Real: Consistency learning-based domain adaptation for\n  instrument segmentation", "comments": "Accepted at MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical tool segmentation in endoscopic videos is an important component of\ncomputer assisted interventions systems. Recent success of image-based\nsolutions using fully-supervised deep learning approaches can be attributed to\nthe collection of big labeled datasets. However, the annotation of a big\ndataset of real videos can be prohibitively expensive and time consuming.\nComputer simulations could alleviate the manual labeling problem, however,\nmodels trained on simulated data do not generalize to real data. This work\nproposes a consistency-based framework for joint learning of simulated and real\n(unlabeled) endoscopic data to bridge this performance generalization issue.\nEmpirical results on two data sets (15 videos of the Cholec80 and EndoVis'15\ndataset) highlight the effectiveness of the proposed \\emph{Endo-Sim2Real}\nmethod for instrument segmentation. We compare the segmentation of the proposed\napproach with state-of-the-art solutions and show that our method improves\nsegmentation both in terms of quality and quantity.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:18:11 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Sahu", "Manish", ""], ["Str\u00f6msd\u00f6rfer", "Ronja", ""], ["Mukhopadhyay", "Anirban", ""], ["Zachow", "Stefan", ""]]}, {"id": "2007.11548", "submitter": "Soroush Seifi", "authors": "Soroush Seifi, Tinne Tuytelaars", "title": "Attend and Segment: Attention Guided Active Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a dynamic environment, an agent with a limited field of view/resource\ncannot fully observe the scene before attempting to parse it. The deployment of\ncommon semantic segmentation architectures is not feasible in such settings. In\nthis paper we propose a method to gradually segment a scene given a sequence of\npartial observations. The main idea is to refine an agent's understanding of\nthe environment by attending the areas it is most uncertain about. Our method\nincludes a self-supervised attention mechanism and a specialized architecture\nto maintain and exploit spatial memory maps for filling-in the unseen areas in\nthe environment. The agent can select and attend an area while relying on the\ncues coming from the visited areas to hallucinate the other parts. We reach a\nmean pixel-wise accuracy of 78.1%, 80.9% and 76.5% on CityScapes, CamVid, and\nKitti datasets by processing only 18% of the image pixels (10 retina-like\nglimpses). We perform an ablation study on the number of glimpses, input image\nsize and effectiveness of retina-like glimpses. We compare our method to\nseveral baselines and show that the optimal results are achieved by having\naccess to a very low resolution view of the scene at the first timestep.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:09:13 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Seifi", "Soroush", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2007.11571", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian\n  Theobalt", "title": "Neural Sparse Voxel Fields", "comments": "20 pages, in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic free-viewpoint rendering of real-world scenes using classical\ncomputer graphics techniques is challenging, because it requires the difficult\nstep of capturing detailed appearance and geometry models. Recent studies have\ndemonstrated promising results by learning scene representations that\nimplicitly encode both geometry and appearance without 3D supervision. However,\nexisting approaches in practice often show blurry renderings caused by the\nlimited network capacity or the difficulty in finding accurate intersections of\ncamera rays with the scene geometry. Synthesizing high-resolution imagery from\nthese representations often requires time-consuming optical ray marching. In\nthis work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene\nrepresentation for fast and high-quality free-viewpoint rendering. NSVF defines\na set of voxel-bounded implicit fields organized in a sparse voxel octree to\nmodel local properties in each cell. We progressively learn the underlying\nvoxel structures with a differentiable ray-marching operation from only a set\nof posed RGB images. With the sparse voxel octree structure, rendering novel\nviews can be accelerated by skipping the voxels containing no relevant scene\ncontent. Our method is typically over 10 times faster than the state-of-the-art\n(namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving\nhigher quality results. Furthermore, by utilizing an explicit sparse voxel\nrepresentation, our method can easily be applied to scene editing and scene\ncomposition. We also demonstrate several challenging tasks, including\nmulti-scene learning, free-viewpoint rendering of a moving human, and\nlarge-scale scene rendering. Code and data are available at our website:\nhttps://github.com/facebookresearch/NSVF.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:51:31 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 21:04:24 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Liu", "Lingjie", ""], ["Gu", "Jiatao", ""], ["Lin", "Kyaw Zaw", ""], ["Chua", "Tat-Seng", ""], ["Theobalt", "Christian", ""]]}, {"id": "2007.11576", "submitter": "Jialin Yuan", "authors": "Jialin Yuan, Chao Chen, Li Fuxin", "title": "Deep Variational Instance Segmentation", "comments": "The work has been accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance Segmentation, which seeks to obtain both class and instance labels\nfor each pixel in the input image, is a challenging task in computer vision.\nState-of-the-art algorithms often employ two separate stages, the first one\ngenerating object proposals and the second one recognizing and refining the\nboundaries. Further, proposals are usually based on detectors such as faster\nR-CNN which search for boxes in the entire image exhaustively. In this paper,\nwe propose a novel algorithm that directly utilizes a fully convolutional\nnetwork (FCN) to predict instance labels. Specifically, we propose a\nvariational relaxation of instance segmentation as minimizing an optimization\nfunctional for a piecewise-constant segmentation problem, which can be used to\ntrain an FCN end-to-end. It extends the classical Mumford-Shah variational\nsegmentation problem to be able to handle permutation-invariant labels in the\nground truth of instance segmentation. Experiments on PASCAL VOC 2012, Semantic\nBoundaries dataset(SBD), and the MSCOCO 2017 dataset show that the proposed\napproach efficiently tackle the instance segmentation task. The source code and\ntrained models will be released with the paper.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:57:49 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 06:17:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yuan", "Jialin", ""], ["Chen", "Chao", ""], ["Fuxin", "Li", ""]]}, {"id": "2007.11610", "submitter": "Garvita Tiwari", "authors": "Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, Gerard Pons-Moll", "title": "SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size\n  Sensitive 3D Clothing", "comments": "European Conference on Computer Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While models of 3D clothing learned from real data exist, no method can\npredict clothing deformation as a function of garment size. In this paper, we\nintroduce SizerNet to predict 3D clothing conditioned on human body shape and\ngarment size parameters, and ParserNet to infer garment meshes and shape under\nclothing with personal details in a single pass from an input mesh. SizerNet\nallows to estimate and visualize the dressing effect of a garment in various\nsizes, and ParserNet allows to edit clothing of an input mesh directly,\nremoving the need for scan segmentation, which is a challenging problem in\nitself. To learn these models, we introduce the SIZER dataset of clothing size\nvariation which includes $100$ different subjects wearing casual clothing items\nin various sizes, totaling to approximately 2000 scans. This dataset includes\nthe scans, registrations to the SMPL model, scans segmented in clothing parts,\ngarment category and size labels. Our experiments show better parsing accuracy\nand size prediction than baseline methods trained on SIZER. The code, model and\ndataset will be released for research purposes.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:13:24 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Tiwari", "Garvita", ""], ["Bhatnagar", "Bharat Lal", ""], ["Tung", "Tony", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2007.11622", "submitter": "Han Cai", "authors": "Han Cai, Chuang Gan, Ligeng Zhu, Song Han", "title": "TinyTL: Reduce Activations, Not Trainable Parameters for Efficient\n  On-Device Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  On-device learning enables edge devices to continually adapt the AI models to\nnew data, which requires a small memory footprint to fit the tight memory\nconstraint of edge devices. Existing work solves this problem by reducing the\nnumber of trainable parameters. However, this doesn't directly translate to\nmemory saving since the major bottleneck is the activations, not parameters. In\nthis work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient\non-device learning. TinyTL freezes the weights while only learns the bias\nmodules, thus no need to store the intermediate activations. To maintain the\nadaptation capacity, we introduce a new memory-efficient bias module, the lite\nresidual module, to refine the feature extractor by learning small residual\nfeature maps adding only 3.8% memory overhead. Extensive experiments show that\nTinyTL significantly saves the memory (up to 6.5x) with little accuracy loss\ncompared to fine-tuning the full network. Compared to fine-tuning the last\nlayer, TinyTL provides significant accuracy improvements (up to 34.1%) with\nlittle memory overhead. Furthermore, combined with feature extractor\nadaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing\naccuracy compared to fine-tuning the full Inception-V3.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:39:53 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:51:03 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 18:24:53 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2021 03:16:21 GMT"}, {"version": "v5", "created": "Sun, 6 Jun 2021 01:23:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cai", "Han", ""], ["Gan", "Chuang", ""], ["Zhu", "Ligeng", ""], ["Han", "Song", ""]]}, {"id": "2007.11634", "submitter": "Pavan Madhusudana", "authors": "Pavan C. Madhusudana, Xiangxu Yu, Neil Birkbeck, Yilin Wang, Balu\n  Adsumilli, Alan C. Bovik", "title": "Subjective and Objective Quality Assessment of High Frame Rate Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frame rate (HFR) videos are becoming increasingly common with the\ntremendous popularity of live, high-action streaming content such as sports.\nAlthough HFR contents are generally of very high quality, high bandwidth\nrequirements make them challenging to deliver efficiently, while simultaneously\nmaintaining their quality. To optimize trade-offs between bandwidth\nrequirements and video quality, in terms of frame rate adaptation, it is\nimperative to understand the intricate relationship between frame rate and\nperceptual video quality. Towards advancing progression in this direction we\ndesigned a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR)\ndataset, which is comprised of 480 videos having 6 different frame rates,\nobtained from 16 diverse contents. In order to understand the combined effects\nof compression and frame rate adjustment, we also processed videos at 5\ncompression levels at each frame rate. To obtain subjective labels on the\nvideos, we conducted a human study yielding 19,000 human quality ratings\nobtained from a pool of 85 human subjects. We also conducted a holistic\nevaluation of existing state-of-the-art Full and No-Reference video quality\nalgorithms, and statistically benchmarked their performance on the new\ndatabase. The LIVE-YT-HFR database has been made available online for public\nuse and evaluation purposes, with hopes that it will help advance research in\nthis exciting video technology direction. It may be obtained at\n\\url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:11:42 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Madhusudana", "Pavan C.", ""], ["Yu", "Xiangxu", ""], ["Birkbeck", "Neil", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2007.11641", "submitter": "Ario Sadafi", "authors": "Ario Sadafi, Asya Makhro, Anna Bogdanova, Nassir Navab, Tingying Peng,\n  Shadi Albarqouni, Carsten Marr", "title": "Attention based Multiple Instance Learning for Classification of Blood\n  Cell Disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Red blood cells are highly deformable and present in various shapes. In blood\ncell disorders, only a subset of all cells is morphologically altered and\nrelevant for the diagnosis. However, manually labeling of all cells is\nlaborious, complicated and introduces inter-expert variability. We propose an\nattention based multiple instance learning method to classify blood samples of\npatients suffering from blood cell disorders. Cells are detected using an R-CNN\narchitecture. With the features extracted for each cell, a multiple instance\nlearning method classifies patient samples into one out of four blood cell\ndisorders. The attention mechanism provides a measure of the contribution of\neach cell to the overall classification and significantly improves the\nnetwork's classification accuracy as well as its interpretability for the\nmedical expert.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:29:40 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Sadafi", "Ario", ""], ["Makhro", "Asya", ""], ["Bogdanova", "Anna", ""], ["Navab", "Nassir", ""], ["Peng", "Tingying", ""], ["Albarqouni", "Shadi", ""], ["Marr", "Carsten", ""]]}, {"id": "2007.11653", "submitter": "Sang Won Lee", "authors": "Sang Won Lee, Yueh-Ting Chiu, Philip Brudnicki, Audrey M. Bischoff,\n  Angus Jelinek, Jenny Zijun Wang, Danielle R. Bogdanowicz, Andrew F. Laine,\n  Jia Guo, and Helen H. Lu", "title": "Darwin's Neural Network: AI-based Strategies for Rapid and Scalable Cell\n  and Coronavirus Screening", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the interdisciplinary scientific field of machine\nperception, computer vision, and biomedical engineering underpin a collection\nof machine learning algorithms with a remarkable ability to decipher the\ncontents of microscope and nanoscope images. Machine learning algorithms are\ntransforming the interpretation and analysis of microscope and nanoscope\nimaging data through use in conjunction with biological imaging modalities.\nThese advances are enabling researchers to carry out real-time experiments that\nwere previously thought to be computationally impossible. Here we adapt the\ntheory of survival of the fittest in the field of computer vision and machine\nperception to introduce a new framework of multi-class instance segmentation\ndeep learning, Darwin's Neural Network (DNN), to carry out morphometric\nanalysis and classification of COVID19 and MERS-CoV collected in vivo and of\nmultiple mammalian cell types in vitro.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:11:06 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lee", "Sang Won", ""], ["Chiu", "Yueh-Ting", ""], ["Brudnicki", "Philip", ""], ["Bischoff", "Audrey M.", ""], ["Jelinek", "Angus", ""], ["Wang", "Jenny Zijun", ""], ["Bogdanowicz", "Danielle R.", ""], ["Laine", "Andrew F.", ""], ["Guo", "Jia", ""], ["Lu", "Helen H.", ""]]}, {"id": "2007.11668", "submitter": "Bo Wu", "authors": "Bo Wu, Haoyu Qin, Alireza Zareian, Carl Vondrick, Shih-Fu Chang", "title": "Analogical Reasoning for Visually Grounded Language Acquisition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children acquire language subconsciously by observing the surrounding world\nand listening to descriptions. They can discover the meaning of words even\nwithout explicit language knowledge, and generalize to novel compositions\neffortlessly. In this paper, we bring this ability to AI, by studying the task\nof Visually grounded Language Acquisition (VLA). We propose a multimodal\ntransformer model augmented with a novel mechanism for analogical reasoning,\nwhich approximates novel compositions by learning semantic mapping and\nreasoning operations from previously seen compositions. Our proposed method,\nAnalogical Reasoning Transformer Networks (ARTNet), is trained on raw\nmultimedia data (video frames and transcripts), and after observing a set of\ncompositions such as \"washing apple\" or \"cutting carrot\", it can generalize and\nrecognize new compositions in new video frames, such as \"washing carrot\" or\n\"cutting apple\". To this end, ARTNet refers to relevant instances in the\ntraining data and uses their visual features and captions to establish\nanalogies with the query image. Then it chooses the suitable verb and noun to\ncreate a new composition that describes the new image best. Extensive\nexperiments on an instructional video dataset demonstrate that the proposed\nmethod achieves significantly better generalization capability and recognition\naccuracy compared to state-of-the-art transformer models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:51:58 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wu", "Bo", ""], ["Qin", "Haoyu", ""], ["Zareian", "Alireza", ""], ["Vondrick", "Carl", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2007.11678", "submitter": "Davis Rempe", "authors": "Davis Rempe, Leonidas J. Guibas, Aaron Hertzmann, Bryan Russell, Ruben\n  Villegas, Jimei Yang", "title": "Contact and Human Dynamics from Monocular Video", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep models predict 2D and 3D kinematic poses from video that are\napproximately accurate, but contain visible errors that violate physical\nconstraints, such as feet penetrating the ground and bodies leaning at extreme\nangles. In this paper, we present a physics-based method for inferring 3D human\nmotion from video sequences that takes initial 2D and 3D pose estimates as\ninput. We first estimate ground contact timings with a novel prediction network\nwhich is trained without hand-labeled data. A physics-based trajectory\noptimization then solves for a physically-plausible motion, based on the\ninputs. We show this process produces motions that are significantly more\nrealistic than those from purely kinematic methods, substantially improving\nquantitative measures of both kinematic and dynamic plausibility. We\ndemonstrate our method on character animation and pose estimation tasks on\ndynamic motions of dancing and sports with complex contact patterns.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:09:11 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 04:02:14 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Rempe", "Davis", ""], ["Guibas", "Leonidas J.", ""], ["Hertzmann", "Aaron", ""], ["Russell", "Bryan", ""], ["Villegas", "Ruben", ""], ["Yang", "Jimei", ""]]}, {"id": "2007.11679", "submitter": "Kirill Mazur", "authors": "Kirill Mazur, Victor Lempitsky", "title": "Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new versatile building block for deep point cloud processing\narchitectures that is equally suited for diverse tasks. This building block\ncombines the ideas of spatial transformers and multi-view convolutional\nnetworks with the efficiency of standard convolutional layers in two and\nthree-dimensional dense grids. The new block operates via multiple parallel\nheads, whereas each head differentiably rasterizes feature representations of\nindividual points into a low-dimensional space, and then uses dense convolution\nto propagate information across points. The results of the processing of\nindividual heads are then combined together resulting in the update of point\nfeatures. Using the new block, we build architectures for both discriminative\n(point cloud segmentation, point cloud classification) and generative (point\ncloud inpainting and image-based point cloud reconstruction) tasks. The\nresulting architectures achieve state-of-the-art performance for these tasks,\ndemonstrating the versatility and universality of the new block for point cloud\nprocessing.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:09:14 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 22:04:58 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 22:01:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mazur", "Kirill", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2007.11689", "submitter": "Matthias Joachim Ehrhardt", "authors": "Matthias J. Ehrhardt", "title": "Multi-modality imaging with structure-promoting regularisers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging with multiple modalities or multiple channels is becoming\nincreasingly important for our modern society. A key tool for understanding and\nearly diagnosis of cancer and dementia is PET-MR, a combined positron emission\ntomography and magnetic resonance imaging scanner which can simultaneously\nacquire functional and anatomical data. Similarly in remote sensing, while\nhyperspectral sensors may allow to characterise and distinguish materials,\ndigital cameras offer high spatial resolution to delineate objects. In both of\nthese examples, the imaging modalities can be considered individually or\njointly. In this chapter we discuss mathematical approaches which allow to\ncombine information from several imaging modalities so that multi-modality\nimaging can be more than just the sum of its components.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:26:37 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ehrhardt", "Matthias J.", ""]]}, {"id": "2007.11690", "submitter": "Aditya Mogadala", "authors": "Aditya Mogadala and Xiaoyu Shen and Dietrich Klakow", "title": "Integrating Image Captioning with Rule-based Entity Masking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image, generating its natural language description (i.e., caption)\nis a well studied problem. Approaches proposed to address this problem usually\nrely on image features that are difficult to interpret. Particularly, these\nimage features are subdivided into global and local features, where global\nfeatures are extracted from the global representation of the image, while local\nfeatures are extracted from the objects detected locally in an image. Although,\nlocal features extract rich visual information from the image, existing models\ngenerate captions in a blackbox manner and humans have difficulty interpreting\nwhich local objects the caption is aimed to represent. Hence in this paper, we\npropose a novel framework for the image captioning with an explicit object\n(e.g., knowledge graph entity) selection process while still maintaining its\nend-to-end training ability. The model first explicitly selects which local\nentities to include in the caption according to a human-interpretable mask,\nthen generate proper captions by attending to selected entities. Experiments\nconducted on the MSCOCO dataset demonstrate that our method achieves good\nperformance in terms of the caption quality and diversity with a more\ninterpretable generating process than previous counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:27:12 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mogadala", "Aditya", ""], ["Shen", "Xiaoyu", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2007.11691", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Debleena Sengupta, Demetri Terzopoulos", "title": "End-to-End Trainable Deep Active Contour Models for Automated Image\n  Segmentation: Delineating Buildings in Aerial Imagery", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated segmentation of buildings in remote sensing imagery is a\nchallenging task that requires the accurate delineation of multiple building\ninstances over typically large image areas. Manual methods are often laborious\nand current deep-learning-based approaches fail to delineate all building\ninstances and do so with adequate accuracy. As a solution, we present Trainable\nDeep Active Contours (TDACs), an automatic image segmentation framework that\nintimately unites Convolutional Neural Networks (CNNs) and Active Contour\nModels (ACMs). The Eulerian energy functional of the ACM component includes\nper-pixel parameter maps that are predicted by the backbone CNN, which also\ninitializes the ACM. Importantly, both the ACM and CNN components are fully\nimplemented in TensorFlow and the entire TDAC architecture is end-to-end\nautomatically differentiable and backpropagation trainable without user\nintervention. TDAC yields fast, accurate, and fully automatic simultaneous\ndelineation of arbitrarily many buildings in the image. We validate the model\non two publicly available aerial image datasets for building segmentation, and\nour results demonstrate that TDAC establishes a new state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:27:17 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Sengupta", "Debleena", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2007.11709", "submitter": "Fatemeh Vakhshiteh", "authors": "Fatemeh Vakhshiteh, Ahmad Nickabadi and Raghavendra Ramachandra", "title": "Adversarial Attacks against Face Recognition: A Comprehensive Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) systems have demonstrated outstanding verification\nperformance, suggesting suitability for real-world applications ranging from\nphoto tagging in social media to automated border control (ABC). In an advanced\nFR system with deep learning-based architecture, however, promoting the\nrecognition efficiency alone is not sufficient, and the system should also\nwithstand potential kinds of attacks designed to target its proficiency. Recent\nstudies show that (deep) FR systems exhibit an intriguing vulnerability to\nimperceptible or perceptible but natural-looking adversarial input images that\ndrive the model to incorrect output predictions. In this article, we present a\ncomprehensive survey on adversarial attacks against FR systems and elaborate on\nthe competence of new countermeasures against them. Further, we propose a\ntaxonomy of existing attack and defense methods based on different criteria. We\ncompare attack methods on the orientation and attributes and defense approaches\non the category. Finally, we explore the challenges and potential research\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 22:46:00 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 12:05:03 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 14:46:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Vakhshiteh", "Fatemeh", ""], ["Nickabadi", "Ahmad", ""], ["Ramachandra", "Raghavendra", ""]]}, {"id": "2007.11726", "submitter": "Chunxia Qin", "authors": "Chunxia Qin, Xiaojun Chen, Jocelyne Troccaz", "title": "A weakly supervised registration-based framework for prostate\n  segmentation via the combination of statistical shape model and CNN", "comments": "there are some mistakes on the section of introduction. Several\n  groups already have reported different prostate segmentation methods which\n  involved prior information. However, we said that there isn't research\n  combined prior information", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise determination of target is an essential procedure in prostate\ninterventions, such as the prostate biopsy, lesion detection and targeted\ntherapy. However, the prostate delineation may be tough in some cases due to\ntissue ambiguity or lack of partial anatomical boundary. To address this\nproblem, we proposed a weakly supervised registration-based framework for the\nprecise prostate segmentation, by combining convolutional neural network (CNN)\nwith statistical shape model (SSM). To obtain the prostate region, an\ninception-based neural network (SSM-Net) was firstly exploited to predict the\nmodel transform, shape control parameters and a fine-tuning vector, for the\ngeneration of prostate boundary. According to the inferred boundary, a\nnormalized distance map was calculated. Then, a residual U-net (ResU-Net) was\nemployed to predict a probability label map from the input images. Finally, the\naverage of the distance map and the probability map was regarded as the\nprostate segmentation. After that, two public dataset PROMISE12 and NCI- ISBI\n2013 were utilized for the model computation and for the network training and\ntesting. The validation results demonstrate that the segmentation framework\nusing a SSM with 9500 nodes achieved the best performance, with a dice of 0.904\nand an average surface distance of 1.88 mm. In addition, we verified the impact\nof model elasticity augmentation and fine-tuning item on the network\nsegmentation capability. As a result, both factors have improved the\ndelineation accuracy, with dice increased by 10% and 7% respectively. In\nconclusion, via the combination of two weakly supervised neural networks, our\nsegmentation method might be an effective and robust approach for prostate\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 00:24:57 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 06:53:19 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Qin", "Chunxia", ""], ["Chen", "Xiaojun", ""], ["Troccaz", "Jocelyne", ""]]}, {"id": "2007.11731", "submitter": "Yin Li", "authors": "Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, Yin Li", "title": "Comprehensive Image Captioning via Scene Graph Decomposition", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of image captioning by revisiting the\nrepresentation of image scene graph. At the core of our method lies the\ndecomposition of a scene graph into a set of sub-graphs, with each sub-graph\ncapturing a semantic component of the input image. We design a deep model to\nselect important sub-graphs, and to decode each selected sub-graph into a\nsingle target sentence. By using sub-graphs, our model is able to attend to\ndifferent components of the image. Our method thus accounts for accurate,\ndiverse, grounded and controllable captioning at the same time. We present\nextensive experiments to demonstrate the benefits of our comprehensive\ncaptioning model. Our method establishes new state-of-the-art results in\ncaption diversity, grounding, and controllability, and compares favourably to\nlatest methods in caption quality. Our project website can be found at\nhttp://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 00:59:21 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Zhong", "Yiwu", ""], ["Wang", "Liwei", ""], ["Chen", "Jianshu", ""], ["Yu", "Dong", ""], ["Li", "Yin", ""]]}, {"id": "2007.11744", "submitter": "Jiajun Wu", "authors": "Andrew Luo, Zhoutong Zhang, Jiajun Wu, Joshua B. Tenenbaum", "title": "End-to-End Optimization of Scene Layout", "comments": "CVPR 2020 (Oral). Project page: http://3dsln.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end variational generative model for scene layout\nsynthesis conditioned on scene graphs. Unlike unconditional scene layout\ngeneration, we use scene graphs as an abstract but general representation to\nguide the synthesis of diverse scene layouts that satisfy relationships\nincluded in the scene graph. This gives rise to more flexible control over the\nsynthesis process, allowing various forms of inputs such as scene layouts\nextracted from sentences or inferred from a single color image. Using our\nconditional layout synthesizer, we can generate various layouts that share the\nsame structure of the input example. In addition to this conditional generation\ndesign, we also integrate a differentiable rendering module that enables layout\nrefinement using only 2D projections of the scene. Given a depth and a\nsemantics map, the differentiable rendering module enables optimizing over the\nsynthesized layout to fit the given input in an analysis-by-synthesis fashion.\nExperiments suggest that our model achieves higher accuracy and diversity in\nconditional scene synthesis and allows exemplar-based scene generation from\nvarious input forms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 01:35:55 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Luo", "Andrew", ""], ["Zhang", "Zhoutong", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2007.11752", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Ari S. Morcos, Diana Marculescu", "title": "Joslim: Joint Widths and Weights Optimization for Slimmable Neural\n  Networks", "comments": "Accepted at ECML-PKDD 2021 (Research Track), 4-page abridged versions\n  have been accepted at non-archival venues including RealML and DMMLSys\n  workshops at ICML'20 and DLP-KDD and AdvML workshops at KDD'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slimmable neural networks provide a flexible trade-off front between\nprediction error and computational requirement (such as the number of\nfloating-point operations or FLOPs) with the same storage requirement as a\nsingle model. They are useful for reducing maintenance overhead for deploying\nmodels to devices with different memory constraints and are useful for\noptimizing the efficiency of a system with many CNNs. However, existing\nslimmable network approaches either do not optimize layer-wise widths or\noptimize the shared-weights and layer-wise widths independently, thereby\nleaving significant room for improvement by joint width and weight\noptimization. In this work, we propose a general framework to enable joint\noptimization for both width configurations and weights of slimmable networks.\nOur framework subsumes conventional and NAS-based slimmable methods as special\ncases and provides flexibility to improve over existing methods. From a\npractical standpoint, we propose Joslim, an algorithm that jointly optimizes\nboth the widths and weights for slimmable nets, which outperforms existing\nmethods for optimizing slimmable networks across various networks, datasets,\nand objectives. Quantitatively, improvements up to 1.7% and 8% in top-1\naccuracy on the ImageNet dataset can be attained for MobileNetV2 considering\nFLOPs and memory footprint, respectively. Our results highlight the potential\nof optimizing the channel counts for different layers jointly with the weights\nfor slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 02:05:03 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 01:33:08 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 21:24:17 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 14:38:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Morcos", "Ari S.", ""], ["Marculescu", "Diana", ""]]}, {"id": "2007.11755", "submitter": "Wei Mao", "authors": "Wei Mao, Miaomiao Liu, Mathieu Salzmann", "title": "History Repeats Itself: Human Motion Prediction via Motion Attention", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction aims to forecast future human poses given a past\nmotion. Whether based on recurrent or feed-forward neural networks, existing\nmethods fail to model the observation that human motion tends to repeat itself,\neven for complex sports actions and cooking activities. Here, we introduce an\nattention-based feed-forward network that explicitly leverages this\nobservation. In particular, instead of modeling frame-wise attention via pose\nsimilarity, we propose to extract motion attention to capture the similarity\nbetween the current motion context and the historical motion sub-sequences.\nAggregating the relevant past motions and processing the result with a graph\nconvolutional network allows us to effectively exploit motion patterns from the\nlong-term history to predict the future poses. Our experiments on Human3.6M,\nAMASS and 3DPW evidence the benefits of our approach for both periodical and\nnon-periodical actions. Thanks to our attention model, it yields\nstate-of-the-art results on all three datasets. Our code is available at\nhttps://github.com/wei-mao-2019/HisRepItself.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 02:12:27 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mao", "Wei", ""], ["Liu", "Miaomiao", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2007.11762", "submitter": "Zhixiang Chi", "authors": "Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei Lu, Jin Tang,\n  Konstantinos N Plataniotis", "title": "All at Once: Temporally Adaptive Multi-Frame Interpolation with Advanced\n  Motion Modeling", "comments": "Accepted at ECCV2020 (poster), project:\n  https://chi-chi-zx.github.io/all-at-once/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in high refresh rate displays as well as the increased\ninterest in high rate of slow motion and frame up-conversion fuel the demand\nfor efficient and cost-effective multi-frame video interpolation solutions. To\nthat regard, inserting multiple frames between consecutive video frames are of\nparamount importance for the consumer electronics industry. State-of-the-art\nmethods are iterative solutions interpolating one frame at the time. They\nintroduce temporal inconsistencies and clearly noticeable visual artifacts.\n  Departing from the state-of-the-art, this work introduces a true multi-frame\ninterpolator. It utilizes a pyramidal style network in the temporal domain to\ncomplete the multi-frame interpolation task in one-shot. A novel flow\nestimation procedure using a relaxed loss function, and an advanced,\ncubic-based, motion model is also used to further boost interpolation accuracy\nwhen complex motion segments are encountered. Results on the Adobe240 dataset\nshow that the proposed method generates visually pleasing, temporally\nconsistent frames, outperforms the current best off-the-shelf method by 1.57db\nin PSNR with 8 times smaller model and 7.7 times faster. The proposed method\ncan be easily extended to interpolate a large number of new frames while\nremaining efficient because of the one-shot mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 02:34:39 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 03:50:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chi", "Zhixiang", ""], ["Nasiri", "Rasoul Mohammadi", ""], ["Liu", "Zheng", ""], ["Lu", "Juwei", ""], ["Tang", "Jin", ""], ["Plataniotis", "Konstantinos N", ""]]}, {"id": "2007.11766", "submitter": "Tatsumi Uezato", "authors": "Tatsumi Uezato, Danfeng Hong, Naoto Yokoya, Wei He", "title": "Guided Deep Decoder: Unsupervised Image Pair Fusion", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fusion of input and guidance images that have a tradeoff in their\ninformation (e.g., hyperspectral and RGB image fusion or pansharpening) can be\ninterpreted as one general problem. However, previous studies applied a\ntask-specific handcrafted prior and did not address the problems with a unified\napproach. To address this limitation, in this study, we propose a guided deep\ndecoder network as a general prior. The proposed network is composed of an\nencoder-decoder network that exploits multi-scale features of a guidance image\nand a deep decoder network that generates an output image. The two networks are\nconnected by feature refinement units to embed the multi-scale features of the\nguidance image into the deep decoder network. The proposed network allows the\nnetwork parameters to be optimized in an unsupervised way without training\ndata. Our results show that the proposed network can achieve state-of-the-art\nperformance in various image fusion problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 03:06:06 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Uezato", "Tatsumi", ""], ["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["He", "Wei", ""]]}, {"id": "2007.11770", "submitter": "Tatsumi Uezato", "authors": "Tatsumi Uezato, Naoto Yokoya, Wei He", "title": "Illumination invariant hyperspectral image unmixing based on a digital\n  surface model", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2963961", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many spectral unmixing models have been developed to address\nspectral variability caused by variable incident illuminations, the mechanism\nof the spectral variability is still unclear. This paper proposes an unmixing\nmodel, named illumination invariant spectral unmixing (IISU). IISU makes the\nfirst attempt to use the radiance hyperspectral data and a LiDAR-derived\ndigital surface model (DSM) in order to physically explain variable\nilluminations and shadows in the unmixing framework. Incident angles, sky\nfactors, visibility from the sun derived from the LiDAR-derived DSM support the\nexplicit explanation of endmember variability in the unmixing process from\nradiance perspective. The proposed model was efficiently solved by a\nstraightforward optimization procedure. The unmixing results showed that the\nother state-of-the-art unmixing models did not work well especially in the\nshaded pixels. On the other hand, the proposed model estimated more accurate\nabundances and shadow compensated reflectance than the existing models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 03:27:02 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Uezato", "Tatsumi", ""], ["Yokoya", "Naoto", ""], ["He", "Wei", ""]]}, {"id": "2007.11782", "submitter": "Wei Ji", "authors": "Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, Huchuan Lu", "title": "Accurate RGB-D Salient Object Detection via Collaborative Learning", "comments": "accepted by ECCV 2020 as a poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Benefiting from the spatial cues embedded in depth images, recent progress on\nRGB-D saliency detection shows impressive ability on some challenge scenarios.\nHowever, there are still two limitations. One hand is that the pooling and\nupsampling operations in FCNs might cause blur object boundaries. On the other\nhand, using an additional depth-network to extract depth features might lead to\nhigh computation and storage cost. The reliance on depth inputs during testing\nalso limits the practical applications of current RGB-D models. In this paper,\nwe propose a novel collaborative learning framework where edge, depth and\nsaliency are leveraged in a more efficient way, which solves those problems\ntactfully. The explicitly extracted edge information goes together with\nsaliency to give more emphasis to the salient regions and object boundaries.\nDepth and saliency learning is innovatively integrated into the high-level\nfeature learning process in a mutual-benefit manner. This strategy enables the\nnetwork to be free of using extra depth networks and depth inputs to make\ninference. To this end, it makes our model more lightweight, faster and more\nversatile. Experiment results on seven benchmark datasets show its superior\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 04:33:36 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ji", "Wei", ""], ["Li", "Jingjing", ""], ["Zhang", "Miao", ""], ["Piao", "Yongri", ""], ["Lu", "Huchuan", ""]]}, {"id": "2007.11784", "submitter": "Furen Xiao", "authors": "Siang-Ruei Wu, Hao-Yun Chang, Florence T Su, Heng-Chun Liao, Wanju\n  Tseng, Chun-Chih Liao, Feipei Lai, Feng-Ming Hsu, Furen Xiao", "title": "Deep Learning Based Segmentation of Various Brain Lesions for\n  Radiosurgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of medical images with deep learning models is rapidly\ndeveloped. In this study, we benchmarked state-of-the-art deep learning\nsegmentation algorithms on our clinical stereotactic radiosurgery dataset,\ndemonstrating the strengths and weaknesses of these algorithms in a fairly\npractical scenario. In particular, we compared the model performances with\nrespect to their sampling method, model architecture, and the choice of loss\nfunctions, identifying the suitable settings for their applications and\nshedding light on the possible improvements.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 09:35:04 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wu", "Siang-Ruei", ""], ["Chang", "Hao-Yun", ""], ["Su", "Florence T", ""], ["Liao", "Heng-Chun", ""], ["Tseng", "Wanju", ""], ["Liao", "Chun-Chih", ""], ["Lai", "Feipei", ""], ["Hsu", "Feng-Ming", ""], ["Xiao", "Furen", ""]]}, {"id": "2007.11797", "submitter": "Saurabh Singh", "authors": "Saurabh Singh, Sami Abu-El-Haija, Nick Johnston, Johannes Ball\\'e,\n  Abhinav Shrivastava, George Toderici", "title": "End-to-end Learning of Compressible Features", "comments": "Accepted at ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pre-trained convolutional neural networks (CNNs) are powerful off-the-shelf\nfeature generators and have been shown to perform very well on a variety of\ntasks. Unfortunately, the generated features are high dimensional and expensive\nto store: potentially hundreds of thousands of floats per example when\nprocessing videos. Traditional entropy based lossless compression methods are\nof little help as they do not yield desired level of compression, while general\npurpose lossy compression methods based on energy compaction (e.g. PCA followed\nby quantization and entropy coding) are sub-optimal, as they are not tuned to\ntask specific objective. We propose a learned method that jointly optimizes for\ncompressibility along with the task objective for learning the features. The\nplug-in nature of our method makes it straight-forward to integrate with any\ntarget objective and trade-off against compressibility. We present results on\nmultiple benchmarks and demonstrate that our method produces features that are\nan order of magnitude more compressible, while having a regularization effect\nthat leads to a consistent improvement in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:17:33 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Singh", "Saurabh", ""], ["Abu-El-Haija", "Sami", ""], ["Johnston", "Nick", ""], ["Ball\u00e9", "Johannes", ""], ["Shrivastava", "Abhinav", ""], ["Toderici", "George", ""]]}, {"id": "2007.11803", "submitter": "Wenbo Li", "authors": "Wenbo Li, Xin Tao, Taian Guo, Lu Qi, Jiangbo Lu, and Jiaya Jia", "title": "MuCAN: Multi-Correspondence Aggregation Network for Video\n  Super-Resolution", "comments": "Accepted By ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) aims to utilize multiple low-resolution frames\nto generate a high-resolution prediction for each frame. In this process,\ninter- and intra-frames are the key sources for exploiting temporal and spatial\ninformation. However, there are a couple of limitations for existing VSR\nmethods. First, optical flow is often used to establish temporal\ncorrespondence. But flow estimation itself is error-prone and affects recovery\nresults. Second, similar patterns existing in natural images are rarely\nexploited for the VSR task. Motivated by these findings, we propose a temporal\nmulti-correspondence aggregation strategy to leverage similar patches across\nframes, and a cross-scale nonlocal-correspondence aggregation scheme to explore\nself-similarity of images across scales. Based on these two new modules, we\nbuild an effective multi-correspondence aggregation network (MuCAN) for VSR.\nOur method achieves state-of-the-art results on multiple benchmark datasets.\nExtensive experiments justify the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:41:27 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Li", "Wenbo", ""], ["Tao", "Xin", ""], ["Guo", "Taian", ""], ["Qi", "Lu", ""], ["Lu", "Jiangbo", ""], ["Jia", "Jiaya", ""]]}, {"id": "2007.11806", "submitter": "Nachuan Ma", "authors": "Nachuan Ma", "title": "Autonomous Removal of Perspective Distortion based on Detection Results\n  of Robotic Elevator Button Corner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elevator button recognition is an important function to realize the\nautonomous operation of elevators. However, challenging image conditions and\nvarious image distortions make it difficult to accurately recognize buttons. In\nthis work, We propose a novel algorithm that can automatically correct\nperspective distortions of elevator panel images based on button corner\ndetection results. The algorithm first leverages DeepLabv3+ model and Hough\nTransform method to obtain button segmentation results and button corner\ndetection results, then utilizes pixel coordinates of standard button corners\nas reference features to estimate camera motions for correcting perspective\ndistortions. The algorithm is much more robust to outliers and noise on the\nremoval of perspective distortion than traditional geometric approaches as it\nonly performs on a single image autonomously. 15 elevator panel images are\ncaptured from different angles of view as the dataset. The experimental results\nshow that our approach significantly outperforms traditional geometric\ntechniques in accuracy and robustness. Rectification results of the proposed\nalgorithm is 77.4% better than the results of traditional geometric algorithm\nin average.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:47:08 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ma", "Nachuan", ""]]}, {"id": "2007.11814", "submitter": "Mei-Chen Yeh", "authors": "Mei-Chen Yeh and Fang Li", "title": "Zero-Shot Recognition through Image-Guided Semantic Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new embedding-based framework for zero-shot learning (ZSL). Most\nembedding-based methods aim to learn the correspondence between an image\nclassifier (visual representation) and its class prototype (semantic\nrepresentation) for each class. Motivated by the binary relevance method for\nmulti-label classification, we propose to inversely learn the mapping between\nan image and a semantic classifier. Given an input image, the proposed\nImage-Guided Semantic Classification (IGSC) method creates a label classifier,\nbeing applied to all label embeddings to determine whether a label belongs to\nthe input image. Therefore, semantic classifiers are image-adaptive and are\ngenerated during inference. IGSC is conceptually simple and can be realized by\na slight enhancement of an existing deep architecture for classification; yet\nit is effective and outperforms state-of-the-art embedding-based generalized\nZSL approaches on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:22:40 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yeh", "Mei-Chen", ""], ["Li", "Fang", ""]]}, {"id": "2007.11823", "submitter": "Ningning Ma", "authors": "Ningning Ma, Xiangyu Zhang, Jiawei Huang, Jian Sun", "title": "WeightNet: Revisiting the Design Space of Weight Networks", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually simple, flexible and effective framework for weight\ngenerating networks. Our approach is general that unifies two current distinct\nand extremely effective SENet and CondConv into the same framework on weight\nspace. The method, called WeightNet, generalizes the two methods by simply\nadding one more grouped fully-connected layer to the attention activation\nlayer. We use the WeightNet, composed entirely of (grouped) fully-connected\nlayers, to directly output the convolutional weight. WeightNet is easy and\nmemory-conserving to train, on the kernel space instead of the feature space.\nBecause of the flexibility, our method outperforms existing approaches on both\nImageNet and COCO detection tasks, achieving better Accuracy-FLOPs and\nAccuracy-Parameter trade-offs. The framework on the flexible weight space has\nthe potential to further improve the performance. Code is available at\nhttps://github.com/megvii-model/WeightNet.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:49:01 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 11:47:42 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Ma", "Ningning", ""], ["Zhang", "Xiangyu", ""], ["Huang", "Jiawei", ""], ["Sun", "Jian", ""]]}, {"id": "2007.11824", "submitter": "Ningning Ma", "authors": "Ningning Ma, Xiangyu Zhang, Jian Sun", "title": "Funnel Activation for Visual Recognition", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually simple but effective funnel activation for image\nrecognition tasks, called Funnel activation (FReLU), that extends ReLU and\nPReLU to a 2D activation by adding a negligible overhead of spatial condition.\nThe forms of ReLU and PReLU are y = max(x, 0) and y = max(x, px), respectively,\nwhile FReLU is in the form of y = max(x,T(x)), where T(x) is the 2D spatial\ncondition. Moreover, the spatial condition achieves a pixel-wise modeling\ncapacity in a simple way, capturing complicated visual layouts with regular\nconvolutions. We conduct experiments on ImageNet, COCO detection, and semantic\nsegmentation tasks, showing great improvements and robustness of FReLU in the\nvisual recognition tasks. Code is available at\nhttps://github.com/megvii-model/FunnelAct.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:02:01 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 11:45:43 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Ma", "Ningning", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2007.11840", "submitter": "Stefano Zorzi", "authors": "Stefano Zorzi, Friedrich Fraundorfer", "title": "Regularization of Building Boundaries in Satellite Images using\n  Adversarial and Regularized Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for building boundary refinement and\nregularization in satellite images using a fully convolutional neural network\ntrained with a combination of adversarial and regularized losses. Compared to a\npure Mask R-CNN model, the overall algorithm can achieve equivalent performance\nin terms of accuracy and completeness. However, unlike Mask R-CNN that produces\nirregular footprints, our framework generates regularized and visually pleasing\nbuilding boundaries which are beneficial in many applications.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:07:55 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Zorzi", "Stefano", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2007.11855", "submitter": "Jinwoo Lee", "authors": "Jinwoo Lee and Minhyuk Sung and Hyunjoon Lee and Junho Kim", "title": "Neural Geometric Parser for Single Image Camera Calibration", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural geometric parser learning single image camera calibration\nfor man-made scenes. Unlike previous neural approaches that rely only on\nsemantic cues obtained from neural networks, our approach considers both\nsemantic and geometric cues, resulting in significant accuracy improvement. The\nproposed framework consists of two networks. Using line segments of an image as\ngeometric cues, the first network estimates the zenith vanishing point and\ngenerates several candidates consisting of the camera rotation and focal\nlength. The second network evaluates each candidate based on the given image\nand the geometric cues, where prior knowledge of man-made scenes is used for\nthe evaluation. With the supervision of datasets consisting of the horizontal\nline and focal length of the images, our networks can be trained to estimate\nthe same camera parameters. Based on the Manhattan world assumption, we can\nfurther estimate the camera rotation and focal length in a weakly supervised\nmanner. The experimental results reveal that the performance of our neural\napproach is significantly higher than that of existing state-of-the-art camera\ncalibration techniques for single images of indoor and outdoor scenes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:29:00 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 02:16:03 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Lee", "Jinwoo", ""], ["Sung", "Minhyuk", ""], ["Lee", "Hyunjoon", ""], ["Kim", "Junho", ""]]}, {"id": "2007.11858", "submitter": "Sheng Jin", "authors": "Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli\n  Ouyang, Ping Luo", "title": "Whole-Body Human Pose Estimation in the Wild", "comments": "To appear on ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the task of 2D human whole-body pose estimation,\nwhich aims to localize dense landmarks on the entire human body including face,\nhands, body, and feet. As existing datasets do not have whole-body annotations,\nprevious methods have to assemble different deep models trained independently\non different datasets of the human face, hand, and body, struggling with\ndataset biases and large model complexity. To fill in this blank, we introduce\nCOCO-WholeBody which extends COCO dataset with whole-body annotations. To our\nbest knowledge, it is the first benchmark that has manual annotations on the\nentire human body, including 133 dense landmarks with 68 on the face, 42 on\nhands and 23 on the body and feet. A single-network model, named ZoomNet, is\ndevised to take into account the hierarchical structure of the full human body\nto solve the scale variation of different body parts of the same person.\nZoomNet is able to significantly outperform existing methods on the proposed\nCOCO-WholeBody dataset. Extensive experiments show that COCO-WholeBody not only\ncan be used to train deep models from scratch for whole-body pose estimation\nbut also can serve as a powerful pre-training dataset for many different tasks\nsuch as facial landmark detection and hand keypoint estimation. The dataset is\npublicly available at https://github.com/jin-s13/COCO-WholeBody.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:35:26 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jin", "Sheng", ""], ["Xu", "Lumin", ""], ["Xu", "Jin", ""], ["Wang", "Can", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Ouyang", "Wanli", ""], ["Luo", "Ping", ""]]}, {"id": "2007.11864", "submitter": "Sheng Jin", "authors": "Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian, Wanli Ouyang,\n  Ping Luo", "title": "Differentiable Hierarchical Graph Grouping for Multi-Person Pose\n  Estimation", "comments": "To appear on ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is challenging because it localizes body\nkeypoints for multiple persons simultaneously. Previous methods can be divided\ninto two streams, i.e. top-down and bottom-up methods. The top-down methods\nlocalize keypoints after human detection, while the bottom-up methods localize\nkeypoints directly and then cluster/group them for different persons, which are\ngenerally more efficient than top-down methods. However, in existing bottom-up\nmethods, the keypoint grouping is usually solved independently from keypoint\ndetection, making them not end-to-end trainable and have sub-optimal\nperformance. In this paper, we investigate a new perspective of human part\ngrouping and reformulate it as a graph clustering task. Especially, we propose\na novel differentiable Hierarchical Graph Grouping (HGG) method to learn the\ngraph grouping in bottom-up multi-person pose estimation task. Moreover, HGG is\neasily embedded into main-stream bottom-up methods. It takes human keypoint\ncandidates as graph nodes and clusters keypoints in a multi-layer graph neural\nnetwork model. The modules of HGG can be trained end-to-end with the keypoint\ndetection network and is able to supervise the grouping process in a\nhierarchical manner. To improve the discrimination of the clustering, we add a\nset of edge discriminators and macro-node discriminators. Extensive experiments\non both COCO and OCHuman datasets demonstrate that the proposed method improves\nthe performance of bottom-up pose estimation methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:46:22 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jin", "Sheng", ""], ["Liu", "Wentao", ""], ["Xie", "Enze", ""], ["Wang", "Wenhai", ""], ["Qian", "Chen", ""], ["Ouyang", "Wanli", ""], ["Luo", "Ping", ""]]}, {"id": "2007.11866", "submitter": "Paul Albert", "authors": "Paul Albert, Diego Ortego, Eric Arazo, Noel E. O'Connor, Kevin\n  McGuinness", "title": "Reliable Label Bootstrapping for Semi-Supervised Learning", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the amount of labels required to train convolutional neural networks\nwithout performance degradation is key to effectively reduce human annotation\nefforts. We propose Reliable Label Bootstrapping (ReLaB), an unsupervised\npreprossessing algorithm which improves the performance of semi-supervised\nalgorithms in extremely low supervision settings. Given a dataset with few\nlabeled samples, we first learn meaningful self-supervised, latent features for\nthe data. Second, a label propagation algorithm propagates the known labels on\nthe unsupervised features, effectively labeling the full dataset in an\nautomatic fashion. Third, we select a subset of correctly labeled (reliable)\nsamples using a label noise detection algorithm. Finally, we train a\nsemi-supervised algorithm on the extended subset. We show that the selection of\nthe network architecture and the self-supervised algorithm are important\nfactors to achieve successful label propagation and demonstrate that ReLaB\nsubstantially improves semi-supervised learning in scenarios of very limited\nsupervision on CIFAR-10, CIFAR-100 and mini-ImageNet. We reach average error\nrates of $\\boldsymbol{22.34}$ with 1 random labeled sample per class on\nCIFAR-10 and lower this error to $\\boldsymbol{8.46}$ when the labeled sample in\neach class is highly representative. Our work is fully reproducible:\nhttps://github.com/PaulAlbert31/ReLaB.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:51:37 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 11:11:52 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Albert", "Paul", ""], ["Ortego", "Diego", ""], ["Arazo", "Eric", ""], ["O'Connor", "Noel E.", ""], ["McGuinness", "Kevin", ""]]}, {"id": "2007.11876", "submitter": "Gabriel Van Zandycke", "authors": "Gabriel Van Zandycke, Christophe De Vleeschouwer", "title": "Real-time CNN-based Segmentation Architecture for Ball Detection in a\n  Single View Setup", "comments": "8 pages, 10 figures", "journal-ref": "Proceedings of the 2nd International Workshop on Multimedia\n  Content Analysis in Sports (2019) 51-58", "doi": "10.1145/3347318.3355517", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of detecting the ball from a single viewpoint\nin the challenging but common case where the ball interacts frequently with\nplayers while being poorly contrasted with respect to the background. We\npropose a novel approach by formulating the problem as a segmentation task\nsolved by an efficient CNN architecture. To take advantage of the ball\ndynamics, the network is fed with a pair of consecutive images. Our inference\nmodel can run in real time without the delay induced by a temporal analysis. We\nalso show that test-time data augmentation allows for a significant increase\nthe detection accuracy. As an additional contribution, we publicly release the\ndataset on which this work is based.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 09:31:32 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Van Zandycke", "Gabriel", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "2007.11888", "submitter": "Siyu Huang", "authors": "Tao Jin, Siyu Huang, Ming Chen, Yingming Li, Zhongfei Zhang", "title": "SBAT: Video Captioning with Sparse Boundary-Aware Transformer", "comments": "Appearing at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of applying the transformer structure\nto video captioning effectively. The vanilla transformer is proposed for\nuni-modal language generation task such as machine translation. However, video\ncaptioning is a multimodal learning problem, and the video features have much\nredundancy between different time steps. Based on these concerns, we propose a\nnovel method called sparse boundary-aware transformer (SBAT) to reduce the\nredundancy in video representation. SBAT employs boundary-aware pooling\noperation for scores from multihead attention and selects diverse features from\ndifferent scenarios. Also, SBAT includes a local correlation scheme to\ncompensate for the local information loss brought by sparse operation. Based on\nSBAT, we further propose an aligned cross-modal encoding scheme to boost the\nmultimodal interaction. Experimental results on two benchmark datasets show\nthat SBAT outperforms the state-of-the-art methods under most of the metrics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 09:57:25 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jin", "Tao", ""], ["Huang", "Siyu", ""], ["Chen", "Ming", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "2007.11899", "submitter": "Fabian Eitel", "authors": "Fabian Eitel, Jan Philipp Albrecht, Martin Weygandt, Friedemann Paul,\n  Kerstin Ritter", "title": "Harnessing spatial homogeneity of neuroimaging data: patch individual\n  filter layers for CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging data, e.g. obtained from magnetic resonance imaging (MRI), is\ncomparably homogeneous due to (1) the uniform structure of the brain and (2)\nadditional efforts to spatially normalize the data to a standard template using\nlinear and non-linear transformations. Convolutional neural networks (CNNs), in\ncontrast, have been specifically designed for highly heterogeneous data, such\nas natural images, by sliding convolutional filters over different positions in\nan image. Here, we suggest a new CNN architecture that combines the idea of\nhierarchical abstraction in neural networks with a prior on the spatial\nhomogeneity of neuroimaging data: Whereas early layers are trained globally\nusing standard convolutional layers, we introduce for higher, more abstract\nlayers patch individual filters (PIF). By learning filters in individual image\nregions (patches) without sharing weights, PIF layers can learn abstract\nfeatures faster and with fewer samples. We thoroughly evaluated PIF layers for\nthree different tasks and data sets, namely sex classification on UK Biobank\ndata, Alzheimer's disease detection on ADNI data and multiple sclerosis\ndetection on private hospital data. We demonstrate that CNNs using PIF layers\nresult in higher accuracies, especially in low sample size settings, and need\nfewer training epochs for convergence. To the best of our knowledge, this is\nthe first study which introduces a prior on brain MRI for CNN learning.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:11:43 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Eitel", "Fabian", ""], ["Albrecht", "Jan Philipp", ""], ["Weygandt", "Martin", ""], ["Paul", "Friedemann", ""], ["Ritter", "Kerstin", ""]]}, {"id": "2007.11901", "submitter": "Wenguan Wang", "authors": "Qinghao Meng, Wenguan Wang, Tianfei Zhou, Jianbing Shen, Luc Van Gool\n  and Dengxin Dai", "title": "Weakly Supervised 3D Object Detection from Lidar Point Cloud", "comments": "ECCV 2020; website: https://github.com/hlesmqh/WS3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is laborious to manually label point cloud data for training high-quality\n3D object detectors. This work proposes a weakly supervised approach for 3D\nobject detection, only requiring a small set of weakly annotated scenes,\nassociated with a few precisely labeled object instances. This is achieved by a\ntwo-stage architecture design. Stage-1 learns to generate cylindrical object\nproposals under weak supervision, i.e., only the horizontal centers of objects\nare click-annotated on bird's view scenes. Stage-2 learns to refine the\ncylindrical proposals to get cuboids and confidence scores, using a few\nwell-labeled object instances. Using only 500 weakly annotated scenes and 534\nprecisely labeled vehicle instances, our method achieves 85-95% the performance\nof current top-leading, fully supervised detectors (which require 3, 712\nexhaustively and precisely annotated scenes with 15, 654 instances). More\nimportantly, with our elaborately designed network architecture, our trained\nmodel can be applied as a 3D object annotator, allowing both automatic and\nactive working modes. The annotations generated by our model can be used to\ntrain 3D object detectors with over 94% of their original performance (under\nmanually labeled data). Our experiments also show our model's potential in\nboosting performance given more training data. Above designs make our approach\nhighly practical and introduce new opportunities for learning 3D object\ndetection with reduced annotation burden.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:12:46 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Meng", "Qinghao", ""], ["Wang", "Wenguan", ""], ["Zhou", "Tianfei", ""], ["Shen", "Jianbing", ""], ["Van Gool", "Luc", ""], ["Dai", "Dengxin", ""]]}, {"id": "2007.11924", "submitter": "Anna Nguyen", "authors": "Anna Nguyen, Adrian Oberf\\\"oll, Michael F\\\"arber", "title": "Right for the Right Reason: Making Image Classification Robust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Convolutional Neural Networks (CNNs)in classifying image\ndata has been thoroughly demonstrated. In order to explain the classification\nto humans, methods for visualizing classification evidence have been developed\nin recent years. These explanations reveal that sometimes images are classified\ncorrectly, but for the wrong reasons,i.e., based on incidental evidence. Of\ncourse, it is desirable that images are classified correctly for the right\nreasons, i.e., based on the actual evidence. To this end, we propose a new\nexplanation quality metric to measure object aligned explanation in image\nclassification which we refer to as theObAlExmetric. Using object detection\napproaches, explanation approaches, and ObAlEx, we quantify the focus of CNNs\non the actual evidence. Moreover, we show that additional training of the CNNs\ncan improve the focus of CNNs without decreasing their accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:55:11 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 14:26:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Nguyen", "Anna", ""], ["Oberf\u00f6ll", "Adrian", ""], ["F\u00e4rber", "Michael", ""]]}, {"id": "2007.11946", "submitter": "Tianze Rong", "authors": "Tianze Rong, Yanjia Zhu, Hongxiang Cai, Yichao Xiong", "title": "A Solution to Product detection in Densely Packed Scenes", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a solution to densely packed scenes dataset SKU-110k. Our work\nis modified from Cascade R-CNN. To solve the problem, we proposed a random crop\nstrategy to ensure both the sampling rate and input scale is relatively\nsufficient as a contrast to the regular random crop. And we adopted some of\ntrick and optimized the hyper-parameters. To grasp the essential feature of the\ndensely packed scenes, we analysis the stages of a detector and investigate the\nbottleneck which limits the performance. As a result, our method obtains 58.7\nmAP on test set of SKU-110k.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:58:45 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 02:52:10 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Rong", "Tianze", ""], ["Zhu", "Yanjia", ""], ["Cai", "Hongxiang", ""], ["Xiong", "Yichao", ""]]}, {"id": "2007.11965", "submitter": "Alexey Bokhovkin", "authors": "Vladislav Ishimtsev, Alexey Bokhovkin, Alexey Artemov, Savva Ignatyev,\n  Matthias Niessner, Denis Zorin, Evgeny Burnaev", "title": "CAD-Deform: Deformable Fitting of CAD Models to 3D Scans", "comments": "25 pages, 13 figures, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape retrieval and alignment are a promising avenue towards turning 3D scans\ninto lightweight CAD representations that can be used for content creation such\nas mobile or AR/VR gaming scenarios. Unfortunately, CAD model retrieval is\nlimited by the availability of models in standard 3D shape collections (e.g.,\nShapeNet). In this work, we address this shortcoming by introducing CAD-Deform,\na method which obtains more accurate CAD-to-scan fits by non-rigidly deforming\nretrieved CAD models. Our key contribution is a new non-rigid deformation model\nincorporating smooth transformations and preservation of sharp features, that\nsimultaneously achieves very tight fits from CAD models to the 3D scan and\nmaintains the clean, high-quality surface properties of hand-modeled CAD\nobjects. A series of thorough experiments demonstrate that our method achieves\nsignificantly tighter scan-to-CAD fits, allowing a more accurate digital\nreplica of the scanned real-world environment while preserving important\ngeometric features present in synthetic CAD environments.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:30:20 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ishimtsev", "Vladislav", ""], ["Bokhovkin", "Alexey", ""], ["Artemov", "Alexey", ""], ["Ignatyev", "Savva", ""], ["Niessner", "Matthias", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2007.11976", "submitter": "Mahesh Chandra", "authors": "Mahesh Chandra", "title": "Comparative Analysis of Polynomial and Rational Approximations of\n  Hyperbolic Tangent Function for VLSI Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks yield the state-of-the-art results in many computer\nvision and human machine interface applications such as object detection,\nspeech recognition etc. Since, these networks are computationally expensive,\ncustomized accelerators are designed for achieving the required performance at\nlower cost and power. One of the key building blocks of these neural networks\nis non-linear activation function such as sigmoid, hyperbolic tangent (tanh),\nand ReLU. A low complexity accurate hardware implementation of the activation\nfunction is required to meet the performance and area targets of the neural\nnetwork accelerators. Even though, various methods and implementations of tanh\nactivation function have been published, a comparative study is missing. This\npaper presents comparative analysis of polynomial and rational methods and\ntheir hardware implementation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:31:02 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 13:41:18 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Chandra", "Mahesh", ""]]}, {"id": "2007.11978", "submitter": "Tao Wang", "authors": "Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng Tang,\n  Steven Hoi, Jiashi Feng", "title": "The Devil is in Classification: A Simple Framework for Long-tail Object\n  Detection and Instance Segmentation", "comments": "LVIS 2019 challenge winner, performance significantly improved after\n  challenge submission, accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing object instance detection and segmentation models only work\nwell on fairly balanced benchmarks where per-category training sample numbers\nare comparable, such as COCO. They tend to suffer performance drop on realistic\ndatasets that are usually long-tailed. This work aims to study and address such\nopen challenges. Specifically, we systematically investigate performance drop\nof the state-of-the-art two-stage instance segmentation model Mask R-CNN on the\nrecent long-tail LVIS dataset, and unveil that a major cause is the inaccurate\nclassification of object proposals. Based on such an observation, we first\nconsider various techniques for improving long-tail classification performance\nwhich indeed enhance instance segmentation results. We then propose a simple\ncalibration framework to more effectively alleviate classification head bias\nwith a bi-level class balanced sampling approach. Without bells and whistles,\nit significantly boosts the performance of instance segmentation for tail\nclasses on the recent LVIS dataset and our sampled COCO-LT dataset. Our\nanalysis provides useful insights for solving long-tail instance detection and\nsegmentation problems, and the straightforward \\emph{SimCal} method can serve\nas a simple but strong baseline. With the method we have won the 2019 LVIS\nchallenge. Codes and models are available at https://github.com/twangnh/SimCal.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:49:07 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 01:40:12 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 00:59:31 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 03:36:02 GMT"}, {"version": "v5", "created": "Tue, 3 Nov 2020 04:11:23 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Tao", ""], ["Li", "Yu", ""], ["Kang", "Bingyi", ""], ["Li", "Junnan", ""], ["Liew", "Junhao", ""], ["Tang", "Sheng", ""], ["Hoi", "Steven", ""], ["Feng", "Jiashi", ""]]}, {"id": "2007.11983", "submitter": "Kenneth Lai", "authors": "Kenneth Lai and Svetlana N. Yanushkevich", "title": "CNN+RNN Depth and Skeleton based Dynamic Hand Gesture Recognition", "comments": null, "journal-ref": "2018 24th International Conference on Pattern Recognition (ICPR),\n  Beijing, 2018, pp. 3451-3456", "doi": "10.1109/ICPR.2018.8545718", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity and gesture recognition is an important component of rapidly\ngrowing domain of ambient intelligence, in particular in assisting living and\nsmart homes. In this paper, we propose to combine the power of two deep\nlearning techniques, the convolutional neural networks (CNN) and the recurrent\nneural networks (RNN), for automated hand gesture recognition using both depth\nand skeleton data. Each of these types of data can be used separately to train\nneural networks to recognize hand gestures. While RNN were reported previously\nto perform well in recognition of sequences of movement for each skeleton joint\ngiven the skeleton information only, this study aims at utilizing depth data\nand apply CNN to extract important spatial information from the depth images.\nTogether, the tandem CNN+RNN is capable of recognizing a sequence of gestures\nmore accurately. As well, various types of fusion are studied to combine both\nthe skeleton and depth information in order to extract temporal-spatial\ninformation. An overall accuracy of 85.46% is achieved on the dynamic hand\ngesture-14/28 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:25:19 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lai", "Kenneth", ""], ["Yanushkevich", "Svetlana N.", ""]]}, {"id": "2007.11984", "submitter": "Ning Wang", "authors": "Ning Wang and Wengang Zhou and Yibing Song and Chao Ma and Wei Liu and\n  Houqiang Li", "title": "Unsupervised Deep Representation Learning for Real-Time Tracking", "comments": "Journal version of the CVPR2019 paper \"Unsupervised Deep Tracking\".\n  Accepted by IJCV. arXiv admin note: text overlap with arXiv:1904.01828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of visual tracking has continuously been brought by deep\nlearning models. Typically, supervised learning is employed to train these\nmodels with expensive labeled data. In order to reduce the workload of manual\nannotations and learn to track arbitrary objects, we propose an unsupervised\nlearning method for visual tracking. The motivation of our unsupervised\nlearning is that a robust tracker should be effective in bidirectional\ntracking. Specifically, the tracker is able to forward localize a target object\nin successive frames and backtrace to its initial position in the first frame.\nBased on such a motivation, in the training process, we measure the consistency\nbetween forward and backward trajectories to learn a robust tracker from\nscratch merely using unlabeled videos. We build our framework on a Siamese\ncorrelation filter network, and propose a multi-frame validation scheme and a\ncost-sensitive loss to facilitate unsupervised learning. Without bells and\nwhistles, the proposed unsupervised tracker achieves the baseline accuracy as\nclassic fully supervised trackers while achieving a real-time speed.\nFurthermore, our unsupervised framework exhibits a potential in leveraging more\nunlabeled or weakly labeled data to further improve the tracking accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:23:12 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wang", "Ning", ""], ["Zhou", "Wengang", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Liu", "Wei", ""], ["Li", "Houqiang", ""]]}, {"id": "2007.11986", "submitter": "Kenneth Lai", "authors": "Kenneth Lai, Xinyuan Tu, and Svetlana Yanushkevich", "title": "Dog Identification using Soft Biometrics and Neural Networks", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN),\n  Budapest, Hungary, 2019, pp. 1-8", "doi": "10.1109/IJCNN.2019.8851971", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of biometric identification of animals,\nspecifically dogs. We apply advanced machine learning models such as deep\nneural network on the photographs of pets in order to determine the pet\nidentity. In this paper, we explore the possibility of using different types of\n\"soft\" biometrics, such as breed, height, or gender, in fusion with \"hard\"\nbiometrics such as photographs of the pet's face. We apply the principle of\ntransfer learning on different Convolutional Neural Networks, in order to\ncreate a network designed specifically for breed classification. The proposed\nnetwork is able to achieve an accuracy of 90.80% and 91.29% when\ndifferentiating between the two dog breeds, for two different datasets. Without\nthe use of \"soft\" biometrics, the identification rate of dogs is 78.09% but by\nusing a decision network to incorporate \"soft\" biometrics, the identification\nrate can achieve an accuracy of 84.94%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:22:46 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lai", "Kenneth", ""], ["Tu", "Xinyuan", ""], ["Yanushkevich", "Svetlana", ""]]}, {"id": "2007.11987", "submitter": "Kenneth Lai", "authors": "Kenneth Lai and Svetlana N. Yanushkevich", "title": "Multi-Metric Evaluation of Thermal-to-Visual Face Recognition", "comments": null, "journal-ref": "2019 Eighth International Conference on Emerging Security\n  Technologies (EST), Colchester, United Kingdom, 2019, pp. 1-6", "doi": "10.1109/EST.2019.8806202", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to address the problem of heterogeneous or\ncross-spectral face recognition using machine learning to synthesize visual\nspectrum face from infrared images. The synthesis of visual-band face images\nallows for more optimal extraction of facial features to be used for face\nidentification and/or verification. We explore the ability to use Generative\nAdversarial Networks (GANs) for face image synthesis, and examine the\nperformance of these images using pre-trained Convolutional Neural Networks\n(CNNs). The features extracted using CNNs are applied in face identification\nand verification. We explore the performance in terms of acceptance rate when\nusing various similarity measures for face verification.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 10:18:34 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lai", "Kenneth", ""], ["Yanushkevich", "Svetlana N.", ""]]}, {"id": "2007.11993", "submitter": "Md. Kamrul Hasan", "authors": "Md. Kamrul Hasan, Md. Ashraful Alam, Md. Toufick E Elahi, Shidhartho\n  Roy, Sifat Redwan Wahid", "title": "CVR-Net: A deep convolutional neural network for coronavirus recognition\n  from chest radiography images", "comments": "31 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel Coronavirus Disease 2019 (COVID-19) is a global pandemic disease\nspreading rapidly around the world. A robust and automatic early recognition of\nCOVID-19, via auxiliary computer-aided diagnostic tools, is essential for\ndisease cure and control. The chest radiography images, such as Computed\nTomography (CT) and X-ray, and deep Convolutional Neural Networks (CNNs), can\nbe a significant and useful material for designing such tools. However,\ndesigning such an automated tool is challenging as a massive number of manually\nannotated datasets are not publicly available yet, which is the core\nrequirement of supervised learning systems. In this article, we propose a\nrobust CNN-based network, called CVR-Net (Coronavirus Recognition Network), for\nthe automatic recognition of the coronavirus from CT or X-ray images. The\nproposed end-to-end CVR-Net is a multi-scale-multi-encoder ensemble model,\nwhere we have aggregated the outputs from two different encoders and their\ndifferent scales to obtain the final prediction probability. We train and test\nthe proposed CVR-Net on three different datasets, where the images have\ncollected from different open-source repositories. We compare our proposed\nCVR-Net with state-of-the-art methods, which are trained and tested on the same\ndatasets. We split three datasets into five different tasks, where each task\nhas a different number of classes, to evaluate the multi-tasking CVR-Net. Our\nmodel achieves an overall F1-score & accuracy of 0.997 & 0.998; 0.963 & 0.964;\n0.816 & 0.820; 0.961 & 0.961; and 0.780 & 0.780, respectively, for task-1 to\ntask-5. As the CVR-Net provides promising results on the small datasets, it can\nbe an auspicious computer-aided diagnostic tool for the diagnosis of\ncoronavirus to assist the clinical practitioners and radiologists. Our source\ncodes and model are publicly available at\nhttps://github.com/kamruleee51/CVR-Net.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:21:29 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Alam", "Md. Ashraful", ""], ["Elahi", "Md. Toufick E", ""], ["Roy", "Shidhartho", ""], ["Wahid", "Sifat Redwan", ""]]}, {"id": "2007.12034", "submitter": "Xiaofang Wang", "authors": "Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael\n  S. Ryoo, Anelia Angelova, Kris M. Kitani and Wei Hua", "title": "AttentionNAS: Spatiotemporal Attention Cell Search for Video\n  Classification", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional operations have two limitations: (1) do not explicitly model\nwhere to focus as the same filter is applied to all the positions, and (2) are\nunsuitable for modeling long-range dependencies as they only operate on a small\nneighborhood. While both limitations can be alleviated by attention operations,\nmany design choices remain to be determined to use attention, especially when\napplying attention to videos. Towards a principled way of applying attention to\nvideos, we address the task of spatiotemporal attention cell search. We propose\na novel search space for spatiotemporal attention cells, which allows the\nsearch algorithm to flexibly explore various design choices in the cell. The\ndiscovered attention cells can be seamlessly inserted into existing backbone\nnetworks, e.g., I3D or S3D, and improve video classification accuracy by more\nthan 2% on both Kinetics-600 and MiT datasets. The discovered attention cells\noutperform non-local blocks on both datasets, and demonstrate strong\ngeneralization across different modalities, backbones, and datasets. Inserting\nour attention cells into I3D-R50 yields state-of-the-art performance on both\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:30:05 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 04:25:23 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wang", "Xiaofang", ""], ["Xiong", "Xuehan", ""], ["Neumann", "Maxim", ""], ["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""], ["Angelova", "Anelia", ""], ["Kitani", "Kris M.", ""], ["Hua", "Wei", ""]]}, {"id": "2007.12036", "submitter": "Sergio Casas", "authors": "Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie Liao, Raquel\n  Urtasun", "title": "Implicit Latent Variable Model for Scene-Consistent Motion Forecasting", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to plan a safe maneuver an autonomous vehicle must accurately\nperceive its environment, and understand the interactions among traffic\nparticipants. In this paper, we aim to learn scene-consistent motion forecasts\nof complex urban traffic directly from sensor data. In particular, we propose\nto characterize the joint distribution over future trajectories via an implicit\nlatent variable model. We model the scene as an interaction graph and employ\npowerful graph neural networks to learn a distributed latent representation of\nthe scene. Coupled with a deterministic decoder, we obtain trajectory samples\nthat are consistent across traffic participants, achieving state-of-the-art\nresults in motion forecasting and interaction understanding. Last but not\nleast, we demonstrate that our motion forecasts result in safer and more\ncomfortable motion planning.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:31:25 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Casas", "Sergio", ""], ["Gulino", "Cole", ""], ["Suo", "Simon", ""], ["Luo", "Katie", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2007.12065", "submitter": "Jeremy Castagno", "authors": "Jeremy Castagno, Ella Atkins", "title": "Polylidar3D -- Fast Polygon Extraction from 3D Data", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flat surfaces captured by 3D point clouds are often used for localization,\nmapping, and modeling. Dense point cloud processing has high computation and\nmemory costs making low-dimensional representations of flat surfaces such as\npolygons desirable. We present Polylidar3D, a non-convex polygon extraction\nalgorithm which takes as input unorganized 3D point clouds (e.g., LiDAR data),\norganized point clouds (e.g., range images), or user-provided meshes.\nNon-convex polygons represent flat surfaces in an environment with interior\ncutouts representing obstacles or holes. The Polylidar3D front-end transforms\ninput data into a half-edge triangular mesh. This representation provides a\ncommon level of input data abstraction for subsequent back-end processing. The\nPolylidar3D back-end is composed of four core algorithms: mesh smoothing,\ndominant plane normal estimation, planar segment extraction, and finally\npolygon extraction. Polylidar3D is shown to be quite fast, making use of CPU\nmulti-threading and GPU acceleration when available. We demonstrate\nPolylidar3D's versatility and speed with real-world datasets including aerial\nLiDAR point clouds for rooftop mapping, autonomous driving LiDAR point clouds\nfor road surface detection, and RGBD cameras for indoor floor/wall detection.\nWe also evaluate Polylidar3D on a challenging planar segmentation benchmark\ndataset. Results consistently show excellent speed and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:22:43 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Castagno", "Jeremy", ""], ["Atkins", "Ella", ""]]}, {"id": "2007.12066", "submitter": "Yanming Sun", "authors": "Yanming Sun, Chunyan Wang", "title": "A Computation-Efficient CNN System for High-Quality Brain Tumor\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Convolutional Neural Network (CNN) system is proposed for\nbrain tumor segmentation. The system consists of three parts, a pre-processing\nblock to reduce the data volume, an application-specific CNN(ASCNN) to segment\ntumor areas precisely, and a refinement block to detect/remove false positive\npixels. The CNN, designed specifically for the task, has 7 convolution layers,\n16 channels per layer, requiring only 11716 parameters. The convolutions\ncombined with max-pooling in the first half of the CNN are performed to\nlocalize tumor areas. Two convolution modes, namely depthwise convolution and\nstandard convolution, are performed in parallel in the first 2 layers to\nextract elementary features efficiently. For a fine classification of\npixel-wise precision in the second half of the CNN, the feature maps are\nmodulated by adding the individually weighted local feature maps generated in\nthe first half of the CNN. The performance of the proposed system has been\nevaluated by an online platform with dataset of Multimodal Brain Tumor Image\nSegmentation Benchmark (BRATS) 2018. Requiring a very low computation volume,\nthe proposed system delivers a high segmentation quality indicated by its\naverage Dice scores of 0.75, 0.88 and 0.76 for enhancing tumor, whole tumor and\ntumor core, respectively, and also by the median Dice scores of 0.85, 0.92, and\n0.86. The consistency in system performance has also been measured,\ndemonstrating that the system is able to reproduce almost the same output to\nthe same input after retraining. The simple structure of the proposed system\nfacilitates its implementation in computation restricted environment, and a\nwide range of applications can thus be expected.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 02:45:54 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 19:23:24 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sun", "Yanming", ""], ["Wang", "Chunyan", ""]]}, {"id": "2007.12072", "submitter": "Liming Jiang", "authors": "Liming Jiang, Changxu Zhang, Mingyang Huang, Chunxiao Liu, Jianping\n  Shi, Chen Change Loy", "title": "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "comments": "ECCV 2020 (Spotlight). Table 2 is updated. GitHub:\n  https://github.com/EndlessSora/TSIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and versatile framework for image-to-image translation.\nWe unearth the importance of normalization layers, and provide a carefully\ndesigned two-stream generative model with newly proposed feature\ntransformations in a coarse-to-fine fashion. This allows multi-scale semantic\nstructure information and style representation to be effectively captured and\nfused by the network, permitting our method to scale to various tasks in both\nunsupervised and supervised settings. No additional constraints (e.g., cycle\nconsistency) are needed, contributing to a very clean and simple method.\nMulti-modal image synthesis with arbitrary style control is made possible. A\nsystematic study compares the proposed method with several state-of-the-art\ntask-specific baselines, verifying its effectiveness in both perceptual quality\nand quantitative evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:34:06 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 11:20:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Jiang", "Liming", ""], ["Zhang", "Changxu", ""], ["Huang", "Mingyang", ""], ["Liu", "Chunxiao", ""], ["Shi", "Jianping", ""], ["Loy", "Chen Change", ""]]}, {"id": "2007.12075", "submitter": "Yujie Zhong", "authors": "Yujie Zhong, Zelu Deng, Sheng Guo, Matthew R. Scott, Weilin Huang", "title": "Representation Sharing for Fast Object Detector Search and Beyond", "comments": "ECCV 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region Proposal Network (RPN) provides strong support for handling the scale\nvariation of objects in two-stage object detection. For one-stage detectors\nwhich do not have RPN, it is more demanding to have powerful sub-networks\ncapable of directly capturing objects of unknown sizes. To enhance such\ncapability, we propose an extremely efficient neural architecture search\nmethod, named Fast And Diverse (FAD), to better explore the optimal\nconfiguration of receptive fields and convolution types in the sub-networks for\none-stage detectors. FAD consists of a designed search space and an efficient\narchitecture search algorithm. The search space contains a rich set of diverse\ntransformations designed specifically for object detection. To cope with the\ndesigned search space, a novel search algorithm termed Representation Sharing\n(RepShare) is proposed to effectively identify the best combinations of the\ndefined transformations. In our experiments, FAD obtains prominent improvements\non two types of one-stage detectors with various backbones. In particular, our\nFAD detector achieves 46.4 AP on MS-COCO (under single-scale testing),\noutperforming the state-of-the-art detectors, including the most recent\nNAS-based detectors, Auto-FPN (searched for 16 GPU-days) and NAS-FCOS (28\nGPU-days), while significantly reduces the search cost to 0.6 GPU-days. Beyond\nobject detection, we further demonstrate the generality of FAD on the more\nchallenging instance segmentation, and expect it to benefit more tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:39:44 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:42:10 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 02:17:36 GMT"}, {"version": "v4", "created": "Fri, 23 Oct 2020 07:55:42 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhong", "Yujie", ""], ["Deng", "Zelu", ""], ["Guo", "Sheng", ""], ["Scott", "Matthew R.", ""], ["Huang", "Weilin", ""]]}, {"id": "2007.12082", "submitter": "Hongyu Li", "authors": "Hongyu Li, Jihe Wang, Yu Zhang, Zirui Wang, and Tiejun Wang", "title": "A Study on Evaluation Standard for Automatic Crack Detection Regard the\n  Random Fractal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reasonable evaluation standard underlies construction of effective deep\nlearning models. However, we find in experiments that the automatic crack\ndetectors based on deep learning are obviously underestimated by the widely\nused mean Average Precision (mAP) standard. This paper presents a study on the\nevaluation standard. It is clarified that the random fractal of crack disables\nthe mAP standard, because the strict box matching in mAP calculation is\nunreasonable for the fractal feature. As a solution, a fractal-available\nevaluation standard named CovEval is proposed to correct the underestimation in\ncrack detection. In CovEval, a different matching process based on the idea of\ncovering box matching is adopted for this issue. In detail, Cover Area rate\n(CAr) is designed as a covering overlap, and a multi-match strategy is employed\nto release the one-to-one matching restriction in mAP. Extended Recall (XR),\nExtended Precision (XP) and Extended F-score (Fext) are defined for scoring the\ncrack detectors. In experiments using several common frameworks for object\ndetection, models get much higher scores in crack detection according to\nCovEval, which matches better with the visual performance. Moreover, based on\nfaster R-CNN framework, we present a case study to optimize a crack detector\nbased on CovEval standard. Recall (XR) of our best model achieves an\nindustrial-level at 95.8, which implies that with reasonable standard for\nevaluation, the methods for object detection are with great potential for\nautomatic industrial inspection.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:46:29 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Li", "Hongyu", ""], ["Wang", "Jihe", ""], ["Zhang", "Yu", ""], ["Wang", "Zirui", ""], ["Wang", "Tiejun", ""]]}, {"id": "2007.12088", "submitter": "Xuchong Qiu", "authors": "Xuchong Qiu and Yang Xiao and Chaohui Wang and Renaud Marlet", "title": "Pixel-Pair Occlusion Relationship Map(P2ORM): Formulation, Inference &\n  Application", "comments": "Accepted to ECCV 2020 as a spotlight. Project page:\n  http://imagine.enpc.fr/~qiux/P2ORM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize concepts around geometric occlusion in 2D images (i.e., ignoring\nsemantics), and propose a novel unified formulation of both occlusion\nboundaries and occlusion orientations via a pixel-pair occlusion relation. The\nformer provides a way to generate large-scale accurate occlusion datasets\nwhile, based on the latter, we propose a novel method for task-independent\npixel-level occlusion relationship estimation from single images. Experiments\non a variety of datasets demonstrate that our method outperforms existing ones\non this task. To further illustrate the value of our formulation, we also\npropose a new depth map refinement method that consistently improve the\nperformance of state-of-the-art monocular depth estimation methods. Our code\nand data are available at http://imagine.enpc.fr/~qiux/P2ORM/.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:52:09 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Qiu", "Xuchong", ""], ["Xiao", "Yang", ""], ["Wang", "Chaohui", ""], ["Marlet", "Renaud", ""]]}, {"id": "2007.12099", "submitter": "Xiang Long", "authors": "Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang,\n  Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, Shilei Wen", "title": "PP-YOLO: An Effective and Efficient Implementation of Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is one of the most important areas in computer vision, which\nplays a key role in various practical scenarios. Due to limitation of hardware,\nit is often necessary to sacrifice accuracy to ensure the infer speed of the\ndetector in practice. Therefore, the balance between effectiveness and\nefficiency of object detector must be considered. The goal of this paper is to\nimplement an object detector with relatively balanced effectiveness and\nefficiency that can be directly applied in actual application scenarios, rather\nthan propose a novel detection model. Considering that YOLOv3 has been widely\nused in practice, we develop a new object detector based on YOLOv3. We mainly\ntry to combine various existing tricks that almost not increase the number of\nmodel parameters and FLOPs, to achieve the goal of improving the accuracy of\ndetector as much as possible while ensuring that the speed is almost unchanged.\nSince all experiments in this paper are conducted based on PaddlePaddle, we\ncall it PP-YOLO. By combining multiple tricks, PP-YOLO can achieve a better\nbalance between effectiveness (45.2% mAP) and efficiency (72.9 FPS), surpassing\nthe existing state-of-the-art detectors such as EfficientDet and YOLOv4.Source\ncode is at https://github.com/PaddlePaddle/PaddleDetection.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:06:16 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 06:10:35 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 03:53:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Long", "Xiang", ""], ["Deng", "Kaipeng", ""], ["Wang", "Guanzhong", ""], ["Zhang", "Yang", ""], ["Dang", "Qingqing", ""], ["Gao", "Yuan", ""], ["Shen", "Hui", ""], ["Ren", "Jianguo", ""], ["Han", "Shumin", ""], ["Ding", "Errui", ""], ["Wen", "Shilei", ""]]}, {"id": "2007.12104", "submitter": "Xianyu Chen", "authors": "Xianyu Chen, Ming Jiang, Qi Zhao", "title": "Leveraging Bottom-Up and Top-Down Attention for Few-Shot Object\n  Detection", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection aims at detecting objects with few annotated\nexamples, which remains a challenging research problem yet to be explored.\nRecent studies have shown the effectiveness of self-learned top-down attention\nmechanisms in object detection and other vision tasks. The top-down attention,\nhowever, is less effective at improving the performance of few-shot detectors.\nDue to the insufficient training data, object detectors cannot effectively\ngenerate attention maps for few-shot examples. To improve the performance and\ninterpretability of few-shot object detectors, we propose an attentive few-shot\nobject detection network (AttFDNet) that takes the advantages of both top-down\nand bottom-up attention. Being task-agnostic, the bottom-up attention serves as\na prior that helps detect and localize naturally salient objects. We further\naddress specific challenges in few-shot object detection by introducing two\nnovel loss terms and a hybrid few-shot learning strategy. Experimental results\nand visualization demonstrate the complementary nature of the two types of\nattention and their roles in few-shot object detection. Codes are available at\nhttps://github.com/chenxy99/AttFDNet.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:12:04 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Chen", "Xianyu", ""], ["Jiang", "Ming", ""], ["Zhao", "Qi", ""]]}, {"id": "2007.12107", "submitter": "Yang Xiao", "authors": "Yang Xiao, Renaud Marlet", "title": "Few-Shot Object Detection and Viewpoint Estimation for Objects in the\n  Wild", "comments": "Accepted as Poster at ECCV 2020, project website:\n  http://imagine.enpc.fr/~xiaoy/FSDetView/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects and estimating their viewpoint in images are key tasks of\n3D scene understanding. Recent approaches have achieved excellent results on\nvery large benchmarks for object detection and viewpoint estimation. However,\nperformances are still lagging behind for novel object categories with few\nsamples. In this paper, we tackle the problems of few-shot object detection and\nfew-shot viewpoint estimation. We propose a meta-learning framework that can be\napplied to both tasks, possibly including 3D data. Our models improve the\nresults on objects of novel classes by leveraging on rich feature information\noriginating from base classes with many samples. A simple joint feature\nembedding module is proposed to make the most of this feature sharing. Despite\nits simplicity, our method outperforms state-of-the-art methods by a large\nmargin on a range of datasets, including PASCAL VOC and MS COCO for few-shot\nobject detection, and Pascal3D+ and ObjectNet3D for few-shot viewpoint\nestimation. And for the first time, we tackle the combination of both few-shot\ntasks, on Object- Net3D, showing promising results. Our code and data are\navailable at http://imagine.enpc.fr/~xiaoy/FSDetView/.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:17:25 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Xiao", "Yang", ""], ["Marlet", "Renaud", ""]]}, {"id": "2007.12130", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Moitreya Chatterjee, Narendra Ahuja", "title": "Sound2Sight: Generating Visual Dynamics from Sound and Context", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning associations across modalities is critical for robust multimodal\nreasoning, especially when a modality may be missing during inference. In this\npaper, we study this problem in the context of audio-conditioned visual\nsynthesis -- a task that is important, for example, in occlusion reasoning.\nSpecifically, our goal is to generate future video frames and their motion\ndynamics conditioned on audio and a few past frames. To tackle this problem, we\npresent Sound2Sight, a deep variational framework, that is trained to learn a\nper frame stochastic prior conditioned on a joint embedding of audio and past\nframes. This embedding is learned via a multi-head attention-based audio-visual\ntransformer encoder. The learned prior is then sampled to further condition a\nvideo forecasting module to generate future frames. The stochastic prior allows\nthe model to sample multiple plausible futures that are consistent with the\nprovided audio and the past context. Moreover, to improve the quality and\ncoherence of the generated frames, we propose a multimodal discriminator that\ndifferentiates between a synthesized and a real audio-visual clip. We\nempirically evaluate our approach, vis-\\'a-vis closely-related prior methods,\non two new datasets viz. (i) Multimodal Stochastic Moving MNIST with a Surprise\nObstacle, (ii) Youtube Paintings; as well as on the existing Audio-Set Drums\ndataset. Our extensive experiments demonstrate that Sound2Sight significantly\noutperforms the state of the art in the generated video quality, while also\nproducing diverse video content.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:57:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Cherian", "Anoop", ""], ["Chatterjee", "Moitreya", ""], ["Ahuja", "Narendra", ""]]}, {"id": "2007.12131", "submitter": "G\\\"ul Varol", "authors": "Samuel Albanie and G\\\"ul Varol and Liliane Momeni and Triantafyllos\n  Afouras and Joon Son Chung and Neil Fox and Andrew Zisserman", "title": "BSL-1K: Scaling up co-articulated sign language recognition using\n  mouthing cues", "comments": "Appears in: European Conference on Computer Vision 2020 (ECCV 2020).\n  28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in fine-grained gesture and action classification, and\nmachine translation, point to the possibility of automated sign language\nrecognition becoming a reality. A key stumbling block in making progress\ntowards this goal is a lack of appropriate training data, stemming from the\nhigh complexity of sign annotation and a limited supply of qualified\nannotators. In this work, we introduce a new scalable approach to data\ncollection for sign recognition in continuous videos. We make use of\nweakly-aligned subtitles for broadcast footage together with a keyword spotting\nmethod to automatically localise sign-instances for a vocabulary of 1,000 signs\nin 1,000 hours of video. We make the following contributions: (1) We show how\nto use mouthing cues from signers to obtain high-quality annotations from video\ndata - the result is the BSL-1K dataset, a collection of British Sign Language\n(BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train\nstrong sign recognition models for co-articulated signs in BSL and that these\nmodels additionally form excellent pretraining for other sign languages and\nbenchmarks - we exceed the state of the art on both the MSASL and WLASL\nbenchmarks. Finally, (3) we propose new large-scale evaluation sets for the\ntasks of sign recognition and sign spotting and provide baselines which we hope\nwill serve to stimulate research in this area.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:59:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Albanie", "Samuel", ""], ["Varol", "G\u00fcl", ""], ["Momeni", "Liliane", ""], ["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Fox", "Neil", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2007.12140", "submitter": "Vladimir Tankovich", "authors": "Vladimir Tankovich, Christian H\\\"ane, Yinda Zhang, Adarsh Kowdle, Sean\n  Fanello, Sofien Bouaziz", "title": "HITNet: Hierarchical Iterative Tile Refinement Network for Real-time\n  Stereo Matching", "comments": "The pretrained models used for submission to benchmarks and sample\n  evaluation scripts can be found at\n  https://github.com/google-research/google-research/tree/master/hitnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents HITNet, a novel neural network architecture for real-time\nstereo matching. Contrary to many recent neural network approaches that operate\non a full cost volume and rely on 3D convolutions, our approach does not\nexplicitly build a volume and instead relies on a fast multi-resolution\ninitialization step, differentiable 2D geometric propagation and warping\nmechanisms to infer disparity hypotheses. To achieve a high level of accuracy,\nour network not only geometrically reasons about disparities but also infers\nslanted plane hypotheses allowing to more accurately perform geometric warping\nand upsampling operations. Our architecture is inherently multi-resolution\nallowing the propagation of information across different levels. Multiple\nexperiments prove the effectiveness of the proposed approach at a fraction of\nthe computation required by state-of-the-art methods. At the time of writing,\nHITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two\nview stereo, ranks 1st on most of the metrics among all the end-to-end learning\napproaches on Middlebury-v3, ranks 1st on the popular KITTI 2012 and 2015\nbenchmarks among the published methods faster than 100ms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:11:48 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 20:23:44 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 17:51:33 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Tankovich", "Vladimir", ""], ["H\u00e4ne", "Christian", ""], ["Zhang", "Yinda", ""], ["Kowdle", "Adarsh", ""], ["Fanello", "Sean", ""], ["Bouaziz", "Sofien", ""]]}, {"id": "2007.12142", "submitter": "Jinjin Gu", "authors": "Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, Chao Dong", "title": "PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual\n  Image Restoration", "comments": "This paper has been accepted for publication at ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) is the key factor for the fast development of\nimage restoration (IR) algorithms. The most recent IR methods based on\nGenerative Adversarial Networks (GANs) have achieved significant improvement in\nvisual performance, but also presented great challenges for quantitative\nevaluation. Notably, we observe an increasing inconsistency between perceptual\nquality and the evaluation results. Then we raise two questions: (1) Can\nexisting IQA methods objectively evaluate recent IR algorithms? (2) When focus\non beating current benchmarks, are we getting better IR algorithms? To answer\nthese questions and promote the development of IQA methods, we contribute a\nlarge-scale IQA dataset, called Perceptual Image Processing Algorithms (PIPAL)\ndataset. Especially, this dataset includes the results of GAN-based methods,\nwhich are missing in previous datasets. We collect more than 1.13 million human\njudgments to assign subjective scores for PIPAL images using the more reliable\n\"Elo system\". Based on PIPAL, we present new benchmarks for both IQA and\nsuper-resolution methods. Our results indicate that existing IQA methods cannot\nfairly evaluate GAN-based IR algorithms. While using appropriate evaluation\nmethods is important, IQA methods should also be updated along with the\ndevelopment of IR algorithms. At last, we improve the performance of IQA\nnetworks on GAN-based distortions by introducing anti-aliasing pooling.\nExperiments show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:15:25 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 08:30:28 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gu", "Jinjin", ""], ["Cai", "Haoming", ""], ["Chen", "Haoyu", ""], ["Ye", "Xiaoxing", ""], ["Ren", "Jimmy", ""], ["Dong", "Chao", ""]]}, {"id": "2007.12146", "submitter": "Yash Kant", "authors": "Yash Kant, Dhruv Batra, Peter Anderson, Alex Schwing, Devi Parikh,\n  Jiasen Lu, Harsh Agrawal", "title": "Spatially Aware Multimodal Transformers for TextVQA", "comments": "Accepted at European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual cues are essential for everyday tasks like buying groceries and using\npublic transport. To develop this assistive technology, we study the TextVQA\ntask, i.e., reasoning about text in images to answer a question. Existing\napproaches are limited in their use of spatial relations and rely on\nfully-connected transformer-like architectures to implicitly learn the spatial\nstructure of a scene. In contrast, we propose a novel spatially aware\nself-attention layer such that each visual entity only looks at neighboring\nentities defined by a spatial graph. Further, each head in our multi-head\nself-attention layer focuses on a different subset of relations. Our approach\nhas two advantages: (1) each head considers local context instead of dispersing\nthe attention amongst all visual entities; (2) we avoid learning redundant\nfeatures. We show that our model improves the absolute accuracy of current\nstate-of-the-art methods on TextVQA by 2.2% overall over an improved baseline,\nand 4.62% on questions that involve spatial reasoning and can be answered\ncorrectly using OCR tokens. Similarly on ST-VQA, we improve the absolute\naccuracy by 4.2%. We further show that spatially aware self-attention improves\nvisual grounding.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:20:55 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 03:10:07 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kant", "Yash", ""], ["Batra", "Dhruv", ""], ["Anderson", "Peter", ""], ["Schwing", "Alex", ""], ["Parikh", "Devi", ""], ["Lu", "Jiasen", ""], ["Agrawal", "Harsh", ""]]}, {"id": "2007.12147", "submitter": "Hang Xu", "authors": "Hang Xu, Shaoju Wang, Xinyue Cai, Wei Zhang, Xiaodan Liang, Zhenguo Li", "title": "CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive\n  Point Blending", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the curve lane detection problem which poses more realistic\nchallenges than conventional lane detection for better facilitating modern\nassisted/autonomous driving systems. Current hand-designed lane detection\nmethods are not robust enough to capture the curve lanes especially the remote\nparts due to the lack of modeling both long-range contextual information and\ndetailed curve trajectory. In this paper, we propose a novel lane-sensitive\narchitecture search framework named CurveLane-NAS to automatically capture both\nlong-ranged coherent and accurate short-range curve information while unifying\nboth architecture search and post-processing on curve lane predictions via\npoint blending. It consists of three search modules: a) a feature fusion search\nmodule to find a better fusion of the local and global context for multi-level\nhierarchy features; b) an elastic backbone search module to explore an\nefficient feature extractor with good semantics and latency; c) an adaptive\npoint blending module to search a multi-level post-processing refinement\nstrategy to combine multi-scale head prediction. The unified framework ensures\nlane-sensitive predictions by the mutual guidance between NAS and adaptive\npoint blending. Furthermore, we also steer forward to release a more\nchallenging benchmark named CurveLanes for addressing the most difficult curve\nlanes. It consists of 150K images with 680K labels.The new dataset can be\ndownloaded at github.com/xbjxh/CurveLanes (already anonymized for this\nsubmission). Experiments on the new CurveLanes show that the SOTA lane\ndetection methods suffer substantial performance drop while our model can still\nreach an 80+% F1-score. Extensive experiments on traditional lane benchmarks\nsuch as CULane also demonstrate the superiority of our CurveLane-NAS, e.g.\nachieving a new SOTA 74.8% F1-score on CULane.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:23:26 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Xu", "Hang", ""], ["Wang", "Shaoju", ""], ["Cai", "Xinyue", ""], ["Zhang", "Wei", ""], ["Liang", "Xiaodan", ""], ["Li", "Zhenguo", ""]]}, {"id": "2007.12148", "submitter": "Laura Zheng", "authors": "Shivam Akhauri, Laura Zheng, Ming Lin", "title": "Enhanced Transfer Learning for Autonomous Driving with Systematic\n  Accident Simulation", "comments": "9 pages; IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation data can be utilized to extend real-world driving data in order to\ncover edge cases, such as vehicle accidents. The importance of handling edge\ncases can be observed in the high societal costs in handling car accidents, as\nwell as potential dangers to human drivers. In order to cover a wide and\ndiverse range of all edge cases, we systemically parameterize and simulate the\nmost common accident scenarios. By applying this data to autonomous driving\nmodels, we show that transfer learning on simulated data sets provide better\ngeneralization and collision avoidance, as compared to random initialization\nmethods. Our results illustrate that information from a model trained on\nsimulated data can be inferred to a model trained on real-world data,\nindicating the potential influence of simulation data in real world models and\nadvancements in handling of anomalous driving scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:27:00 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Akhauri", "Shivam", ""], ["Zheng", "Laura", ""], ["Lin", "Ming", ""]]}, {"id": "2007.12163", "submitter": "Andrew Brown", "authors": "Andrew Brown, Weidi Xie, Vicky Kalogeiton, Andrew Zisserman", "title": "Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimising a ranking-based metric, such as Average Precision (AP), is\nnotoriously challenging due to the fact that it is non-differentiable, and\nhence cannot be optimised directly using gradient-descent methods. To this end,\nwe introduce an objective that optimises instead a smoothed approximation of\nAP, coined Smooth-AP. Smooth-AP is a plug-and-play objective function that\nallows for end-to-end training of deep networks with a simple and elegant\nimplementation. We also present an analysis for why directly optimising the\nranking based metric of AP offers benefits over other deep metric learning\nlosses. We apply Smooth-AP to standard retrieval benchmarks: Stanford Online\nproducts and VehicleID, and also evaluate on larger-scale datasets: INaturalist\nfor fine-grained category retrieval, and VGGFace2 and IJB-C for face retrieval.\nIn all cases, we improve the performance over the state-of-the-art, especially\nfor larger-scale datasets, thus demonstrating the effectiveness and scalability\nof Smooth-AP to real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:52:03 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 18:02:12 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Brown", "Andrew", ""], ["Xie", "Weidi", ""], ["Kalogeiton", "Vicky", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2007.12173", "submitter": "Unnat Jain", "authors": "Luca Weihs, Unnat Jain, Jordi Salvador, Svetlana Lazebnik, Aniruddha\n  Kembhavi, Alexander Schwing", "title": "Bridging the Imitation Gap by Adaptive Insubordination", "comments": "The first two authors contributed equally. Project page:\n  https://unnat.github.io/advisor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When expert supervision is available, practitioners often use imitation\nlearning with varying degrees of success. We show that when an expert has\naccess to privileged information that is unavailable to the student, this\ninformation is marginalized in the student policy during imitation learning\nresulting in an \"imitation gap\" and, potentially, poor results. Prior work\nbridges this gap via a progression from imitation learning to reinforcement\nlearning. While often successful, gradual progression fails for tasks that\nrequire frequent switches between exploration and memorization skills. To\nbetter address these tasks and alleviate the imitation gap we propose 'Adaptive\nInsubordination' (ADVISOR), which dynamically weights imitation and\nreward-based reinforcement learning losses during training, enabling switching\nbetween imitation and exploration. On a suite of challenging didactic and\nMiniGrid tasks, we show that ADVISOR outperforms pure imitation, pure\nreinforcement learning, as well as their sequential and parallel combinations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:59:57 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:48:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Weihs", "Luca", ""], ["Jain", "Unnat", ""], ["Salvador", "Jordi", ""], ["Lazebnik", "Svetlana", ""], ["Kembhavi", "Aniruddha", ""], ["Schwing", "Alexander", ""]]}, {"id": "2007.12211", "submitter": "Jing Zhang", "authors": "Jing Zhang, Jianwen Xie, Nick Barnes", "title": "Learning Noise-Aware Encoder-Decoder from Noisy Labels by Alternating\n  Back-Propagation for Saliency Detection", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a noise-aware encoder-decoder framework to\ndisentangle a clean saliency predictor from noisy training examples, where the\nnoisy labels are generated by unsupervised handcrafted feature-based methods.\nThe proposed model consists of two sub-models parameterized by neural networks:\n(1) a saliency predictor that maps input images to clean saliency maps, and (2)\na noise generator, which is a latent variable model that produces noises from\nGaussian latent vectors. The whole model that represents noisy labels is a sum\nof the two sub-models. The goal of training the model is to estimate the\nparameters of both sub-models, and simultaneously infer the corresponding\nlatent vector of each noisy label. We propose to train the model by using an\nalternating back-propagation (ABP) algorithm, which alternates the following\ntwo steps: (1) learning back-propagation for estimating the parameters of two\nsub-models by gradient ascent, and (2) inferential back-propagation for\ninferring the latent vectors of training noisy examples by Langevin Dynamics.\nTo prevent the network from converging to trivial solutions, we utilize an\nedge-aware smoothness loss to regularize hidden saliency maps to have similar\nstructures as their corresponding images. Experimental results on several\nbenchmark datasets indicate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 18:47:36 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhang", "Jing", ""], ["Xie", "Jianwen", ""], ["Barnes", "Nick", ""]]}, {"id": "2007.12212", "submitter": "Anurag Roy", "authors": "Anurag Roy, Vinay Kumar Verma, Kripabandhu Ghosh, Saptarshi Ghosh", "title": "ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot\n  Retrieval of Images from Textual Descriptions", "comments": "Accepted in CIKM-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing algorithms for cross-modal Information Retrieval are based on a\nsupervised train-test setup, where a model learns to align the mode of the\nquery (e.g., text) to the mode of the documents (e.g., images) from a given\ntraining set. Such a setup assumes that the training set contains an exhaustive\nrepresentation of all possible classes of queries. In reality, a retrieval\nmodel may need to be deployed on previously unseen classes, which implies a\nzero-shot IR setup. In this paper, we propose a novel GAN-based model for\nzero-shot text to image retrieval. When given a textual description as the\nquery, our model can retrieve relevant images in a zero-shot setup. The\nproposed model is trained using an Expectation-Maximization framework.\nExperiments on multiple benchmark datasets show that our proposed model\ncomfortably outperforms several state-of-the-art zero-shot text to image\nretrieval models, as well as zero-shot classification and hashing models\nsuitably used for retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 18:50:03 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 11:57:35 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 11:41:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Roy", "Anurag", ""], ["Verma", "Vinay Kumar", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2007.12229", "submitter": "Luis Felipe Muller De Oliveira Henriques", "authors": "Ruy Luiz Milidi\\'u and Luis Felipe M\\\"uller", "title": "SeismoFlow -- Data augmentation for the class imbalance problem", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several application areas, such as medical diagnosis, spam filtering,\nfraud detection, and seismic data analysis, it is very usual to find relevant\nclassification tasks where some class occurrences are rare. This is the so\ncalled class imbalance problem, which is a challenge in machine learning. In\nthis work, we propose the SeismoFlow a flow-based generative model to create\nsynthetic samples, aiming to address the class imbalance. Inspired by the Glow\nmodel, it uses interpolation on the learned latent space to produce synthetic\nsamples for one rare class. We apply our approach to the development of a\nseismogram signal quality classifier. We introduce a dataset composed\nof5.223seismograms that are distributed between the good, medium, and bad\nclasses and with their respective frequencies of 66.68%,31.54%, and 1.76%. Our\nmethodology is evaluated on a stratified 10-fold cross-validation setting,\nusing the Miniceptionmodel as a baseline, and assessing the effects of adding\nthe generated samples on the training set of each iteration. In our\nexperiments, we achieve an improvement of 13.9% on the rare class F1-score,\nwhile not hurting the metric value for the other classes and thus observing the\noverall accuracy improvement. Our empirical findings indicate that our method\ncan generate high-quality synthetic seismograms with realistic looking and\nsufficient plurality to help the Miniception model to overcome the class\nimbalance problem. We believe that our results are a step forward in solving\nboth the task of seismogram signal quality classification and class imbalance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 19:48:23 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 14:42:37 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Milidi\u00fa", "Ruy Luiz", ""], ["M\u00fcller", "Luis Felipe", ""]]}, {"id": "2007.12248", "submitter": "Eric Chu", "authors": "Eric Chu, Deb Roy, Jacob Andreas", "title": "Are Visual Explanations Useful? A Case Study in Model-in-the-Loop\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized controlled trial for a model-in-the-loop regression\ntask, with the goal of measuring the extent to which (1) good explanations of\nmodel predictions increase human accuracy, and (2) faulty explanations decrease\nhuman trust in the model. We study explanations based on visual saliency in an\nimage-based age prediction task for which humans and learned models are\nindividually capable but not highly proficient and frequently disagree. Our\nexperimental design separates model quality from explanation quality, and makes\nit possible to compare treatments involving a variety of explanations of\nvarying levels of quality. We find that presenting model predictions improves\nhuman accuracy. However, visual explanations of various kinds fail to\nsignificantly alter human accuracy or trust in the model - regardless of\nwhether explanations characterize an accurate model, an inaccurate one, or are\ngenerated randomly and independently of the input image. These findings suggest\nthe need for greater evaluation of explanations in downstream decision making\ntasks, better design-based tools for presenting explanations to users, and\nbetter approaches for generating explanations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:39:40 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Chu", "Eric", ""], ["Roy", "Deb", ""], ["Andreas", "Jacob", ""]]}, {"id": "2007.12256", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Zeynep Akata, Elisa Ricci, Barbara Caputo", "title": "Towards Recognizing Unseen Categories in Unseen Domains", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep visual recognition systems suffer from severe performance\ndegradation when they encounter new images from classes and scenarios unseen\nduring training. Hence, the core challenge of Zero-Shot Learning (ZSL) is to\ncope with the semantic-shift whereas the main challenge of Domain Adaptation\nand Domain Generalization (DG) is the domain-shift. While historically ZSL and\nDG tasks are tackled in isolation, this work develops with the ambitious goal\nof solving them jointly,i.e. by recognizing unseen visual concepts in unseen\ndomains. We presentCuMix (CurriculumMixup for recognizing unseen categories in\nunseen domains), a holistic algorithm to tackle ZSL, DG and ZSL+DG. The key\nidea of CuMix is to simulate the test-time domain and semantic shift using\nimages and features from unseen domains and categories generated by mixing up\nthe multiple source domains and categories available during training. Moreover,\na curriculum-based mixing policy is devised to generate increasingly complex\ntraining samples. Results on standard SL and DG datasets and on ZSL+DG using\nthe DomainNet benchmark demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 21:09:28 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 07:48:25 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Akata", "Zeynep", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""]]}, {"id": "2007.12287", "submitter": "Evonne Ng", "authors": "Evonne Ng, Shiry Ginosar, Trevor Darrell, Hanbyul Joo", "title": "Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learned deep prior of body motion for 3D hand shape\nsynthesis and estimation in the domain of conversational gestures. Our model\nbuilds upon the insight that body motion and hand gestures are strongly\ncorrelated in non-verbal communication settings. We formulate the learning of\nthis prior as a prediction task of 3D hand shape over time given body motion\ninput alone. Trained with 3D pose estimations obtained from a large-scale\ndataset of internet videos, our hand prediction model produces convincing 3D\nhand gestures given only the 3D motion of the speaker's arms as input. We\ndemonstrate the efficacy of our method on hand gesture synthesis from body\nmotion input, and as a strong body prior for single-view image-based 3D hand\npose estimation. We demonstrate that our method outperforms previous\nstate-of-the-art approaches and can generalize beyond the monologue-based\ntraining data to multi-person conversations. Video results are available at\nhttp://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 22:58:15 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:13:34 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 15:13:12 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ng", "Evonne", ""], ["Ginosar", "Shiry", ""], ["Darrell", "Trevor", ""], ["Joo", "Hanbyul", ""]]}, {"id": "2007.12296", "submitter": "Shane Sims", "authors": "Shane D. Sims", "title": "Frequency Domain-based Perceptual Loss for Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Frequency Domain Perceptual Loss (FDPL), a loss function for\nsingle image super resolution (SR). Unlike previous loss functions used to\ntrain SR models, which are all calculated in the pixel (spatial) domain, FDPL\nis computed in the frequency domain. By working in the frequency domain we can\nencourage a given model to learn a mapping that prioritizes those frequencies\nmost related to human perception. While the goal of FDPL is not to maximize the\nPeak Signal to Noise Ratio (PSNR), we found that there is a correlation between\ndecreasing FDPL and increasing PSNR. Training a model with FDPL results in a\nhigher average PSRN (30.94), compared to the same model trained with pixel loss\n(30.59), as measured on the Set5 image dataset. We also show that our method\nachieves higher qualitative results, which is the goal of a perceptual loss\nfunction. However, it is not clear that the improved perceptual quality is due\nto the slightly higher PSNR or the perceptual nature of FDPL.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 23:35:22 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Sims", "Shane D.", ""]]}, {"id": "2007.12303", "submitter": "Shervin Minaee", "authors": "Narges Saeedizadeh, Shervin Minaee, Rahele Kafieh, Shakib Yazdani,\n  Milan Sonka", "title": "COVID TV-UNet: Segmenting COVID-19 Chest CT Images Using Connectivity\n  Imposed U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel corona-virus disease (COVID-19) pandemic has caused a major\noutbreak in more than 200 countries around the world, leading to a severe\nimpact on the health and life of many people globally. As of mid-July 2020,\nmore than 12 million people were infected, and more than 570,000 death were\nreported. Computed Tomography (CT) images can be used as an alternative to the\ntime-consuming RT-PCR test, to detect COVID-19. In this work we propose a\nsegmentation framework to detect chest regions in CT images, which are infected\nby COVID-19. We use an architecture similar to U-Net model, and train it to\ndetect ground glass regions, on pixel level. As the infected regions tend to\nform a connected component (rather than randomly distributed pixels), we add a\nsuitable regularization term to the loss function, to promote connectivity of\nthe segmentation map for COVID-19 pixels. 2D-anisotropic total-variation is\nused for this purpose, and therefore the proposed model is called \"TV-UNet\".\nThrough experimental results on a relatively large-scale CT segmentation\ndataset of around 900 images, we show that adding this new regularization term\nleads to 2\\% gain on overall segmentation performance compared to the U-Net\nmodel. Our experimental analysis, ranging from visual evaluation of the\npredicted segmentation results to quantitative assessment of segmentation\nperformance (precision, recall, Dice score, and mIoU) demonstrated great\nability to identify COVID-19 associated regions of the lungs, achieving a mIoU\nrate of over 99\\%, and a Dice score of around 86\\%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 00:19:21 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 23:00:35 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 22:59:04 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Saeedizadeh", "Narges", ""], ["Minaee", "Shervin", ""], ["Kafieh", "Rahele", ""], ["Yazdani", "Shakib", ""], ["Sonka", "Milan", ""]]}, {"id": "2007.12326", "submitter": "Lingyi Liu", "authors": "Lingyi Liu, Yunpeng Bai, and Ying Li", "title": "Locality-Aware Rotated Ship Detection in High-Resolution Remote Sensing\n  Imagery Based on Multi-Scale Convolutional Network", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ship detection has been an active and vital topic in the field of remote\nsensing for a decade, but it is still a challenging problem due to the large\nscale variations, the high aspect ratios, the intensive arrangement, and the\nbackground clutter disturbance. In this letter, we propose a locality-aware\nrotated ship detection (LARSD) framework based on a multi-scale convolutional\nneural network (CNN) to tackle these issues. The proposed framework applies a\nUNet-like multi-scale CNN to generate multi-scale feature maps with high-level\nsemantic information in high resolution. Then, a rotated anchor-based\nregression is applied for directly predicting the probability, the edge\ndistances, and the angle of ships. Finally, a locality-aware score alignment is\nproposed to fix the mismatch between classification results and location\nresults caused by the independence of each subnet. Furthermore, to enlarge the\ndatasets of ship detection, we build a new high-resolution ship detection\n(HRSD) dataset, where 2499 images and 9269 instances were collected from Google\nEarth with different resolutions. Experiments based on public dataset HRSC2016\nand our HRSD dataset demonstrate that our detection method achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:01:42 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liu", "Lingyi", ""], ["Bai", "Yunpeng", ""], ["Li", "Ying", ""]]}, {"id": "2007.12342", "submitter": "Yuanhan Zhang", "authors": "Yuanhan Zhang, Zhenfei Yin, Yidong Li, Guojun Yin, Junjie Yan, Jing\n  Shao, and Ziwei Liu", "title": "CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich\n  Annotations", "comments": "To appear in ECCV 2020. Dataset is available at:\n  https://github.com/Davidzhangyuanhan/CelebA-Spoof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As facial interaction systems are prevalently deployed, security and\nreliability of these systems become a critical issue, with substantial research\nefforts devoted. Among them, face anti-spoofing emerges as an important area,\nwhose objective is to identify whether a presented face is live or spoof.\nThough promising progress has been achieved, existing works still have\ndifficulty in handling complex spoof attacks and generalizing to real-world\nscenarios. The main reason is that current face anti-spoofing datasets are\nlimited in both quantity and diversity. To overcome these obstacles, we\ncontribute a large-scale face anti-spoofing dataset, CelebA-Spoof, with the\nfollowing appealing properties: 1) Quantity: CelebA-Spoof comprises of 625,537\npictures of 10,177 subjects, significantly larger than the existing datasets.\n2) Diversity: The spoof images are captured from 8 scenes (2 environments * 4\nillumination conditions) with more than 10 sensors. 3) Annotation Richness:\nCelebA-Spoof contains 10 spoof type annotations, as well as the 40 attribute\nannotations inherited from the original CelebA dataset. Equipped with\nCelebA-Spoof, we carefully benchmark existing methods in a unified multi-task\nframework, Auxiliary Information Embedding Network (AENet), and reveal several\nvaluable observations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 04:28:29 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:52:18 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 07:16:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhang", "Yuanhan", ""], ["Yin", "Zhenfei", ""], ["Li", "Yidong", ""], ["Yin", "Guojun", ""], ["Yan", "Junjie", ""], ["Shao", "Jing", ""], ["Liu", "Ziwei", ""]]}, {"id": "2007.12348", "submitter": "Yilun Du", "authors": "Yilun Du, Kevin Smith, Tomer Ulman, Joshua Tenenbaum, Jiajun Wu", "title": "Unsupervised Discovery of 3D Physical Objects from Video", "comments": "ICLR 2021; project webpage at http://yilundu.github.io/podnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of unsupervised physical object discovery. While\nexisting frameworks aim to decompose scenes into 2D segments based off each\nobject's appearance, we explore how physics, especially object interactions,\nfacilitates disentangling of 3D geometry and position of objects from video, in\nan unsupervised manner. Drawing inspiration from developmental psychology, our\nPhysical Object Discovery Network (POD-Net) uses both multi-scale pixel cues\nand physical motion cues to accurately segment observable and partially\noccluded objects of varying sizes, and infer properties of those objects. Our\nmodel reliably segments objects on both synthetic and real scenes. The\ndiscovered object properties can also be used to reason about physical events.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 04:46:21 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:06:37 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 02:03:08 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Du", "Yilun", ""], ["Smith", "Kevin", ""], ["Ulman", "Tomer", ""], ["Tenenbaum", "Joshua", ""], ["Wu", "Jiajun", ""]]}, {"id": "2007.12360", "submitter": "Silvia Bucci", "authors": "Silvia Bucci, Mohammad Reza Loghmani, Tatiana Tommasi", "title": "On the Effectiveness of Image Rotation for Open Set Domain Adaptation", "comments": "accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Set Domain Adaptation (OSDA) bridges the domain gap between a labeled\nsource domain and an unlabeled target domain, while also rejecting target\nclasses that are not present in the source. To avoid negative transfer, OSDA\ncan be tackled by first separating the known/unknown target samples and then\naligning known target samples with the source data. We propose a novel method\nto addresses both these problems using the self-supervised task of rotation\nrecognition. Moreover, we assess the performance with a new open set metric\nthat properly balances the contribution of recognizing the known classes and\nrejecting the unknown samples. Comparative experiments with existing OSDA\nmethods on the standard Office-31 and Office-Home benchmarks show that: (i) our\nmethod outperforms its competitors, (ii) reproducibility for this field is a\ncrucial issue to tackle, (iii) our metric provides a reliable tool to allow\nfair open set evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:54:07 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Bucci", "Silvia", ""], ["Loghmani", "Mohammad Reza", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2007.12362", "submitter": "Sridhar K V", "authors": "K.V. Sridhar and Raghu vamshi Hemadri", "title": "Performance analysis of weighted low rank model with sparse image\n  histograms for face recognition under lowlevel illumination and occlusion", "comments": "12 pages, 8 figres, 4 Tables, International conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a broad range of computer vision applications, the purpose of Low-rank\nmatrix approximation (LRMA) models is to recover the underlying low-rank matrix\nfrom its degraded observation. The latest LRMA methods - Robust Principal\nComponent Analysis (RPCA) resort to using the nuclear norm minimization (NNM)\nas a convex relaxation of the non-convex rank minimization. However, NNM tends\nto over-shrink the rank components and treats the different rank components\nequally, limiting its flexibility in practical applications. We use a more\nflexible model, namely the Weighted Schatten p-Norm Minimization (WSNM), to\ngeneralize the NNM to the Schatten p-norm minimization with weights assigned to\ndifferent singular values. The proposed WSNM not only gives a better\napproximation to the original low-rank assumption but also considers the\nimportance of different rank components. In this paper, a comparison of the\nlow-rank recovery performance of two LRMA algorithms- RPCA and WSNM is brought\nout on occluded human facial images. The analysis is performed on facial images\nfrom the Yale database and over own database , where different facial\nexpressions, spectacles, varying illumination account for the facial\nocclusions. The paper also discusses the prominent trends observed from the\nexperimental results performed through the application of these algorithms. As\nlow-rank images sometimes might fail to capture the details of a face\nadequately, we further propose a novel method to use the image-histogram of the\nsparse images thus obtained to identify the individual in any given image.\nExtensive experimental results show, both qualitatively and quantitatively,\nthat WSNM surpasses RPCA in its performance more effectively by removing facial\nocclusions, thus giving recovered low-rank images of higher PSNR and SSIM.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:59:28 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Sridhar", "K. V.", ""], ["Hemadri", "Raghu vamshi", ""]]}, {"id": "2007.12368", "submitter": "Silvia Bucci", "authors": "Silvia Bucci, Antonio D'Innocente, Yujun Liao, Fabio Maria Carlucci,\n  Barbara Caputo, Tatiana Tommasi", "title": "Self-Supervised Learning Across Domains", "comments": "Accepted at IEEE T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human adaptability relies crucially on learning and merging knowledge from\nboth supervised and unsupervised tasks: the parents point out few important\nconcepts, but then the children fill in the gaps on their own. This is\nparticularly effective, because supervised learning can never be exhaustive and\nthus learning autonomously allows to discover invariances and regularities that\nhelp to generalize. In this paper we propose to apply a similar approach to the\nproblem of object recognition across domains: our model learns the semantic\nlabels in a supervised fashion, and broadens its understanding of the data by\nlearning from self-supervised signals on the same images. This secondary task\nhelps the network to focus on object shapes, learning concepts like spatial\norientation and part correlation, while acting as a regularizer for the\nclassification task over multiple visual domains. Extensive experiments confirm\nour intuition and show that our multi-task method combining supervised and\nself-supervised knowledge shows competitive results with respect to more\ncomplex domain generalization and adaptation solutions. It also proves its\npotential in the novel and challenging predictive and partial domain adaptation\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:19:53 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 13:51:53 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bucci", "Silvia", ""], ["D'Innocente", "Antonio", ""], ["Liao", "Yujun", ""], ["Carlucci", "Fabio Maria", ""], ["Caputo", "Barbara", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2007.12387", "submitter": "Qi Fan", "authors": "Qi Fan, Lei Ke, Wenjie Pei, Chi-Keung Tang, Yu-Wing Tai", "title": "Commonality-Parsing Network across Shape and Appearance for Partially\n  Supervised Instance Segmentation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Partially supervised instance segmentation aims to perform learning on\nlimited mask-annotated categories of data thus eliminating expensive and\nexhaustive mask annotation. The learned models are expected to be generalizable\nto novel categories. Existing methods either learn a transfer function from\ndetection to segmentation, or cluster shape priors for segmenting novel\ncategories. We propose to learn the underlying class-agnostic commonalities\nthat can be generalized from mask-annotated categories to novel categories.\nSpecifically, we parse two types of commonalities: 1) shape commonalities which\nare learned by performing supervised learning on instance boundary prediction;\nand 2) appearance commonalities which are captured by modeling pairwise\naffinities among pixels of feature maps to optimize the separability between\ninstance and the background. Incorporating both the shape and appearance\ncommonalities, our model significantly outperforms the state-of-the-art methods\non both partially supervised setting and few-shot setting for instance\nsegmentation on COCO dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:23:44 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Fan", "Qi", ""], ["Ke", "Lei", ""], ["Pei", "Wenjie", ""], ["Tang", "Chi-Keung", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2007.12391", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai and David Bull", "title": "Artificial Intelligence in the Creative Industries: A Review", "comments": null, "journal-ref": "Artif Intell Rev (2021) 1-68", "doi": "10.1007/s10462-021-10039-7", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:29:52 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 17:04:03 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 20:30:14 GMT"}, {"version": "v4", "created": "Tue, 2 Mar 2021 16:03:20 GMT"}, {"version": "v5", "created": "Sat, 19 Jun 2021 20:30:57 GMT"}, {"version": "v6", "created": "Fri, 2 Jul 2021 11:35:08 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Bull", "David", ""]]}, {"id": "2007.12392", "submitter": "Rui Huang", "authors": "Rui Huang, Wanyue Zhang, Abhijit Kundu, Caroline Pantofaru, David A\n  Ross, Thomas Funkhouser, Alireza Fathi", "title": "An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects in 3D LiDAR data is a core technology for autonomous\ndriving and other robotics applications. Although LiDAR data is acquired over\ntime, most of the 3D object detection algorithms propose object bounding boxes\nindependently for each frame and neglect the useful information available in\nthe temporal domain. To address this problem, in this paper we propose a sparse\nLSTM-based multi-frame 3d object detection algorithm. We use a U-Net style 3D\nsparse convolution network to extract features for each frame's LiDAR\npoint-cloud. These features are fed to the LSTM module together with the hidden\nand memory features from last frame to predict the 3d objects in the current\nframe as well as hidden and memory features that are passed to the next frame.\nExperiments on the Waymo Open Dataset show that our algorithm outperforms the\ntraditional frame by frame approach by 7.5% mAP@0.7 and other multi-frame\napproaches by 1.2% while using less memory and computation per frame. To the\nbest of our knowledge, this is the first work to use an LSTM for 3D object\ndetection in sparse point clouds.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:34:15 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Huang", "Rui", ""], ["Zhang", "Wanyue", ""], ["Kundu", "Abhijit", ""], ["Pantofaru", "Caroline", ""], ["Ross", "David A", ""], ["Funkhouser", "Thomas", ""], ["Fathi", "Alireza", ""]]}, {"id": "2007.12402", "submitter": "Ka Leong Cheng", "authors": "Ka Leong Cheng, Zhaoyang Yang, Qifeng Chen, Yu-Wing Tai", "title": "Fully Convolutional Networks for Continuous Sign Language Recognition", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": "10.1007/978-3-030-58586-0_41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous sign language recognition (SLR) is a challenging task that\nrequires learning on both spatial and temporal dimensions of signing frame\nsequences. Most recent work accomplishes this by using CNN and RNN hybrid\nnetworks. However, training these networks is generally non-trivial, and most\nof them fail in learning unseen sequence patterns, causing an unsatisfactory\nperformance for online recognition. In this paper, we propose a fully\nconvolutional network (FCN) for online SLR to concurrently learn spatial and\ntemporal features from weakly annotated video sequences with only\nsentence-level annotations given. A gloss feature enhancement (GFE) module is\nintroduced in the proposed network to enforce better sequence alignment\nlearning. The proposed network is end-to-end trainable without any\npre-training. We conduct experiments on two large scale SLR datasets.\nExperiments show that our method for continuous SLR is effective and performs\nwell in online recognition.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 08:16:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cheng", "Ka Leong", ""], ["Yang", "Zhaoyang", ""], ["Chen", "Qifeng", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2007.12407", "submitter": "Zhi Hou", "authors": "Zhi Hou, Xiaojiang Peng, Yu Qiao, Dacheng Tao", "title": "Visual Compositional Learning for Human-Object Interaction Detection", "comments": "Accepted in ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object interaction (HOI) detection aims to localize and infer\nrelationships between human and objects in an image. It is challenging because\nan enormous number of possible combinations of objects and verbs types forms a\nlong-tail distribution. We devise a deep Visual Compositional Learning (VCL)\nframework, which is a simple yet efficient framework to effectively address\nthis problem. VCL first decomposes an HOI representation into object and verb\nspecific features, and then composes new interaction samples in the feature\nspace via stitching the decomposed features. The integration of decomposition\nand composition enables VCL to share object and verb features among different\nHOI samples and images, and to generate new interaction samples and new types\nof HOI, and thus largely alleviates the long-tail distribution problem and\nbenefits low-shot or zero-shot HOI detection. Extensive experiments demonstrate\nthat the proposed VCL can effectively improve the generalization of HOI\ndetection on HICO-DET and V-COCO and outperforms the recent state-of-the-art\nmethods on HICO-DET. Code is available at https://github.com/zhihou7/VCL.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 08:37:40 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 12:47:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hou", "Zhi", ""], ["Peng", "Xiaojiang", ""], ["Qiao", "Yu", ""], ["Tao", "Dacheng", ""]]}, {"id": "2007.12411", "submitter": "Yingzhen Li", "authors": "Chaochao Lu, Richard E. Turner, Yingzhen Li, Nate Kushman", "title": "Interpreting Spatially Infinite Generative Models", "comments": "ICML 2020 workshop on Human Interpretability in Machine Learning (WHI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deep generative models of images and other spatial modalities can\nonly generate fixed sized outputs. The generated images have exactly the same\nresolution as the training images, which is dictated by the number of layers in\nthe underlying neural network. Recent work has shown, however, that feeding\nspatial noise vectors into a fully convolutional neural network enables both\ngeneration of arbitrary resolution output images as well as training on\narbitrary resolution training images. While this work has provided impressive\nempirical results, little theoretical interpretation was provided to explain\nthe underlying generative process. In this paper we provide a firm theoretical\ninterpretation for infinite spatial generation, by drawing connections to\nspatial stochastic processes. We use the resulting intuition to improve upon\nexisting spatially infinite generative models to enable more efficient training\nthrough a model that we call an infinite generative adversarial network, or\n$\\infty$-GAN. Experiments on world map generation, panoramic images and texture\nsynthesis verify the ability of $\\infty$-GAN to efficiently generate images of\narbitrary size.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:00:41 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Lu", "Chaochao", ""], ["Turner", "Richard E.", ""], ["Li", "Yingzhen", ""], ["Kushman", "Nate", ""]]}, {"id": "2007.12415", "submitter": "Xi Li", "authors": "Hanbin Zhao, Hao Zeng, Xin Qin, Yongjian Fu, Hui Wang, Bourahla Omar,\n  and Xi Li", "title": "What and Where: Learn to Plug Adapters via NAS for Multi-Domain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem, multi-domain learning (MDL)\ntypically seeks for a set of effective lightweight domain-specific adapter\nmodules plugged into a common domain-agnostic network. Usually, existing ways\nof adapter plugging and structure design are handcrafted and fixed for all\ndomains before model learning, resulting in the learning inflexibility and\ncomputational intensiveness. With this motivation, we propose to learn a\ndata-driven adapter plugging strategy with Neural Architecture Search (NAS),\nwhich automatically determines where to plug for those adapter modules.\nFurthermore, we propose a NAS-adapter module for adapter structure design in a\nNAS-driven learning scheme, which automatically discovers effective adapter\nmodule structures for different domains. Experimental results demonstrate the\neffectiveness of our MDL model against existing approaches under the conditions\nof comparable performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:12:37 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 13:54:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhao", "Hanbin", ""], ["Zeng", "Hao", ""], ["Qin", "Xin", ""], ["Fu", "Yongjian", ""], ["Wang", "Hui", ""], ["Omar", "Bourahla", ""], ["Li", "Xi", ""]]}, {"id": "2007.12421", "submitter": "Khanh Tran Mr", "authors": "Thuong-Khanh Tran, Quang-Nhat Vo, Xiaopeng Hong, Xiaobai Li and\n  Guoying Zhao", "title": "Micro-expression spotting: A new benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expressions (MEs) are brief and involuntary facial expressions that\noccur when people are trying to hide their true feelings or conceal their\nemotions. Based on psychology research, MEs play an important role in\nunderstanding genuine emotions, which leads to many potential applications.\nTherefore, ME analysis has become an attractive topic for various research\nareas, such as psychology, law enforcement, and psychotherapy. In the computer\nvision field, the study of MEs can be divided into two main tasks, spotting and\nrecognition, which are used to identify positions of MEs in videos and\ndetermine the emotion category of the detected MEs, respectively. Recently,\nalthough much research has been done, no fully automatic system for analyzing\nMEs has yet been constructed on a practical level for two main reasons: most of\nthe research on MEs only focuses on the recognition part, while abandoning the\nspotting task; current public datasets for ME spotting are not challenging\nenough to support developing a robust spotting algorithm. The contributions of\nthis paper are threefold: (1) we introduce an extension of the SMIC-E database,\nnamely the SMIC-E-Long database, which is a new challenging benchmark for ME\nspotting; (2) we suggest a new evaluation protocol that standardizes the\ncomparison of various ME spotting techniques; (3) extensive experiments with\nhandcrafted and deep learning-based approaches on the SMIC-E-Long database are\nperformed for baseline evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 09:18:41 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 14:06:39 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tran", "Thuong-Khanh", ""], ["Vo", "Quang-Nhat", ""], ["Hong", "Xiaopeng", ""], ["Li", "Xiaobai", ""], ["Zhao", "Guoying", ""]]}, {"id": "2007.12449", "submitter": "Luyan Liu", "authors": "Luyan Liu, Kai Ma, Yefeng Zheng", "title": "Learning Crisp Edge Detector Using Logical Refinement Network", "comments": "Accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is a fundamental problem in different computer vision tasks.\nRecently, edge detection algorithms achieve satisfying improvement built upon\ndeep learning. Although most of them report favorable evaluation scores, they\noften fail to accurately localize edges and give thick and blurry boundaries.\nIn addition, most of them focus on 2D images and the challenging 3D edge\ndetection is still under-explored. In this work, we propose a novel logical\nrefinement network for crisp edge detection, which is motivated by the logical\nrelationship between segmentation and edge maps and can be applied to both 2D\nand 3D images. The network consists of a joint object and edge detection\nnetwork and a crisp edge refinement network, which predicts more accurate,\nclearer and thinner high quality binary edge maps without any post-processing.\nExtensive experiments are conducted on the 2D nuclei images from Kaggle 2018\nData Science Bowl and a private 3D microscopy images of a monkey brain, which\nshow outstanding performance compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 11:12:48 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liu", "Luyan", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.12450", "submitter": "Nikolaos Adaloglou", "authors": "Nikolas Adaloglou, Nicholas Vretos and Petros Daras", "title": "Multi-view adaptive graph convolutions for graph classification", "comments": "Accepted as a poster on ECCV 2020, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel multi-view methodology for graph-based neural networks\nis proposed. A systematic and methodological adaptation of the key concepts of\nclassical deep learning methods such as convolution, pooling and multi-view\narchitectures is developed for the context of non-Euclidean manifolds. The aim\nof the proposed work is to present a novel multi-view graph convolution layer,\nas well as a new view pooling layer making use of: a) a new hybrid Laplacian\nthat is adjusted based on feature distance metric learning, b) multiple\ntrainable representations of a feature matrix of a graph, using trainable\ndistance matrices, adapting the notion of views to graphs and c) a multi-view\ngraph aggregation scheme called graph view pooling, in order to synthesise\ninformation from the multiple generated views. The aforementioned layers are\nused in an end-to-end graph neural network architecture for graph\nclassification and show competitive results to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 11:14:24 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Adaloglou", "Nikolas", ""], ["Vretos", "Nicholas", ""], ["Daras", "Petros", ""]]}, {"id": "2007.12463", "submitter": "Gy\\\"orgy Kov\\'acs", "authors": "Attila Fazekas and Gy\\\"orgy Kov\\'acs", "title": "Approximately Optimal Binning for the Piecewise Constant Approximation\n  of the Normalized Unexplained Variance (nUV) Dissimilarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced Matching by Tone Mapping (MTM) dissimilarity measure\nenables template matching under smooth non-linear distortions and also has a\nwell-established mathematical background. MTM operates by binning the template,\nbut the ideal binning for a particular problem is an open question. By pointing\nout an important analogy between the well known mutual information (MI) and\nMTM, we introduce the term \"normalized unexplained variance\" (nUV) for MTM to\nemphasize its relevance and applicability beyond image processing. Then, we\nprovide theoretical results on the optimal binning technique for the nUV\nmeasure and propose algorithms to find approximate solutions. The theoretical\nfindings are supported by numerical experiments. Using the proposed techniques\nfor binning shows 4-13% increase in terms of AUC scores with statistical\nsignificance, enabling us to conclude that the proposed binning techniques have\nthe potential to improve the performance of the nUV measure in real\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 11:55:28 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Fazekas", "Attila", ""], ["Kov\u00e1cs", "Gy\u00f6rgy", ""]]}, {"id": "2007.12470", "submitter": "Stefano Zorzi", "authors": "Stefano Zorzi, Ksenia Bittner, Friedrich Fraundorfer", "title": "Map-Repair: Deep Cadastre Maps Alignment and Temporal Inconsistencies\n  Fix in Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fast developing countries it is hard to trace new buildings\nconstruction or old structures destruction and, as a result, to keep the\nup-to-date cadastre maps. Moreover, due to the complexity of urban regions or\ninconsistency of data used for cadastre maps extraction, the errors in form of\nmisalignment is a common problem. In this work, we propose an end-to-end deep\nlearning approach which is able to solve inconsistencies between the input\nintensity image and the available building footprints by correcting label\nnoises and, at the same time, misalignments if needed. The obtained results\ndemonstrate the robustness of the proposed method to even severely misaligned\nexamples that makes it potentially suitable for real applications, like\nOpenStreetMap correction.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 12:11:28 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zorzi", "Stefano", ""], ["Bittner", "Ksenia", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2007.12494", "submitter": "Jiaxiang Shang", "authors": "Jiaxiang Shang, Tianwei Shen, Shiwei Li, Lei Zhou, Mingmin Zhen, Tian\n  Fang, Long Quan", "title": "Self-Supervised Monocular 3D Face Reconstruction by Occlusion-Aware\n  Multi-view Geometry Consistency", "comments": "Accepted to ECCV 2020, supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-based approaches, in which models are trained by single-view\nimages have shown promising results for monocular 3D face reconstruction, but\nthey suffer from the ill-posed face pose and depth ambiguity issue. In contrast\nto previous works that only enforce 2D feature constraints, we propose a\nself-supervised training architecture by leveraging the multi-view geometry\nconsistency, which provides reliable constraints on face pose and depth\nestimation. We first propose an occlusion-aware view synthesis method to apply\nmulti-view geometry consistency to self-supervised learning. Then we design\nthree novel loss functions for multi-view consistency, including the pixel\nconsistency loss, the depth consistency loss, and the facial landmark-based\nepipolar loss. Our method is accurate and robust, especially under large\nvariations of expressions, poses, and illumination conditions. Comprehensive\nexperiments on the face alignment and 3D face reconstruction benchmarks have\ndemonstrated superiority over state-of-the-art methods. Our code and model are\nreleased in https://github.com/jiaxiangshang/MGCNet.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 12:36:09 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Shang", "Jiaxiang", ""], ["Shen", "Tianwei", ""], ["Li", "Shiwei", ""], ["Zhou", "Lei", ""], ["Zhen", "Mingmin", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "2007.12496", "submitter": "Tahjid Ashfaque Mostafa", "authors": "Tahjid Ashfaque Mostafa, Irene Cheng", "title": "Parkinson's Disease Detection with Ensemble Architectures based on\n  ILSVRC Models", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.00682", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore various neural network architectures using Magnetic\nResonance (MR) T1 images of the brain to identify Parkinson's Disease (PD),\nwhich is one of the most common neurodegenerative and movement disorders. We\npropose three ensemble architectures combining some winning Convolutional\nNeural Network models of ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC). All of our proposed architectures outperform existing approaches to\ndetect PD from MR images, achieving upto 95\\% detection accuracy. We also find\nthat when we construct our ensemble architecture using models pretrained on the\nImageNet dataset unrelated to PD, the detection performance is significantly\nbetter compared to models without any prior training. Our finding suggests a\npromising direction when no or insufficient training data is available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:40:47 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Mostafa", "Tahjid Ashfaque", ""], ["Cheng", "Irene", ""]]}, {"id": "2007.12499", "submitter": "Aditya Shrivastava", "authors": "Aditya Shrivastava", "title": "Adma: A Flexible Loss Function for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Highly increased interest in Artificial Neural Networks (ANNs) have resulted\nin impressively wide-ranging improvements in its structure. In this work, we\ncome up with the idea that instead of static plugins that the currently\navailable loss functions are, they should by default be flexible in nature. A\nflexible loss function can be a more insightful navigator for neural networks\nleading to higher convergence rates and therefore reaching the optimum accuracy\nmore quickly. The insights to help decide the degree of flexibility can be\nderived from the complexity of ANNs, the data distribution, selection of\nhyper-parameters and so on. In the wake of this, we introduce a novel flexible\nloss function for neural networks. The function is shown to characterize a\nrange of fundamentally unique properties from which, much of the properties of\nother loss functions are only a subset and varying the flexibility parameter in\nthe function allows it to emulate the loss curves and the learning behavior of\nprevalent static loss functions. The extensive experimentation performed with\nthe loss function demonstrates that it is able to give state-of-the-art\nperformance on selected data sets. Thus, in all the idea of flexibility itself\nand the proposed function built upon it carry the potential to open to a new\ninteresting chapter in deep learning research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 02:41:09 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Shrivastava", "Aditya", ""]]}, {"id": "2007.12515", "submitter": "Liangming Chen", "authors": "Liangming Chen, Long Jin, Xiujuan Du, Shuai Li, and Mei Liu", "title": "Deforming the Loss Surface", "comments": "This paper is not perfect yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, it is usually assumed that the shape of the loss surface is\nfixed. Differently, a novel concept of deformation operator is first proposed\nin this paper to deform the loss surface, thereby improving the optimization.\nDeformation function, as a type of deformation operator, can improve the\ngeneralization performance. Moreover, various deformation functions are\ndesigned, and their contributions to the loss surface are further provided.\nThen, the original stochastic gradient descent optimizer is theoretically\nproved to be a flat minima filter that owns the talent to filter out the sharp\nminima. Furthermore, the flatter minima could be obtained by exploiting the\nproposed deformation functions, which is verified on CIFAR-100, with\nvisualizations of loss landscapes near the critical points obtained by both the\noriginal optimizer and optimizer enhanced by deformation functions. The\nexperimental results show that deformation functions do find flatter regions.\nMoreover, on ImageNet, CIFAR-10, and CIFAR-100, popular convolutional neural\nnetworks enhanced by deformation functions are compared with the corresponding\noriginal models, where significant improvements are observed on all of the\ninvolved models equipped with deformation functions. For example, the top-1\ntest accuracy of ResNet-20 on CIFAR-100 increases by 1.46%, with insignificant\nadditional computational overhead.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 13:17:46 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 02:15:37 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Liangming", ""], ["Jin", "Long", ""], ["Du", "Xiujuan", ""], ["Li", "Shuai", ""], ["Liu", "Mei", ""]]}, {"id": "2007.12519", "submitter": "Kejun Wang", "authors": "Jing Chen (1), Chenhui Wang (2), Kejun Wang (1), Chaoqun Yin (1), Cong\n  Zhao (1), Tao Xu (1), Xinyi Zhang (1), Ziqiang Huang (1), Meichen Liu (1),\n  Tao Yang (1) ((1) College of Intelligent Systems Science and Engineering,\n  Harbin Engineering University, Harbin, China., (2) UCLA Department of\n  Statistics, Los Angeles, CA.)", "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition\n  in the Wild", "comments": "Neural Comput & Applic (2021)", "journal-ref": null, "doi": "10.1007/s00521-020-05616-w", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of affective computing in the wild setting is underpinned by\ndatabases. Existing multimodal emotion databases in the real-world conditions\nare few and small, with a limited number of subjects and expressed in a single\nlanguage. To meet this requirement, we collected, annotated, and prepared to\nrelease a new natural state video database (called HEU Emotion). HEU Emotion\ncontains a total of 19,004 video clips, which is divided into two parts\naccording to the data source. The first part contains videos downloaded from\nTumblr, Google, and Giphy, including 10 emotions and two modalities (facial\nexpression and body posture). The second part includes corpus taken manually\nfrom movies, TV series, and variety shows, consisting of 10 emotions and three\nmodalities (facial expression, body posture, and emotional speech). HEU Emotion\nis by far the most extensive multi-modal emotional database with 9,951\nsubjects. In order to provide a benchmark for emotion recognition, we used many\nconventional machine learning and deep learning methods to evaluate HEU\nEmotion. We proposed a Multi-modal Attention module to fuse multi-modal\nfeatures adaptively. After multi-modal fusion, the recognition accuracies for\nthe two parts increased by 2.19% and 4.01% respectively over those of\nsingle-modal facial expression recognition.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 13:36:52 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Jing", ""], ["Wang", "Chenhui", ""], ["Wang", "Kejun", ""], ["Yin", "Chaoqun", ""], ["Zhao", "Cong", ""], ["Xu", "Tao", ""], ["Zhang", "Xinyi", ""], ["Huang", "Ziqiang", ""], ["Liu", "Meichen", ""], ["Yang", "Tao", ""]]}, {"id": "2007.12525", "submitter": "Kishor Datta Gupta", "authors": "Md Manjurul Ahsan, Kishor Datta Gupta, Mohammad Maminur Islam, Sajib\n  Sen, Md. Lutfar Rahman, Mohammad Shakhawat Hossain", "title": "Study of Different Deep Learning Approach with Explainable AI for\n  Screening Patients with COVID-19 Symptoms: Using CT Scan and Chest X-ray\n  Image Dataset", "comments": "This is a work in progress, it should not be relied upon without\n  context to guide clinical practice or health-related behavior and should not\n  be reported in news media as established information without consulting\n  multiple experts in the field", "journal-ref": null, "doi": "10.3390/make2040027", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 disease caused more than 100,000 deaths so far in\nthe USA alone. It is necessary to conduct an initial screening of patients with\nthe symptoms of COVID-19 disease to control the spread of the disease. However,\nit is becoming laborious to conduct the tests with the available testing kits\ndue to the growing number of patients. Some studies proposed CT scan or chest\nX-ray images as an alternative solution. Therefore, it is essential to use\nevery available resource, instead of either a CT scan or chest X-ray to conduct\na large number of tests simultaneously. As a result, this study aims to develop\na deep learning-based model that can detect COVID-19 patients with better\naccuracy both on CT scan and chest X-ray image dataset. In this work, eight\ndifferent deep learning approaches such as VGG16, InceptionResNetV2, ResNet50,\nDenseNet201, VGG19, MobilenetV2, NasNetMobile, and ResNet15V2 have been tested\non two dataset-one dataset includes 400 CT scan images, and another dataset\nincludes 400 chest X-ray images studied. Besides, Local Interpretable\nModel-agnostic Explanations (LIME) is used to explain the model's\ninterpretability. Using LIME, test results demonstrate that it is conceivable\nto interpret top features that should have worked to build a trust AI framework\nto distinguish between patients with COVID-19 symptoms with other patients.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 13:51:58 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ahsan", "Md Manjurul", ""], ["Gupta", "Kishor Datta", ""], ["Islam", "Mohammad Maminur", ""], ["Sen", "Sajib", ""], ["Rahman", "Md. Lutfar", ""], ["Hossain", "Mohammad Shakhawat", ""]]}, {"id": "2007.12530", "submitter": "Ilias Papastratis", "authors": "Nikolas Adaloglou, Theocharis Chatzis, Ilias Papastratis, Andreas\n  Stergioulas, Georgios Th. Papadopoulos, Vassia Zacharopoulou, George J.\n  Xydopoulos, Klimnis Atzakas, Dimitris Papazachariou, and Petros Daras", "title": "A Comprehensive Study on Deep Learning-based Methods for Sign Language\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2021.3070438", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a comparative experimental assessment of computer vision-based\nmethods for sign language recognition is conducted. By implementing the most\nrecent deep neural network methods in this field, a thorough evaluation on\nmultiple publicly available datasets is performed. The aim of the present study\nis to provide insights on sign language recognition, focusing on mapping\nnon-segmented video streams to glosses. For this task, two new sequence\ntraining criteria, known from the fields of speech and scene text recognition,\nare introduced. Furthermore, a plethora of pretraining schemes is thoroughly\ndiscussed. Finally, a new RGB+D dataset for the Greek sign language is created.\nTo the best of our knowledge, this is the first sign language dataset where\nsentence and gloss level annotations are provided for a video capture.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:07:01 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 19:32:15 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Adaloglou", "Nikolas", ""], ["Chatzis", "Theocharis", ""], ["Papastratis", "Ilias", ""], ["Stergioulas", "Andreas", ""], ["Papadopoulos", "Georgios Th.", ""], ["Zacharopoulou", "Vassia", ""], ["Xydopoulos", "George J.", ""], ["Atzakas", "Klimnis", ""], ["Papazachariou", "Dimitris", ""], ["Daras", "Petros", ""]]}, {"id": "2007.12540", "submitter": "Menelaos Kanakis", "authors": "Menelaos Kanakis, David Bruggemann, Suman Saha, Stamatios Georgoulis,\n  Anton Obukhov, Luc Van Gool", "title": "Reparameterizing Convolutions for Incremental Multi-Task Learning\n  without Task Interference", "comments": "European Conference on Computer Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task networks are commonly utilized to alleviate the need for a large\nnumber of highly specialized single-task networks. However, two common\nchallenges in developing multi-task models are often overlooked in literature.\nFirst, enabling the model to be inherently incremental, continuously\nincorporating information from new tasks without forgetting the previously\nlearned ones (incremental learning). Second, eliminating adverse interactions\namongst tasks, which has been shown to significantly degrade the single-task\nperformance in a multi-task setup (task interference). In this paper, we show\nthat both can be achieved simply by reparameterizing the convolutions of\nstandard neural network architectures into a non-trainable shared part (filter\nbank) and task-specific parts (modulators), where each modulator has a fraction\nof the filter bank parameters. Thus, our reparameterization enables the model\nto learn new tasks without adversely affecting the performance of existing\nones. The results of our ablation study attest the efficacy of the proposed\nreparameterization. Moreover, our method achieves state-of-the-art on two\nchallenging multi-task learning benchmarks, PASCAL-Context and NYUD, and also\ndemonstrates superior incremental learning capability as compared to its close\ncompetitors.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:44:46 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kanakis", "Menelaos", ""], ["Bruggemann", "David", ""], ["Saha", "Suman", ""], ["Georgoulis", "Stamatios", ""], ["Obukhov", "Anton", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.12553", "submitter": "Chaitanya Ahuja", "authors": "Chaitanya Ahuja, Dong Won Lee, Yukiko I. Nakano, Louis-Philippe\n  Morency", "title": "Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker\n  Conditional-Mixture Approach", "comments": "24 pages, 12 figures", "journal-ref": "European Conference on Computer Vision 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How can we teach robots or virtual assistants to gesture naturally? Can we go\nfurther and adapt the gesturing style to follow a specific speaker? Gestures\nthat are naturally timed with corresponding speech during human communication\nare called co-speech gestures. A key challenge, called gesture style transfer,\nis to learn a model that generates these gestures for a speaking agent 'A' in\nthe gesturing style of a target speaker 'B'. A secondary goal is to\nsimultaneously learn to generate co-speech gestures for multiple speakers while\nremembering what is unique about each speaker. We call this challenge style\npreservation. In this paper, we propose a new model, named Mix-StAGE, which\ntrains a single model for multiple speakers while learning unique style\nembeddings for each speaker's gestures in an end-to-end manner. A novelty of\nMix-StAGE is to learn a mixture of generative models which allows for\nconditioning on the unique gesture style of each speaker. As Mix-StAGE\ndisentangles style and content of gestures, gesturing styles for the same input\nspeech can be altered by simply switching the style embeddings. Mix-StAGE also\nallows for style preservation when learning simultaneously from multiple\nspeakers. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS),\ndesigned to study gesture generation and style transfer. Our proposed Mix-StAGE\nmodel significantly outperforms the previous state-of-the-art approach for\ngesture generation and provides a path towards performing gesture style\ntransfer across multiple speakers. Link to code, data, and videos:\nhttp://chahuja.com/mix-stage\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:01:02 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Ahuja", "Chaitanya", ""], ["Lee", "Dong Won", ""], ["Nakano", "Yukiko I.", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2007.12562", "submitter": "Carola Figueroa Flores", "authors": "Carola Figueroa-Flores, Bogdan Raducanu, David Berga, and Joost van de\n  Weijer", "title": "Hallucinating Saliency Maps for Fine-Grained Image Classification for\n  Limited Data Domains", "comments": "Accepted to VISIGRAPP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the saliency methods are evaluated on their ability to generate\nsaliency maps, and not on their functionality in a complete vision pipeline,\nlike for instance, image classification. In the current paper, we propose an\napproach which does not require explicit saliency maps to improve image\nclassification, but they are learned implicitely, during the training of an\nend-to-end image classification task. We show that our approach obtains similar\nresults as the case when the saliency maps are provided explicitely. Combining\nRGB data with saliency maps represents a significant advantage for object\nrecognition, especially for the case when training data is limited. We validate\nour method on several datasets for fine-grained classification tasks (Flowers,\nBirds and Cars). In addition, we show that our saliency estimation method,\nwhich is trained without any saliency groundtruth data, obtains competitive\nresults on real image saliency benchmark (Toronto), and outperforms deep\nsaliency models with synthetic images (SID4VAM).\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:08:55 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 11:09:47 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 10:29:57 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Figueroa-Flores", "Carola", ""], ["Raducanu", "Bogdan", ""], ["Berga", "David", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2007.12568", "submitter": "Eitan Richardson", "authors": "Eitan Richardson and Yair Weiss", "title": "The Surprising Effectiveness of Linear Unsupervised Image-to-Image\n  Translation", "comments": "Preprint - under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised image-to-image translation is an inherently ill-posed problem.\nRecent methods based on deep encoder-decoder architectures have shown\nimpressive results, but we show that they only succeed due to a strong locality\nbias, and they fail to learn very simple nonlocal transformations (e.g. mapping\nupside down faces to upright faces). When the locality bias is removed, the\nmethods are too powerful and may fail to learn simple local transformations. In\nthis paper we introduce linear encoder-decoder architectures for unsupervised\nimage to image translation. We show that learning is much easier and faster\nwith these architectures and yet the results are surprisingly effective. In\nparticular, we show a number of local problems for which the results of the\nlinear methods are comparable to those of state-of-the-art architectures but\nwith a fraction of the training time, and a number of nonlocal problems for\nwhich the state-of-the-art fails while linear methods succeed.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:21:25 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Richardson", "Eitan", ""], ["Weiss", "Yair", ""]]}, {"id": "2007.12577", "submitter": "Simon Evain", "authors": "Simon Evain and Christine Guillemot", "title": "A Lightweight Neural Network for Monocular View Generation with\n  Occlusion Handling", "comments": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in December 2019", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2960689", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a very lightweight neural network architecture,\ntrained on stereo data pairs, which performs view synthesis from one single\nimage. With the growing success of multi-view formats, this problem is indeed\nincreasingly relevant. The network returns a prediction built from disparity\nestimation, which fills in wrongly predicted regions using a occlusion handling\ntechnique. To do so, during training, the network learns to estimate the\nleft-right consistency structural constraint on the pair of stereo input\nimages, to be able to replicate it at test time from one single image. The\nmethod is built upon the idea of blending two predictions: a prediction based\non disparity estimation, and a prediction based on direct minimization in\noccluded regions. The network is also able to identify these occluded areas at\ntraining and at test time by checking the pixelwise left-right consistency of\nthe produced disparity maps. At test time, the approach can thus generate a\nleft-side and a right-side view from one input image, as well as a depth map\nand a pixelwise confidence measure in the prediction. The work outperforms\nvisually and metric-wise state-of-the-art approaches on the challenging KITTI\ndataset, all while reducing by a very significant order of magnitude (5 or 10\ntimes) the required number of parameters (6.5 M).\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:29:01 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Evain", "Simon", ""], ["Guillemot", "Christine", ""]]}, {"id": "2007.12578", "submitter": "Xingyu Li", "authors": "Hanwen Liang, Konstantinos N. Plataniotis, Xingyu Li", "title": "Stain Style Transfer of Histopathology Images Via Structure-Preserved\n  Generative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational histopathology image diagnosis becomes increasingly popular and\nimportant, where images are segmented or classified for disease diagnosis by\ncomputers. While pathologists do not struggle with color variations in slides,\ncomputational solutions usually suffer from this critical issue. To address the\nissue of color variations in histopathology images, this study proposes two\nstain style transfer models, SSIM-GAN and DSCSI-GAN, based on the generative\nadversarial networks. By cooperating structural preservation metrics and\nfeedback of an auxiliary diagnosis net in learning, medical-relevant\ninformation presented by image texture, structure, and chroma-contrast features\nis preserved in color-normalized images. Particularly, the smart treat of\nchromatic image content in our DSCSI-GAN model helps to achieve noticeable\nnormalization improvement in image regions where stains mix due to histological\nsubstances co-localization. Extensive experimentation on public histopathology\nimage sets indicates that our methods outperform prior arts in terms of\ngenerating more stain-consistent images, better preserving histological\ninformation in images, and obtaining significantly higher learning efficiency.\nOur python implementation is published on\nhttps://github.com/hanwen0529/DSCSI-GAN.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:30:19 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Hanwen", ""], ["Plataniotis", "Konstantinos N.", ""], ["Li", "Xingyu", ""]]}, {"id": "2007.12587", "submitter": "Stefano Zorzi", "authors": "Stefano Zorzi, Ksenia Bittner, Friedrich Fraundorfer", "title": "Machine-learned Regularization and Polygonization of Building\n  Segmentation Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a machine learning based approach for automatic regularization and\npolygonization of building segmentation masks. Taking an image as input, we\nfirst predict building segmentation maps exploiting generic fully convolutional\nnetwork (FCN). A generative adversarial network (GAN) is then involved to\nperform a regularization of building boundaries to make them more realistic,\ni.e., having more rectilinear outlines which construct right angles if\nrequired. This is achieved through the interplay between the discriminator\nwhich gives a probability of input image being true and generator that learns\nfrom discriminator's response to create more realistic images. Finally, we\ntrain the backbone convolutional neural network (CNN) which is adapted to\npredict sparse outcomes corresponding to building corners out of regularized\nbuilding segmentation results. Experiments on three building segmentation\ndatasets demonstrate that the proposed method is not only capable of obtaining\naccurate results, but also of producing visually pleasing building outlines\nparameterized as polygons.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:38:35 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 13:38:44 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 14:34:11 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zorzi", "Stefano", ""], ["Bittner", "Ksenia", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2007.12619", "submitter": "Zhisheng Zhong", "authors": "Zhisheng Zhong, Hiroaki Akutsu and Kiyoharu Aizawa", "title": "Channel-Level Variable Quantization Network for Deep Image Compression", "comments": "Proceedings of International Joint Conference on Artificial\n  Intelligence (IJCAI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image compression systems mainly contain four components: encoder,\nquantizer, entropy model, and decoder. To optimize these four components, a\njoint rate-distortion framework was proposed, and many deep neural\nnetwork-based methods achieved great success in image compression. However,\nalmost all convolutional neural network-based methods treat channel-wise\nfeature maps equally, reducing the flexibility in handling different types of\ninformation. In this paper, we propose a channel-level variable quantization\nnetwork to dynamically allocate more bitrates for significant channels and\nwithdraw bitrates for negligible channels. Specifically, we propose a variable\nquantization controller. It consists of two key components: the channel\nimportance module, which can dynamically learn the importance of channels\nduring training, and the splitting-merging module, which can allocate different\nbitrates for different channels. We also formulate the quantizer into a\nGaussian mixture model manner. Quantitative and qualitative experiments verify\nthe effectiveness of the proposed model and demonstrate that our method\nachieves superior performance and can produce much better visual\nreconstructions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:20:39 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhong", "Zhisheng", ""], ["Akutsu", "Hiroaki", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2007.12622", "submitter": "Junhuem Park", "authors": "Junheum Park, Keunsoo Ko, Chul Lee, Chang-Su Kim", "title": "BMBC:Bilateral Motion Estimation with Bilateral Cost Volume for Video\n  Interpolation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video interpolation increases the temporal resolution of a video sequence by\nsynthesizing intermediate frames between two consecutive frames. We propose a\nnovel deep-learning-based video interpolation algorithm based on bilateral\nmotion estimation. First, we develop the bilateral motion network with the\nbilateral cost volume to estimate bilateral motions accurately. Then, we\napproximate bi-directional motions to predict a different kind of bilateral\nmotions. We then warp the two input frames using the estimated bilateral\nmotions. Next, we develop the dynamic filter generation network to yield\ndynamic blending filters. Finally, we combine the warped frames using the\ndynamic blending filters to generate intermediate frames. Experimental results\nshow that the proposed algorithm outperforms the state-of-the-art video\ninterpolation algorithms on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:07:51 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Park", "Junheum", ""], ["Ko", "Keunsoo", ""], ["Lee", "Chul", ""], ["Kim", "Chang-Su", ""]]}, {"id": "2007.12623", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, Jagadeesan Jayender", "title": "Real-time Dense Reconstruction of Tissue Surface from Stereo Optical\n  Video", "comments": null, "journal-ref": "IEEE transactions on medical imaging 39, no. 2 (2019): 400-412", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to reconstruct dense three-dimensional (3D) model of\ntissue surface from stereo optical videos in real-time, the basic idea of which\nis to first extract 3D information from video frames by using stereo matching,\nand then to mosaic the reconstructed 3D models. To handle the common low\ntexture regions on tissue surfaces, we propose effective post-processing steps\nfor the local stereo matching method to enlarge the radius of constraint, which\ninclude outliers removal, hole filling and smoothing. Since the tissue models\nobtained by stereo matching are limited to the field of view of the imaging\nmodality, we propose a model mosaicking method by using a novel feature-based\nsimultaneously localization and mapping (SLAM) method to align the models. Low\ntexture regions and the varying illumination condition may lead to a large\npercentage of feature matching outliers. To solve this problem, we propose\nseveral algorithms to improve the robustness of SLAM, which mainly include (1)\na histogram voting-based method to roughly select possible inliers from the\nfeature matching results, (2) a novel 1-point RANSAC-based P$n$P algorithm\ncalled as DynamicR1PP$n$P to track the camera motion and (3) a GPU-based\niterative closest points (ICP) and bundle adjustment (BA) method to refine the\ncamera motion estimation results. Experimental results on ex- and in vivo data\nshowed that the reconstructed 3D models have high resolution texture with an\naccuracy error of less than 2 mm. Most algorithms are highly parallelized for\nGPU computation, and the average runtime for processing one key frame is 76.3\nms on stereo images with 960x540 resolution.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:14:05 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhou", "Haoyin", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "2007.12625", "submitter": "Feihu Huang", "authors": "Feihu Huang, Lue Tao, Songcan Chen", "title": "Accelerated Stochastic Gradient-free and Projection-free Methods", "comments": "Accepted to ICML 2020, 34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we propose a class of accelerated stochastic gradient-free and\nprojection-free (a.k.a., zeroth-order Frank-Wolfe) methods to solve the\nconstrained stochastic and finite-sum nonconvex optimization. Specifically, we\npropose an accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW) method\nbased on the variance reduced technique of SPIDER/SpiderBoost and a novel\nmomentum accelerated technique. Moreover, under some mild conditions, we prove\nthat the Acc-SZOFW has the function query complexity of\n$O(d\\sqrt{n}\\epsilon^{-2})$ for finding an $\\epsilon$-stationary point in the\nfinite-sum problem, which improves the exiting best result by a factor of\n$O(\\sqrt{n}\\epsilon^{-2})$, and has the function query complexity of\n$O(d\\epsilon^{-3})$ in the stochastic problem, which improves the exiting best\nresult by a factor of $O(\\epsilon^{-1})$. To relax the large batches required\nin the Acc-SZOFW, we further propose a novel accelerated stochastic\nzeroth-order Frank-Wolfe (Acc-SZOFW*) based on a new variance reduced technique\nof STORM, which still reaches the function query complexity of\n$O(d\\epsilon^{-3})$ in the stochastic problem without relying on any large\nbatches. In particular, we present an accelerated framework of the Frank-Wolfe\nmethods based on the proposed momentum accelerated technique. The extensive\nexperimental results on black-box adversarial attack and robust black-box\nclassification demonstrate the efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:50:15 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 15:45:54 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Huang", "Feihu", ""], ["Tao", "Lue", ""], ["Chen", "Songcan", ""]]}, {"id": "2007.12668", "submitter": "Fatemeh Karimi Nejadasl", "authors": "Deyvid Kochanov, Fatemeh Karimi Nejadasl, and Olaf Booij", "title": "KPRNet: Improving projection-based LiDAR semantic segmentation", "comments": "\"ECCV 2020. Code and pre-trained models at\n  https://github.com/DeyvidKochanov-TomTom/kprnet\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an important component in the perception systems of\nautonomous vehicles. In this work, we adopt recent advances in both image and\npoint cloud segmentation to achieve a better accuracy in the task of segmenting\nLiDAR scans. KPRNet improves the convolutional neural network architecture of\n2D projection methods and utilizes KPConv to replace the commonly used\npost-processing techniques with a learnable point-wise component which allows\nus to obtain more accurate 3D labels. With these improvements our model\noutperforms the current best method on the SemanticKITTI benchmark, reaching an\nmIoU of 63.1.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:35:14 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 10:43:18 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Kochanov", "Deyvid", ""], ["Nejadasl", "Fatemeh Karimi", ""], ["Booij", "Olaf", ""]]}, {"id": "2007.12672", "submitter": "Nooshin Mojab", "authors": "Nooshin Mojab, Vahid Noroozi, Darvin Yi, Manoj Prabhakar Nallabothula,\n  Abdullah Aleem, Phillip S. Yu, Joelle A. Hallak", "title": "Real-World Multi-Domain Data Applications for Generalizations to\n  Clinical Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With promising results of machine learning based models in computer vision,\napplications on medical imaging data have been increasing exponentially.\nHowever, generalizations to complex real-world clinical data is a persistent\nproblem. Deep learning models perform well when trained on standardized\ndatasets from artificial settings, such as clinical trials. However, real-world\ndata is different and translations are yielding varying results. The complexity\nof real-world applications in healthcare could emanate from a mixture of\ndifferent data distributions across multiple device domains alongside the\ninevitable noise sourced from varying image resolutions, human errors, and the\nlack of manual gradings. In addition, healthcare applications not only suffer\nfrom the scarcity of labeled data, but also face limited access to unlabeled\ndata due to HIPAA regulations, patient privacy, ambiguity in data ownership,\nand challenges in collecting data from different sources. These limitations\npose additional challenges to applying deep learning algorithms in healthcare\nand clinical translations. In this paper, we utilize self-supervised\nrepresentation learning methods, formulated effectively in transfer learning\nsettings, to address limited data availability. Our experiments verify the\nimportance of diverse real-world data for generalization to clinical settings.\nWe show that by employing a self-supervised approach with transfer learning on\na multi-domain real-world dataset, we can achieve 16% relative improvement on a\nstandardized dataset over supervised baselines.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:41:23 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Mojab", "Nooshin", ""], ["Noroozi", "Vahid", ""], ["Yi", "Darvin", ""], ["Nallabothula", "Manoj Prabhakar", ""], ["Aleem", "Abdullah", ""], ["Yu", "Phillip S.", ""], ["Hallak", "Joelle A.", ""]]}, {"id": "2007.12684", "submitter": "Luyu Yang", "authors": "Luyu Yang, Yan Wang, Mingfei Gao, Abhinav Shrivastava, Kilian Q.\n  Weinberger, Wei-Lun Chao, Ser-Nam Lim", "title": "Deep Co-Training with Task Decomposition for Semi-Supervised Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised domain adaptation (SSDA) aims to adapt models trained from a\nlabeled source domain to a different but related target domain, from which\nunlabeled data and a small set of labeled data are provided. Current methods\nthat treat source and target supervision without distinction overlook their\ninherent discrepancy, resulting in a source-dominated model that has not\neffectively use the target supervision. In this paper, we argue that the\nlabeled target data needs to be distinguished for effective SSDA, and propose\nto explicitly decompose the SSDA task into two sub-tasks: a semi-supervised\nlearning (SSL) task in the target domain and an unsupervised domain adaptation\n(UDA) task across domains. By doing so, the two sub-tasks can better leverage\nthe corresponding supervision and thus yield very different classifiers. To\nintegrate the strengths of the two classifiers, we apply the well-established\nco-training framework, in which the two classifiers exchange their high\nconfident predictions to iteratively \"teach each other\" so that both\nclassifiers can excel in the target domain. We call our approach Deep\nCo-training with Task decomposition (DeCoTa). DeCoTa requires no adversarial\ntraining and is easy to implement. Moreover, DeCoTa is well-founded on the\ntheoretical condition of when co-training would succeed. As a result, DeCoTa\nachieves state-of-the-art results on several SSDA datasets, outperforming the\nprior art by a notable 4% margin on DomainNet.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:57:54 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 03:11:50 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 03:50:00 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 00:32:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Luyu", ""], ["Wang", "Yan", ""], ["Gao", "Mingfei", ""], ["Shrivastava", "Abhinav", ""], ["Weinberger", "Kilian Q.", ""], ["Chao", "Wei-Lun", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2007.12685", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar, RajKumar Soundrapandiyan", "title": "Semantic Segmentation With Multi Scale Spatial Attention For Self\n  Driving Cars", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel neural network using multi scale feature\nfusion at various scales for accurate and efficient semantic image\nsegmentation. We used ResNet based feature extractor, dilated convolutional\nlayers in downsampling part, atrous convolutional layers in the upsampling part\nand used concat operation to merge them. A new attention module is proposed to\nencode more contextual information and enhance the receptive field of the\nnetwork. We present an in depth theoretical analysis of our network with\ntraining and optimization details. Our network was trained and tested on the\nCamvid dataset and Cityscapes dataset using mean accuracy per class and\nIntersection Over Union (IOU) as the evaluation metrics. Our model outperforms\nprevious state of the art methods on semantic segmentation achieving mean IOU\nvalue of 74.12 while running at >100 FPS.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 20:19:09 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 19:53:31 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 22:56:11 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sagar", "Abhinav", ""], ["Soundrapandiyan", "RajKumar", ""]]}, {"id": "2007.12749", "submitter": "Xiaotong Liu", "authors": "Hong Xuan, Abby Stylianou, Xiaotong Liu, Robert Pless", "title": "Hard negative examples are hard, but useful", "comments": "CV, Triplet loss, Image embedding, 14 pages, 9 figures, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triplet loss is an extremely common approach to distance metric learning.\nRepresentations of images from the same class are optimized to be mapped closer\ntogether in an embedding space than representations of images from different\nclasses. Much work on triplet losses focuses on selecting the most useful\ntriplets of images to consider, with strategies that select dissimilar examples\nfrom the same class or similar examples from different classes. The consensus\nof previous research is that optimizing with the \\textit{hardest} negative\nexamples leads to bad training behavior. That's a problem -- these hardest\nnegatives are literally the cases where the distance metric fails to capture\nsemantic similarity. In this paper, we characterize the space of triplets and\nderive why hard negatives make triplet loss training fail. We offer a simple\nfix to the loss function and show that, with this fix, optimizing with hard\nnegative examples becomes feasible. This leads to more generalizable features,\nand image retrieval results that outperform state of the art for datasets with\nhigh intra-class variance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 19:34:58 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:40:51 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xuan", "Hong", ""], ["Stylianou", "Abby", ""], ["Liu", "Xiaotong", ""], ["Pless", "Robert", ""]]}, {"id": "2007.12750", "submitter": "Jiasen Lu", "authors": "Michael Cogswell, Jiasen Lu, Rishabh Jain, Stefan Lee, Devi Parikh,\n  Dhruv Batra", "title": "Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we develop visually grounded dialog agents that can efficiently adapt to\nnew tasks without forgetting how to talk to people? Such agents could leverage\na larger variety of existing data to generalize to new tasks, minimizing\nexpensive data collection and annotation. In this work, we study a setting we\ncall \"Dialog without Dialog\", which requires agents to develop visually\ngrounded dialog models that can adapt to new tasks without language level\nsupervision. By factorizing intention and language, our model minimizes\nlinguistic drift after fine-tuning for new tasks. We present qualitative\nresults, automated metrics, and human studies that all show our model can adapt\nto new tasks and maintain language quality. Baselines either fail to perform\nwell at new tasks or experience language drift, becoming unintelligible to\nhumans. Code has been made available at\nhttps://github.com/mcogswell/dialog_without_dialog\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 19:35:57 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cogswell", "Michael", ""], ["Lu", "Jiasen", ""], ["Jain", "Rishabh", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "2007.12764", "submitter": "Shadrokh Samavi", "authors": "Ghazale Ghorbanzade, Zahra Nabizadeh-ShahreBabak, Shadrokh Samavi,\n  Nader Karimi, Ali Emami, Pejman Khadivi", "title": "Selection of Proper EEG Channels for Subject Intention Classification\n  Using Deep Learning", "comments": "10 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain signals could be used to control devices to assist individuals with\ndisabilities. Signals such as electroencephalograms are complicated and hard to\ninterpret. A set of signals are collected and should be classified to identify\nthe intention of the subject. Different approaches have tried to reduce the\nnumber of channels before sending them to a classifier. We are proposing a deep\nlearning-based method for selecting an informative subset of channels that\nproduce high classification accuracy. The proposed network could be trained for\nan individual subject for the selection of an appropriate set of channels.\nReduction of the number of channels could reduce the complexity of\nbrain-computer-interface devices. Our method could find a subset of channels.\nThe accuracy of our approach is comparable with a model trained on all\nchannels. Hence, our model's temporal and power costs are low, while its\naccuracy is kept high.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 20:40:10 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 19:27:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ghorbanzade", "Ghazale", ""], ["Nabizadeh-ShahreBabak", "Zahra", ""], ["Samavi", "Shadrokh", ""], ["Karimi", "Nader", ""], ["Emami", "Ali", ""], ["Khadivi", "Pejman", ""]]}, {"id": "2007.12806", "submitter": "Minh Vo", "authors": "Minh Vo, Yaser Sheikh, and Srinivasa G. Narasimhan", "title": "Spatiotemporal Bundle Adjustment for Dynamic 3D Human Reconstruction in\n  the Wild", "comments": "Accepted to IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D\npoint triangulation to reconstruct a static scene. The triangulation\nconstraint, however, is invalid for moving points captured in multiple\nunsynchronized videos and bundle adjustment is not designed to estimate the\ntemporal alignment between cameras. We present a spatiotemporal bundle\nadjustment framework that jointly optimizes four coupled sub-problems:\nestimating camera intrinsics and extrinsics, triangulating static 3D points, as\nwell as sub-frame temporal alignment between cameras and computing 3D\ntrajectories of dynamic points. Key to our joint optimization is the careful\nintegration of physics-based motion priors within the reconstruction pipeline,\nvalidated on a large motion capture corpus of human subjects. We devise an\nincremental reconstruction and alignment algorithm to strictly enforce the\nmotion prior during the spatiotemporal bundle adjustment. This algorithm is\nfurther made more efficient by a divide and conquer scheme while still\nmaintaining high accuracy. We apply this algorithm to reconstruct 3D motion\ntrajectories of human bodies in dynamic events captured by multiple\nuncalibrated and unsynchronized video cameras in the wild. To make the\nreconstruction visually more interpretable, we fit a statistical 3D human body\nmodel to the asynchronous video streams.Compared to the baseline, the fitting\nsignificantly benefits from the proposed spatiotemporal bundle adjustment\nprocedure. Because the videos are aligned with sub-frame precision, we\nreconstruct 3D motion at much higher temporal resolution than the input videos.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:50:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Vo", "Minh", ""], ["Sheikh", "Yaser", ""], ["Narasimhan", "Srinivasa G.", ""]]}, {"id": "2007.12808", "submitter": "Stefan Schneider", "authors": "Stefan Schneider and Alex Zhuang", "title": "Counting Fish and Dolphins in Sonar Images Using Deep Learning", "comments": "19 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning provides the opportunity to improve upon conflicting reports\nconsidering the relationship between the Amazon river's fish and dolphin\nabundance and reduced canopy cover as a result of deforestation. Current\nmethods of fish and dolphin abundance estimates are performed by on-site\nsampling using visual and capture/release strategies. We propose a novel\napproach to calculating fish abundance using deep learning for fish and dolphin\nestimates from sonar images taken from the back of a trolling boat. We consider\na data set of 143 images ranging from 0-34 fish, and 0-3 dolphins provided by\nthe Fund Amazonia research group. To overcome the data limitation, we test the\ncapabilities of data augmentation on an unconventional 15/85 training/testing\nsplit. Using 20 training images, we simulate a gradient of data up to 25,000\nimages using augmented backgrounds and randomly placed/rotation cropped fish\nand dolphin taken from the training set. We then train four multitask network\narchitectures: DenseNet201, InceptionNetV2, Xception, and MobileNetV2 to\npredict fish and dolphin numbers using two function approximation methods:\nregression and classification. For regression, Densenet201 performed best for\nfish and Xception best for dolphin with mean squared errors of 2.11 and 0.133\nrespectively. For classification, InceptionResNetV2 performed best for fish and\nMobileNetV2 best for dolphins with a mean error of 2.07 and 0.245 respectively.\nConsidering the 123 testing images, our results show the success of data\nsimulation for limited sonar data sets. We find DenseNet201 is able to identify\ndolphins after approximately 5000 training images, while fish required the full\n25,000. Our method can be used to lower costs and expedite the data analysis of\nfish and dolphin abundance to real-time along the Amazon river and river\nsystems worldwide.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:52:03 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Schneider", "Stefan", ""], ["Zhuang", "Alex", ""]]}, {"id": "2007.12813", "submitter": "Aydogan Ozcan", "authors": "Onur Kulce, Deniz Mengu, Yair Rivenson, Aydogan Ozcan", "title": "All-Optical Information Processing Capacity of Diffractive Surfaces", "comments": "31 Pages, 6 Figures, 1 Table", "journal-ref": "Light: Science & Applications (2021)", "doi": "10.1038/s41377-020-00439-9", "report-no": null, "categories": "eess.IV cs.CV cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise engineering of materials and surfaces has been at the heart of some\nof the recent advances in optics and photonics. These advances around the\nengineering of materials with new functionalities have also opened up exciting\navenues for designing trainable surfaces that can perform computation and\nmachine learning tasks through light-matter interaction and diffraction. Here,\nwe analyze the information processing capacity of coherent optical networks\nformed by diffractive surfaces that are trained to perform an all-optical\ncomputational task between a given input and output field-of-view. We show that\nthe dimensionality of the all-optical solution space covering the\ncomplex-valued transformations between the input and output fields-of-view is\nlinearly proportional to the number of diffractive surfaces within the optical\nnetwork, up to a limit that is dictated by the extent of the input and output\nfields-of-view. Deeper diffractive networks that are composed of larger numbers\nof trainable surfaces can cover a higher dimensional subspace of the\ncomplex-valued linear transformations between a larger input field-of-view and\na larger output field-of-view, and exhibit depth advantages in terms of their\nstatistical inference, learning and generalization capabilities for different\nimage classification tasks, when compared with a single trainable diffractive\nsurface. These analyses and conclusions are broadly applicable to various forms\nof diffractive surfaces, including e.g., plasmonic and/or dielectric-based\nmetasurfaces and flat optics that can be used to form all-optical processors.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 00:40:46 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 03:49:56 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kulce", "Onur", ""], ["Mengu", "Deniz", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2007.12829", "submitter": "Guo Zhong", "authors": "Shi-Xun Lina, Guo Zhongb, Ting Shu", "title": "Joint Featurewise Weighting and Lobal Structure Learning for Multi-view\n  Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering integrates multiple feature sets, which reveal distinct\naspects of the data and provide complementary information to each other, to\nimprove the clustering performance. It remains challenging to effectively\nexploit complementary information across multiple views since the original data\noften contain noise and are highly redundant. Moreover, most existing\nmulti-view clustering methods only aim to explore the consistency of all views\nwhile ignoring the local structure of each view. However, it is necessary to\ntake the local structure of each view into consideration, because different\nviews would present different geometric structures while admitting the same\ncluster structure. To address the above issues, we propose a novel multi-view\nsubspace clustering method via simultaneously assigning weights for different\nfeatures and capturing local information of data in view-specific\nself-representation feature spaces. Especially, a common cluster structure\nregularization is adopted to guarantee consistency among different views. An\nefficient algorithm based on an augmented Lagrangian multiplier is also\ndeveloped to solve the associated optimization problem. Experiments conducted\non several benchmark datasets demonstrate that the proposed method achieves\nstate-of-the-art performance. We provide the Matlab code on\nhttps://github.com/Ekin102003/JFLMSC.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:57:57 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Lina", "Shi-Xun", ""], ["Zhongb", "Guo", ""], ["Shu", "Ting", ""]]}, {"id": "2007.12831", "submitter": "Yi Wang", "authors": "Yi Wang, Junhui Hou, Xinyu Hou, and Lap-Pui Chau", "title": "A Self-Training Approach for Point-Supervised Object Detection and\n  Counting in Crowds", "comments": "12 pages. Accepted for Publication at IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3055632", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel self-training approach named Crowd-SDNet\nthat enables a typical object detector trained only with point-level\nannotations (i.e., objects are labeled with points) to estimate both the center\npoints and sizes of crowded objects. Specifically, during training, we utilize\nthe available point annotations to supervise the estimation of the center\npoints of objects directly. Based on a locally-uniform distribution assumption,\nwe initialize pseudo object sizes from the point-level supervisory information,\nwhich are then leveraged to guide the regression of object sizes via a\ncrowdedness-aware loss. Meanwhile, we propose a confidence and order-aware\nrefinement scheme to continuously refine the initial pseudo object sizes such\nthat the ability of the detector is increasingly boosted to detect and count\nobjects in crowds simultaneously. Moreover, to address extremely crowded\nscenes, we propose an effective decoding method to improve the detector's\nrepresentation ability. Experimental results on the WiderFace benchmark show\nthat our approach significantly outperforms state-of-the-art point-supervised\nmethods under both detection and counting tasks, i.e., our method improves the\naverage precision by more than 10% and reduces the counting error by 31.2%.\nBesides, our method obtains the best results on the crowd counting and\nlocalization datasets (i.e., ShanghaiTech and NWPU-Crowd) and vehicle counting\ndatasets (i.e., CARPK and PUCPR+) compared with state-of-the-art\ncounting-by-detection methods. The code will be publicly available at\nhttps://github.com/WangyiNTU/Point-supervised-crowd-detection.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 02:14:42 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 13:24:00 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 07:00:06 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wang", "Yi", ""], ["Hou", "Junhui", ""], ["Hou", "Xinyu", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2007.12858", "submitter": "Di Qiu", "authors": "Di Qiu, Lok Ming Lui", "title": "Modal Uncertainty Estimation via Discrete Latent Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important problems in the real world don't have unique solutions. It is\nthus important for machine learning models to be capable of proposing different\nplausible solutions with meaningful probability measures. In this work we\nintroduce such a deep learning framework that learns the one-to-many mappings\nbetween the inputs and outputs, together with faithful uncertainty measures. We\ncall our framework {\\it modal uncertainty estimation} since we model the\none-to-many mappings to be generated through a set of discrete latent\nvariables, each representing a latent mode hypothesis that explains the\ncorresponding type of input-output relationship. The discrete nature of the\nlatent representations thus allows us to estimate for any input the conditional\nprobability distribution of the outputs very effectively. Both the discrete\nlatent space and its uncertainty estimation are jointly learned during\ntraining. We motivate our use of discrete latent space through the multi-modal\nposterior collapse problem in current conditional generative models, then\ndevelop the theoretical background, and extensively validate our method on both\nsynthetic and realistic tasks. Our framework demonstrates significantly more\naccurate uncertainty estimation than the current state-of-the-art methods, and\nis informative and convenient for practical use.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 05:29:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Qiu", "Di", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2007.12868", "submitter": "Zhengqin Li", "authors": "Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Meng Song, Yuhan Liu,\n  Yu-Ying Yeh, Rui Zhu, Nitesh Gundavarapu, Jia Shi, Sai Bi, Zexiang Xu,\n  Hong-Xing Yu, Kalyan Sunkavalli, Milo\\v{s} Ha\\v{s}an, Ravi Ramamoorthi,\n  Manmohan Chandraker", "title": "OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for creating large-scale photorealistic datasets\nof indoor scenes, with ground truth geometry, material, lighting and semantics.\nOur goal is to make the dataset creation process widely accessible,\ntransforming scans into photorealistic datasets with high-quality ground truth\nfor appearance, layout, semantic labels, high quality spatially-varying BRDF\nand complex lighting, including direct, indirect and visibility components.\nThis enables important applications in inverse rendering, scene understanding\nand robotics. We show that deep networks trained on the proposed dataset\nachieve competitive performance for shape, material and lighting estimation on\nreal images, enabling photorealistic augmented reality applications, such as\nobject insertion and material editing. We also show our semantic labels may be\nused for segmentation and multi-task learning. Finally, we demonstrate that our\nframework may also be integrated with physics engines, to create virtual\nrobotics environments with unique ground truth such as friction coefficients\nand correspondence to real scenes. The dataset and all the tools to create such\ndatasets will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 06:48:47 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 07:32:14 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Zhengqin", ""], ["Yu", "Ting-Wei", ""], ["Sang", "Shen", ""], ["Wang", "Sarah", ""], ["Song", "Meng", ""], ["Liu", "Yuhan", ""], ["Yeh", "Yu-Ying", ""], ["Zhu", "Rui", ""], ["Gundavarapu", "Nitesh", ""], ["Shi", "Jia", ""], ["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Yu", "Hong-Xing", ""], ["Sunkavalli", "Kalyan", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Ramamoorthi", "Ravi", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.12869", "submitter": "Ankit Ravankar", "authors": "Zhaoyu Pan, Takanori Emaru, Ankit Ravankar, Yukinori Kobayashi", "title": "Applying Semantic Segmentation to Autonomous Cars in the Snowy\n  Environment", "comments": "4 pages, 5 Figures", "journal-ref": "36th Annual Conference of the Robot Society of Japan, Nagoya, 2018", "doi": null, "report-no": "18A1853718", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper mainly focuses on environment perception in snowy situations which\nforms the backbone of the autonomous driving technology. For the purpose,\nsemantic segmentation is employed to classify the objects while the vehicle is\ndriven autonomously. We train the Fully Convolutional Networks (FCN) on our own\ndataset and present the experimental results. Finally, the outcomes are\nanalyzed to give a conclusion. It can be concluded that the database still\nneeds to be optimized and a favorable algorithm should be proposed to get\nbetter results.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 07:07:23 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Pan", "Zhaoyu", ""], ["Emaru", "Takanori", ""], ["Ravankar", "Ankit", ""], ["Kobayashi", "Yukinori", ""]]}, {"id": "2007.12881", "submitter": "Trung-Nghia Le", "authors": "Jinnan Yan, Trung-Nghia Le, Khanh-Duy Nguyen, Minh-Triet Tran,\n  Thanh-Toan Do, Tam V. Nguyen", "title": "MirrorNet: Bio-Inspired Camouflaged Object Segmentation", "comments": "Accepted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3064443", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camouflaged objects are generally difficult to be detected in their natural\nenvironment even for human beings. In this paper, we propose a novel\nbio-inspired network, named the MirrorNet, that leverages both instance\nsegmentation and mirror stream for the camouflaged object segmentation.\nDifferently from existing networks for segmentation, our proposed network\npossesses two segmentation streams: the main stream and the mirror stream\ncorresponding with the original image and its flipped image, respectively. The\noutput from the mirror stream is then fused into the main stream's result for\nthe final camouflage map to boost up the segmentation accuracy. Extensive\nexperiments conducted on the public CAMO dataset demonstrate the effectiveness\nof our proposed network. Our proposed method achieves 89% in accuracy,\noutperforming the state-of-the-arts.\n  Project Page: https://sites.google.com/view/ltnghia/research/camo\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 08:31:15 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 14:14:53 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 04:39:35 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Yan", "Jinnan", ""], ["Le", "Trung-Nghia", ""], ["Nguyen", "Khanh-Duy", ""], ["Tran", "Minh-Triet", ""], ["Do", "Thanh-Toan", ""], ["Nguyen", "Tam V.", ""]]}, {"id": "2007.12885", "submitter": "Xinqi Zhu", "authors": "Xinqi Zhu and Chang Xu and Dacheng Tao", "title": "Learning Disentangled Representations with Latent Variation\n  Predictability", "comments": "14 pages, ECCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent traversal is a popular approach to visualize the disentangled latent\nrepresentations. Given a bunch of variations in a single unit of the latent\nrepresentation, it is expected that there is a change in a single factor of\nvariation of the data while others are fixed. However, this impressive\nexperimental observation is rarely explicitly encoded in the objective function\nof learning disentangled representations. This paper defines the variation\npredictability of latent disentangled representations. Given image pairs\ngenerated by latent codes varying in a single dimension, this varied dimension\ncould be closely correlated with these image pairs if the representation is\nwell disentangled. Within an adversarial generation process, we encourage\nvariation predictability by maximizing the mutual information between latent\nvariations and corresponding image pairs. We further develop an evaluation\nmetric that does not rely on the ground-truth generative factors to measure the\ndisentanglement of latent representations. The proposed variation\npredictability is a general constraint that is applicable to the VAE and GAN\nframeworks for boosting disentanglement of latent representations. Experiments\nshow that the proposed variation predictability correlates well with existing\nground-truth-required metrics and the proposed algorithm is effective for\ndisentanglement learning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 08:54:26 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhu", "Xinqi", ""], ["Xu", "Chang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2007.12887", "submitter": "Xinqi Zhu", "authors": "Xinqi Zhu and Chang Xu and Langwen Hui and Cewu Lu and Dacheng Tao", "title": "Approximated Bilinear Modules for Temporal Modeling", "comments": "8 pages, ICCV19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two less-emphasized temporal properties of video: 1. Temporal\ncues are fine-grained; 2. Temporal modeling needs reasoning. To tackle both\nproblems at once, we exploit approximated bilinear modules (ABMs) for temporal\nmodeling. There are two main points making the modules effective: two-layer\nMLPs can be seen as a constraint approximation of bilinear operations, thus can\nbe used to construct deep ABMs in existing CNNs while reusing pretrained\nparameters; frame features can be divided into static and dynamic parts because\nof visual repetition in adjacent frames, which enables temporal modeling to be\nmore efficient. Multiple ABM variants and implementations are investigated,\nfrom high performance to high efficiency. Specifically, we show how two-layer\nsubnets in CNNs can be converted to temporal bilinear modules by adding an\nauxiliary-branch. Besides, we introduce snippet sampling and shifting inference\nto boost sparse-frame video classification performance. Extensive ablation\nstudies are conducted to show the effectiveness of proposed techniques. Our\nmodels can outperform most state-of-the-art methods on Something-Something v1\nand v2 datasets without Kinetics pretraining, and are also competitive on other\nYouTube-like action recognition datasets. Our code is available on\nhttps://github.com/zhuxinqimac/abm-pytorch.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 09:07:35 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhu", "Xinqi", ""], ["Xu", "Chang", ""], ["Hui", "Langwen", ""], ["Lu", "Cewu", ""], ["Tao", "Dacheng", ""]]}, {"id": "2007.12898", "submitter": "Daniel Korat", "authors": "Daniel Korat", "title": "3D Neural Network for Lung Cancer Risk Prediction on CT Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an estimated 160,000 deaths in 2018, lung cancer is the most common\ncause of cancer death in the United States. Lung cancer CT screening has been\nshown to reduce mortality by up to 40% and is now included in US screening\nguidelines. Reducing the high error rates in lung cancer screening is\nimperative because of the high clinical and financial costs caused by diagnosis\nmistakes. Despite the use of standards for radiological diagnosis, persistent\ninter-grader variability and incomplete characterization of comprehensive\nimaging findings remain as limitations of current methods. These limitations\nsuggest opportunities for more sophisticated systems to improve performance and\ninter-reader consistency. In this report, we reproduce a state-of-the-art deep\nlearning algorithm for lung cancer risk prediction. Our model predicts\nmalignancy probability and risk bucket classification from lung CT studies.\nThis allows for risk categorization of patients being screened and suggests the\nmost appropriate surveillance and management. Combining our solution high\naccuracy, consistency and fully automated nature, our approach may enable\nhighly efficient screening procedures and accelerate the adoption of lung\ncancer screening.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 10:01:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Korat", "Daniel", ""]]}, {"id": "2007.12909", "submitter": "Ehsan Nowroozi", "authors": "Mauro Barni, Kassem Kallas, Ehsan Nowroozi, Benedetta Tondi", "title": "CNN Detection of GAN-Generated Face Images based on Cross-Band\n  Co-occurrences Analysis", "comments": "(6 pages, 2 figures, 4 tables), (IEEE International Workshop on\n  Information Forensics and Security - WIFS 2020, New York, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last-generation GAN models allow to generate synthetic images which are\nvisually indistinguishable from natural ones, raising the need to develop tools\nto distinguish fake and natural images thus contributing to preserve the\ntrustworthiness of digital images. While modern GAN models can generate very\nhigh-quality images with no visible spatial artifacts, reconstruction of\nconsistent relationships among colour channels is expectedly more difficult. In\nthis paper, we propose a method for distinguishing GAN-generated from natural\nimages by exploiting inconsistencies among spectral bands, with specific focus\non the generation of synthetic face images. Specifically, we use cross-band\nco-occurrence matrices, in addition to spatial co-occurrence matrices, as input\nto a CNN model, which is trained to distinguish between real and synthetic\nfaces. The results of our experiments confirm the goodness of our approach\nwhich outperforms a similar detection technique based on intra-band spatial\nco-occurrences only. The performance gain is particularly significant with\nregard to robustness against post-processing, like geometric transformations,\nfiltering and contrast manipulations.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 10:55:04 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:43:28 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Barni", "Mauro", ""], ["Kallas", "Kassem", ""], ["Nowroozi", "Ehsan", ""], ["Tondi", "Benedetta", ""]]}, {"id": "2007.12911", "submitter": "Maria Perez-Ortiz", "authors": "Mar\\'ia P\\'erez-Ortiz and Omar Rivasplata and John Shawe-Taylor and\n  Csaba Szepesv\\'ari", "title": "Tighter risk certificates for neural networks", "comments": "Preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an empirical study regarding training probabilistic\nneural networks using training objectives derived from PAC-Bayes bounds. In the\ncontext of probabilistic neural networks, the output of training is a\nprobability distribution over network weights. We present two training\nobjectives, used here for the first time in connection with training neural\nnetworks. These two training objectives are derived from tight PAC-Bayes\nbounds. We also re-implement a previously used training objective based on a\nclassical PAC-Bayes bound, to compare the properties of the predictors learned\nusing the different training objectives. We compute risk certificates that are\nvalid on any unseen examples for the learnt predictors. We further experiment\nwith different types of priors on the weights (both data-free and\ndata-dependent priors) and neural network architectures. Our experiments on\nMNIST and CIFAR-10 show that our training methods produce competitive test set\nerrors and non-vacuous risk bounds with much tighter values than previous\nresults in the literature, showing promise not only to guide the learning\nalgorithm through bounding the risk but also for model selection. These\nobservations suggest that the methods studied here might be good candidates for\nself-certified learning, in the sense of certifying the risk on any unseen data\nwithout the need for data-splitting protocols.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 11:02:16 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 11:09:09 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["P\u00e9rez-Ortiz", "Mar\u00eda", ""], ["Rivasplata", "Omar", ""], ["Shawe-Taylor", "John", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "2007.12918", "submitter": "Bahram Zonooz", "authors": "Hemang Chawla, Matti Jukola, Terence Brouns, Elahe Arani, and Bahram\n  Zonooz", "title": "Crowdsourced 3D Mapping: A Combined Multi-View Geometry and\n  Self-Supervised Learning Approach", "comments": "Accepted at 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to efficiently utilize crowdsourced visual data carries immense\npotential for the domains of large scale dynamic mapping and autonomous\ndriving. However, state-of-the-art methods for crowdsourced 3D mapping assume\nprior knowledge of camera intrinsics. In this work, we propose a framework that\nestimates the 3D positions of semantically meaningful landmarks such as traffic\nsigns without assuming known camera intrinsics, using only monocular color\ncamera and GPS. We utilize multi-view geometry as well as deep learning based\nself-calibration, depth, and ego-motion estimation for traffic sign\npositioning, and show that combining their strengths is important for\nincreasing the map coverage. To facilitate research on this task, we construct\nand make available a KITTI based 3D traffic sign ground truth positioning\ndataset. Using our proposed framework, we achieve an average single-journey\nrelative and absolute positioning accuracy of 39cm and 1.26m respectively, on\nthis dataset.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 12:10:16 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chawla", "Hemang", ""], ["Jukola", "Matti", ""], ["Brouns", "Terence", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2007.12927", "submitter": "Jo\\~ao Sacramento", "authors": "Johannes von Oswald, Seijin Kobayashi, Jo\\~ao Sacramento, Alexander\n  Meulemans, Christian Henning, Benjamin F. Grewe", "title": "Neural networks with late-phase weights", "comments": "25 pages, 6 figures", "journal-ref": "Published as a conference paper at ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largely successful method of training neural networks is to learn their\nweights using some variant of stochastic gradient descent (SGD). Here, we show\nthat the solutions found by SGD can be further improved by ensembling a subset\nof the weights in late stages of learning. At the end of learning, we obtain\nback a single model by taking a spatial average in weight space. To avoid\nincurring increased computational costs, we investigate a family of\nlow-dimensional late-phase weight models which interact multiplicatively with\nthe remaining parameters. Our results show that augmenting standard models with\nlate-phase weights improves generalization in established benchmarks such as\nCIFAR-10/100, ImageNet and enwik8. These findings are complemented with a\ntheoretical analysis of a noisy quadratic problem which provides a simplified\npicture of the late phases of neural network learning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:23:37 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 16:41:19 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 18:38:41 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["von Oswald", "Johannes", ""], ["Kobayashi", "Seijin", ""], ["Sacramento", "Jo\u00e3o", ""], ["Meulemans", "Alexander", ""], ["Henning", "Christian", ""], ["Grewe", "Benjamin F.", ""]]}, {"id": "2007.12928", "submitter": "Peng Zhao", "authors": "Hongying Liu, Zhubo Ruan, Peng Zhao, Chao Dong, Fanhua Shang, Yuanyuan\n  Liu, Linlin Yang", "title": "Video Super Resolution Based on Deep Learning: A Comprehensive Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has made great progress in many fields such as\nimage recognition, natural language processing, speech recognition and video\nsuper-resolution. In this survey, we comprehensively investigate 33\nstate-of-the-art video super-resolution (VSR) methods based on deep learning.\nIt is well known that the leverage of information within video frames is\nimportant for video super-resolution. Thus we propose a taxonomy and classify\nthe methods into six sub-categories according to the ways of utilizing\ninter-frame information. Moreover, the architectures and implementation details\nof all the methods are depicted in detail. Finally, we summarize and compare\nthe performance of the representative VSR method on some benchmark datasets. We\nalso discuss some challenges, which need to be further addressed by researchers\nin the community of VSR. To the best of our knowledge, this work is the first\nsystematic review on VSR tasks, and it is expected to make a contribution to\nthe development of recent studies in this area and potentially deepen our\nunderstanding to the VSR techniques based on deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:39:54 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 05:15:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Liu", "Hongying", ""], ["Ruan", "Zhubo", ""], ["Zhao", "Peng", ""], ["Dong", "Chao", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Yang", "Linlin", ""]]}, {"id": "2007.12935", "submitter": "Adel Hafiane", "authors": "Yanbo Feng, Adel Hafiane, H\\'el\\`ene Laurent", "title": "A deep learning based multiscale approach to segment cancer area in\n  liver whole slide image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of liver cancer segmentation in Whole Slide\nImage (WSI). We propose a multi-scale image processing method based on\nautomatic end-to-end deep neural network algorithm for segmentation of cancer\narea. A seven-levels gaussian pyramid representation of the histopathological\nimage was built to provide the texture information in different scales. In this\nwork, several neural architectures were compared using the original image level\nfor the training procedure. The proposed method is based on U-Net applied to\nseven levels of various resolutions (pyramidal subsumpling). The predictions in\ndifferent levels are combined through a voting mechanism. The final\nsegmentation result is generated at the original image level. Partial color\nnormalization and weighted overlapping method were applied in preprocessing and\nprediction separately. The results show the effectiveness of the proposed\nmulti-scales approach achieving better scores compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:54:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Feng", "Yanbo", ""], ["Hafiane", "Adel", ""], ["Laurent", "H\u00e9l\u00e8ne", ""]]}, {"id": "2007.12942", "submitter": "Peng Su", "authors": "Peng Su, Shixiang Tang, Peng Gao, Di Qiu, Ni Zhao, Xiaogang Wang", "title": "Gradient Regularized Contrastive Learning for Continual Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings can quickly adapt to environmental changes by leveraging\nlearning experience. However, the poor ability of adapting to dynamic\nenvironments remains a major challenge for AI models. To better understand this\nissue, we study the problem of continual domain adaptation, where the model is\npresented with a labeled source domain and a sequence of unlabeled target\ndomains. There are two major obstacles in this problem: domain shifts and\ncatastrophic forgetting. In this work, we propose Gradient Regularized\nContrastive Learning to solve the above obstacles. At the core of our method,\ngradient regularization plays two key roles: (1) enforces the gradient of\ncontrastive loss not to increase the supervised training loss on the source\ndomain, which maintains the discriminative power of learned features; (2)\nregularizes the gradient update on the new domain not to increase the\nclassification loss on the old target domains, which enables the model to adapt\nto an in-coming target domain while preserving the performance of previously\nobserved domains. Hence our method can jointly learn both semantically\ndiscriminative and domain-invariant features with labeled source domain and\nunlabeled target domains. The experiments on Digits, DomainNet and\nOffice-Caltech benchmarks demonstrate the strong performance of our approach\nwhen compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 14:30:03 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Su", "Peng", ""], ["Tang", "Shixiang", ""], ["Gao", "Peng", ""], ["Qiu", "Di", ""], ["Zhao", "Ni", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2007.12944", "submitter": "Rinon Gal", "authors": "Rinon Gal, Amit Bermano, Hao Zhang, Daniel Cohen-Or", "title": "MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part\n  Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MRGAN, a multi-rooted adversarial network which generates\npart-disentangled 3D point-cloud shapes without part-based shape supervision.\nThe network fuses multiple branches of tree-structured graph convolution layers\nwhich produce point clouds, with learnable constant inputs at the tree roots.\nEach branch learns to grow a different shape part, offering control over the\nshape generation at the part level. Our network encourages disentangled\ngeneration of semantic parts via two key ingredients: a root-mixing training\nstrategy which helps decorrelate the different branches to facilitate\ndisentanglement, and a set of loss terms designed with part disentanglement and\nshape semantics in mind. Of these, a novel convexity loss incentivizes the\ngeneration of parts that are more convex, as semantic parts tend to be. In\naddition, a root-dropping loss further ensures that each root seeds a single\npart, preventing the degeneration or over-growth of the point-producing\nbranches. We evaluate the performance of our network on a number of 3D shape\nclasses, and offer qualitative and quantitative comparisons to previous works\nand baseline approaches. We demonstrate the controllability offered by our\npart-disentangled generation through two applications for shape modeling: part\nmixing and individual part variation, without receiving segmented shapes as\ninput.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 14:41:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gal", "Rinon", ""], ["Bermano", "Amit", ""], ["Zhang", "Hao", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2007.12979", "submitter": "Yi Fang", "authors": "Lingjing Wang, Xiang Li, Yi Fang", "title": "GP-Aligner: Unsupervised Non-rigid Groupwise Point Set Registration\n  Based On Optimized Group Latent Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a novel method named GP-Aligner to deal with the\nproblem of non-rigid groupwise point set registration. Compared to previous\nnon-learning approaches, our proposed method gains competitive advantages by\nleveraging the power of deep neural networks to effectively and efficiently\nlearn to align a large number of highly deformed 3D shapes with superior\nperformance. Unlike most learning-based methods that use an explicit feature\nencoding network to extract the per-shape features and their correlations, our\nmodel leverages a model-free learnable latent descriptor to characterize the\ngroup relationship. More specifically, for a given group we first define an\noptimizable Group Latent Descriptor (GLD) to characterize the gruopwise\nrelationship among a group of point sets. Each GLD is randomly initialized from\na Gaussian distribution and then concatenated with the coordinates of each\npoint of the associated point sets in the group. A neural network-based decoder\nis further constructed to predict the coherent drifts as the desired\ntransformation from input groups of shapes to aligned groups of shapes. During\nthe optimization process, GP-Aligner jointly updates all GLDs and weight\nparameters of the decoder network towards the minimization of an unsupervised\ngroupwise alignment loss. After optimization, for each group our model\ncoherently drives each point set towards a middle, common position (shape)\nwithout specifying one as the target. GP-Aligner does not require large-scale\ntraining data for network training and it can directly align groups of point\nsets in a one-stage optimization process. GP-Aligner shows both accuracy and\ncomputational efficiency improvement in comparison with state-of-the-art\nmethods for groupwise point set registration. Moreover, GP-Aligner is shown\ngreat efficiency in aligning a large number of groups of real-world 3D shapes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 17:09:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Lingjing", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2007.13003", "submitter": "Zhenlin Xu", "authors": "Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, Marc Niethammer", "title": "Robust and Generalizable Visual Representation Learning via Random\n  Convolutions", "comments": "ICLR 2021. Code is available at\n  https://github.com/wildphoton/RandConv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While successful for various computer vision tasks, deep neural networks have\nshown to be vulnerable to texture style shifts and small perturbations to which\nhumans are robust. In this work, we show that the robustness of neural networks\ncan be greatly improved through the use of random convolutions as data\naugmentation. Random convolutions are approximately shape-preserving and may\ndistort local textures. Intuitively, randomized convolutions create an infinite\nnumber of new domains with similar global shapes but random local textures.\nTherefore, we explore using outputs of multi-scale random convolutions as new\nimages or mixing them with the original images during training. When applying a\nnetwork trained with our approach to unseen domains, our method consistently\nimproves the performance on domain generalization benchmarks and is scalable to\nImageNet. In particular, in the challenging scenario of generalizing to the\nsketch domain in PACS and to ImageNet-Sketch, our method outperforms\nstate-of-art methods by a large margin. More interestingly, our method can\nbenefit downstream tasks by providing a more robust pretrained visual\nrepresentation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 19:52:25 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 05:42:07 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 16:12:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xu", "Zhenlin", ""], ["Liu", "Deyi", ""], ["Yang", "Junlin", ""], ["Raffel", "Colin", ""], ["Niethammer", "Marc", ""]]}, {"id": "2007.13005", "submitter": "Daniel Kang", "authors": "Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, Matei\n  Zaharia", "title": "Jointly Optimizing Preprocessing and Inference for DNN-based Visual\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks (DNNs) are an increasingly popular way to query\nlarge corpora of data, their significant runtime remains an active area of\nresearch. As a result, researchers have proposed systems and optimizations to\nreduce these costs by allowing users to trade off accuracy and speed. In this\nwork, we examine end-to-end DNN execution in visual analytics systems on modern\naccelerators. Through a novel measurement study, we show that the preprocessing\nof data (e.g., decoding, resizing) can be the bottleneck in many visual\nanalytics systems on modern hardware.\n  To address the bottleneck of preprocessing, we introduce two optimizations\nfor end-to-end visual analytics systems. First, we introduce novel methods of\nachieving accuracy and throughput trade-offs by using natively present,\nlow-resolution visual data. Second, we develop a runtime engine for efficient\nvisual DNN inference. This runtime engine a) efficiently pipelines\npreprocessing and DNN execution for inference, b) places preprocessing\noperations on the CPU or GPU in a hardware- and input-aware manner, and c)\nefficiently manages memory and threading for high throughput execution. We\nimplement these optimizations in a novel system, Smol, and evaluate Smol on\neight visual datasets. We show that its optimizations can achieve up to 5.9x\nend-to-end throughput improvements at a fixed accuracy over recent work in\nvisual analytics.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 20:26:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kang", "Daniel", ""], ["Mathur", "Ankit", ""], ["Veeramacheneni", "Teja", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.13007", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Ximing Lu, Donald Weaver, Joann G. Elmore, Hannaneh\n  Hajishirzi, Linda Shapiro", "title": "HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast\n  Biopsy Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training end-to-end networks for classifying gigapixel size histopathological\nimages is computationally intractable. Most approaches are patch-based and\nfirst learn local representations (patch-wise) before combining these local\nrepresentations to produce image-level decisions. However, dividing large\ntissue structures into patches limits the context available to these networks,\nwhich may reduce their ability to learn representations from clinically\nrelevant structures. In this paper, we introduce a novel attention-based\nnetwork, the Holistic ATtention Network (HATNet) to classify breast biopsy\nimages. We streamline the histopathological image classification pipeline and\nshow how to learn representations from gigapixel size images end-to-end. HATNet\nextends the bag-of-words approach and uses self-attention to encode global\ninformation, allowing it to learn representations from clinically relevant\ntissue structures without any explicit supervision. It outperforms the previous\nbest network Y-Net, which uses supervision in the form of tissue-level\nsegmentation masks, by 8%. Importantly, our analysis reveals that HATNet learns\nrepresentations from clinically relevant structures, and it matches the\nclassification accuracy of human pathologists for this challenging test set.\nOur source code is available at \\url{https://github.com/sacmehta/HATNet}\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 20:42:21 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mehta", "Sachin", ""], ["Lu", "Ximing", ""], ["Weaver", "Donald", ""], ["Elmore", "Joann G.", ""], ["Hajishirzi", "Hannaneh", ""], ["Shapiro", "Linda", ""]]}, {"id": "2007.13010", "submitter": "Eddie Huang", "authors": "Eddie Huang, Sahil Gupta", "title": "Style is a Distribution of Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer (NST) is a powerful image generation technique that\nuses a convolutional neural network (CNN) to merge the content of one image\nwith the style of another. Contemporary methods of NST use first or second\norder statistics of the CNN's features to achieve transfers with relatively\nlittle computational cost. However, these methods cannot fully extract the\nstyle from the CNN's features. We present a new algorithm for style transfer\nthat fully extracts the style from the features by redefining the style loss as\nthe Wasserstein distance between the distribution of features. Thus, we set a\nnew standard in style transfer quality. In addition, we state two important\ninterpretations of NST. The first is a re-emphasis from Li et al., which states\nthat style is simply the distribution of features. The second states that NST\nis a type of generative adversarial network (GAN) problem.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 21:17:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Huang", "Eddie", ""], ["Gupta", "Sahil", ""]]}, {"id": "2007.13034", "submitter": "Weicheng Kuo", "authors": "Weicheng Kuo, Anelia Angelova, Tsung-Yi Lin, Angela Dai", "title": "Mask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve", "comments": "ECCV 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition has seen significant progress in the image domain, with\nfocus primarily on 2D perception. We propose to leverage existing large-scale\ndatasets of 3D models to understand the underlying 3D structure of objects seen\nin an image by constructing a CAD-based representation of the objects and their\nposes. We present Mask2CAD, which jointly detects objects in real-world images\nand for each detected object, optimizes for the most similar CAD model and its\npose. We construct a joint embedding space between the detected regions of an\nimage corresponding to an object and 3D CAD models, enabling retrieval of CAD\nmodels for an input RGB image. This produces a clean, lightweight\nrepresentation of the objects in an image; this CAD-based representation\nensures a valid, efficient shape representation for applications such as\ncontent creation or interactive scenarios, and makes a step towards\nunderstanding the transformation of real-world imagery to a synthetic domain.\nExperiments on real-world images from Pix3D demonstrate the advantage of our\napproach in comparison to state of the art. To facilitate future research, we\nadditionally propose a new image-to-3D baseline on ScanNet which features\nlarger shape diversity, real-world occlusions, and challenging image views.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 00:08:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kuo", "Weicheng", ""], ["Angelova", "Anelia", ""], ["Lin", "Tsung-Yi", ""], ["Dai", "Angela", ""]]}, {"id": "2007.13044", "submitter": "Xavier-Lewis Palmer", "authors": "Akwarandu Ugo Nwachuku, Xavier Lewis-Palmer, Darlington Ahiale Akogo", "title": "A Preliminary Exploration into an Alternative CellLineNet: An\n  Evolutionary Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Within this paper, the exploration of an evolutionary approach to an\nalternative CellLineNet: a convolutional neural network adept at the\nclassification of epithelial breast cancer cell lines, is presented. This\nevolutionary algorithm introduces control variables that guide the search of\narchitectures in the search space of inverted residual blocks, bottleneck\nblocks, residual blocks and a basic 2x2 convolutional block. The promise of\nEvoCELL is predicting what combination or arrangement of the feature extracting\nblocks that produce the best model architecture for a given task. Therein, the\nperformance of how the fittest model evolved after each generation is shown.\nThe final evolved model CellLineNet V2 classifies 5 types of epithelial breast\ncell lines consisting of two human cancer lines, 2 normal immortalized lines,\nand 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A and HC11). The\nMulticlass Cell Line Classification Convolutional Neural Network extends our\nearlier work on a Binary Breast Cancer Cell Line Classification model. This\npaper presents an on-going exploratory approach to neural network architecture\ndesign and is presented for further study.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 02:36:56 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Nwachuku", "Akwarandu Ugo", ""], ["Lewis-Palmer", "Xavier", ""], ["Akogo", "Darlington Ahiale", ""]]}, {"id": "2007.13049", "submitter": "Rui Xiang", "authors": "Rui Xiang, Rongjie Lai, Hongkai Zhao", "title": "A Dual Iterative Refinement Method for Non-rigid Shape Matching", "comments": "9 pages, 9 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a simple and efficient dual iterative refinement (DIR) method\nis proposed for dense correspondence between two nearly isometric shapes. The\nkey idea is to use dual information, such as spatial and spectral, or local and\nglobal features, in a complementary and effective way, and extract more\naccurate information from current iteration to use for the next iteration. In\neach DIR iteration, starting from current correspondence, a zoom-in process at\neach point is used to select well matched anchor pairs by a local mapping\ndistortion criterion. These selected anchor pairs are then used to align\nspectral features (or other appropriate global features) whose dimension\nadaptively matches the capacity of the selected anchor pairs. Thanks to the\neffective combination of complementary information in a data-adaptive way, DIR\nis not only efficient but also robust to render accurate results within a few\niterations. By choosing appropriate dual features, DIR has the flexibility to\nhandle patch and partial matching as well. Extensive experiments on various\ndata sets demonstrate the superiority of DIR over other state-of-the-art\nmethods in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 03:35:37 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 07:49:03 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Xiang", "Rui", ""], ["Lai", "Rongjie", ""], ["Zhao", "Hongkai", ""]]}, {"id": "2007.13072", "submitter": "WanHong Huang", "authors": "Wanhong Huang, Rui Geng", "title": "Approaches of large-scale images recognition with more than 50,000\n  categoris", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though current CV models have been able to achieve high levels of accuracy on\nsmall-scale images classification dataset with hundreds or thousands of\ncategories, many models become infeasible in computational or space consumption\nwhen it comes to large-scale dataset with more than 50,000 categories. In this\npaper, we provide a viable solution for classifying large-scale species\ndatasets using traditional CV techniques such as.features extraction and\nprocessing, BOVW(Bag of Visual Words) and some statistical learning technics\nlike Mini-Batch K-Means,SVM which are used in our works. And then mixed with a\nneural network model. When applying these techniques, we have done some\noptimization in time and memory consumption, so that it can be feasible for\nlarge-scale dataset. And we also use some technics to reduce the impact of\nmislabeling data. We use a dataset with more than 50, 000 categories, and all\noperations are done on common computer with l 6GB RAM and a CPU of 3. OGHz. Our\ncontributions are: 1) analysis what problems may meet in the training\nprocesses, and presents several feasible ways to solve these problems. 2) Make\ntraditional CV models combined with neural network models provide some feasible\nscenarios for training large-scale classified datasets within the constraints\nof time and spatial resources.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 07:33:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Huang", "Wanhong", ""], ["Geng", "Rui", ""]]}, {"id": "2007.13078", "submitter": "Sriram Nochur Narayanan", "authors": "Sriram N N, Buyu Liu, Francesco Pittaluga, Manmohan Chandraker", "title": "SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose advances that address two key challenges in future trajectory\nprediction: (i) multimodality in both training data and predictions and (ii)\nconstant time inference regardless of number of agents. Existing trajectory\npredictions are fundamentally limited by lack of diversity in training data,\nwhich is difficult to acquire with sufficient coverage of possible modes. Our\nfirst contribution is an automatic method to simulate diverse trajectories in\nthe top-view. It uses pre-existing datasets and maps as initialization, mines\nexisting trajectories to represent realistic driving behaviors and uses a\nmulti-agent vehicle dynamics simulator to generate diverse new trajectories\nthat cover various modes and are consistent with scene layout constraints. Our\nsecond contribution is a novel method that generates diverse predictions while\naccounting for scene semantics and multi-agent interactions, with constant-time\ninference independent of the number of agents. We propose a convLSTM with novel\nstate pooling operations and losses to predict scene-consistent states of\nmultiple agents in a single forward pass, along with a CVAE for diversity. We\nvalidate our proposed multi-agent trajectory prediction approach by training\nand testing on the proposed simulated dataset and existing real datasets of\ntraffic scenes. In both cases, our approach outperforms SOTA methods by a large\nmargin, highlighting the benefits of both our diverse dataset simulation and\nconstant-time diverse trajectory prediction methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 08:17:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["N", "Sriram N", ""], ["Liu", "Buyu", ""], ["Pittaluga", "Francesco", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.13083", "submitter": "Li Rui", "authors": "Rui Li, Chenxi Duan, Shunyi Zheng, Ce Zhang and Peter M. Atkinson", "title": "MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed\n  Images", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2021.3052886", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of remotely sensed images plays an important role in\nland resource management, yield estimation, and economic assessment. U-Net, a\ndeep encoder-decoder architecture, has been used frequently for image\nsegmentation with high accuracy. In this Letter, we incorporate multi-scale\nfeatures generated by different layers of U-Net and design a multi-scale skip\nconnected and asymmetric-convolution-based U-Net (MACU-Net), for segmentation\nusing fine-resolution remotely sensed images. Our design has the following\nadvantages: (1) The multi-scale skip connections combine and realign semantic\nfeatures contained in both low-level and high-level feature maps; (2) the\nasymmetric convolution block strengthens the feature representation and feature\nextraction capability of a standard convolution layer. Experiments conducted on\ntwo remotely sensed datasets captured by different satellite sensors\ndemonstrate that the proposed MACU-Net transcends the U-Net, U-NetPPL, U-Net\n3+, amongst other benchmark approaches. Code is available at\nhttps://github.com/lironui/MACU-Net.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 08:56:47 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 02:52:23 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Rui", ""], ["Duan", "Chenxi", ""], ["Zheng", "Shunyi", ""], ["Zhang", "Ce", ""], ["Atkinson", "Peter M.", ""]]}, {"id": "2007.13092", "submitter": "Chenjie Wang", "authors": "Chenjie Wang and Chengyuan Li and Bin Luo", "title": "U2-ONet: A Two-level Nested Octave U-structure with Multiscale Attention\n  Mechanism for Moving Instances Segmentation", "comments": "10 pages, 7 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most scenes in practical applications are dynamic scenes containing moving\nobjects, so segmenting accurately moving objects is crucial for many computer\nvision applications. In order to efficiently segment out all moving objects in\nthe scene, regardless of whether the object has a predefined semantic label, we\npropose a two-level nested Octave U-structure network with a multiscale\nattention mechanism called U2-ONet. Each stage of U2-ONet is filled with our\nnewly designed Octave ReSidual U-block (ORSU) to enhance the ability to obtain\nmore context information at different scales while reducing spatial redundancy\nof feature maps. In order to efficiently train our multi-scale deep network, we\nintroduce a hierarchical training supervision strategy that calculates the loss\nat each level while adding a knowledge matching loss to keep the optimization\nconsistency. Experimental results show that our method achieves\nstate-of-the-art performance in several general moving objects segmentation\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 10:12:42 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Chenjie", ""], ["Li", "Chengyuan", ""], ["Luo", "Bin", ""]]}, {"id": "2007.13098", "submitter": "Hongtao Yang", "authors": "Hongtao Yang, Tong Zhang, Wenbing Huang, Xuming He, Fatih Porikli", "title": "Towards Purely Unsupervised Disentanglement of Appearance and Shape for\n  Person Images Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been a fairly of research interests in exploring the\ndisentanglement of appearance and shape from human images. Most existing\nendeavours pursuit this goal by either using training images with annotations\nor regulating the training process with external clues such as human skeleton,\nbody segmentation or cloth patches etc. In this paper, we aim to address this\nchallenge in a more unsupervised manner---we do not require any annotation nor\nany external task-specific clues. To this end, we formulate an\nencoder-decoder-like network to extract both the shape and appearance features\nfrom input images at the same time, and train the parameters by three losses:\nfeature adversarial loss, color consistency loss and reconstruction loss. The\nfeature adversarial loss mainly impose little to none mutual information\nbetween the extracted shape and appearance features, while the color\nconsistency loss is to encourage the invariance of person appearance\nconditioned on different shapes. More importantly, our unsupervised\n(Unsupervised learning has many interpretations in different tasks. To be\nclear, in this paper, we refer unsupervised learning as learning without\ntask-specific human annotations, pairs or any form of weak supervision.)\nframework utilizes learned shape features as masks which are applied to the\ninput itself in order to obtain clean appearance features. Without using fixed\ninput human skeleton, our network better preserves the conditional human\nposture while requiring less supervision. Experimental results on DeepFashion\nand Market1501 demonstrate that the proposed method achieves clean\ndisentanglement and is able to synthesis novel images of comparable quality\nwith state-of-the-art weakly-supervised or even supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 10:56:37 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 00:49:59 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yang", "Hongtao", ""], ["Zhang", "Tong", ""], ["Huang", "Wenbing", ""], ["He", "Xuming", ""], ["Porikli", "Fatih", ""]]}, {"id": "2007.13101", "submitter": "Renlong Jie", "authors": "Renlong Jie, Junbin Gao, Andrey Vasnev, Min-ngoc Tran", "title": "Regularized Flexible Activation Function Combinations for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation in deep neural networks is fundamental to achieving non-linear\nmappings. Traditional studies mainly focus on finding fixed activations for a\nparticular set of learning tasks or model architectures. The research on\nflexible activation is quite limited in both designing philosophy and\napplication scenarios. In this study, three principles of choosing flexible\nactivation components are proposed and a general combined form of flexible\nactivation functions is implemented. Based on this, a novel family of flexible\nactivation functions that can replace sigmoid or tanh in LSTM cells are\nimplemented, as well as a new family by combining ReLU and ELUs. Also, two new\nregularisation terms based on assumptions as prior knowledge are introduced. It\nhas been shown that LSTM models with proposed flexible activations P-Sig-Ramp\nprovide significant improvements in time series forecasting, while the proposed\nP-E2-ReLU achieves better and more stable performance on lossy image\ncompression tasks with convolutional auto-encoders. In addition, the proposed\nregularization terms improve the convergence, performance and stability of the\nmodels with flexible activation functions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 11:32:52 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 13:45:49 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Jie", "Renlong", ""], ["Gao", "Junbin", ""], ["Vasnev", "Andrey", ""], ["Tran", "Min-ngoc", ""]]}, {"id": "2007.13106", "submitter": "Sohaib Younis", "authors": "Sohaib Younis, Marco Schmidt, Claus Weiland, Stefan Dressler, Bernhard\n  Seeger, Thomas Hickler", "title": "Detection and Annotation of Plant Organs from Digitized Herbarium Scans\n  using Deep Learning", "comments": "Replaced version due to change in location of supplement dataset. For\n  supplementary dataset go to: https://doi.pangaea.de/10.1594/PANGAEA.920895\n  Article submitted to Biodiversity Data Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As herbarium specimens are increasingly becoming digitized and accessible in\nonline repositories, advanced computer vision techniques are being used to\nextract information from them. The presence of certain plant organs on\nherbarium sheets is useful information in various scientific contexts and\nautomatic recognition of these organs will help mobilize such information. In\nour study we use deep learning to detect plant organs on digitized herbarium\nspecimens with Faster R-CNN. For our experiment we manually annotated hundreds\nof herbarium scans with thousands of bounding boxes for six types of plant\norgans and used them for training and evaluating the plant organ detection\nmodel. The model worked particularly well on leaves and stems, while flowers\nwere also present in large numbers in the sheets, but not equally well\nrecognized.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 11:50:50 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 14:02:44 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Younis", "Sohaib", ""], ["Schmidt", "Marco", ""], ["Weiland", "Claus", ""], ["Dressler", "Stefan", ""], ["Seeger", "Bernhard", ""], ["Hickler", "Thomas", ""]]}, {"id": "2007.13118", "submitter": "Md Sahidullah", "authors": "Md Sahidullah, Achintya Kumar Sarkar, Ville Vestman, Xuechen Liu,\n  Romain Serizel, Tomi Kinnunen, Zheng-Hua Tan, Emmanuel Vincent", "title": "UIAI System for Short-Duration Speaker Verification Challenge 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the system description of the UIAI entry for the\nshort-duration speaker verification (SdSV) challenge 2020. Our focus is on Task\n1 dedicated to text-dependent speaker verification. We investigate different\nfeature extraction and modeling approaches for automatic speaker verification\n(ASV) and utterance verification (UV). We have also studied different fusion\nstrategies for combining UV and ASV modules. Our primary submission to the\nchallenge is the fusion of seven subsystems which yields a normalized minimum\ndetection cost function (minDCF) of 0.072 and an equal error rate (EER) of\n2.14% on the evaluation set. The single system consisting of a pass-phrase\nidentification based model with phone-discriminative bottleneck features gives\na normalized minDCF of 0.118 and achieves 19% relative improvement over the\nstate-of-the-art challenge baseline.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:32:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Sahidullah", "Md", ""], ["Sarkar", "Achintya Kumar", ""], ["Vestman", "Ville", ""], ["Liu", "Xuechen", ""], ["Serizel", "Romain", ""], ["Kinnunen", "Tomi", ""], ["Tan", "Zheng-Hua", ""], ["Vincent", "Emmanuel", ""]]}, {"id": "2007.13119", "submitter": "Zhen Lei", "authors": "Chubin Zhuang and Zhen Lei and Stan Z. Li", "title": "SADet: Learning An Efficient and Accurate Pedestrian Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the anchor-based detectors have taken a big step forward in\npedestrian detection, the overall performance of algorithm still needs further\nimprovement for practical applications, \\emph{e.g.}, a good trade-off between\nthe accuracy and efficiency. To this end, this paper proposes a series of\nsystematic optimization strategies for the detection pipeline of one-stage\ndetector, forming a single shot anchor-based detector (SADet) for efficient and\naccurate pedestrian detection, which includes three main improvements. Firstly,\nwe optimize the sample generation process by assigning soft tags to the outlier\nsamples to generate semi-positive samples with continuous tag value between $0$\nand $1$, which not only produces more valid samples, but also strengthens the\nrobustness of the model. Secondly, a novel Center-$IoU$ loss is applied as a\nnew regression loss for bounding box regression, which not only retains the\ngood characteristics of IoU loss, but also solves some defects of it. Thirdly,\nwe also design Cosine-NMS for the postprocess of predicted bounding boxes, and\nfurther propose adaptive anchor matching to enable the model to adaptively\nmatch the anchor boxes to full or visible bounding boxes according to the\ndegree of occlusion, making the NMS and anchor matching algorithms more\nsuitable for occluded pedestrian detection. Though structurally simple, it\npresents state-of-the-art result and real-time speed of $20$ FPS for\nVGA-resolution images ($640 \\times 480$) on challenging pedestrian detection\nbenchmarks, i.e., CityPersons, Caltech, and human detection benchmark\nCrowdHuman, leading to a new attractive pedestrian detector.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:32:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhuang", "Chubin", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "2007.13120", "submitter": "Seonwook Park", "authors": "Seonwook Park and Emre Aksan and Xucong Zhang and Otmar Hilliges", "title": "Towards End-to-end Video-based Eye-Tracking", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating eye-gaze from images alone is a challenging task, in large parts\ndue to un-observable person-specific factors. Achieving high accuracy typically\nrequires labeled data from test users which may not be attainable in real\napplications. We observe that there exists a strong relationship between what\nusers are looking at and the appearance of the user's eyes. In response to this\nunderstanding, we propose a novel dataset and accompanying method which aims to\nexplicitly learn these semantic and temporal relationships. Our video dataset\nconsists of time-synchronized screen recordings, user-facing camera views, and\neye gaze data, which allows for new benchmarks in temporal gaze tracking as\nwell as label-free refinement of gaze. Importantly, we demonstrate that the\nfusion of information from visual stimuli as well as eye images can lead\ntowards achieving performance similar to literature-reported figures acquired\nthrough supervised personalization. Our final method yields significant\nperformance improvements on our proposed EVE dataset, with up to a 28 percent\nimprovement in Point-of-Gaze estimates (resulting in 2.49 degrees in angular\nerror), paving the path towards high-accuracy screen-based eye tracking purely\nfrom webcam sensors. The dataset and reference source code are available at\nhttps://ait.ethz.ch/projects/2020/EVE\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:39:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Park", "Seonwook", ""], ["Aksan", "Emre", ""], ["Zhang", "Xucong", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2007.13124", "submitter": "Lei Ke", "authors": "Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, Chi-Keung Tang", "title": "GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and\n  Scene-aware Supervision", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end framework named as GSNet (Geometric and\nScene-aware Network), which jointly estimates 6DoF poses and reconstructs\ndetailed 3D car shapes from single urban street view. GSNet utilizes a unique\nfour-way feature extraction and fusion scheme and directly regresses 6DoF poses\nand shapes in a single forward pass. Extensive experiments show that our\ndiverse feature extraction and fusion scheme can greatly improve model\nperformance. Based on a divide-and-conquer 3D shape representation strategy,\nGSNet reconstructs 3D vehicle shape with great detail (1352 vertices and 2700\nfaces). This dense mesh representation further leads us to consider geometrical\nconsistency and scene context, and inspires a new multi-objective loss function\nto regularize network training, which in turn improves the accuracy of 6D pose\nestimation and validates the merit of jointly performing both tasks. We\nevaluate GSNet on the largest multi-task ApolloCar3D benchmark and achieve\nstate-of-the-art performance both quantitatively and qualitatively. Project\npage is available at https://lkeab.github.io/gsnet/.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 13:05:55 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ke", "Lei", ""], ["Li", "Shichao", ""], ["Sun", "Yanan", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2007.13135", "submitter": "Peng Gao", "authors": "Lei Shi, Kai Shuang, Shijie Geng, Peng Su, Zhengkai Jiang, Peng Gao,\n  Zuohui Fu, Gerard de Melo, Sen Su", "title": "Contrastive Visual-Linguistic Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several multi-modality representation learning approaches such as LXMERT and\nViLBERT have been proposed recently. Such approaches can achieve superior\nperformance due to the high-level semantic information captured during\nlarge-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual\nregion regression and classification loss, they often suffer from domain gap\nand noisy label problems, based on the visual features having been pretrained\non the Visual Genome dataset. To overcome these issues, we propose unbiased\nContrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual\nself-supervised loss built upon contrastive learning. We evaluate CVLP on\nseveral down-stream tasks, including VQA, GQA and NLVR2 to validate the\nsuperiority of contrastive learning on multi-modality representation learning.\nOur code is available at: https://github.com/ArcherYunDong/CVLP-.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 14:26:18 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Shi", "Lei", ""], ["Shuang", "Kai", ""], ["Geng", "Shijie", ""], ["Su", "Peng", ""], ["Jiang", "Zhengkai", ""], ["Gao", "Peng", ""], ["Fu", "Zuohui", ""], ["de Melo", "Gerard", ""], ["Su", "Sen", ""]]}, {"id": "2007.13138", "submitter": "Abhijit Kundu", "authors": "Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian\n  Brewington, Thomas Funkhouser, Caroline Pantofaru", "title": "Virtual Multi-view Fusion for 3D Semantic Segmentation", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of 3D meshes is an important problem for 3D scene\nunderstanding. In this paper we revisit the classic multiview representation of\n3D meshes and study several techniques that make them effective for 3D semantic\nsegmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our\nmethod effectively chooses different virtual views of the 3D mesh and renders\nmultiple 2D channels for training an effective 2D semantic segmentation model.\nFeatures from multiple per view predictions are finally fused on 3D mesh\nvertices to predict mesh semantic segmentation labels. Using the large scale\nindoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual\nviews enable more effective training of 2D semantic segmentation networks than\nprevious multiview approaches. When the 2D per pixel predictions are aggregated\non 3D surfaces, our virtual multiview fusion method is able to achieve\nsignificantly better 3D semantic segmentation results compared to all prior\nmultiview approaches and competitive with recent 3D convolution approaches.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 14:46:55 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kundu", "Abhijit", ""], ["Yin", "Xiaoqi", ""], ["Fathi", "Alireza", ""], ["Ross", "David", ""], ["Brewington", "Brian", ""], ["Funkhouser", "Thomas", ""], ["Pantofaru", "Caroline", ""]]}, {"id": "2007.13143", "submitter": "Chenglong Li", "authors": "Chenglong Li, Lei Liu, Andong Lu, Qing Ji, and Jin Tang", "title": "Challenge-Aware RGBT Tracking", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB and thermal source data suffer from both shared and specific challenges,\nand how to explore and exploit them plays a critical role to represent the\ntarget appearance in RGBT tracking. In this paper, we propose a novel\nchallenge-aware neural network to handle the modality-shared challenges (e.g.,\nfast motion, scale variation and occlusion) and the modality-specific ones\n(e.g., illumination variation and thermal crossover) for RGBT tracking. In\nparticular, we design several parameter-shared branches in each layer to model\nthe target appearance under the modality-shared challenges, and several\nparameterindependent branches under the modality-specific ones. Based on the\nobservation that the modality-specific cues of different modalities usually\ncontains the complementary advantages, we propose a guidance module to transfer\ndiscriminative features from one modality to another one, which could enhance\nthe discriminative ability of some weak modality. Moreover, all branches are\naggregated together in an adaptive manner and parallel embedded in the backbone\nnetwork to efficiently form more discriminative target representations. These\nchallenge-aware branches are able to model the target appearance under certain\nchallenges so that the target representations can be learnt by a few parameters\neven in the situation of insufficient training data. From the experimental\nresults we will show that our method operates at a real-time speed while\nperforming well against the state-of-the-art methods on three benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 15:11:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Chenglong", ""], ["Liu", "Lei", ""], ["Lu", "Andong", ""], ["Ji", "Qing", ""], ["Tang", "Jin", ""]]}, {"id": "2007.13145", "submitter": "Guanying Chen", "authors": "Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K.\n  Wong", "title": "Deep Photometric Stereo for Non-Lambertian Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of photometric stereo, in both calibrated\nand uncalibrated scenarios, for non-Lambertian surfaces based on deep learning.\nWe first introduce a fully convolutional deep network for calibrated\nphotometric stereo, which we call PS-FCN. Unlike traditional approaches that\nadopt simplified reflectance models to make the problem tractable, our method\ndirectly learns the mapping from reflectance observations to surface normal,\nand is able to handle surfaces with general and unknown isotropic reflectance.\nAt test time, PS-FCN takes an arbitrary number of images and their associated\nlight directions as input and predicts a surface normal map of the scene in a\nfast feed-forward pass. To deal with the uncalibrated scenario where light\ndirections are unknown, we introduce a new convolutional network, named LCNet,\nto estimate light directions from input images. The estimated light directions\nand the input images are then fed to PS-FCN to determine the surface normals.\nOur method does not require a pre-defined set of light directions and can\nhandle multiple images in an order-agnostic manner. Thorough evaluation of our\napproach on both synthetic and real datasets shows that it outperforms\nstate-of-the-art methods in both calibrated and uncalibrated scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 15:20:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Guanying", ""], ["Han", "Kai", ""], ["Shi", "Boxin", ""], ["Matsushita", "Yasuyuki", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "2007.13172", "submitter": "Giorgos Tolias", "authors": "Giorgos Tolias, Tomas Jenicek, Ond\\v{r}ej Chum", "title": "Learning and aggregating deep local descriptors for instance-level\n  recognition", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method to learn deep local descriptors for\ninstance-level recognition. The training only requires examples of positive and\nnegative image pairs and is performed as metric learning of sum-pooled global\nimage descriptors. At inference, the local descriptors are provided by the\nactivations of internal components of the network. We demonstrate why such an\napproach learns local descriptors that work well for image similarity\nestimation with classical efficient match kernel methods. The experimental\nvalidation studies the trade-off between performance and memory requirements of\nthe state-of-the-art image search approach based on match kernels. Compared to\nexisting local descriptors, the proposed ones perform better in two\ninstance-level recognition tasks and keep memory requirements lower. We\nexperimentally show that global descriptors are not effective enough at large\nscale and that local descriptors are essential. We achieve state-of-the-art\nperformance, in some cases even with a backbone network as small as ResNet18.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 16:30:56 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Tolias", "Giorgos", ""], ["Jenicek", "Tomas", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "2007.13215", "submitter": "Weifeng Chen", "authors": "Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton,\n  Jia Deng", "title": "OASIS: A Large-Scale Dataset for Single Image 3D in the Wild", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view 3D is the task of recovering 3D properties such as depth and\nsurface normals from a single image. We hypothesize that a major obstacle to\nsingle-image 3D is data. We address this issue by presenting Open Annotations\nof Single Image Surfaces (OASIS), a dataset for single-image 3D in the wild\nconsisting of annotations of detailed 3D geometry for 140,000 images. We train\nand evaluate leading models on a variety of single-image 3D tasks. We expect\nOASIS to be a useful resource for 3D vision research. Project site:\nhttps://pvl.cs.princeton.edu/OASIS.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 20:46:41 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Weifeng", ""], ["Qian", "Shengyi", ""], ["Fan", "David", ""], ["Kojima", "Noriyuki", ""], ["Hamilton", "Max", ""], ["Deng", "Jia", ""]]}, {"id": "2007.13224", "submitter": "Md Hasib Zunair", "authors": "Hasib Zunair, Aimon Rahman, Nabeel Mohammed, Joseph Paul Cohen", "title": "Uniformizing Techniques to Process CT scans with 3D CNNs for\n  Tuberculosis Prediction", "comments": "Accepted for publication at the MICCAI 2020 International Workshop on\n  PRedictive Intelligence In MEdicine (PRIME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to medical image analysis on volumetric data uses deep 2D\nconvolutional neural networks (CNNs). This is largely attributed to the\nchallenges imposed by the nature of the 3D data: variable volume size, GPU\nexhaustion during optimization. However, dealing with the individual slices\nindependently in 2D CNNs deliberately discards the depth information which\nresults in poor performance for the intended task. Therefore, it is important\nto develop methods that not only overcome the heavy memory and computation\nrequirements but also leverage the 3D information. To this end, we evaluate a\nset of volume uniformizing methods to address the aforementioned issues. The\nfirst method involves sampling information evenly from a subset of the volume.\nAnother method exploits the full geometry of the 3D volume by interpolating\nover the z-axis. We demonstrate performance improvements using controlled\nablation studies as well as put this approach to the test on the ImageCLEF\nTuberculosis Severity Assessment 2019 benchmark. We report 73% area under curve\n(AUC) and binary classification accuracy (ACC) of 67.5% on the test set beating\nall methods which leveraged only image information (without using clinical\nmeta-data) achieving 5-th position overall. All codes and models are made\navailable at https://github.com/hasibzunair/uniformizing-3D.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 21:53:47 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zunair", "Hasib", ""], ["Rahman", "Aimon", ""], ["Mohammed", "Nabeel", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "2007.13249", "submitter": "Peixian Chen", "authors": "Peixian Chen, Pingyang Dai, Jianzhuang Liu, Feng Zheng, Qi Tian,\n  Rongrong Ji", "title": "Dual Distribution Alignment Network for Generalizable Person\n  Re-Identification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) serves as a promising solution to handle person\nRe-Identification (Re-ID), which trains the model using labels from the source\ndomain alone, and then directly adopts the trained model to the target domain\nwithout model updating. However, existing DG approaches are usually disturbed\nby serious domain variations due to significant dataset variations.\nSubsequently, DG highly relies on designing domain-invariant features, which is\nhowever not well exploited, since most existing approaches directly mix\nmultiple datasets to train DG based models without considering the local\ndataset similarities, i.e., examples that are very similar but from different\ndomains. In this paper, we present a Dual Distribution Alignment Network\n(DDAN), which handles this challenge by mapping images into a domain-invariant\nfeature space by selectively aligning distributions of multiple source domains.\nSuch an alignment is conducted by dual-level constraints, i.e., the domain-wise\nadversarial feature learning and the identity-wise similarity enhancement. We\nevaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID)\nbenchmark. Quantitative results demonstrate that the proposed DDAN can well\nalign the distributions of various source domains, and significantly\noutperforms all existing domain generalization approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:08:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Peixian", ""], ["Dai", "Pingyang", ""], ["Liu", "Jianzhuang", ""], ["Zheng", "Feng", ""], ["Tian", "Qi", ""], ["Ji", "Rongrong", ""]]}, {"id": "2007.13251", "submitter": "Bas Peters", "authors": "Bas Peters", "title": "Point-to-set distance functions for weakly supervised segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When pixel-level masks or partial annotations are not available for training\nneural networks for semantic segmentation, it is possible to use higher-level\ninformation in the form of bounding boxes, or image tags. In the imaging\nsciences, many applications do not have an object-background structure and\nbounding boxes are not available. Any available annotation typically comes from\nground truth or domain experts. A direct way to train without masks is using\nprior knowledge on the size of objects/classes in the segmentation. We present\na new algorithm to include such information via constraints on the network\noutput, implemented via projection-based point-to-set distance functions. This\ntype of distance functions always has the same functional form of the\nderivative, and avoids the need to adapt penalty functions to different\nconstraints, as well as issues related to constraining properties typically\nassociated with non-differentiable functions. Whereas object size information\nis known to enable object segmentation from bounding boxes from datasets with\nmany general and medical images, we show that the applications extend to the\nimaging sciences where data represents indirect measurements, even in the case\nof single examples. We illustrate the capabilities in case of a) one or more\nclasses do not have any annotation; b) there is no annotation at all; c) there\nare bounding boxes. We use data for hyperspectral time-lapse imaging, object\nsegmentation in corrupted images, and sub-surface aquifer mapping from\nairborne-geophysical remote-sensing data. The examples verify that the\ndeveloped methodology alleviates difficulties with annotating non-visual\nimagery for a range of experimental settings.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:15:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Peters", "Bas", ""]]}, {"id": "2007.13262", "submitter": "Siwen Luo", "authors": "Siwen Luo, Soyeon Caren Han, Kaiyuan Sun and Josiah Poon", "title": "REXUP: I REason, I EXtract, I UPdate with Structured Compositional\n  Reasoning for Visual Question Answering", "comments": "Accepted by ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) is a challenging multi-modal task that\nrequires not only the semantic understanding of both images and questions, but\nalso the sound perception of a step-by-step reasoning process that would lead\nto the correct answer. So far, most successful attempts in VQA have been\nfocused on only one aspect, either the interaction of visual pixel features of\nimages and word features of questions, or the reasoning process of answering\nthe question in an image with simple objects. In this paper, we propose a deep\nreasoning VQA model with explicit visual structure-aware textual information,\nand it works well in capturing step-by-step reasoning process and detecting a\ncomplex object-relationship in photo-realistic images. REXUP network consists\nof two branches, image object-oriented and scene graph oriented, which jointly\nworks with super-diagonal fusion compositional attention network. We\nquantitatively and qualitatively evaluate REXUP on the GQA dataset and conduct\nextensive ablation studies to explore the reasons behind REXUP's effectiveness.\nOur best model significantly outperforms the precious state-of-the-art, which\ndelivers 92.7% on the validation set and 73.1% on the test-dev set.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:54:50 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 09:18:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Luo", "Siwen", ""], ["Han", "Soyeon Caren", ""], ["Sun", "Kaiyuan", ""], ["Poon", "Josiah", ""]]}, {"id": "2007.13264", "submitter": "Peixian Chen", "authors": "Pingyang Dai, Peixian Chen, Qiong Wu, Xiaopeng Hong, Qixiang Ye, Qi\n  Tian, Rongrong Ji", "title": "Learning Task-oriented Disentangled Representations for Unsupervised\n  Domain Adaptation", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to address the domain-shift problem\nbetween a labeled source domain and an unlabeled target domain. Many efforts\nhave been made to address the mismatch between the distributions of training\nand testing data, but unfortunately, they ignore the task-oriented information\nacross domains and are inflexible to perform well in complicated open-set\nscenarios. Many efforts have been made to eliminate the mismatch between the\ndistributions of training and testing data by learning domain-invariant\nrepresentations. However, the learned representations are usually not\ntask-oriented, i.e., being class-discriminative and domain-transferable\nsimultaneously. This drawback limits the flexibility of UDA in complicated\nopen-set tasks where no labels are shared between domains. In this paper, we\nbreak the concept of task-orientation into task-relevance and task-irrelevance,\nand propose a dynamic task-oriented disentangling network (DTDN) to learn\ndisentangled representations in an end-to-end fashion for UDA. The dynamic\ndisentangling network effectively disentangles data representations into two\ncomponents: the task-relevant ones embedding critical information associated\nwith the task across domains, and the task-irrelevant ones with the remaining\nnon-transferable or disturbing information. These two components are\nregularized by a group of task-specific objective functions across domains.\nSuch regularization explicitly encourages disentangling and avoids the use of\ngenerative models or decoders. Experiments in complicated, open-set scenarios\n(retrieval tasks) and empirical benchmarks (classification tasks) demonstrate\nthat the proposed method captures rich disentangled information and achieves\nsuperior performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 01:21:18 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dai", "Pingyang", ""], ["Chen", "Peixian", ""], ["Wu", "Qiong", ""], ["Hong", "Xiaopeng", ""], ["Ye", "Qixiang", ""], ["Tian", "Qi", ""], ["Ji", "Rongrong", ""]]}, {"id": "2007.13278", "submitter": "R Devon Hjelm", "authors": "R Devon Hjelm and Philip Bachman", "title": "Representation Learning with Video Deep InfoMax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has made unsupervised pretraining relevant again for\ndifficult computer vision tasks. The most effective self-supervised methods\ninvolve prediction tasks based on features extracted from diverse views of the\ndata. DeepInfoMax (DIM) is a self-supervised method which leverages the\ninternal structure of deep networks to construct such views, forming prediction\ntasks between local features which depend on small patches in an image and\nglobal features which depend on the whole image. In this paper, we extend DIM\nto the video domain by leveraging similar structure in spatio-temporal\nnetworks, producing a method we call Video Deep InfoMax(VDIM). We find that\ndrawing views from both natural-rate sequences and temporally-downsampled\nsequences yields results on Kinetics-pretrained action recognition tasks which\nmatch or outperform prior state-of-the-art methods that use more costly\nlarge-time-scale transformer models. We also examine the effects of data\naugmentation and fine-tuning methods, accomplishingSoTA by a large margin when\ntraining only on the UCF-101 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:28:47 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 01:27:14 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Hjelm", "R Devon", ""], ["Bachman", "Philip", ""]]}, {"id": "2007.13284", "submitter": "Zuoxiang Zeng", "authors": "Wei Zhang and Zuoxiang Zeng", "title": "Research Progress of Convolutional Neural Network and its Application in\n  Object Detection", "comments": "11 pages, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the improvement of computer performance and the increase of data volume,\nthe object detection based on convolutional neural network (CNN) has become the\nmain algorithm for object detection. This paper summarizes the research\nprogress of convolutional neural networks and their applications in object\ndetection, and focuses on analyzing and discussing a specific idea and method\nof applying convolutional neural networks for object detection, pointing out\nthe current deficiencies and future development direction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:45:59 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhang", "Wei", ""], ["Zeng", "Zuoxiang", ""]]}, {"id": "2007.13303", "submitter": "Luyang Zhu", "authors": "Luyang Zhu, Konstantinos Rematas, Brian Curless, Steve Seitz, Ira\n  Kemelmacher-Shlizerman", "title": "Reconstructing NBA Players", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great progress has been made in 3D body pose and shape estimation from a\nsingle photo. Yet, state-of-the-art results still suffer from errors due to\nchallenging body poses, modeling clothing, and self occlusions. The domain of\nbasketball games is particularly challenging, as it exhibits all of these\nchallenges. In this paper, we introduce a new approach for reconstruction of\nbasketball players that outperforms the state-of-the-art. Key to our approach\nis a new method for creating poseable, skinned models of NBA players, and a\nlarge database of meshes (derived from the NBA2K19 video game), that we are\nreleasing to the research community. Based on these models, we introduce a new\nmethod that takes as input a single photo of a clothed player in any basketball\npose and outputs a high resolution mesh and 3D pose for that player. We\ndemonstrate substantial improvement over state-of-the-art, single-image methods\nfor body shape reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 04:09:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhu", "Luyang", ""], ["Rematas", "Konstantinos", ""], ["Curless", "Brian", ""], ["Seitz", "Steve", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2007.13310", "submitter": "Guo-Jun Qi", "authors": "Haohang Xu, Hongkai Xiong, Guo-Jun Qi", "title": "K-Shot Contrastive Learning of Visual Features with Multiple Instance\n  Augmentations", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the $K$-Shot Contrastive Learning (KSCL) of visual\nfeatures by applying multiple augmentations to investigate the sample\nvariations within individual instances. It aims to combine the advantages of\ninter-instance discrimination by learning discriminative features to\ndistinguish between different instances, as well as intra-instance variations\nby matching queries against the variants of augmented samples over instances.\nParticularly, for each instance, it constructs an instance subspace to model\nthe configuration of how the significant factors of variations in $K$-shot\naugmentations can be combined to form the variants of augmentations. Given a\nquery, the most relevant variant of instances is then retrieved by projecting\nthe query onto their subspaces to predict the positive instance class. This\ngeneralizes the existing contrastive learning that can be viewed as a special\none-shot case. An eigenvalue decomposition is performed to configure instance\nsubspaces, and the embedding network can be trained end-to-end through the\ndifferentiable subspace configuration. Experiment results demonstrate the\nproposed $K$-shot contrastive learning achieves superior performances to the\nstate-of-the-art unsupervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 04:56:41 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 08:00:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Xu", "Haohang", ""], ["Xiong", "Hongkai", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2007.13312", "submitter": "Yoshitomo Matsubara", "authors": "Yoshitomo Matsubara, Marco Levorato", "title": "Split Computing for Complex Object Detectors: Challenges and Preliminary\n  Results", "comments": "Accepted to EMDL '20 (4th International Workshop on Embedded and\n  Mobile Deep Learning) co-located with ACM MobiCom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the trends of mobile and edge computing for DNN models, an\nintermediate option, split computing, has been attracting attentions from the\nresearch community. Previous studies empirically showed that while mobile and\nedge computing often would be the best options in terms of total inference\ntime, there are some scenarios where split computing methods can achieve\nshorter inference time. All the proposed split computing approaches, however,\nfocus on image classification tasks, and most are assessed with small datasets\nthat are far from the practical scenarios. In this paper, we discuss the\nchallenges in developing split computing methods for powerful R-CNN object\ndetectors trained on a large dataset, COCO 2017. We extensively analyze the\nobject detectors in terms of layer-wise tensor size and model size, and show\nthat naive split computing methods would not reduce inference time. To the best\nof our knowledge, this is the first study to inject small bottlenecks to such\nobject detectors and unveil the potential of a split computing approach. The\nsource code and trained models' weights used in this study are available at\nhttps://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors .\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 05:03:37 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 05:14:59 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Matsubara", "Yoshitomo", ""], ["Levorato", "Marco", ""]]}, {"id": "2007.13314", "submitter": "Zhi Chen", "authors": "Zhi Chen, Sen Wang, Jingjing Li, Zi Huang", "title": "Rethinking Generative Zero-Shot Learning: An Ensemble Learning\n  Perspective for Recognising Visual Patches", "comments": "ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is commonly used to address the very pervasive\nproblem of predicting unseen classes in fine-grained image classification and\nother tasks. One family of solutions is to learn synthesised unseen visual\nsamples produced by generative models from auxiliary semantic information, such\nas natural language descriptions. However, for most of these models,\nperformance suffers from noise in the form of irrelevant image backgrounds.\nFurther, most methods do not allocate a calculated weight to each semantic\npatch. Yet, in the real world, the discriminative power of features can be\nquantified and directly leveraged to improve accuracy and reduce computational\ncomplexity. To address these issues, we propose a novel framework called\nmulti-patch generative adversarial nets (MPGAN) that synthesises local patch\nfeatures and labels unseen classes with a novel weighted voting strategy. The\nprocess begins by generating discriminative visual features from noisy text\ndescriptions for a set of predefined local patches using multiple specialist\ngenerative models. The features synthesised from each patch for unseen classes\nare then used to construct an ensemble of diverse supervised classifiers, each\ncorresponding to one local patch. A voting strategy averages the probability\ndistributions output from the classifiers and, given that some patches are more\ndiscriminative than others, a discrimination-based attention mechanism helps to\nweight each patch accordingly. Extensive experiments show that MPGAN has\nsignificantly greater accuracy than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 05:49:44 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 03:50:28 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 01:14:32 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chen", "Zhi", ""], ["Wang", "Sen", ""], ["Li", "Jingjing", ""], ["Huang", "Zi", ""]]}, {"id": "2007.13332", "submitter": "Nan Zhuang", "authors": "Nan Zhuang and Cheng Yang", "title": "Few-shot Knowledge Transfer for Fine-grained Cartoon Face Generation", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in generating fine-grained cartoon faces for\nvarious groups. We assume that one of these groups consists of sufficient\ntraining data while the others only contain few samples. Although the cartoon\nfaces of these groups share similar style, the appearances in various groups\ncould still have some specific characteristics, which makes them differ from\neach other. A major challenge of this task is how to transfer knowledge among\ngroups and learn group-specific characteristics with only few samples. In order\nto solve this problem, we propose a two-stage training process. First, a basic\ntranslation model for the basic group (which consists of sufficient data) is\ntrained. Then, given new samples of other groups, we extend the basic model by\ncreating group-specific branches for each new group. Group-specific branches\nare updated directly to capture specific appearances for each group while the\nremaining group-shared parameters are updated indirectly to maintain the\ndistribution of intermediate feature space. In this manner, our approach is\ncapable to generate high-quality cartoon faces for various groups.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 07:13:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhuang", "Nan", ""], ["Yang", "Cheng", ""]]}, {"id": "2007.13344", "submitter": "Jinxian Liu", "authors": "Jinxian Liu, Minghui Yu, Bingbing Ni and Ye Chen", "title": "Self-Prediction for Joint Instance and Semantic Segmentation of Point\n  Clouds", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel learning scheme named Self-Prediction for 3D instance and\nsemantic segmentation of point clouds. Distinct from most existing methods that\nfocus on designing convolutional operators, our method designs a new learning\nscheme to enhance point relation exploring for better segmentation. More\nspecifically, we divide a point cloud sample into two subsets and construct a\ncomplete graph based on their representations. Then we use label propagation\nalgorithm to predict labels of one subset when given labels of the other\nsubset. By training with this Self-Prediction task, the backbone network is\nconstrained to fully explore relational context/geometric/shape information and\nlearn more discriminative features for segmentation. Moreover, a general\nassociated framework equipped with our Self-Prediction scheme is designed for\nenhancing instance and semantic segmentation simultaneously, where instance and\nsemantic representations are combined to perform Self-Prediction. Through this\nway, instance and semantic segmentation are collaborated and mutually\nreinforced. Significant performance improvements on instance and semantic\nsegmentation compared with baseline are achieved on S3DIS and ShapeNet. Our\nmethod achieves state-of-the-art instance segmentation results on S3DIS and\ncomparable semantic segmentation results compared with state-of-the-arts on\nS3DIS and ShapeNet when we only take PointNet++ as the backbone network.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 07:58:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liu", "Jinxian", ""], ["Yu", "Minghui", ""], ["Ni", "Bingbing", ""], ["Chen", "Ye", ""]]}, {"id": "2007.13354", "submitter": "Masashi Fukuhara", "authors": "Masashi Fukuhara, Kazuhiko Fujiwara, Yoshihiro Maruyama and Hiroyasu\n  Itoh", "title": "Feature visualization of Raman spectrum analysis with deep convolutional\n  neural network", "comments": null, "journal-ref": "Analytica Chimica Acta, Volume 1087, 9 December 2019, Pages 11-19", "doi": "10.1016/j.aca.2019.08.064", "report-no": null, "categories": "cs.CV eess.IV physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a recognition and feature visualization method that uses a\ndeep convolutional neural network for Raman spectrum analysis. The\nvisualization is achieved by calculating important regions in the spectra from\nweights in pooling and fully-connected layers. The method is first examined for\nsimple Lorentzian spectra, then applied to the spectra of pharmaceutical\ncompounds and numerically mixed amino acids. We investigate the effects of the\nsize and number of convolution filters on the extracted regions for Raman-peak\nsignals using the Lorentzian spectra. It is confirmed that the Raman peak\ncontributes to the recognition by visualizing the extracted features. A\nnear-zero weight value is obtained at the background level region, which\nappears to be used for baseline correction. Common component extraction is\nconfirmed by an evaluation of numerically mixed amino acid spectra. High weight\nvalues at the common peaks and negative values at the distinctive peaks appear,\neven though the model is given one-hot vectors as the training labels (without\na mix ratio). This proposed method is potentially suitable for applications\nsuch as the validation of trained models, ensuring the reliability of common\ncomponent extraction from compound samples for spectral analysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:15:38 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Fukuhara", "Masashi", ""], ["Fujiwara", "Kazuhiko", ""], ["Maruyama", "Yoshihiro", ""], ["Itoh", "Hiroyasu", ""]]}, {"id": "2007.13373", "submitter": "Jaeseok Choi", "authors": "Jaeseok Choi, Yeji Song and Nojun Kwak", "title": "Part-Aware Data Augmentation for 3D Object Detection in Point Cloud", "comments": "This paper has been accepted by IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has greatly contributed to improving the performance in\nimage recognition tasks, and a lot of related studies have been conducted.\nHowever, data augmentation on 3D point cloud data has not been much explored.\n3D label has more sophisticated and rich structural information than the 2D\nlabel, so it enables more diverse and effective data augmentation. In this\npaper, we propose part-aware data augmentation (PA-AUG) that can better utilize\nrich information of 3D label to enhance the performance of 3D object detectors.\nPA-AUG divides objects into partitions and stochastically applies five\naugmentation methods to each local region. It is compatible with existing point\ncloud data augmentation methods and can be used universally regardless of the\ndetector's architecture. PA-AUG has improved the performance of\nstate-of-the-art 3D object detector for all classes of the KITTI dataset and\nhas the equivalent effect of increasing the train data by about 2.5$\\times$. We\nalso show that PA-AUG not only increases performance for a given dataset but\nalso is robust to corrupted data. The code is available at\nhttps://github.com/sky77764/pa-aug.pytorch\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:47:19 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 05:35:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Choi", "Jaeseok", ""], ["Song", "Yeji", ""], ["Kwak", "Nojun", ""]]}, {"id": "2007.13374", "submitter": "Hao Wang", "authors": "Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao", "title": "Decomposed Generation Networks with Structure Prediction for Recipe\n  Generation from Food Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recipe generation from food images and ingredients is a challenging task,\nwhich requires the interpretation of the information from another modality.\nDifferent from the image captioning task, where the captions usually have one\nsentence, cooking instructions contain multiple sentences and have obvious\nstructures. To help the model capture the recipe structure and avoid missing\nsome cooking details, we propose a novel framework: Decomposed Generation\nNetworks (DGN) with structure prediction, to get more structured and complete\nrecipe generation outputs. To be specific, we split each cooking instruction\ninto several phases, and assign different sub-generators to each phase. Our\napproach includes two novel ideas: (i) learning the recipe structures with the\nglobal structure prediction component and (ii) producing recipe phases in the\nsub-generator output component based on the predicted structure. Extensive\nexperiments on the challenging large-scale Recipe1M dataset validate the\neffectiveness of our proposed model DGN, which improves the performance over\nthe state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:47:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Hao", ""], ["Lin", "Guosheng", ""], ["Hoi", "Steven C. H.", ""], ["Miao", "Chunyan", ""]]}, {"id": "2007.13376", "submitter": "Chong Zhou", "authors": "Penghao Zhou, Chong Zhou, Pai Peng, Junlong Du, Xing Sun, Xiaowei Guo,\n  Feiyue Huang", "title": "NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination", "comments": "Accepted at the ACM International Conference on Multimedia (ACM MM)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy-NMS inherently raises a dilemma, where a lower NMS threshold will\npotentially lead to a lower recall rate and a higher threshold introduces more\nfalse positives. This problem is more severe in pedestrian detection because\nthe instance density varies more intensively. However, previous works on NMS\ndon't consider or vaguely consider the factor of the existent of nearby\npedestrians. Thus, we propose Nearby Objects Hallucinator (NOH), which\npinpoints the objects nearby each proposal with a Gaussian distribution,\ntogether with NOH-NMS, which dynamically eases the suppression for the space\nthat might contain other objects with a high likelihood. Compared to\nGreedy-NMS, our method, as the state-of-the-art, improves by $3.9\\%$ AP,\n$5.1\\%$ Recall, and $0.8\\%$ $\\text{MR}^{-2}$ on CrowdHuman to $89.0\\%$ AP and\n$92.9\\%$ Recall, and $43.9\\%$ $\\text{MR}^{-2}$ respectively.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:51:55 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhou", "Penghao", ""], ["Zhou", "Chong", ""], ["Peng", "Pai", ""], ["Du", "Junlong", ""], ["Sun", "Xing", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""]]}, {"id": "2007.13384", "submitter": "Manoj Vemparala", "authors": "Alexander Frickenstein, Manoj-Rohit Vemparala, Nael Fasfous, Laura\n  Hauenschild, Naveen-Shankar Nagaraja, Christian Unger, Walter Stechele", "title": "ALF: Autoencoder-based Low-rank Filter-sharing for Efficient\n  Convolutional Neural Networks", "comments": "Accepted by DAC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closing the gap between the hardware requirements of state-of-the-art\nconvolutional neural networks and the limited resources constraining embedded\napplications is the next big challenge in deep learning research. The\ncomputational complexity and memory footprint of such neural networks are\ntypically daunting for deployment in resource constrained environments. Model\ncompression techniques, such as pruning, are emphasized among other\noptimization methods for solving this problem. Most existing techniques require\ndomain expertise or result in irregular sparse representations, which increase\nthe burden of deploying deep learning applications on embedded hardware\naccelerators. In this paper, we propose the autoencoder-based low-rank\nfilter-sharing technique technique (ALF). When applied to various networks, ALF\nis compared to state-of-the-art pruning methods, demonstrating its efficient\ncompression capabilities on theoretical metrics as well as on an accurate,\ndeterministic hardware-model. In our experiments, ALF showed a reduction of\n70\\% in network parameters, 61\\% in operations and 41\\% in execution time, with\nminimal loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:01:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Frickenstein", "Alexander", ""], ["Vemparala", "Manoj-Rohit", ""], ["Fasfous", "Nael", ""], ["Hauenschild", "Laura", ""], ["Nagaraja", "Naveen-Shankar", ""], ["Unger", "Christian", ""], ["Stechele", "Walter", ""]]}, {"id": "2007.13393", "submitter": "Yifan Xu", "authors": "Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh", "title": "Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D\n  Reconstruction with Symmetry", "comments": "European Conference on Computer Vision 2020 (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep implicit field regression methods are effective for 3D reconstruction\nfrom single-view images. However, the impact of different sampling patterns on\nthe reconstruction quality is not well-understood. In this work, we first study\nthe effect of point set discrepancy on the network training. Based on Farthest\nPoint Sampling algorithm, we propose a sampling scheme that theoretically\nencourages better generalization performance, and results in fast convergence\nfor SGD-based optimization algorithms. Secondly, based on the reflective\nsymmetry of an object, we propose a feature fusion method that alleviates\nissues due to self-occlusions which makes it difficult to utilize local image\nfeatures. Our proposed system Ladybird is able to create high quality 3D object\nreconstructions from a single input image. We evaluate Ladybird on a large\nscale 3D dataset (ShapeNet) demonstrating highly competitive results in terms\nof Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:17:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Xu", "Yifan", ""], ["Fan", "Tianqi", ""], ["Yuan", "Yi", ""], ["Singh", "Gurprit", ""]]}, {"id": "2007.13404", "submitter": "Christos Kyrkou", "authors": "Christos Kyrkou", "title": "YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart\n  Camera Applications", "comments": null, "journal-ref": null, "doi": "10.1049/iet-cvi.2019.0897", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep Learning-based object detectors can enhance the capabilities of smart\ncamera systems in a wide spectrum of machine vision applications including\nvideo surveillance, autonomous driving, robots and drones, smart factory, and\nhealth monitoring. Pedestrian detection plays a key role in all these\napplications and deep learning can be used to construct accurate\nstate-of-the-art detectors. However, such complex paradigms do not scale easily\nand are not traditionally implemented in resource-constrained smart cameras for\non-device processing which offers significant advantages in situations when\nreal-time monitoring and robustness are vital. Efficient neural networks can\nnot only enable mobile applications and on-device experiences but can also be a\nkey enabler of privacy and security allowing a user to gain the benefits of\nneural networks without needing to send their data to the server to be\nevaluated. This work addresses the challenge of achieving a good trade-off\nbetween accuracy and speed for efficient deployment of deep-learning-based\npedestrian detection in smart camera applications. A computationally efficient\narchitecture is introduced based on separable convolutions and proposes\nintegrating dense connections across layers and multi-scale feature fusion to\nimprove representational capacity while decreasing the number of parameters and\noperations. In particular, the contributions of this work are the following: 1)\nAn efficient backbone combining multi-scale feature operations, 2) a more\nelaborate loss function for improved localization, 3) an anchor-less approach\nfor detection, The proposed approach called YOLOpeds is evaluated using the\nPETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides\nreal-time sustained operation of over 30 frames per second with detection rates\nin the range of 86% outperforming existing deep learning models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:50:11 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:23:12 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kyrkou", "Christos", ""]]}, {"id": "2007.13406", "submitter": "Weihua Liu", "authors": "Weihua Liu, Xiabi Liu, Murong Wang and Ling Ma", "title": "Contraction Mapping of Feature Norms for Classifier Learning on the Data\n  with Different Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular softmax loss and its recent extensions have achieved great\nsuccess in the deep learning-based image classification. However, the data for\ntraining image classifiers usually has different quality. Ignoring such\nproblem, the correct classification of low quality data is hard to be solved.\nIn this paper, we discover the positive correlation between the feature norm of\nan image and its quality through careful experiments on various applications\nand various deep neural networks. Based on this finding, we propose a\ncontraction mapping function to compress the range of feature norms of training\nimages according to their quality and embed this contraction mapping function\ninto softmax loss or its extensions to produce novel learning objectives. The\nexperiments on various classification applications, including handwritten digit\nrecognition, lung nodule classification, face verification and face\nrecognition, demonstrate that the proposed approach is promising to effectively\ndeal with the problem of learning on the data with different quality and leads\nto the significant and stable improvements in the classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:53:55 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 01:07:53 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Liu", "Weihua", ""], ["Liu", "Xiabi", ""], ["Wang", "Murong", ""], ["Ma", "Ling", ""]]}, {"id": "2007.13408", "submitter": "Sina Amirrajab", "authors": "Sina Amirrajab, Samaneh Abbasi-Sureshjani, Yasmina Al Khalil, Cristian\n  Lorenz, Juergen Weese, Josien Pluim, and Marcel Breeuwer", "title": "XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on\n  Anatomically Variable XCAT Phantoms", "comments": "Accepted for MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have provided promising data\nenrichment solutions by synthesizing high-fidelity images. However, generating\nlarge sets of labeled images with new anatomical variations remains unexplored.\nWe propose a novel method for synthesizing cardiac magnetic resonance (CMR)\nimages on a population of virtual subjects with a large anatomical variation,\nintroduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human\nphantom. We investigate two conditional image synthesis approaches grounded on\na semantically-consistent mask-guided image generation technique: 4-class and\n8-class XCAT-GANs. The 4-class technique relies on only the annotations of the\nheart; while the 8-class technique employs a predicted multi-tissue label map\nof the heart-surrounding organs and provides better guidance for our\nconditional image synthesis. For both techniques, we train our conditional\nXCAT-GAN with real images paired with corresponding labels and subsequently at\nthe inference time, we substitute the labels with the XCAT derived ones.\nTherefore, the trained network accurately transfers the tissue-specific\ntextures to the new label maps. By creating 33 virtual subjects of synthetic\nCMR images at the end-diastolic and end-systolic phases, we evaluate the\nusefulness of such data in the downstream cardiac cavity segmentation task\nunder different augmentation strategies. Results demonstrate that even with\nonly 20% of real images (40 volumes) seen during training, segmentation\nperformance is retained with the addition of synthetic CMR images. Moreover,\nthe improvement in utilizing synthetic images for augmenting the real data is\nevident through the reduction of Hausdorff distance up to 28% and an increase\nin the Dice score up to 5%, indicating a higher similarity to the ground truth\nin all dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 10:05:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 14:27:59 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Amirrajab", "Sina", ""], ["Abbasi-Sureshjani", "Samaneh", ""], ["Khalil", "Yasmina Al", ""], ["Lorenz", "Cristian", ""], ["Weese", "Juergen", ""], ["Pluim", "Josien", ""], ["Breeuwer", "Marcel", ""]]}, {"id": "2007.13417", "submitter": "Wufei Ma", "authors": "Wufei Ma, Elizabeth Kautz, Arun Baskaran, Aritra Chowdhury, Vineet\n  Joshi, B\\\"ulent Yener, Daniel Lewis", "title": "Image-driven discriminative and generative machine learning algorithms\n  for establishing microstructure-processing relationships", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": "10.1063/5.0013720", "report-no": null, "categories": "physics.app-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate methods of microstructure representation for the purpose of\npredicting processing condition from microstructure image data. A binary alloy\n(uranium-molybdenum) that is currently under development as a nuclear fuel was\nstudied for the purpose of developing an improved machine learning approach to\nimage recognition, characterization, and building predictive capabilities\nlinking microstructure to processing conditions. Here, we test different\nmicrostructure representations and evaluate model performance based on the F1\nscore. A F1 score of 95.1% was achieved for distinguishing between micrographs\ncorresponding to ten different thermo-mechanical material processing\nconditions. We find that our newly developed microstructure representation\ndescribes image data well, and the traditional approach of utilizing area\nfractions of different phases is insufficient for distinguishing between\nmultiple classes using a relatively small, imbalanced original data set of 272\nimages. To explore the applicability of generative methods for supplementing\nsuch limited data sets, generative adversarial networks were trained to\ngenerate artificial microstructure images. Two different generative networks\nwere trained and tested to assess performance. Challenges and best practices\nassociated with applying machine learning to limited microstructure image data\nsets is also discussed. Our work has implications for quantitative\nmicrostructure analysis, and development of microstructure-processing\nrelationships in limited data sets typical of metallurgical process design\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 10:36:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ma", "Wufei", ""], ["Kautz", "Elizabeth", ""], ["Baskaran", "Arun", ""], ["Chowdhury", "Aritra", ""], ["Joshi", "Vineet", ""], ["Yener", "B\u00fclent", ""], ["Lewis", "Daniel", ""]]}, {"id": "2007.13428", "submitter": "Dongbao Yang", "authors": "Dongbao Yang, Yu Zhou, Dayan Wu, Can Ma, Fei Yang, Weiping Wang", "title": "Two-Level Residual Distillation based Triple Network for Incremental\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern object detection methods based on convolutional neural network suffer\nfrom severe catastrophic forgetting in learning new classes without original\ndata. Due to time consumption, storage burden and privacy of old data, it is\ninadvisable to train the model from scratch with both old and new data when new\nobject classes emerge after the model trained. In this paper, we propose a\nnovel incremental object detector based on Faster R-CNN to continuously learn\nfrom new object classes without using old data. It is a triple network where an\nold model and a residual model as assistants for helping the incremental model\nlearning on new classes without forgetting the previous learned knowledge. To\nbetter maintain the discrimination of features between old and new classes, the\nresidual model is jointly trained on new classes in the incremental learning\nprocedure. In addition, a corresponding distillation scheme is designed to\nguide the training process, which consists of a two-level residual distillation\nloss and a joint classification distillation loss. Extensive experiments on\nVOC2007 and COCO are conducted, and the results demonstrate that the proposed\nmethod can effectively learn to incrementally detect objects of new classes,\nand the problem of catastrophic forgetting is mitigated in this context.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:04:57 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yang", "Dongbao", ""], ["Zhou", "Yu", ""], ["Wu", "Dayan", ""], ["Ma", "Can", ""], ["Yang", "Fei", ""], ["Wang", "Weiping", ""]]}, {"id": "2007.13467", "submitter": "Kuan Zhu", "authors": "Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, Jinqiao Wang", "title": "Identity-Guided Human Semantic Parsing for Person Re-Identification", "comments": "Accepted by ECCV 2020 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing alignment-based methods have to employ the pretrained human parsing\nmodels to achieve the pixel-level alignment, and cannot identify the personal\nbelongings (e.g., backpacks and reticule) which are crucial to person re-ID. In\nthis paper, we propose the identity-guided human semantic parsing approach\n(ISP) to locate both the human body parts and personal belongings at\npixel-level for aligned person re-ID only with person identity labels. We\ndesign the cascaded clustering on feature maps to generate the pseudo-labels of\nhuman parts. Specifically, for the pixels of all images of a person, we first\ngroup them to foreground or background and then group the foreground pixels to\nhuman parts. The cluster assignments are subsequently used as pseudo-labels of\nhuman parts to supervise the part estimation and ISP iteratively learns the\nfeature maps and groups them. Finally, local features of both human body parts\nand personal belongings are obtained according to the selflearned part\nestimation, and only features of visible parts are utilized for the retrieval.\nExtensive experiments on three widely used datasets validate the superiority of\nISP over lots of state-of-the-art methods. Our code is available at\nhttps://github.com/CASIA-IVA-Lab/ISP-reID.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:12:27 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhu", "Kuan", ""], ["Guo", "Haiyun", ""], ["Liu", "Zhiwei", ""], ["Tang", "Ming", ""], ["Wang", "Jinqiao", ""]]}, {"id": "2007.13484", "submitter": "Shuyue Jia", "authors": "Shuyue Jia, Yimin Hou, Yan Shi, Yang Li", "title": "Attention-based Graph ResNet for Motor Intent Detection from Raw EEG\n  signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In previous studies, decoding electroencephalography (EEG) signals has not\nconsidered the topological relationship of EEG electrodes. However, the latest\nneuroscience has suggested brain network connectivity. Thus, the exhibited\ninteraction between EEG channels might not be appropriately measured via\nEuclidean distance. To fill the gap, an attention-based graph residual network,\na novel structure of Graph Convolutional Neural Network (GCN), was presented to\ndetect human motor intents from raw EEG signals, where the topological\nstructure of EEG electrodes was built as a graph. Meanwhile, deep residual\nlearning with a full-attention architecture was introduced to address the\ndegradation problem concerning deeper networks in raw EEG motor imagery (MI)\ndata. Individual variability, the critical and longstanding challenge\nunderlying EEG signals, has been successfully handled with the state-of-the-art\nperformance, 98.08% accuracy at the subject level, 94.28% for 20 subjects.\nNumerical results were promising that the implementation of the\ngraph-structured topology was superior to decode raw EEG data. The innovative\ndeep learning approach was expected to entail a universal method towards both\nneuroscience research and real-world EEG-based practical applications, e.g.,\nseizure prediction.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:29:48 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Jia", "Shuyue", ""], ["Hou", "Yimin", ""], ["Shi", "Yan", ""], ["Li", "Yang", ""]]}, {"id": "2007.13516", "submitter": "Mahesh Chandra", "authors": "Mahesh Chandra", "title": "Hardware Implementation of Hyperbolic Tangent Function using Catmull-Rom\n  Spline Interpolation", "comments": "4 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:2007.11976", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks yield the state of the art results in many computer\nvision and human machine interface tasks such as object recognition, speech\nrecognition etc. Since, these networks are computationally expensive,\ncustomized accelerators are designed for achieving the required performance at\nlower cost and power. One of the key building blocks of these neural networks\nis non-linear activation function such as sigmoid, hyperbolic tangent (tanh),\nand ReLU. A low complexity accurate hardware implementation of the activation\nfunction is required to meet the performance and area targets of the neural\nnetwork accelerators. This paper presents an implementation of tanh function\nusing the Catmull-Rom spline interpolation. State of the art results are\nachieved using this method with comparatively smaller logic area.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:11:59 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chandra", "Mahesh", ""]]}, {"id": "2007.13521", "submitter": "Naser Damer", "authors": "Naser Damer, Jonas Henry Grebe, Cong Chen, Fadi Boutros, Florian\n  Kirchbuchner and Arjan Kuijper", "title": "The Effect of Wearing a Mask on Face Recognition Performance: an\n  Exploratory Study", "comments": "Accepted at BIOSIG2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has become essential in our daily lives as a convenient and\ncontactless method of accurate identity verification. Process such as identity\nverification at automatic border control gates or the secure login to\nelectronic devices are increasingly dependant on such technologies. The recent\nCOVID-19 pandemic have increased the value of hygienic and contactless identity\nverification. However, the pandemic led to the wide use of face masks,\nessential to keep the pandemic under control. The effect of wearing a mask on\nface recognition in a collaborative environment is currently sensitive yet\nunderstudied issue. We address that by presenting a specifically collected\ndatabase containing three session, each with three different capture\ninstructions, to simulate realistic use cases. We further study the effect of\nmasked face probes on the behaviour of three top-performing face recognition\nsystems, two academic solutions and one commercial off-the-shelf (COTS) system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:03:32 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 18:57:25 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Damer", "Naser", ""], ["Grebe", "Jonas Henry", ""], ["Chen", "Cong", ""], ["Boutros", "Fadi", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2007.13538", "submitter": "T Deepika", "authors": "T.Deepika, G.Karpaga Kannan", "title": "A Novel adaptive optimization of Dual-Tree Complex Wavelet Transform for\n  Medical Image Fusion", "comments": "Conference on Computing Communication and Signal Processing. arXiv\n  admin note: text overlap with arXiv:2007.11488", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, many research achievements are made in the medical image\nfusion field. Fusion is basically extraction of best of inputs and conveying it\nto the output. Medical Image fusion means that several of various modality\nimage information is comprehended together to form one image to express its\ninformation. The aim of image fusion is to integrate complementary and\nredundant information. In this paper, a multimodal image fusion algorithm based\non the dual-tree complex wavelet transform (DT-CWT) and adaptive particle swarm\noptimization (APSO) is proposed. Fusion is achieved through the formation of a\nfused pyramid using the DTCWT coefficients from the decomposed pyramids of the\nsource images. The coefficients are fused by the weighted average method based\non pixels, and the weights are estimated by the APSO to gain optimal fused\nimages. The fused image is obtained through conventional inverse dual-tree\ncomplex wavelet transform reconstruction process. Experiment results show that\nthe proposed method based on adaptive particle swarm optimization algorithm is\nremarkably better than the method based on particle swarm optimization. The\nresulting fused images are compared visually and through benchmarks such as\nEntropy (E), Peak Signal to Noise Ratio, (PSNR), Root Mean Square Error (RMSE),\nStandard deviation (SD) and Structure Similarity Index Metric (SSIM)\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:34:01 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Deepika", "T.", ""], ["Kannan", "G. Karpaga", ""]]}, {"id": "2007.13547", "submitter": "Changsheng Li", "authors": "Changsheng Li and Chong Liu and Lixin Duan and Peng Gao and Kai Zheng", "title": "Reconstruction Regularized Deep Metric Learning for Multi-label Image\n  Classification", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel deep metric learning method to tackle the\nmulti-label image classification problem. In order to better learn the\ncorrelations among images features, as well as labels, we attempt to explore a\nlatent space, where images and labels are embedded via two unique deep neural\nnetworks, respectively. To capture the relationships between image features and\nlabels, we aim to learn a \\emph{two-way} deep distance metric over the\nembedding space from two different views, i.e., the distance between one image\nand its labels is not only smaller than those distances between the image and\nits labels' nearest neighbors, but also smaller than the distances between the\nlabels and other images corresponding to the labels' nearest neighbors.\nMoreover, a reconstruction module for recovering correct labels is incorporated\ninto the whole framework as a regularization term, such that the label\nembedding space is more representative. Our model can be trained in an\nend-to-end manner. Experimental results on publicly available image datasets\ncorroborate the efficacy of our method compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:28:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Changsheng", ""], ["Liu", "Chong", ""], ["Duan", "Lixin", ""], ["Gao", "Peng", ""], ["Zheng", "Kai", ""]]}, {"id": "2007.13551", "submitter": "Shitong Luo", "authors": "Shitong Luo, Wei Hu", "title": "Differentiable Manifold Reconstruction for Point Cloud Denoising", "comments": "This work has been accepted to ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413727", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point clouds are often perturbed by noise due to the inherent limitation\nof acquisition equipments, which obstructs downstream tasks such as surface\nreconstruction, rendering and so on. Previous works mostly infer the\ndisplacement of noisy points from the underlying surface, which however are not\ndesignated to recover the surface explicitly and may lead to sub-optimal\ndenoising results. To this end, we propose to learn the underlying manifold of\na noisy point cloud from differentiably subsampled points with trivial noise\nperturbation and their embedded neighborhood feature, aiming to capture\nintrinsic structures in point clouds. Specifically, we present an\nautoencoder-like neural network. The encoder learns both local and non-local\nfeature representations of each point, and then samples points with low noise\nvia an adaptive differentiable pooling operation. Afterwards, the decoder\ninfers the underlying manifold by transforming each sampled point along with\nthe embedded feature of its neighborhood to a local surface centered around the\npoint. By resampling on the reconstructed manifold, we obtain a denoised point\ncloud. Further, we design an unsupervised training loss, so that our network\ncan be trained in either an unsupervised or supervised fashion. Experiments\nshow that our method significantly outperforms state-of-the-art denoising\nmethods under both synthetic noise and real world noise. The code and data are\navailable at https://github.com/luost26/DMRDenoise\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:31:41 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 09:23:44 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Luo", "Shitong", ""], ["Hu", "Wei", ""]]}, {"id": "2007.13559", "submitter": "Changhee Han", "authors": "Changhee Han, Leonardo Rundo, Kohei Murao, Tomoyuki Noguchi, Yuki\n  Shimahara, Zoltan Adam Milacski, Saori Koshino, Evis Sala, Hideki Nakayama,\n  Shinichi Satoh", "title": "MADGAN: unsupervised Medical Anomaly Detection GAN using multiple\n  adjacent brain MRI slice reconstruction", "comments": "23 pages, 11 figures, submitted to BMC Bioinformatics. Extended\n  version of arXiv:1906.06114", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning can discover various unseen abnormalities, relying on\nlarge-scale unannotated medical images of healthy subjects. Towards this,\nunsupervised methods reconstruct a 2D/3D single medical image to detect\noutliers either in the learned feature space or from high reconstruction loss.\nHowever, without considering continuity between multiple adjacent slices, they\ncannot directly discriminate diseases composed of the accumulation of subtle\nanatomical anomalies, such as Alzheimer's Disease (AD). Moreover, no study has\nshown how unsupervised anomaly detection is associated with either disease\nstages, various (i.e., more than two types of) diseases, or multi-sequence\nMagnetic Resonance Imaging (MRI) scans. Therefore, we propose unsupervised\nMedical Anomaly Detection Generative Adversarial Network (MADGAN), a novel\ntwo-step method using GAN-based multiple adjacent brain MRI slice\nreconstruction to detect brain anomalies at different stages on multi-sequence\nstructural MRI: (Reconstruction) Wasserstein loss with Gradient Penalty + 100\nL1 loss-trained on 3 healthy brain axial MRI slices to reconstruct the next 3\nones-reconstructs unseen healthy/abnormal scans; (Diagnosis) Average L2 loss\nper scan discriminates them, comparing the ground truth/reconstructed slices.\nFor training, we use two different datasets composed of 1,133 healthy\nT1-weighted (T1) and 135 healthy contrast-enhanced T1 (T1c) brain MRI scans for\ndetecting AD and brain metastases/various diseases, respectively. Our\nSelf-Attention MADGAN can detect AD on T1 scans at a very early stage, Mild\nCognitive Impairment (MCI), with Area Under the Curve (AUC) 0.727, and AD at a\nlate stage with AUC 0.894, while detecting brain metastases on T1c scans with\nAUC 0.921.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:56:12 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 10:43:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Han", "Changhee", ""], ["Rundo", "Leonardo", ""], ["Murao", "Kohei", ""], ["Noguchi", "Tomoyuki", ""], ["Shimahara", "Yuki", ""], ["Milacski", "Zoltan Adam", ""], ["Koshino", "Saori", ""], ["Sala", "Evis", ""], ["Nakayama", "Hideki", ""], ["Satoh", "Shinichi", ""]]}, {"id": "2007.13595", "submitter": "Jianlei Yang", "authors": "Pengcheng Dai, Jianlei Yang, Xucheng Ye, Xingzhou Cheng, Junyu Luo,\n  Linghao Song, Yiran Chen, Weisheng Zhao", "title": "SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional\n  Neural Networks Training", "comments": "published on DAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training Convolutional Neural Networks (CNNs) usually requires a large number\nof computational resources. In this paper, \\textit{SparseTrain} is proposed to\naccelerate CNN training by fully exploiting the sparsity. It mainly involves\nthree levels of innovations: activation gradients pruning algorithm, sparse\ntraining dataflow, and accelerator architecture. By applying a stochastic\npruning algorithm on each layer, the sparsity of back-propagation gradients can\nbe increased dramatically without degrading training accuracy and convergence\nrate. Moreover, to utilize both \\textit{natural sparsity} (resulted from ReLU\nor Pooling layers) and \\textit{artificial sparsity} (brought by pruning\nalgorithm), a sparse-aware architecture is proposed for training acceleration.\nThis architecture supports forward and back-propagation of CNN by adopting\n1-Dimensional convolution dataflow. We have built %a simple compiler to map\nCNNs topology onto \\textit{SparseTrain}, and a cycle-accurate architecture\nsimulator to evaluate the performance and efficiency based on the synthesized\ndesign with $14nm$ FinFET technologies. Evaluation results on AlexNet/ResNet\nshow that \\textit{SparseTrain} could achieve about $2.7 \\times$ speedup and\n$2.2 \\times$ energy efficiency improvement on average compared with the\noriginal training process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:01:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dai", "Pengcheng", ""], ["Yang", "Jianlei", ""], ["Ye", "Xucheng", ""], ["Cheng", "Xingzhou", ""], ["Luo", "Junyu", ""], ["Song", "Linghao", ""], ["Chen", "Yiran", ""], ["Zhao", "Weisheng", ""]]}, {"id": "2007.13632", "submitter": "Yi Zhang", "authors": "Yi Zhang, Jitao Sang", "title": "Towards Accuracy-Fairness Paradox: Adversarial Example-based Data\n  Augmentation for Visual Debiasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning fairness concerns about the biases towards certain protected\nor sensitive group of people when addressing the target tasks. This paper\nstudies the debiasing problem in the context of image classification tasks. Our\ndata analysis on facial attribute recognition demonstrates (1) the attribution\nof model bias from imbalanced training data distribution and (2) the potential\nof adversarial examples in balancing data distribution. We are thus motivated\nto employ adversarial example to augment the training data for visual\ndebiasing. Specifically, to ensure the adversarial generalization as well as\ncross-task transferability, we propose to couple the operations of target task\nclassifier training, bias task classifier training, and adversarial example\ngeneration. The generated adversarial examples supplement the target task\ntraining dataset via balancing the distribution over bias variables in an\nonline fashion. Results on simulated and real-world debiasing experiments\ndemonstrate the effectiveness of the proposed solution in simultaneously\nimproving model accuracy and fairness. Preliminary experiment on few-shot\nlearning further shows the potential of adversarial attack-based pseudo sample\ngeneration as alternative solution to make up for the training data lackage.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:17:52 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 08:29:49 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhang", "Yi", ""], ["Sang", "Jitao", ""]]}, {"id": "2007.13635", "submitter": "Anton Razzhigaev", "authors": "Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam Tursynbek,\n  and Aleksandr Petiushko", "title": "Black-Box Face Recovery from Identity Features", "comments": null, "journal-ref": "ECCV Workshops (5) 2020: 462-475", "doi": "10.1007/978-3-030-68238-5_34", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel algorithm based on an it-erative sampling of\nrandom Gaussian blobs for black-box face recovery, given only an output feature\nvector of deep face recognition systems. We attack the state-of-the-art face\nrecognition system (ArcFace) to test our algorithm. Another network with\ndifferent architecture (FaceNet) is used as an independent critic showing that\nthe target person can be identified with the reconstructed image even with no\naccess to the attacked model. Furthermore, our algorithm requires a\nsignificantly less number of queries compared to the state-of-the-art solution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:25:38 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 09:32:17 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 13:24:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Razzhigaev", "Anton", ""], ["Kireev", "Klim", ""], ["Kaziakhmedov", "Edgar", ""], ["Tursynbek", "Nurislam", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "2007.13638", "submitter": "Yunpeng Shi", "authors": "Yunpeng Shi and Gilad Lerman", "title": "Message Passing Least Squares Framework and its Application to Rotation\n  Synchronization", "comments": "To Appear in ICML 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm for solving group synchronization under\nhigh levels of corruption and noise, while we focus on rotation\nsynchronization. We first describe our recent theoretically guaranteed message\npassing algorithm that estimates the corruption levels of the measured group\nratios. We then propose a novel reweighted least squares method to estimate the\ngroup elements, where the weights are initialized and iteratively updated using\nthe estimated corruption levels. We demonstrate the superior performance of our\nalgorithm over state-of-the-art methods for rotation synchronization using both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:39:19 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 18:14:00 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 02:00:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shi", "Yunpeng", ""], ["Lerman", "Gilad", ""]]}, {"id": "2007.13640", "submitter": "Eero Simoncelli", "authors": "Zahra Kadkhodaie and Eero P. Simoncelli", "title": "Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser", "comments": "19 pages, 12 figures. Changes: more detailed description of\n  relationships to previous literature, including empirical comparisons for\n  super-resolution, debarring, and compressive sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior probability models are a fundamental component of many image processing\nproblems, but density estimation is notoriously difficult for high-dimensional\nsignals such as photographic images. Deep neural networks have provided\nstate-of-the-art solutions for problems such as denoising, which implicitly\nrely on a prior probability model of natural images. Here, we develop a robust\nand general methodology for making use of this implicit prior. We rely on a\nstatistical result due to Miyasawa (1961), who showed that the least-squares\nsolution for removing additive Gaussian noise can be written directly in terms\nof the gradient of the log of the noisy signal density. We use this fact to\ndevelop a stochastic coarse-to-fine gradient ascent procedure for drawing\nhigh-probability samples from the implicit prior embedded within a CNN trained\nto perform blind (i.e., with unknown noise level) least-squares denoising. A\ngeneralization of this algorithm to constrained sampling provides a method for\nusing the implicit prior to solve any linear inverse problem, with no\nadditional training. We demonstrate this general form of transfer learning in\nmultiple applications, using the same algorithm to produce state-of-the-art\nlevels of unsupervised performance for deblurring, super-resolution,\ninpainting, and compressive sensing.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:40:46 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 23:49:27 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 02:34:03 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kadkhodaie", "Zahra", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "2007.13648", "submitter": "Perry Gibson", "authors": "Perry Gibson, Jos\\'e Cano", "title": "Orpheus: A New Deep Learning Framework for Easy Deployment and\n  Evaluation of Edge Inference", "comments": "To be published as a poster in 2020 IEEE International Symposium on\n  Performance Analysis of Systems and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Optimising deep learning inference across edge devices and optimisation\ntargets such as inference time, memory footprint and power consumption is a key\nchallenge due to the ubiquity of neural networks. Today, production deep\nlearning frameworks provide useful abstractions to aid machine learning\nengineers and systems researchers. However, in exchange they can suffer from\ncompatibility challenges (especially on constrained platforms), inaccessible\ncode complexity, or design choices that otherwise limit research from a systems\nperspective. This paper presents Orpheus, a new deep learning framework for\neasy prototyping, deployment and evaluation of inference optimisations. Orpheus\nfeatures a small codebase, minimal dependencies, and a simple process for\nintegrating other third party systems. We present some preliminary evaluation\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:54:40 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 20:58:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Gibson", "Perry", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2007.13657", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur", "title": "Towards Learning Convolutions from Scratch", "comments": "18 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is one of the most essential components of architectures used in\ncomputer vision. As machine learning moves towards reducing the expert bias and\nlearning it from data, a natural next step seems to be learning\nconvolution-like structures from scratch. This, however, has proven elusive.\nFor example, current state-of-the-art architecture search algorithms use\nconvolution as one of the existing modules rather than learning it from data.\nIn an attempt to understand the inductive bias that gives rise to convolutions,\nwe investigate minimum description length as a guiding principle and show that\nin some settings, it can indeed be indicative of the performance of\narchitectures. To find architectures with small description length, we propose\n$\\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on\nfully-connected networks for image classification tasks, learns architectures\nwith local connections and achieves state-of-the-art accuracies for training\nfully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%)\nbridging the gap between fully-connected and convolutional nets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:13:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Neyshabur", "Behnam", ""]]}, {"id": "2007.13666", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando\n  De la Torre", "title": "3D Human Shape and Pose from a Single Low-Resolution Image with\n  Self-Supervised Learning", "comments": "ECCV 2020, project page:\n  https://sites.google.com/view/xiangyuxu/3d_eccv20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human shape and pose estimation from monocular images has been an active\narea of research in computer vision, having a substantial impact on the\ndevelopment of new applications, from activity recognition to creating virtual\navatars. Existing deep learning methods for 3D human shape and pose estimation\nrely on relatively high-resolution input images; however, high-resolution\nvisual content is not always available in several practical scenarios such as\nvideo surveillance and sports broadcasting. Low-resolution images in real\nscenarios can vary in a wide range of sizes, and a model trained in one\nresolution does not typically degrade gracefully across resolutions. Two common\napproaches to solve the problem of low-resolution input are applying\nsuper-resolution techniques to the input images which may result in visual\nartifacts, or simply training one model for each resolution, which is\nimpractical in many realistic applications. To address the above issues, this\npaper proposes a novel algorithm called RSC-Net, which consists of a\nResolution-aware network, a Self-supervision loss, and a Contrastive learning\nscheme. The proposed network is able to learn the 3D body shape and pose across\ndifferent resolutions with a single model. The self-supervision loss encourages\nscale-consistency of the output, and the contrastive learning scheme enforces\nscale-consistency of the deep features. We show that both these new training\nlosses provide robustness when learning 3D shape and pose in a\nweakly-supervised manner. Extensive experiments demonstrate that the RSC-Net\ncan achieve consistently better results than the state-of-the-art methods for\nchallenging low-resolution images.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:19:52 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 17:22:43 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xu", "Xiangyu", ""], ["Chen", "Hao", ""], ["Moreno-Noguer", "Francesc", ""], ["Jeni", "Laszlo A.", ""], ["De la Torre", "Fernando", ""]]}, {"id": "2007.13678", "submitter": "Philippe Reiter", "authors": "Philippe Reiter", "title": "Cloud Detection through Wavelet Transforms in Machine Learning and Deep\n  Learning", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud detection is a specialized application of image recognition and object\ndetection using remotely sensed data. The task presents a number of challenges,\nincluding analyzing images obtained in visible, infrared and multi-spectral\nfrequencies, usually without ground truth data for comparison. Moreover,\nmachine learning and deep learning (MLDL) algorithms applied to this task are\nrequired to be computationally efficient, as they are typically deployed in\nlow-power devices and called to operate in real-time.\n  This paper explains Wavelet Transform (WT) theory, comparing it to more\nwidely used image and signal processing transforms, and explores the use of WT\nas a powerful signal compressor and feature extractor for MLDL classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 20:55:11 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Reiter", "Philippe", ""]]}, {"id": "2007.13683", "submitter": "Nilanjan Ray", "authors": "Abhishek Nan and Matthew Tennant and Uriel Rubin and Nilanjan Ray", "title": "Ordinary Differential Equation and Complex Matrix Exponential for\n  Multi-resolution Image Registration", "comments": "Software: https://github.com/abnan/ODECME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autograd-based software packages have recently renewed interest in image\nregistration using homography and other geometric models by gradient descent\nand optimization, e.g., AirLab and DRMIME. In this work, we emphasize on using\ncomplex matrix exponential (CME) over real matrix exponential to compute\ntransformation matrices. CME is theoretically more suitable and practically\nprovides faster convergence as our experiments show. Further, we demonstrate\nthat the use of an ordinary differential equation (ODE) as an optimizable\ndynamical system can adapt the transformation matrix more accurately to the\nmulti-resolution Gaussian pyramid for image registration. Our experiments\ninclude four publicly available benchmark datasets, two of them 2D and the\nother two being 3D. Experiments demonstrate that our proposed method yields\nsignificantly better registration compared to a number of off-the-shelf,\npopular, state-of-the-art image registration toolboxes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:51:25 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Nan", "Abhishek", ""], ["Tennant", "Matthew", ""], ["Rubin", "Uriel", ""], ["Ray", "Nilanjan", ""]]}, {"id": "2007.13693", "submitter": "Ferran Par\\'es", "authors": "Ferran Par\\'es, Anna Arias-Duart, Dario Garcia-Gasulla, Gema\n  Campo-Franc\\'es, Nina Viladrich, Eduard Ayguad\\'e, Jes\\'us Labarta", "title": "The MAMe Dataset: On the relevance of High Resolution and Variable Shape\n  image properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the image classification task, the most common approach is to resize all\nimages in a dataset to a unique shape, while reducing their precision to a size\nwhich facilitates experimentation at scale. This practice has benefits from a\ncomputational perspective, but it entails negative side-effects on performance\ndue to loss of information and image deformation. In this work we introduce the\nMAMe dataset, an image classification dataset with remarkable high resolution\nand variable shape properties. The goal of MAMe is to provide a tool for\nstudying the impact of such properties in image classification, while\nmotivating research in the field. The MAMe dataset contains thousands of\nartworks from three different museums, and proposes a classification task\nconsisting on differentiating between 29 mediums (i.e. materials and\ntechniques) supervised by art experts. After reviewing the singularity of MAMe\nin the context of current image classification tasks, a thorough description of\nthe task is provided, together with dataset statistics. Experiments are\nconducted to evaluate the impact of using high resolution images, variable\nshape inputs and both properties at the same time. Results illustrate the\npositive impact in performance when using high resolution images, while\nhighlighting the lack of solutions to exploit variable shapes. An additional\nexperiment exposes the distinctiveness between the MAMe dataset and the\nprototypical ImageNet dataset. Finally, the baselines are inspected using\nexplainability methods and expert knowledge, to gain insights on the challenges\nthat remain ahead.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:13:14 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 11:32:06 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 10:57:06 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Par\u00e9s", "Ferran", ""], ["Arias-Duart", "Anna", ""], ["Garcia-Gasulla", "Dario", ""], ["Campo-Franc\u00e9s", "Gema", ""], ["Viladrich", "Nina", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""]]}, {"id": "2007.13701", "submitter": "Anastasiia Kornilova", "authors": "Anatasiia Kornilova, Mikhail Salnikov, Olga Novitskaya, Maria\n  Begicheva, Egor Sevriugov, Kirill Shcherbakov, Valeriya Pronina, Dmitry V.\n  Dylov", "title": "Deep learning Framework for Mobile Microscopy", "comments": null, "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9434133", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mobile microscopy is a promising technology to assist and to accelerate\ndisease diagnostics, with its widespread adoption being hindered by the\nmediocre quality of acquired images. Although some paired image translation and\nsuper-resolution approaches for mobile microscopy have emerged, a set of\nessential challenges, necessary for automating it in a high-throughput setting,\nstill await to be addressed. The issues like in-focus/out-of-focus\nclassification, fast scanning deblurring, focus-stacking, etc. -- all have\nspecific peculiarities when the data are recorded using a mobile device. In\nthis work, we aspire to create a comprehensive pipeline by connecting a set of\nmethods purposely tuned to mobile microscopy: (1) a CNN model for stable\nin-focus / out-of-focus classification, (2) modified DeblurGAN architecture for\nimage deblurring, (3) FuseGAN model for combining in-focus parts from multiple\nimages to boost the detail. We discuss the limitations of the existing\nsolutions developed for professional clinical microscopes, propose\ncorresponding improvements, and compare to the other state-of-the-art mobile\nanalytics solutions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:27:59 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 10:46:49 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 14:51:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kornilova", "Anatasiia", ""], ["Salnikov", "Mikhail", ""], ["Novitskaya", "Olga", ""], ["Begicheva", "Maria", ""], ["Sevriugov", "Egor", ""], ["Shcherbakov", "Kirill", ""], ["Pronina", "Valeriya", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2007.13704", "submitter": "Javier Cremona", "authors": "Javier Cremona, Lucas Uzal, Taih\\'u Pire", "title": "WGANVO: Monocular Visual Odometry based on Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present WGANVO, a Deep Learning based monocular Visual\nOdometry method. In particular, a neural network is trained to regress a pose\nestimate from an image pair. The training is performed using a semi-supervised\napproach. Unlike geometry based monocular methods, the proposed method can\nrecover the absolute scale of the scene without neither prior knowledge nor\nextra information. The evaluation of the system is carried out on the\nwell-known KITTI dataset where it is shown to work in real time and the\naccuracy obtained is encouraging to continue the development of Deep Learning\nbased methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:31:24 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cremona", "Javier", ""], ["Uzal", "Lucas", ""], ["Pire", "Taih\u00fa", ""]]}, {"id": "2007.13712", "submitter": "Hsin-Hsiung Huang", "authors": "Chih-Wei Chen, Charles Harrison, and Hsin-Hsiung Huang", "title": "The Unsupervised Method of Vessel Movement Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world application scenarios, it is crucial for marine navigators and\nsecurity analysts to predict vessel movement trajectories at sea based on the\nAutomated Identification System (AIS) data in a given time span. This article\npresents an unsupervised method of ship movement trajectory prediction which\nrepresents the data in a three-dimensional space which consists of time\ndifference between points, the scaled error distance between the tested and its\npredicted forward and backward locations, and the space-time angle. The\nrepresentation feature space reduces the search scope for the next point to a\ncollection of candidates which fit the local path prediction well, and\ntherefore improve the accuracy. Unlike most statistical learning or deep\nlearning methods, the proposed clustering-based trajectory reconstruction\nmethod does not require computationally expensive model training. This makes\nreal-time reliable and accurate prediction feasible without using a training\nset. Our results show that the most prediction trajectories accurately consist\nof the true vessel paths.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:45:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 03:46:27 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 15:42:43 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Chih-Wei", ""], ["Harrison", "Charles", ""], ["Huang", "Hsin-Hsiung", ""]]}, {"id": "2007.13715", "submitter": "Kenzo Lobos-Tsunekawa", "authors": "Kenzo Lobos-Tsunekawa, Tatsuya Harada", "title": "Point Cloud Based Reinforcement Learning for Sim-to-Real and Partial\n  Observability in Visual Navigation", "comments": "Accepted to IROS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL), among other learning-based methods, represents\npowerful tools to solve complex robotic tasks (e.g., actuation, manipulation,\nnavigation, etc.), with the need for real-world data to train these systems as\none of its most important limitations. The use of simulators is one way to\naddress this issue, yet knowledge acquired in simulations does not work\ndirectly in the real-world, which is known as the sim-to-real transfer problem.\nWhile previous works focus on the nature of the images used as observations\n(e.g., textures and lighting), which has proven useful for a sim-to-sim\ntransfer, they neglect other concerns regarding said observations, such as\nprecise geometrical meanings, failing at robot-to-robot, and thus in\nsim-to-real transfers. We propose a method that learns on an observation space\nconstructed by point clouds and environment randomization, generalizing among\nrobots and simulators to achieve sim-to-real, while also addressing partial\nobservability. We demonstrate the benefits of our methodology on the point goal\nnavigation task, in which our method proves to be highly unaffected to unseen\nscenarios produced by robot-to-robot transfer, outperforms image-based\nbaselines in robot-randomized experiments, and presents high performances in\nsim-to-sim conditions. Finally, we perform several experiments to validate the\nsim-to-real transfer to a physical domestic robot platform, confirming the\nout-of-the-box performance of our system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:46:59 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Lobos-Tsunekawa", "Kenzo", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2007.13727", "submitter": "Shengyi Qian", "authors": "Shengyi Qian, Linyi Jin, David F. Fouhey", "title": "Associative3D: Volumetric Reconstruction from Sparse Views", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of 3D volumetric reconstruction from two views\nof a scene with an unknown camera. While seemingly easy for humans, this\nproblem poses many challenges for computers since it requires simultaneously\nreconstructing objects in the two views while also figuring out their\nrelationship. We propose a new approach that estimates reconstructions,\ndistributions over the camera/object and camera/camera transformations, as well\nas an inter-view object affinity matrix. This information is then jointly\nreasoned over to produce the most likely explanation of the scene. We train and\ntest our approach on a dataset of indoor scenes, and rigorously evaluate the\nmerits of our joint reasoning approach. Our experiments show that it is able to\nrecover reasonable scenes from sparse views, while the problem is still\nchallenging. Project site: https://jasonqsy.github.io/Associative3D\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:58:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Qian", "Shengyi", ""], ["Jin", "Linyi", ""], ["Fouhey", "David F.", ""]]}, {"id": "2007.13729", "submitter": "Chuang Gan", "authors": "Chuang Gan, Xiaoyu Chen, Phillip Isola, Antonio Torralba, Joshua B.\n  Tenenbaum", "title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events", "comments": "Project page: http://noisy-agent.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans integrate multiple sensory modalities (e.g. visual and audio) to build\na causal understanding of the physical world. In this work, we propose a novel\ntype of intrinsic motivation for Reinforcement Learning (RL) that encourages\nthe agent to understand the causal effect of its actions through auditory event\nprediction. First, we allow the agent to collect a small amount of acoustic\ndata and use K-means to discover underlying auditory event clusters. We then\ntrain a neural network to predict the auditory events and use the prediction\nerrors as intrinsic rewards to guide RL exploration. Experimental results on\nAtari games show that our new intrinsic motivation significantly outperforms\nseveral state-of-the-art baselines. We further visualize our noisy agents'\nbehavior in a physics environment and demonstrate that our newly designed\nintrinsic reward leads to the emergence of physical interaction behaviors (e.g.\ncontact with objects).\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:59:08 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gan", "Chuang", ""], ["Chen", "Xiaoyu", ""], ["Isola", "Phillip", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2007.13732", "submitter": "Bin Yang", "authors": "Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, Raquel\n  Urtasun", "title": "Learning Lane Graph Representations for Motion Forecasting", "comments": "ECCV 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a motion forecasting model that exploits a novel structured map\nrepresentation as well as actor-map interactions. Instead of encoding\nvectorized maps as raster images, we construct a lane graph from raw map data\nto explicitly preserve the map structure. To capture the complex topology and\nlong range dependencies of the lane graph, we propose LaneGCN which extends\ngraph convolutions with multiple adjacency matrices and along-lane dilation. To\ncapture the complex interactions between actors and maps, we exploit a fusion\nnetwork consisting of four types of interactions, actor-to-lane, lane-to-lane,\nlane-to-actor and actor-to-actor. Powered by LaneGCN and actor-map\ninteractions, our model is able to predict accurate and realistic multi-modal\ntrajectories. Our approach significantly outperforms the state-of-the-art on\nthe large scale Argoverse motion forecasting benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:59:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Hu", "Rui", ""], ["Chen", "Yun", ""], ["Liao", "Renjie", ""], ["Feng", "Song", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2007.13816", "submitter": "Kaiwen Duan", "authors": "Kaiwen Duan, Lingxi Xie, Honggang Qi, Song Bai, Qingming Huang, Qi\n  Tian", "title": "Corner Proposal Network for Anchor-free, Two-stage Object Detection", "comments": "18 pages (including 3 pages of References), 3 figures, 7 tables,\n  accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of object detection is to determine the class and location of\nobjects in an image. This paper proposes a novel anchor-free, two-stage\nframework which first extracts a number of object proposals by finding\npotential corner keypoint combinations and then assigns a class label to each\nproposal by a standalone classification stage. We demonstrate that these two\nstages are effective solutions for improving recall and precision,\nrespectively, and they can be integrated into an end-to-end network. Our\napproach, dubbed Corner Proposal Network (CPN), enjoys the ability to detect\nobjects of various scales and also avoids being confused by a large number of\nfalse-positive proposals. On the MS-COCO dataset, CPN achieves an AP of 49.2%\nwhich is competitive among state-of-the-art object detection methods. CPN also\nfits the scenario of computational efficiency, which achieves an AP of\n41.6%/39.7% at 26.2/43.3 FPS, surpassing most competitors with the same\ninference speed. Code is available at https://github.com/Duankaiwen/CPNDet\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:04:57 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Duan", "Kaiwen", ""], ["Xie", "Lingxi", ""], ["Qi", "Honggang", ""], ["Bai", "Song", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2007.13831", "submitter": "Tanveer Syeda-Mahmood", "authors": "Tanveer Syeda-Mahmood, Ken C. L. Wong, Yaniv Gur, Joy T. Wu, Ashutosh\n  Jadhav, Satyananda Kashyap, Alexandros Karargyris, Anup Pillai, Arjun Sharma,\n  Ali Bin Syed, Orest Boyko, Mehdi Moradi", "title": "Chest X-ray Report Generation through Fine-Grained Label Learning", "comments": "11 pages, 5 figures, to appear in MICCAI 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining automated preliminary read reports for common exams such as chest\nX-rays will expedite clinical workflows and improve operational efficiencies in\nhospitals. However, the quality of reports generated by current automated\napproaches is not yet clinically acceptable as they cannot ensure the correct\ndetection of a broad spectrum of radiographic findings nor describe them\naccurately in terms of laterality, anatomical location, severity, etc. In this\nwork, we present a domain-aware automatic chest X-ray radiology report\ngeneration algorithm that learns fine-grained description of findings from\nimages and uses their pattern of occurrences to retrieve and customize similar\nreports from a large report database. We also develop an automatic labeling\nalgorithm for assigning such descriptors to images and build a novel deep\nlearning network that recognizes both coarse and fine-grained descriptions of\nfindings. The resulting report generation algorithm significantly outperforms\nthe state of the art using established score metrics.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:50:56 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Syeda-Mahmood", "Tanveer", ""], ["Wong", "Ken C. L.", ""], ["Gur", "Yaniv", ""], ["Wu", "Joy T.", ""], ["Jadhav", "Ashutosh", ""], ["Kashyap", "Satyananda", ""], ["Karargyris", "Alexandros", ""], ["Pillai", "Anup", ""], ["Sharma", "Arjun", ""], ["Syed", "Ali Bin", ""], ["Boyko", "Orest", ""], ["Moradi", "Mehdi", ""]]}, {"id": "2007.13834", "submitter": "Shachar Praisler", "authors": "Eyal Gofer, Shachar Praisler and Guy Gilboa", "title": "Adaptive LiDAR Sampling and Depth Completion using Ensemble Variance", "comments": "for associated examples, see https://www.vision-and-sensing.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of depth completion, with or without image\ndata, where an algorithm may measure the depth of a prescribed limited number\nof pixels. The algorithmic challenge is to choose pixel positions strategically\nand dynamically to maximally reduce overall depth estimation error. This\nsetting is realized in daytime or nighttime depth completion for autonomous\nvehicles with a programmable LiDAR. Our method uses an ensemble of predictors\nto define a sampling probability over pixels. This probability is proportional\nto the variance of the predictions of ensemble members, thus highlighting\npixels that are difficult to predict. By additionally proceeding in several\nprediction phases, we effectively reduce redundant sampling of similar pixels.\nOur ensemble-based method may be implemented using any depth-completion\nlearning algorithm, such as a state-of-the-art neural network, treated as a\nblack box. In particular, we also present a simple and effective Random\nForest-based algorithm, and similarly use its internal ensemble in our design.\nWe conduct experiments on the KITTI dataset, using the neural network algorithm\nof Ma et al. and our Random Forest based learner for implementing our method.\nThe accuracy of both implementations exceeds the state of the art. Compared\nwith a random or grid sampling pattern, our method allows a reduction by a\nfactor of 4-10 in the number of measurements required to attain the same\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:54:42 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Gofer", "Eyal", ""], ["Praisler", "Shachar", ""], ["Gilboa", "Guy", ""]]}, {"id": "2007.13838", "submitter": "Alex Gaudio", "authors": "Asim Smailagic and Anupma Sharan and Pedro Costa and Adrian Galdran\n  and Alex Gaudio and Aur\\'elio Campilho", "title": "Learned Pre-Processing for Automatic Diabetic Retinopathy Detection on\n  Eye Fundus Images", "comments": "Accepted to International Conference on Image Analysis and\n  Recognition ICIAR 2019 Published at\n  https://doi.org/10.1007/978-3-030-27272-2_32", "journal-ref": null, "doi": "10.1007/978-3-030-27272-2_32", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy is the leading cause of blindness in the working-age\npopulation of the world. The main aim of this paper is to improve the accuracy\nof Diabetic Retinopathy detection by implementing a shadow removal and color\ncorrection step as a preprocessing stage from eye fundus images. For this, we\nrely on recent findings indicating that application of image dehazing on the\ninverted intensity domain amounts to illumination compensation. Inspired by\nthis work, we propose a Shadow Removal Layer that allows us to learn the\npre-processing function for a particular task. We show that learning the\npre-processing function improves the performance of the network on the Diabetic\nRetinopathy detection task.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:06:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Smailagic", "Asim", ""], ["Sharan", "Anupma", ""], ["Costa", "Pedro", ""], ["Galdran", "Adrian", ""], ["Gaudio", "Alex", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "2007.13839", "submitter": "Yifeng Zhang", "authors": "Yifeng Zhang, Ming Jiang, Qi Zhao", "title": "Saliency Prediction with External Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decades have seen great progress in saliency prediction, with the\nsuccess of deep neural networks that are able to encode high-level semantics.\nYet, while humans have the innate capability in leveraging their knowledge to\ndecide where to look (e.g. people pay more attention to familiar faces such as\ncelebrities), saliency prediction models have only been trained with large\neye-tracking datasets. This work proposes to bridge this gap by explicitly\nincorporating external knowledge for saliency models as humans do. We develop\nnetworks that learn to highlight regions by incorporating prior knowledge of\nsemantic relationships, be it general or domain-specific, depending on the task\nof interest. At the core of the method is a new Graph Semantic Saliency Network\n(GraSSNet) that constructs a graph that encodes semantic relationships learned\nfrom external knowledge. A Spatial Graph Attention Network is then developed to\nupdate saliency features based on the learned graph. Experiments show that the\nproposed model learns to predict saliency from the external knowledge and\noutperforms the state-of-the-art on four saliency benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:12:28 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhang", "Yifeng", ""], ["Jiang", "Ming", ""], ["Zhao", "Qi", ""]]}, {"id": "2007.13854", "submitter": "Alex Gaudio", "authors": "Qiqi Xiao and Jiaxu Zou and Muqiao Yang and Alex Gaudio and Kris\n  Kitani and Asim Smailagic and Pedro Costa and Min Xu", "title": "Improving Lesion Segmentation for Diabetic Retinopathy using Adversarial\n  Learning", "comments": "Accepted to International Conference on Image Analysis and\n  Recognition, ICIAR 2019. Published at\n  https://doi.org/10.1007/978-3-030-27272-2_29 Code:\n  https://github.com/zoujx96/DR-segmentation", "journal-ref": null, "doi": "10.1007/978-3-030-27272-2_29", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is a leading cause of blindness in working age\nadults. DR lesions can be challenging to identify in fundus images, and\nautomatic DR detection systems can offer strong clinical value. Of the publicly\navailable labeled datasets for DR, the Indian Diabetic Retinopathy Image\nDataset (IDRiD) presents retinal fundus images with pixel-level annotations of\nfour distinct lesions: microaneurysms, hemorrhages, soft exudates and hard\nexudates. We utilize the HEDNet edge detector to solve a semantic segmentation\ntask on this dataset, and then propose an end-to-end system for pixel-level\nsegmentation of DR lesions by incorporating HEDNet into a Conditional\nGenerative Adversarial Network (cGAN). We design a loss function that adds\nadversarial loss to segmentation loss. Our experiments show that the addition\nof the adversarial loss improves the lesion segmentation performance over the\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:43:36 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Xiao", "Qiqi", ""], ["Zou", "Jiaxu", ""], ["Yang", "Muqiao", ""], ["Gaudio", "Alex", ""], ["Kitani", "Kris", ""], ["Smailagic", "Asim", ""], ["Costa", "Pedro", ""], ["Xu", "Min", ""]]}, {"id": "2007.13866", "submitter": "Bowen Wen", "authors": "Bowen Wen, Chaitanya Mitash, Baozhang Ren, Kostas E. Bekris", "title": "se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n  Residuals in Synthetic Domains", "comments": null, "journal-ref": "International Conference on Intelligent Robots and Systems (IROS)\n  2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the 6D pose of objects in video sequences is important for robot\nmanipulation. This task, however, introduces multiple challenges: (i) robot\nmanipulation involves significant occlusions; (ii) data and annotations are\ntroublesome and difficult to collect for 6D poses, which complicates machine\nlearning solutions, and (iii) incremental error drift often accumulates in long\nterm tracking to necessitate re-initialization of the object's pose. This work\nproposes a data-driven optimization approach for long-term, 6D pose tracking.\nIt aims to identify the optimal relative pose given the current RGB-D\nobservation and a synthetic image conditioned on the previous best estimate and\nthe object's model. The key contribution in this context is a novel neural\nnetwork architecture, which appropriately disentangles the feature encoding to\nhelp reduce domain shift, and an effective 3D orientation representation via\nLie Algebra. Consequently, even when the network is trained only with synthetic\ndata can work effectively over real images. Comprehensive experiments over\nbenchmarks - existing ones as well as a new dataset with significant occlusions\nrelated to object manipulation - show that the proposed approach achieves\nconsistently robust estimates and outperforms alternatives, even though they\nhave been trained with real images. The approach is also the most\ncomputationally efficient among the alternatives and achieves a tracking\nfrequency of 90.9Hz.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:09:36 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wen", "Bowen", ""], ["Mitash", "Chaitanya", ""], ["Ren", "Baozhang", ""], ["Bekris", "Kostas E.", ""]]}, {"id": "2007.13867", "submitter": "Martin Humenberger", "authors": "Martin Humenberger and Yohann Cabon and Nicolas Guerin and Julien\n  Morat and J\\'er\\^ome Revaud and Philippe Rerole and No\\'e Pion and Cesar de\n  Souza and Vincent Leroy and Gabriela Csurka", "title": "Robust Image Retrieval-based Visual Localization using Kapture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a versatile method for visual localization. It is\nbased on robust image retrieval for coarse camera pose estimation and robust\nlocal features for accurate pose refinement. Our method is top ranked on\nvarious public datasets showing its ability of generalization and its great\nvariety of applications. To facilitate experiments, we introduce kapture, a\nflexible data format and processing pipeline for structure from motion and\nvisual localization that is released open source. We furthermore provide all\ndatasets used in this paper in the kapture format to facilitate research and\ndata processing. Code and datasets can be found at\nhttps://github.com/naver/kapture, more information, updates, and news can be\nfound at https://europe.naverlabs.com/research/3d-vision/kapture.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:10:35 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 12:47:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Humenberger", "Martin", ""], ["Cabon", "Yohann", ""], ["Guerin", "Nicolas", ""], ["Morat", "Julien", ""], ["Revaud", "J\u00e9r\u00f4me", ""], ["Rerole", "Philippe", ""], ["Pion", "No\u00e9", ""], ["de Souza", "Cesar", ""], ["Leroy", "Vincent", ""], ["Csurka", "Gabriela", ""]]}, {"id": "2007.13870", "submitter": "Lanlan Liu", "authors": "Lanlan Liu, Mingzhe Wang, Jia Deng", "title": "A Unified Framework of Surrogate Loss by Refactoring and Interpolation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce UniLoss, a unified framework to generate surrogate losses for\ntraining deep networks with gradient descent, reducing the amount of manual\ndesign of task-specific surrogate losses. Our key observation is that in many\ncases, evaluating a model with a performance metric on a batch of examples can\nbe refactored into four steps: from input to real-valued scores, from scores to\ncomparisons of pairs of scores, from comparisons to binary variables, and from\nbinary variables to the final performance metric. Using this refactoring we\ngenerate differentiable approximations for each non-differentiable step through\ninterpolation. Using UniLoss, we can optimize for different tasks and metrics\nusing one unified framework, achieving comparable performance compared with\ntask-specific losses. We validate the effectiveness of UniLoss on three tasks\nand four datasets. Code is available at\nhttps://github.com/princeton-vl/uniloss.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:16:51 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Liu", "Lanlan", ""], ["Wang", "Mingzhe", ""], ["Deng", "Jia", ""]]}, {"id": "2007.13886", "submitter": "Yan Zhang", "authors": "Yan Zhang and Michael J. Black and Siyu Tang", "title": "Perpetual Motion: Generating Unbounded Human Motion", "comments": "15 pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of human motion using machine learning methods has been widely\nstudied. In essence it is a time-series modeling problem involving predicting\nhow a person will move in the future given how they moved in the past. Existing\nmethods, however, typically have a short time horizon, predicting a only few\nframes to a few seconds of human motion. Here we focus on long-term prediction;\nthat is, generating long sequences (potentially infinite) of human motion that\nis plausible. Furthermore, we do not rely on a long sequence of input motion\nfor conditioning, but rather, can predict how someone will move from as little\nas a single pose. Such a model has many uses in graphics (video games and crowd\nanimation) and vision (as a prior for human motion estimation or for dataset\ncreation). To address this problem, we propose a model to generate\nnon-deterministic, \\textit{ever-changing}, perpetual human motion, in which the\nglobal trajectory and the body pose are cross-conditioned. We introduce a novel\nKL-divergence term with an implicit, unknown, prior. We train this using a\nheavy-tailed function of the KL divergence of a white-noise Gaussian process,\nallowing latent sequence temporal dependency. We perform systematic experiments\nto verify its effectiveness and find that it is superior to baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:50:36 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhang", "Yan", ""], ["Black", "Michael J.", ""], ["Tang", "Siyu", ""]]}, {"id": "2007.13887", "submitter": "Devendra Jangid", "authors": "Devendra K. Jangid, Neal R. Brodnik, Amil Khan, McLean P. Echlin,\n  Tresa M. Pollock, Sam Daly, B. S. Manjunath", "title": "3DMaterialGAN: Learning 3D Shape Representation from Latent Space for\n  Materials Science Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of computer vision, unsupervised learning for 2D object\ngeneration has advanced rapidly in the past few years. However, 3D object\ngeneration has not garnered the same attention or success as its predecessor.\nTo facilitate novel progress at the intersection of computer vision and\nmaterials science, we propose a 3DMaterialGAN network that is capable of\nrecognizing and synthesizing individual grains whose morphology conforms to a\ngiven 3D polycrystalline material microstructure. This Generative Adversarial\nNetwork (GAN) architecture yields complex 3D objects from probabilistic latent\nspace vectors with no additional information from 2D rendered images. We show\nthat this method performs comparably or better than state-of-the-art on\nbenchmark annotated 3D datasets, while also being able to distinguish and\ngenerate objects that are not easily annotated, such as grain morphologies. The\nvalue of our algorithm is demonstrated with analysis on experimental real-world\ndata, namely generating 3D grain structures found in a commercially relevant\nwrought titanium alloy, which were validated through statistical shape\ncomparison. This framework lays the foundation for the recognition and\nsynthesis of polycrystalline material microstructures, which are used in\nadditive manufacturing, aerospace, and structural design applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:55:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Jangid", "Devendra K.", ""], ["Brodnik", "Neal R.", ""], ["Khan", "Amil", ""], ["Echlin", "McLean P.", ""], ["Pollock", "Tresa M.", ""], ["Daly", "Sam", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2007.13890", "submitter": "Djebril Mekhazni", "authors": "Djebril Mekhazni, Amran Bhuiyan, George Ekladious and Eric Granger", "title": "Unsupervised Domain Adaptation in the Dissimilarity Space for Person\n  Re-identification", "comments": "14 pages (16 pages with references), 7 figures, conference ECCV", "journal-ref": null, "doi": "10.1007/978-3-030-58583-9_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) remains a challenging task in many real-word\nvideo analytics and surveillance applications, even though state-of-the-art\naccuracy has improved considerably with the advent of deep learning (DL) models\ntrained on large image datasets. Given the shift in distributions that\ntypically occurs between video data captured from the source and target\ndomains, and absence of labeled data from the target domain, it is difficult to\nadapt a DL model for accurate recognition of target data. We argue that for\npair-wise matchers that rely on metric learning, e.g., Siamese networks for\nperson ReID, the unsupervised domain adaptation (UDA) objective should consist\nin aligning pair-wise dissimilarity between domains, rather than aligning\nfeature representations. Moreover, dissimilarity representations are more\nsuitable for designing open-set ReID systems, where identities differ in the\nsource and target domains. In this paper, we propose a novel\nDissimilarity-based Maximum Mean Discrepancy (D-MMD) loss for aligning\npair-wise distances that can be optimized via gradient descent. From a person\nReID perspective, the evaluation of D-MMD loss is straightforward since the\ntracklet information allows to label a distance vector as being either\nwithin-class or between-class. This allows approximating the underlying\ndistribution of target pair-wise distances for D-MMD loss optimization, and\naccordingly align source and target distance distributions. Empirical results\nwith three challenging benchmark datasets show that the proposed D-MMD loss\ndecreases as source and domain distributions become more similar. Extensive\nexperimental evaluation also indicates that UDA methods that rely on the D-MMD\nloss can significantly outperform baseline and state-of-the-art UDA methods for\nperson ReID without the common requirement for data augmentation and/or complex\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 22:10:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Mekhazni", "Djebril", ""], ["Bhuiyan", "Amran", ""], ["Ekladious", "George", ""], ["Granger", "Eric", ""]]}, {"id": "2007.13903", "submitter": "Bahram Marami", "authors": "Bahram Marami and Atabak Reza Royaee", "title": "Automatic Detection and Classification of Waste Consumer Medications for\n  Proper Management and Disposal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year, millions of pounds of medicines remain unused in the U.S. and are\nsubject to an in-home disposal, i.e., kept in medicine cabinets, flushed in\ntoilet or thrown in regular trash. In-home disposal, however, can negatively\nimpact the environment and public health. The drug take-back programs (drug\ntake-backs) sponsored by the Drug Enforcement Administration (DEA) and its\nstate and industry partners collect unused consumer medications and provide the\nbest alternative to in-home disposal of medicines. However, the drug take-backs\nare expensive to operate and not widely available. In this paper, we show that\nartificial intelligence (AI) can be applied to drug take-backs to render them\noperationally more efficient. Since identification of any waste is crucial to a\nproper disposal, we showed that it is possible to accurately identify loose\nconsumer medications solely based on the physical features and visual\nappearance. We have developed an automatic technique that uses deep neural\nnetworks and computer vision to identify and segregate solid medicines. We\napplied the technique to images of about one thousand loose pills and succeeded\nin correctly identifying the pills with an accuracy of 0.912 and top-5 accuracy\nof 0.984. We also showed that hazardous pills could be distinguished from\nnon-hazardous pills within the dataset with an accuracy of 0.984. We believe\nthat the power of artificial intelligence could be harnessed in products that\nwould facilitate the operation of the drug take-backs more efficiently and help\nthem become widely available throughout the country.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 23:03:14 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Marami", "Bahram", ""], ["Royaee", "Atabak Reza", ""]]}, {"id": "2007.13912", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Yunsheng Li, Jose Costa Pereira, Mohammad Saberian and\n  Nuno Vasconcelos", "title": "Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings", "comments": "Accepted at International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hash codes are produced by binarizing the embeddings of convolutional\nneural networks (CNN) trained for either classification or retrieval. While\nproxy embeddings achieve good performance on both tasks, they are non-trivial\nto binarize, due to a rotational ambiguity that encourages non-binary\nembeddings. The use of a fixed set of proxies (weights of the CNN\nclassification layer) is proposed to eliminate this ambiguity, and a procedure\nto design proxy sets that are nearly optimal for both classification and\nhashing is introduced. The resulting hash-consistent large margin (HCLM)\nproxies are shown to encourage saturation of hashing units, thus guaranteeing a\nsmall binarization error, while producing highly discriminative hash-codes. A\nsemantic extension (sHCLM), aimed to improve hashing performance in a transfer\nscenario, is also proposed. Extensive experiments show that sHCLM embeddings\nachieve significant improvements over state-of-the-art hashing procedures on\nseveral small and large datasets, both within and beyond the set of training\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 23:47:43 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Morgado", "Pedro", ""], ["Li", "Yunsheng", ""], ["Pereira", "Jose Costa", ""], ["Saberian", "Mohammad", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2007.13913", "submitter": "David Chan", "authors": "David M. Chan, Sudheendra Vijayanarasimhan, David A. Ross, John Canny", "title": "Active Learning for Video Description With Cluster-Regularized Ensemble\n  Ranking", "comments": "Published at the 15th Asian Conference on Computer Vision (ACCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video captioning aims to train models to generate text descriptions\nfor all segments in a video, however, the most effective approaches require\nlarge amounts of manual annotation which is slow and expensive. Active learning\nis a promising way to efficiently build a training set for video captioning\ntasks while reducing the need to manually label uninformative examples. In this\nwork we both explore various active learning approaches for automatic video\ncaptioning and show that a cluster-regularized ensemble strategy provides the\nbest active learning approach to efficiently gather training sets for video\ncaptioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using\nboth transformer and LSTM based captioning models and show that our novel\nstrategy can achieve high performance while using up to 60% fewer training data\nthan the strong state of the art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 23:52:41 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 17:39:56 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 23:38:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chan", "David M.", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Ross", "David A.", ""], ["Canny", "John", ""]]}, {"id": "2007.13916", "submitter": "Senthil Purushwalkam", "authors": "Senthil Purushwalkam, Abhinav Gupta", "title": "Demystifying Contrastive Self-Supervised Learning: Invariances,\n  Augmentations and Dataset Biases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning approaches have recently surpassed\ntheir supervised learning counterparts on downstream tasks like object\ndetection and image classification. Somewhat mysteriously the recent gains in\nperformance come from training instance classification models, treating each\nimage and it's augmented versions as samples of a single class. In this work,\nwe first present quantitative experiments to demystify these gains. We\ndemonstrate that approaches like MOCO and PIRL learn occlusion-invariant\nrepresentations. However, they fail to capture viewpoint and category instance\ninvariance which are crucial components for object recognition. Second, we\ndemonstrate that these approaches obtain further gains from access to a clean\nobject-centric training dataset like Imagenet. Finally, we propose an approach\nto leverage unstructured videos to learn representations that possess higher\nviewpoint invariance. Our results show that the learned representations\noutperform MOCOv2 trained on the same data in terms of invariances encoded and\nthe performance on downstream image classification and semantic segmentation\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 00:11:31 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 05:38:11 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Purushwalkam", "Senthil", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2007.13928", "submitter": "Hung Hoang Manh", "authors": "Hoang Manh Hung, Hyung-Jeong Yang, Soo-Hyung Kim, and Guee-Sang Lee", "title": "Variants of BERT, Random Forests and SVM approach for Multimodal\n  Emotion-Target Sub-challenge", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion recognition has become a major problem in computer vision in recent\nyears that made a lot of effort by researchers to overcome the difficulties in\nthis task. In the field of affective computing, emotion recognition has a wide\nrange of applications, such as healthcare, robotics, human-computer\ninteraction. Due to its practical importance for other tasks, many techniques\nand approaches have been investigated for different problems and various data\nsources. Nevertheless, comprehensive fusion of the audio-visual and language\nmodalities to get the benefits from them is still a problem to solve. In this\npaper, we present and discuss our classification methodology for MuSe-Topic\nSub-challenge, as well as the data and results. For the topic classification,\nwe ensemble two language models which are ALBERT and RoBERTa to predict 10\nclasses of topics. Moreover, for the classification of valence and arousal, SVM\nand Random forests are employed in conjunction with feature selection to\nenhance the performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 01:15:50 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Hung", "Hoang Manh", ""], ["Yang", "Hyung-Jeong", ""], ["Kim", "Soo-Hyung", ""], ["Lee", "Guee-Sang", ""]]}, {"id": "2007.13952", "submitter": "Yuankai Huo", "authors": "Zheyu Zhu, Yuzhe Lu, Ruining Deng, Haichun Yang, Agnes B. Fogo,\n  Yuankai Huo", "title": "EasierPath: An Open-source Tool for Human-in-the-loop Deep Learning of\n  Renal Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable morphological phenotyping studies in nephrology have emerged in\nthe past few years, aiming to discover hidden regularities between clinical and\nimaging phenotypes. Such studies have been largely enabled by deep learning\nbased image analysis to extract sparsely located targeting objects (e.g.,\nglomeruli) on high-resolution whole slide images (WSI). However, such methods\nneed to be trained using labor-intensive high-quality annotations, ideally\nlabeled by pathologists. Inspired by the recent \"human-in-the-loop\" strategy,\nwe developed EasierPath, an open-source tool to integrate human physicians and\ndeep learning algorithms for efficient large-scale pathological image\nquantification as a loop. Using EasierPath, physicians are able to (1) optimize\nthe recall and precision of deep learning object detection outcomes adaptively,\n(2) seamlessly support deep learning outcomes refining using either our\nEasierPath or prevalent ImageScope software without changing physician's user\nhabit, and (3) manage and phenotype each object with user-defined classes. As a\nuser case of EasierPath, we present the procedure of curating large-scale\nglomeruli in an efficient human-in-the-loop fashion (with two loops). From the\nexperiments, the EasierPath saved 57 % of the annotation efforts to curate\n8,833 glomeruli during the second loop. Meanwhile, the average precision of\nglomerular detection was leveraged from 0.504 to 0.620. The EasierPath software\nhas been released as open-source to enable the large-scale glomerular\nprototyping. The code can be found in https://github.com/yuankaihuo/EasierPath\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 02:21:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhu", "Zheyu", ""], ["Lu", "Yuzhe", ""], ["Deng", "Ruining", ""], ["Yang", "Haichun", ""], ["Fogo", "Agnes B.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2007.13960", "submitter": "Wei Jing", "authors": "En Yen Puang and Keng Peng Tee and Wei Jing", "title": "KOVIS: Keypoint-based Visual Servoing with Zero-Shot Sim-to-Real\n  Transfer for Robotics Manipulation", "comments": "Accepted by IROS 2020", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9341370", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present KOVIS, a novel learning-based, calibration-free visual servoing\nmethod for fine robotic manipulation tasks with eye-in-hand stereo camera\nsystem. We train the deep neural network only in the simulated environment; and\nthe trained model could be directly used for real-world visual servoing tasks.\nKOVIS consists of two networks. The first keypoint network learns the keypoint\nrepresentation from the image using with an autoencoder. Then the visual\nservoing network learns the motion based on keypoints extracted from the camera\nimage. The two networks are trained end-to-end in the simulated environment by\nself-supervised learning without manual data labeling. After training with data\naugmentation, domain randomization, and adversarial examples, we are able to\nachieve zero-shot sim-to-real transfer to real-world robotic manipulation\ntasks. We demonstrate the effectiveness of the proposed method in both\nsimulated environment and real-world experiment with different robotic\nmanipulation tasks, including grasping, peg-in-hole insertion with 4mm\nclearance, and M13 screw insertion. The demo video is available at\nhttp://youtu.be/gfBJBR2tDzA\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 02:53:28 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Puang", "En Yen", ""], ["Tee", "Keng Peng", ""], ["Jing", "Wei", ""]]}, {"id": "2007.13970", "submitter": "Zengyi Qin", "authors": "Zengyi Qin, Jinglu Wang, Yan Lu", "title": "Weakly Supervised 3D Object Detection from Point Clouds", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial task in scene understanding is 3D object detection, which aims to\ndetect and localize the 3D bounding boxes of objects belonging to specific\nclasses. Existing 3D object detectors heavily rely on annotated 3D bounding\nboxes during training, while these annotations could be expensive to obtain and\nonly accessible in limited scenarios. Weakly supervised learning is a promising\napproach to reducing the annotation requirement, but existing weakly supervised\nobject detectors are mostly for 2D detection rather than 3D. In this work, we\npropose VS3D, a framework for weakly supervised 3D object detection from point\nclouds without using any ground truth 3D bounding box for training. First, we\nintroduce an unsupervised 3D proposal module that generates object proposals by\nleveraging normalized point cloud densities. Second, we present a cross-modal\nknowledge distillation strategy, where a convolutional neural network learns to\npredict the final results from the 3D object proposals by querying a teacher\nnetwork pretrained on image datasets. Comprehensive experiments on the\nchallenging KITTI dataset demonstrate the superior performance of our VS3D in\ndiverse evaluation settings. The source code and pretrained models are publicly\navailable at\nhttps://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 03:30:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Qin", "Zengyi", ""], ["Wang", "Jinglu", ""], ["Lu", "Yan", ""]]}, {"id": "2007.13971", "submitter": "Sibo Zhu", "authors": "Kieran Strobel, Sibo Zhu, Raphael Chang, Skanda Koppula", "title": "Accurate, Low-Latency Visual Perception for Autonomous\n  Racing:Challenges, Mechanisms, and Practical Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous racing provides the opportunity to test safety-critical perception\npipelines at their limit. This paper describes the practical challenges and\nsolutions to applying state-of-the-art computer vision algorithms to build a\nlow-latency, high-accuracy perception system for DUT18 Driverless (DUT18D), a\n4WD electric race car with podium finishes at all Formula Driverless\ncompetitions for which it raced. The key components of DUT18D include\nYOLOv3-based object detection, pose estimation, and time synchronization on its\ndual stereovision/monovision camera setup. We highlight modifications required\nto adapt perception CNNs to racing domains, improvements to loss functions used\nfor pose estimation, and methodologies for sub-microsecond camera\nsynchronization among other improvements. We perform a thorough experimental\nevaluation of the system, demonstrating its accuracy and low-latency in\nreal-world racing scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 03:33:41 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Strobel", "Kieran", ""], ["Zhu", "Sibo", ""], ["Chang", "Raphael", ""], ["Koppula", "Skanda", ""]]}, {"id": "2007.13976", "submitter": "Yoshiki Masuyama", "authors": "Yoshiki Masuyama, Yoshiaki Bando, Kohei Yatabe, Yoko Sasaki, Masaki\n  Onishi, Yasuhiro Oikawa", "title": "Self-supervised Neural Audio-Visual Sound Source Localization via\n  Probabilistic Spatial Modeling", "comments": "Accepted for publication in 2020 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting sound source objects within visual observation is important for\nautonomous robots to comprehend surrounding environments. Since sounding\nobjects have a large variety with different appearances in our living\nenvironments, labeling all sounding objects is impossible in practice. This\ncalls for self-supervised learning which does not require manual labeling. Most\nof conventional self-supervised learning uses monaural audio signals and images\nand cannot distinguish sound source objects having similar appearances due to\npoor spatial information in audio signals. To solve this problem, this paper\npresents a self-supervised training method using 360{\\deg} images and\nmultichannel audio signals. By incorporating with the spatial information in\nmultichannel audio signals, our method trains deep neural networks (DNNs) to\ndistinguish multiple sound source objects. Our system for localizing sound\nsource objects in the image is composed of audio and visual DNNs. The visual\nDNN is trained to localize sound source candidates within an input image. The\naudio DNN verifies whether each candidate actually produces sound or not. These\nDNNs are jointly trained in a self-supervised manner based on a probabilistic\nspatial audio model. Experimental results with simulated data showed that the\nDNNs trained by our method localized multiple speakers. We also demonstrate\nthat the visual DNN detected objects including talking visitors and specific\nexhibits from real data recorded in a science museum.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 03:52:53 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Masuyama", "Yoshiki", ""], ["Bando", "Yoshiaki", ""], ["Yatabe", "Kohei", ""], ["Sasaki", "Yoko", ""], ["Onishi", "Masaki", ""], ["Oikawa", "Yasuhiro", ""]]}, {"id": "2007.13988", "submitter": "Kyle Olszewski", "authors": "Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski,\n  Hao Li", "title": "Monocular Real-Time Volumetric Performance Capture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first approach to volumetric performance capture and\nnovel-view rendering at real-time speed from monocular video, eliminating the\nneed for expensive multi-view systems or cumbersome pre-acquisition of a\npersonalized template model. Our system reconstructs a fully textured 3D human\nfrom each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While\nPIFu achieves high-resolution reconstruction in a memory-efficient manner, its\ncomputationally expensive inference prevents us from deploying such a system\nfor real-time applications. To this end, we propose a novel hierarchical\nsurface localization algorithm and a direct rendering method without explicitly\nextracting surface meshes. By culling unnecessary regions for evaluation in a\ncoarse-to-fine manner, we successfully accelerate the reconstruction by two\norders of magnitude from the baseline without compromising the quality.\nFurthermore, we introduce an Online Hard Example Mining (OHEM) technique that\neffectively suppresses failure modes due to the rare occurrence of challenging\nexamples. We adaptively update the sampling probability of the training data\nbased on the current reconstruction accuracy, which effectively alleviates\nreconstruction artifacts. Our experiments and evaluations demonstrate the\nrobustness of our system to various challenging angles, illuminations, poses,\nand clothing styles. We also show that our approach compares favorably with the\nstate-of-the-art monocular performance capture. Our proposed approach removes\nthe need for multi-view studio settings and enables a consumer-accessible\nsolution for volumetric capture.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 04:45:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Ruilong", ""], ["Xiu", "Yuliang", ""], ["Saito", "Shunsuke", ""], ["Huang", "Zeng", ""], ["Olszewski", "Kyle", ""], ["Li", "Hao", ""]]}, {"id": "2007.13992", "submitter": "Junde Li", "authors": "Junde Li, Swaroop Ghosh", "title": "Quantum-soft QUBO Suppression for Accurate Object Detection", "comments": "Accepted on ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-maximum suppression (NMS) has been adopted by default for removing\nredundant object detections for decades. It eliminates false positives by only\nkeeping the image M with highest detection score and images whose overlap ratio\nwith M is less than a predefined threshold. However, this greedy algorithm may\nnot work well for object detection under occlusion scenario where true\npositives with lower detection scores are possibly suppressed. In this paper,\nwe first map the task of removing redundant detections into Quadratic\nUnconstrained Binary Optimization (QUBO) framework that consists of detection\nscore from each bounding box and overlap ratio between pair of bounding boxes.\nNext, we solve the QUBO problem using the proposed Quantum-soft QUBO\nSuppression (QSQS) algorithm for fast and accurate detection by exploiting\nquantum computing advantages. Experiments indicate that QSQS improves mean\naverage precision from 74.20% to 75.11% for PASCAL VOC 2007. It consistently\noutperforms NMS and soft-NMS for Reasonable subset of benchmark pedestrian\ndetection CityPersons.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 05:12:51 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Junde", ""], ["Ghosh", "Swaroop", ""]]}, {"id": "2007.13993", "submitter": "Jun Zhang", "authors": "Jun Zhang and Mina Henein and Robert Mahony and Viorela Ila", "title": "Robust Ego and Object 6-DoF Motion Estimation and Tracking", "comments": "7 pages, 6 figures, 5 tables, accepted in the IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of tracking self-motion as well as motion of objects in the scene\nusing information from a camera is known as multi-body visual odometry and is a\nchallenging task. This paper proposes a robust solution to achieve accurate\nestimation and consistent track-ability for dynamic multi-body visual odometry.\nA compact and effective framework is proposed leveraging recent advances in\nsemantic instance-level segmentation and accurate optical flow estimation. A\nnovel formulation, jointly optimizing SE(3) motion and optical flow is\nintroduced that improves the quality of the tracked points and the motion\nestimation accuracy. The proposed approach is evaluated on the virtual KITTI\nDataset and tested on the real KITTI Dataset, demonstrating its applicability\nto autonomous driving applications. For the benefit of the community, we make\nthe source code public.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 05:12:56 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhang", "Jun", ""], ["Henein", "Mina", ""], ["Mahony", "Robert", ""], ["Ila", "Viorela", ""]]}, {"id": "2007.14001", "submitter": "Hasara Maithree", "authors": "Hasara Maithree, Dilan Dinushka, Adeesha Wijayasiri", "title": "Change Detection Using Synthetic Aperture Radar Videos", "comments": null, "journal-ref": "AIRCC Publishing Corporation, Volume 10, Number 10, July 2020", "doi": "10.5121/csit.2020.101011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many researches have been carried out for change detection using temporal SAR\nimages. In this paper an algorithm for change detection using SAR videos has\nbeen proposed. There are various challenges related to SAR videos such as high\nlevel of speckle noise, rotation of SAR image frames of the video around a\nparticular axis due to the circular movement of airborne vehicle, non-uniform\nback scattering of SAR pulses. Hence conventional change detection algorithms\nused for optical videos and SAR temporal images cannot be directly utilized for\nSAR videos. We propose an algorithm which is a combination of optical flow\ncalculation using Lucas Kanade (LK) method and blob detection. The developed\nmethod follows a four steps approach: image filtering and enhancement, applying\nLK method, blob analysis and combining LK method with blob analysis. The\nperformance of the developed approach was tested on SAR videos available on\nSandia National Laboratories website and SAR videos generated by a SAR\nsimulator.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 05:53:10 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Maithree", "Hasara", ""], ["Dinushka", "Dilan", ""], ["Wijayasiri", "Adeesha", ""]]}, {"id": "2007.14006", "submitter": "Danfeng Hong", "authors": "Lianru Gao and Danfeng Hong and Jing Yao and Bing Zhang and Paolo\n  Gamba and Jocelyn Chanussot", "title": "Spectral Superresolution of Multispectral Imagery with Joint Sparse and\n  Low-Rank Learning", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2020", "doi": "10.1109/TGRS.2020.3000684", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive attention has been widely paid to enhance the spatial resolution of\nhyperspectral (HS) images with the aid of multispectral (MS) images in remote\nsensing. However, the ability in the fusion of HS and MS images remains to be\nimproved, particularly in large-scale scenes, due to the limited acquisition of\nHS images. Alternatively, we super-resolve MS images in the spectral domain by\nthe means of partially overlapped HS images, yielding a novel and promising\ntopic: spectral superresolution (SSR) of MS imagery. This is challenging and\nless investigated task due to its high ill-posedness in inverse imaging. To\nthis end, we develop a simple but effective method, called joint sparse and\nlow-rank learning (J-SLoL), to spectrally enhance MS images by jointly learning\nlow-rank HS-MS dictionary pairs from overlapped regions. J-SLoL infers and\nrecovers the unknown hyperspectral signals over a larger coverage by sparse\ncoding on the learned dictionary pair. Furthermore, we validate the SSR\nperformance on three HS-MS datasets (two for classification and one for\nunmixing) in terms of reconstruction, classification, and unmixing by comparing\nwith several existing state-of-the-art baselines, showing the effectiveness and\nsuperiority of the proposed J-SLoL algorithm. Furthermore, the codes and\ndatasets will be available at:\nhttps://github.com/danfenghong/IEEE\\_TGRS\\_J-SLoL, contributing to the RS\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 06:08:44 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Gao", "Lianru", ""], ["Hong", "Danfeng", ""], ["Yao", "Jing", ""], ["Zhang", "Bing", ""], ["Gamba", "Paolo", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2007.14007", "submitter": "Danfeng Hong", "authors": "Ke Zheng and Lianru Gao and Wenzhi Liao and Danfeng Hong and Bing\n  Zhang and Ximin Cui and Jocelyn Chanussot", "title": "Coupled Convolutional Neural Network with Adaptive Response Function\n  Learning for Unsupervised Hyperspectral Super-Resolution", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing,2020", "doi": "10.1109/TGRS.2020.3006534", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limitations of hyperspectral imaging systems, hyperspectral\nimagery (HSI) often suffers from poor spatial resolution, thus hampering many\napplications of the imagery. Hyperspectral super-resolution refers to fusing\nHSI and MSI to generate an image with both high spatial and high spectral\nresolutions. Recently, several new methods have been proposed to solve this\nfusion problem, and most of these methods assume that the prior information of\nthe Point Spread Function (PSF) and Spectral Response Function (SRF) are known.\nHowever, in practice, this information is often limited or unavailable. In this\nwork, an unsupervised deep learning-based fusion method - HyCoNet - that can\nsolve the problems in HSI-MSI fusion without the prior PSF and SRF information\nis proposed. HyCoNet consists of three coupled autoencoder nets in which the\nHSI and MSI are unmixed into endmembers and abundances based on the linear\nunmixing model. Two special convolutional layers are designed to act as a\nbridge that coordinates with the three autoencoder nets, and the PSF and SRF\nparameters are learned adaptively in the two convolution layers during the\ntraining process. Furthermore, driven by the joint loss function, the proposed\nmethod is straightforward and easily implemented in an end-to-end training\nmanner. The experiments performed in the study demonstrate that the proposed\nmethod performs well and produces robust results for different datasets and\narbitrary PSFs and SRFs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 06:17:02 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zheng", "Ke", ""], ["Gao", "Lianru", ""], ["Liao", "Wenzhi", ""], ["Hong", "Danfeng", ""], ["Zhang", "Bing", ""], ["Cui", "Ximin", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2007.14033", "submitter": "Taner Ince", "authors": "Taner Ince", "title": "Superpixel Based Graph Laplacian Regularization for Sparse Hyperspectral\n  Unmixing", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient spatial regularization method using superpixel segmentation and\ngraph Laplacian regularization is proposed for sparse hyperspectral unmixing\nmethod. Since it is likely to find spectrally similar pixels in a homogeneous\nregion, we use a superpixel segmentation algorithm to extract the homogeneous\nregions by considering the image boundaries. We first extract the homogeneous\nregions, which are called superpixels, then a weighted graph in each superpixel\nis constructed by selecting $K$-nearest pixels in each superpixel. Each node in\nthe graph represents the spectrum of a pixel and edges connect the similar\npixels inside the superpixel. The spatial similarity is investigated using\ngraph Laplacian regularization. Sparsity regularization for abundance matrix is\nprovided using a weighted sparsity promoting norm. Experimental results on\nsimulated and real data sets show the superiority of the proposed algorithm\nover the well-known algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 07:30:50 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 13:12:49 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ince", "Taner", ""]]}, {"id": "2007.14035", "submitter": "Alexander Schperberg", "authors": "Alexander Schperberg, Kenny Chen, Stephanie Tsuei, Michael Jewett,\n  Joshua Hooks, Stefano Soatto, Ankur Mehta, Dennis Hong", "title": "Risk-Averse MPC via Visual-Inertial Input and Recurrent Networks for\n  Online Collision Avoidance", "comments": "Accepted to the 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), Las Vegas, USA. First two authors contributed\n  equally. For supplementary video, see\n  https://www.youtube.com/watch?v=td4K55Tj-U8", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9341070", "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an online path planning architecture that extends\nthe model predictive control (MPC) formulation to consider future location\nuncertainties for safer navigation through cluttered environments. Our\nalgorithm combines an object detection pipeline with a recurrent neural network\n(RNN) which infers the covariance of state estimates through each step of our\nMPC's finite time horizon. The RNN model is trained on a dataset that comprises\nof robot and landmark poses generated from camera images and inertial\nmeasurement unit (IMU) readings via a state-of-the-art visual-inertial odometry\nframework. To detect and extract object locations for avoidance, we use a\ncustom-trained convolutional neural network model in conjunction with a feature\nextractor to retrieve 3D centroid and radii boundaries of nearby obstacles. The\nrobustness of our methods is validated on complex quadruped robot dynamics and\ncan be generally applied to most robotic platforms, demonstrating autonomous\nbehaviors that can plan fast and collision-free paths towards a goal point.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 07:34:30 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Schperberg", "Alexander", ""], ["Chen", "Kenny", ""], ["Tsuei", "Stephanie", ""], ["Jewett", "Michael", ""], ["Hooks", "Joshua", ""], ["Soatto", "Stefano", ""], ["Mehta", "Ankur", ""], ["Hong", "Dennis", ""]]}, {"id": "2007.14050", "submitter": "Yuanqi Chen", "authors": "Yuanqi Chen, Xiaoming Yu, Shan Liu, Ge Li", "title": "Toward Zero-Shot Unsupervised Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable success in unsupervised image-to-image\ntranslation. However, if there has no access to enough images in target\nclasses, learning a mapping from source classes to the target classes always\nsuffers from mode collapse, which limits the application of the existing\nmethods. In this work, we propose a zero-shot unsupervised image-to-image\ntranslation framework to address this limitation, by associating categories\nwith their side information like attributes. To generalize the translator to\nprevious unseen classes, we introduce two strategies for exploiting the space\nspanned by the semantic attributes. Specifically, we propose to preserve\nsemantic relations to the visual space and expand attribute space by utilizing\nattribute vectors of unseen classes, thus encourage the translator to explore\nthe modes of unseen classes. Quantitative and qualitative results on different\ndatasets demonstrate the effectiveness of our proposed approach. Moreover, we\ndemonstrate that our framework can be applied to many tasks, such as zero-shot\nclassification and fashion design.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 08:13:18 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Yuanqi", ""], ["Yu", "Xiaoming", ""], ["Liu", "Shan", ""], ["Li", "Ge", ""]]}, {"id": "2007.14103", "submitter": "Moses Bomera", "authors": "Andrew Katumba, Moses Bomera, Cosmas Mwikirize, Gorret Namulondo, Mary\n  Gorret Ajero, Idd Ramathani, Olivia Nakayima, Grace Nakabonge, Dorothy\n  Okello, Jonathan Serugunda", "title": "A Deep Learning-based Detector for Brown Spot Disease in Passion Fruit\n  Plant Leaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pests and diseases pose a key challenge to passion fruit farmers across\nUganda and East Africa in general. They lead to loss of investment as yields\nreduce and losses increases. As the majority of the farmers, including passion\nfruit farmers, in the country are smallholder farmers from low-income\nhouseholds, they do not have the sufficient information and means to combat\nthese challenges. While, passion fruits have the potential to improve the\nwell-being of these farmers as they have a short maturity period and high\nmarket value , without the required knowledge about the health of their crops,\nfarmers cannot intervene promptly to turn the situation around.\n  For this work, we have partnered with the Uganda National Crop Research\nInstitute (NaCRRI) to develop a dataset of expertly labelled passion fruit\nplant leaves and fruits, both diseased and healthy. We have made use of their\nextension service to collect images from 5 districts in Uganda,\n  With the dataset in place, we are employing state-of-the-art techniques in\nmachine learning, and specifically deep learning, techniques at scale for\nobject detection and classification to correctly determine the health status of\npassion fruit plants and provide an accurate diagnosis for positive\ndetections.This work focuses on two major diseases woodiness (viral) and brown\nspot (fungal) diseases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:17:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:51:21 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Katumba", "Andrew", ""], ["Bomera", "Moses", ""], ["Mwikirize", "Cosmas", ""], ["Namulondo", "Gorret", ""], ["Ajero", "Mary Gorret", ""], ["Ramathani", "Idd", ""], ["Nakayima", "Olivia", ""], ["Nakabonge", "Grace", ""], ["Okello", "Dorothy", ""], ["Serugunda", "Jonathan", ""]]}, {"id": "2007.14110", "submitter": "Shaolei Liu", "authors": "Shaolei Liu, Manning Wang, Zhijian Song", "title": "WaveFuse: A Unified Deep Framework for Image Fusion with Discrete\n  Wavelet Transform", "comments": "12 pages,11 figures,5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised image fusion architecture for multiple application\nscenarios based on the combination of multi-scale discrete wavelet transform\nthrough regional energy and deep learning. To our best knowledge, this is the\nfirst time the conventional image fusion method has been combined with deep\nlearning. The useful information of feature maps can be utilized adequately\nthrough multi-scale discrete wavelet transform in our proposed method.Compared\nwith other state-of-the-art fusion method, the proposed algorithm exhibits\nbetter fusion performance in both subjective and objective evaluation.\nMoreover, it's worth mentioning that comparable fusion performance trained in\nCOCO dataset can be obtained by training with a much smaller dataset with only\nhundreds of images chosen randomly from COCO. Hence, the training time is\nshortened substantially, leading to the improvement of the model's performance\nboth in practicality and training efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:30:47 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 13:16:37 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 14:17:21 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Shaolei", ""], ["Wang", "Manning", ""], ["Song", "Zhijian", ""]]}, {"id": "2007.14118", "submitter": "Julia Wolleb", "authors": "Julia Wolleb, Robin Sandk\\\"uhler and Philippe C. Cattin", "title": "DeScarGAN: Disease-Specific Anomaly Detection with Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection and localization in medical images is a challenging task,\nespecially when the anomaly exhibits a change of existing structures, e.g.,\nbrain atrophy or changes in the pleural space due to pleural effusions. In this\nwork, we present a weakly supervised and detail-preserving method that is able\nto detect structural changes of existing anatomical structures. In contrast to\nstandard anomaly detection methods, our method extracts information about the\ndisease characteristics from two groups: a group of patients affected by the\nsame disease and a healthy control group. Together with identity-preserving\nmechanisms, this enables our method to extract highly disease-specific\ncharacteristics for a more detailed detection of structural changes. We\ndesigned a specific synthetic data set to evaluate and compare our method\nagainst state-of-the-art anomaly detection methods. Finally, we show the\nperformance of our method on chest X-ray images. Our method called DeScarGAN\noutperforms other anomaly detection methods on the synthetic data set and by\nvisual inspection on the chest X-ray image data set.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:50:38 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wolleb", "Julia", ""], ["Sandk\u00fchler", "Robin", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "2007.14120", "submitter": "Anna-Kathrin Kopetzki", "authors": "Anna-Kathrin Kopetzki, Stephan G\\\"unnemann", "title": "Reachable Sets of Classifiers and Regression Models: (Non-)Robustness\n  Analysis and Robust Training", "comments": "Published as a journal paper at ECML PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks achieve outstanding accuracy in classification and regression\ntasks. However, understanding their behavior still remains an open challenge\nthat requires questions to be addressed on the robustness, explainability and\nreliability of predictions. We answer these questions by computing reachable\nsets of neural networks, i.e. sets of outputs resulting from continuous sets of\ninputs. We provide two efficient approaches that lead to over- and\nunder-approximations of the reachable set. This principle is highly versatile,\nas we show. First, we use it to analyze and enhance the robustness properties\nof both classifiers and regression models. This is in contrast to existing\nworks, which are mainly focused on classification. Specifically, we verify\n(non-)robustness, propose a robust training procedure, and show that our\napproach outperforms adversarial attacks as well as state-of-the-art methods of\nverifying classifiers for non-norm bound perturbations. Second, we provide\ntechniques to distinguish between reliable and non-reliable predictions for\nunlabeled inputs, to quantify the influence of each feature on a prediction,\nand compute a feature ranking.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:58:06 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 16:38:47 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kopetzki", "Anna-Kathrin", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2007.14126", "submitter": "Daniel Rodriguez Criado", "authors": "Daniel Rodriguez-Criado, Pilar Bachiller, Pablo Bustos, George\n  Vogiatzis, Luis J. Manso", "title": "Multi-camera Torso Pose Estimation using Graph Neural Networks", "comments": "6 pages, accepted in ROMAN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Estimating the location and orientation of humans is an essential skill for\nservice and assistive robots. To achieve a reliable estimation in a wide area\nsuch as an apartment, multiple RGBD cameras are frequently used. Firstly, these\nsetups are relatively expensive. Secondly, they seldom perform an effective\ndata fusion using the multiple camera sources at an early stage of the\nprocessing pipeline. Occlusions and partial views make this second point very\nrelevant in these scenarios. The proposal presented in this paper makes use of\ngraph neural networks to merge the information acquired from multiple camera\nsources, achieving a mean absolute error below 125 mm for the location and 10\ndegrees for the orientation using low-resolution RGB images. The experiments,\nconducted in an apartment with three cameras, benchmarked two different graph\nneural network implementations and a third architecture based on fully\nconnected layers. The software used has been released as open-source in a\npublic repository (https://github.com/vangiel/WheresTheFellow).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 11:14:02 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Rodriguez-Criado", "Daniel", ""], ["Bachiller", "Pilar", ""], ["Bustos", "Pablo", ""], ["Vogiatzis", "George", ""], ["Manso", "Luis J.", ""]]}, {"id": "2007.14137", "submitter": "Tai-Xiang Jiang", "authors": "Tai-Xiang Jiang, Michael K. Ng, Junjun Pan, Guangjing Song", "title": "Nonnegative Low Rank Tensor Approximation and its Application to\n  Multi-dimensional Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of this paper is to develop a new algorithm for computing\nNonnegative Low Rank Tensor (NLRT) approximation for nonnegative tensors that\narise in many multi-dimensional imaging applications. Nonnegativity is one of\nthe important property as each pixel value refer to nonzero light intensity in\nimage data acquisition. Our approach is different from classical nonnegative\ntensor factorization (NTF) which has been studied for many years. For a given\nnonnegative tensor, the classical NTF approach is to determine nonnegative low\nrank tensor by computing factor matrices or tensors (for example, CPD finds\nfactor matrices while Tucker decomposition finds core tensor and factor\nmatrices), such that the distance between this nonnegative low rank tensor and\ngiven tensor is as small as possible. The proposed NLRT approach is different\nfrom the classical NTF. It determines a nonnegative low rank tensor without\nusing decompositions or factorization methods. The minimized distance by the\nproposed NLRT method can be smaller than that by the NTF method, and it implies\nthat the proposed NLRT method can obtain a better low rank tensor\napproximation. The proposed NLRT approximation algorithm is derived by using\nthe alternating averaged projection on the product of low rank matrix manifolds\nand non-negativity property. We show the convergence of the alternating\nprojection algorithm. Experimental results for synthetic data and\nmulti-dimensional images are presented to demonstrate the performance of the\nproposed NLRT method is better than that of existing NTF methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 11:52:19 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Jiang", "Tai-Xiang", ""], ["Ng", "Michael K.", ""], ["Pan", "Junjun", ""], ["Song", "Guangjing", ""]]}, {"id": "2007.14164", "submitter": "Wenhao Jiang", "authors": "Shaoxiang Chen, Wenhao Jiang, Wei Liu, Yu-Gang Jiang", "title": "Learning Modality Interaction for Temporal Sentence Localization and\n  Event Captioning in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating sentences to describe events and temporally\nlocalizing sentences in a video are two important tasks that bridge language\nand videos. Recent techniques leverage the multimodal nature of videos by using\noff-the-shelf features to represent videos, but interactions between modalities\nare rarely explored. Inspired by the fact that there exist cross-modal\ninteractions in the human brain, we propose a novel method for learning\npairwise modality interactions in order to better exploit complementary\ninformation for each pair of modalities in videos and thus improve performances\non both tasks. We model modality interaction in both the sequence and channel\nlevels in a pairwise fashion, and the pairwise interaction also provides some\nexplainability for the predictions of target tasks. We demonstrate the\neffectiveness of our method and validate specific design choices through\nextensive ablation studies. Our method turns out to achieve state-of-the-art\nperformances on four standard benchmark datasets: MSVD and MSR-VTT (event\ncaptioning task), and Charades-STA and ActivityNet Captions (temporal sentence\nlocalization task).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:40:59 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Shaoxiang", ""], ["Jiang", "Wenhao", ""], ["Liu", "Wei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2007.14177", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Jing Zhang, Fuzhi Wu, Youyong Kong, Guanyu Yang, Lotfi\n  Senhadji, Huazhong Shu", "title": "Generative networks as inverse problems with fractional wavelet\n  scattering networks", "comments": "27 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning is a hot research topic in the field of machine learning\nmethods and applications. Generative Adversarial Networks (GANs) and\nVariational Auto-Encoders (VAEs) provide impressive image generations from\nGaussian white noise, but both of them are difficult to train since they need\nto train the generator (or encoder) and the discriminator (or decoder)\nsimultaneously, which is easy to cause unstable training. In order to solve or\nalleviate the synchronous training difficult problems of GANs and VAEs,\nrecently, researchers propose Generative Scattering Networks (GSNs), which use\nwavelet scattering networks (ScatNets) as the encoder to obtain the features\n(or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder\nto generate the image. The advantage of GSNs is the parameters of ScatNets are\nnot needed to learn, and the disadvantage of GSNs is that the expression\nability of ScatNets is slightly weaker than CNNs and the dimensional reduction\nmethod of Principal Component Analysis (PCA) is easy to lead overfitting in the\ntraining of GSNs, and therefore affect the generated quality in the testing\nprocess. In order to further improve the quality of generated images while keep\nthe advantages of GSNs, this paper proposes Generative Fractional Scattering\nNetworks (GFRSNs), which use more expressive fractional wavelet scattering\nnetworks (FrScatNets) instead of ScatNets as the encoder to obtain the features\n(or FrScatNet embeddings) and use the similar CNNs of GSNs as the decoder to\ngenerate the image. Additionally, this paper develops a new dimensional\nreduction method named Feature-Map Fusion (FMF) instead of PCA for better\nkeeping the information of FrScatNets and the effect of image fusion on the\nquality of image generation is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:58:15 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wu", "Jiasong", ""], ["Zhang", "Jing", ""], ["Wu", "Fuzhi", ""], ["Kong", "Youyong", ""], ["Yang", "Guanyu", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "2007.14178", "submitter": "Alptekin Temizel", "authors": "Mete Can Kaya, Alperen \\.Inci, Alptekin Temizel", "title": "Optimization of XNOR Convolution for Binary Convolutional Neural\n  Networks on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary convolutional networks have lower computational load and lower memory\nfoot-print compared to their full-precision counterparts. So, they are a\nfeasible alternative for the deployment of computer vision applications on\nlimited capacity embedded devices. Once trained on less resource-constrained\ncomputational environments, they can be deployed for real-time inference on\nsuch devices. In this study, we propose an implementation of binary\nconvolutional network inference on GPU by focusing on optimization of XNOR\nconvolution. Experimental results show that using GPU can provide a speed-up of\nup to $42.61\\times$ with a kernel size of $3\\times3$. The implementation is\npublicly available at\nhttps://github.com/metcan/Binary-Convolutional-Neural-Network-Inference-on-GPU\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:01:17 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Kaya", "Mete Can", ""], ["\u0130nci", "Alperen", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2007.14226", "submitter": "Marimuthu Kalimuthu", "authors": "Marimuthu Kalimuthu, Fabrizio Nunnari, Daniel Sonntag", "title": "A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption\n  2020 Task", "comments": "Camera-ready version for ImageCLEF-2020.\n  http://ceur-ws.org/Vol-2696/paper_93.pdf", "journal-ref": "CEUR-WS, Volume 2696, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of ImageCLEFmed Caption task is to develop a system that\nautomatically labels radiology images with relevant medical concepts. We\ndescribe our Deep Neural Network (DNN) based approach for tackling this\nproblem. On the challenge test set of 3,534 radiology images, our system\nachieves an F1 score of 0.375 and ranks high, 12th among all systems that were\nsuccessfully submitted to the challenge, whereby we only rely on the provided\ndata sources and do not use any external medical knowledge or ontologies, or\npretrained models from other medical image repositories or application domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 09:30:13 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 23:00:51 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 19:34:50 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kalimuthu", "Marimuthu", ""], ["Nunnari", "Fabrizio", ""], ["Sonntag", "Daniel", ""]]}, {"id": "2007.14235", "submitter": "Tim Pearce", "authors": "Tim Pearce, Andrew Y.K. Foong, Alexandra Brintrup", "title": "Structured Weight Priors for Convolutional Neural Networks", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection of an architectural prior well suited to a task (e.g. convolutions\nfor image data) is crucial to the success of deep neural networks (NNs).\nConversely, the weight priors within these architectures are typically left\nvague, e.g.~independent Gaussian distributions, which has led to debate over\nthe utility of Bayesian deep learning. This paper explores the benefits of\nadding structure to weight priors. It initially considers first-layer filters\nof a convolutional NN, designing a prior based on random Gabor filters. Second,\nit considers adding structure to the prior of final-layer weights by estimating\nhow each hidden feature relates to each class. Empirical results suggest that\nthese structured weight priors lead to more meaningful functional priors for\nimage data. This contributes to the ongoing discussion on the importance of\nweight priors.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 13:05:51 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Pearce", "Tim", ""], ["Foong", "Andrew Y. K.", ""], ["Brintrup", "Alexandra", ""]]}, {"id": "2007.14239", "submitter": "Gabriel Bernardino", "authors": "Gabriel Bernardino, Oualid Benkarim, Mar\\'ia Sanz-de la Garza, Susanna\n  Prat-Gonz\\`alez, \\'Alvaro Sepulveda-Martinez, F\\`atima Crispi, Marta Sitges,\n  Mathieu De Craene, Bart Bijnens, Miguel \\'Angel Gonz\\'alez Ballester", "title": "Handling confounding variables in statistical shape analysis --\n  application to cardiac remodelling", "comments": "This paper has been acccepted for publication in Medical Image\n  Analysis. Please find the final version with its supplementary materials at\n  doi.org/10.1016/j.media.2020.101792. Shared under license CC-BY-NC-ND", "journal-ref": null, "doi": "10.1016/j.media.2020.101792", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape analysis is a powerful tool to assess organ morphologies\nand find shape changes associated to a particular disease. However, imbalance\nin confounding factors, such as demographics might invalidate the analysis if\nnot taken into consideration. Despite the methodological advances in the field,\nproviding new methods that are able to capture complex and regional shape\ndifferences, the relationship between non-imaging information and shape\nvariability has been overlooked. We present a linear statistical shape analysis\nframework that finds shape differences unassociated to a controlled set of\nconfounding variables. It includes two confounding correction methods:\nconfounding deflation and adjustment. We applied our framework to a cardiac\nmagnetic resonance imaging dataset, consisting of the cardiac ventricles of 89\ntriathletes and 77 controls, to identify cardiac remodelling due to the\npractice of endurance exercise. To test robustness to confounders, subsets of\nthis dataset were generated by randomly removing controls with low body mass\nindex, thus introducing imbalance. The analysis of the whole dataset indicates\nan increase of ventricular volumes and myocardial mass in athletes, which is\nconsistent with the clinical literature. However, when confounders are not\ntaken into consideration no increase of myocardial mass is found. Using the\ndownsampled datasets, we find that confounder adjustment methods are needed to\nfind the real remodelling patterns in imbalanced datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:05:34 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bernardino", "Gabriel", ""], ["Benkarim", "Oualid", ""], ["la Garza", "Mar\u00eda Sanz-de", ""], ["Prat-Gonz\u00e0lez", "Susanna", ""], ["Sepulveda-Martinez", "\u00c1lvaro", ""], ["Crispi", "F\u00e0tima", ""], ["Sitges", "Marta", ""], ["De Craene", "Mathieu", ""], ["Bijnens", "Bart", ""], ["Ballester", "Miguel \u00c1ngel Gonz\u00e1lez", ""]]}, {"id": "2007.14245", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "Bayesian Multi Scale Neural Network for Crowd Counting", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd Counting is a difficult but important problem in computer vision.\nConvolutional Neural Networks based on estimating the density map over the\nimage has been highly successful in this domain. However dense crowd counting\nremains an open problem because of severe occlusion and perspective view in\nwhich people can be present at various sizes. In this work, we propose a new\nnetwork which uses a ResNet based feature extractor, downsampling block which\nuses dilated convolutions and upsampling block using transposed convolutions.\nWe present a novel aggregation module which makes our network robust to the\nperspective view problem. We present the optimization details, loss functions\nand the algorithm used in our work. On evaluating on ShanghaiTech, UCF-CC-50\nand UCF-QNRF datasets using MSE and MAE as evaluation metrics, our network\noutperforms previous state of the art approaches while giving uncertainty\nestimates in a principled bayesian manner.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:43:20 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 19:57:51 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2007.14249", "submitter": "Tiange Luo", "authors": "Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, Liwei Wang", "title": "RANDOM MASK: Towards Robust Convolutional Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.08432", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of neural networks has recently been highlighted by the\nadversarial examples, i.e., inputs added with well-designed perturbations which\nare imperceptible to humans but can cause the network to give incorrect\noutputs. In this paper, we design a new CNN architecture that by itself has\ngood robustness. We introduce a simple but powerful technique, Random Mask, to\nmodify existing CNN structures. We show that CNN with Random Mask achieves\nstate-of-the-art performance against black-box adversarial attacks without\napplying any adversarial training. We next investigate the adversarial examples\nwhich 'fool' a CNN with Random Mask. Surprisingly, we find that these\nadversarial examples often 'fool' humans as well. This raises fundamental\nquestions on how to define adversarial examples and robustness properly.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:10:28 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Luo", "Tiange", ""], ["Cai", "Tianle", ""], ["Zhang", "Mengxiao", ""], ["Chen", "Siyu", ""], ["Wang", "Liwei", ""]]}, {"id": "2007.14267", "submitter": "Yat Hong Lam", "authors": "Yat-Hong Lam, Alireza Zare, Francesco Cricri, Jani Lainema, Miska\n  Hannuksela", "title": "Efficient Adaptation of Neural Network Filter for Video Compression", "comments": "Accepted in ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient finetuning methodology for neural-network filters\nwhich are applied as a postprocessing artifact-removal step in video coding\npipelines. The fine-tuning is performed at encoder side to adapt the neural\nnetwork to the specific content that is being encoded. In order to maximize the\nPSNR gain and minimize the bitrate overhead, we propose to finetune only the\nconvolutional layers' biases. The proposed method achieves convergence much\nfaster than conventional finetuning approaches, making it suitable for\npractical applications. The weight-update can be included into the video\nbitstream generated by the existing video codecs. We show that our method\nachieves up to 9.7% average BD-rate gain when compared to the state-of-art\nVersatile Video Coding (VVC) standard codec on 7 test sequences.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:24:28 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 09:07:25 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lam", "Yat-Hong", ""], ["Zare", "Alireza", ""], ["Cricri", "Francesco", ""], ["Lainema", "Jani", ""], ["Hannuksela", "Miska", ""]]}, {"id": "2007.14283", "submitter": "Mengyang Zhao", "authors": "Mengyang Zhao, Aadarsh Jha, Quan Liu, Bryan A. Millis, Anita\n  Mahadevan-Jansen, Le Lu, Bennett A. Landman, Matthew J.Tyskac and Yuankai Huo", "title": "Faster Mean-shift: GPU-accelerated clustering for cosine embedding-based\n  cell segmentation and tracking", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2021.102048", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, single-stage embedding based deep learning algorithms gain\nincreasing attention in cell segmentation and tracking. Compared with the\ntraditional \"segment-then-associate\" two-stage approach, a single-stage\nalgorithm not only simultaneously achieves consistent instance cell\nsegmentation and tracking but also gains superior performance when\ndistinguishing ambiguous pixels on boundaries and overlaps. However, the\ndeployment of an embedding based algorithm is restricted by slow inference\nspeed (e.g., around 1-2 mins per frame). In this study, we propose a novel\nFaster Mean-shift algorithm, which tackles the computational bottleneck of\nembedding based cell segmentation and tracking. Different from previous\nGPU-accelerated fast mean-shift algorithms, a new online seed optimization\npolicy (OSOP) is introduced to adaptively determine the minimal number of\nseeds, accelerate computation, and save GPU memory. With both embedding\nsimulation and empirical validation via the four cohorts from the ISBI cell\ntracking challenge, the proposed Faster Mean-shift algorithm achieved 7-10\ntimes speedup compared to the state-of-the-art embedding based cell instance\nsegmentation and tracking algorithm. Our Faster Mean-shift algorithm also\nachieved the highest computational speed compared to other GPU benchmarks with\noptimized memory consumption. The Faster Mean-shift is a plug-and-play model,\nwhich can be employed on other pixel embedding based clustering inference for\nmedical image analysis. (Plug-and-play model is publicly available:\nhttps://github.com/masqm/Faster-Mean-Shift)\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:52:51 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 02:37:04 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhao", "Mengyang", ""], ["Jha", "Aadarsh", ""], ["Liu", "Quan", ""], ["Millis", "Bryan A.", ""], ["Mahadevan-Jansen", "Anita", ""], ["Lu", "Le", ""], ["Landman", "Bennett A.", ""], ["Tyskac", "Matthew J.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2007.14284", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Mrigank Raman, Sumanth Varambally, Prathosh AP", "title": "Discrepancy Minimization in Domain Generalization with Generative\n  Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) deals with the problem of domain shift where a\nmachine learning model trained on multiple-source domains fail to generalize\nwell on a target domain with different statistics. Multiple approaches have\nbeen proposed to solve the problem of domain generalization by learning domain\ninvariant representations across the source domains that fail to guarantee\ngeneralization on the shifted target domain. We propose a Generative Nearest\nNeighbor based Discrepancy Minimization (GNNDM) method which provides a\ntheoretical guarantee that is upper bounded by the error in the labeling\nprocess of the target. We employ a Domain Discrepancy Minimization Network\n(DDMN) that learns domain agnostic features to produce a single source domain\nwhile preserving the class labels of the data points. Features extracted from\nthis source domain are learned using a generative model whose latent space is\nused as a sampler to retrieve the nearest neighbors for the target data points.\nThe proposed method does not require access to the domain labels (a more\nrealistic scenario) as opposed to the existing approaches. Empirically, we show\nthe efficacy of our method on two datasets: PACS and VLCS. Through extensive\nexperimentation, we demonstrate the effectiveness of the proposed method that\noutperforms several state-of-the-art DG methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:54:25 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Pandey", "Prashant", ""], ["Raman", "Mrigank", ""], ["Varambally", "Sumanth", ""], ["AP", "Prathosh", ""]]}, {"id": "2007.14292", "submitter": "Miki Morimatsu", "authors": "Miki Morimatsu, Yusuke Monno, Masayuki Tanaka, Masatoshi Okutomi", "title": "Monochrome and Color Polarization Demosaicking Using Edge-Aware Residual\n  Interpolation", "comments": "Accepted in ICIP2020. Dataset and code are available at\n  http://www.ok.sc.e.titech.ac.jp/res/PolarDem/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A division-of-focal-plane or microgrid image polarimeter enables us to\nacquire a set of polarization images in one shot. Since the polarimeter\nconsists of an image sensor equipped with a monochrome or color polarization\nfilter array (MPFA or CPFA), the demosaicking process to interpolate missing\npixel values plays a crucial role in obtaining high-quality polarization\nimages. In this paper, we propose a novel MPFA demosaicking method based on\nedge-aware residual interpolation (EARI) and also extend it to CPFA\ndemosaicking. The key of EARI is a new edge detector for generating an\neffective guide image used to interpolate the missing pixel values. We also\npresent a newly constructed full color-polarization image dataset captured\nusing a 3-CCD camera and a rotating polarizer. Using the dataset, we\nexperimentally demonstrate that our EARI-based method outperforms existing\nmethods in MPFA and CPFA demosaicking.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:04:36 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Morimatsu", "Miki", ""], ["Monno", "Yusuke", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2007.14314", "submitter": "Matt Poyser", "authors": "Matt Poyser, Amir Atapour-Abarghouei, Toby P. Breckon", "title": "On the Impact of Lossy Image and Video Compression on the Performance of\n  Deep Convolutional Neural Network Architectures", "comments": "8 pages, 21 figures, to be published in ICPR 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in generalized image understanding have seen a surge in the\nuse of deep convolutional neural networks (CNN) across a broad range of\nimage-based detection, classification and prediction tasks. Whilst the reported\nperformance of these approaches is impressive, this study investigates the\nhitherto unapproached question of the impact of commonplace image and video\ncompression techniques on the performance of such deep learning architectures.\nFocusing on the JPEG and H.264 (MPEG-4 AVC) as a representative proxy for\ncontemporary lossy image/video compression techniques that are in common use\nwithin network-connected image/video devices and infrastructure, we examine the\nimpact on performance across five discrete tasks: human pose estimation,\nsemantic segmentation, object detection, action recognition, and monocular\ndepth estimation. As such, within this study we include a variety of network\narchitectures and domains spanning end-to-end convolution, encoder-decoder,\nregion-based CNN (R-CNN), dual-stream, and generative adversarial networks\n(GAN). Our results show a non-linear and non-uniform relationship between\nnetwork performance and the level of lossy compression applied. Notably,\nperformance decreases significantly below a JPEG quality (quantization) level\nof 15% and a H.264 Constant Rate Factor (CRF) of 40. However, retraining said\narchitectures on pre-compressed imagery conversely recovers network performance\nby up to 78.4% in some cases. Furthermore, there is a correlation between\narchitectures employing an encoder-decoder pipeline and those that demonstrate\nresilience to lossy image compression. The characteristics of the relationship\nbetween input compression to output task performance can be used to inform\ndesign decisions within future image/video devices and infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:37:37 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Poyser", "Matt", ""], ["Atapour-Abarghouei", "Amir", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2007.14350", "submitter": "Ran Chen", "authors": "Ran Chen, Yong Liu, Mengdan Zhang, Shu Liu, Bei Yu, and Yu-Wing Tai", "title": "Dive Deeper Into Box for Object Detection", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anchor free methods have defined the new frontier in state-of-the-art object\ndetection researches where accurate bounding box estimation is the key to the\nsuccess of these methods. However, even the bounding box has the highest\nconfidence score, it is still far from perfect at localization. To this end, we\npropose a box reorganization method(DDBNet), which can dive deeper into the box\nfor more accurate localization. At the first step, drifted boxes are filtered\nout because the contents in these boxes are inconsistent with target semantics.\nNext, the selected boxes are broken into boundaries, and the well-aligned\nboundaries are searched and grouped into a sort of optimal boxes toward\ntightening instances more precisely. Experimental results show that our method\nis effective which leads to state-of-the-art performance for object detection.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:49:05 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Ran", ""], ["Liu", "Yong", ""], ["Zhang", "Mengdan", ""], ["Liu", "Shu", ""], ["Yu", "Bei", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2007.14352", "submitter": "Zhou Huang", "authors": "Zhou Huang, Huai-Xin Chen, Tao Zhou, Yun-Zhi Yang and Bi-Yuan Liu", "title": "Multi-level Cross-modal Interaction Network for RGB-D Salient Object\n  Detection", "comments": null, "journal-ref": "Neurocomputing 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth cues with affluent spatial information have been proven beneficial in\nboosting salient object detection (SOD), while the depth quality directly\naffects the subsequent SOD performance. However, it is inevitable to obtain\nsome low-quality depth cues due to limitations of its acquisition devices,\nwhich can inhibit the SOD performance. Besides, existing methods tend to\ncombine RGB images and depth cues in a direct fusion or a simple fusion module,\nwhich makes they can not effectively exploit the complex correlations between\nthe two sources. Moreover, few methods design an appropriate module to fully\nfuse multi-level features, resulting in cross-level feature interaction\ninsufficient. To address these issues, we propose a novel Multi-level\nCross-modal Interaction Network (MCINet) for RGB-D based SOD. Our MCI-Net\nincludes two key components: 1) a cross-modal feature learning network, which\nis used to learn the high-level features for the RGB images and depth cues,\neffectively enabling the correlations between the two sources to be exploited;\nand 2) a multi-level interactive integration network, which integrates\nmulti-level cross-modal features to boost the SOD performance. Extensive\nexperiments on six benchmark datasets demonstrate the superiority of our\nMCI-Net over 14 state-of-the-art methods, and validate the effectiveness of\ndifferent components in our MCI-Net. More important, our MCI-Net significantly\nimproves the SOD performance as well as has a higher FPS.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 02:21:02 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 08:38:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Huang", "Zhou", ""], ["Chen", "Huai-Xin", ""], ["Zhou", "Tao", ""], ["Yang", "Yun-Zhi", ""], ["Liu", "Bi-Yuan", ""]]}, {"id": "2007.14354", "submitter": "Roey Ron", "authors": "Roey Ron, Gil Elbaz", "title": "Detection and Segmentation of Custom Objects using High Distraction\n  Photorealistic Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We show a straightforward and useful methodology for performing instance\nsegmentation using synthetic data. We apply this methodology on a basic case\nand derived insights through quantitative analysis. We created a new public\ndataset: The Expo Markers Dataset intended for detection and segmentation\ntasks. This dataset contains 5,000 synthetic photorealistic images with their\ncorresponding pixel-perfect segmentation ground truth. The goal is to achieve\nhigh performance on manually-gathered and annotated real-world data of custom\nobjects. We do that by creating 3D models of the target objects and other\npossible distraction objects and place them within a simulated environment.\nExpo Markers were chosen for this task, fitting our requirements of a custom\nobject due to the exact texture, size and 3D shape. An additional advantage is\nthe availability of this object in offices around the world for easy testing\nand validation of our results. We generate the data using a domain\nrandomization technique that also simulates other photorealistic objects in the\nscene, known as distraction objects. These objects provide visual complexity,\nocclusions, and lighting challenges to help our model gain robustness in\ntraining. We are also releasing our manually-gathered datasets used for\ncomparison and evaluation of our synthetic dataset. This white-paper provides\nstrong evidence that photorealistic simulated data can be used in practical\nreal world applications as a more scalable and flexible solution than\nmanually-captured data. Code is available at the following address:\nhttps://github.com/DataGenResearchTeam/expo_markers\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:33:42 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 07:21:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ron", "Roey", ""], ["Elbaz", "Gil", ""]]}, {"id": "2007.14361", "submitter": "Kenneth Lai", "authors": "Kenneth Lai, Helder C. R. Oliveira, Ming Hou, Svetlana N.\n  Yanushkevich, and Vlad Shmerko", "title": "Assessing Risks of Biases in Cognitive Decision Support Systems", "comments": "submitted to 28th European Signal Processing Conference (EUSIPCO\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing, assessing, countering, and mitigating the biases of different\nnature from heterogeneous sources is a critical problem in designing a\ncognitive Decision Support System (DSS). An example of such a system is a\ncognitive biometric-enabled security checkpoint. Biased algorithms affect the\ndecision-making process in an unpredictable way, e.g. face recognition for\ndifferent demographic groups may severely impact the risk assessment at a\ncheckpoint. This paper addresses a challenging research question on how to\nmanage an ensemble of biases? We provide performance projections of the DSS\noperational landscape in terms of biases. A probabilistic reasoning technique\nis used for assessment of the risk of such biases. We also provide a\nmotivational experiment using face biometric component of the checkpoint system\nwhich highlights the discovery of an ensemble of biases and the techniques to\nassess their risks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:53:45 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Lai", "Kenneth", ""], ["Oliveira", "Helder C. R.", ""], ["Hou", "Ming", ""], ["Yanushkevich", "Svetlana N.", ""], ["Shmerko", "Vlad", ""]]}, {"id": "2007.14366", "submitter": "Bin Yang", "authors": "Bin Yang, Runsheng Guo, Ming Liang, Sergio Casas, Raquel Urtasun", "title": "RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We tackle the problem of exploiting Radar for perception in the context of\nself-driving as Radar provides complementary information to other sensors such\nas LiDAR or cameras in the form of Doppler velocity. The main challenges of\nusing Radar are the noise and measurement ambiguities which have been a\nstruggle for existing simple input or output fusion methods. To better address\nthis, we propose a new solution that exploits both LiDAR and Radar sensors for\nperception. Our approach, dubbed RadarNet, features a voxel-based early fusion\nand an attention-based late fusion, which learn from data to exploit both\ngeometric and dynamic information of Radar data. RadarNet achieves\nstate-of-the-art results on two large-scale real-world datasets in the tasks of\nobject detection and velocity estimation. We further show that exploiting Radar\nimproves the perception capabilities of detecting faraway objects and\nunderstanding the motion of dynamic objects.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:15:02 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Yang", "Bin", ""], ["Guo", "Runsheng", ""], ["Liang", "Ming", ""], ["Casas", "Sergio", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2007.14390", "submitter": "Pedro Porto Buarque de Gusmao", "authors": "Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan\n  Parcollet, Pedro P. B. de Gusm\\~ao, Nicholas D. Lane", "title": "Flower: A Friendly Federated Learning Research Framework", "comments": "Open-Source, mobile-friendly Federated Learning framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) has emerged as a promising technique for edge devices\nto collaboratively learn a shared prediction model, while keeping their\ntraining data on the device, thereby decoupling the ability to do machine\nlearning from the need to store the data in the cloud. However, FL is difficult\nto implement and deploy in practice, considering the heterogeneity in mobile\ndevices, e.g., different programming languages, frameworks, and hardware\naccelerators. Although there are a few frameworks available to simulate FL\nalgorithms (e.g., TensorFlow Federated), they do not support implementing FL\nworkloads on mobile devices. Furthermore, these frameworks are designed to\nsimulate FL in a server environment and hence do not allow experimentation in\ndistributed mobile settings for a large number of clients. In this paper, we\npresent Flower (https://flower.dev/), a FL framework which is both agnostic\ntowards heterogeneous client environments and also scales to a large number of\nclients, including mobile and embedded devices. Flower's abstractions let\ndevelopers port existing mobile workloads with little overhead, regardless of\nthe programming language or ML framework used, while also allowing researchers\nflexibility to experiment with novel approaches to advance the\nstate-of-the-art. We describe the design goals and implementation\nconsiderations of Flower and show our experiences in evaluating the performance\nof FL across clients with heterogeneous computational and communication\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:59:07 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 11:45:41 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 11:06:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Beutel", "Daniel J.", ""], ["Topal", "Taner", ""], ["Mathur", "Akhil", ""], ["Qiu", "Xinchi", ""], ["Parcollet", "Titouan", ""], ["de Gusm\u00e3o", "Pedro P. B.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2007.14419", "submitter": "Shi  Chen", "authors": "Shi Chen, Ming Jiang, Jinhui Yang, Qi Zhao", "title": "AiR: Attention with Reasoning Capability", "comments": "ECCV2020 (Oral), supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While attention has been an increasingly popular component in deep neural\nnetworks to both interpret and boost performance of models, little work has\nexamined how attention progresses to accomplish a task and whether it is\nreasonable. In this work, we propose an Attention with Reasoning capability\n(AiR) framework that uses attention to understand and improve the process\nleading to task outcomes. We first define an evaluation metric based on a\nsequence of atomic reasoning operations, enabling quantitative measurement of\nattention that considers the reasoning process. We then collect human\neye-tracking and answer correctness data, and analyze various machine and human\nattentions on their reasoning capability and how they impact task performance.\nFurthermore, we propose a supervision method to jointly and progressively\noptimize attention, reasoning, and task performance so that models learn to\nlook at regions of interests by following a reasoning process. We demonstrate\nthe effectiveness of the proposed framework in analyzing and modeling attention\nwith better reasoning capability and task performance. The code and data are\navailable at https://github.com/szzexpoi/AiR\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 18:09:45 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Shi", ""], ["Jiang", "Ming", ""], ["Yang", "Jinhui", ""], ["Zhao", "Qi", ""]]}, {"id": "2007.14432", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez Fern\\'andez, Franklin Barrientos Porras, Robert H.\n  Gilman, Macarena Vittet Mondonedo, Patricia Sheen, Mirko Zimic", "title": "A Convolutional Neural Network for gaze preference detection: A\n  potential tool for diagnostics of autism spectrum disorder in children", "comments": "Pre-printed version for submission in a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of autism spectrum disorder (ASD) is known to improve the\nquality of life of affected individuals. However, diagnosis is often delayed\neven in wealthier countries including the US, largely due to the fact that gold\nstandard diagnostic tools such as the Autism Diagnostic Observation Schedule\n(ADOS) and the Autism Diagnostic Interview-Revised (ADI-R) are time consuming\nand require expertise to administer. This trend is even more pronounced lower\nresources settings due to a lack of trained experts. As a result, alternative,\nless technical methods that leverage the unique ways in which children with ASD\nreact to visual stimulation in a controlled environment have been developed to\nhelp facilitate early diagnosis. Previous studies have shown that, when exposed\nto a video that presents both social and abstract scenes side by side, a child\nwith ASD will focus their attention towards the abstract images on the screen\nto a greater extent than a child without ASD. Such differential responses make\nit possible to implement an algorithm for the rapid diagnosis of ASD based on\neye tracking against different visual stimuli. Here we propose a convolutional\nneural network (CNN) algorithm for gaze prediction using images extracted from\na one-minute stimulus video. Our model achieved a high accuracy rate and\nrobustness for prediction of gaze direction with independent persons and\nemploying a different camera than the one used during testing. In addition to\nthis, the proposed algorithm achieves a fast response time, providing a near\nreal-time evaluation of ASD. Thereby, by applying the proposed method, we could\nsignificantly reduce the diagnosis time and facilitate the diagnosis of ASD in\nlow resource regions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 18:47:21 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Fern\u00e1ndez", "Dennis N\u00fa\u00f1ez", ""], ["Porras", "Franklin Barrientos", ""], ["Gilman", "Robert H.", ""], ["Mondonedo", "Macarena Vittet", ""], ["Sheen", "Patricia", ""], ["Zimic", "Mirko", ""]]}, {"id": "2007.14433", "submitter": "Rohit Gupta", "authors": "Xiaoyu Zhang, Ajmal Mian, Rohit Gupta, Nazanin Rahnavard and Mubarak\n  Shah", "title": "Cassandra: Detecting Trojaned Networks from Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are being widely deployed for many critical tasks due to\ntheir high classification accuracy. In many cases, pre-trained models are\nsourced from vendors who may have disrupted the training pipeline to insert\nTrojan behaviors into the models. These malicious behaviors can be triggered at\nthe adversary's will and hence, cause a serious threat to the widespread\ndeployment of deep models. We propose a method to verify if a pre-trained model\nis Trojaned or benign. Our method captures fingerprints of neural networks in\nthe form of adversarial perturbations learned from the network gradients.\nInserting backdoors into a network alters its decision boundaries which are\neffectively encoded in their adversarial perturbations. We train a two stream\nnetwork for Trojan detection from its global ($L_\\infty$ and $L_2$ bounded)\nperturbations and the localized region of high energy within each perturbation.\nThe former encodes decision boundaries of the network and latter encodes the\nunknown trigger shape. We also propose an anomaly detection method to identify\nthe target class in a Trojaned network. Our methods are invariant to the\ntrigger type, trigger size, training data and network architecture. We evaluate\nour methods on MNIST, NIST-Round0 and NIST-Round1 datasets, with up to 1,000\npre-trained models making this the largest study to date on Trojaned network\ndetection, and achieve over 92\\% detection accuracy to set the new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:00:40 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zhang", "Xiaoyu", ""], ["Mian", "Ajmal", ""], ["Gupta", "Rohit", ""], ["Rahnavard", "Nazanin", ""], ["Shah", "Mubarak", ""]]}, {"id": "2007.14449", "submitter": "Muhammad Naseer Subhani", "authors": "M.Naseer Subhani and Mohsen Ali", "title": "Learning from Scale-Invariant Examples for Domain Adaptation in Semantic\n  Segmentation", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning approaches for unsupervised domain adaptation (UDA)\nof semantic segmentation models suffer from challenges of predicting and\nselecting reasonable good quality pseudo labels. In this paper, we propose a\nnovel approach of exploiting scale-invariance property of the semantic\nsegmentation model for self-supervised domain adaptation. Our algorithm is\nbased on a reasonable assumption that, in general, regardless of the size of\nthe object and stuff (given context) the semantic labeling should be unchanged.\nWe show that this constraint is violated over the images of the target domain,\nand hence could be used to transfer labels in-between differently scaled\npatches. Specifically, we show that semantic segmentation model produces output\nwith high entropy when presented with scaled-up patches of target domain, in\ncomparison to when presented original size images. These scale-invariant\nexamples are extracted from the most confident images of the target domain.\nDynamic class specific entropy thresholding mechanism is presented to filter\nout unreliable pseudo-labels. Furthermore, we also incorporate the focal loss\nto tackle the problem of class imbalance in self-supervised learning. Extensive\nexperiments have been performed, and results indicate that exploiting the\nscale-invariant labeling, we outperform existing self-supervised based\nstate-of-the-art domain adaptation methods. Specifically, we achieve 1.3% and\n3.8% of lead for GTA5 to Cityscapes and SYNTHIA to Cityscapes with VGG16-FCN8\nbaseline network.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:40:45 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Subhani", "M. Naseer", ""], ["Ali", "Mohsen", ""]]}, {"id": "2007.14450", "submitter": "Jinwei Zhang", "authors": "Jinwei Zhang, Hang Zhang, Alan Wang, Qihao Zhang, Mert Sabuncu, Pascal\n  Spincemaille, Thanh D. Nguyen, Yi Wang", "title": "Extending LOUPE for K-space Under-sampling Pattern Optimization in\n  Multi-coil MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The previously established LOUPE (Learning-based Optimization of the\nUnder-sampling Pattern) framework for optimizing the k-space sampling pattern\nin MRI was extended in three folds: firstly, fully sampled multi-coil k-space\ndata from the scanner, rather than simulated k-space data from magnitude MR\nimages in LOUPE, was retrospectively under-sampled to optimize the\nunder-sampling pattern of in-vivo k-space data; secondly, binary stochastic\nk-space sampling, rather than approximate stochastic k-space sampling of LOUPE\nduring training, was applied together with a straight-through (ST) estimator to\nestimate the gradient of the threshold operation in a neural network; thirdly,\nmodified unrolled optimization network, rather than modified U-Net in LOUPE,\nwas used as the reconstruction network in order to reconstruct multi-coil data\nproperly and reduce the dependency on training data. Experimental results show\nthat when dealing with the in-vivo k-space data, unrolled optimization network\nwith binary under-sampling block and ST estimator had better reconstruction\nperformance compared to the ones with either U-Net reconstruction network or\napproximate sampling pattern optimization network, and once trained, the\nlearned optimal sampling pattern worked better than the hand-crafted variable\ndensity sampling pattern when deployed with other conventional reconstruction\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:41:47 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zhang", "Jinwei", ""], ["Zhang", "Hang", ""], ["Wang", "Alan", ""], ["Zhang", "Qihao", ""], ["Sabuncu", "Mert", ""], ["Spincemaille", "Pascal", ""], ["Nguyen", "Thanh D.", ""], ["Wang", "Yi", ""]]}, {"id": "2007.14456", "submitter": "Alex Gaudio", "authors": "Alex Gaudio and Asim Smailagic and Aur\\'elio Campilho", "title": "Enhancement of Retinal Fundus Images via Pixel Color Amplification", "comments": "Accepted to International Conference on Image Analysis and\n  Recognition, ICIAR 2020 ; // Published at\n  https://doi.org/10.1007/978-3-030-50516-5_26 ;// CODE, SLIDES, and an\n  expanded/modified 20 page version https://github.com/adgaudio/ietk-ret", "journal-ref": null, "doi": "10.1007/978-3-030-50516-5_26", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pixel color amplification theory and family of enhancement\nmethods to facilitate segmentation tasks on retinal images. Our novel\nre-interpretation of the image distortion model underlying dehazing theory\nshows how three existing priors commonly used by the dehazing community and a\nnovel fourth prior are related. We utilize the theory to develop a family of\nenhancement methods for retinal images, including novel methods for whole image\nbrightening and darkening. We show a novel derivation of the Unsharp Masking\nalgorithm. We evaluate the enhancement methods as a pre-processing step to a\nchallenging multi-task segmentation problem and show large increases in\nperformance on all tasks, with Dice score increases over a no-enhancement\nbaseline by as much as 0.491. We provide evidence that our enhancement\npreprocessing is useful for unbalanced and difficult data. We show that the\nenhancements can perform class balancing by composing them together.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:56:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Gaudio", "Alex", ""], ["Smailagic", "Asim", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "2007.14465", "submitter": "Slimane Larabi", "authors": "Slimane Larabi", "title": "Towards 3D Visualization of Video from Frames", "comments": "paper not published", "journal-ref": null, "doi": null, "report-no": "RIIMA_Report_02_2020", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain theoretically how to reconstruct the 3D scene from successive\nframes in order to see the video in 3D. To do this, features, associated to\nmoving rigid objects in 3D, are extracted in frames and matched. The vanishing\npoint computed in frame corresponding to the direction of moving object is used\nfor 3D positioning of the 3D structure of the moving object. First experiments\nare conducted and the obtained results are shown and publicly available. They\ndemonstrate the feasibility of our method. We conclude this paper by future\nworks in order to improve this method tacking into account non-rigid objects\nand the case of moving camera.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:37:42 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Larabi", "Slimane", ""]]}, {"id": "2007.14471", "submitter": "R. Omar Chavez-Garcia", "authors": "R. Omar Chavez-Garcia, Emian Furger, Samuele Kronauer, Christian\n  Brianza, Marco Scarf\\`o, Luca Diviani and Alessandro Giusti", "title": "Learning to predict metal deformations in hot-rolling processes", "comments": "Accepted for publication in the IEEE Robotics & Automation Letters\n  (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hot-rolling is a metal forming process that produces a workpiece with a\ndesired target cross-section from an input workpiece through a sequence of\nplastic deformations; each deformation is generated by a stand composed of\nopposing rolls with a specific geometry. In current practice, the rolling\nsequence (i.e., the sequence of stands and the geometry of their rolls) needed\nto achieve a given final cross-section is designed by experts based on previous\nexperience, and iteratively refined in a costly trial-and-error process. Finite\nElement Method simulations are increasingly adopted to make this process more\nefficient and to test potential rolling sequences, achieving good accuracy at\nthe cost of long simulation times, limiting the practical use of the approach.\nWe propose a supervised learning approach to predict the deformation of a given\nworkpiece by a set of rolls with a given geometry; the model is trained on a\nlarge dataset of procedurally-generated FEM simulations, which we publish as\nsupplementary material. The resulting predictor is four orders of magnitude\nfaster than simulations, and yields an average Jaccard Similarity Index of\n0.972 (against ground truth from simulations) and 0.925 (against real-world\nmeasured deformations); we additionally report preliminary results on using the\npredictor for automatic planning of rolling sequences.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:33:44 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chavez-Garcia", "R. Omar", ""], ["Furger", "Emian", ""], ["Kronauer", "Samuele", ""], ["Brianza", "Christian", ""], ["Scarf\u00f2", "Marco", ""], ["Diviani", "Luca", ""], ["Giusti", "Alessandro", ""]]}, {"id": "2007.14472", "submitter": "Li Chen", "authors": "Li Chen, Thomas Hatsukami, Jenq-Neng Hwang, Chun Yuan", "title": "Automated Intracranial Artery Labeling using a Graph Neural Network and\n  Hierarchical Refinement", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically labeling intracranial arteries (ICA) with their anatomical\nnames is beneficial for feature extraction and detailed analysis of\nintracranial vascular structures. There are significant variations in the ICA\ndue to natural and pathological causes, making it challenging for automated\nlabeling. However, the existing public dataset for evaluation of anatomical\nlabeling is limited. We construct a comprehensive dataset with 729 Magnetic\nResonance Angiography scans and propose a Graph Neural Network (GNN) method to\nlabel arteries by classifying types of nodes and edges in an attributed\nrelational graph. In addition, a hierarchical refinement framework is developed\nfor further improving the GNN outputs to incorporate structural and relational\nknowledge about the ICA. Our method achieved a node labeling accuracy of 97.5%,\nand 63.8% of scans were correctly labeled for all Circle of Willis nodes, on a\ntesting set of 105 scans with both healthy and diseased subjects. This is a\nsignificant improvement over available state-of-the-art methods. Automatic\nartery labeling is promising to minimize manual effort in characterizing the\ncomplicated ICA networks and provides valuable information for the\nidentification of geometric risk factors of vascular disease. Our code and\ndataset are available at https://github.com/clatfd/GNN-ARTLABEL.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 06:22:35 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Li", ""], ["Hatsukami", "Thomas", ""], ["Hwang", "Jenq-Neng", ""], ["Yuan", "Chun", ""]]}, {"id": "2007.14485", "submitter": "Shuting Liao", "authors": "Shuting Liao, Li-Yu Liu, Ting-An Chen, Kuang-Yu Chen and Fushing Hsieh", "title": "Color-complexity enabled exhaustive color-dots identification and\n  spatial patterns testing in images", "comments": "21 pages, 21 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0251258", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted color-dots with varying shapes and sizes in images are first\nexhaustively identified, and then their multiscale 2D geometric patterns are\nextracted for testing spatial uniformness in a progressive fashion. Based on\ncolor theory in physics, we develop a new color-identification algorithm\nrelying on highly associative relations among the three color-coordinates: RGB\nor HSV. Such high associations critically imply low color-complexity of a color\nimage, and renders potentials of exhaustive identification of targeted\ncolor-dots of all shapes and sizes. Via heterogeneous shaded regions and\nlighting conditions, our algorithm is shown being robust, practical and\nefficient comparing with the popular Contour and OpenCV approaches. Upon all\nidentified color-pixels, we form color-dots as individually connected networks\nwith shapes and sizes. We construct minimum spanning trees (MST) as spatial\ngeometries of dot-collectives of various size-scales. Given a size-scale, the\ndistribution of distances between immediate neighbors in the observed MST is\nextracted, so do many simulated MSTs under the spatial uniformness assumption.\nWe devise a new algorithm for testing 2D spatial uniformness based on a\nHierarchical clustering tree upon all involving MSTs. Our developments are\nillustrated on images obtained by mimicking chemical spraying via drone in\nPrecision Agriculture.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:06:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liao", "Shuting", ""], ["Liu", "Li-Yu", ""], ["Chen", "Ting-An", ""], ["Chen", "Kuang-Yu", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2007.14487", "submitter": "Mingrui Zhang", "authors": "Mingrui Zhang and Matthew D. Piggott", "title": "Unsupervised Learning of Particle Image Velocimetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Image Velocimetry (PIV) is a classical flow estimation problem which\nis widely considered and utilised, especially as a diagnostic tool in\nexperimental fluid dynamics and the remote sensing of environmental flows.\nRecently, the development of deep learning based methods has inspired new\napproaches to tackle the PIV problem. These supervised learning based methods\nare driven by large volumes of data with ground truth training information.\nHowever, it is difficult to collect reliable ground truth data in large-scale,\nreal-world scenarios. Although synthetic datasets can be used as alternatives,\nthe gap between the training set-ups and real-world scenarios limits\napplicability. We present here what we believe to be the first work which takes\nan unsupervised learning based approach to tackle PIV problems. The proposed\napproach is inspired by classic optical flow methods. Instead of using ground\ntruth data, we make use of photometric loss between two consecutive image\nframes, consistency loss in bidirectional flow estimates and spatial smoothness\nloss to construct the total unsupervised loss function. The approach shows\nsignificant potential and advantages for fluid flow estimation. Results\npresented here demonstrate that our method outputs competitive results compared\nwith classical PIV methods as well as supervised learning based methods for a\nbroad PIV dataset, and even outperforms these existing approaches in some\ndifficult flow cases. Codes and trained models are available at\nhttps://github.com/erizmr/UnLiteFlowNet-PIV.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:08:37 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zhang", "Mingrui", ""], ["Piggott", "Matthew D.", ""]]}, {"id": "2007.14509", "submitter": "Joseph Robinson", "authors": "Joseph P. Robinson, Zaid Khan, Yu Yin, Ming Shao, Yun Fu", "title": "Families In Wild Multimedia (FIW MM): A Multi-Modal Database for\n  Recognizing Kinship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship is a soft biometric detectable in media with an abundance of\npractical applications. Despite the difficulty of detecting kinship, annual\ndata challenges using still-images have consistently improved performances and\nattracted new researchers. Now, systems reach performance levels unforeseeable\na decade ago, closing in on performances acceptable to deploy in practice.\nSimilar to other biometric tasks, we expect systems can benefit from additional\nmodalities. We hypothesize that adding modalities to FIW, which contains only\nstill-images, will improve performance. Thus, to narrow the gap between\nresearch and reality and enhance the power of kinship recognition systems, we\nextend FIW with multimedia (MM) data (i.e., video, audio, and text captions).\nSpecifically, we introduce the first publicly available multi-task MM kinship\ndataset. To build FIW MM, we developed machinery to automatically collect,\nannotate, and prepare the data, requiring minimal human input and no financial\ncost. The proposed MM corpus allows the problem statements to be more realistic\ntemplate-based protocols. We show significant improvements in all benchmarks\nwith the added modalities. The results highlight edge cases to inspire future\nresearch with different areas of improvement. FIW MM provides the data required\nto increase the potential of automated systems to detect kinship in MM. It also\nallows experts from diverse fields to collaborate in novel ways.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:36:57 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 20:58:56 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 04:10:53 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 06:31:05 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Robinson", "Joseph P.", ""], ["Khan", "Zaid", ""], ["Yin", "Yu", ""], ["Shao", "Ming", ""], ["Fu", "Yun", ""]]}, {"id": "2007.14510", "submitter": "Yuanhao Gong", "authors": "Yuanhao Gong", "title": "Decompose X-ray Images for Bone and Soft Tissue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bones are always wrapped by soft tissues. As a result, bones in their X-ray\nimages are obscured and become unclear. In this paper, we tackle this problem\nand propose a novel task to virtually decompose the soft tissue and bone by\nimage processing algorithms. This task is fundamentally different from\nsegmentation because the decomposed images share the same imaging domain. Our\ndecomposition task is also fundamentally different from the conventional image\nenhancement. We propose a new mathematical model for such decomposition. Our\nmodel is ill-posed and thus it requires some priors. With proper assumptions,\nour model can be solved by solving a standard Laplace equation. The resulting\nbone image is theoretically guaranteed to have better contrast than the\noriginal input image. Therefore, the details of bones get enhanced and become\nclearer. Several numerical experiments confirm the effective and efficiency of\nour method. Our approach is important for clinical diagnosis, surgery planning,\nrecognition, deep learning, etc.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:38:54 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Gong", "Yuanhao", ""]]}, {"id": "2007.14511", "submitter": "Bin Cheng", "authors": "Bin Cheng, Inderjot Singh Saggu, Raunak Shah, Gaurav Bansal, Dinesh\n  Bharadia", "title": "$S^3$Net: Semantic-Aware Self-supervised Depth Estimation with Monocular\n  Videos and Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solving depth estimation with monocular cameras enables the possibility of\nwidespread use of cameras as low-cost depth estimation sensors in applications\nsuch as autonomous driving and robotics. However, learning such a scalable\ndepth estimation model would require a lot of labeled data which is expensive\nto collect. There are two popular existing approaches which do not require\nannotated depth maps: (i) using labeled synthetic and unlabeled real data in an\nadversarial framework to predict more accurate depth, and (ii) unsupervised\nmodels which exploit geometric structure across space and time in monocular\nvideo frames. Ideally, we would like to leverage features provided by both\napproaches as they complement each other; however, existing methods do not\nadequately exploit these additive benefits. We present $S^3$Net, a\nself-supervised framework which combines these complementary features: we use\nsynthetic and real-world images for training while exploiting geometric,\ntemporal, as well as semantic constraints. Our novel consolidated architecture\nprovides a new state-of-the-art in self-supervised depth estimation using\nmonocular videos. We present a unique way to train this self-supervised\nframework, and achieve (i) more than $15\\%$ improvement over previous synthetic\nsupervised approaches that use domain adaptation and (ii) more than $10\\%$\nimprovement over previous self-supervised approaches which exploit geometric\nconstraints from the real data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:40:54 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Cheng", "Bin", ""], ["Saggu", "Inderjot Singh", ""], ["Shah", "Raunak", ""], ["Bansal", "Gaurav", ""], ["Bharadia", "Dinesh", ""]]}, {"id": "2007.14513", "submitter": "Chaoyang He", "authors": "Chaoyang He, Murali Annavaram, Salman Avestimehr", "title": "Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge", "comments": "This paper is accepted to NeurIPS 2020. We propose FedGKT, attempting\n  to address one of the core problems of federated learning: training deep\n  neural networks in resource-constrained edge devices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scaling up the convolutional neural network (CNN) size (e.g., width, depth,\netc.) is known to effectively improve model accuracy. However, the large model\nsize impedes training on resource-constrained edge devices. For instance,\nfederated learning (FL) may place undue burden on the compute capability of\nedge nodes, even though there is a strong practical need for FL due to its\nprivacy and confidentiality properties. To address the resource-constrained\nreality of edge devices, we reformulate FL as a group knowledge transfer\ntraining algorithm, called FedGKT. FedGKT designs a variant of the alternating\nminimization approach to train small CNNs on edge nodes and periodically\ntransfer their knowledge by knowledge distillation to a large server-side CNN.\nFedGKT consolidates several advantages into a single framework: reduced demand\nfor edge computation, lower communication bandwidth for large CNNs, and\nasynchronous training, all while maintaining model accuracy comparable to\nFedAvg. We train CNNs designed based on ResNet-56 and ResNet-110 using three\ndistinct datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D.\nvariants. Our results show that FedGKT can obtain comparable or even slightly\nhigher accuracy than FedAvg. More importantly, FedGKT makes edge training\naffordable. Compared to the edge training using FedAvg, FedGKT demands 9 to 17\ntimes less computational power (FLOPs) on edge devices and requires 54 to 105\ntimes fewer parameters in the edge CNN. Our source code is released at FedML\n(https://fedml.ai).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:48:52 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 00:48:40 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 23:25:17 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2020 07:24:42 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["He", "Chaoyang", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2007.14524", "submitter": "Morteza Haghir Chehreghani", "authors": "Andreas Demetriou, Henrik Alfsv{\\aa}g, Sadegh Rahrovani, Morteza\n  Haghir Chehreghani", "title": "A Deep Learning Framework for Generation and Analysis of Driving\n  Scenario Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified deep learning framework for generation and analysis of\ndriving scenario trajectories, and validate its effectiveness in a principled\nway. In order to model and generate scenarios of trajectories with different\nlength, we develop two approaches. First, we adapt the Recurrent Conditional\nGenerative Adversarial Networks (RC-GAN) by conditioning on the length of the\ntrajectories. This provides us flexibility to generate variable-length driving\ntrajectories, a desirable feature for scenario test case generation in the\nverification of self-driving cars. Second, we develop an architecture based on\nRecurrent Autoencoder with GANs in order to obviate the variable length issue,\nwherein we train a GAN to learn/generate the latent representations of original\ntrajectories. In this approach, we train an integrated feed-forward neural\nnetwork to estimate the length of the trajectories to be able to bring them\nback from the latent space representation. In addition to trajectory\ngeneration, we employ the trained autoencoder as a feature extractor, for the\npurpose of clustering and anomaly detection, in order to obtain further\ninsights on the collected scenario dataset. We experimentally investigate the\nperformance of the proposed framework on real-world scenario trajectories\nobtained from in-field data collection.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 23:33:05 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Demetriou", "Andreas", ""], ["Alfsv\u00e5g", "Henrik", ""], ["Rahrovani", "Sadegh", ""], ["Chehreghani", "Morteza Haghir", ""]]}, {"id": "2007.14549", "submitter": "Han Wang", "authors": "Han Wang, Chen Wang and Lihua Xie", "title": "Online Visual Place Recognition via Saliency Re-identification", "comments": "Accepted by IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2020 (IROS)", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2020", "doi": "10.1109/IROS45743.2020.9341703", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential component of visual simultaneous localization and mapping\n(SLAM), place recognition is crucial for robot navigation and autonomous\ndriving. Existing methods often formulate visual place recognition as feature\nmatching, which is computationally expensive for many robotic applications with\nlimited computing power, e.g., autonomous driving and cleaning robot. Inspired\nby the fact that human beings always recognize a place by remembering salient\nregions or landmarks that are more attractive or interesting than others, we\nformulate visual place recognition as saliency re-identification. In the\nmeanwhile, we propose to perform both saliency detection and re-identification\nin frequency domain, in which all operations become element-wise. The\nexperiments show that our proposed method achieves competitive accuracy and\nmuch higher speed than the state-of-the-art feature-based methods. The proposed\nmethod is open-sourced and available at\nhttps://github.com/wh200720041/SRLCD.git.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 01:53:45 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Wang", "Han", ""], ["Wang", "Chen", ""], ["Xie", "Lihua", ""]]}, {"id": "2007.14552", "submitter": "Tianyu Liu", "authors": "Tianyu Liu", "title": "Compare and Select: Video Summarization with Multi-Agent Reinforcement\n  Learning", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization aims at generating concise video summaries from the\nlengthy videos, to achieve better user watching experience. Due to the\nsubjectivity, purely supervised methods for video summarization may bring the\ninherent errors from the annotations. To solve the subjectivity problem, we\nstudy the general user summarization process. General users usually watch the\nwhole video, compare interesting clips and select some clips to form a final\nsummary. Inspired by the general user behaviours, we formulate the\nsummarization process as multiple sequential decision-making processes, and\npropose Comparison-Selection Network (CoSNet) based on multi-agent\nreinforcement learning. Each agent focuses on a video clip and constantly\nchanges its focus during the iterations, and the final focus clips of all\nagents form the summary. The comparison network provides the agent with the\nvisual feature from clips and the chronological feature from the past round,\nwhile the selection network of the agent makes decisions on the change of its\nfocus clip. The specially designed unsupervised reward and supervised reward\ntogether contribute to the policy advancement, each containing local and global\nparts. Extensive experiments on two benchmark datasets show that CoSNet\noutperforms state-of-the-art unsupervised methods with the unsupervised reward\nand surpasses most supervised methods with the complete reward.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:14:24 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Liu", "Tianyu", ""]]}, {"id": "2007.14556", "submitter": "Shibiao Xu", "authors": "Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng, Jun Xiao, Qimin\n  Peng, Xiaopeng Zhang", "title": "Accurate 2D soft segmentation of medical image via SoftGAN network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 2D lung nodules segmentation from medical Computed Tomography (CT)\nimages is crucial in medical applications. Most current approaches cannot\nachieve precise segmentation results that preserving both rich edge details\ndescription and smooth transition representations between image regions due to\nthe tininess, complexities, and irregularities of lung nodule shapes. To\naddress this issue, we propose a novel Cascaded Generative Adversarial Network\n(CasGAN) to cope with CT images super-resolution and segmentation tasks, in\nwhich the semantic soft segmentation form on precise lesion representation is\nintroduced for the first time according to our knowledge, and lesion edges can\nbe retained accurately after our segmentation that can promote rapid\nacquisition of high-quality large-scale annotation data based on RECIST weak\nsupervision information. Extensive experiments validate that our CasGAN\noutperforms the state-of-the-art methods greatly in segmentation quality, which\nis also robust on the application of medical images beyond lung nodules.\nBesides, we provide a challenging lung nodules soft segmentation dataset of\nmedical CT images for further studies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:38:02 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Changwei", ""], ["Xu", "Rongtao", ""], ["Xu", "Shibiao", ""], ["Meng", "Weiliang", ""], ["Xiao", "Jun", ""], ["Peng", "Qimin", ""], ["Zhang", "Xiaopeng", ""]]}, {"id": "2007.14557", "submitter": "Jinlong Peng", "authors": "Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying\n  Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Yanwei Fu", "title": "Chained-Tracker: Chaining Paired Attentive Regression Results for\n  End-to-End Joint Multiple-Object Detection and Tracking", "comments": "European Conference on Computer Vision 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Multiple-Object Tracking (MOT) methods either follow the\ntracking-by-detection paradigm to conduct object detection, feature extraction\nand data association separately, or have two of the three subtasks integrated\nto form a partially end-to-end solution. Going beyond these sub-optimal\nframeworks, we propose a simple online model named Chained-Tracker (CTracker),\nwhich naturally integrates all the three subtasks into an end-to-end solution\n(the first as far as we know). It chains paired bounding boxes regression\nresults estimated from overlapping nodes, of which each node covers two\nadjacent frames. The paired regression is made attentive by object-attention\n(brought by a detection module) and identity-attention (ensured by an ID\nverification module). The two major novelties: chained structure and paired\nattentive regression, make CTracker simple, fast and effective, setting new\nMOTA records on MOT16 and MOT17 challenge datasets (67.6 and 66.6,\nrespectively), without relying on any extra training data. The source code of\nCTracker can be found at: github.com/pjl1995/CTracker.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:38:49 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Peng", "Jinlong", ""], ["Wang", "Changan", ""], ["Wan", "Fangbin", ""], ["Wu", "Yang", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Fu", "Yanwei", ""]]}, {"id": "2007.14558", "submitter": "Yu Yao", "authors": "Yu Yao, Ella Atkins, Matthew Johnson-Roberson, Ram Vasudevan, Xiaoxiao\n  Du", "title": "BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal\n  Goal Estimation", "comments": "Main paper: 8 pages, 5 figures, 5 tables Supplement: 4 pages, 2\n  figrues, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is an essential task in robotic applications\nsuch as autonomous driving and robot navigation. State-of-the-art trajectory\npredictors use a conditional variational autoencoder (CVAE) with recurrent\nneural networks (RNNs) to encode observed trajectories and decode multi-modal\nfuture trajectories. This process can suffer from accumulated errors over long\nprediction horizons (>=2 seconds). This paper presents BiTraP, a\ngoal-conditioned bi-directional multi-modal trajectory prediction method based\non the CVAE. BiTraP estimates the goal (end-point) of trajectories and\nintroduces a novel bi-directional decoder to improve longer-term trajectory\nprediction accuracy. Extensive experiments show that BiTraP generalizes to both\nfirst-person view (FPV) and bird's-eye view (BEV) scenarios and outperforms\nstate-of-the-art results by ~10-50%. We also show that different choices of\nnon-parametric versus parametric target models in the CVAE directly influence\nthe predicted multi-modal trajectory distributions. These results provide\nguidance on trajectory predictor design for robotic applications such as\ncollision avoidance and navigation systems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:40:17 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 17:30:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yao", "Yu", ""], ["Atkins", "Ella", ""], ["Johnson-Roberson", "Matthew", ""], ["Vasudevan", "Ram", ""], ["Du", "Xiaoxiao", ""]]}, {"id": "2007.14560", "submitter": "Vishal Kaushal", "authors": "Vishal Kaushal, Suraj Kothawade, Rishabh Iyer, Ganesh Ramakrishnan", "title": "Realistic Video Summarization through VISIOCITY: A New Benchmark and\n  Evaluation Framework", "comments": "19 pages, 1 figure, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video summarization is still an unsolved problem due to several\nchallenges. We take steps towards making automatic video summarization more\nrealistic by addressing them. Firstly, the currently available datasets either\nhave very short videos or have few long videos of only a particular type. We\nintroduce a new benchmarking dataset VISIOCITY which comprises of longer videos\nacross six different categories with dense concept annotations capable of\nsupporting different flavors of video summarization and can be used for other\nvision problems. Secondly, for long videos, human reference summaries are\ndifficult to obtain. We present a novel recipe based on pareto optimality to\nautomatically generate multiple reference summaries from indirect ground truth\npresent in VISIOCITY. We show that these summaries are at par with human\nsummaries. Thirdly, we demonstrate that in the presence of multiple ground\ntruth summaries (due to the highly subjective nature of the task), learning\nfrom a single combined ground truth summary using a single loss function is not\na good idea. We propose a simple recipe VISIOCITY-SUM to enhance an existing\nmodel using a combination of losses and demonstrate that it beats the current\nstate of the art techniques when tested on VISIOCITY. We also show that a\nsingle measure to evaluate a summary, as is the current typical practice, falls\nshort. We propose a framework for better quantitative assessment of summary\nquality which is closer to human judgment than a single measure, say F1. We\nreport the performance of a few representative techniques of video\nsummarization on VISIOCITY assessed using various measures and bring out the\nlimitation of the techniques and/or the assessment mechanism in modeling human\njudgment and demonstrate the effectiveness of our evaluation framework in doing\nso.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:44:35 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 09:42:26 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2007.14579", "submitter": "T.J. Tsai", "authors": "Daniel Yang and TJ Tsai", "title": "Camera-Based Piano Sheet Music Identification", "comments": "8 pages, 3 figures, 2 tables. Accepted paper at the International\n  Society for Music Information Retrieval Conference (ISMIR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for large-scale retrieval of piano sheet music\nimages. Our work differs from previous studies on sheet music retrieval in two\nways. First, we investigate the problem at a much larger scale than previous\nstudies, using all solo piano sheet music images in the entire IMSLP dataset as\na searchable database. Second, we use cell phone images of sheet music as our\ninput queries, which lends itself to a practical, user-facing application. We\nshow that a previously proposed fingerprinting method for sheet music retrieval\nis far too slow for a real-time application, and we diagnose its shortcomings.\nWe propose a novel hashing scheme called dynamic n-gram fingerprinting that\nsignificantly reduces runtime while simultaneously boosting retrieval accuracy.\nIn experiments on IMSLP data, our proposed method achieves a mean reciprocal\nrank of 0.85 and an average runtime of 0.98 seconds per query.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 03:55:27 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yang", "Daniel", ""], ["Tsai", "TJ", ""]]}, {"id": "2007.14581", "submitter": "Zhemin Li", "authors": "Zhemin Li, Zhi-Qin John Xu, Tao Luo, Hongxia Wang", "title": "A regularized deep matrix factorized model of matrix completion for\n  image restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been an important approach of using matrix completion to perform image\nrestoration. Most previous works on matrix completion focus on the low-rank\nproperty by imposing explicit constraints on the recovered matrix, such as the\nconstraint of the nuclear norm or limiting the dimension of the matrix\nfactorization component. Recently, theoretical works suggest that deep linear\nneural network has an implicit bias towards low rank on matrix completion.\nHowever, low rank is not adequate to reflect the intrinsic characteristics of a\nnatural image. Thus, algorithms with only the constraint of low rank are\ninsufficient to perform image restoration well. In this work, we propose a\nRegularized Deep Matrix Factorized (RDMF) model for image restoration, which\nutilizes the implicit bias of the low rank of deep neural networks and the\nexplicit bias of total variation. We demonstrate the effectiveness of our RDMF\nmodel with extensive experiments, in which our method surpasses the state of\nart models in common examples, especially for the restoration from very few\nobservations. Our work sheds light on a more general framework for solving\nother inverse problems by combining the implicit bias of deep learning with\nexplicit regularization.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:05:35 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Li", "Zhemin", ""], ["Xu", "Zhi-Qin John", ""], ["Luo", "Tao", ""], ["Wang", "Hongxia", ""]]}, {"id": "2007.14587", "submitter": "T.J. Tsai", "authors": "TJ Tsai and Kevin Ji", "title": "Composer Style Classification of Piano Sheet Music Images Using Language\n  Model Pretraining", "comments": "8 pages, 7 figures. Accepted paper at the International Society for\n  Music Information Retrieval Conference (ISMIR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies composer style classification of piano sheet music images.\nPrevious approaches to the composer classification task have been limited by a\nscarcity of data. We address this issue in two ways: (1) we recast the problem\nto be based on raw sheet music images rather than a symbolic music format, and\n(2) we propose an approach that can be trained on unlabeled data. Our approach\nfirst converts the sheet music image into a sequence of musical \"words\" based\non the bootleg feature representation, and then feeds the sequence into a text\nclassifier. We show that it is possible to significantly improve classifier\nperformance by first training a language model on a set of unlabeled data,\ninitializing the classifier with the pretrained language model weights, and\nthen finetuning the classifier on a small amount of labeled data. We train\nAWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in\nIMSLP. We find that transformer-based architectures outperform CNN and LSTM\nmodels, and pretraining boosts classification accuracy for the GPT-2 model from\n46\\% to 70\\% on a 9-way classification task. The trained model can also be used\nas a feature extractor that projects piano sheet music into a feature space\nthat characterizes compositional style.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:13:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Tsai", "TJ", ""], ["Ji", "Kevin", ""]]}, {"id": "2007.14589", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Yuan Zhou, Nicha C. Dvornek, Muhan Zhang, Juntang Zhuang,\n  Pamela Ventola, and James S Duncan", "title": "Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis", "comments": "11 pages, 4 figures", "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how certain brain regions relate to a specific neurological\ndisorder has been an important area of neuroimaging research. A promising\napproach to identify the salient regions is using Graph Neural Networks (GNNs),\nwhich can be used to analyze graph structured data, e.g. brain networks\nconstructed by functional magnetic resonance imaging (fMRI). We propose an\ninterpretable GNN framework with a novel salient region selection mechanism to\ndetermine neurological brain biomarkers associated with disorders.\nSpecifically, we design novel regularized pooling layers that highlight salient\nregions of interests (ROIs) so that we can infer which ROIs are important to\nidentify a certain disease based on the node pooling scores calculated by the\npooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN),\nencourages reasonable ROI-selection and provides flexibility to preserve either\nindividual- or group-level patterns. We apply the PR-GNN framework on a\nBiopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different\nchoices of the hyperparameters and show that PR-GNN outperforms baseline\nmethods in terms of classification accuracy. The salient ROI detection results\nshow high correspondence with the previous neuroimaging-derived biomarkers for\nASD.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:19:36 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Zhou", "Yuan", ""], ["Dvornek", "Nicha C.", ""], ["Zhang", "Muhan", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S", ""]]}, {"id": "2007.14592", "submitter": "Wenjie Jian", "authors": "Zongqian Zhan (1), Wenjie Jian (1), Yihui Li (1), Xin Wang (2) and\n  Yang Yue (1) ((1) School of Geodesy and Geomatics, Wuhan University, China,\n  (2) Leibniz University Hannover Institute of Geodesy)", "title": "A SLAM Map Restoration Algorithm Based on Submaps and an Undirected\n  Connected Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual simultaneous localization and mapping (SLAM) systems have been\nshown to be accurate and robust, and have real-time performance capabilities on\nboth indoor and ground datasets. However, these methods can be problematic when\ndealing with aerial frames captured by a camera mounted on an unmanned aerial\nvehicle (UAV) because the flight height of the UAV can be difficult to control\nand is easily affected by the environment.To cope with the case of lost\ntracking, many visual SLAM systems employ a relocalization strategy. This\ninvolves the tracking thread continuing the online working by inspecting the\nconnections between the subsequent new frames and the generated map before the\ntracking was lost. To solve the missing map problem, which is an issue in many\napplications , after the tracking is lost, based on monocular visual SLAM, we\npresent a method of reconstructing a complete global map of UAV datasets by\nsequentially merging the submaps via the corresponding undirected connected\ngraph. Specifically, submaps are repeatedly generated, from the initialization\nprocess to the place where the tracking is lost, and a corresponding undirected\nconnected graph is built by considering these submaps as nodes and the common\nmap points within two submaps as edges. The common map points are then\ndetermined by the bag-of-words (BoW) method, and the submaps are merged if they\nare found to be connected with the online map in the undirect connected graph.\nTo demonstrate the performance of the proposed method, we first investigated\nthe performance on a UAV dataset, and the experimental results showed that, in\nthe case of several tracking failures, the integrity of the mapping was\nsignificantly better than that of the current mainstream SLAM method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:26:36 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zhan", "Zongqian", ""], ["Jian", "Wenjie", ""], ["Li", "Yihui", ""], ["Wang", "Xin", ""], ["Yue", "Yang", ""]]}, {"id": "2007.14606", "submitter": "Yuncong Chen", "authors": "Yuncong Chen and Will Warren", "title": "3D Fusion of Infrared Images with Dense RGB Reconstruction from Multiple\n  Views -- with Application to Fire-fighting Robots", "comments": "Technical report submitted to 2013 DRS Student Infrared Imaging\n  Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project integrates infrared and RGB imagery to produce dense 3D\nenvironment models reconstructed from multiple views. The resulting 3D map\ncontains both thermal and RGB information which can be used in robotic\nfire-fighting applications to identify victims and active fire areas.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 05:19:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Yuncong", ""], ["Warren", "Will", ""]]}, {"id": "2007.14612", "submitter": "Feng Liu", "authors": "Yiyang Zhang, Feng Liu, Zhen Fang, Bo Yuan, Guangquan Zhang, Jie Lu", "title": "Clarinet: A One-step Approach Towards Budget-friendly Unsupervised\n  Domain Adaptation", "comments": "This paper has been accepted by IJCAI-PRICAI 2020. Yiyang Zhang, Feng\n  Liu and Zhen Fang equally contribute to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In unsupervised domain adaptation (UDA), classifiers for the target domain\nare trained with massive true-label data from the source domain and unlabeled\ndata from the target domain. However, it may be difficult to collect\nfully-true-label data in a source domain given a limited budget. To mitigate\nthis problem, we consider a novel problem setting where the classifier for the\ntarget domain has to be trained with complementary-label data from the source\ndomain and unlabeled data from the target domain named budget-friendly UDA\n(BFUDA). The key benefit is that it is much less costly to collect\ncomplementary-label source data (required by BFUDA) than collecting the\ntrue-label source data (required by ordinary UDA). To this end, the\ncomplementary label adversarial network (CLARINET) is proposed to solve the\nBFUDA problem. CLARINET maintains two deep networks simultaneously, where one\nfocuses on classifying complementary-label source data and the other takes care\nof the source-to-target distributional adaptation. Experiments show that\nCLARINET significantly outperforms a series of competent baselines.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 05:31:58 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 06:09:13 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Yiyang", ""], ["Liu", "Feng", ""], ["Fang", "Zhen", ""], ["Yuan", "Bo", ""], ["Zhang", "Guangquan", ""], ["Lu", "Jie", ""]]}, {"id": "2007.14615", "submitter": "Wenshuang Liu", "authors": "Wenshuang Liu, Wenting Chen, Linlin Shen", "title": "Translate the Facial Regions You Like Using Region-Wise Normalization", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though GAN (Generative Adversarial Networks) based technique has greatly\nadvanced the performance of image synthesis and face translation, only few\nworks available in literature provide region based style encoding and\ntranslation. We propose in this paper a region-wise normalization framework,\nfor region level face translation. While per-region style is encoded using\navailable approach, we build a so called RIN (region-wise normalization) block\nto individually inject the styles into per-region feature maps and then fuse\nthem for following convolution and upsampling. Both shape and texture of\ndifferent regions can thus be translated to various target styles. A region\nmatching loss has also been proposed to significantly reduce the inference\nbetween regions during the translation process. Extensive experiments on three\npublicly available datasets, i.e. Morph, RaFD and CelebAMask-HQ, suggest that\nour approach demonstrate a large improvement over state-of-the-art methods like\nStarGAN, SEAN and FUNIT. Our approach has further advantages in precise control\nof the regions to be translated. As a result, region level expression changes\nand step by step make up can be achieved. The video demo is available at\nhttps://youtu.be/ceRqsbzXAfk.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 05:55:49 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Liu", "Wenshuang", ""], ["Chen", "Wenting", ""], ["Shen", "Linlin", ""]]}, {"id": "2007.14621", "submitter": "M. Salman Asif", "authors": "Rakib Hyder, Zikui Cai, and M. Salman Asif", "title": "Solving Phase Retrieval with a Learned Reference", "comments": "Accepted to ECCV 2020. Code is available at\n  https://github.com/CSIPlab/learnPR_reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier phase retrieval is a classical problem that deals with the recovery\nof an image from the amplitude measurements of its Fourier coefficients.\nConventional methods solve this problem via iterative (alternating)\nminimization by leveraging some prior knowledge about the structure of the\nunknown image. The inherent ambiguities about shift and flip in the Fourier\nmeasurements make this problem especially difficult; and most of the existing\nmethods use several random restarts with different permutations. In this paper,\nwe assume that a known (learned) reference is added to the signal before\ncapturing the Fourier amplitude measurements. Our method is inspired by the\nprinciple of adding a reference signal in holography. To recover the signal, we\nimplement an iterative phase retrieval method as an unrolled network. Then we\nuse back propagation to learn the reference that provides us the best\nreconstruction for a fixed number of phase retrieval iterations. We performed a\nnumber of simulations on a variety of datasets under different conditions and\nfound that our proposed method for phase retrieval via unrolled network and\nlearned reference provides near-perfect recovery at fixed (small) computational\ncost. We compared our method with standard Fourier phase retrieval methods and\nobserved significant performance enhancement using the learned reference.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 06:17:25 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Hyder", "Rakib", ""], ["Cai", "Zikui", ""], ["Asif", "M. Salman", ""]]}, {"id": "2007.14625", "submitter": "Lei Bi", "authors": "Lei Bi, Jinman Kim, Tingwei Su, Michael Fulham, David Dagan Feng,\n  Guang Ning", "title": "Deep Multi-Scale Resemblance Network for the Sub-class Differentiation\n  of Adrenal Masses on Computed Tomography Images", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The accurate classification of mass lesions in the adrenal glands\n('adrenal masses'), detected with computed tomography (CT), is important for\ndiagnosis and patient management. Adrenal masses can be benign or malignant and\nthe benign masses have varying prevalence. Classification methods based on\nconvolutional neural networks (CNN) are the state-of-the-art in maximizing\ninter-class differences in large medical imaging training datasets. The\napplication of CNNs, to adrenal masses is challenging due to large intra-class\nvariations, large inter-class similarities and imbalanced training data due to\nthe size of masses. Methods: We developed a deep multi-scale resemblance\nnetwork (DMRN) to overcome these limitations and leveraged paired CNNs to\nevaluate the intra-class similarities. We used multi-scale feature embedding to\nimprove the inter-class separability by iteratively combining complementary\ninformation produced at different scales of the input to create structured\nfeature descriptors. We also augmented the training data with randomly sampled\npaired adrenal masses to reduce the influence of imbalanced training data. We\nused 229 CT scans of patients with adrenal masses. Results: Our method had the\nbest results compared to state-of-the-art methods. Conclusion: Our DMRN\nsub-classified adrenal masses on CT and was superior to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 06:24:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bi", "Lei", ""], ["Kim", "Jinman", ""], ["Su", "Tingwei", ""], ["Fulham", "Michael", ""], ["Feng", "David Dagan", ""], ["Ning", "Guang", ""]]}, {"id": "2007.14626", "submitter": "Yuankai Qi", "authors": "Yuankai Qi, Zizheng Pan, Shengping Zhang, Anton van den Hengel, Qi Wu", "title": "Object-and-Action Aware Model for Visual Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) is unique in that it requires turning\nrelatively general natural-language instructions into robot agent actions, on\nthe basis of the visible environment. This requires to extract value from two\nvery different types of natural-language information. The first is object\ndescription (e.g., 'table', 'door'), each presenting as a tip for the agent to\ndetermine the next action by finding the item visible in the environment, and\nthe second is action specification (e.g., 'go straight', 'turn left') which\nallows the robot to directly predict the next movements without relying on\nvisual perceptions. However, most existing methods pay few attention to\ndistinguish these information from each other during instruction encoding and\nmix together the matching between textual object/action encoding and visual\nperception/orientation features of candidate viewpoints. In this paper, we\npropose an Object-and-Action Aware Model (OAAM) that processes these two\ndifferent forms of natural language based instruction separately. This enables\neach process to match object-centered/action-centered instruction to their own\ncounterpart visual perception/action orientation flexibly. However, one\nside-issue caused by above solution is that an object mentioned in instructions\nmay be observed in the direction of two or more candidate viewpoints, thus the\nOAAM may not predict the viewpoint on the shortest path as the next action. To\nhandle this problem, we design a simple but effective path loss to penalize\ntrajectories deviating from the ground truth path. Experimental results\ndemonstrate the effectiveness of the proposed model and path loss, and the\nsuperiority of their combination with a 50% SPL score on the R2R dataset and a\n40% CLS score on the R4R dataset in unseen environments, outperforming the\nprevious state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 06:32:18 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Qi", "Yuankai", ""], ["Pan", "Zizheng", ""], ["Zhang", "Shengping", ""], ["Hengel", "Anton van den", ""], ["Wu", "Qi", ""]]}, {"id": "2007.14628", "submitter": "Dylan Campbell", "authors": "Dylan Campbell, Liu Liu, Stephen Gould", "title": "Solving the Blind Perspective-n-Point Problem End-To-End With Robust\n  Differentiable Geometric Optimization", "comments": "Presented at ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind Perspective-n-Point (PnP) is the problem of estimating the position and\norientation of a camera relative to a scene, given 2D image points and 3D scene\npoints, without prior knowledge of the 2D-3D correspondences. Solving for pose\nand correspondences simultaneously is extremely challenging since the search\nspace is very large. Fortunately it is a coupled problem: the pose can be found\neasily given the correspondences and vice versa. Existing approaches assume\nthat noisy correspondences are provided, that a good pose prior is available,\nor that the problem size is small. We instead propose the first fully\nend-to-end trainable network for solving the blind PnP problem efficiently and\nglobally, that is, without the need for pose priors. We make use of recent\nresults in differentiating optimization problems to incorporate geometric model\nfitting into an end-to-end learning framework, including Sinkhorn, RANSAC and\nPnP algorithms. Our proposed approach significantly outperforms other methods\non synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 06:35:45 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 02:51:35 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Campbell", "Dylan", ""], ["Liu", "Liu", ""], ["Gould", "Stephen", ""]]}, {"id": "2007.14638", "submitter": "Yifan Jiang", "authors": "Yifan Jiang, Han Chen, Murray Loew, and Hanseok Ko", "title": "COVID-19 CT Image Synthesis with a Conditional Generative Adversarial\n  Network", "comments": "Accepted by IEEE Journal of Biomedical and Health Informatics (J-BHI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has\nspread rapidly since December 2019. Real-time reverse transcription polymerase\nchain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play\nan important role in COVID-19 diagnosis. Chest CT imaging offers the benefits\nof quick reporting, a low cost, and high sensitivity for the detection of\npulmonary infection. Recently, deep-learning-based computer vision methods have\ndemonstrated great promise for use in medical imaging applications, including\nX-rays, magnetic resonance imaging, and CT imaging. However, training a\ndeep-learning model requires large volumes of data, and medical staff faces a\nhigh risk when collecting COVID-19 CT data due to the high infectivity of the\ndisease. Another issue is the lack of experts available for data labeling. In\norder to meet the data requirements for COVID-19 CT imaging, we propose a CT\nimage synthesis approach based on a conditional generative adversarial network\nthat can effectively generate high-quality and realistic COVID-19 CT images for\nuse in deep-learning-based medical imaging tasks. Experimental results show\nthat the proposed method outperforms other state-of-the-art image synthesis\nmethods with the generated COVID-19 CT images and indicates promising for\nvarious machine learning applications including semantic segmentation and\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 07:20:06 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:53:57 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Jiang", "Yifan", ""], ["Chen", "Han", ""], ["Loew", "Murray", ""], ["Ko", "Hanseok", ""]]}, {"id": "2007.14658", "submitter": "Toby Perrett", "authors": "Toby Perrett, Alessandro Masullo, Tilo Burghardt, Majid Mirmehdi, Dima\n  Damen", "title": "Meta-Learning with Context-Agnostic Initialisations", "comments": "Accepted at ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning approaches have addressed few-shot problems by finding\ninitialisations suited for fine-tuning to target tasks. Often there are\nadditional properties within training data (which we refer to as context), not\nrelevant to the target task, which act as a distractor to meta-learning,\nparticularly when the target task contains examples from a novel context not\nseen during training. We address this oversight by incorporating a\ncontext-adversarial component into the meta-learning process. This produces an\ninitialisation for fine-tuning to target which is both context-agnostic and\ntask-generalised. We evaluate our approach on three commonly used meta-learning\nalgorithms and two problems. We demonstrate our context-agnostic meta-learning\nimproves results in each case. First, we report on Omniglot few-shot character\nclassification, using alphabets as context. An average improvement of 4.3% is\nobserved across methods and tasks when classifying characters from an unseen\nalphabet. Second, we evaluate on a dataset for personalised energy expenditure\npredictions from video, using participant knowledge as context. We demonstrate\nthat context-agnostic meta-learning decreases the average mean square error by\n30%.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:08:38 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 06:50:34 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Perrett", "Toby", ""], ["Masullo", "Alessandro", ""], ["Burghardt", "Tilo", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""]]}, {"id": "2007.14671", "submitter": "Ali Alizadeh", "authors": "Yunus Bicer, Ali Alizadeh, Nazim Kemal Ure, Ahmetcan Erdogan, and\n  Orkun Kizilirmak", "title": "Sample Efficient Interactive End-to-End Deep Learning for Self-Driving\n  Cars with Selective Multi-Class Safe Dataset Aggregation", "comments": "6 pages, 6 figures, IROS2019 conference", "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Macau, China, 2019, pp. 2629-2634", "doi": "10.1109/IROS40897.2019.8967948", "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to develop a sample efficient end-to-end deep\nlearning method for self-driving cars, where we attempt to increase the value\nof the information extracted from samples, through careful analysis obtained\nfrom each call to expert driver\\'s policy. End-to-end imitation learning is a\npopular method for computing self-driving car policies. The standard approach\nrelies on collecting pairs of inputs (camera images) and outputs (steering\nangle, etc.) from an expert policy and fitting a deep neural network to this\ndata to learn the driving policy. Although this approach had some successful\ndemonstrations in the past, learning a good policy might require a lot of\nsamples from the expert driver, which might be resource-consuming. In this\nwork, we develop a novel framework based on the Safe Dateset Aggregation (safe\nDAgger) approach, where the current learned policy is automatically segmented\ninto different trajectory classes, and the algorithm identifies trajectory\nsegments or classes with the weak performance at each step. Once the trajectory\nsegments with weak performance identified, the sampling algorithm focuses on\ncalling the expert policy only on these segments, which improves the\nconvergence rate. The presented simulation results show that the proposed\napproach can yield significantly better performance compared to the standard\nSafe DAgger algorithm while using the same amount of samples from the expert.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:38:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bicer", "Yunus", ""], ["Alizadeh", "Ali", ""], ["Ure", "Nazim Kemal", ""], ["Erdogan", "Ahmetcan", ""], ["Kizilirmak", "Orkun", ""]]}, {"id": "2007.14672", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih\n  Porikli", "title": "Stylized Adversarial Defense", "comments": "Code is available at this http https://github.com/Muzammal-Naseer/SAT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolution Neural Networks (CNNs) can easily be fooled by subtle,\nimperceptible changes to the input images. To address this vulnerability,\nadversarial training creates perturbation patterns and includes them in the\ntraining set to robustify the model. In contrast to existing adversarial\ntraining methods that only use class-boundary information (e.g., using a cross\nentropy loss), we propose to exploit additional information from the feature\nspace to craft stronger adversaries that are in turn used to learn a robust\nmodel. Specifically, we use the style and content information of the target\nsample from another class, alongside its class boundary information to create\nadversarial perturbations. We apply our proposed multi-task objective in a\ndeeply supervised manner, extracting multi-scale feature knowledge to create\nmaximally separating adversaries. Subsequently, we propose a max-margin\nadversarial training approach that minimizes the distance between source image\nand its adversary and maximizes the distance between the adversary and the\ntarget image. Our adversarial training approach demonstrates strong robustness\ncompared to state of the art defenses, generalizes well to naturally occurring\ncorruptions and data distributional shifts, and retains the model accuracy on\nclean examples.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:38:10 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Porikli", "Fatih", ""]]}, {"id": "2007.14682", "submitter": "Philipp Rimle", "authors": "Philipp Rimle, Pelin Dogan, Markus Gross", "title": "Enriching Video Captions With Contextual Text", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding video content and generating caption with context is an\nimportant and challenging task. Unlike prior methods that typically attempt to\ngenerate generic video captions without context, our architecture\ncontextualizes captioning by infusing extracted information from relevant text\ndata. We propose an end-to-end sequence-to-sequence model which generates video\ncaptions based on visual input, and mines relevant knowledge such as names and\nlocations from contextual text. In contrast to previous approaches, we do not\npreprocess the text further, and let the model directly learn to attend over\nit. Guided by the visual input, the model is able to copy words from the\ncontextual text via a pointer-generator network, allowing to produce more\nspecific video captions. We show competitive performance on the News Video\nDataset and, through ablation studies, validate the efficacy of contextual\nvideo captioning as well as individual design choices in our model\narchitecture.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:58:52 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Rimle", "Philipp", ""], ["Dogan", "Pelin", ""], ["Gross", "Markus", ""]]}, {"id": "2007.14690", "submitter": "Di Xie", "authors": "Fanfan Ye and Shiliang Pu and Qiaoyong Zhong and Chao Li and Di Xie\n  and Huiming Tang", "title": "Dynamic GCN: Context-enriched Topology Learning for Skeleton-based\n  Action Recognition", "comments": "accepted by ACMMM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have attracted increasing interests for\nthe task of skeleton-based action recognition. The key lies in the design of\nthe graph structure, which encodes skeleton topology information. In this\npaper, we propose Dynamic GCN, in which a novel convolutional neural network\nnamed Contextencoding Network (CeN) is introduced to learn skeleton topology\nautomatically. In particular, when learning the dependency between two joints,\ncontextual features from the rest joints are incorporated in a global manner.\nCeN is extremely lightweight yet effective, and can be embedded into a graph\nconvolutional layer. By stacking multiple CeN-enabled graph convolutional\nlayers, we build Dynamic GCN. Notably, as a merit of CeN, dynamic graph\ntopologies are constructed for different input samples as well as graph\nconvolutional layers of various depths. Besides, three alternative context\nmodeling architectures are well explored, which may serve as a guideline for\nfuture research on graph topology learning. CeN brings only ~7% extra FLOPs for\nthe baseline model, and Dynamic GCN achieves better performance with\n$2\\times$~$4\\times$ fewer FLOPs than existing methods. By further combining\nstatic physical body connections and motion modalities, we achieve\nstate-of-the-art performance on three large-scale benchmarks, namely NTU-RGB+D,\nNTU-RGB+D 120 and Skeleton-Kinetics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:12:06 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Ye", "Fanfan", ""], ["Pu", "Shiliang", ""], ["Zhong", "Qiaoyong", ""], ["Li", "Chao", ""], ["Xie", "Di", ""], ["Tang", "Huiming", ""]]}, {"id": "2007.14726", "submitter": "Di Ma", "authors": "Di Ma, Fan Zhang and David R. Bull", "title": "Video compression with low complexity CNN-based spatial resolution\n  adaptation", "comments": null, "journal-ref": null, "doi": "10.1117/12.2567633", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It has recently been demonstrated that spatial resolution adaptation can be\nintegrated within video compression to improve overall coding performance by\nspatially down-sampling before encoding and super-resolving at the decoder.\nSignificant improvements have been reported when convolutional neural networks\n(CNNs) were used to perform the resolution up-sampling. However, this approach\nsuffers from high complexity at the decoder due to the employment of CNN-based\nsuper-resolution. In this paper, a novel framework is proposed which supports\nthe flexible allocation of complexity between the encoder and decoder. This\napproach employs a CNN model for video down-sampling at the encoder and uses a\nLanczos3 filter to reconstruct full resolution at the decoder. The proposed\nmethod was integrated into the HEVC HM 16.20 software and evaluated on JVET UHD\ntest sequences using the All Intra configuration. The experimental results\ndemonstrate the potential of the proposed approach, with significant bitrate\nsavings (more than 10%) over the original HEVC HM, coupled with reduced\ncomputational complexity at both encoder (29%) and decoder (10%).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 10:20:36 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2007.14728", "submitter": "Xiaohang Fu", "authors": "Xiaohang Fu, Lei Bi, Ashnil Kumar, Michael Fulham and Jinman Kim", "title": "Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung\n  Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal positron emission tomography-computed tomography (PET-CT) is used\nroutinely in the assessment of cancer. PET-CT combines the high sensitivity for\ntumor detection with PET and anatomical information from CT. Tumor segmentation\nis a critical element of PET-CT but at present, there is not an accurate\nautomated segmentation method. Segmentation tends to be done manually by\ndifferent imaging experts and it is labor-intensive and prone to errors and\ninconsistency. Previous automated segmentation methods largely focused on\nfusing information that is extracted separately from the PET and CT modalities,\nwith the underlying assumption that each modality contains complementary\ninformation. However, these methods do not fully exploit the high PET tumor\nsensitivity that can guide the segmentation. We introduce a multimodal spatial\nattention module (MSAM) that automatically learns to emphasize regions (spatial\nareas) related to tumors and suppress normal regions with physiologic\nhigh-uptake. The resulting spatial attention maps are subsequently employed to\ntarget a convolutional neural network (CNN) for segmentation of areas with\nhigher tumor likelihood. Our MSAM can be applied to common backbone\narchitectures and trained end-to-end. Our experimental results on two clinical\nPET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma\n(STS) validate the effectiveness of the MSAM in these different cancer types.\nWe show that our MSAM, with a conventional U-Net backbone, surpasses the\nstate-of-the-art lung tumor segmentation approach by a margin of 7.6% in Dice\nsimilarity coefficient (DSC).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 10:27:22 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 04:50:27 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Fu", "Xiaohang", ""], ["Bi", "Lei", ""], ["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "2007.14741", "submitter": "Syed Afaq Ali Shah", "authors": "Syed Afaq Ali Shah, Weifeng Deng, Jianxin Li, Muhammad Aamir Cheema,\n  Abdul Bais", "title": "CommuNety: A Deep Learning System for the Prediction of Cohesive Social\n  Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective mining of social media, which consists of a large number of users\nis a challenging task. Traditional approaches rely on the analysis of text data\nrelated to users to accomplish this task. However, text data lacks significant\ninformation about the social users and their associated groups. In this paper,\nwe propose CommuNety, a deep learning system for the prediction of cohesive\nsocial networks using images. The proposed deep learning model consists of\nhierarchical CNN architecture to learn descriptive features related to each\ncohesive network. The paper also proposes a novel Face Co-occurrence Frequency\nalgorithm to quantify existence of people in images, and a novel photo ranking\nmethod to analyze the strength of relationship between different individuals in\na predicted social network. We extensively evaluate the proposed technique on\nPIPA dataset and compare with state-of-the-art methods. Our experimental\nresults demonstrate the superior performance of the proposed technique for the\nprediction of relationship between different individuals and the cohesiveness\nof communities.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 11:03:22 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Shah", "Syed Afaq Ali", ""], ["Deng", "Weifeng", ""], ["Li", "Jianxin", ""], ["Cheema", "Muhammad Aamir", ""], ["Bais", "Abdul", ""]]}, {"id": "2007.14745", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann and Jonas Adler", "title": "On the unreasonable effectiveness of CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods using convolutional neural networks (CNN) have been\nsuccessfully applied to virtually all imaging problems, and particularly in\nimage reconstruction tasks with ill-posed and complicated imaging models. In an\nattempt to put upper bounds on the capability of baseline CNNs for solving\nimage-to-image problems we applied a widely used standard off-the-shelf network\narchitecture (U-Net) to the \"inverse problem\" of XOR decryption from noisy data\nand show acceptable results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 11:16:20 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Adler", "Jonas", ""]]}, {"id": "2007.14772", "submitter": "Jiale Cao", "authors": "Jiale Cao, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan,\n  Yanwei Pang, Ling Shao", "title": "SipMask: Spatial Information Preservation for Fast Image and Video\n  Instance Segmentation", "comments": "ECCV2020; Code: https://github.com/JialeCao001/SipMask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-stage instance segmentation approaches have recently gained popularity\ndue to their speed and simplicity, but are still lagging behind in accuracy,\ncompared to two-stage methods. We propose a fast single-stage instance\nsegmentation method, called SipMask, that preserves instance-specific spatial\ninformation by separating mask prediction of an instance to different\nsub-regions of a detected bounding-box. Our main contribution is a novel\nlight-weight spatial preservation (SP) module that generates a separate set of\nspatial coefficients for each sub-region within a bounding-box, leading to\nimproved mask predictions. It also enables accurate delineation of spatially\nadjacent instances. Further, we introduce a mask alignment weighting loss and a\nfeature alignment scheme to better correlate mask prediction with object\ndetection. On COCO test-dev, our SipMask outperforms the existing single-stage\nmethods. Compared to the state-of-the-art single-stage TensorMask, SipMask\nobtains an absolute gain of 1.0% (mask AP), while providing a four-fold\nspeedup. In terms of real-time capabilities, SipMask outperforms YOLACT with an\nabsolute gain of 3.0% (mask AP) under similar settings, while operating at\ncomparable speed on a Titan Xp. We also evaluate our SipMask for real-time\nvideo instance segmentation, achieving promising results on YouTube-VIS\ndataset. The source code is available at\nhttps://github.com/JialeCao001/SipMask.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:21:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Cao", "Jiale", ""], ["Anwer", "Rao Muhammad", ""], ["Cholakkal", "Hisham", ""], ["Khan", "Fahad Shahbaz", ""], ["Pang", "Yanwei", ""], ["Shao", "Ling", ""]]}, {"id": "2007.14777", "submitter": "Ashad Kabir", "authors": "Nihad Karim Chowdhury, Md. Muhtadir Rahman, Muhammad Ashad Kabir", "title": "PDCOVIDNet: A Parallel-Dilated Convolutional Neural Network Architecture\n  for Detecting COVID-19 from Chest X-Ray Images", "comments": null, "journal-ref": "Health information science and systems, 2020", "doi": "10.1007/s13755-020-00119-3", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic continues to severely undermine the prosperity of the\nglobal health system. To combat this pandemic, effective screening techniques\nfor infected patients are indispensable. There is no doubt that the use of\nchest X-ray images for radiological assessment is one of the essential\nscreening techniques. Some of the early studies revealed that the patient's\nchest X-ray images showed abnormalities, which is natural for patients infected\nwith COVID-19. In this paper, we proposed a parallel-dilated convolutional\nneural network (CNN) based COVID-19 detection system from chest x-ray images,\nnamed as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available\nchest X-ray collection fully preloaded and enhanced, and then classified by the\nproposed method. Differing convolution dilation rate in a parallel form\ndemonstrates the proof-of-principle for using PDCOVIDNet to extract\nradiological features for COVID-19 detection. Accordingly, we have assisted our\nmethod with two visualization methods, which are specifically designed to\nincrease understanding of the key components associated with COVID-19\ninfection. Both visualization methods compute gradients for a given image\ncategory related to feature maps of the last convolutional layer to create a\nclass-discriminative region. In our experiment, we used a total of 2,905 chest\nX-ray images, comprising three cases (such as COVID-19, normal, and viral\npneumonia), and empirical evaluations revealed that the proposed method\nextracted more significant features expeditiously related to the suspected\ndisease. The experimental results demonstrate that our proposed method\nsignificantly improves performance metrics: accuracy, precision, recall, and F1\nscores reach 96.58%, 96.58%, 96.59%, and 96.58%, respectively, which is\ncomparable or enhanced compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:28:16 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Chowdhury", "Nihad Karim", ""], ["Rahman", "Md. Muhtadir", ""], ["Kabir", "Muhammad Ashad", ""]]}, {"id": "2007.14790", "submitter": "Saba Heidari Gheshlaghi", "authors": "Saba Heidari Gheshlaghi, Omid Dehzangi, Ali Dabouei, Annahita\n  Amireskandari, Ali Rezai, Nasser M Nasrabadi", "title": "Efficient OCT Image Segmentation Using Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a Neural Architecture Search (NAS) for retinal layer\nsegmentation in Optical Coherence Tomography (OCT) scans. We incorporate the\nUnet architecture in the NAS framework as its backbone for the segmentation of\nthe retinal layers in our collected and pre-processed OCT image dataset. At the\npre-processing stage, we conduct super resolution and image processing\ntechniques on the raw OCT scans to improve the quality of the raw images. For\nour search strategy, different primitive operations are suggested to find the\ndown- & up-sampling cell blocks, and the binary gate method is applied to make\nthe search strategy practical for the task in hand. We empirically evaluated\nour method on our in-house OCT dataset. The experimental results demonstrate\nthat the self-adapting NAS-Unet architecture substantially outperformed the\ncompetitive human-designed architecture by achieving 95.4% in mean Intersection\nover Union metric and 78.7% in Dice similarity coefficient.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 02:48:07 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Gheshlaghi", "Saba Heidari", ""], ["Dehzangi", "Omid", ""], ["Dabouei", "Ali", ""], ["Amireskandari", "Annahita", ""], ["Rezai", "Ali", ""], ["Nasrabadi", "Nasser M", ""]]}, {"id": "2007.14808", "submitter": "Justus Thies", "authors": "Justus Thies and Michael Zollh\\\"ofer and Marc Stamminger and Christian\n  Theobalt and Matthias Nie{\\ss}ner", "title": "Face2Face: Real-time Face Capture and Reenactment of RGB Videos", "comments": "https://justusthies.github.io/posts/acm-research-highlight/", "journal-ref": "CVPR2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Face2Face, a novel approach for real-time facial reenactment of a\nmonocular target video sequence (e.g., Youtube video). The source sequence is\nalso a monocular video stream, captured live with a commodity webcam. Our goal\nis to animate the facial expressions of the target video by a source actor and\nre-render the manipulated output video in a photo-realistic fashion. To this\nend, we first address the under-constrained problem of facial identity recovery\nfrom monocular video by non-rigid model-based bundling. At run time, we track\nfacial expressions of both source and target video using a dense photometric\nconsistency measure. Reenactment is then achieved by fast and efficient\ndeformation transfer between source and target. The mouth interior that best\nmatches the re-targeted expression is retrieved from the target sequence and\nwarped to produce an accurate fit. Finally, we convincingly re-render the\nsynthesized target face on top of the corresponding video stream such that it\nseamlessly blends with the real-world illumination. We demonstrate our method\nin a live setup, where Youtube videos are reenacted in real time.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:47:16 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Stamminger", "Marc", ""], ["Theobalt", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2007.14812", "submitter": "C\\'edric Picron", "authors": "C\\'edric Picron, Punarjay Chakravarty, Tom Roussel, Tinne Tuytelaars", "title": "What My Motion tells me about Your Pose: A Self-Supervised Monocular 3D\n  Vehicle Detector", "comments": "ICRA 2021 (presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the orientation of an observed vehicle relative to an\nAutonomous Vehicle (AV) from monocular camera data is an important building\nblock in estimating its 6 DoF pose. Current Deep Learning based solutions for\nplacing a 3D bounding box around this observed vehicle are data hungry and do\nnot generalize well. In this paper, we demonstrate the use of monocular visual\nodometry for the self-supervised fine-tuning of a model for orientation\nestimation pre-trained on a reference domain. Specifically, while transitioning\nfrom a virtual dataset (vKITTI) to nuScenes, we recover up to 70% of the\nperformance of a fully supervised method. We subsequently demonstrate an\noptimization-based monocular 3D bounding box detector built on top of the\nself-supervised vehicle orientation estimator without the requirement of\nexpensive labeled data. This allows 3D vehicle detection algorithms to be\nself-trained from large amounts of monocular camera data from existing\ncommercial vehicle fleets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:58:40 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 18:11:37 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Picron", "C\u00e9dric", ""], ["Chakravarty", "Punarjay", ""], ["Roussel", "Tom", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2007.14846", "submitter": "Abbas Khosravi", "authors": "Afshar Shamsi Jokandan, Hamzeh Asgharnezhad, Shirin Shamsi Jokandan,\n  Abbas Khosravi, Parham M.Kebria, Darius Nahavandi, Saeid Nahavandi, and Dipti\n  Srinivasan", "title": "An Uncertainty-aware Transfer Learning-based Framework for Covid-19\n  Diagnosis", "comments": "9 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early and reliable detection of COVID-19 infected patients is essential\nto prevent and limit its outbreak. The PCR tests for COVID-19 detection are not\navailable in many countries and also there are genuine concerns about their\nreliability and performance. Motivated by these shortcomings, this paper\nproposes a deep uncertainty-aware transfer learning framework for COVID-19\ndetection using medical images. Four popular convolutional neural networks\n(CNNs) including VGG16, ResNet50, DenseNet121, and InceptionResNetV2 are first\napplied to extract deep features from chest X-ray and computed tomography (CT)\nimages. Extracted features are then processed by different machine learning and\nstatistical modelling techniques to identify COVID-19 cases. We also calculate\nand report the epistemic uncertainty of classification results to identify\nregions where the trained models are not confident about their decisions (out\nof distribution problem). Comprehensive simulation results for X-ray and CT\nimage datasets indicate that linear support vector machine and neural network\nmodels achieve the best results as measured by accuracy, sensitivity,\nspecificity, and AUC. Also it is found that predictive uncertainty estimates\nare much higher for CT images compared to X-ray images.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 20:15:01 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Jokandan", "Afshar Shamsi", ""], ["Asgharnezhad", "Hamzeh", ""], ["Jokandan", "Shirin Shamsi", ""], ["Khosravi", "Abbas", ""], ["Kebria", "Parham M.", ""], ["Nahavandi", "Darius", ""], ["Nahavandi", "Saeid", ""], ["Srinivasan", "Dipti", ""]]}, {"id": "2007.14848", "submitter": "Shuang Yu", "authors": "Shuang Yu, Hong-Yu Zhou, Kai Ma, Cheng Bian, Chunyan Chu, Hanruo Liu,\n  Yefeng Zheng", "title": "Difficulty-aware Glaucoma Classification with Multi-Rater Consensus\n  Modeling", "comments": null, "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical images are generally labeled by multiple experts before the final\nground-truth labels are determined. Consensus or disagreement among experts\nregarding individual images reflects the gradeability and difficulty levels of\nthe image. However, when being used for model training, only the final\nground-truth label is utilized, while the critical information contained in the\nraw multi-rater gradings regarding the image being an easy/hard case is\ndiscarded. In this paper, we aim to take advantage of the raw multi-rater\ngradings to improve the deep learning model performance for the glaucoma\nclassification task. Specifically, a multi-branch model structure is proposed\nto predict the most sensitive, most specifical and a balanced fused result for\nthe input images. In order to encourage the sensitivity branch and specificity\nbranch to generate consistent results for consensus labels and opposite results\nfor disagreement labels, a consensus loss is proposed to constrain the output\nof the two branches. Meanwhile, the consistency/inconsistency between the\nprediction results of the two branches implies the image being an easy/hard\ncase, which is further utilized to encourage the balanced fusion branch to\nconcentrate more on the hard cases. Compared with models trained only with the\nfinal ground-truth labels, the proposed method using multi-rater consensus\ninformation has achieved superior performance, and it is also able to estimate\nthe difficulty levels of individual input images when making the prediction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:04:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yu", "Shuang", ""], ["Zhou", "Hong-Yu", ""], ["Ma", "Kai", ""], ["Bian", "Cheng", ""], ["Chu", "Chunyan", ""], ["Liu", "Hanruo", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.14852", "submitter": "Shuang Yu", "authors": "Wenting Chen, Shuang Yu, Junde Wu, Kai Ma, Cheng Bian, Chunyan Chu,\n  Linlin Shen, Yefeng Zheng", "title": "TR-GAN: Topology Ranking GAN with Triplet Loss for Retinal Artery/Vein\n  Classification", "comments": null, "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Retinal artery/vein (A/V) classification lays the foundation for the\nquantitative analysis of retinal vessels, which is associated with potential\nrisks of various cardiovascular and cerebral diseases. The topological\nconnection relationship, which has been proved effective in improving the A/V\nclassification performance for the conventional graph based method, has not\nbeen exploited by the deep learning based method. In this paper, we propose a\nTopology Ranking Generative Adversarial Network (TR-GAN) to improve the\ntopology connectivity of the segmented arteries and veins, and further to boost\nthe A/V classification performance. A topology ranking discriminator based on\nordinal regression is proposed to rank the topological connectivity level of\nthe ground-truth, the generated A/V mask and the intentionally shuffled mask.\nThe ranking loss is further back-propagated to the generator to generate better\nconnected A/V masks. In addition, a topology preserving module with triplet\nloss is also proposed to extract the high-level topological features and\nfurther to narrow the feature distance between the predicted A/V mask and the\nground-truth. The proposed framework effectively increases the topological\nconnectivity of the predicted A/V masks and achieves state-of-the-art A/V\nclassification performance on the publicly available AV-DRIVE dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:11:19 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chen", "Wenting", ""], ["Yu", "Shuang", ""], ["Wu", "Junde", ""], ["Ma", "Kai", ""], ["Bian", "Cheng", ""], ["Chu", "Chunyan", ""], ["Shen", "Linlin", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2007.14863", "submitter": "Wesley Lobato Passos", "authors": "Wesley L. Passos (1), Eduardo A. B. da Silva (1), Sergio L. Netto (1),\n  Gabriel M. Araujo (2), Amaro A. de Lima (2) ((1) PEE/COPPE/DEL/POLI,\n  Universidade Federal do Rio de Janeiro, (2) Centro Federal de Educa\\c{c}\\~ao\n  Tecnol\\'ogica Celso Suckow da Fonseca)", "title": "Spatio-temporal Consistency to Detect Potential Aedes aegypti Breeding\n  Grounds in Aerial Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every year, the \\textit{Aedes aegypti} mosquito infects thousands of people\nwith diseases such as dengue, zika, chikungunya, and urban yellow fever. The\nmain form to combat these diseases is to avoid the transmitter reproduction by\nsearching and eliminating the potential mosquito breeding grounds. In this\nwork, we introduce a comprehensive database of aerial videos recorded with a\ndrone, where all objects of interest are identified by their respective\nbounding boxes, and describe an object detection system based on deep neural\nnetworks. We track the objects by employing phase correlation to obtain the\nspatial alignment between them along the video frames. By doing so, we are\ncapable of registering the detected objects, minimizing false positives and\ncorrecting most false negatives. Using the ResNet-101-FPN as a backbone, it is\npossible to obtain 0.78 in terms of \\textit{F1-score} on the proposed dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:30:54 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Passos", "Wesley L.", ""], ["da Silva", "Eduardo A. B.", ""], ["Netto", "Sergio L.", ""], ["Araujo", "Gabriel M.", ""], ["de Lima", "Amaro A.", ""]]}, {"id": "2007.14878", "submitter": "Junzhe Zhang Mr", "authors": "Zhongang Cai, Junzhe Zhang, Daxuan Ren, Cunjun Yu, Haiyu Zhao, Shuai\n  Yi, Chai Kiat Yeo, Chen Change Loy", "title": "MessyTable: Instance Association in Multiple Camera Views", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interesting and challenging dataset that features a large\nnumber of scenes with messy tables captured from multiple camera views. Each\nscene in this dataset is highly complex, containing multiple object instances\nthat could be identical, stacked and occluded by other instances. The key\nchallenge is to associate all instances given the RGB image of all views. The\nseemingly simple task surprisingly fails many popular methods or heuristics\nthat we assume good performance in object association. The dataset challenges\nexisting methods in mining subtle appearance differences, reasoning based on\ncontexts, and fusing appearance with geometric cues for establishing an\nassociation. We report interesting findings with some popular baselines, and\ndiscuss how this dataset could help inspire new problems and catalyse more\nrobust formulations to tackle real-world instance association problems. Project\npage:\n$\\href{https://caizhongang.github.io/projects/MessyTable/}{\\text{MessyTable}}$\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:57:13 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Cai", "Zhongang", ""], ["Zhang", "Junzhe", ""], ["Ren", "Daxuan", ""], ["Yu", "Cunjun", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Yeo", "Chai Kiat", ""], ["Loy", "Chen Change", ""]]}, {"id": "2007.14886", "submitter": "Martin Schuessler", "authors": "Milagros Miceli and Martin Schuessler and Tianling Yang", "title": "Between Subjectivity and Imposition: Power Dynamics in Data Annotation\n  for Computer Vision", "comments": "accepted for CSCW 2020, will be published in October 2020 issue of\n  PACM HCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of data is fundamental to machine learning. This paper\ninvestigates practices of image data annotation as performed in industrial\ncontexts. We define data annotation as a sense-making practice, where\nannotators assign meaning to data through the use of labels. Previous\nhuman-centered investigations have largely focused on annotators subjectivity\nas a major cause for biased labels. We propose a wider view on this issue:\nguided by constructivist grounded theory, we conducted several weeks of\nfieldwork at two annotation companies. We analyzed which structures, power\nrelations, and naturalized impositions shape the interpretation of data. Our\nresults show that the work of annotators is profoundly informed by the\ninterests, values, and priorities of other actors above their station.\nArbitrary classifications are vertically imposed on annotators, and through\nthem, on data. This imposition is largely naturalized. Assigning meaning to\ndata is often presented as a technical matter. This paper shows it is, in fact,\nan exercise of power with multiple implications for individuals and society.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:02:56 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 11:03:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Miceli", "Milagros", ""], ["Schuessler", "Martin", ""], ["Yang", "Tianling", ""]]}, {"id": "2007.14895", "submitter": "Muhammad E. H. Chowdhury", "authors": "Tawsifur Rahman, Amith Khandakar, Muhammad Abdul Kadir, Khandaker R.\n  Islam, Khandaker F. Islam, Rashid Mazhar, Tahir Hamid, Mohammad T. Islam,\n  Zaid B. Mahbub, Mohamed Arselene Ayari, Muhammad E. H. Chowdhury", "title": "Reliable Tuberculosis Detection using Chest X-ray with Deep Learning,\n  Segmentation and Visualization", "comments": "15 pages, 12 figure and 5 Tables", "journal-ref": "IEEE Access 2020", "doi": "10.1109/ACCESS.2020.3031384", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis (TB) is a chronic lung disease that occurs due to bacterial\ninfection and is one of the top 10 leading causes of death. Accurate and early\ndetection of TB is very important, otherwise, it could be life-threatening. In\nthis work, we have detected TB reliably from the chest X-ray images using image\npre-processing, data augmentation, image segmentation, and deep-learning\nclassification techniques. Several public databases were used to create a\ndatabase of 700 TB infected and 3500 normal chest X-ray images for this study.\nNine different deep CNNs (ResNet18, ResNet50, ResNet101, ChexNet, InceptionV3,\nVgg19, DenseNet201, SqueezeNet, and MobileNet), which were used for transfer\nlearning from their pre-trained initial weights and trained, validated and\ntested for classifying TB and non-TB normal cases. Three different experiments\nwere carried out in this work: segmentation of X-ray images using two different\nU-net models, classification using X-ray images, and segmented lung images. The\naccuracy, precision, sensitivity, F1-score, specificity in the detection of\ntuberculosis using X-ray images were 97.07 %, 97.34 %, 97.07 %, 97.14 % and\n97.36 % respectively. However, segmented lungs for the classification\noutperformed than whole X-ray image-based classification and accuracy,\nprecision, sensitivity, F1-score, specificity were 99.9 %, 99.91 %, 99.9 %,\n99.9 %, and 99.52 % respectively. The paper also used a visualization technique\nto confirm that CNN learns dominantly from the segmented lung regions results\nin higher detection accuracy. The proposed method with state-of-the-art\nperformance can be useful in the computer-aided faster diagnosis of\ntuberculosis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:11:34 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Rahman", "Tawsifur", ""], ["Khandakar", "Amith", ""], ["Kadir", "Muhammad Abdul", ""], ["Islam", "Khandaker R.", ""], ["Islam", "Khandaker F.", ""], ["Mazhar", "Rashid", ""], ["Hamid", "Tahir", ""], ["Islam", "Mohammad T.", ""], ["Mahbub", "Zaid B.", ""], ["Ayari", "Mohamed Arselene", ""], ["Chowdhury", "Muhammad E. H.", ""]]}, {"id": "2007.14902", "submitter": "Li Rui", "authors": "Rui Li, Jianlin Su, Chenxi Duan, Shunyi Zheng", "title": "Linear Attention Mechanism: An Efficient Attention for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, to remedy this deficiency, we propose a Linear Attention\nMechanism which is approximate to dot-product attention with much less memory\nand computational costs. The efficient design makes the incorporation between\nattention mechanisms and neural networks more flexible and versatile.\nExperiments conducted on semantic segmentation demonstrated the effectiveness\nof linear attention mechanism. Code is available at\nhttps://github.com/lironui/Linear-Attention-Mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:18:46 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 03:56:14 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 05:43:22 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Li", "Rui", ""], ["Su", "Jianlin", ""], ["Duan", "Chenxi", ""], ["Zheng", "Shunyi", ""]]}, {"id": "2007.14913", "submitter": "Prakhar Kulshreshtha", "authors": "Prakhar Kulshreshtha and Tanaya Guha", "title": "Dynamic Character Graph via Online Face Clustering for Movie Analysis", "comments": "accepted for publication in Multimedia Tools and Applications (MMTA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective approach to automated movie content analysis involves building a\nnetwork (graph) of its characters. Existing work usually builds a static\ncharacter graph to summarize the content using metadata, scripts or manual\nannotations. We propose an unsupervised approach to building a dynamic\ncharacter graph that captures the temporal evolution of character interaction.\nWe refer to this as the character interaction graph(CIG). Our approach has two\ncomponents:(i) an online face clustering algorithm that discovers the\ncharacters in the video stream as they appear, and (ii) simultaneous creation\nof a CIG using the temporal dynamics of the resulting clusters. We demonstrate\nthe usefulness of the CIG for two movie analysis tasks: narrative structure\n(acts) segmentation, and major character retrieval. Our evaluation on\nfull-length movies containing more than 5000 face tracks shows that the\nproposed approach achieves superior performance for both the tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:37:30 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kulshreshtha", "Prakhar", ""], ["Guha", "Tanaya", ""]]}, {"id": "2007.14918", "submitter": "Bin Yang", "authors": "Thomas Buhl Andersen, R\\'ogvi Eliasen, Mikkel Jarlund, Bin Yang", "title": "Force myography benchmark data for hand gesture recognition and transfer\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Force myography has recently gained increasing attention for hand gesture\nrecognition tasks. However, there is a lack of publicly available benchmark\ndata, with most existing studies collecting their own data often with custom\nhardware and for varying sets of gestures. This limits the ability to compare\nvarious algorithms, as well as the possibility for research to be done without\nfirst needing to collect data oneself. We contribute to the advancement of this\nfield by making accessible a benchmark dataset collected using a commercially\navailable sensor setup from 20 persons covering 18 unique gestures, in the hope\nof allowing further comparison of results as well as easier entry into this\nfield of research. We illustrate one use-case for such data, showing how we can\nimprove gesture recognition accuracy by utilising transfer learning to\nincorporate data from multiple other persons. This also illustrates that the\ndataset can serve as a benchmark dataset to facilitate research on transfer\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:43:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Andersen", "Thomas Buhl", ""], ["Eliasen", "R\u00f3gvi", ""], ["Jarlund", "Mikkel", ""], ["Yang", "Bin", ""]]}, {"id": "2007.14937", "submitter": "Jonathan Stroud", "authors": "Jonathan C. Stroud, David A. Ross, Chen Sun, Jia Deng, Rahul\n  Sukthankar, Cordelia Schmid", "title": "Learning Video Representations from Textual Web Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos found on the Internet are paired with pieces of text, such as titles\nand descriptions. This text typically describes the most important content in\nthe video, such as the objects in the scene and the actions being performed.\nBased on this observation, we propose to use such text as a method for learning\nvideo representations. To accomplish this, we propose a data collection process\nand use it to collect 70M video clips shared publicly on the Internet, and we\nthen train a model to pair each video with its associated text. We fine-tune\nthe model on several down-stream action recognition tasks, including Kinetics,\nHMDB-51, and UCF-101. We find that this approach is an effective method of\npretraining video representations. Specifically, it leads to improvements over\nfrom-scratch training on all benchmarks, outperforms many methods for\nself-supervised and webly-supervised video representation learning, and\nachieves an improvement of 2.2% accuracy on HMDB-51.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 16:19:50 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Stroud", "Jonathan C.", ""], ["Ross", "David A.", ""], ["Sun", "Chen", ""], ["Deng", "Jia", ""], ["Sukthankar", "Rahul", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2007.14943", "submitter": "Andrea De Maio", "authors": "Andrea De Maio and Simon Lacroix", "title": "Simultaneously Learning Corrections and Error Models for Geometry-based\n  Visual Odometry Methods", "comments": "Accepted in IEEE Robotics and Automation Letters and IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper fosters the idea that deep learning methods can be used to\ncomplement classical visual odometry pipelines to improve their accuracy and to\nassociate uncertainty models to their estimations. We show that the biases\ninherent to the visual odometry process can be faithfully learned and\ncompensated for, and that a learning architecture associated with a\nprobabilistic loss function can jointly estimate a full covariance matrix of\nthe residual errors, defining an error model capturing the heteroscedasticity\nof the process. Experiments on autonomous driving image sequences assess the\npossibility to concurrently improve visual odometry and estimate an error\nassociated with its outputs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 16:35:40 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["De Maio", "Andrea", ""], ["Lacroix", "Simon", ""]]}, {"id": "2007.14958", "submitter": "Dewi Yokelson", "authors": "Dewi Yokelson", "title": "Advancing Visual Specification of Code Requirements for Graphs", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in the humanities are among the many who are now exploring the\nworld of big data. They have begun to use programming languages like Python or\nR and their corresponding libraries to manipulate large data sets and discover\nbrand new insights. One of the major hurdles that still exists is incorporating\nvisualizations of this data into their projects. Visualization libraries can be\ndifficult to learn how to use, even for those with formal training. Yet these\nvisualizations are crucial for recognizing themes and communicating results to\nnot only other researchers, but also the general public. This paper focuses on\nproducing meaningful visualizations of data using machine learning. We allow\nthe user to visually specify their code requirements in order to lower the\nbarrier for humanities researchers to learn how to program visualizations. We\nuse a hybrid model, combining a neural network and optical character\nrecognition to generate the code to create the visualization.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:01:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yokelson", "Dewi", ""]]}, {"id": "2007.14979", "submitter": "Alan Wang", "authors": "Alan Q. Wang, Adrian V. Dalca, and Mert R. Sabuncu", "title": "Neural Network-based Reconstruction in Compressed Sensing MRI Without\n  Fully-sampled Training Data", "comments": "to be published in MLMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Sensing MRI (CS-MRI) has shown promise in reconstructing\nunder-sampled MR images, offering the potential to reduce scan times. Classical\ntechniques minimize a regularized least-squares cost function using an\nexpensive iterative optimization procedure. Recently, deep learning models have\nbeen developed that model the iterative nature of classical techniques by\nunrolling iterations in a neural network. While exhibiting superior\nperformance, these methods require large quantities of ground-truth images and\nhave shown to be non-robust to unseen data. In this paper, we explore a novel\nstrategy to train an unrolled reconstruction network in an unsupervised fashion\nby adopting a loss function widely-used in classical optimization schemes. We\ndemonstrate that this strategy achieves lower loss and is computationally cheap\ncompared to classical optimization solvers while also exhibiting superior\nrobustness compared to supervised models. Code is available at\nhttps://github.com/alanqrwang/HQSNet.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:46:55 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Alan Q.", ""], ["Dalca", "Adrian V.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2007.14994", "submitter": "Santiago Toledo-Cort\\'es", "authors": "Santiago Toledo-Cort\\'es, Melissa De La Pava, Oscar Perd\\'omo, and\n  Fabio A. Gonz\\'alez", "title": "Hybrid Deep Learning Gaussian Process for Diabetic Retinopathy Diagnosis\n  and Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is one of the microvascular complications of\nDiabetes Mellitus, which remains as one of the leading causes of blindness\nworldwide. Computational models based on Convolutional Neural Networks\nrepresent the state of the art for the automatic detection of DR using eye\nfundus images. Most of the current work address this problem as a binary\nclassification task. However, including the grade estimation and quantification\nof predictions uncertainty can potentially increase the robustness of the\nmodel. In this paper, a hybrid Deep Learning-Gaussian process method for DR\ndiagnosis and uncertainty quantification is presented. This method combines the\nrepresentational power of deep learning, with the ability to generalize from\nsmall datasets of Gaussian process models. The results show that uncertainty\nquantification in the predictions improves the interpretability of the method\nas a diagnostic support tool. The source code to replicate the experiments is\npublicly available at https://github.com/stoledoc/DLGP-DR-Diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 04:10:42 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Toledo-Cort\u00e9s", "Santiago", ""], ["De La Pava", "Melissa", ""], ["Perd\u00f3mo", "Oscar", ""], ["Gonz\u00e1lez", "Fabio A.", ""]]}, {"id": "2007.15036", "submitter": "Radek Mackowiak", "authors": "Radek Mackowiak, Lynton Ardizzone, Ullrich K\\\"othe, Carsten Rother", "title": "Generative Classifiers as a Basis for Trustworthy Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the maturing of deep learning systems, trustworthiness is becoming\nincreasingly important for model assessment. We understand trustworthiness as\nthe combination of explainability and robustness. Generative classifiers (GCs)\nare a promising class of models that are said to naturally accomplish these\nqualities. However, this has mostly been demonstrated on simple datasets such\nas MNIST and CIFAR in the past. In this work, we firstly develop an\narchitecture and training scheme that allows GCs to operate on a more relevant\nlevel of complexity for practical computer vision, namely the ImageNet\nchallenge. Secondly, we demonstrate the immense potential of GCs for\ntrustworthy image classification. Explainability and some aspects of robustness\nare vastly improved compared to feed-forward models, even when the GCs are just\napplied naively. While not all trustworthiness problems are solved completely,\nwe observe that GCs are a highly promising basis for further algorithms and\nmodifications. We release our trained model for download in the hope that it\nserves as a starting point for other generative classification tasks, in much\nthe same way as pretrained ResNet architectures do for discriminative\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:09:48 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 18:36:36 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mackowiak", "Radek", ""], ["Ardizzone", "Lynton", ""], ["K\u00f6the", "Ullrich", ""], ["Rother", "Carsten", ""]]}, {"id": "2007.15068", "submitter": "Liqian Ma", "authors": "Liqian Ma, Zhe Lin, Connelly Barnes, Alexei A. Efros, Jingwan Lu", "title": "Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ubiquity of smartphones, it is popular to take photos of one's\nself, or \"selfies.\" Such photos are convenient to take, because they do not\nrequire specialized equipment or a third-party photographer. However, in\nselfies, constraints such as human arm length often make the body pose look\nunnatural. To address this issue, we introduce $\\textit{unselfie}$, a novel\nphotographic transformation that automatically translates a selfie into a\nneutral-pose portrait. To achieve this, we first collect an unpaired dataset,\nand introduce a way to synthesize paired training data for self-supervised\nlearning. Then, to $\\textit{unselfie}$ a photo, we propose a new three-stage\npipeline, where we first find a target neutral pose, inpaint the body texture,\nand finally refine and composite the person on the background. To obtain a\nsuitable target neutral pose, we propose a novel nearest pose search module\nthat makes the reposing task easier and enables the generation of multiple\nneutral-pose results among which users can choose the best one they like.\nQualitative and quantitative evaluations show the superiority of our pipeline\nover alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 19:21:02 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ma", "Liqian", ""], ["Lin", "Zhe", ""], ["Barnes", "Connelly", ""], ["Efros", "Alexei A.", ""], ["Lu", "Jingwan", ""]]}, {"id": "2007.15103", "submitter": "Aneeshan Sain", "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, Yi-Zhe Song", "title": "Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image\n  Retrieval", "comments": "Accepted for ORAL presentation in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch as an image search query is an ideal alternative to text in capturing\nthe fine-grained visual details. Prior successes on fine-grained sketch-based\nimage retrieval (FG-SBIR) have demonstrated the importance of tackling the\nunique traits of sketches as opposed to photos, e.g., temporal vs. static,\nstrokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a\nfurther trait of sketches that has been overlooked to date, that is, they are\nhierarchical in terms of the levels of detail -- a person typically sketches up\nto various extents of detail to depict an object. This hierarchical structure\nis often visually distinct. In this paper, we design a novel network that is\ncapable of cultivating sketch-specific hierarchies and exploiting them to match\nsketch with photo at corresponding hierarchical levels. In particular, features\nfrom a sketch and a photo are enriched using cross-modal co-attention, coupled\nwith hierarchical node fusion at every level to form a better embedding space\nto conduct retrieval. Experiments on common benchmarks show our method to\noutperform state-of-the-arts by a significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:50:25 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 17:14:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Sain", "Aneeshan", ""], ["Bhunia", "Ayan Kumar", ""], ["Yang", "Yongxin", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2007.15107", "submitter": "Mo Shan", "authors": "Mo Shan, Vikas Dhiman, Qiaojun Feng, Jinzhao Li and Nikolay Atanasov", "title": "OrcVIO: Object residual constrained Visual-Inertial Odometry", "comments": "Submitted to T-RO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing object-level semantic information into simultaneous localization\nand mapping (SLAM) system is critical. It not only improves the performance but\nalso enables tasks specified in terms of meaningful objects. This work presents\nOrcVIO, for visual-inertial odometry tightly coupled with tracking and\noptimization over structured object models. OrcVIO differentiates through\nsemantic feature and bounding-box reprojection errors to perform batch\noptimization over the pose and shape of objects. The estimated object states\naid in real-time incremental optimization over the IMU-camera states. The\nability of OrcVIO for accurate trajectory estimation and large-scale\nobject-level mapping is evaluated using real data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:01:37 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 15:43:18 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 21:22:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shan", "Mo", ""], ["Dhiman", "Vikas", ""], ["Feng", "Qiaojun", ""], ["Li", "Jinzhao", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2007.15109", "submitter": "Pasquale Antonante", "authors": "Pasquale Antonante, Vasileios Tzoumas, Heng Yang, Luca Carlone", "title": "Outlier-Robust Estimation: Hardness, Minimally Tuned Algorithms, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear estimation in robotics and vision is typically plagued with\noutliers due to wrong data association, or to incorrect detections from signal\nprocessing and machine learning methods. This paper introduces two unifying\nformulations for outlier-robust estimation, Generalized Maximum Consensus\n(G-MC) and Generalized Truncated Least Squares (G-TLS), and investigates\nfundamental limits, practical algorithms, and applications. Our first\ncontribution is a proof that outlier-robust estimation is inapproximable: in\nthe worst case, it is impossible to (even approximately) find the set of\noutliers, even with slower-than-polynomial-time algorithms (particularly,\nalgorithms running in quasi-polynomial time). As a second contribution, we\nreview and extend two general-purpose algorithms. The first, Adaptive Trimming\n(ADAPT), is combinatorial, and is suitable for G-MC; the second, Graduated\nNon-Convexity (GNC), is based on homotopy methods, and is suitable for G-TLS.\nWe extend ADAPT and GNC to the case where the user does not have prior\nknowledge of the inlier-noise statistics (or the statistics may vary over time)\nand is unable to guess a reasonable threshold to separate inliers from outliers\n(as the one commonly used in RANSAC). We propose the first minimally tuned\nalgorithms for outlier rejection, that dynamically decide how to separate\ninliers from outliers. Our third contribution is an evaluation of the proposed\nalgorithms on robot perception problems: mesh registration, image-based object\ndetection (shape alignment), and pose graph optimization. ADAPT and GNC execute\nin real-time, are deterministic, outperform RANSAC, and are robust up to 80-90%\noutliers. Their minimally tuned versions also compare favorably with the state\nof the art, even though they do not rely on a noise bound for the inliers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:06:13 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 20:57:54 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 16:19:31 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Antonante", "Pasquale", ""], ["Tzoumas", "Vasileios", ""], ["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "2007.15122", "submitter": "You-Yi Jau", "authors": "You-Yi Jau, Rui Zhu, Hao Su, Manmohan Chandraker", "title": "Deep Keypoint-Based Camera Pose Estimation with Geometric Constraints", "comments": "8 pages, 5 figures, to appear at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating relative camera poses from consecutive frames is a fundamental\nproblem in visual odometry (VO) and simultaneous localization and mapping\n(SLAM), where classic methods consisting of hand-crafted features and\nsampling-based outlier rejection have been a dominant choice for over a decade.\nAlthough multiple works propose to replace these modules with learning-based\ncounterparts, most have not yet been as accurate, robust and generalizable as\nconventional methods. In this paper, we design an end-to-end trainable\nframework consisting of learnable modules for detection, feature extraction,\nmatching and outlier rejection, while directly optimizing for the geometric\npose objective. We show both quantitatively and qualitatively that pose\nestimation performance may be achieved on par with the classic pipeline.\nMoreover, we are able to show by end-to-end training, the key components of the\npipeline could be significantly improved, which leads to better\ngeneralizability to unseen datasets compared to existing learning-based\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:41:31 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Jau", "You-Yi", ""], ["Zhu", "Rui", ""], ["Su", "Hao", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.15124", "submitter": "Chen Jin", "authors": "Chen Jin, Ryutaro Tanno, Moucheng Xu, Thomy Mertzanidou, Daniel C.\n  Alexander", "title": "Foveation for Segmentation of Ultra-High Resolution Images", "comments": "22 pages, 15 figures, corrected metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of ultra-high resolution images is challenging because of their\nenormous size, consisting of millions or even billions of pixels. Typical\nsolutions include dividing input images into patches of fixed size and/or\ndown-sampling to meet memory constraints. Such operations incur information\nloss in the field-of-view (FoV) i.e., spatial coverage and the image\nresolution. The impact on segmentation performance is, however, as yet\nunderstudied. In this work, we start with a motivational experiment which\ndemonstrates that the trade-off between FoV and resolution affects the\nsegmentation performance on ultra-high resolution images---and furthermore, its\ninfluence also varies spatially according to the local patterns in different\nareas. We then introduce foveation module, a learnable \"dataloader\" which, for\na given ultra-high resolution image, adaptively chooses the appropriate\nconfiguration (FoV/resolution trade-off) of the input patch to feed to the\ndownstream segmentation model at each spatial location of the image. The\nfoveation module is jointly trained with the segmentation network to maximise\nthe task performance. We demonstrate on three publicly available\nhigh-resolution image datasets that the foveation module consistently improves\nsegmentation performance over the cases trained with patches of fixed\nFoV/resolution trade-off. Our approach achieves the SoTA performance on the\nDeepGlobe aerial image dataset. On the Gleason2019 histopathology dataset, our\nmodel achieves better segmentation accuracy for the two most clinically\nimportant and ambiguous classes (Gleason Grade 3 and 4) than the top performers\nin the challenge by 13.1% and 7.5%, and improves on the average performance of\n6 human experts by 6.5% and 7.5%. Our code and trained models are available at\n$\\text{https://github.com/lxasqjc/Foveation-Segmentation}$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:44:39 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 16:53:18 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Jin", "Chen", ""], ["Tanno", "Ryutaro", ""], ["Xu", "Moucheng", ""], ["Mertzanidou", "Thomy", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "2007.15131", "submitter": "Mou-Cheng Xu", "authors": "Mou-Cheng Xu and Neil P. Oxtoby and Daniel C. Alexander and Joseph\n  Jacob", "title": "Learning To Pay Attention To Mistakes", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In convolutional neural network based medical image segmentation, the\nperiphery of foreground regions representing malignant tissues may be\ndisproportionately assigned as belonging to the background class of healthy\ntissues\n\\cite{attenUnet}\\cite{AttenUnet2018}\\cite{InterSeg}\\cite{UnetFrontNeuro}\\cite{LearnActiveContour}.\nThis leads to high false negative detection rates. In this paper, we propose a\nnovel attention mechanism to directly address such high false negative rates,\ncalled Paying Attention to Mistakes. Our attention mechanism steers the models\ntowards false positive identification, which counters the existing bias towards\nfalse negatives. The proposed mechanism has two complementary implementations:\n(a) \"explicit\" steering of the model to attend to a larger Effective Receptive\nField on the foreground areas; (b) \"implicit\" steering towards false positives,\nby attending to a smaller Effective Receptive Field on the background areas. We\nvalidated our methods on three tasks: 1) binary dense prediction between\nvehicles and the background using CityScapes; 2) Enhanced Tumour Core\nsegmentation with multi-modal MRI scans in BRATS2018; 3) segmenting stroke\nlesions using ultrasound images in ISLES2018. We compared our methods with\nstate-of-the-art attention mechanisms in medical imaging, including\nself-attention, spatial-attention and spatial-channel mixed attention. Across\nall of the three different tasks, our models consistently outperform the\nbaseline models in Intersection over Union (IoU) and/or Hausdorff Distance\n(HD). For instance, in the second task, the \"explicit\" implementation of our\nmechanism reduces the HD of the best baseline by more than $26\\%$, whilst\nimproving the IoU by more than $3\\%$. We believe our proposed attention\nmechanism can benefit a wide range of medical and computer vision tasks, which\nsuffer from over-detection of background.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 22:01:28 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 10:12:12 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 10:37:49 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Xu", "Mou-Cheng", ""], ["Oxtoby", "Neil P.", ""], ["Alexander", "Daniel C.", ""], ["Jacob", "Joseph", ""]]}, {"id": "2007.15144", "submitter": "Scott Workman", "authors": "Scott Workman, M. Usman Rafique, Hunter Blanton, Connor Greenwell,\n  Nathan Jacobs", "title": "Single Image Cloud Detection via Multi-Image Fusion", "comments": "IEEE International Geoscience and Remote Sensing Symposium (IGARSS)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artifacts in imagery captured by remote sensing, such as clouds, snow, and\nshadows, present challenges for various tasks, including semantic segmentation\nand object detection. A primary challenge in developing algorithms for\nidentifying such artifacts is the cost of collecting annotated training data.\nIn this work, we explore how recent advances in multi-image fusion can be\nleveraged to bootstrap single image cloud detection. We demonstrate that a\nnetwork optimized to estimate image quality also implicitly learns to detect\nclouds. To support the training and evaluation of our approach, we collect a\nlarge dataset of Sentinel-2 images along with a per-pixel semantic labelling\nfor land cover. Through various experiments, we demonstrate that our method\nreduces the need for annotated training data and improves cloud detection\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 22:52:28 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Workman", "Scott", ""], ["Rafique", "M. Usman", ""], ["Blanton", "Hunter", ""], ["Greenwell", "Connor", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2007.15151", "submitter": "Wenhan Xia", "authors": "Wenhan Xia, Hongxu Yin, Xiaoliang Dai, Niraj K. Jha", "title": "Fully Dynamic Inference with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks are powerful and widely applicable models that\nextract task-relevant information through multi-level abstraction. Their\ncross-domain success, however, is often achieved at the expense of\ncomputational cost, high memory bandwidth, and long inference latency, which\nprevents their deployment in resource-constrained and time-sensitive scenarios,\nsuch as edge-side inference and self-driving cars. While recently developed\nmethods for creating efficient deep neural networks are making their real-world\ndeployment more feasible by reducing model size, they do not fully exploit\ninput properties on a per-instance basis to maximize computational efficiency\nand task accuracy. In particular, most existing methods typically use a\none-size-fits-all approach that identically processes all inputs. Motivated by\nthe fact that different images require different feature embeddings to be\naccurately classified, we propose a fully dynamic paradigm that imparts deep\nconvolutional neural networks with hierarchical inference dynamics at the level\nof layers and individual convolutional filters/channels. Two compact networks,\ncalled Layer-Net (L-Net) and Channel-Net (C-Net), predict on a per-instance\nbasis which layers or filters/channels are redundant and therefore should be\nskipped. L-Net and C-Net also learn how to scale retained computation outputs\nto maximize task accuracy. By integrating L-Net and C-Net into a joint design\nframework, called LC-Net, we consistently outperform state-of-the-art dynamic\nframeworks with respect to both efficiency and classification accuracy. On the\nCIFAR-10 dataset, LC-Net results in up to 11.9$\\times$ fewer floating-point\noperations (FLOPs) and up to 3.3% higher accuracy compared to other dynamic\ninference methods. On the ImageNet dataset, LC-Net achieves up to 1.4$\\times$\nfewer FLOPs and up to 4.6% higher Top-1 accuracy than the other methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 23:17:48 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Xia", "Wenhan", ""], ["Yin", "Hongxu", ""], ["Dai", "Xiaoliang", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2007.15156", "submitter": "Xingchen Zhang", "authors": "Xingchen Zhang", "title": "Benchmarking and Comparing Multi-exposure Image Fusion Algorithms", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-exposure image fusion (MEF) is an important area in computer vision and\nhas attracted increasing interests in recent years. Apart from conventional\nalgorithms, deep learning techniques have also been applied to multi-exposure\nimage fusion. However, although much efforts have been made on developing MEF\nalgorithms, the lack of benchmark makes it difficult to perform fair and\ncomprehensive performance comparison among MEF algorithms, thus significantly\nhindering the development of this field. In this paper, we fill this gap by\nproposing a benchmark for multi-exposure image fusion (MEFB) which consists of\na test set of 100 image pairs, a code library of 16 algorithms, 20 evaluation\nmetrics, 1600 fused images and a software toolkit. To the best of our\nknowledge, this is the first benchmark in the field of multi-exposure image\nfusion. Extensive experiments have been conducted using MEFB for comprehensive\nperformance evaluation and for identifying effective algorithms. We expect that\nMEFB will serve as an effective platform for researchers to compare\nperformances and investigate MEF algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 00:19:37 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zhang", "Xingchen", ""]]}, {"id": "2007.15157", "submitter": "Yu Xiang", "authors": "Yu Xiang, Christopher Xie, Arsalan Mousavian, Dieter Fox", "title": "Learning RGB-D Feature Embeddings for Unseen Object Instance\n  Segmentation", "comments": "CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting unseen objects in cluttered scenes is an important skill that\nrobots need to acquire in order to perform tasks in new environments. In this\nwork, we propose a new method for unseen object instance segmentation by\nlearning RGB-D feature embeddings from synthetic data. A metric learning loss\nfunction is utilized to learn to produce pixel-wise feature embeddings such\nthat pixels from the same object are close to each other and pixels from\ndifferent objects are separated in the embedding space. With the learned\nfeature embeddings, a mean shift clustering algorithm can be applied to\ndiscover and segment unseen objects. We further improve the segmentation\naccuracy with a new two-stage clustering algorithm. Our method demonstrates\nthat non-photorealistic synthetic RGB and depth images can be used to learn\nfeature embeddings that transfer well to real-world images for unseen object\ninstance segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 00:23:07 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 05:46:54 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 10:45:42 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Xiang", "Yu", ""], ["Xie", "Christopher", ""], ["Mousavian", "Arsalan", ""], ["Fox", "Dieter", ""]]}, {"id": "2007.15161", "submitter": "Nguyen Huu Phong", "authors": "Nguyen Huu Phong, Bernardete Ribeiro", "title": "Rethinking Recurrent Neural Networks and Other Improvements for Image\n  Classification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the long history of machine learning, which dates back several decades,\nrecurrent neural networks (RNNs) have been used mainly for sequential data and\ntime series and generally with 1D information. Even in some rare studies on 2D\nimages, these networks are used merely to learn and generate data sequentially\nrather than for image recognition tasks. In this study, we propose integrating\nan RNN as an additional layer when designing image recognition models. We also\ndevelop end-to-end multimodel ensembles that produce expert predictions using\nseveral models. In addition, we extend the training strategy so that our model\nperforms comparably to leading models and can even match the state-of-the-art\nmodels on several challenging datasets (e.g., SVHN (0.99), Cifar-100 (0.9027)\nand Cifar-10 (0.9852)). Moreover, our model sets a new record on the Surrey\ndataset (0.949). The source code of the methods provided in this article is\navailable at https://github.com/leonlha/e2e-3m and http://nguyenhuuphong.me.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 00:40:50 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:47:41 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 04:21:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Phong", "Nguyen Huu", ""], ["Ribeiro", "Bernardete", ""]]}, {"id": "2007.15167", "submitter": "Nguyen Huu Phong", "authors": "Nguyen Huu Phong, Bernardete Ribeiro", "title": "An Improvement for Capsule Networks using Depthwise Separable\n  Convolution", "comments": "6 pages", "journal-ref": "IbPRIA 2019: Pattern Recognition and Image Analysis", "doi": "10.1007/978-3-030-31332-6_45", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks face a critical problem in computer vision in the sense that\nthe image background can challenge its performance, although they learn very\nwell on training data. In this work, we propose to improve Capsule Networks'\narchitecture by replacing the Standard Convolution with a Depthwise Separable\nConvolution. This new design significantly reduces the model's total parameters\nwhile increases stability and offers competitive accuracy. In addition, the\nproposed model on $64\\times64$ pixel images outperforms standard models on\n$32\\times32$ and $64\\times64$ pixel images. Moreover, we empirically evaluate\nthese models with Deep Learning architectures using state-of-the-art Transfer\nLearning networks such as Inception V3 and MobileNet V1. The results show that\nCapsule Networks perform equivalently against Deep Learning models. To the best\nof our knowledge, we believe that this is the first work on the integration of\nDepthwise Separable Convolution into Capsule Networks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 00:58:34 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Phong", "Nguyen Huu", ""], ["Ribeiro", "Bernardete", ""]]}, {"id": "2007.15176", "submitter": "Sujoy Paul", "authors": "Sujoy Paul, Yi-Hsuan Tsai, Samuel Schulter, Amit K. Roy-Chowdhury,\n  Manmohan Chandraker", "title": "Domain Adaptive Semantic Segmentation Using Weak Labels", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning semantic segmentation models requires a huge amount of pixel-wise\nlabeling. However, labeled data may only be available abundantly in a domain\ndifferent from the desired target domain, which only has minimal or no\nannotations. In this work, we propose a novel framework for domain adaptation\nin semantic segmentation with image-level weak labels in the target domain. The\nweak labels may be obtained based on a model prediction for unsupervised domain\nadaptation (UDA), or from a human annotator in a new weakly-supervised domain\nadaptation (WDA) paradigm for semantic segmentation. Using weak labels is both\npractical and useful, since (i) collecting image-level target annotations is\ncomparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the\nopportunity for category-wise domain alignment. Our framework uses weak labels\nto enable the interplay between feature alignment and pseudo-labeling,\nimproving both in the process of domain adaptation. Specifically, we develop a\nweak-label classification module to enforce the network to attend to certain\ncategories, and then use such training signals to guide the proposed\ncategory-wise alignment method. In experiments, we show considerable\nimprovements with respect to the existing state-of-the-arts in UDA and present\na new benchmark in the WDA setting. Project page is at\nhttp://www.nec-labs.com/~mas/WeakSegDA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 01:33:57 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 10:05:48 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Paul", "Sujoy", ""], ["Tsai", "Yi-Hsuan", ""], ["Schulter", "Samuel", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2007.15194", "submitter": "Zhengqi Li", "authors": "Zhengqi Li, Wenqi Xian, Abe Davis, Noah Snavely", "title": "Crowdsampling the Plenoptic Function", "comments": "ECCV, 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular tourist landmarks are captured in a multitude of online, public\nphotos. These photos represent a sparse and unstructured sampling of the\nplenoptic function for a particular scene. In this paper,we present a new\napproach to novel view synthesis under time-varying illumination from such\ndata. Our approach builds on the recent multi-plane image (MPI) format for\nrepresenting local light fields under fixed viewing conditions. We introduce a\nnew DeepMPI representation, motivated by observations on the sparsity structure\nof the plenoptic function, that allows for real-time synthesis of\nphotorealistic views that are continuous in both space and across changes in\nlighting. Our method can synthesize the same compelling parallax and\nview-dependent effects as previous MPI methods, while simultaneously\ninterpolating along changes in reflectance and illumination with time. We show\nhow to learn a model of these effects in an unsupervised way from an\nunstructured collection of photos without temporal registration, demonstrating\nsignificant improvements over recent work in neural rendering. More information\ncan be found crowdsampling.io.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:52:10 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Li", "Zhengqi", ""], ["Xian", "Wenqi", ""], ["Davis", "Abe", ""], ["Snavely", "Noah", ""]]}, {"id": "2007.15217", "submitter": "Yuexi Zhang", "authors": "Yuexi Zhang, Yin Wang, Octavia Camps, Mario Sznaier", "title": "Key Frame Proposal Network for Efficient Pose Estimation in Videos", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation in video relies on local information by either\nestimating each frame independently or tracking poses across frames. In this\npaper, we propose a novel method combining local approaches with global\ncontext. We introduce a light weighted, unsupervised, key frame proposal\nnetwork (K-FPN) to select informative frames and a learned dictionary to\nrecover the entire pose sequence from these frames. The K-FPN speeds up the\npose estimation and provides robustness to bad frames with occlusion, motion\nblur, and illumination changes, while the learned dictionary provides global\ndynamic context. Experiments on Penn Action and sub-JHMDB datasets show that\nthe proposed method achieves state-of-the-art accuracy, with substantial\nspeed-up.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:06:40 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zhang", "Yuexi", ""], ["Wang", "Yin", ""], ["Camps", "Octavia", ""], ["Sznaier", "Mario", ""]]}, {"id": "2007.15235", "submitter": "Guillermo Arturo Mart\\'inez-Mascorro Mr.", "authors": "Guillermo A. Mart\\'inez-Mascorro, Jos\\'e C. Ortiz-Bayliss, Hugo\n  Terashima-Mar\\'in", "title": "Detecting Suspicious Behavior: How to Deal with Visual Similarity\n  through Neural Networks", "comments": null, "journal-ref": "2020 IEEE ANDESCON", "doi": "10.1109/ANDESCON50619.2020.9272175", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suspicious behavior is likely to threaten security, assets, life, or freedom.\nThis behavior has no particular pattern, which complicates the tasks to detect\nit and define it. Even for human observers, it is complex to spot suspicious\nbehavior in surveillance videos. Some proposals to tackle abnormal and\nsuspicious behavior-related problems are available in the literature. However,\nthey usually suffer from high false-positive rates due to different classes\nwith high visual similarity. The Pre-Crime Behavior method removes information\nrelated to a crime commission to focus on suspicious behavior before the crime\nhappens. The resulting samples from different types of crime have a high-visual\nsimilarity with normal-behavior samples. To address this problem, we\nimplemented 3D Convolutional Neural Networks and trained them under different\napproaches. Also, we tested different values in the number-of-filter parameter\nto optimize computational resources. Finally, the comparison between the\nperformance using different training approaches shows the best option to\nimprove the suspicious behavior detection on surveillance videos.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:13:52 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Mart\u00ednez-Mascorro", "Guillermo A.", ""], ["Ortiz-Bayliss", "Jos\u00e9 C.", ""], ["Terashima-Mar\u00edn", "Hugo", ""]]}, {"id": "2007.15237", "submitter": "Armin Aligholian", "authors": "Armin Aligholian, Alireza Shahsavari, Emma Stewart, Ed Cortez, Hamed\n  Mohsenian-Rad", "title": "Unsupervised Event Detection, Clustering, and Use Case Exposition in\n  Micro-PMU Measurements", "comments": "8 pages, 12 figures, R1 IEEE Trans. on SmartGrid", "journal-ref": null, "doi": "10.1109/TSG.2021.3063088", "report-no": null, "categories": "eess.SP cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution-level phasor measurement units, a.k.a, micro-PMUs, report a\nlarge volume of high resolution phasor measurements which constitute a variety\nof event signatures of different phenomena that occur all across power\ndistribution feeders. In order to implement an event-based analysis that has\nuseful applications for the utility operator, one needs to extract these events\nfrom a large volume of micro-PMU data. However, due to the infrequent,\nunscheduled, and unknown nature of the events, it is often a challenge to even\nfigure out what kind of events are out there to capture and scrutinize. In this\npaper, we seek to address this open problem by developing an unsupervised\napproach, which requires minimal prior human knowledge. First, we develop an\nunsupervised event detection method based on the concept of Generative\nAdversarial Networks (GAN). It works by training deep neural networks that\nlearn the characteristics of the normal trends in micro-PMU measurements; and\naccordingly detect an event when there is any abnormality. We also propose a\ntwo-step unsupervised clustering method, based on a novel linear mixed integer\nprogramming formulation. It helps us categorize events based on their origin in\nthe first step and their similarity in the second step. The active nature of\nthe proposed clustering method makes it capable of identifying new clusters of\nevents on an ongoing basis. The proposed unsupervised event detection and\nclustering methods are applied to real-world micro-PMU data. Results show that\nthey can outperform the prevalent methods in the literature. These methods also\nfacilitate our further analysis to identify important clusters of events that\nlead to unmasking several use cases that could be of value to the utility\noperator.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:20:29 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 21:23:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Aligholian", "Armin", ""], ["Shahsavari", "Alireza", ""], ["Stewart", "Emma", ""], ["Cortez", "Ed", ""], ["Mohsenian-Rad", "Hamed", ""]]}, {"id": "2007.15240", "submitter": "Chuan Guo", "authors": "Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng,\n  Minglun Gong and Li Cheng", "title": "Action2Motion: Conditioned Generation of 3D Human Motions", "comments": "13 pages, ACM MultiMedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413635", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is a relatively established task, where givenan input\nsequence of human motion, the goal is to predict its ac-tion category. This\npaper, on the other hand, considers a relativelynew problem, which could be\nthought of as an inverse of actionrecognition: given a prescribed action type,\nwe aim to generateplausible human motion sequences in 3D. Importantly, the set\nofgenerated motions are expected to maintain itsdiversityto be ableto explore\nthe entire action-conditioned motion space; meanwhile,each sampled sequence\nfaithfully resembles anaturalhuman bodyarticulation dynamics. Motivated by\nthese objectives, we followthe physics law of human kinematics by adopting the\nLie Algebratheory to represent thenaturalhuman motions; we also propose\natemporal Variational Auto-Encoder (VAE) that encourages adiversesampling of\nthe motion space. A new 3D human motion dataset, HumanAct12, is also\nconstructed. Empirical experiments overthree distinct human motion datasets\n(including ours) demonstratethe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:29:59 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Guo", "Chuan", ""], ["Zuo", "Xinxin", ""], ["Wang", "Sen", ""], ["Zou", "Shihao", ""], ["Sun", "Qingyao", ""], ["Deng", "Annan", ""], ["Gong", "Minglun", ""], ["Cheng", "Li", ""]]}, {"id": "2007.15243", "submitter": "Guohao Yu", "authors": "Guohao Yu, Alina Zare, Weihuang Xu, Roser Matamala, Joel\n  Reyes-Cabrera, Felix B. Fritschi, Thomas E. Juenger", "title": "Weakly Supervised Minirhizotron Image Segmentation with MIL-CAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multiple instance learning class activation map (MIL-CAM)\napproach for pixel-level minirhizotron image segmentation given weak\nimage-level labels. Minirhizotrons are used to image plant roots in situ.\nMinirhizotron imagery is often composed of soil containing a few long and thin\nroot objects of small diameter. The roots prove to be challenging for existing\nsemantic image segmentation methods to discriminate. In addition to learning\nfrom weak labels, our proposed MIL-CAM approach re-weights the root versus soil\npixels during analysis for improved performance due to the heavy imbalance\nbetween soil and root pixels. The proposed approach outperforms other attention\nmap and multiple instance learning methods for localization of root objects in\nminirhizotron imagery.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:49:30 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yu", "Guohao", ""], ["Zare", "Alina", ""], ["Xu", "Weihuang", ""], ["Matamala", "Roser", ""], ["Reyes-Cabrera", "Joel", ""], ["Fritschi", "Felix B.", ""], ["Juenger", "Thomas E.", ""]]}, {"id": "2007.15244", "submitter": "Mahdi Davoodikakhki", "authors": "Mahdi Davoodikakhki, KangKang Yin", "title": "Hierarchical Action Classification with Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on human action classification has made significant progresses in\nthe past few years. Most deep learning methods focus on improving performance\nby adding more network components. We propose, however, to better utilize\nauxiliary mechanisms, including hierarchical classification, network pruning,\nand skeleton-based preprocessing, to boost the model robustness and\nperformance. We test the effectiveness of our method on four commonly used\ntesting datasets: NTU RGB+D 60, NTU RGB+D 120, Northwestern-UCLA Multiview\nAction 3D, and UTD Multimodal Human Action Dataset. Our experiments show that\nour method can achieve either comparable or better performance on all four\ndatasets. In particular, our method sets up a new baseline for NTU 120, the\nlargest dataset among the four. We also analyze our method with extensive\ncomparisons and ablation studies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:49:42 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Davoodikakhki", "Mahdi", ""], ["Yin", "KangKang", ""]]}, {"id": "2007.15248", "submitter": "Nandan Kumar Jha", "authors": "Nandan Kumar Jha, Sparsh Mittal, Binod Kumar, and Govardhan Mattela", "title": "DeepPeep: Exploiting Design Ramifications to Decipher the Architecture\n  of Compact DNNs", "comments": "Accepted at The ACM Journal on Emerging Technologies in Computing\n  Systems (JETC), 2020. 25 pages, 11 tables, and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable predictive performance of deep neural networks (DNNs) has led\nto their adoption in service domains of unprecedented scale and scope. However,\nthe widespread adoption and growing commercialization of DNNs have underscored\nthe importance of intellectual property (IP) protection. Devising techniques to\nensure IP protection has become necessary due to the increasing trend of\noutsourcing the DNN computations on the untrusted accelerators in cloud-based\nservices. The design methodologies and hyper-parameters of DNNs are crucial\ninformation, and leaking them may cause massive economic loss to the\norganization. Furthermore, the knowledge of DNN's architecture can increase the\nsuccess probability of an adversarial attack where an adversary perturbs the\ninputs and alter the prediction.\n  In this work, we devise a two-stage attack methodology \"DeepPeep\" which\nexploits the distinctive characteristics of design methodologies to\nreverse-engineer the architecture of building blocks in compact DNNs. We show\nthe efficacy of \"DeepPeep\" on P100 and P4000 GPUs. Additionally, we propose\nintelligent design maneuvering strategies for thwarting IP theft through the\nDeepPeep attack and proposed \"Secure MobileNet-V1\". Interestingly, compared to\nvanilla MobileNet-V1, secure MobileNet-V1 provides a significant reduction in\ninference latency ($\\approx$60%) and improvement in predictive performance\n($\\approx$2%) with very-low memory and computation overheads.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:01:41 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Jha", "Nandan Kumar", ""], ["Mittal", "Sparsh", ""], ["Kumar", "Binod", ""], ["Mattela", "Govardhan", ""]]}, {"id": "2007.15255", "submitter": "Terrance DeVries", "authors": "Terrance DeVries, Michal Drozdzal and Graham W. Taylor", "title": "Instance Selection for GANs", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have led to their\nwidespread adoption for the purposes of generating high quality synthetic\nimagery. While capable of generating photo-realistic images, these models often\nproduce unrealistic samples which fall outside of the data manifold. Several\nrecently proposed techniques attempt to avoid spurious samples, either by\nrejecting them after generation, or by truncating the model's latent space.\nWhile effective, these methods are inefficient, as a large fraction of training\ntime and model capacity are dedicated towards samples that will ultimately go\nunused. In this work we propose a novel approach to improve sample quality:\naltering the training dataset via instance selection before model training has\ntaken place. By refining the empirical data distribution before training, we\nredirect model capacity towards high-density regions, which ultimately improves\nsample fidelity, lowers model capacity requirements, and significantly reduces\ntraining time. Code is available at\nhttps://github.com/uoguelph-mlrg/instance_selection_for_gans.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:33:51 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 04:43:07 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["DeVries", "Terrance", ""], ["Drozdzal", "Michal", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2007.15258", "submitter": "Kazuya Nishimura", "authors": "Kazuya Nishimura, Junya Hayashida, Chenyang Wang, Dai Fei Elmer Ker,\n  Ryoma Bise", "title": "Weakly-Supervised Cell Tracking via Backward-and-Forward Propagation", "comments": "17 pages, figures, Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weakly-supervised cell tracking method that can train a\nconvolutional neural network (CNN) by using only the annotation of \"cell\ndetection\" (i.e., the coordinates of cell positions) without association\ninformation, in which cell positions can be easily obtained by nuclear\nstaining. First, we train co-detection CNN that detects cells in successive\nframes by using weak-labels. Our key assumption is that co-detection CNN\nimplicitly learns association in addition to detection. To obtain the\nassociation, we propose a backward-and-forward propagation method that analyzes\nthe correspondence of cell positions in the outputs of co-detection CNN.\nExperiments demonstrated that the proposed method can associate cells by\nanalyzing co-detection CNN. Even though the method uses only weak supervision,\nthe performance of our method was almost the same as the state-of-the-art\nsupervised method. Code is publicly available in\nhttps://github.com/naivete5656/WSCTBFP\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:41:22 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Nishimura", "Kazuya", ""], ["Hayashida", "Junya", ""], ["Wang", "Chenyang", ""], ["Ker", "Dai Fei Elmer", ""], ["Bise", "Ryoma", ""]]}, {"id": "2007.15269", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yang Cheng, Shaoyuan Xu, Qian Lin, Jan Allebach", "title": "The Blessing and the Curse of the Noise behind Facial Landmark\n  Annotations", "comments": "10 pages, 10 figures, accepted to Electronic Imaging Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolving algorithms for 2D facial landmark detection empower people to\nrecognize faces, analyze facial expressions, etc. However, existing methods\nstill encounter problems of unstable facial landmarks when applied to videos.\nBecause previous research shows that the instability of facial landmarks is\ncaused by the inconsistency of labeling quality among the public datasets, we\nwant to have a better understanding of the influence of annotation noise in\nthem. In this paper, we make the following contributions: 1) we propose two\nmetrics that quantitatively measure the stability of detected facial landmarks,\n2) we model the annotation noise in an existing public dataset, 3) we\ninvestigate the influence of different types of noise in training face\nalignment neural networks, and propose corresponding solutions. Our results\ndemonstrate improvements in both accuracy and stability of detected facial\nlandmarks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:13:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Cheng", "Yang", ""], ["Xu", "Shaoyuan", ""], ["Lin", "Qian", ""], ["Allebach", "Jan", ""]]}, {"id": "2007.15271", "submitter": "Cecilia Pasquini", "authors": "Mattia Bonomi and Cecilia Pasquini and Giulia Boato", "title": "Dynamic texture analysis for detecting fake faces in video sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation of manipulated multimedia content involving human characters has\nreached in the last years unprecedented realism, calling for automated\ntechniques to expose synthetically generated faces in images and videos. This\nwork explores the analysis of spatio-temporal texture dynamics of the video\nsignal, with the goal of characterizing and distinguishing real and fake\nsequences. We propose to build a binary decision on the joint analysis of\nmultiple temporal segments and, in contrast to previous approaches, to exploit\nthe textural dynamics of both the spatial and temporal dimensions. This is\nachieved through the use of Local Derivative Patterns on Three Orthogonal\nPlanes (LDP-TOP), a compact feature representation known to be an important\nasset for the detection of face spoofing attacks. Experimental analyses on\nstate-of-the-art datasets of manipulated videos show the discriminative power\nof such descriptors in separating real and fake sequences, and also identifying\nthe creation method used. Linear Support Vector Machines (SVMs) are used which,\ndespite the lower complexity, yield comparable performance to previously\nproposed deep models for fake content detection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:21:24 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bonomi", "Mattia", ""], ["Pasquini", "Cecilia", ""], ["Boato", "Giulia", ""]]}, {"id": "2007.15273", "submitter": "Xin Yang", "authors": "Yuhao Huang, Xin Yang, Rui Li, Jikuan Qian, Xiaoqiong Huang, Wenlong\n  Shi, Haoran Dou, Chaoyu Chen, Yuanji Zhang, Huanjia Luo, Alejandro Frangi, Yi\n  Xiong, Dong Ni", "title": "Searching Collaborative Agents for Multi-plane Localization in 3D\n  Ultrasound", "comments": "Early accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D ultrasound (US) is widely used due to its rich diagnostic information,\nportability and low cost. Automated standard plane (SP) localization in US\nvolume not only improves efficiency and reduces user-dependence, but also\nboosts 3D US interpretation. In this study, we propose a novel Multi-Agent\nReinforcement Learning (MARL) framework to localize multiple uterine SPs in 3D\nUS simultaneously. Our contribution is two-fold. First, we equip the MARL with\na one-shot neural architecture search (NAS) module to obtain the optimal agent\nfor each plane. Specifically, Gradient-based search using Differentiable\nArchitecture Sampler (GDAS) is employed to accelerate and stabilize the\ntraining process. Second, we propose a novel collaborative strategy to\nstrengthen agents' communication. Our strategy uses recurrent neural network\n(RNN) to learn the spatial relationship among SPs effectively. Extensively\nvalidated on a large dataset, our approach achieves the accuracy of 7.05\ndegree/2.21mm, 8.62 degree/2.36mm and 5.93 degree/0.89mm for the mid-sagittal,\ntransverse and coronal plane localization, respectively. The proposed MARL\nframework can significantly increase the plane localization accuracy and reduce\nthe computational cost and model size.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:23:55 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Huang", "Yuhao", ""], ["Yang", "Xin", ""], ["Li", "Rui", ""], ["Qian", "Jikuan", ""], ["Huang", "Xiaoqiong", ""], ["Shi", "Wenlong", ""], ["Dou", "Haoran", ""], ["Chen", "Chaoyu", ""], ["Zhang", "Yuanji", ""], ["Luo", "Huanjia", ""], ["Frangi", "Alejandro", ""], ["Xiong", "Yi", ""], ["Ni", "Dong", ""]]}, {"id": "2007.15310", "submitter": "Junyu Lin", "authors": "Junyu Lin, Lei Xu, Yingqi Liu, Xiangyu Zhang", "title": "Black-box Adversarial Sample Generation Based on Differential Evolution", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are being used in various daily tasks such as\nobject detection, speech processing, and machine translation. However, it is\nknown that DNNs suffer from robustness problems -- perturbed inputs called\nadversarial samples leading to misbehaviors of DNNs. In this paper, we propose\na black-box technique called Black-box Momentum Iterative Fast Gradient Sign\nMethod (BMI-FGSM) to test the robustness of DNN models. The technique does not\nrequire any knowledge of the structure or weights of the target DNN. Compared\nto existing white-box testing techniques that require accessing model internal\ninformation such as gradients, our technique approximates gradients through\nDifferential Evolution and uses approximated gradients to construct adversarial\nsamples. Experimental results show that our technique can achieve 100% success\nin generating adversarial samples to trigger misclassification, and over 95%\nsuccess in generating samples to trigger misclassification to a specific target\noutput label. It also demonstrates better perturbation distance and better\ntransferability. Compared to the state-of-the-art black-box technique, our\ntechnique is more efficient. Furthermore, we conduct testing on the commercial\nAliyun API and successfully trigger its misbehavior within a limited number of\nqueries, demonstrating the feasibility of real-world black-box attack.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:43:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Lin", "Junyu", ""], ["Xu", "Lei", ""], ["Liu", "Yingqi", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2007.15330", "submitter": "Yukai Lin", "authors": "Yukai Lin, Viktor Larsson, Marcel Geppert, Zuzana Kukelova, Marc\n  Pollefeys and Torsten Sattler", "title": "Infrastructure-based Multi-Camera Calibration using Radial Projections", "comments": "ECCV 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58517-4_20", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-camera systems are an important sensor platform for intelligent systems\nsuch as self-driving cars. Pattern-based calibration techniques can be used to\ncalibrate the intrinsics of the cameras individually. However, extrinsic\ncalibration of systems with little to no visual overlap between the cameras is\na challenge. Given the camera intrinsics, infrastucture-based calibration\ntechniques are able to estimate the extrinsics using 3D maps pre-built via SLAM\nor Structure-from-Motion. In this paper, we propose to fully calibrate a\nmulti-camera system from scratch using an infrastructure-based approach.\nAssuming that the distortion is mainly radial, we introduce a two-stage\napproach. We first estimate the camera-rig extrinsics up to a single unknown\ntranslation component per camera. Next, we solve for both the intrinsic\nparameters and the missing translation components. Extensive experiments on\nmultiple indoor and outdoor scenes with multiple multi-camera systems show that\nour calibration method achieves high accuracy and robustness. In particular,\nour approach is more robust than the naive approach of first estimating\nintrinsic parameters and pose per camera before refining the extrinsic\nparameters of the system. The implementation is available at\nhttps://github.com/youkely/InfrasCal.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:21:04 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 14:23:51 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lin", "Yukai", ""], ["Larsson", "Viktor", ""], ["Geppert", "Marcel", ""], ["Kukelova", "Zuzana", ""], ["Pollefeys", "Marc", ""], ["Sattler", "Torsten", ""]]}, {"id": "2007.15340", "submitter": "Lizhen Wang", "authors": "Lizhen Wang, Xiaochen Zhao, Tao Yu, Songtao Wang, Yebin Liu", "title": "NormalGAN: Learning Detailed 3D Human from a Single RGB-D Image", "comments": "10 pages, 11 figures, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose NormalGAN, a fast adversarial learning-based method to reconstruct\nthe complete and detailed 3D human from a single RGB-D image. Given a single\nfront-view RGB-D image, NormalGAN performs two steps: front-view RGB-D\nrectification and back-view RGBD inference. The final model was then generated\nby simply combining the front-view and back-view RGB-D information. However,\ninferring backview RGB-D image with high-quality geometric details and\nplausible texture is not trivial. Our key observation is: Normal maps generally\nencode much more information of 3D surface details than RGB and depth images.\nTherefore, learning geometric details from normal maps is superior than other\nrepresentations. In NormalGAN, an adversarial learning framework conditioned by\nnormal maps is introduced, which is used to not only improve the front-view\ndepth denoising performance, but also infer the back-view depth image with\nsurprisingly geometric details. Moreover, for texture recovery, we remove\nshading information from the front-view RGB image based on the refined normal\nmap, which further improves the quality of the back-view color inference.\nResults and experiments on both testing data set and real captured data\ndemonstrate the superior performance of our approach. Given a consumer RGB-D\nsensor, NormalGAN can generate the complete and detailed 3D human\nreconstruction results in 20 fps, which further enables convenient interactive\nexperiences in telepresence, AR/VR and gaming scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:35:46 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Wang", "Lizhen", ""], ["Zhao", "Xiaochen", ""], ["Yu", "Tao", ""], ["Wang", "Songtao", ""], ["Liu", "Yebin", ""]]}, {"id": "2007.15381", "submitter": "Koki Takeshita", "authors": "Koki Takeshita, Juntaro Shioyama and Seiichi Uchida", "title": "Label or Message: A Large-Scale Experimental Survey of Texts and Objects\n  Co-Occurrence", "comments": "Accepted at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our daily life is surrounded by textual information. Nowadays, the automatic\ncollection of textual information becomes possible owing to the drastic\nimprovement of scene text detectors and recognizer. The purpose of this paper\nis to conduct a large-scale survey of co-occurrence between visual objects\n(such as book and car) and scene texts with a large image dataset and a\nstate-of-the-art scene text detector and recognizer. Especially, we focus on\nthe function of \"label\" texts, which are attached to objects for detailing the\nobjects. By analyzing co-occurrence between objects and scene texts, it is\npossible to observe the statistics about the label texts and understand how the\nscene texts will be useful for recognizing the objects and vice versa.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 11:18:10 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Takeshita", "Koki", ""], ["Shioyama", "Juntaro", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2007.15417", "submitter": "Eleni Charou Dr", "authors": "Antigoni Panagiotopoulou, Lazaros Grammatikopoulos, Eleni Charou,\n  Emmanuel Bratsolis, Nicholas Madamopoulos and John Petrogonas", "title": "Very Deep Super-Resolution of Remotely Sensed Images with Mean Square\n  Error and Var-norm Estimators as Loss Functions", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, very deep super-resolution (VDSR) method is presented for\nimproving the spatial resolution of remotely sensed (RS) images for scale\nfactor 4. The VDSR net is re-trained with Sentinel-2 images and with drone aero\northophoto images, thus becomes RS-VDSR and Aero-VDSR, respectively. A novel\nloss function, the Var-norm estimator, is proposed in the regression layer of\nthe convolutional neural network during re-training and prediction. According\nto numerical and optical comparisons, the proposed nets RS-VDSR and Aero-VDSR\ncan outperform VDSR during prediction with RS images. RS-VDSR outperforms VDSR\nup to 3.16 dB in terms of PSNR in Sentinel-2 images.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:26:38 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Panagiotopoulou", "Antigoni", ""], ["Grammatikopoulos", "Lazaros", ""], ["Charou", "Eleni", ""], ["Bratsolis", "Emmanuel", ""], ["Madamopoulos", "Nicholas", ""], ["Petrogonas", "John", ""]]}, {"id": "2007.15422", "submitter": "Min Xu", "authors": "Liangyong Yu, Ran Li, Xiangrui Zeng, Hongyi Wang, Jie Jin, Ge Yang,\n  Rui Jiang, Min Xu", "title": "Few shot domain adaptation for in situ macromolecule structural\n  classification in cryo-electron tomograms", "comments": "This article has been accepted for publication in Bioinformatics\n  Published by Oxford University Press", "journal-ref": "Bioinformatics 2020", "doi": "10.1093/bioinformatics/btaa671", "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Cryo-Electron Tomography (cryo-ET) visualizes structure and\nspatial organization of macromolecules and their interactions with other\nsubcellular components inside single cells in the close-to-native state at\nsub-molecular resolution. Such information is critical for the accurate\nunderstanding of cellular processes. However, subtomogram classification\nremains one of the major challenges for the systematic recognition and recovery\nof the macromolecule structures in cryo-ET because of imaging limits and data\nquantity. Recently, deep learning has significantly improved the throughput and\naccuracy of large-scale subtomogram classification. However often it is\ndifficult to get enough high-quality annotated subtomogram data for supervised\ntraining due to the enormous expense of labeling. To tackle this problem, it is\nbeneficial to utilize another already annotated dataset to assist the training\nprocess. However, due to the discrepancy of image intensity distribution\nbetween source domain and target domain, the model trained on subtomograms in\nsource domainmay perform poorly in predicting subtomogram classes in the target\ndomain.\n  Results: In this paper, we adapt a few shot domain adaptation method for deep\nlearning based cross-domain subtomogram classification. The essential idea of\nour method consists of two parts: 1) take full advantage of the distribution of\nplentiful unlabeled target domain data, and 2) exploit the correlation between\nthe whole source domain dataset and few labeled target domain data. Experiments\nconducted on simulated and real datasets show that our method achieves\nsignificant improvement on cross domain subtomogram classification compared\nwith baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:39:21 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yu", "Liangyong", ""], ["Li", "Ran", ""], ["Zeng", "Xiangrui", ""], ["Wang", "Hongyi", ""], ["Jin", "Jie", ""], ["Yang", "Ge", ""], ["Jiang", "Rui", ""], ["Xu", "Min", ""]]}, {"id": "2007.15429", "submitter": "Ho Yin Sze-To", "authors": "Antonio Sze-To, Hamid Tizhoosh", "title": "Searching for Pneumothorax in Half a Million Chest X-Ray Images", "comments": "AIME 2020 International Conference on AI in Medicine, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumothorax, a collapsed or dropped lung, is a fatal condition typically\ndetected on a chest X-ray by an experienced radiologist. Due to shortage of\nsuch experts, automated detection systems based on deep neural networks have\nbeen developed. Nevertheless, applying such systems in practice remains a\nchallenge. These systems, mostly compute a single probability as output, may\nnot be enough for diagnosis. On the contrary, content-based medical image\nretrieval (CBIR) systems, such as image search, can assist clinicians for\ndiagnostic purposes by enabling them to compare the case they are examining\nwith previous (already diagnosed) cases. However, there is a lack of study on\nsuch attempt. In this study, we explored the use of image search to classify\npneumothorax among chest X-ray images. All chest X-ray images were first tagged\nwith deep pretrained features, which were obtained from existing deep learning\nmodels. Given a query chest X-ray image, the majority voting of the top K\nretrieved images was then used as a classifier, in which similar cases in the\narchive of past cases are provided besides the probability output. In our\nexperiments, 551,383 chest X-ray images were obtained from three large recently\nreleased public datasets. Using 10-fold cross-validation, it is shown that\nimage search on deep pretrained features achieved promising results compared to\nthose obtained by traditional classifiers trained on the same features. To the\nbest of knowledge, it is the first study to demonstrate that deep pretrained\nfeatures can be used for CBIR of pneumothorax in half a million chest X-ray\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 13:03:52 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Sze-To", "Antonio", ""], ["Tizhoosh", "Hamid", ""]]}, {"id": "2007.15444", "submitter": "Ali Hamdi", "authors": "Ali Hamdi, Du Yong Kim, Flora D. Salim", "title": "flexgrid2vec: Learning Efficient Visual Representations Vectors", "comments": "28 pages, Submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose flexgrid2vec, a novel approach for image representation learning.\nExisting visual representation methods suffer from several issues, including\nthe need for highly intensive computation, the risk of losing in-depth\nstructural information, and the specificity of the method to certain shapes or\nobjects. flexgrid2vec converts an image to a low-dimensional feature vector. We\nrepresent each image with a graph of flexible, unique node locations and edge\ndistances. flexgrid2vec is a multi-channel Graph Convolutional Network (GCN)\nthat learns features of the most representative image patches. We have\ninvestigated both spectral and non-spectral implementations of the GCN\nnode-embedding. Specifically, we have implemented flexgrid2vec based on\ndifferent node-aggregation methods such as vector summation, concatenation, and\nnormalisation with eigenvector centrality. We compare the performance of\nflexgrid2vec with a set of state-of-the-art visual representation learning\nmodels on binary and multi-class image classification tasks. Although we\nutilise imbalanced, low-size and low-resolution datasets, flexgrid2vec shows\nstable and outstanding results against well-known base classifiers.\nflexgrid2vec achieves 96.23% on CIFAR-10, 83.05% on CIFAR-100, 94.50% on\nSTL-10, 98.8% on ASIRRA, and 89.69% on COCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 13:21:00 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 21:01:52 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 11:02:19 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 20:41:36 GMT"}, {"version": "v5", "created": "Mon, 26 Apr 2021 12:53:05 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hamdi", "Ali", ""], ["Kim", "Du Yong", ""], ["Salim", "Flora D.", ""]]}, {"id": "2007.15484", "submitter": "Nihar Shrikant Bendre", "authors": "Nihar Bendre, Hugo Terashima Mar\\'in, and Peyman Najafirad", "title": "Learning from Few Samples: A Survey", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been able to outperform humans in some cases like\nimage recognition and image classification. However, with the emergence of\nvarious novel categories, the ability to continuously widen the learning\ncapability of such networks from limited samples, still remains a challenge.\nTechniques like Meta-Learning and/or few-shot learning showed promising\nresults, where they can learn or generalize to a novel category/task based on\nprior knowledge. In this paper, we perform a study of the existing few-shot\nmeta-learning techniques in the computer vision domain based on their method\nand evaluation metrics. We provide a taxonomy for the techniques and categorize\nthem as data-augmentation, embedding, optimization and semantics based learning\nfor few-shot, one-shot and zero-shot settings. We then describe the seminal\nwork done in each category and discuss their approach towards solving the\npredicament of learning from few samples. Lastly we provide a comparison of\nthese techniques on the commonly used benchmark datasets: Omniglot, and\nMiniImagenet, along with a discussion towards the future direction of improving\nthe performance of these techniques towards the final goal of outperforming\nhumans.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:28:57 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bendre", "Nihar", ""], ["Mar\u00edn", "Hugo Terashima", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2007.15486", "submitter": "Wei Zeng", "authors": "Wei Zeng, Chengqiao Lin, Juncong Lin, Jincheng Jiang, Jiazhi Xia,\n  Cagatay Turkay, Wei Chen", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction\n  with Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are being increasingly used for urban traffic\nprediction where spatiotemporal traffic data is aggregated into sequentially\norganized matrices that are then fed into convolution-based residual neural\nnetworks. However, the widely known modifiable areal unit problem within such\naggregation processes can lead to perturbations in the network inputs. This\nissue can significantly destabilize the feature embeddings and the predictions,\nrendering deep networks much less useful for the experts. This paper approaches\nthis challenge by leveraging unit visualization techniques that enable the\ninvestigation of many-to-many relationships between dynamically varied\nmulti-scalar aggregations of urban traffic data and neural network predictions.\nThrough regular exchanges with a domain expert, we design and develop a visual\nanalytics solution that integrates 1) a Bivariate Map equipped with an advanced\nbivariate colormap to simultaneously depict input traffic and prediction errors\nacross space, 2) a Morans I Scatterplot that provides local indicators of\nspatial association analysis, and 3) a Multi-scale Attribution View that\narranges non-linear dot plots in a tree layout to promote model analysis and\ncomparison across scales. We evaluate our approach through a series of case\nstudies involving a real-world dataset of Shenzhen taxi trips, and through\ninterviews with domain experts. We observe that geographical scale variations\nhave important impact on prediction performances, and interactive visual\nexploration of dynamically varying inputs and outputs benefit experts in the\ndevelopment of deep traffic prediction models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:32:17 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 08:18:04 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 14:20:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zeng", "Wei", ""], ["Lin", "Chengqiao", ""], ["Lin", "Juncong", ""], ["Jiang", "Jincheng", ""], ["Xia", "Jiazhi", ""], ["Turkay", "Cagatay", ""], ["Chen", "Wei", ""]]}, {"id": "2007.15488", "submitter": "Le Hui", "authors": "Mingmei Cheng, Le Hui, Jin Xie, Jian Yang and Hui Kong", "title": "Cascaded Non-local Neural Network for Point Cloud Semantic Segmentation", "comments": "Accepted by IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2020 (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a cascaded non-local neural network for point cloud\nsegmentation. The proposed network aims to build the long-range dependencies of\npoint clouds for the accurate segmentation. Specifically, we develop a novel\ncascaded non-local module, which consists of the neighborhood-level,\nsuperpoint-level and global-level non-local blocks. First, in the\nneighborhood-level block, we extract the local features of the centroid points\nof point clouds by assigning different weights to the neighboring points. The\nextracted local features of the centroid points are then used to encode the\nsuperpoint-level block with the non-local operation. Finally, the global-level\nblock aggregates the non-local features of the superpoints for semantic\nsegmentation in an encoder-decoder framework. Benefiting from the cascaded\nstructure, geometric structure information of different neighborhoods with the\nsame label can be propagated. In addition, the cascaded structure can largely\nreduce the computational cost of the original non-local operation on point\nclouds. Experiments on different indoor and outdoor datasets show that our\nmethod achieves state-of-the-art performance and effectively reduces the time\nconsumption and memory occupation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:34:43 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Cheng", "Mingmei", ""], ["Hui", "Le", ""], ["Xie", "Jin", ""], ["Yang", "Jian", ""], ["Kong", "Hui", ""]]}, {"id": "2007.15506", "submitter": "Tyler Lixuan Zhu", "authors": "Tyler Zhu, Per Karlsson, Christoph Bregler", "title": "SimPose: Effectively Learning DensePose and Surface Normals of People\n  from Simulated Data", "comments": "To appear in the Proceedings of ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a proliferation of generic domain-adaptation approaches, we report a\nsimple yet effective technique for learning difficult per-pixel 2.5D and 3D\nregression representations of articulated people. We obtained strong\nsim-to-real domain generalization for the 2.5D DensePose estimation task and\nthe 3D human surface normal estimation task. On the multi-person DensePose\nMSCOCO benchmark, our approach outperforms the state-of-the-art methods which\nare trained on real images that are densely labelled. This is an important\nresult since obtaining human manifold's intrinsic uv coordinates on real images\nis time consuming and prone to labeling noise. Additionally, we present our\nmodel's 3D surface normal predictions on the MSCOCO dataset that lacks any real\n3D surface normal labels. The key to our approach is to mitigate the\n\"Inter-domain Covariate Shift\" with a carefully selected training batch from a\nmixture of domain samples, a deep batch-normalized residual network, and a\nmodified multi-task learning objective. Our approach is complementary to\nexisting domain-adaptation techniques and can be applied to other dense\nper-pixel pose estimation problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:59:38 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zhu", "Tyler", ""], ["Karlsson", "Per", ""], ["Bregler", "Christoph", ""]]}, {"id": "2007.15523", "submitter": "Morteza Babaie", "authors": "Morteza Babaie, Hany Kashani, Meghana D. Kumar, Hamid.R. Tizhoosh", "title": "A new Local Radon Descriptor for Content-Based Image Search", "comments": "{To appear in International Conference on AI in Medicine (AIME 2020),\n  University of Minnesota, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) is an essential part of computer vision\nresearch, especially in medical expert systems. Having a discriminative image\ndescriptor with the least number of parameters for tuning is desirable in CBIR\nsystems. In this paper, we introduce a new simple descriptor based on the\nhistogram of local Radon projections. We also propose a very fast\nconvolution-based local Radon estimator to overcome the slow process of Radon\nprojections. We performed our experiments using pathology images (KimiaPath24)\nand lung CT patches and test our proposed solution for medical image\nprocessing. We achieved superior results compared with other histogram-based\ndescriptors such as LBP and HoG as well as some pre-trained CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:22:57 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Babaie", "Morteza", ""], ["Kashani", "Hany", ""], ["Kumar", "Meghana D.", ""], ["Tizhoosh", "Hamid. R.", ""]]}, {"id": "2007.15540", "submitter": "Kento Doi", "authors": "Kento Doi, Ryuhei Hamaguchi, Shun Iwase, Rio Yokota, Yutaka Matsuo,\n  Ken Sakurada", "title": "Epipolar-Guided Deep Object Matching for Scene Change Detection", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a viewpoint-robust object-based change detection network\n(OBJ-CDNet). Mobile cameras such as drive recorders capture images from\ndifferent viewpoints each time due to differences in camera trajectory and\nshutter timing. However, previous methods for pixel-wise change detection are\nvulnerable to the viewpoint differences because they assume aligned image pairs\nas inputs. To cope with the difficulty, we introduce a deep graph matching\nnetwork that establishes object correspondence between an image pair. The\nintroduction enables us to detect object-wise scene changes without precise\nimage alignment. For more accurate object matching, we propose an\nepipolar-guided deep graph matching network (EGMNet), which incorporates the\nepipolar constraint into the deep graph matching layer used in OBJCDNet. To\nevaluate our network's robustness against viewpoint differences, we created\nsynthetic and real datasets for scene change detection from an image pair. The\nexperimental results verified the effectiveness of our network.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:48:40 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Doi", "Kento", ""], ["Hamaguchi", "Ryuhei", ""], ["Iwase", "Shun", ""], ["Yokota", "Rio", ""], ["Matsuo", "Yutaka", ""], ["Sakurada", "Ken", ""]]}, {"id": "2007.15546", "submitter": "Ine Dirks", "authors": "Sofie Tilborghs, Ine Dirks, Lucas Fidon, Siri Willems, Tom Eelbode,\n  Jeroen Bertels, Bart Ilsen, Arne Brys, Adriana Dubbeldam, Nico Buls,\n  Panagiotis Gonidakis, Sebasti\\'an Amador S\\'anchez, Annemiek Snoeckx, Paul M.\n  Parizel, Johan de Mey, Dirk Vandermeulen, Tom Vercauteren, David Robben, Dirk\n  Smeets, Frederik Maes, Jef Vandemeulebroucke, Paul Suetens", "title": "Comparative study of deep learning methods for the automatic\n  segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients", "comments": "Updated acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on COVID-19 suggests that CT imaging provides useful\ninformation to assess disease progression and assist diagnosis, in addition to\nhelp understanding the disease. There is an increasing number of studies that\npropose to use deep learning to provide fast and accurate quantification of\nCOVID-19 using chest CT scans. The main tasks of interest are the automatic\nsegmentation of lung and lung lesions in chest CT scans of confirmed or\nsuspected COVID-19 patients. In this study, we compare twelve deep learning\nalgorithms using a multi-center dataset, including both open-source and\nin-house developed algorithms. Results show that ensembling different methods\ncan boost the overall test set performance for lung segmentation, binary lesion\nsegmentation and multiclass lesion segmentation, resulting in mean Dice scores\nof 0.982, 0.724 and 0.469, respectively. The resulting binary lesions were\nsegmented with a mean absolute volume error of 91.3 ml. In general, the task of\ndistinguishing different lesion types was more difficult, with a mean absolute\nvolume difference of 152 ml and mean Dice scores of 0.369 and 0.523 for\nconsolidation and ground glass opacity, respectively. All methods perform\nbinary lesion segmentation with an average volume error that is better than\nvisual assessment by human raters, suggesting these methods are mature enough\nfor a large-scale evaluation for use in clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 10:40:39 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 15:44:08 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 18:53:13 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Tilborghs", "Sofie", ""], ["Dirks", "Ine", ""], ["Fidon", "Lucas", ""], ["Willems", "Siri", ""], ["Eelbode", "Tom", ""], ["Bertels", "Jeroen", ""], ["Ilsen", "Bart", ""], ["Brys", "Arne", ""], ["Dubbeldam", "Adriana", ""], ["Buls", "Nico", ""], ["Gonidakis", "Panagiotis", ""], ["S\u00e1nchez", "Sebasti\u00e1n Amador", ""], ["Snoeckx", "Annemiek", ""], ["Parizel", "Paul M.", ""], ["de Mey", "Johan", ""], ["Vandermeulen", "Dirk", ""], ["Vercauteren", "Tom", ""], ["Robben", "David", ""], ["Smeets", "Dirk", ""], ["Maes", "Frederik", ""], ["Vandemeulebroucke", "Jef", ""], ["Suetens", "Paul", ""]]}, {"id": "2007.15548", "submitter": "Yi Zhou", "authors": "Yi Zhou, Guillermo Gallego, Shaojie Shen", "title": "Event-based Stereo Visual Odometry", "comments": "18 pages, 18 figures, 7 tables", "journal-ref": "IEEE Transaction on Robotics (T-RO) 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are bio-inspired vision sensors whose pixels work\nindependently from each other and respond asynchronously to brightness changes,\nwith microsecond resolution. Their advantages make it possible to tackle\nchallenging scenarios in robotics, such as high-speed and high dynamic range\nscenes. We present a solution to the problem of visual odometry from the data\nacquired by a stereo event-based camera rig. Our system follows a parallel\ntracking-and-mapping approach, where novel solutions to each subproblem (3D\nreconstruction and camera pose estimation) are developed with two objectives in\nmind: being principled and efficient, for real-time operation with commodity\nhardware. To this end, we seek to maximize the spatio-temporal consistency of\nstereo event-based data while using a simple and efficient representation.\nSpecifically, the mapping module builds a semi-dense 3D map of the scene by\nfusing depth estimates from multiple local viewpoints (obtained by\nspatio-temporal consistency) in a probabilistic fashion. The tracking module\nrecovers the pose of the stereo rig by solving a registration problem that\nnaturally arises due to the chosen map and event data representation.\nExperiments on publicly available datasets and on our own recordings\ndemonstrate the versatility of the proposed method in natural scenes with\ngeneral 6-DoF motion. The system successfully leverages the advantages of\nevent-based cameras to perform visual odometry in challenging illumination\nconditions, such as low-light and high dynamic range, while running in\nreal-time on a standard CPU. We release the software and dataset under an open\nsource licence to foster research in the emerging topic of event-based SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:53:28 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:52:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhou", "Yi", ""], ["Gallego", "Guillermo", ""], ["Shen", "Shaojie", ""]]}, {"id": "2007.15551", "submitter": "Clifford Parker", "authors": "Clifford Seth Parker, William Brent Seales, Pnina Shor", "title": "Quantitative Distortion Analysis of Flattening Applied to the Scroll\n  from En-Gedi", "comments": "10 pages, 11 figures. In: Art & Archaeology, 2nd International\n  Conference. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-invasive volumetric imaging can now capture the internal structure and\ndetailed evidence of ink-based writing from within the confines of damaged and\ndeteriorated manuscripts that cannot be physically opened. As demonstrated\nrecently on the En-Gedi scroll, our \"virtual unwrapping\" software pipeline\nenables the recovery of substantial ink-based text from damaged artifacts at a\nquality high enough for serious critical textual analysis. However, the quality\nof the resulting images is defined by the subjective evaluation of scholars,\nand a choice of specific algorithms and parameters must be available at each\nstage in the pipeline in order to maximize the output quality.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:55:50 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Parker", "Clifford Seth", ""], ["Seales", "William Brent", ""], ["Shor", "Pnina", ""]]}, {"id": "2007.15560", "submitter": "Yacine Khraimeche", "authors": "Yacine Khraimeche, Guillaume-Alexandre Bilodeau, David Steele, and\n  Harshad Mahadik", "title": "Unsupervised Disentanglement GAN for Domain Adaptive Person\n  Re-Identification", "comments": "8 pages, 5 figures, submitted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent person re-identification (ReID) methods achieve high accuracy in\na supervised setting, their generalization to an unlabelled domain is still an\nopen problem. In this paper, we introduce a novel unsupervised disentanglement\ngenerative adversarial network (UD-GAN) to address the domain adaptation issue\nof supervised person ReID. Our framework jointly trains a ReID network for\ndiscriminative features extraction in a source labelled domain using identity\nannotation, and adapts the ReID model to an unlabelled target domain by\nlearning disentangled latent representations on the domain. Identity-unrelated\nfeatures in the target domain are distilled from the latent features. As a\nresult, the ReID features better encompass the identity of a person in the\nunsupervised domain. We conducted experiments on the Market1501, DukeMTMC and\nMSMT17 datasets. Results show that the unsupervised domain adaptation problem\nin ReID is very challenging. Nevertheless, our method shows improvement in half\nof the domain transfers and achieve state-of-the-art performance for one of\nthem.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:07:05 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Khraimeche", "Yacine", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Steele", "David", ""], ["Mahadik", "Harshad", ""]]}, {"id": "2007.15576", "submitter": "Jinlong Peng", "authors": "Jinlong Peng, Yueyang Gu, Yabiao Wang, Chengjie Wang, Jilin Li, Feiyue\n  Huang", "title": "Dense Scene Multiple Object Tracking with Box-Plane Matching", "comments": "ACM Multimedia 2020 GC paper. ACM Multimedia Grand Challenge HiEve\n  2020 Track-1 Winner", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) is an important task in computer vision. MOT\nis still challenging due to the occlusion problem, especially in dense scenes.\nFollowing the tracking-by-detection framework, we propose the Box-Plane\nMatching (BPM) method to improve the MOT performacne in dense scenes. First, we\ndesign the Layer-wise Aggregation Discriminative Model (LADM) to filter the\nnoisy detections. Then, to associate remaining detections correctly, we\nintroduce the Global Attention Feature Model (GAFM) to extract appearance\nfeature and use it to calculate the appearance similarity between history\ntracklets and current detections. Finally, we propose the Box-Plane Matching\nstrategy to achieve data association according to the motion similarity and\nappearance similarity between tracklets and detections. With the effectiveness\nof the three modules, our team achieves the 1st place on the Track-1\nleaderboard in the ACM MM Grand Challenge HiEve 2020.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:39:22 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 14:09:07 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Peng", "Jinlong", ""], ["Gu", "Yueyang", ""], ["Wang", "Yabiao", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2007.15602", "submitter": "Yinbo Liu", "authors": "Yin-Bo Liu, Ming Zeng, Qing-Hao Meng", "title": "Heatmap-based Vanishing Point boosts Lane Detection", "comments": "5 pages, 3 figures, submitted to IEEE journal, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based lane detection (LD) is a key part of autonomous driving\ntechnology, and it is also a challenging problem. As one of the important\nconstraints of scene composition, vanishing point (VP) may provide a useful\nclue for lane detection. In this paper, we proposed a new multi-task fusion\nnetwork architecture for high-precision lane detection. Firstly, the ERFNet was\nused as the backbone to extract the hierarchical features of the road image.\nThen, the lanes were detected using image segmentation. Finally, combining the\noutput of lane detection and the hierarchical features extracted by the\nbackbone, the lane VP was predicted using heatmap regression. The proposed\nfusion strategy was tested using the public CULane dataset. The experimental\nresults suggest that the lane detection accuracy of our method outperforms\nthose of state-of-the-art (SOTA) methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:17:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Liu", "Yin-Bo", ""], ["Zeng", "Ming", ""], ["Meng", "Qing-Hao", ""]]}, {"id": "2007.15610", "submitter": "He Huang", "authors": "He Huang, Yuanwei Chen, Wei Tang, Wenhao Zheng, Qing-Guo Chen, Yao Hu,\n  Philip Yu", "title": "Multi-label Zero-shot Classification by Learning to Transfer from\n  External Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-label zero-shot classification aims to predict multiple unseen class\nlabels for an input image. It is more challenging than its single-label\ncounterpart. On one hand, the unconstrained number of labels assigned to each\nimage makes the model more easily overfit to those seen classes. On the other\nhand, there is a large semantic gap between seen and unseen classes in the\nexisting multi-label classification datasets. To address these difficult\nissues, this paper introduces a novel multi-label zero-shot classification\nframework by learning to transfer from external knowledge. We observe that\nImageNet is commonly used to pretrain the feature extractor and has a large and\nfine-grained label space. This motivates us to exploit it as external knowledge\nto bridge the seen and unseen classes and promote generalization. Specifically,\nwe construct a knowledge graph including not only classes from the target\ndataset but also those from ImageNet. Since ImageNet labels are not available\nin the target dataset, we propose a novel PosVAE module to infer their initial\nstates in the extended knowledge graph. Then we design a relational graph\nconvolutional network (RGCN) to propagate information among classes and achieve\nknowledge transfer. Experimental results on two benchmark datasets demonstrate\nthe effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:26:46 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 01:29:56 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Huang", "He", ""], ["Chen", "Yuanwei", ""], ["Tang", "Wei", ""], ["Zheng", "Wenhao", ""], ["Chen", "Qing-Guo", ""], ["Hu", "Yao", ""], ["Yu", "Philip", ""]]}, {"id": "2007.15627", "submitter": "Nicolai H\\\"ani", "authors": "Nicolai H\\\"ani, Selim Engin, Jun-Jee Chao and Volkan Isler", "title": "Continuous Object Representation Networks: Novel View Synthesis without\n  Target View Supervision", "comments": "To appear at Advances in Neural Information Processing Systems 33\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel View Synthesis (NVS) is concerned with synthesizing views under camera\nviewpoint transformations from one or multiple input images. NVS requires\nexplicit reasoning about 3D object structure and unseen parts of the scene to\nsynthesize convincing results. As a result, current approaches typically rely\non supervised training with either ground truth 3D models or multiple target\nimages. We propose Continuous Object Representation Networks (CORN), a\nconditional architecture that encodes an input image's geometry and appearance\nthat map to a 3D consistent scene representation. We can train CORN with only\ntwo source images per object by combining our model with a neural renderer. A\nkey feature of CORN is that it requires no ground truth 3D models or target\nview supervision. Regardless, CORN performs well on challenging tasks such as\nnovel view synthesis and single-view 3D reconstruction and achieves performance\ncomparable to state-of-the-art approaches that use direct supervision. For\nup-to-date information, data, and code, please see our project page:\nhttps://nicolaihaeni.github.io/corn/.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:49:44 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:19:10 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["H\u00e4ni", "Nicolai", ""], ["Engin", "Selim", ""], ["Chao", "Jun-Jee", ""], ["Isler", "Volkan", ""]]}, {"id": "2007.15629", "submitter": "Namdar Homayounfar", "authors": "Namdar Homayounfar, Yuwen Xiong, Justin Liang, Wei-Chiu Ma, Raquel\n  Urtasun", "title": "LevelSet R-CNN: A Deep Variational Method for Instance Segmentation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining precise instance segmentation masks is of high importance in many\nmodern applications such as robotic manipulation and autonomous driving.\nCurrently, many state of the art models are based on the Mask R-CNN framework\nwhich, while very powerful, outputs masks at low resolutions which could result\nin imprecise boundaries. On the other hand, classic variational methods for\nsegmentation impose desirable global and local data and geometry constraints on\nthe masks by optimizing an energy functional. While mathematically elegant,\ntheir direct dependence on good initialization, non-robust image cues and\nmanual setting of hyperparameters renders them unsuitable for modern\napplications. We propose LevelSet R-CNN, which combines the best of both worlds\nby obtaining powerful feature representations that are combined in an\nend-to-end manner with a variational segmentation framework. We demonstrate the\neffectiveness of our approach on COCO and Cityscapes datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:52:18 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Homayounfar", "Namdar", ""], ["Xiong", "Yuwen", ""], ["Liang", "Justin", ""], ["Ma", "Wei-Chiu", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2007.15646", "submitter": "David Bau iii", "authors": "David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba", "title": "Rewriting a Deep Generative Model", "comments": "ECCV 2020 (oral). Code at https://github.com/davidbau/rewriting. For\n  videos and demos see https://rewriting.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model such as a GAN learns to model a rich set of semantic\nand physical rules about the target distribution, but up to now, it has been\nobscure how such rules are encoded in the network, or how a rule could be\nchanged. In this paper, we introduce a new problem setting: manipulation of\nspecific rules encoded by a deep generative model. To address the problem, we\npropose a formulation in which the desired rule is changed by manipulating a\nlayer of a deep network as a linear associative memory. We derive an algorithm\nfor modifying one entry of the associative memory, and we demonstrate that\nseveral interesting structural rules can be located and modified within the\nlayers of state-of-the-art generative models. We present a user interface to\nenable users to interactively change the rules of a generative model to achieve\ndesired effects, and we show several proof-of-concept applications. Finally,\nresults on multiple datasets demonstrate the advantage of our method against\nstandard fine-tuning methods and edit transfer algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:58:16 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bau", "David", ""], ["Liu", "Steven", ""], ["Wang", "Tongzhou", ""], ["Zhu", "Jun-Yan", ""], ["Torralba", "Antonio", ""]]}, {"id": "2007.15649", "submitter": "Jason Y. Zhang", "authors": "Jason Y. Zhang and Sam Pepose and Hanbyul Joo and Deva Ramanan and\n  Jitendra Malik and Angjoo Kanazawa", "title": "Perceiving 3D Human-Object Spatial Arrangements from a Single Image in\n  the Wild", "comments": "In ECCV 2020. v2: Updated Related Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that infers spatial arrangements and shapes of humans and\nobjects in a globally consistent 3D scene, all from a single image in-the-wild\ncaptured in an uncontrolled environment. Notably, our method runs on datasets\nwithout any scene- or object-level 3D supervision. Our key insight is that\nconsidering humans and objects jointly gives rise to \"3D common sense\"\nconstraints that can be used to resolve ambiguity. In particular, we introduce\na scale loss that learns the distribution of object size from data; an\nocclusion-aware silhouette re-projection loss to optimize object pose; and a\nhuman-object interaction loss to capture the spatial layout of objects with\nwhich humans interact. We empirically validate that our constraints\ndramatically reduce the space of likely 3D spatial configurations. We\ndemonstrate our approach on challenging, in-the-wild images of humans\ninteracting with large objects (such as bicycles, motorcycles, and surfboards)\nand handheld objects (such as laptops, tennis rackets, and skateboards). We\nquantify the ability of our approach to recover human-object arrangements and\noutline remaining challenges in this relatively domain. The project webpage can\nbe found at https://jasonyzhang.com/phosa.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:59:50 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 20:17:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhang", "Jason Y.", ""], ["Pepose", "Sam", ""], ["Joo", "Hanbyul", ""], ["Ramanan", "Deva", ""], ["Malik", "Jitendra", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2007.15651", "submitter": "Taesung Park", "authors": "Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "comments": "ECCV 2020. Please visit\n  https://taesungp.github.io/ContrastiveUnpairedTranslation/ for introduction\n  videos and more. v3 contains typo fixes and citation update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image-to-image translation, each patch in the output should reflect the\ncontent of the corresponding patch in the input, independent of domain. We\npropose a straightforward method for doing so -- maximizing mutual information\nbetween the two, using a framework based on contrastive learning. The method\nencourages two elements (corresponding patches) to map to a similar point in a\nlearned feature space, relative to other elements (other patches) in the\ndataset, referred to as negatives. We explore several critical design choices\nfor making contrastive learning effective in the image synthesis setting.\nNotably, we use a multilayer, patch-based approach, rather than operate on\nentire images. Furthermore, we draw negatives from within the input image\nitself, rather than from the rest of the dataset. We demonstrate that our\nframework enables one-sided translation in the unpaired image-to-image\ntranslation setting, while improving quality and reducing training time. In\naddition, our method can even be extended to the training setting where each\n\"domain\" is only a single image.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:59:58 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 21:32:40 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 17:33:08 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Park", "Taesung", ""], ["Efros", "Alexei A.", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""]]}, {"id": "2007.15678", "submitter": "Wei Peng", "authors": "Wei Peng and Jingang Shi and Zhaoqiang Xia and Guoying Zhao", "title": "Mix Dimension in Poincar\\'{e} Geometry for 3D Skeleton-based Action\n  Recognition", "comments": "Accepted by ACM MM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph Convolutional Networks (GCNs) have already demonstrated their powerful\nability to model the irregular data, e.g., skeletal data in human action\nrecognition, providing an exciting new way to fuse rich structural information\nfor nodes residing in different parts of a graph. In human action recognition,\ncurrent works introduce a dynamic graph generation mechanism to better capture\nthe underlying semantic skeleton connections and thus improves the performance.\nIn this paper, we provide an orthogonal way to explore the underlying\nconnections. Instead of introducing an expensive dynamic graph generation\nparadigm, we build a more efficient GCN on a Riemann manifold, which we think\nis a more suitable space to model the graph data, to make the extracted\nrepresentations fit the embedding matrix. Specifically, we present a novel\nspatial-temporal GCN (ST-GCN) architecture which is defined via the Poincar\\'e\ngeometry such that it is able to better model the latent anatomy of the\nstructure data. To further explore the optimal projection dimension in the\nRiemann space, we mix different dimensions on the manifold and provide an\nefficient way to explore the dimension for each ST-GCN layer. With the final\nresulted architecture, we evaluate our method on two current largest scale 3D\ndatasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that\nthe model could achieve a superior performance under any given evaluation\nmetrics with only 40\\% model size when compared with the previous best GCN\nmethod, which proves the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 18:23:18 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 14:19:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Peng", "Wei", ""], ["Shi", "Jingang", ""], ["Xia", "Zhaoqiang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2007.15683", "submitter": "Mingyang Li", "authors": "Xinru Yang, Haozhi Qi, Mingyang Li, Alexander Hauptmann", "title": "From A Glance to \"Gotcha\": Interactive Facial Image Retrieval with\n  Progressive Relevance Feedback", "comments": null, "journal-ref": "The SIGIR 2020 Workshop on Applied Interactive Information Systems", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image retrieval plays a significant role in forensic investigations\nwhere an untrained witness tries to identify a suspect from a massive pool of\nimages. However, due to the difficulties in describing human facial appearances\nverbally and directly, people naturally tend to depict by referring to\nwell-known existing images and comparing specific areas of faces with them and\nit is also challenging to provide complete comparison at each time. Therefore,\nwe propose an end-to-end framework to retrieve facial images with relevance\nfeedback progressively provided by the witness, enabling an exploitation of\nhistory information during multiple rounds and an interactive and iterative\napproach to retrieving the mental image. With no need of any extra annotations,\nour model can be applied at the cost of a little response effort. We experiment\non \\texttt{CelebA} and evaluate the performance by ranking percentile and\nachieve 99\\% under the best setting. Since this topic remains little explored\nto the best of our knowledge, we hope our work can serve as a stepping stone\nfor further research.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 18:46:25 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yang", "Xinru", ""], ["Qi", "Haozhi", ""], ["Li", "Mingyang", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2007.15693", "submitter": "Carlos Anjos", "authors": "Carlos E. M. dos Anjos, Manuel R. V. Avila, Adna G. P. Vasconcelos,\n  Aurea M.P. Neta, Lizianne C. Medeiros, Alexandre G. Evsukoff and Rodrigo\n  Surmas", "title": "Deep learning for lithological classification of carbonate rock micro-CT\n  images", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to the ongoing development, pre-salt carbonate reservoir\ncharacterization remains a challenge, primarily due to inherent geological\nparticularities. These challenges stimulate the use of well-established\ntechnologies, such as artificial intelligence algorithms, for image\nclassification tasks. Therefore, this work intends to present an application of\ndeep learning techniques to identify patterns in Brazilian pre-salt carbonate\nrock microtomographic images, thus making possible lithological classification.\nFour convolutional neural network models were proposed. The first model\nincludes three convolutional layers followed by fully connected layers and is\nused as a base model for the following proposals. In the next two models, we\nreplace the max pooling layer with a spatial pyramid pooling and a global\naverage pooling layer. The last model uses a combination of spatial pyramid\npooling followed by global average pooling in place of the last pooling layer.\nAll models are compared using original images, when possible, as well as\nresized images. The dataset consists of 6,000 images from three different\nclasses. The model performances were evaluated by each image individually, as\nwell as by the most frequently predicted class for each sample. According to\naccuracy, Model 2 trained on resized images achieved the best results, reaching\nan average of 75.54% for the first evaluation approach and an average of 81.33%\nfor the second. We developed a workflow to automate and accelerate the\nlithology classification of Brazilian pre-salt carbonate samples by\ncategorizing microtomographic images using deep learning algorithms in a\nnon-destructive way.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 19:14:00 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Anjos", "Carlos E. M. dos", ""], ["Avila", "Manuel R. V.", ""], ["Vasconcelos", "Adna G. P.", ""], ["Neta", "Aurea M. P.", ""], ["Medeiros", "Lizianne C.", ""], ["Evsukoff", "Alexandre G.", ""], ["Surmas", "Rodrigo", ""]]}, {"id": "2007.15757", "submitter": "Darshan Venkatrayappa", "authors": "Darshan Venkatrayappa, Agn\\`es Desolneux, Jean-Michel Hubert, Josselin\n  Manceau", "title": "Unidentified Floating Object detection in maritime environment using\n  dictionary learning", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maritime domain is one of the most challenging scenarios for object detection\ndue to the complexity of the observed scene. In this article, we present a new\napproach to detect unidentified floating objects in the maritime environment.\nThe proposed approach is capable of detecting floating objects without any\nprior knowledge of their visual appearance, shape or location. The input image\nfrom the video stream is denoised using a visual dictionary learned from a\nK-SVD algorithm. The denoised image is made of self-similar content. Later, we\nextract the residual image, which is the difference between the original image\nand the denoised (self-similar) image. Thus, the residual image contains noise\nand salient structures (objects). These salient structures can be extracted\nusing an a contrario model. We demonstrate the capabilities of our algorithm by\ntesting it on videos exhibiting varying maritime scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:59:09 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Venkatrayappa", "Darshan", ""], ["Desolneux", "Agn\u00e8s", ""], ["Hubert", "Jean-Michel", ""], ["Manceau", "Josselin", ""]]}, {"id": "2007.15778", "submitter": "Leo Tam", "authors": "Leo K. Tam, Xiaosong Wang, Evrim Turkbey, Kevin Lu, Yuhong Wen, and\n  Daguang Xu", "title": "Weakly supervised one-stage vision and language disease detection using\n  large scale pneumonia and pneumothorax studies", "comments": "Accepted at Medical Image Computing and Computer-Assisted\n  Intervention -- MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting clinically relevant objects in medical images is a challenge\ndespite large datasets due to the lack of detailed labels. To address the label\nissue, we utilize the scene-level labels with a detection architecture that\nincorporates natural language information. We present a challenging new set of\nradiologist paired bounding box and natural language annotations on the\npublicly available MIMIC-CXR dataset especially focussed on pneumonia and\npneumothorax. Along with the dataset, we present a joint vision language weakly\nsupervised transformer layer-selected one-stage dual head detection\narchitecture (LITERATI) alongside strong baseline comparisons with class\nactivation mapping (CAM), gradient CAM, and relevant implementations on the NIH\nChestXray-14 and MIMIC-CXR dataset. Borrowing from advances in vision language\narchitectures, the LITERATI method demonstrates joint image and referring\nexpression (objects localized in the image using natural language) input for\ndetection that scales in a purely weakly supervised fashion. The architectural\nmodifications address three obstacles -- implementing a supervised vision and\nlanguage detection method in a weakly supervised fashion, incorporating\nclinical referring expression natural language information, and generating high\nfidelity detections with map probabilities. Nevertheless, the challenging\nclinical nature of the radiologist annotations including subtle references,\nmulti-instance specifications, and relatively verbose underlying medical\nreports, ensures the vision language detection task at scale remains\nstimulating for future investigation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 00:04:14 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Tam", "Leo K.", ""], ["Wang", "Xiaosong", ""], ["Turkbey", "Evrim", ""], ["Lu", "Kevin", ""], ["Wen", "Yuhong", ""], ["Xu", "Daguang", ""]]}, {"id": "2007.15781", "submitter": "Baoxiong Jia", "authors": "Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, Song-chun Zhu", "title": "LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task\n  Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and interpreting human actions is a long-standing challenge and\na critical indicator of perception in artificial intelligence. However, a few\nimperative components of daily human activities are largely missed in prior\nliterature, including the goal-directed actions, concurrent multi-tasks, and\ncollaborations among multi-agents. We introduce the LEMMA dataset to provide a\nsingle home to address these missing dimensions with meticulously designed\nsettings, wherein the number of tasks and agents varies to highlight different\nlearning objectives. We densely annotate the atomic-actions with human-object\ninteractions to provide ground-truths of the compositionality, scheduling, and\nassignment of daily activities. We further devise challenging compositional\naction recognition and action/task anticipation benchmarks with baseline models\nto measure the capability of compositional action understanding and temporal\nreasoning. We hope this effort would drive the machine vision community to\nexamine goal-directed human activities and further study the task scheduling\nand assignment in the real world.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 00:13:54 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Jia", "Baoxiong", ""], ["Chen", "Yixin", ""], ["Huang", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-chun", ""]]}, {"id": "2007.15796", "submitter": "Yue Meng", "authors": "Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid\n  Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris", "title": "AR-Net: Adaptive Frame Resolution for Efficient Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is an open and challenging problem in computer vision.\nWhile current state-of-the-art models offer excellent recognition results,\ntheir computational expense limits their impact for many real-world\napplications. In this paper, we propose a novel approach, called AR-Net\n(Adaptive Resolution Network), that selects on-the-fly the optimal resolution\nfor each frame conditioned on the input for efficient action recognition in\nlong untrimmed videos. Specifically, given a video frame, a policy network is\nused to decide what input resolution should be used for processing by the\naction recognition model, with the goal of improving both accuracy and\nefficiency. We efficiently train the policy network jointly with the\nrecognition model using standard back-propagation. Extensive experiments on\nseveral challenging action recognition benchmark datasets well demonstrate the\nefficacy of our proposed approach over state-of-the-art methods. The project\npage can be found at https://mengyuest.github.io/AR-Net\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 01:36:04 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Meng", "Yue", ""], ["Lin", "Chung-Ching", ""], ["Panda", "Rameswar", ""], ["Sattigeri", "Prasanna", ""], ["Karlinsky", "Leonid", ""], ["Oliva", "Aude", ""], ["Saenko", "Kate", ""], ["Feris", "Rogerio", ""]]}, {"id": "2007.15815", "submitter": "Weizhe Lin", "authors": "Weizhe Lin, Indigo Orton, Qingbiao Li, Gabriela Pavarini, Marwa\n  Mahmoud", "title": "Looking At The Body: Automatic Analysis of Body Gestures and\n  Self-Adaptors in Psychological Distress", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychological distress is a significant and growing issue in society.\nAutomatic detection, assessment, and analysis of such distress is an active\narea of research. Compared to modalities such as face, head, and vocal,\nresearch investigating the use of the body modality for these tasks is\nrelatively sparse. This is, in part, due to the limited available datasets and\ndifficulty in automatically extracting useful body features. Recent advances in\npose estimation and deep learning have enabled new approaches to this modality\nand domain. To enable this research, we have collected and analyzed a new\ndataset containing full body videos for short interviews and self-reported\ndistress labels. We propose a novel method to automatically detect\nself-adaptors and fidgeting, a subset of self-adaptors that has been shown to\nbe correlated with psychological distress. We perform analysis on statistical\nbody gestures and fidgeting features to explore how distress levels affect\nparticipants' behaviors. We then propose a multi-modal approach that combines\ndifferent feature representations using Multi-modal Deep Denoising\nAuto-Encoders and Improved Fisher Vector Encoding. We demonstrate that our\nproposed model, combining audio-visual features with automatically detected\nfidgeting behavioral cues, can successfully predict distress levels in a\ndataset labeled with self-reported anxiety and depression levels.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:45:00 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lin", "Weizhe", ""], ["Orton", "Indigo", ""], ["Li", "Qingbiao", ""], ["Pavarini", "Gabriela", ""], ["Mahmoud", "Marwa", ""]]}, {"id": "2007.15817", "submitter": "Bo Gao", "authors": "Bo Gao and M. W. Spratling", "title": "Robust Template Matching via Hierarchical Convolutional Features from a\n  Shape Biased CNN", "comments": "11 pages, 2 figures and 4 tables. This paper was accepted by ICIVIS\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a template in a search image is an important task underlying many\ncomputer vision applications. Recent approaches perform template matching in a\ndeep feature-space, produced by a convolutional neural network (CNN), which is\nfound to provide more tolerance to changes in appearance. In this article we\ninvestigate if enhancing the CNN's encoding of shape information can produce\nmore distinguishable features that improve the performance of template\nmatching. This investigation results in a new template matching method that\nproduces state-of-the-art results on a standard benchmark. To confirm these\nresults we also create a new benchmark and show that the proposed method also\noutperforms existing techniques on this new dataset. Our code and dataset is\navailable at: https://github.com/iminfine/Deep-DIM.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:07:52 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 11:33:07 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 02:01:36 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Gao", "Bo", ""], ["Spratling", "M. W.", ""]]}, {"id": "2007.15818", "submitter": "Yoshitomo Matsubara", "authors": "Yoshitomo Matsubara, Marco Levorato", "title": "Neural Compression and Filtering for Edge-assisted Real-time Object\n  Detection in Challenged Networks", "comments": "Accepted to ICPR 2020 in the 1st round (the 25th International\n  Conference on Pattern Recognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edge computing paradigm places compute-capable devices - edge servers -\nat the network edge to assist mobile devices in executing data analysis tasks.\nIntuitively, offloading compute-intense tasks to edge servers can reduce their\nexecution time. However, poor conditions of the wireless channel connecting the\nmobile devices to the edge servers may degrade the overall capture-to-output\ndelay achieved by edge offloading. Herein, we focus on edge computing\nsupporting remote object detection by means of Deep Neural Networks (DNNs), and\ndevelop a framework to reduce the amount of data transmitted over the wireless\nlink. The core idea we propose builds on recent approaches splitting DNNs into\nsections - namely head and tail models - executed by the mobile device and edge\nserver, respectively. The wireless link, then, is used to transport the output\nof the last layer of the head model to the edge server, instead of the DNN\ninput. Most prior work focuses on classification tasks and leaves the DNN\nstructure unaltered. Herein, our focus is on DNNs for three different object\ndetection tasks, which present a much more convoluted structure, and modify the\narchitecture of the network to: (i) achieve in-network compression by\nintroducing a bottleneck layer in the early layers on the head model, and (ii)\nprefilter pictures that do not contain objects of interest using a\nconvolutional neural network. Results show that the proposed technique\nrepresents an effective intermediate option between local and edge computing in\na parameter region where these extreme point solutions fail to provide\nsatisfactory performance. The code and trained models are available at\nhttps://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors .\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:11:46 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 18:03:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Matsubara", "Yoshitomo", ""], ["Levorato", "Marco", ""]]}, {"id": "2007.15820", "submitter": "Ekim Yurtsever", "authors": "Ekim Yurtsever, Dongfang Yang, Ibrahim Mert Koc, Keith A. Redmill", "title": "Blending Generative Adversarial Image Synthesis with Rendering for\n  Computer Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional computer graphics pipelines require detailed 3D models, meshes,\ntextures, and rendering engines to generate 2D images from 3D scenes. These\nprocesses are labor-intensive. We introduce Hybrid Neural Computer Graphics\n(HNCG) as an alternative. The contribution is a novel image formation strategy\nto reduce the 3D model and texture complexity of computer graphics pipelines.\nOur main idea is straightforward: Given a 3D scene, render only important\nobjects of interest and use generative adversarial processes for synthesizing\nthe rest of the image. To this end, we propose a novel image formation strategy\nto form 2D semantic images from 3D scenery consisting of simple object models\nwithout textures. These semantic images are then converted into photo-realistic\nRGB images with a state-of-the-art conditional Generative Adversarial Network\n(cGAN) based image synthesizer trained on real-world data. Meanwhile, objects\nof interest are rendered using a physics-based graphics engine. This is\nnecessary as we want to have full control over the appearance of objects of\ninterest. Finally, the partially-rendered and cGAN synthesized images are\nblended with a blending GAN. We show that the proposed framework outperforms\nconventional rendering with ablation and comparison studies. Semantic retention\nand Fr\\'echet Inception Distance (FID) measurements were used as the main\nperformance metrics.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:25:17 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yurtsever", "Ekim", ""], ["Yang", "Dongfang", ""], ["Koc", "Ibrahim Mert", ""], ["Redmill", "Keith A.", ""]]}, {"id": "2007.15829", "submitter": "Yadan Luo", "authors": "Yadan Luo, Zi Huang, Zijian Wang, Zheng Zhang, Mahsa Baktashmotlagh", "title": "Adversarial Bipartite Graph Learning for Video Domain Adaptation", "comments": "Proceedings of the 28th ACM International Conference on Multimedia\n  (MM '20)", "journal-ref": null, "doi": "10.1145/3394171.3413897", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation techniques, which focus on adapting models between\ndistributionally different domains, are rarely explored in the video\nrecognition area due to the significant spatial and temporal shifts across the\nsource (i.e. training) and target (i.e. test) domains. As such, recent works on\nvisual domain adaptation which leverage adversarial learning to unify the\nsource and target video representations and strengthen the feature\ntransferability are not highly effective on the videos. To overcome this\nlimitation, in this paper, we learn a domain-agnostic video classifier instead\nof learning domain-invariant representations, and propose an Adversarial\nBipartite Graph (ABG) learning framework which directly models the\nsource-target interactions with a network topology of the bipartite graph.\nSpecifically, the source and target frames are sampled as heterogeneous\nvertexes while the edges connecting two types of nodes measure the affinity\namong them. Through message-passing, each vertex aggregates the features from\nits heterogeneous neighbors, forcing the features coming from the same class to\nbe mixed evenly. Explicitly exposing the video classifier to such cross-domain\nrepresentations at the training and test stages makes our model less biased to\nthe labeled source data, which in-turn results in achieving a better\ngeneralization on the target domain. To further enhance the model capacity and\ntestify the robustness of the proposed architecture on difficult transfer\ntasks, we extend our model to work in a semi-supervised setting using an\nadditional video-level bipartite graph. Extensive experiments conducted on four\nbenchmarks evidence the effectiveness of the proposed approach over the SOTA\nmethods on the task of video recognition.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:48:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Luo", "Yadan", ""], ["Huang", "Zi", ""], ["Wang", "Zijian", ""], ["Zhang", "Zheng", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2007.15831", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast and Hamid Heydarian and Marc T. P. Adam and Megan E.\n  Rollo", "title": "OREBA: A Dataset for Objectively Recognizing Eating Behaviour and\n  Associated Intake", "comments": "To be published in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3026965", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of intake gestures is a key element of automatic dietary\nmonitoring. Several types of sensors, including inertial measurement units\n(IMU) and video cameras, have been used for this purpose. The common machine\nlearning approaches make use of the labeled sensor data to automatically learn\nhow to make detections. One characteristic, especially for deep learning\nmodels, is the need for large datasets. To meet this need, we collected the\nObjectively Recognizing Eating Behavior and Associated Intake (OREBA) dataset.\nThe OREBA dataset aims to provide comprehensive multi-sensor data recorded\nduring the course of communal meals for researchers interested in intake\ngesture detection. Two scenarios are included, with 100 participants for a\ndiscrete dish and 102 participants for a shared dish, totalling 9069 intake\ngestures. Available sensor data consists of synchronized frontal video and IMU\nwith accelerometer and gyroscope for both hands. We report the details of data\ncollection and annotation, as well as details of sensor processing. The results\nof studies on IMU and video data involving deep learning models are reported to\nprovide a baseline for future research. Specifically, the best baseline models\nachieve performances of $F_1$ = 0.853 for the discrete dish using video and\n$F_1$ = 0.852 for the shared dish using inertial data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:54:05 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 06:06:24 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 23:55:23 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Heydarian", "Hamid", ""], ["Adam", "Marc T. P.", ""], ["Rollo", "Megan E.", ""]]}, {"id": "2007.15837", "submitter": "Seonwook Park", "authors": "Xucong Zhang and Seonwook Park and Thabo Beeler and Derek Bradley and\n  Siyu Tang and Otmar Hilliges", "title": "ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head\n  Pose and Gaze Variation", "comments": "Accepted at ECCV 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze estimation is a fundamental task in many applications of computer\nvision, human computer interaction and robotics. Many state-of-the-art methods\nare trained and tested on custom datasets, making comparison across methods\nchallenging. Furthermore, existing gaze estimation datasets have limited head\npose and gaze variations, and the evaluations are conducted using different\nprotocols and metrics. In this paper, we propose a new gaze estimation dataset\ncalled ETH-XGaze, consisting of over one million high-resolution images of\nvarying gaze under extreme head poses. We collect this dataset from 110\nparticipants with a custom hardware setup including 18 digital SLR cameras and\nadjustable illumination conditions, and a calibrated system to record ground\ntruth gaze targets. We show that our dataset can significantly improve the\nrobustness of gaze estimation methods across different head poses and gaze\nangles. Additionally, we define a standardized experimental protocol and\nevaluation metric on ETH-XGaze, to better unify gaze estimation research going\nforward. The dataset and benchmark website are available at\nhttps://ait.ethz.ch/projects/2020/ETH-XGaze\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:15:53 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Zhang", "Xucong", ""], ["Park", "Seonwook", ""], ["Beeler", "Thabo", ""], ["Bradley", "Derek", ""], ["Tang", "Siyu", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2007.15840", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yan Zhang, Mingliang Xu, Li Zhang, Yi Yang, Shuicheng Yan", "title": "A Survey on Concept Factorization: From Shallow to Deep Representation\n  Learning", "comments": "Please cite this work as: Zhao Zhang, Yan Zhang, Mingliang Xu, Li\n  Zhang, Yi Yang and Shuicheng Yan, \"A Survey on Concept Factorization: From\n  Shallow to Deep Representation Learning,\" Information Processing and\n  Management (IPM), Jan 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of learned features by representation learning determines the\nperformance of learning algorithms and the related application tasks (such as\nhigh-dimensional data clustering). As a relatively new paradigm for\nrepresentation learning, Concept Factorization (CF) has attracted a great deal\nof interests in the areas of machine learning and data mining for over a\ndecade. Lots of effective CF based methods have been proposed based on\ndifferent perspectives and properties, but note that it still remains not easy\nto grasp the essential connections and figure out the underlying explanatory\nfactors from exiting studies. In this paper, we therefore survey the recent\nadvances on CF methodologies and the potential benchmarks by categorizing and\nsummarizing the current methods. Specifically, we first re-view the root CF\nmethod, and then explore the advancement of CF-based representation learning\nranging from shallow to deep/multilayer cases. We also introduce the potential\napplication areas of CF-based methods. Finally, we point out some future\ndirections for studying the CF-based representation learning. Overall, this\nsurvey provides an insightful overview of both theoretical basis and current\ndevelopments in the field of CF, which can also help the interested researchers\nto understand the current trends of CF and find the most appropriate CF\ntechniques to deal with particular applications.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:19:14 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 01:56:01 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 08:45:58 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhang", "Zhao", ""], ["Zhang", "Yan", ""], ["Xu", "Mingliang", ""], ["Zhang", "Li", ""], ["Yang", "Yi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2007.15841", "submitter": "David Paulius", "authors": "Maxat Alibayev, David Paulius and Yu Sun", "title": "Estimating Motion Codes from Demonstration Videos", "comments": "IROS 2020 Submission -- 6 pages; initial upload (Last updated July\n  31st 2020)", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9341065", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A motion taxonomy can encode manipulations as a binary-encoded\nrepresentation, which we refer to as motion codes. These motion codes innately\nrepresent a manipulation action in an embedded space that describes the\nmotion's mechanical features, including contact and trajectory type. The key\nadvantage of using motion codes for embedding is that motions can be more\nappropriately defined with robotic-relevant features, and their distances can\nbe more reasonably measured using these motion features. In this paper, we\ndevelop a deep learning pipeline to extract motion codes from demonstration\nvideos in an unsupervised manner so that knowledge from these videos can be\nproperly represented and used for robots. Our evaluations show that motion\ncodes can be extracted from demonstrations of action in the EPIC-KITCHENS\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:20:31 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Alibayev", "Maxat", ""], ["Paulius", "David", ""], ["Sun", "Yu", ""]]}, {"id": "2007.15850", "submitter": "Sohaib Ahmad", "authors": "Sohaib Ahmad, Christopher Geiger, Benjamin Fuller", "title": "Resist : Reconstruction of irises from templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition systems transform an iris image into a feature vector. The\nseminal pipeline segments an image into iris and non-iris pixels, normalizes\nthis region into a fixed-dimension rectangle, and extracts features which are\nstored and called a template (Daugman, 2009). This template is stored on a\nsystem. A future reading of an iris can be transformed and compared against\ntemplate vectors to determine or verify the identity of an individual. As\ntemplates are often stored together, they are a valuable target to an attacker.\nWe show how to invert templates across a variety of iris recognition systems.\nThat is, we show how to transform templates into realistic looking iris images\nthat are also deemed as the same iris by the corresponding recognition system.\nOur inversion is based on a convolutional neural network architecture we call\nRESIST (REconStructing IriSes from Templates). We apply RESIST to a traditional\nGabor filter pipeline, to a DenseNet (Huang et al., CVPR 2017) feature\nextractor, and to a DenseNet architecture that works without normalization.\nBoth DenseNet feature extractors are based on the recent ThirdEye recognition\nsystem (Ahmad and Fuller, BTAS 2019). When training and testing using the\nND-0405 dataset, reconstructed images demonstrate a rank-1 accuracy of 100%,\n76%, and 96% respectively for the three pipelines. The core of our approach is\nsimilar to an autoencoder. However, standalone training the core produced low\naccuracy. The final architecture integrates into an generative adversarial\nnetwork (Goodfellow et al., NeurIPS, 2014) producing higher accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:08:28 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 05:19:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ahmad", "Sohaib", ""], ["Geiger", "Christopher", ""], ["Fuller", "Benjamin", ""]]}, {"id": "2007.15855", "submitter": "Teppei Suzuki", "authors": "Teppei Suzuki, Keisuke Ozawa, Yusuke Sekikawa", "title": "Rethinking PointNet Embedding for Faster and Compact Model", "comments": "To appear in 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PointNet, which is the widely used point-wise embedding method and known as a\nuniversal approximator for continuous set functions, can process one million\npoints per second. Nevertheless, real-time inference for the recent development\nof high-performing sensors is still challenging with existing neural\nnetwork-based methods, including PointNet. In ordinary cases, the embedding\nfunction of PointNet behaves like a soft-indicator function that is activated\nwhen the input points exist in a certain local region of the input space.\nLeveraging this property, we reduce the computational costs of point-wise\nembedding by replacing the embedding function of PointNet with the\nsoft-indicator function by Gaussian kernels. Moreover, we show that the\nGaussian kernels also satisfy the universal approximation theorem that PointNet\nsatisfies. In experiments, we verify that our model using the Gaussian kernels\nachieves comparable results to baseline methods, but with much fewer\nfloating-point operations per sample up to 92% reduction from PointNet.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:35:44 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 07:07:49 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Suzuki", "Teppei", ""], ["Ozawa", "Keisuke", ""], ["Sekikawa", "Yusuke", ""]]}, {"id": "2007.15857", "submitter": "Yichen Shen", "authors": "Yichen Shen, Zhilu Zhang, Mert R. Sabuncu, Lin Sun", "title": "Real-Time Uncertainty Estimation in Computer Vision via\n  Uncertainty-Aware Distribution Distillation", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV), 2021; Equal contribution: Yichen Shen, Zhilu Zhang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrated estimates of uncertainty are critical for many real-world computer\nvision applications of deep learning. While there are several widely-used\nuncertainty estimation methods, dropout inference stands out for its simplicity\nand efficacy. This technique, however, requires multiple forward passes through\nthe network during inference and therefore can be too resource-intensive to be\ndeployed in real-time applications. We propose a simple, easy-to-optimize\ndistillation method for learning the conditional predictive distribution of a\npre-trained dropout model for fast, sample-free uncertainty estimation in\ncomputer vision tasks. We empirically test the effectiveness of the proposed\nmethod on both semantic segmentation and depth estimation tasks and demonstrate\nour method can significantly reduce the inference time, enabling real-time\nuncertainty quantification, while achieving improved quality of both the\nuncertainty estimates and predictive performance over the regular dropout\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:40:39 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 03:52:54 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Shen", "Yichen", ""], ["Zhang", "Zhilu", ""], ["Sabuncu", "Mert R.", ""], ["Sun", "Lin", ""]]}, {"id": "2007.15861", "submitter": "Dipesh Tamboli", "authors": "Sravanti Addepalli, Dipesh Tamboli, R. Venkatesh Babu, Biplab Banerjee", "title": "Saliency-driven Class Impressions for Feature Visualization of Deep\n  Neural Networks", "comments": "ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a data-free method of extracting Impressions of\neach class from the classifier's memory. The Deep Learning regime empowers\nclassifiers to extract distinct patterns (or features) of a given class from\ntraining data, which is the basis on which they generalize to unseen data.\nBefore deploying these models on critical applications, it is advantageous to\nvisualize the features considered to be essential for classification. Existing\nvisualization methods develop high confidence images consisting of both\nbackground and foreground features. This makes it hard to judge what the\ncrucial features of a given class are. In this work, we propose a\nsaliency-driven approach to visualize discriminative features that are\nconsidered most important for a given task. Another drawback of existing\nmethods is that confidence of the generated visualizations is increased by\ncreating multiple instances of the given class. We restrict the algorithm to\ndevelop a single object per image, which helps further in extracting features\nof high confidence and also results in better visualizations. We further\ndemonstrate the generation of negative images as naturally fused images of two\nor more classes.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 06:11:06 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Addepalli", "Sravanti", ""], ["Tamboli", "Dipesh", ""], ["Babu", "R. Venkatesh", ""], ["Banerjee", "Biplab", ""]]}, {"id": "2007.15874", "submitter": "Yehui Yang", "authors": "Dalu Yang, Yehui Yang, Tiantian Huang, Binghong Wu, Lei Wang, Yanwu Xu", "title": "Residual-CycleGAN based Camera Adaptation for Robust Diabetic\n  Retinopathy Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are extensive researches focusing on automated diabetic reti-nopathy\n(DR) detection from fundus images. However, the accuracy drop is ob-served when\napplying these models in real-world DR screening, where the fun-dus camera\nbrands are different from the ones used to capture the training im-ages. How\ncan we train a classification model on labeled fundus images ac-quired from\nonly one camera brand, yet still achieves good performance on im-ages taken by\nother brands of cameras? In this paper, we quantitatively verify the impact of\nfundus camera brands related domain shift on the performance of DR\nclassification models, from an experimental perspective. Further, we pro-pose\ncamera-oriented residual-CycleGAN to mitigate the camera brand differ-ence by\ndomain adaptation and achieve increased classification performance on target\ncamera images. Extensive ablation experiments on both the EyePACS da-taset and\na private dataset show that the camera brand difference can signifi-cantly\nimpact the classification performance and prove that our proposed meth-od can\neffectively improve the model performance on the target domain. We have\ninferred and labeled the camera brand for each image in the EyePACS da-taset\nand will publicize the camera brand labels for further research on domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:10:21 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yang", "Dalu", ""], ["Yang", "Yehui", ""], ["Huang", "Tiantian", ""], ["Wu", "Binghong", ""], ["Wang", "Lei", ""], ["Xu", "Yanwu", ""]]}, {"id": "2007.15883", "submitter": "Yehui Yang", "authors": "Xu Sun, Xingxing Cao, Yehui Yang, Lei Wang, Yanwu Xu", "title": "Robust Retinal Vessel Segmentation from a Data Augmentation Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is a fundamental step in screening, diagnosis,\nand treatment of various cardiovascular and ophthalmic diseases. Robustness is\none of the most critical requirements for practical utilization, since the test\nimages may be captured using different fundus cameras, or be affected by\nvarious pathological changes. We investigate this problem from a data\naugmentation perspective, with the merits of no additional training data or\ninference time. In this paper, we propose two new data augmentation modules,\nnamely, channel-wise random Gamma correction and channel-wise random vessel\naugmentation. Given a training color fundus image, the former applies random\ngamma correction on each color channel of the entire image, while the latter\nintentionally enhances or decreases only the fine-grained blood vessel regions\nusing morphological transformations. With the additional training samples\ngenerated by applying these two modules sequentially, a model could learn more\ninvariant and discriminating features against both global and local\ndisturbances. Experimental results on both real-world and synthetic datasets\ndemonstrate that our method can improve the performance and robustness of a\nclassic convolutional neural network architecture. Source codes are available\nhttps://github.com/PaddlePaddle/Research/tree/master/CV/robust_vessel_segmentation\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:37:14 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Sun", "Xu", ""], ["Cao", "Xingxing", ""], ["Yang", "Yehui", ""], ["Wang", "Lei", ""], ["Xu", "Yanwu", ""]]}, {"id": "2007.15897", "submitter": "Linchuan Xu", "authors": "Linchuan Xu, Jun Huang, Atsushi Nitanda, Ryo Asaoka, Kenji Yamanishi", "title": "A Novel Global Spatial Attention Mechanism in Convolutional Neural\n  Network for Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial attention has been introduced to convolutional neural networks (CNNs)\nfor improving both their performance and interpretability in visual tasks\nincluding image classification. The essence of the spatial attention is to\nlearn a weight map which represents the relative importance of activations\nwithin the same layer or channel. All existing attention mechanisms are local\nattentions in the sense that weight maps are image-specific. However, in the\nmedical field, there are cases that all the images should share the same weight\nmap because the set of images record the same kind of symptom related to the\nsame object and thereby share the same structural content. In this paper, we\nthus propose a novel global spatial attention mechanism in CNNs mainly for\nmedical image classification. The global weight map is instantiated by a\ndecision boundary between important pixels and unimportant pixels. And we\npropose to realize the decision boundary by a binary classifier in which the\nintensities of all images at a pixel are the features of the pixel. The binary\nclassification is integrated into an image classification CNN and is to be\noptimized together with the CNN. Experiments on two medical image datasets and\none facial expression dataset showed that with the proposed attention, not only\nthe performance of four powerful CNNs which are GoogleNet, VGG, ResNet, and\nDenseNet can be improved, but also meaningful attended regions can be obtained,\nwhich is beneficial for understanding the content of images of a domain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 08:24:34 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Xu", "Linchuan", ""], ["Huang", "Jun", ""], ["Nitanda", "Atsushi", ""], ["Asaoka", "Ryo", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "2007.15916", "submitter": "Justin Van Der Hout", "authors": "Justin van der Hout, Zolt\\'an D'Haese, Mark Hasegawa-Johnson, Odette\n  Scharenborg", "title": "Evaluating Automatically Generated Phoneme Captions for Images", "comments": "Accepted at Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image2Speech is the relatively new task of generating a spoken description of\nan image. This paper presents an investigation into the evaluation of this\ntask. For this, first an Image2Speech system was implemented which generates\nimage captions consisting of phoneme sequences. This system outperformed the\noriginal Image2Speech system on the Flickr8k corpus. Subsequently, these\nphoneme captions were converted into sentences of words. The captions were\nrated by human evaluators for their goodness of describing the image. Finally,\nseveral objective metric scores of the results were correlated with these human\nratings. Although BLEU4 does not perfectly correlate with human ratings, it\nobtained the highest correlation among the investigated metrics, and is the\nbest currently existing metric for the Image2Speech task. Current metrics are\nlimited by the fact that they assume their input to be words. A more\nappropriate metric for the Image2Speech task should assume its input to be\nparts of words, i.e. phonemes, instead.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:21:13 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["van der Hout", "Justin", ""], ["D'Haese", "Zolt\u00e1n", ""], ["Hasegawa-Johnson", "Mark", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2007.15920", "submitter": "Eleni Charou Dr", "authors": "Maria Karatzoglidi, Georgios Felekis and Eleni Charou", "title": "Neural Style Transfer for Remote Sensing", "comments": "10 pages, 5 figures, presented in 2nd Greek Remote Sensing Workshop\n  RSSAC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known technique outlined in the paper of Leon A. Gatys et al., A\nNeural Algorithm of Artistic Style, has become a trending topic both in\nacademic literature and industrial applications. Neural Style Transfer (NST)\nconstitutes an essential tool for a wide range of applications, such as\nartistic stylization of 2D images, user-assisted creation tools and production\ntools for entertainment applications. The purpose of this study is to present a\nmethod for creating artistic maps from satellite images, based on the NST\nalgorithm. This method includes three basic steps (i) application of semantic\nimage segmentation on the original satellite image, dividing its content into\nclasses (i.e. land, water), (ii) application of neural style transfer for each\nclass and (iii) creation of a collage, i.e. an artistic image consisting of a\ncombination of the two stylized image generated on the previous step.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:30:48 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Karatzoglidi", "Maria", ""], ["Felekis", "Georgios", ""], ["Charou", "Eleni", ""]]}, {"id": "2007.15958", "submitter": "Margit Antal", "authors": "Szil\\'ard Nemes, Margit Antal", "title": "Feature Learning for Accelerometer based Gait Recognition", "comments": "23 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in pattern matching, such as speech or object recognition\nsupport the viability of feature learning with deep learning solutions for gait\nrecognition. Past papers have evaluated deep neural networks trained in a\nsupervised manner for this task. In this work, we investigated both supervised\nand unsupervised approaches. Feature extractors using similar architectures\nincorporated into end-to-end models and autoencoders were compared based on\ntheir ability of learning good representations for a gait verification system.\nBoth feature extractors were trained on the IDNet dataset then used for feature\nextraction on the ZJU-GaitAccel dataset. Results show that autoencoders are\nvery close to discriminative end-to-end models with regards to their feature\nlearning ability and that fully convolutional models are able to learn good\nfeature representations, regardless of the training strategy.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 10:58:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Nemes", "Szil\u00e1rd", ""], ["Antal", "Margit", ""]]}, {"id": "2007.15963", "submitter": "Le Zhang", "authors": "Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga\n  Ciccarelli, Frederik Barkhof and Daniel C. Alexander", "title": "Disentangling Human Error from the Ground Truth in Segmentation of\n  Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen increasing use of supervised learning methods for\nsegmentation tasks. However, the predictive performance of these algorithms\ndepends on the quality of labels. This problem is particularly pertinent in the\nmedical image domain, where both the annotation cost and inter-observer\nvariability are high. In a typical label acquisition process, different human\nexperts provide their estimates of the \"true\" segmentation labels under the\ninfluence of their own biases and competence levels. Treating these noisy\nlabels blindly as the ground truth limits the performance that automatic\nsegmentation algorithms can achieve. In this work, we present a method for\njointly learning, from purely noisy observations alone, the reliability of\nindividual annotators and the true segmentation label distributions, using two\ncoupled CNNs. The separation of the two is achieved by encouraging the\nestimated annotators to be maximally unreliable while achieving high fidelity\nwith the noisy training data. We first define a toy segmentation dataset based\non MNIST and study the properties of the proposed algorithm. We then\ndemonstrate the utility of the method on three public medical imaging\nsegmentation datasets with simulated (when necessary) and real diverse\nannotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours);\n3) LIDC-IDRI (lung abnormalities). In all cases, our method outperforms\ncompeting methods and relevant baselines particularly in cases where the number\nof annotations is small and the amount of disagreement is large. The\nexperiments also show strong ability to capture the complex spatial\ncharacteristics of annotators' mistakes.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 11:03:12 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 12:22:02 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 12:18:45 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 10:26:51 GMT"}, {"version": "v5", "created": "Fri, 23 Oct 2020 12:15:04 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhang", "Le", ""], ["Tanno", "Ryutaro", ""], ["Xu", "Mou-Cheng", ""], ["Jin", "Chen", ""], ["Jacob", "Joseph", ""], ["Ciccarelli", "Olga", ""], ["Barkhof", "Frederik", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "2007.16005", "submitter": "Benjamin Busam", "authors": "Patrick Ruhkamp and Ruiqi Gong and Nassir Navab and Benjamin Busam", "title": "DynaMiTe: A Dynamic Local Motion Model with Temporal Constraints for\n  Robust Real-Time Feature Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature based visual odometry and SLAM methods require accurate and fast\ncorrespondence matching between consecutive image frames for precise camera\npose estimation in real-time. Current feature matching pipelines either rely\nsolely on the descriptive capabilities of the feature extractor or need\ncomputationally complex optimization schemes. We present the lightweight\npipeline DynaMiTe, which is agnostic to the descriptor input and leverages\nspatial-temporal cues with efficient statistical measures. The theoretical\nbackbone of the method lies within a probabilistic formulation of feature\nmatching and the respective study of physically motivated constraints. A\ndynamically adaptable local motion model encapsulates groups of features in an\nefficient data structure. Temporal constraints transfer information of the\nlocal motion model across time, thus additionally reducing the search space\ncomplexity for matching. DynaMiTe achieves superior results both in terms of\nmatching accuracy and camera pose estimation with high frame rates,\noutperforming state-of-the-art matching methods while being computationally\nmore efficient.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 12:18:18 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ruhkamp", "Patrick", ""], ["Gong", "Ruiqi", ""], ["Navab", "Nassir", ""], ["Busam", "Benjamin", ""]]}, {"id": "2007.16032", "submitter": "Junyu Gao", "authors": "Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan", "title": "Pixel-wise Crowd Understanding via Synthetic Data", "comments": "Accepted by IJCV. arXiv admin note: text overlap with\n  arXiv:1903.03303", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd analysis via computer vision techniques is an important topic in the\nfield of video surveillance, which has wide-spread applications including crowd\nmonitoring, public safety, space design and so on. Pixel-wise crowd\nunderstanding is the most fundamental task in crowd analysis because of its\nfiner results for video sequences or still images than other analysis tasks.\nUnfortunately, pixel-level understanding needs a large amount of labeled\ntraining data. Annotating them is an expensive work, which causes that current\ncrowd datasets are small. As a result, most algorithms suffer from over-fitting\nto varying degrees. In this paper, take crowd counting and segmentation as\nexamples from the pixel-wise crowd understanding, we attempt to remedy these\nproblems from two aspects, namely data and methodology. Firstly, we develop a\nfree data collector and labeler to generate synthetic and labeled crowd scenes\nin a computer game, Grand Theft Auto V. Then we use it to construct a\nlarge-scale, diverse synthetic crowd dataset, which is named as \"GCC Dataset\".\nSecondly, we propose two simple methods to improve the performance of crowd\nunderstanding via exploiting the synthetic data. To be specific, 1) supervised\ncrowd understanding: pre-train a crowd analysis model on the synthetic data,\nthen fine-tune it using the real data and labels, which makes the model perform\nbetter on the real world; 2) crowd understanding via domain adaptation:\ntranslate the synthetic data to photo-realistic images, then train the model on\ntranslated data and labels. As a result, the trained model works well in real\ncrowd scenes.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:37:51 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 01:18:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Qi", ""], ["Gao", "Junyu", ""], ["Lin", "Wei", ""], ["Yuan", "Yuan", ""]]}, {"id": "2007.16054", "submitter": "Francesco Cricri", "authors": "Nannan Zou and Honglei Zhang and Francesco Cricri and Hamed R.\n  Tavakoli and Jani Lainema and Miska Hannuksela and Emre Aksu and Esa Rahtu", "title": "Learning to Learn to Compress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an end-to-end meta-learned system for image\ncompression. Traditional machine learning based approaches to image compression\ntrain one or more neural network for generalization performance. However, at\ninference time, the encoder or the latent tensor output by the encoder can be\noptimized for each test image. This optimization can be regarded as a form of\nadaptation or benevolent overfitting to the input content. In order to reduce\nthe gap between training and inference conditions, we propose a new training\nparadigm for learned image compression, which is based on meta-learning. In a\nfirst phase, the neural networks are trained normally. In a second phase, the\nModel-Agnostic Meta-learning approach is adapted to the specific case of image\ncompression, where the inner-loop performs latent tensor overfitting, and the\nouter loop updates both encoder and decoder neural networks based on the\noverfitting performance. Furthermore, after meta-learning, we propose to\noverfit and cluster the bias terms of the decoder on training image patches, so\nthat at inference time the optimal content-specific bias terms can be selected\nat encoder-side. Finally, we propose a new probability model for lossless\ncompression, which combines concepts from both multi-scale and super-resolution\nprobability model approaches. We show the benefits of all our proposed ideas\nvia carefully designed experiments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:13:53 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 16:18:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zou", "Nannan", ""], ["Zhang", "Honglei", ""], ["Cricri", "Francesco", ""], ["Tavakoli", "Hamed R.", ""], ["Lainema", "Jani", ""], ["Hannuksela", "Miska", ""], ["Aksu", "Emre", ""], ["Rahtu", "Esa", ""]]}, {"id": "2007.16072", "submitter": "Julian Wiederer", "authors": "Julian Wiederer, Arij Bouazizi, Ulrich Kressel and Vasileios\n  Belagiannis", "title": "Traffic Control Gesture Recognition for Autonomous Vehicles", "comments": "8 pages, 8 figures, 3 tables, accepted by IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A car driver knows how to react on the gestures of the traffic officers.\nClearly, this is not the case for the autonomous vehicle, unless it has road\ntraffic control gesture recognition functionalities. In this work, we address\nthe limitation of the existing autonomous driving datasets to provide learning\ndata for traffic control gesture recognition. We introduce a dataset that is\nbased on 3D body skeleton input to perform traffic control gesture\nclassification on every time step. Our dataset consists of 250 sequences from\nseveral actors, ranging from 16 to 90 seconds per sequence. To evaluate our\ndataset, we propose eight sequential processing models based on deep neural\nnetworks such as recurrent networks, attention mechanism, temporal\nconvolutional networks and graph convolutional networks. We present an\nextensive evaluation and analysis of all approaches for our dataset, as well as\nreal-world quantitative evaluation. The code and dataset is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:40:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wiederer", "Julian", ""], ["Bouazizi", "Arij", ""], ["Kressel", "Ulrich", ""], ["Belagiannis", "Vasileios", ""]]}, {"id": "2007.16100", "submitter": "Zhijian Liu", "authors": "Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui\n  Wang, Song Han", "title": "Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution", "comments": "ECCV 2020. The first two authors contributed equally to this work.\n  Project page: http://spvnas.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving cars need to understand 3D scenes efficiently and accurately in\norder to drive safely. Given the limited hardware resources, existing 3D\nperception models are not able to recognize small instances (e.g., pedestrians,\ncyclists) very well due to the low-resolution voxelization and aggressive\ndownsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv),\na lightweight 3D module that equips the vanilla Sparse Convolution with the\nhigh-resolution point-based branch. With negligible overhead, this point-based\nbranch is able to preserve the fine details even from large outdoor scenes. To\nexplore the spectrum of efficient 3D models, we first define a flexible\narchitecture design space based on SPVConv, and we then present 3D Neural\nArchitecture Search (3D-NAS) to search the optimal network architecture over\nthis diverse design space efficiently and effectively. Experimental results\nvalidate that the resulting SPVNAS model is fast and accurate: it outperforms\nthe state-of-the-art MinkowskiNet by 3.3%, ranking 1st on the competitive\nSemanticKITTI leaderboard. It also achieves 8x computation reduction and 3x\nmeasured speedup over MinkowskiNet with higher accuracy. Finally, we transfer\nour method to 3D object detection, and it achieves consistent improvements over\nthe one-stage detection baseline on KITTI.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:27:27 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 13:53:20 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Tang", "Haotian", ""], ["Liu", "Zhijian", ""], ["Zhao", "Shengyu", ""], ["Lin", "Yujun", ""], ["Lin", "Ji", ""], ["Wang", "Hanrui", ""], ["Han", "Song", ""]]}, {"id": "2007.16102", "submitter": "Amelia Jim\\'enez-S\\'anchez", "authors": "Amelia Jim\\'enez-S\\'anchez, Diana Mateus, Sonja Kirchhoff, Chlodwig\n  Kirchhoff, Peter Biberthaler, Nassir Navab, Miguel A. Gonz\\'alez Ballester,\n  Gemma Piella", "title": "Curriculum learning for annotation-efficient medical image analysis:\n  scheduling data with prior knowledge and uncertainty", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) for multi-class classification require\ntraining on large, representative, and high quality annotated datasets.\nHowever, in the field of medical imaging, data and annotations are both\ndifficult and expensive to acquire. Moreover, they frequently suffer from\nhighly imbalanced distributions, and potentially noisy labels due to intra- or\ninter-expert disagreement. To deal with such challenges, we propose a unified\ncurriculum learning framework to schedule the order and pace of the training\nsamples presented to the optimizer. Our novel framework reunites three\nstrategies consisting of individually weighting training samples, reordering\nthe training set, or sampling subsets of data. The core of these strategies is\na scoring function ranking the training samples according to either difficulty\nor uncertainty. We define the scoring function from domain-specific prior\nknowledge or by directly measuring the uncertainty in the predictions. We\nperform a variety of experiments with a clinical dataset for the multi-class\nclassification of proximal femur fractures and the publicly available MNIST\ndataset. Our results show that the sequence and weight of the training samples\nplay an important role in the optimization process of CNNs. Proximal femur\nfracture classification is improved up to the performance of experienced trauma\nsurgeons. We further demonstrate the benefits of our unified curriculum\nlearning method for three controlled and challenging digit recognition\nscenarios: with limited amounts of data, under class-imbalance, and in the\npresence of label noise.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:28:33 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Amelia", ""], ["Mateus", "Diana", ""], ["Kirchhoff", "Sonja", ""], ["Kirchhoff", "Chlodwig", ""], ["Biberthaler", "Peter", ""], ["Navab", "Nassir", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""], ["Piella", "Gemma", ""]]}, {"id": "2007.16103", "submitter": "Yinghuan Shi", "authors": "Yinghuan Shi and Wanqi Yang and Kim-Han Thung and Hao Wang and Yang\n  Gao and Yang Pan and Li Zhang and Dinggang Shen", "title": "Learning-based Computer-aided Prescription Model for Parkinson's\n  Disease: A Data-driven Perspective", "comments": "IEEE JBHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a novel problem: \"automatic prescription\nrecommendation for PD patients.\" To realize this goal, we first build a dataset\nby collecting 1) symptoms of PD patients, and 2) their prescription drug\nprovided by neurologists. Then, we build a novel computer-aided prescription\nmodel by learning the relation between observed symptoms and prescription drug.\nFinally, for the new coming patients, we could recommend (predict) suitable\nprescription drug on their observed symptoms by our prescription model. From\nthe methodology part, our proposed model, namely Prescription viA Learning\nlAtent Symptoms (PALAS), could recommend prescription using the multi-modality\nrepresentation of the data. In PALAS, a latent symptom space is learned to\nbetter model the relationship between symptoms and prescription drug, as there\nis a large semantic gap between them. Moreover, we present an efficient\nalternating optimization method for PALAS. We evaluated our method using the\ndata collected from 136 PD patients at Nanjing Brain Hospital, which can be\nregarded as a large dataset in PD research community. The experimental results\ndemonstrate the effectiveness and clinical potential of our method in this\nrecommendation task, if compared with other competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:34:35 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Shi", "Yinghuan", ""], ["Yang", "Wanqi", ""], ["Thung", "Kim-Han", ""], ["Wang", "Hao", ""], ["Gao", "Yang", ""], ["Pan", "Yang", ""], ["Zhang", "Li", ""], ["Shen", "Dinggang", ""]]}, {"id": "2007.16112", "submitter": "Zhiwu Huang", "authors": "Yan Wu, Aoming Liu, Zhiwu Huang, Siwei Zhang, Luc Van Gool", "title": "Neural Architecture Search as Sparse Supernet", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at enlarging the problem of Neural Architecture Search (NAS)\nfrom Single-Path and Multi-Path Search to automated Mixed-Path Search. In\nparticular, we model the NAS problem as a sparse supernet using a new\ncontinuous architecture representation with a mixture of sparsity constraints.\nThe sparse supernet enables us to automatically achieve sparsely-mixed paths\nupon a compact set of nodes. To optimize the proposed sparse supernet, we\nexploit a hierarchical accelerated proximal gradient algorithm within a\nbi-level optimization framework. Extensive experiments on Convolutional Neural\nNetwork and Recurrent Neural Network search demonstrate that the proposed\nmethod is capable of searching for compact, general and powerful neural\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:51:52 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:35:16 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wu", "Yan", ""], ["Liu", "Aoming", ""], ["Huang", "Zhiwu", ""], ["Zhang", "Siwei", ""], ["Van Gool", "Luc", ""]]}, {"id": "2007.16118", "submitter": "Tong Wu", "authors": "Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, Yu Wang", "title": "Physical Adversarial Attack on Vehicle Detector in the Carla Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the issue of physical adversarial examples for\nobject detectors in the wild. Specifically, we proposed to generate adversarial\npatterns to be applied on vehicle surface so that it's not recognizable by\ndetectors in the photo-realistic Carla simulator. Our approach contains two\nmain techniques, an Enlarge-and-Repeat process and a Discrete Searching method,\nto craft mosaic-like adversarial vehicle textures without access to neither the\nmodel weight of the detector nor a differential rendering procedure. The\nexperimental results demonstrate the effectiveness of our approach in the\nsimulator.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 15:04:45 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 12:36:01 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wu", "Tong", ""], ["Ning", "Xuefei", ""], ["Li", "Wenshuo", ""], ["Huang", "Ranran", ""], ["Yang", "Huazhong", ""], ["Wang", "Yu", ""]]}, {"id": "2007.16124", "submitter": "Xin Xu", "authors": "Xin Xu, Shiqin Wang, Zheng Wang, Xiaolong Zhang, and Ruimin Hu", "title": "Exploring Image Enhancement for Salient Object Detection in Low Light\n  Images", "comments": "Appearing at ACM Transactions on Multimedia Computing,\n  Communications, and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low light images captured in a non-uniform illumination environment usually\nare degraded with the scene depth and the corresponding environment lights.\nThis degradation results in severe object information loss in the degraded\nimage modality, which makes the salient object detection more challenging due\nto low contrast property and artificial light influence. However, existing\nsalient object detection models are developed based on the assumption that the\nimages are captured under a sufficient brightness environment, which is\nimpractical in real-world scenarios. In this work, we propose an image\nenhancement approach to facilitate the salient object detection in low light\nimages. The proposed model directly embeds the physical lighting model into the\ndeep neural network to describe the degradation of low light images, in which\nthe environment light is treated as a point-wise variate and changes with local\ncontent. Moreover, a Non-Local-Block Layer is utilized to capture the\ndifference of local content of an object against its local neighborhood\nfavoring regions. To quantitative evaluation, we construct a low light Images\ndataset with pixel-level human-labeled ground-truth annotations and report\npromising results on four public datasets and our benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 15:09:03 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Xu", "Xin", ""], ["Wang", "Shiqin", ""], ["Wang", "Zheng", ""], ["Zhang", "Xiaolong", ""], ["Hu", "Ruimin", ""]]}, {"id": "2007.16133", "submitter": "Xin Yang", "authors": "Junxiong Yu, Chaoyu Chen, Xin Yang, Yi Wang, Dan Yan, Jianxing Zhang,\n  Dong Ni", "title": "Computer-aided Tumor Diagnosis in Automated Breast Ultrasound using 3D\n  Detection Network", "comments": "Early Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated breast ultrasound (ABUS) is a new and promising imaging modality\nfor breast cancer detection and diagnosis, which could provide intuitive 3D\ninformation and coronal plane information with great diagnostic value. However,\nmanually screening and diagnosing tumors from ABUS images is very\ntime-consuming and overlooks of abnormalities may happen. In this study, we\npropose a novel two-stage 3D detection network for locating suspected lesion\nareas and further classifying lesions as benign or malignant tumors.\nSpecifically, we propose a 3D detection network rather than frequently-used\nsegmentation network to locate lesions in ABUS images, thus our network can\nmake full use of the spatial context information in ABUS images. A novel\nsimilarity loss is designed to effectively distinguish lesions from background.\nThen a classification network is employed to identify the located lesions as\nbenign or malignant. An IoU-balanced classification loss is adopted to improve\nthe correlation between classification and localization task. The efficacy of\nour network is verified from a collected dataset of 418 patients with 145\nbenign tumors and 273 malignant tumors. Experiments show our network attains a\nsensitivity of 97.66% with 1.23 false positives (FPs), and has an area under\nthe curve(AUC) value of 0.8720.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 15:25:07 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yu", "Junxiong", ""], ["Chen", "Chaoyu", ""], ["Yang", "Xin", ""], ["Wang", "Yi", ""], ["Yan", "Dan", ""], ["Zhang", "Jianxing", ""], ["Ni", "Dong", ""]]}, {"id": "2007.16149", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes and Lu\\'is A. Alexandre", "title": "HMCNAS: Neural Architecture Search using Hidden Markov Chains and\n  Bayesian Optimization", "comments": "9 pages, 1 figure, 2 tables, neural architecture search, macro-search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search has achieved state-of-the-art performance in a\nvariety of tasks, out-performing human-designed networks. However, many\nassumptions, that require human definition, related with the problems being\nsolved or the models generated are still needed: final model architectures,\nnumber of layers to be sampled, forced operations, small search spaces, which\nultimately contributes to having models with higher performances at the cost of\ninducing bias into the system. In this paper, we propose HMCNAS, which is\ncomposed of two novel components: i) a method that leverages information about\nhuman-designed models to autonomously generate a complex search space, and ii)\nan Evolutionary Algorithm with Bayesian Optimization that is capable of\ngenerating competitive CNNs from scratch, without relying on human-defined\nparameters or small search spaces. The experimental results show that the\nproposed approach results in competitive architectures obtained in a very short\ntime. HMCNAS provides a step towards generalizing NAS, by providing a way to\ncreate competitive models, without requiring any human knowledge about the\nspecific task.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:04:08 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lopes", "Vasco", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "2007.16189", "submitter": "Emin Orhan", "authors": "A. Emin Orhan, Vaibhav V. Gupta, Brenden M. Lake", "title": "Self-supervised learning through the eyes of a child", "comments": "Published as a conference paper at NeurIPS 2020; v3 adds a reference,\n  fixes a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within months of birth, children develop meaningful expectations about the\nworld around them. How much of this early knowledge can be explained through\ngeneric learning mechanisms applied to sensory data, and how much of it\nrequires more substantive innate inductive biases? Addressing this fundamental\nquestion in its full generality is currently infeasible, but we can hope to\nmake real progress in more narrowly defined domains, such as the development of\nhigh-level visual categories, thanks to improvements in data collecting\ntechnology and recent progress in deep learning. In this paper, our goal is\nprecisely to achieve such progress by utilizing modern self-supervised deep\nlearning methods and a recent longitudinal, egocentric video dataset recorded\nfrom the perspective of three young children (Sullivan et al., 2020). Our\nresults demonstrate the emergence of powerful, high-level visual\nrepresentations from developmentally realistic natural videos using generic\nself-supervised learning objectives.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:33:45 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 18:25:27 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 18:24:16 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Orhan", "A. Emin", ""], ["Gupta", "Vaibhav V.", ""], ["Lake", "Brenden M.", ""]]}, {"id": "2007.16195", "submitter": "Mohammed Hamzah Abed", "authors": "Mohammed Hamzah Abed, Ali H. Alsaeedi, Ali D. Alfoudi, Abayomi M.\n  Otebolaku, Yasmeen Sajid Razooqi", "title": "Palm Vein Identification based on hybrid features selection model", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Palm vein identification (PVI) is a modern biometric security technique used\nfor increasing security and authentication systems. The key characteristics of\npalm vein patterns include, its uniqueness to each individual, unforgettable,\nnon-intrusive and cannot be taken by an unauthorized person. However, the\nextracted features from the palm vein pattern are huge with high redundancy. In\nthis paper, we propose a combine model of two-Dimensional Discrete Wavelet\nTransform, Principal Component Analysis (PCA), and Particle Swarm Optimization\n(PSO) (2D-DWTPP) to enhance prediction of vein palm patterns. The 2D-DWT\nExtracts features from palm vein images, PCA reduces the redundancy in palm\nvein features. The system has been trained in selecting high reverent features\nbased on the wrapper model. The PSO feeds wrapper model by an optimal subset of\nfeatures. The proposed system uses four classifiers as an objective function to\ndetermine VPI which include Support Vector Machine (SVM), K Nearest Neighbor\n(KNN), Decision Tree (DT) and Na\\\"ive Bayes (NB). The empirical result shows\nthe proposed system Iit satisfied best results with SVM. The proposed 2D-DWTPP\nmodel has been evaluated and the results shown remarkable efficiency in\ncomparison with Alexnet and classifier without feature selection.\nExperimentally, our model has better accuracy reflected by (98.65) while\nAlexnet has (63.5) and applied classifier without feature selection has\n(78.79).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:46:57 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Abed", "Mohammed Hamzah", ""], ["Alsaeedi", "Ali H.", ""], ["Alfoudi", "Ali D.", ""], ["Otebolaku", "Abayomi M.", ""], ["Razooqi", "Yasmeen Sajid", ""]]}, {"id": "2007.16198", "submitter": "Vishal Mandal", "authors": "Vishal Mandal and Yaw Adu-Gyamfi", "title": "Object Detection and Tracking Algorithms for Vehicle Counting: A\n  Comparative Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancement in the field of deep learning and high performance\ncomputing has highly augmented the scope of video based vehicle counting\nsystem. In this paper, the authors deploy several state of the art object\ndetection and tracking algorithms to detect and track different classes of\nvehicles in their regions of interest (ROI). The goal of correctly detecting\nand tracking vehicles' in their ROI is to obtain an accurate vehicle count.\nMultiple combinations of object detection models coupled with different\ntracking systems are applied to access the best vehicle counting framework. The\nmodels' addresses challenges associated to different weather conditions,\nocclusion and low-light settings and efficiently extracts vehicle information\nand trajectories through its computationally rich training and feedback cycles.\nThe automatic vehicle counts resulting from all the model combinations are\nvalidated and compared against the manually counted ground truths of over 9\nhours' traffic video data obtained from the Louisiana Department of\nTransportation and Development. Experimental results demonstrate that the\ncombination of CenterNet and Deep SORT, Detectron2 and Deep SORT, and YOLOv4\nand Deep SORT produced the best overall counting percentage for all vehicles.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:49:27 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Mandal", "Vishal", ""], ["Adu-Gyamfi", "Yaw", ""]]}]