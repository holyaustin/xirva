[{"id": "1101.0139", "submitter": "Prasanta K. Panigrahi", "authors": "Madhur Srivastava, Prateek Katiyar, Yashwant Yashu, Satish K. Singha\n  and Prasanta K. Panigrahi", "title": "A Fast Statistical Method for Multilevel Thresholding in Wavelet Domain", "comments": "33 pages, 10 figures, 7 tables, written with double spacing and\n  larger font", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is proposed for the segmentation of image into multiple levels\nusing mean and standard deviation in the wavelet domain. The procedure provides\nfor variable size segmentation with bigger block size around the mean, and\nhaving smaller blocks at the ends of histogram plot of each horizontal,\nvertical and diagonal components, while for the approximation component it\nprovides for finer block size around the mean, and larger blocks at the ends of\nhistogram plot coefficients. It is found that the proposed algorithm has\nsignificantly less time complexity, achieves superior PSNR and Structural\nSimilarity Measurement Index as compared to similar space domain algorithms[1].\nIn the process it highlights finer image structures not perceptible in the\noriginal image. It is worth emphasizing that after the segmentation only 16 (at\nthreshold level 3) wavelet coefficients captures the significant variation of\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 19:12:48 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Srivastava", "Madhur", ""], ["Katiyar", "Prateek", ""], ["Yashu", "Yashwant", ""], ["Singha", "Satish K.", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1101.0237", "submitter": "Michael Lew", "authors": "E.R. Gast and Michael S. Lew", "title": "A Framework for Real-Time Face and Facial Feature Tracking using Optical\n  Flow Pre-estimation and Template Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": "LML20100401", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a framework for tracking head movements and capturing the\nmovements of the mouth and both the eyebrows in real-time. We present a head\ntracker which is a combination of a optical flow and a template based tracker.\nThe estimation of the optical flow head tracker is used as starting point for\nthe template tracker which fine-tunes the head estimation. This approach\ntogether with re-updating the optical flow points prevents the head tracker\nfrom drifting. This combination together with our switching scheme, makes our\ntracker very robust against fast movement and motion-blur. We also propose a\nway to reduce the influence of partial occlusion of the head. In both the\noptical flow and the template based tracker we identify and exclude occluded\npoints.\n", "versions": [{"version": "v1", "created": "Fri, 31 Dec 2010 12:16:29 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Gast", "E. R.", ""], ["Lew", "Michael S.", ""]]}, {"id": "1101.0242", "submitter": "Michael Lew", "authors": "Xiaojing Chen and Michael S. Lew", "title": "Binary and nonbinary description of hypointensity in human brain MR\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": "LML20080101", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accumulating evidence has shown that iron is involved in the mechanism\nunderlying many neurodegenerative diseases, such as Alzheimer's disease,\nParkinson's disease and Huntington's disease. Abnormal (higher) iron\naccumulation has been detected in the brains of most neurodegenerative\npatients, especially in the basal ganglia region. Presence of iron leads to\nchanges in MR signal in both magnitude and phase. Accordingly, tissues with\nhigh iron concentration appear hypo-intense (darker than usual) in MR\ncontrasts. In this report, we proposed an improved binary hypointensity\ndescription and a novel nonbinary hypointensity description based on principle\ncomponents analysis. Moreover, Kendall's rank correlation coefficient was used\nto compare the complementary and redundant information provided by the two\nmethods in order to better understand the individual descriptions of iron\naccumulation in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Dec 2010 12:26:04 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Chen", "Xiaojing", ""], ["Lew", "Michael S.", ""]]}, {"id": "1101.0384", "submitter": "Jamal Ahmad Dargham", "authors": "Chelsia Amy Doukim, Jamal Ahmad Dargham, Ali Chekima and Sigeru Omatu", "title": "Combining Neural Networks for Skin Detection", "comments": "11 pages, journal articles; ISSN 0976 - 710X, Academy & Industry\n  Research Collaboration Center, December 2010 available at\n  http://www.airccse.org/journal/sipij/currentissue.html", "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.1, No.2, PP. 1-11", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two types of combining strategies were evaluated namely combining skin\nfeatures and combining skin classifiers. Several combining rules were applied\nwhere the outputs of the skin classifiers are combined using binary operators\nsuch as the AND and the OR operators, \"Voting\", \"Sum of Weights\" and a new\nneural network. Three chrominance components from the YCbCr colour space that\ngave the highest correct detection on their single feature MLP were selected as\nthe combining parameters. A major issue in designing a MLP neural network is to\ndetermine the optimal number of hidden units given a set of training patterns.\nTherefore, a \"coarse to fine search\" method to find the number of neurons in\nthe hidden layer is proposed. The strategy of combining Cb/Cr and Cr features\nimproved the correct detection by 3.01% compared to the best single feature MLP\ngiven by Cb-Cr. The strategy of combining the outputs of three skin classifiers\nusing the \"Sum of Weights\" rule further improved the correct detection by 4.38%\ncompared to the best single feature MLP.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jan 2011 04:53:22 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Doukim", "Chelsia Amy", ""], ["Dargham", "Jamal Ahmad", ""], ["Chekima", "Ali", ""], ["Omatu", "Sigeru", ""]]}, {"id": "1101.0457", "submitter": "Ayatullah Faruk Mollah", "authors": "Ayatullah Faruk Mollah, Subhadip Basu, Mita Nasipuri", "title": "Segmentation of Camera Captured Business Card Images for Mobile Devices", "comments": null, "journal-ref": "International Journal of Computer Science and Applications, Vol.\n  1, Issue 1, pp. 33-37, June 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to huge deformation in the camera captured images, variety in nature of\nthe business cards and the computational constraints of the mobile devices,\ndesign of an efficient Business Card Reader (BCR) is challenging to the\nresearchers. Extraction of text regions and segmenting them into characters is\none of such challenges. In this paper, we have presented an efficient character\nsegmentation technique for business card images captured by a cell-phone\ncamera, designed in our present work towards developing an efficient BCR. At\nfirst, text regions are extracted from the card images and then the skewed ones\nare corrected using a computationally efficient skew correction technique. At\nlast, these skew corrected text regions are segmented into lines and characters\nbased on horizontal and vertical histogram. Experiments show that the present\ntechnique is efficient and applicable for mobile devices, and the mean\nsegmentation accuracy of 97.48% is achieved with 3 mega-pixel (500-600 dpi)\nimages. It takes only 1.1 seconds for segmentation including all the\npreprocessing steps on a moderately powerful notebook (DualCore T2370, 1.73\nGHz, 1GB RAM, 1MB L2 Cache).\n", "versions": [{"version": "v1", "created": "Mon, 3 Jan 2011 06:15:08 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Mollah", "Ayatullah Faruk", ""], ["Basu", "Subhadip", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1101.1266", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain and Klaus Obermayer", "title": "Extending Bron Kerbosch for Solving the Maximum Weight Clique Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution extends the Bron Kerbosch algorithm for solving the maximum\nweight clique problem, where continuous-valued weights are assigned to both,\nvertices and edges. We applied the proposed algorithm to graph matching\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jan 2011 17:44:46 GMT"}], "update_date": "2011-01-07", "authors_parsed": [["Jain", "Brijnesh", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1101.1602", "submitter": "Jasni Mohamad Zain", "authors": "Nor Amizam Jusoh, Jasni Mohamad Zain", "title": "Application of Freeman Chain Codes: An Alternative Recognition Technique\n  for Malaysian Car Plates", "comments": "6 pages, 8 figures", "journal-ref": "International Journal of Computer Science and Network Security,\n  Vol. 9 No. 11 pp. 222-227, November 2009, ISSN 1738-7906", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various applications of car plate recognition systems have been developed\nusing various kinds of methods and techniques by researchers all over the\nworld. The applications developed were only suitable for specific country due\nto its standard specification endorsed by the transport department of\nparticular countries. The Road Transport Department of Malaysia also has\nendorsed a specification for car plates that includes the font and size of\ncharacters that must be followed by car owners. However, there are cases where\nthis specification is not followed. Several applications have been developed in\nMalaysia to overcome this problem. However, there is still problem in achieving\n100% recognition accuracy. This paper is mainly focused on conducting an\nexperiment using chain codes technique to perform recognition for different\ntypes of fonts used in Malaysian car plates.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jan 2011 16:22:20 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Jusoh", "Nor Amizam", ""], ["Zain", "Jasni Mohamad", ""]]}, {"id": "1101.2243", "submitter": "Chenguang Lu", "authors": "Chenguang Lu", "title": "Illustrating Color Evolution and Color Blindness by the Decoding Model\n  of Color Vision", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A symmetrical model of color vision, the decoding model as a new version of\nzone model, was introduced. The model adopts new continuous-valued logic and\nworks in a way very similar to the way a 3-8 decoder in a numerical circuit\nworks. By the decoding model, Young and Helmholtz's tri-pigment theory and\nHering's opponent theory are unified more naturally; opponent process, color\nevolution, and color blindness are illustrated more concisely. According to the\ndecoding model, we can obtain a transform from RGB system to HSV system, which\nis formally identical to the popular transform for computer graphics provided\nby Smith (1978). Advantages, problems, and physiological tests of the decoding\nmodel are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 12 Dec 2010 20:49:00 GMT"}], "update_date": "2011-01-13", "authors_parsed": [["Lu", "Chenguang", ""]]}, {"id": "1101.2286", "submitter": "St\\'ephane Mallat", "authors": "St\\'ephane Mallat", "title": "Group Invariant Scattering", "comments": "78 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper constructs translation invariant operators on L2(R^d), which are\nLipschitz continuous to the action of diffeomorphisms. A scattering propagator\nis a path ordered product of non-linear and non-commuting operators, each of\nwhich computes the modulus of a wavelet transform. A local integration defines\na windowed scattering transform, which is proved to be Lipschitz continuous to\nthe action of diffeomorphisms. As the window size increases, it converges to a\nwavelet scattering transform which is translation invariant. Scattering\ncoefficients also provide representations of stationary processes. Expected\nvalues depend upon high order moments and can discriminate processes having the\nsame power spectrum. Scattering operators are extended on L2 (G), where G is a\ncompact Lie group, and are invariant under the action of G. Combining a\nscattering on L2(R^d) and on Ld (SO(d)) defines a translation and rotation\ninvariant scattering on L2(R^d).\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 07:55:51 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2011 17:44:29 GMT"}, {"version": "v3", "created": "Sun, 15 Apr 2012 11:20:32 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Mallat", "St\u00e9phane", ""]]}, {"id": "1101.2312", "submitter": "Jan Urban", "authors": "Jan Urban", "title": "Automatic segmentation of HeLa cell images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the possibilities for segmentation of cells from their\nbackground and each other in digital image were tested, combined and improoved.\nLot of images with young, adult and mixture cells were able to prove the\nquality of described algorithms. Proper segmentation is one of the main task of\nimage analysis and steps order differ from work to work, depending on input\nimages. Reply for biologicaly given question was looking for in this work,\nincluding filtration, details emphasizing, segmentation and sphericity\ncomputing. Order of algorithms and way to searching for them was also\ndescribed. Some questions and ideas for further work were mentioned in the\nconclusion part.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 10:16:11 GMT"}], "update_date": "2011-01-13", "authors_parsed": [["Urban", "Jan", ""]]}, {"id": "1101.2427", "submitter": "Eduardo Valle", "authors": "Eduardo Valle, Sandra de Avila, Antonio da Luz Jr., Fillipe de Souza,\n  Marcelo Coelho, Arnaldo Ara\\'ujo", "title": "Content-Based Filtering for Video Sharing Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare the use of several features in the task of content\nfiltering for video social networks, a very challenging task, not only because\nthe unwanted content is related to very high-level semantic concepts (e.g.,\npornography, violence, etc.) but also because videos from social networks are\nextremely assorted, preventing the use of constrained a priori information. We\npropose a simple method, able to combine diverse evidence, coming from\ndifferent features and various video elements (entire video, shots, frames,\nkeyframes, etc.). We evaluate our method in three social network applications,\nrelated to the detection of unwanted content - pornographic videos, violent\nvideos, and videos posted to artificially manipulate popularity scores. Using\nchallenging test databases, we show that this simple scheme is able to obtain\ngood results, provided that adequate features are chosen. Moreover, we\nestablish a representation using codebooks of spatiotemporal local descriptors\nas critical to the success of the method in all three contexts. This is\nconsequential, since the state-of-the-art still relies heavily on static\nfeatures for the tasks addressed.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 19:13:26 GMT"}], "update_date": "2011-01-13", "authors_parsed": [["Valle", "Eduardo", ""], ["de Avila", "Sandra", ""], ["Luz", "Antonio da", "Jr."], ["de Souza", "Fillipe", ""], ["Coelho", "Marcelo", ""], ["Ara\u00fajo", "Arnaldo", ""]]}, {"id": "1101.2491", "submitter": "Vikas Dongre", "authors": "V J Dongre, V H Mankar", "title": "A Review of Research on Devnagari Character Recognition", "comments": "8 pages, 1 Figure, 8 Tables, Journal paper", "journal-ref": "International Journal of Computer Applications (0975 - 8887)\n  Volume 12- No.2, November 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  English Character Recognition (CR) has been extensively studied in the last\nhalf century and progressed to a level, sufficient to produce technology driven\napplications. But same is not the case for Indian languages which are\ncomplicated in terms of structure and computations. Rapidly growing\ncomputational power may enable the implementation of Indic CR methodologies.\nDigital document processing is gaining popularity for application to office and\nlibrary automation, bank and postal services, publishing houses and\ncommunication technology. Devnagari being the national language of India,\nspoken by more than 500 million people, should be given special attention so\nthat document retrieval and analysis of rich ancient and modern Indian\nliterature can be effectively done. This article is intended to serve as a\nguide and update for the readers, working in the Devnagari Optical Character\nRecognition (DOCR) area. An overview of DOCR systems is presented and the\navailable DOCR techniques are reviewed. The current status of DOCR is discussed\nand directions for future research are suggested.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jan 2011 04:00:30 GMT"}], "update_date": "2011-01-14", "authors_parsed": [["Dongre", "V J", ""], ["Mankar", "V H", ""]]}, {"id": "1101.2987", "submitter": "Mahesh  Pal Dr.", "authors": "Mahesh Pal", "title": "Support vector machines/relevance vector machine for remote sensing\n  classification: A review", "comments": "19 pages", "journal-ref": "Proceeding of the Workshop on Application of advanced soft\n  computing Techniques in Geo-spatial Data Analysis. Department of Civil\n  Engineering, IIT Bombay, Sept. 22-23,2008, 211-227", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based machine learning algorithms are based on mapping data from the\noriginal input feature space to a kernel feature space of higher dimensionality\nto solve a linear problem in that space. Over the last decade, kernel based\nclassification and regression approaches such as support vector machines have\nwidely been used in remote sensing as well as in various civil engineering\napplications. In spite of their better performance with different datasets,\nsupport vector machines still suffer from shortcomings such as\nvisualization/interpretation of model, choice of kernel and kernel specific\nparameter as well as the regularization parameter. Relevance vector machines\nare another kernel based approach being explored for classification and\nregression with in last few years. The advantages of the relevance vector\nmachines over the support vector machines is the availability of probabilistic\npredictions, using arbitrary kernel functions and not requiring setting of the\nregularization parameter. This paper presents a state-of-the-art review of SVM\nand RVM in remote sensing and provides some details of their use in other civil\nengineering application also.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jan 2011 13:29:12 GMT"}], "update_date": "2011-01-18", "authors_parsed": [["Pal", "Mahesh", ""]]}, {"id": "1101.3124", "submitter": "Xinyu Xing", "authors": "Xinyu Xing, Yu-Li Liang, Hanqiang Cheng, Jianxun Dang, Sui Huang,\n  Richard Han, Xue Liu, Qin Lv, Shivakant Mishra", "title": "SafeVchat: Detecting Obscene Content and Misbehaving Users in Online\n  Video Chat Services", "comments": "The 20th International World Wide Web Conference (WWW 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online video chat services such as Chatroulette, Omegle, and vChatter that\nrandomly match pairs of users in video chat sessions are fast becoming very\npopular, with over a million users per month in the case of Chatroulette. A key\nproblem encountered in such systems is the presence of flashers and obscene\ncontent. This problem is especially acute given the presence of underage minors\nin such systems. This paper presents SafeVchat, a novel solution to the problem\nof flasher detection that employs an array of image detection algorithms. A key\ncontribution of the paper concerns how the results of the individual detectors\nare fused together into an overall decision classifying the user as misbehaving\nor not, based on Dempster-Shafer Theory. The paper introduces a novel,\nmotion-based skin detection method that achieves significantly higher recall\nand better precision. The proposed methods have been evaluated over real world\ndata and image traces obtained from Chatroulette.com.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jan 2011 05:11:04 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["Xing", "Xinyu", ""], ["Liang", "Yu-Li", ""], ["Cheng", "Hanqiang", ""], ["Dang", "Jianxun", ""], ["Huang", "Sui", ""], ["Han", "Richard", ""], ["Liu", "Xue", ""], ["Lv", "Qin", ""], ["Mishra", "Shivakant", ""]]}, {"id": "1101.3354", "submitter": "Stephen O'Hara", "authors": "Stephen O'Hara and Bruce A. Draper", "title": "Introduction to the Bag of Features Paradigm for Image Classification\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen the growing popularity of Bag of Features (BoF)\napproaches to many computer vision tasks, including image classification, video\nsearch, robot localization, and texture recognition. Part of the appeal is\nsimplicity. BoF methods are based on orderless collections of quantized local\nimage descriptors; they discard spatial information and are therefore\nconceptually and computationally simpler than many alternative methods. Despite\nthis, or perhaps because of this, BoF-based systems have set new performance\nstandards on popular image classification benchmarks and have achieved\nscalability breakthroughs in image retrieval. This paper presents an\nintroduction to BoF image representations, describes critical design choices,\nand surveys the BoF literature. Emphasis is placed on recent techniques that\nmitigate quantization errors, improve feature detection, and speed up image\nretrieval. At the same time, unresolved issues and fundamental challenges are\nraised. Among the unresolved issues are determining the best techniques for\nsampling images, describing local image features, and evaluating system\nperformance. Among the more fundamental challenges are how and whether BoF\nmethods can contribute to localizing objects in complex images, or to\nassociating high-level semantics with natural images. This survey should be\nuseful both for introducing new investigators to the field and for providing\nexisting researchers with a consolidated reference to related work.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jan 2011 23:27:08 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["O'Hara", "Stephen", ""], ["Draper", "Bruce A.", ""]]}, {"id": "1101.3381", "submitter": "Federico Schl\\\"uter", "authors": "Facundo Bromberg and Federico Schl\\\"uter", "title": "Efficient Independence-Based MAP Approach for Robust Markov Networks\n  Structure Discovery", "comments": "28 pages, 8 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the IB-score, a family of independence-based score\nfunctions for robust learning of Markov networks independence structures.\nMarkov networks are a widely used graphical representation of probability\ndistributions, with many applications in several fields of science. The main\nadvantage of the IB-score is the possibility of computing it without the need\nof estimation of the numerical parameters, an NP-hard problem, usually solved\nthrough an approximate, data-intensive, iterative optimization. We derive a\nformal expression for the IB-score from first principles, mainly maximum a\nposteriori and conditional independence properties, and exemplify several\ninstantiations of it, resulting in two novel algorithms for structure learning:\nIBMAP-HC and IBMAP-TS. Experimental results over both artificial and real world\ndata show these algorithms achieve important error reductions in the learnt\nstructures when compared with the state-of-the-art independence-based structure\nlearning algorithm GSMN, achieving increments of more than 50% in the amount of\nindependencies they encode correctly, and in some cases, learning correctly\nover 90% of the edges that GSMN learnt incorrectly. Theoretical analysis shows\nIBMAP-HC proceeds efficiently, achieving these improvements in a time\npolynomial to the number of random variables in the domain.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 04:10:01 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bromberg", "Facundo", ""], ["Schl\u00fcter", "Federico", ""]]}, {"id": "1101.3391", "submitter": "Thorsten Riess", "authors": "Thorsten Riess, Christian Dietz, Martin Tomas, Elisa Ferrando-May and\n  Dorit Merhof", "title": "Automated Image Processing for the Analysis of DNA Repair Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient repair of cellular DNA is essential for the maintenance and\ninheritance of genomic information. In order to cope with the high frequency of\nspontaneous and induced DNA damage, a multitude of repair mechanisms have\nevolved. These are enabled by a wide range of protein factors specifically\nrecognizing different types of lesions and finally restoring the normal DNA\nsequence. This work focuses on the repair factor XPC (xeroderma pigmentosum\ncomplementation group C), which identifies bulky DNA lesions and initiates\ntheir removal via the nucleotide excision repair pathway. The binding of XPC to\ndamaged DNA can be visualized in living cells by following the accumulation of\na fluorescent XPC fusion at lesions induced by laser microirradiation in a\nfluorescence microscope. In this work, an automated image processing pipeline\nis presented which allows to identify and quantify the accumulation reaction\nwithout any user interaction. The image processing pipeline comprises a\npreprocessing stage where the image stack data is filtered and the nucleus of\ninterest is segmented. Afterwards, the images are registered to each other in\norder to account for movements of the cell, and then a bounding box enclosing\nthe XPC-specific signal is automatically determined. Finally, the\ntime-dependent relocation of XPC is evaluated by analyzing the intensity change\nwithin this box. Comparison of the automated processing results with the manual\nevaluation yields qualitatively similar results. However, the automated\nanalysis provides more accurate, reproducible data with smaller standard\nerrors. The image processing pipeline presented in this work allows for an\nefficient analysis of large amounts of experimental data with no user\ninteraction required.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 06:48:35 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["Riess", "Thorsten", ""], ["Dietz", "Christian", ""], ["Tomas", "Martin", ""], ["Ferrando-May", "Elisa", ""], ["Merhof", "Dorit", ""]]}, {"id": "1101.3755", "submitter": "Shriprakash Sinha", "authors": "Shriprakash Sinha", "title": "Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev\n  Inequality", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating adequate number of clusters in multidimensional data is an open\narea of research, given a level of compromise made on the quality of acceptable\nresults. The manuscript addresses the issue by formulating a transductive\ninductive learning algorithm which uses multivariate Chebyshev inequality.\nConsidering clustering problem in imaging, theoretical proofs for a particular\nlevel of compromise are derived to show the convergence of the reconstruction\nerror to a finite value with increasing (a) number of unseen examples and (b)\nthe number of clusters, respectively. Upper bounds for these error rates are\nalso proved. Non-parametric estimates of these error from a random sample of\nsequences empirically point to a stable number of clusters. Lastly, the\ngeneralization of algorithm can be applied to multidimensional data sets from\ndifferent fields.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 19:05:15 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 14:27:35 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Sinha", "Shriprakash", ""]]}, {"id": "1101.4301", "submitter": "Michael Bronstein", "authors": "Artiom Kovnatsky, Michael M. Bronstein, Alexander M. Bronstein, Ron\n  Kimmel", "title": "Diffusion framework for geometric and photometric data fusion in\n  non-rigid shape analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the use of the diffusion geometry framework for the\nfusion of geometric and photometric information in local and global shape\ndescriptors. Our construction is based on the definition of a diffusion process\non the shape manifold embedded into a high-dimensional space where the\nembedding coordinates represent the photometric information. Experimental\nresults show that such data fusion is useful in coping with different\nchallenges of shape analysis where pure geometric and pure photometric methods\nfail.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jan 2011 16:41:20 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Kovnatsky", "Artiom", ""], ["Bronstein", "Michael M.", ""], ["Bronstein", "Alexander M.", ""], ["Kimmel", "Ron", ""]]}, {"id": "1101.4373", "submitter": "Klaus Frick", "authors": "Klaus Frick, Philipp Marnitz, Axel Munk", "title": "Statistical Multiresolution Dantzig Estimation in Imaging: Fundamental\n  Concepts and Algorithmic Framework", "comments": null, "journal-ref": "Electron. J. Stat. 6 (2012) 231-268", "doi": "10.1214/12-EJS671", "report-no": null, "categories": "stat.AP cs.CV cs.SY math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are concerned with fully automatic and locally adaptive\nestimation of functions in a \"signal + noise\"-model where the regression\nfunction may additionally be blurred by a linear operator, e.g. by a\nconvolution. To this end, we introduce a general class of statistical\nmultiresolution estimators and develop an algorithmic framework for computing\nthose. By this we mean estimators that are defined as solutions of convex\noptimization problems with supremum-type constraints. We employ a combination\nof the alternating direction method of multipliers with Dykstra's algorithm for\ncomputing orthogonal projections onto intersections of convex sets and prove\nnumerical convergence. The capability of the proposed method is illustrated by\nvarious examples from imaging and signal detection.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jan 2011 13:46:43 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2011 00:24:44 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2012 09:25:47 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Frick", "Klaus", ""], ["Marnitz", "Philipp", ""], ["Munk", "Axel", ""]]}, {"id": "1101.4749", "submitter": "Osman  G\\\"unay", "authors": "Osman Gunay and Behcet Ugur Toreyin and Kivanc Kose and A. Enis Cetin", "title": "Online Adaptive Decision Fusion Framework Based on Entropic Projections\n  onto Convex Sets with Application to Wildfire Detection in Video", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1117/1.3595426", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an Entropy functional based online Adaptive Decision Fusion\n(EADF) framework is developed for image analysis and computer vision\napplications. In this framework, it is assumed that the compound algorithm\nconsists of several sub-algorithms each of which yielding its own decision as a\nreal number centered around zero, representing the confidence level of that\nparticular sub-algorithm. Decision values are linearly combined with weights\nwhich are updated online according to an active fusion method based on\nperforming entropic projections onto convex sets describing sub-algorithms. It\nis assumed that there is an oracle, who is usually a human operator, providing\nfeedback to the decision fusion method. A video based wildfire detection system\nis developed to evaluate the performance of the algorithm in handling the\nproblems where data arrives sequentially. In this case, the oracle is the\nsecurity guard of the forest lookout tower verifying the decision of the\ncombined algorithm. Simulation results are presented. The EADF framework is\nalso tested with a standard dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 09:11:49 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Gunay", "Osman", ""], ["Toreyin", "Behcet Ugur", ""], ["Kose", "Kivanc", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1101.4918", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "Using Feature Weights to Improve Performance of Neural Networks", "comments": "2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different features have different relevance to a particular learning problem.\nSome features are less relevant; while some very important. Instead of\nselecting the most relevant features using feature selection, an algorithm can\nbe given this knowledge of feature importance based on expert opinion or prior\nlearning. Learning can be faster and more accurate if learners take feature\nimportance into account. Correlation aided Neural Networks (CANN) is presented\nwhich is such an algorithm. CANN treats feature importance as the correlation\ncoefficient between the target attribute and the features. CANN modifies normal\nfeed-forward Neural Network to fit both correlation values and training data.\nEmpirical evaluation shows that CANN is faster and more accurate than applying\nthe two step approach of feature selection and then using normal learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 20:24:25 GMT"}], "update_date": "2011-01-26", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1101.4924", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "A Generalized Method for Integrating Rule-based Knowledge into Inductive\n  Methods Through Virtual Sample Creation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid learning methods use theoretical knowledge of a domain and a set of\nclassified examples to develop a method for classification. Methods that use\ndomain knowledge have been shown to perform better than inductive learners.\nHowever, there is no general method to include domain knowledge into all\ninductive learning algorithms as all hybrid methods are highly specialized for\na particular algorithm. We present an algorithm that will take domain knowledge\nin the form of propositional rules, generate artificial examples from the rules\nand also remove instances likely to be flawed. This enriched dataset then can\nbe used by any learning algorithm. Experimental results of different scenarios\nare shown that demonstrate this method to be more effective than simple\ninductive learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 20:42:01 GMT"}], "update_date": "2011-01-26", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1101.5320", "submitter": "Laurent Jacques", "authors": "Laurent Jacques, Laurent Duval, Caroline Chaux, Gabriel Peyr\\'e", "title": "A Panorama on Multiscale Geometric Representations, Intertwining\n  Spatial, Directional and Frequency Selectivity", "comments": "65 pages, 33 figures, 303 references", "journal-ref": "Signal Processing, Volume 91, Issue 12, December 2011, Pages\n  2699-2730", "doi": "10.1016/j.sigpro.2011.04.025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The richness of natural images makes the quest for optimal representations in\nimage processing and computer vision challenging. The latter observation has\nnot prevented the design of image representations, which trade off between\nefficiency and complexity, while achieving accurate rendering of smooth regions\nas well as reproducing faithful contours and textures. The most recent ones,\nproposed in the past decade, share an hybrid heritage highlighting the\nmultiscale and oriented nature of edges and patterns in images. This paper\npresents a panorama of the aforementioned literature on decompositions in\nmultiscale, multi-orientation bases or dictionaries. They typically exhibit\nredundancy to improve sparsity in the transformed domain and sometimes its\ninvariance with respect to simple geometric deformations (translation,\nrotation). Oriented multiscale dictionaries extend traditional wavelet\nprocessing and may offer rotation invariance. Highly redundant dictionaries\nrequire specific algorithms to simplify the search for an efficient (sparse)\nrepresentation. We also discuss the extension of multiscale geometric\ndecompositions to non-Euclidean domains such as the sphere or arbitrary meshed\nsurfaces. The etymology of panorama suggests an overview, based on a choice of\npartially overlapping \"pictures\". We hope that this paper will contribute to\nthe appreciation and apprehension of a stream of current research directions in\nimage understanding.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 15:53:30 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2011 07:18:20 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Jacques", "Laurent", ""], ["Duval", "Laurent", ""], ["Chaux", "Caroline", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1101.5687", "submitter": "Alex Bronstein", "authors": "Jonathan Pokrass, Alexander M. Bronstein, Michael M. Bronstein", "title": "A correspondence-less approach to matching of deformable shapes", "comments": "Preprint submitted to Intl. Conference on Scale Space and Variational\n  Methods (SSVM'11)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a match between partially available deformable shapes is a\nchallenging problem with numerous applications. The problem is usually\napproached by computing local descriptors on a pair of shapes and then\nestablishing a point-wise correspondence between the two. In this paper, we\nintroduce an alternative correspondence-less approach to matching fragments to\nan entire shape undergoing a non-rigid deformation. We use diffusion geometric\ndescriptors and optimize over the integration domains on which the integral\ndescriptors of the two parts match. The problem is regularized using the\nMumford-Shah functional. We show an efficient discretization based on the\nAmbrosio-Tortorelli approximation generalized to triangular meshes. Experiments\ndemonstrating the success of the proposed method are presented.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jan 2011 10:37:23 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Pokrass", "Jonathan", ""], ["Bronstein", "Alexander M.", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1101.5766", "submitter": "Joan Bruna", "authors": "Joan Bruna and St\\'ephane Mallat", "title": "Geometric Models with Co-occurrence Groups", "comments": "6 pages, ESANN 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A geometric model of sparse signal representations is introduced for classes\nof signals. It is computed by optimizing co-occurrence groups with a maximum\nlikelihood estimate calculated with a Bernoulli mixture model. Applications to\nface image compression and MNIST digit classification illustrate the\napplicability of this model.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 12:05:12 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1101.5785", "submitter": "Guoshen Yu", "authors": "Guoshen Yu and Guillermo Sapiro", "title": "Statistical Compressed Sensing of Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2168521", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A novel framework of compressed sensing, namely statistical compressed\nsensing (SCS), that aims at efficiently sampling a collection of signals that\nfollow a statistical distribution, and achieving accurate reconstruction on\naverage, is introduced. SCS based on Gaussian models is investigated in depth.\nFor signals that follow a single Gaussian model, with Gaussian or Bernoulli\nsensing matrices of O(k) measurements, considerably smaller than the O(k\nlog(N/k)) required by conventional CS based on sparse models, where N is the\nsignal dimension, and with an optimal decoder implemented via linear filtering,\nsignificantly faster than the pursuit decoders applied in conventional CS, the\nerror of SCS is shown tightly upper bounded by a constant times the best k-term\napproximation error, with overwhelming probability. The failure probability is\nalso significantly smaller than that of conventional sparsity-oriented CS.\nStronger yet simpler results further show that for any sensing matrix, the\nerror of Gaussian SCS is upper bounded by a constant times the best k-term\napproximation with probability one, and the bound constant can be efficiently\ncalculated. For Gaussian mixture models (GMMs), that assume multiple Gaussian\ndistributions and that each signal follows one of them with an unknown index, a\npiecewise linear estimator is introduced to decode SCS. The accuracy of model\nselection, at the heart of the piecewise linear decoder, is analyzed in terms\nof the properties of the Gaussian distributions and the number of sensing\nmeasurements. A maximum a posteriori expectation-maximization algorithm that\niteratively estimates the Gaussian models parameters, the signals model\nselection, and decodes the signals, is presented for GMM-based SCS. In real\nimage sensing applications, GMM-based SCS is shown to lead to improved results\ncompared to conventional CS, at a considerably lower computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 17:16:55 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Yu", "Guoshen", ""], ["Sapiro", "Guillermo", ""]]}]